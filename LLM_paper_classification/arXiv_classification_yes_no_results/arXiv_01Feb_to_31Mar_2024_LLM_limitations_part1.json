[
    {
        "title": "Multi-hop Question Answering under Temporal Knowledge Editing",
        "authors": [
            "Keyuan Cheng",
            "Gang Lin",
            "Haoyang Fei",
            "Yuxuan zhai",
            "Lu Yu",
            "Muhammad Asif Ali",
            "Lijie Hu",
            "Di Wang"
        ],
        "published": "2024-03-30T23:22:51Z",
        "summary": "Multi-hop question answering (MQA) under knowledge editing (KE) has garnered\nsignificant attention in the era of large language models. However, existing\nmodels for MQA under KE exhibit poor performance when dealing with questions\ncontaining explicit temporal contexts. To address this limitation, we propose a\nnovel framework, namely TEMPoral knowLEdge augmented Multi-hop Question\nAnswering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a\ntime-aware graph (TAG) to store edit knowledge in a structured manner. Then,\nthrough our proposed inference path, structural retrieval, and joint reasoning\nstages, TEMPLE-MQA effectively discerns temporal contexts within the question\nquery. Experiments on benchmark datasets demonstrate that TEMPLE-MQA\nsignificantly outperforms baseline models. Additionally, we contribute a new\ndataset, namely TKEMQA, which serves as the inaugural benchmark tailored\nspecifically for MQA with temporal scopes.",
        "pdf_link": "https://arxiv.org/pdf/2404.00492v1.pdf"
    },
    {
        "title": "PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression",
        "authors": [
            "Muhammad Asif Ali",
            "Zhengping Li",
            "Shu Yang",
            "Keyuan Cheng",
            "Yang Cao",
            "Tianhao Huang",
            "Lijie Hu",
            "Lu Yu",
            "Di Wang"
        ],
        "published": "2024-03-30T23:07:58Z",
        "summary": "Large language models (LLMs) have shown exceptional abilities for multiple\ndifferent natural language processing tasks. While prompting is a crucial tool\nfor LLM inference, we observe that there is a significant cost associated with\nexceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead\nto sub-standard results in terms of readability and interpretability of the\ncompressed prompt, with a detrimental impact on prompt utility. To address\nthis, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an\neffective strategy for prompt compression over task-agnostic and task-aware\nprompts. PROMPT-SAW uses the prompt's textual information to build a graph,\nlater extracts key information elements in the graph to come up with the\ncompressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the\nexisting GSM8k benchmark for task-agnostic prompts in order to provide a\ncomprehensive evaluation platform. Experimental evaluation using benchmark\ndatasets shows that prompts compressed by PROMPT-SAW are not only better in\nterms of readability, but they also outperform the best-performing baseline\nmodels by up to 14.3 and 13.7 respectively for task-aware and task-agnostic\nsettings while compressing the original prompt text by 33.0 and 56.7.",
        "pdf_link": "https://arxiv.org/pdf/2404.00489v1.pdf"
    },
    {
        "title": "Contextual AI Journaling: Integrating LLM and Time Series Behavioral Sensing Technology to Promote Self-Reflection and Well-being using the MindScape App",
        "authors": [
            "Subigya Nepal",
            "Arvind Pillai",
            "William Campbell",
            "Talie Massachi",
            "Eunsol Soul Choi",
            "Orson Xu",
            "Joanna Kuc",
            "Jeremy Huckins",
            "Jason Holden",
            "Colin Depp",
            "Nicholas Jacobson",
            "Mary Czerwinski",
            "Eric Granholm",
            "Andrew T. Campbell"
        ],
        "published": "2024-03-30T23:01:34Z",
        "summary": "MindScape aims to study the benefits of integrating time series behavioral\npatterns (e.g., conversational engagement, sleep, location) with Large Language\nModels (LLMs) to create a new form of contextual AI journaling, promoting\nself-reflection and well-being. We argue that integrating behavioral sensing in\nLLMs will likely lead to a new frontier in AI. In this Late-Breaking Work\npaper, we discuss the MindScape contextual journal App design that uses LLMs\nand behavioral sensing to generate contextual and personalized journaling\nprompts crafted to encourage self-reflection and emotional development. We also\ndiscuss the MindScape study of college students based on a preliminary user\nstudy and our upcoming study to assess the effectiveness of contextual AI\njournaling in promoting better well-being on college campuses. MindScape\nrepresents a new application class that embeds behavioral intelligence in AI.",
        "pdf_link": "https://arxiv.org/pdf/2404.00487v1.pdf"
    },
    {
        "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
        "authors": [
            "Shu Yang",
            "Jiayuan Su",
            "Han Jiang",
            "Mengdi Li",
            "Keyuan Cheng",
            "Muhammad Asif Ali",
            "Lijie Hu",
            "Di Wang"
        ],
        "published": "2024-03-30T22:41:05Z",
        "summary": "With the rise of large language models (LLMs), ensuring they embody the\nprinciples of being helpful, honest, and harmless (3H), known as Human\nAlignment, becomes crucial. While existing alignment methods like RLHF, DPO,\netc., effectively fine-tune LLMs to match preferences in the preference\ndataset, they often lead LLMs to highly receptive human input and external\nevidence, even when this information is poisoned. This leads to a tendency for\nLLMs to be Adaptive Chameleons when external evidence conflicts with their\nparametric memory. This exacerbates the risk of LLM being attacked by external\npoisoned data, which poses a significant security risk to LLM system\napplications such as Retrieval-augmented generation (RAG). To address the\nchallenge, we propose a novel framework: Dialectical Alignment (DA), which (1)\nutilizes AI feedback to identify optimal strategies for LLMs to navigate\ninter-context conflicts and context-memory conflicts with different external\nevidence in context window (i.e., different ratios of poisoned factual\ncontexts); (2) constructs the SFT dataset as well as the preference dataset\nbased on the AI feedback and strategies above; (3) uses the above datasets for\nLLM alignment to defense poisoned context attack while preserving the\neffectiveness of in-context knowledge editing. Our experiments show that the\ndialectical alignment model improves poisoned data attack defense by 20 and\ndoes not require any additional prompt engineering or prior declaration of\n``you may be attacked`` to the LLMs' context window.",
        "pdf_link": "https://arxiv.org/pdf/2404.00486v1.pdf"
    },
    {
        "title": "Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4",
        "authors": [
            "Aryo Pradipta Gema",
            "Giwon Hong",
            "Pasquale Minervini",
            "Luke Daines",
            "Beatrice Alex"
        ],
        "published": "2024-03-30T22:27:21Z",
        "summary": "The NLI4CT task assesses Natural Language Inference systems in predicting\nwhether hypotheses entail or contradict evidence from Clinical Trial Reports.\nIn this study, we evaluate various Large Language Models (LLMs) with multiple\nstrategies, including Chain-of-Thought, In-Context Learning, and\nParameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the\nconsistency of LLMs by merging adapters that were fine-tuned separately using\ntriplet and language modelling objectives. We found that merging the two PEFT\nadapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs.\nHowever, our novel methods did not produce more accurate results than GPT-4 in\nterms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks\njoint-first in the competition with 0.8328. Finally, our contamination analysis\nwith GPT-4 indicates that there was no test data leakage.",
        "pdf_link": "https://arxiv.org/pdf/2404.00484v1.pdf"
    },
    {
        "title": "Linguistic Calibration of Language Models",
        "authors": [
            "Neil Band",
            "Xuechen Li",
            "Tengyu Ma",
            "Tatsunori Hashimoto"
        ],
        "published": "2024-03-30T20:47:55Z",
        "summary": "Language models (LMs) may lead their users to make suboptimal downstream\ndecisions when they confidently hallucinate. This issue can be mitigated by\nhaving the LM verbally convey the probability that its claims are correct, but\nexisting models cannot produce text with calibrated confidence statements.\nThrough the lens of decision-making, we formalize linguistic calibration for\nlong-form generations: an LM is linguistically calibrated if its generations\nenable its users to make calibrated probabilistic predictions. This definition\nenables a training framework where a supervised finetuning step bootstraps an\nLM to emit long-form generations with confidence statements such as \"I estimate\na 30% chance of...\" or \"I am certain that...\", followed by a reinforcement\nlearning step which rewards generations that enable a user to provide\ncalibrated answers to related questions. We linguistically calibrate Llama 2 7B\nand find in automated and human evaluations of long-form generations that it is\nsignificantly more calibrated than strong finetuned factuality baselines with\ncomparable accuracy. These findings generalize under distribution shift on\nquestion-answering and under a significant task shift to person biography\ngeneration. Our results demonstrate that long-form generations may be\ncalibrated end-to-end by constructing an objective in the space of the\npredictions that users make in downstream decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2404.00474v1.pdf"
    },
    {
        "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
        "authors": [
            "Eli Schwartz",
            "Leshem Choshen",
            "Joseph Shtok",
            "Sivan Doveh",
            "Leonid Karlinsky",
            "Assaf Arbelle"
        ],
        "published": "2024-03-30T19:46:59Z",
        "summary": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark.",
        "pdf_link": "https://arxiv.org/pdf/2404.00459v1.pdf"
    },
    {
        "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
        "authors": [
            "Letian Peng",
            "Zilong Wang",
            "Feng Yao",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2024-03-30T19:43:45Z",
        "summary": "Information extraction (IE) is a fundamental area in natural language\nprocessing where prompting large language models (LLMs), even with in-context\nexamples, cannot defeat small LMs tuned on very small IE datasets. We observe\nthat IE tasks, such as named entity recognition and relation extraction, all\nfocus on extracting important information, which can be formalized as a\nlabel-to-span matching. In this paper, we propose a novel framework MetaIE to\nbuild a small LM as meta-model by learning to extract \"important information\",\ni.e., the meta-understanding of IE, so that this meta-model can be adapted to\nall kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains\nthe small LM via a symbolic distillation from an LLM following the\nlabel-to-span scheme. We construct the distillation dataset via sampling\nsentences from language model pre-training datasets (e.g., OpenWebText in our\nimplementation) and prompting an LLM to identify the typed spans of \"important\ninformation\". We evaluate the meta-model under the few-shot adaptation setting.\nExtensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer\na better starting point for few-shot tuning on IE datasets and outperform other\nmeta-models from (1) vanilla language model pre-training, (2) multi-IE-task\npre-training with human annotations, and (3) single-IE-task symbolic\ndistillation from LLM. Moreover, we provide comprehensive analyses of MetaIE,\nsuch as the size of the distillation dataset, the meta-model architecture, and\nthe size of the meta-model.",
        "pdf_link": "https://arxiv.org/pdf/2404.00457v1.pdf"
    },
    {
        "title": "Do Vision-Language Models Understand Compound Nouns?",
        "authors": [
            "Sonal Kumar",
            "Sreyan Ghosh",
            "S Sakshi",
            "Utkarsh Tyagi",
            "Dinesh Manocha"
        ],
        "published": "2024-03-30T16:54:45Z",
        "summary": "Open-vocabulary vision-language models (VLMs) like CLIP, trained using\ncontrastive loss, have emerged as a promising new paradigm for text-to-image\nretrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as\nwell as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark\nwith 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in\ninterpreting CNs. The Compun benchmark challenges a VLM for text-to-image\nretrieval where, given a text prompt with a CN, the task is to select the\ncorrect image that shows the CN among a pair of distractor images that show the\nconstituent nouns that make up the CN. Next, we perform an in-depth analysis to\nhighlight CLIPs' limited understanding of certain types of CNs. Finally, we\npresent an alternative framework that moves beyond hand-written templates for\ntext prompts widely used by CLIP-like models. We employ a Large Language Model\nto generate multiple diverse captions that include the CN as an object in the\nscene described by the caption. Our proposed method improves CN understanding\nof CLIP by 8.25% on Compun. Code and benchmark are available at:\nhttps://github.com/sonalkum/Compun",
        "pdf_link": "https://arxiv.org/pdf/2404.00419v1.pdf"
    },
    {
        "title": "CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP",
        "authors": [
            "Chandra Kiran Reddy Evuru",
            "Sreyan Ghosh",
            "Sonal Kumar",
            "Ramaneswaran S",
            "Utkarsh Tyagi",
            "Dinesh Manocha"
        ],
        "published": "2024-03-30T16:47:06Z",
        "summary": "We present CoDa (Constrained Generation based Data Augmentation), a\ncontrollable, effective, and training-free data augmentation technique for\nlow-resource (data-scarce) NLP. Our approach is based on prompting\noff-the-shelf instruction-following Large Language Models (LLMs) for generating\ntext that satisfies a set of constraints. Precisely, we extract a set of simple\nconstraints from every instance in the low-resource dataset and verbalize them\nto prompt an LLM to generate novel and diverse training instances. Our findings\nreveal that synthetic data that follows simple constraints in the downstream\ndataset act as highly effective augmentations, and CoDa can achieve this\nwithout intricate decoding-time constrained generation techniques or\nfine-tuning with complex algorithms that eventually make the model biased\ntoward the small number of training instances. Additionally, CoDa is the first\nframework that provides users explicit control over the augmentation generation\nprocess, thereby also allowing easy adaptation to several domains. We\ndemonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3\nlow-resource settings. CoDa outperforms all our baselines, qualitatively and\nquantitatively, with improvements of 0.12%-7.19%. Code is available here:\nhttps://github.com/Sreyan88/CoDa",
        "pdf_link": "https://arxiv.org/pdf/2404.00415v1.pdf"
    },
    {
        "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order",
        "authors": [
            "Taishi Nakamura",
            "Mayank Mishra",
            "Simone Tedeschi",
            "Yekun Chai",
            "Jason T Stillerman",
            "Felix Friedrich",
            "Prateek Yadav",
            "Tanmay Laud",
            "Vu Minh Chien",
            "Terry Yue Zhuo",
            "Diganta Misra",
            "Ben Bogin",
            "Xuan-Son Vu",
            "Marzena Karpinska",
            "Arnav Varma Dantuluri",
            "Wojciech Kusa",
            "Tommaso Furlanello",
            "Rio Yokota",
            "Niklas Muennighoff",
            "Suhas Pai",
            "Tosin Adewumi",
            "Veronika Laippala",
            "Xiaozhe Yao",
            "Adalberto Junior",
            "Alpay Ariyak",
            "Aleksandr Drozd",
            "Jordan Clive",
            "Kshitij Gupta",
            "Liangyu Chen",
            "Qi Sun",
            "Ken Tsui",
            "Noah Persaud",
            "Nour Fahmy",
            "Tianlong Chen",
            "Mohit Bansal",
            "Nicolo Monti",
            "Tai Dang",
            "Ziyang Luo",
            "Tien-Tung Bui",
            "Roberto Navigli",
            "Virendra Mehta",
            "Matthew Blumberg",
            "Victor May",
            "Huu Nguyen",
            "Sampo Pyysalo"
        ],
        "published": "2024-03-30T15:38:54Z",
        "summary": "Pretrained language models underpin several AI applications, but their high\ncomputational cost for training limits accessibility. Initiatives such as BLOOM\nand StarCoder aim to democratize access to pretrained models for collaborative\ncommunity development. However, such existing models face challenges: limited\nmultilingual capabilities, continual pretraining causing catastrophic\nforgetting, whereas pretraining from scratch is computationally expensive, and\ncompliance with AI safety and development laws. This paper presents Aurora-M, a\n15B parameter multilingual open-source model trained on English, Finnish,\nHindi, Japanese, Vietnamese, and code. Continually pretrained from\nStarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence. Aurora-M is rigorously evaluated across various tasks and\nlanguages, demonstrating robustness against catastrophic forgetting and\noutperforming alternatives in multilingual settings, particularly in safety\nevaluations. To promote responsible open-source LLM development, Aurora-M and\nits variants are released at\nhttps://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .",
        "pdf_link": "https://arxiv.org/pdf/2404.00399v1.pdf"
    },
    {
        "title": "Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks",
        "authors": [
            "Hyunjae Kim",
            "Hyeon Hwang",
            "Jiwoo Lee",
            "Sihyeon Park",
            "Dain Kim",
            "Taewhoo Lee",
            "Chanwoong Yoon",
            "Jiwoong Sohn",
            "Donghee Choi",
            "Jaewoo Kang"
        ],
        "published": "2024-03-30T14:09:00Z",
        "summary": "While recent advancements in commercial large language models (LM) have shown\npromising results in medical tasks, their closed-source nature poses\nsignificant privacy and security concerns, hindering their widespread use in\nthe medical field. Despite efforts to create open-source models, their limited\nparameters often result in insufficient multi-step reasoning capabilities\nrequired for solving complex medical problems. To address this, we introduce\nMeerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was\ntrained using our new synthetic dataset consisting of high-quality\nchain-of-thought reasoning paths sourced from 18 medical textbooks, along with\ndiverse instruction-following datasets. Our system achieved remarkable accuracy\nacross seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as\noutperforming the previous best 7B models such as MediTron-7B and BioMistral-7B\nby 13.4% and 9.8%, respectively. Notably, it surpassed the passing threshold of\nthe United States Medical Licensing Examination (USMLE) for the first time for\na 7B-parameter model. Additionally, our system offered more detailed free-form\nresponses to clinical queries compared to existing 7B and 13B models,\napproaching the performance level of GPT-3.5. This significantly narrows the\nperformance gap with large LMs, showcasing its effectiveness in addressing\ncomplex medical challenges.",
        "pdf_link": "https://arxiv.org/pdf/2404.00376v1.pdf"
    },
    {
        "title": "Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation",
        "authors": [
            "Zhenhua Liu",
            "Tong Zhu",
            "Jianxiang Xiang",
            "Wenliang Chen"
        ],
        "published": "2024-03-30T13:28:51Z",
        "summary": "Data augmentation (DA) is crucial to mitigate model training instability and\nover-fitting problems in low-resource open-domain dialogue generation. However,\ntraditional DA methods often neglect semantic data diversity, restricting the\noverall quality. Recently, large language models (LLM) have been used for DA to\ngenerate diversified dialogues. However, they have limited controllability and\ntend to generate dialogues with a distribution shift compared to the seed\ndialogues. To maximize the augmentation diversity and address the\ncontrollability problem, we propose \\textbf{S}ummary-based \\textbf{D}ialogue\n\\textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability\nof LLM by using dialogue summaries as a planning tool. Based on summaries, SDA\ncan generate high-quality and diverse dialogue data even with a small seed\ndataset. To evaluate the efficacy of data augmentation methods for open-domain\ndialogue, we designed a clustering-based metric to characterize the semantic\ndiversity of the augmented dialogue data. The experimental results show that\nSDA can augment high-quality and semantically diverse dialogues given a small\nseed dataset and an LLM, and the augmented data can boost the performance of\nopen-domain dialogue models.",
        "pdf_link": "https://arxiv.org/pdf/2404.00361v1.pdf"
    },
    {
        "title": "Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange",
        "authors": [
            "Ankit Satpute",
            "Noah Giessing",
            "Andre Greiner-Petter",
            "Moritz Schubotz",
            "Olaf Teschke",
            "Akiko Aizawa",
            "Bela Gipp"
        ],
        "published": "2024-03-30T12:48:31Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\nvarious natural language tasks, often achieving performances that surpass those\nof humans. Despite these advancements, the domain of mathematics presents a\ndistinctive challenge, primarily due to its specialized structure and the\nprecision it demands. In this study, we adopted a two-step approach for\ninvestigating the proficiency of LLMs in answering mathematical questions.\nFirst, we employ the most effective LLMs, as identified by their performance on\nmath question-answer benchmarks, to generate answers to 78 questions from the\nMath Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that\nshowed the highest performance, focusing on the quality and accuracy of its\nanswers through manual evaluation. We found that GPT-4 performs best (nDCG of\n0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering\nmathematics questions and outperforms the current best approach on ArqMATH3\nTask1, considering P@10. Our Case analysis indicates that while the GPT-4 can\ngenerate relevant responses in certain instances, it does not consistently\nanswer all questions accurately. This paper explores the current limitations of\nLLMs in navigating complex mathematical problem-solving. Through case analysis,\nwe shed light on the gaps in LLM capabilities within mathematics, thereby\nsetting the stage for future research and advancements in AI-driven\nmathematical reasoning. We make our code and findings publicly available for\nresearch: \\url{https://github.com/gipplab/LLM-Investig-MathStackExchange}",
        "pdf_link": "https://arxiv.org/pdf/2404.00344v1.pdf"
    },
    {
        "title": "Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation",
        "authors": [
            "Yuji Naraki",
            "Ryosuke Yamaki",
            "Yoshikazu Ikeda",
            "Takafumi Horie",
            "Hiroki Naganuma"
        ],
        "published": "2024-03-30T12:13:57Z",
        "summary": "In the field of Natural Language Processing (NLP), Named Entity Recognition\n(NER) is recognized as a critical technology, employed across a wide array of\napplications. Traditional methodologies for annotating datasets for NER models\nare challenged by high costs and variations in dataset quality. This research\nintroduces a novel hybrid annotation approach that synergizes human effort with\nthe capabilities of Large Language Models (LLMs). This approach not only aims\nto ameliorate the noise inherent in manual annotations, such as omissions,\nthereby enhancing the performance of NER models, but also achieves this in a\ncost-effective manner. Additionally, by employing a label mixing strategy, it\naddresses the issue of class imbalance encountered in LLM-based annotations.\nThrough an analysis across multiple datasets, this method has been consistently\nshown to provide superior performance compared to traditional annotation\nmethods, even under constrained budget conditions. This study illuminates the\npotential of leveraging LLMs to improve dataset quality, introduces a novel\ntechnique to mitigate class imbalances, and demonstrates the feasibility of\nachieving high-performance NER in a cost-effective way.",
        "pdf_link": "https://arxiv.org/pdf/2404.01334v1.pdf"
    },
    {
        "title": "Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation",
        "authors": [
            "Arjun P S",
            "Andrew Melnik",
            "Gora Chand Nandi"
        ],
        "published": "2024-03-30T10:54:59Z",
        "summary": "Recent advancements in Generative Artificial Intelligence, particularly in\nthe realm of Large Language Models (LLMs) and Large Vision Language Models\n(LVLMs), have enabled the prospect of leveraging cognitive planners within\nrobotic systems. This work focuses on solving the object goal navigation\nproblem by mimicking human cognition to attend, perceive and store task\nspecific information and generate plans with the same. We introduce a\ncomprehensive framework capable of exploring an unfamiliar environment in\nsearch of an object by leveraging the capabilities of Large Language\nModels(LLMs) and Large Vision Language Models (LVLMs) in understanding the\nunderlying semantics of our world. A challenging task in using LLMs to generate\nhigh level sub-goals is to efficiently represent the environment around the\nrobot. We propose to use a 3D scene modular representation, with semantically\nrich descriptions of the object, to provide the LLM with task relevant\ninformation. But providing the LLM with a mass of contextual information (rich\n3D scene semantic representation), can lead to redundant and inefficient plans.\nWe propose to use an LLM based pruner that leverages the capabilities of\nin-context learning to prune out irrelevant goal specific information.",
        "pdf_link": "https://arxiv.org/pdf/2404.00318v1.pdf"
    },
    {
        "title": "ST-LLM: Large Language Models Are Effective Temporal Learners",
        "authors": [
            "Ruyang Liu",
            "Chen Li",
            "Haoran Tang",
            "Yixiao Ge",
            "Ying Shan",
            "Ge Li"
        ],
        "published": "2024-03-30T10:11:26Z",
        "summary": "Large Language Models (LLMs) have showcased impressive capabilities in text\ncomprehension and generation, prompting research efforts towards video LLMs to\nfacilitate human-AI interaction at the video level. However, how to effectively\nencode and understand videos in video-based dialogue systems remains to be\nsolved. In this paper, we investigate a straightforward yet unexplored\nquestion: Can we feed all spatial-temporal tokens into the LLM, thus delegating\nthe task of video sequence modeling to the LLMs? Surprisingly, this simple\napproach yields significant improvements in video understanding. Based upon\nthis, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal\nsequence modeling inside LLM. Furthermore, to address the overhead and\nstability issues introduced by uncompressed video tokens within LLMs, we\ndevelop a dynamic masking strategy with tailor-made training objectives. For\nparticularly long videos, we have also designed a global-local input module to\nbalance efficiency and effectiveness. Consequently, we harness LLM for\nproficient spatial-temporal modeling, while upholding efficiency and stability.\nExtensive experimental results attest to the effectiveness of our method.\nThrough a more concise model and training pipeline, ST-LLM establishes a new\nstate-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been\navailable at https://github.com/TencentARC/ST-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2404.00308v1.pdf"
    },
    {
        "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
        "authors": [
            "Md Saroar Jahan",
            "Mourad Oussalah",
            "Djamila Romaissa Beddia",
            "Jhuma kabir Mim",
            "Nabil Arhab"
        ],
        "published": "2024-03-30T09:55:58Z",
        "summary": "The surge of interest in data augmentation within the realm of NLP has been\ndriven by the need to address challenges posed by hate speech domains, the\ndynamic nature of social media vocabulary, and the demands for large-scale\nneural networks requiring extensive training data. However, the prevalent use\nof lexical substitution in data augmentation has raised concerns, as it may\ninadvertently alter the intended meaning, thereby impacting the efficacy of\nsupervised machine learning models. In pursuit of suitable data augmentation\nmethods, this study explores both established legacy approaches and\ncontemporary practices such as Large Language Models (LLM), including GPT in\nHate Speech detection. Additionally, we propose an optimized utilization of\nBERT-based encoder models with contextual cosine similarity filtration,\nexposing significant limitations in prior synonym substitution methods. Our\ncomparative analysis encompasses five popular augmentation techniques: WordNet\nand Fast-Text synonym replacement, Back-translation, BERT-mask contextual\naugmentation, and LLM. Our analysis across five benchmarked datasets revealed\nthat while traditional methods like back-translation show low label alteration\nrates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence\ndiversity but at the cost of higher label alteration rates (over 6%). Our\nproposed BERT-based contextual cosine similarity filtration markedly reduced\nlabel alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1\nperformance. However, augmenting data with GPT-3 not only avoided overfitting\nwith up to sevenfold data increase but also improved embedding space coverage\nby 15% and classification F1 score by 1.4% over traditional methods, and by\n0.8% over our method.",
        "pdf_link": "https://arxiv.org/pdf/2404.00303v1.pdf"
    },
    {
        "title": "Instruction-Driven Game Engines on Large Language Models",
        "authors": [
            "Hongqiu Wu",
            "Y. Wang",
            "Xingyuan Liu",
            "Hai Zhao",
            "Min Zhang"
        ],
        "published": "2024-03-30T08:02:16Z",
        "summary": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.",
        "pdf_link": "https://arxiv.org/pdf/2404.00276v2.pdf"
    },
    {
        "title": "Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits",
        "authors": [
            "Zhivar Sourati",
            "Meltem Ozcan",
            "Colin McDaniel",
            "Alireza Ziabari",
            "Nuan Wen",
            "Ala Tak",
            "Fred Morstatter",
            "Morteza Dehghani"
        ],
        "published": "2024-03-30T06:49:17Z",
        "summary": "Prior research has established associations between individuals' language\nusage and their personal traits; our linguistic patterns reveal information\nabout our personalities, emotional states, and beliefs. However, with the\nincreasing adoption of Large Language Models (LLMs) as writing assistants in\neveryday writing, a critical question emerges: are authors' linguistic patterns\nstill predictive of their personal traits when LLMs are involved in the writing\nprocess? We investigate the impact of LLMs on the linguistic markers of\ndemographic and psychological traits, specifically examining three LLMs -\nGPT3.5, Llama 2, and Gemini - across six different traits: gender, age,\npolitical affiliation, personality, empathy, and morality. Our findings\nindicate that although the use of LLMs slightly reduces the predictive power of\nlinguistic patterns over authors' personal traits, the significant changes are\ninfrequent, and the use of LLMs does not fully diminish the predictive power of\nauthors' linguistic patterns over their personal traits. We also note that some\ntheoretically established lexical-based linguistic markers lose their\nreliability as predictors when LLMs are used in the writing process. Our\nfindings have important implications for the study of linguistic markers of\npersonal traits in the age of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2404.00267v2.pdf"
    },
    {
        "title": "DeFT: Flash Tree-attention with IO-Awareness for Efficient Tree-search-based LLM Inference",
        "authors": [
            "Jinwei Yao",
            "Kaiqi Chen",
            "Kexun Zhang",
            "Jiaxuan You",
            "Binhang Yuan",
            "Zeke Wang",
            "Tao Lin"
        ],
        "published": "2024-03-30T04:34:54Z",
        "summary": "Decoding using tree search can greatly enhance the inference quality for\ntransformer-based Large Language Models (LLMs). Depending on the guidance\nsignal, it searches for the best path from root to leaf in the tree by forming\nLLM outputs to improve controllability, reasoning ability, alignment, et\ncetera. However, current tree decoding strategies and their inference systems\ndo not suit each other well due to redundancy in computation, memory\nfootprints, and memory access, resulting in inefficient inference. To address\nthis issue, we propose DeFT, an IO-aware tree attention algorithm that\nmaintains memory-efficient attention calculation with low memory footprints in\ntwo stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to\ngroup QKV wisely for high utilization of GPUs and reduction of memory\nreads/writes for the KV cache between GPU global memory and on-chip shared\nmemory as much as possible; (2) Attention Calculation: we calculate partial\nattention of each QKV groups in a fused kernel then apply a Tree-topology-aware\nGlobal Reduction strategy to get final attention. Thanks to a reduction in KV\ncache IO by 3.6-4.5$\\times$, along with an additional reduction in IO for\n$\\mathbf{Q} \\mathbf{K}^\\top$ and Softmax equivalent to 25% of the total KV\ncache IO, DeFT can achieve a speedup of 1.7-2.4$\\times$ in end-to-end latency\nacross two practical reasoning tasks over the SOTA attention algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2404.00242v1.pdf"
    },
    {
        "title": "A Survey of using Large Language Models for Generating Infrastructure as Code",
        "authors": [
            "Kalahasti Ganesh Srivatsa",
            "Sabyasachi Mukhopadhyay",
            "Ganesh Katrapati",
            "Manish Shrivastava"
        ],
        "published": "2024-03-30T02:57:55Z",
        "summary": "Infrastructure as Code (IaC) is a revolutionary approach which has gained\nsignificant prominence in the Industry. IaC manages and provisions IT\ninfrastructure using machine-readable code by enabling automation, consistency\nacross the environments, reproducibility, version control, error reduction and\nenhancement in scalability. However, IaC orchestration is often a painstaking\neffort which requires specialised skills as well as a lot of manual effort.\nAutomation of IaC is a necessity in the present conditions of the Industry and\nin this survey, we study the feasibility of applying Large Language Models\n(LLM) to address this problem. LLMs are large neural network-based models which\nhave demonstrated significant language processing abilities and shown to be\ncapable of following a range of instructions within a broad scope. Recently,\nthey have also been adapted for code understanding and generation tasks\nsuccessfully, which makes them a promising choice for the automatic generation\nof IaC configurations. In this survey, we delve into the details of IaC, usage\nof IaC in different platforms, their challenges, LLMs in terms of\ncode-generation aspects and the importance of LLMs in IaC along with our own\nexperiments. Finally, we conclude by presenting the challenges in this area and\nhighlighting the scope for future research.",
        "pdf_link": "https://arxiv.org/pdf/2404.00227v1.pdf"
    },
    {
        "title": "Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark",
        "authors": [
            "Baolong Bi",
            "Shenghua Liu",
            "Yiwei Wang",
            "Lingrui Mei",
            "Xueqi Cheng"
        ],
        "published": "2024-03-30T02:08:28Z",
        "summary": "The rapid development of large language models (LLMs) enables them to convey\nfactual knowledge in a more human-like fashion. Extensive efforts have been\nmade to reduce factual hallucinations by modifying LLMs with factuality\ndecoding. However, they also pose risks of hindering knowledge updates, as they\nmake models overly confident in known facts. In this work, we first revisite\nthe current factuality decoding methods and verified their effectiveness in\nenhancing factual accuracy. Subsequently, we conduct further evaluation of\nseveral strong factuality decoding methods on the knowledge editing benchmark.\nAll these decoding methods significantly diminish the performance of llama2\nmodels compared to their original decoding, with the largest decrease being a\nstaggering 81.3\\%. This further indicates that the current existing decoding\nmethods still cannot perfectly address the factual hallucinations, as they\noverlook the importance of preserving the flexibility for knowledge editing.\nTherefore, our work suggests that research into factual alignment should\nsimultaneously focus on the effectiveness of knowledge editing.",
        "pdf_link": "https://arxiv.org/pdf/2404.00216v1.pdf"
    },
    {
        "title": "Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning",
        "authors": [
            "Nick Mecklenburg",
            "Yiyou Lin",
            "Xiaoxiao Li",
            "Daniel Holstein",
            "Leonardo Nunes",
            "Sara Malvar",
            "Bruno Silva",
            "Ranveer Chandra",
            "Vijay Aski",
            "Pavan Kumar Reddy Yannam",
            "Tolga Aktas",
            "Todd Hendry"
        ],
        "published": "2024-03-30T01:56:07Z",
        "summary": "In recent years, Large Language Models (LLMs) have shown remarkable\nperformance in generating human-like text, proving to be a valuable asset\nacross various applications. However, adapting these models to incorporate new,\nout-of-domain knowledge remains a challenge, particularly for facts and events\nthat occur after the model's knowledge cutoff date. This paper investigates the\neffectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge\ninjection in LLMs, specifically focusing on the domain of recent sporting\nevents. We compare different dataset generation strategies -- token-based and\nfact-based scaling -- to create training data that helps the model learn new\ninformation. Our experiments on GPT-4 demonstrate that while token-based\nscaling can lead to improvements in Q&A accuracy, it may not provide uniform\ncoverage of new knowledge. Fact-based scaling, on the other hand, offers a more\nsystematic approach to ensure even coverage across all facts. We present a\nnovel dataset generation process that leads to more effective knowledge\ningestion through SFT, and our results show considerable performance\nimprovements in Q&A tasks related to out-of-domain knowledge. This study\ncontributes to the understanding of domain adaptation for LLMs and highlights\nthe potential of SFT in enhancing the factuality of LLM responses in specific\nknowledge domains.",
        "pdf_link": "https://arxiv.org/pdf/2404.00213v2.pdf"
    },
    {
        "title": "Multi-Conditional Ranking with Large Language Models",
        "authors": [
            "Pouya Pezeshkpour",
            "Estevam Hruschka"
        ],
        "published": "2024-03-30T01:26:05Z",
        "summary": "Utilizing large language models (LLMs) to rank a set of items has become a\ncommon approach in recommendation and retrieval systems. Typically, these\nsystems focus on ordering a substantial number of documents in a monotonic\norder based on a given query. However, real-world scenarios often present a\ndifferent challenge: ranking a comparatively smaller set of items, but\naccording to a variety of diverse and occasionally conflicting conditions. In\nthis paper, we define and explore the task of multi-conditional ranking by\nintroducing MCRank, a benchmark tailored for assessing multi-conditional\nranking across various item types and conditions. Our analysis of LLMs using\nMCRank indicates a significant decrease in performance as the number and\ncomplexity of items and conditions grow. To overcome this limitation, we\npropose a novel decomposed reasoning method, consisting of EXtracting and\nSorting the conditions, and then Iterativly Ranking the items (EXSIR). Our\nextensive experiments show that this decomposed reasoning method enhances LLMs'\nperformance significantly, achieving up to a 12% improvement over existing\nLLMs. We also provide a detailed analysis of LLMs performance across various\ncondition categories, and examine the effectiveness of decomposition step.\nFurthermore, we compare our method with existing approaches such as\nChain-of-Thought and an encoder-type ranking model, demonstrating the\nsuperiority of our approach and complexity of MCR task. We released our dataset\nand code.",
        "pdf_link": "https://arxiv.org/pdf/2404.00211v1.pdf"
    },
    {
        "title": "EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs",
        "authors": [
            "Cheng Jiayang",
            "Lin Qiu",
            "Chunkit Chan",
            "Xin Liu",
            "Yangqiu Song",
            "Zheng Zhang"
        ],
        "published": "2024-03-30T01:16:37Z",
        "summary": "Narrative reasoning relies on the understanding of eventualities in story\ncontexts, which requires a wealth of background world knowledge. To help\nmachines leverage such knowledge, existing solutions can be categorized into\ntwo groups. Some focus on implicitly modeling eventuality knowledge by\npretraining language models (LMs) with eventuality-aware objectives. However,\nthis approach breaks down knowledge structures and lacks interpretability.\nOthers explicitly collect world knowledge of eventualities into structured\neventuality-centric knowledge graphs (KGs). However, existing research on\nleveraging these knowledge sources for free-texts is limited. In this work, we\npropose an initial comprehensive framework called EventGround, which aims to\ntackle the problem of grounding free-texts to eventuality-centric KGs for\ncontextualized narrative reasoning. We identify two critical problems in this\ndirection: the event representation and sparsity problems. We provide simple\nyet effective parsing and partial information extraction methods to tackle\nthese problems. Experimental results demonstrate that our approach consistently\noutperforms baseline models when combined with graph neural network (GNN) or\nlarge language model (LLM) based graph reasoning models. Our framework,\nincorporating grounded knowledge, achieves state-of-the-art performance while\nproviding interpretable evidence.",
        "pdf_link": "https://arxiv.org/pdf/2404.00209v1.pdf"
    },
    {
        "title": "Conceptual and Unbiased Reasoning in Language Models",
        "authors": [
            "Ben Zhou",
            "Hongming Zhang",
            "Sihao Chen",
            "Dian Yu",
            "Hongwei Wang",
            "Baolin Peng",
            "Dan Roth",
            "Dong Yu"
        ],
        "published": "2024-03-30T00:53:53Z",
        "summary": "Conceptual reasoning, the ability to reason in abstract and high-level\nperspectives, is key to generalization in human cognition. However, limited\nstudy has been done on large language models' capability to perform conceptual\nreasoning. In this work, we bridge this gap and propose a novel\nconceptualization framework that forces models to perform conceptual reasoning\non abstract questions and generate solutions in a verifiable symbolic space.\nUsing this framework as an analytical tool, we show that existing large\nlanguage models fall short on conceptual reasoning, dropping 9% to 28% on\nvarious benchmarks compared to direct inference methods. We then discuss how\nmodels can improve since high-level abstract reasoning is key to unbiased and\ngeneralizable decision-making. We propose two techniques to add trustworthy\ninduction signals by generating familiar questions with similar underlying\nreasoning paths and asking models to perform self-refinement. Experiments show\nthat our proposed techniques improve models' conceptual reasoning performance\nby 8% to 11%, achieving a more robust reasoning system that relies less on\ninductive biases.",
        "pdf_link": "https://arxiv.org/pdf/2404.00205v1.pdf"
    },
    {
        "title": "GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs",
        "authors": [
            "Xiao Liu",
            "Jiawei Zhang"
        ],
        "published": "2024-03-29T23:04:04Z",
        "summary": "This study introduces GPTA, a Large Language Model assistance training\nframework, that enhances the training of downstream task models via prefix\nprompt. By minimizing data exposure to LLM, the framework addresses the\nsecurity and legal challenges of applying LLM in downstream task model\ntraining. GPTA utilizes a new synergistic training approach, optimizing the\ndownstream models with parameter gradients and LLMs with the novel ``dialogue\ngradient''. The framework not only demonstrates significant improvements in\nmodel performance across six NLP benchmark datasets, but also reduces\noverfitting in low-resource scenarios effectively. The detailed analyses\nfurther validate that our pioneer framework provides a cost-efficient and\nadaptive method for downstream task model training with LLM support.",
        "pdf_link": "https://arxiv.org/pdf/2404.00189v1.pdf"
    },
    {
        "title": "Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value",
        "authors": [
            "Behnam Mohammadi"
        ],
        "published": "2024-03-29T22:49:43Z",
        "summary": "The emergence of large language models (LLMs) has opened up exciting\npossibilities for simulating human behavior and cognitive processes, with\npotential applications in various domains, including marketing research and\nconsumer behavior analysis. However, the validity of utilizing LLMs as\nstand-ins for human subjects remains uncertain due to glaring divergences that\nsuggest fundamentally different underlying processes at play and the\nsensitivity of LLM responses to prompt variations. This paper presents a novel\napproach based on Shapley values from cooperative game theory to interpret LLM\nbehavior and quantify the relative contribution of each prompt component to the\nmodel's output. Through two applications-a discrete choice experiment and an\ninvestigation of cognitive biases-we demonstrate how the Shapley value method\ncan uncover what we term \"token noise\" effects, a phenomenon where LLM\ndecisions are disproportionately influenced by tokens providing minimal\ninformative content. This phenomenon raises concerns about the robustness and\ngeneralizability of insights obtained from LLMs in the context of human\nbehavior simulation. Our model-agnostic approach extends its utility to\nproprietary LLMs, providing a valuable tool for marketers and researchers to\nstrategically optimize prompts and mitigate apparent cognitive biases. Our\nfindings underscore the need for a more nuanced understanding of the factors\ndriving LLM responses before relying on them as substitutes for human subjects\nin research settings. We emphasize the importance of researchers reporting\nresults conditioned on specific prompt templates and exercising caution when\ndrawing parallels between human behavior and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2404.01332v1.pdf"
    },
    {
        "title": "Uncovering Bias in Large Vision-Language Models with Counterfactuals",
        "authors": [
            "Phillip Howard",
            "Anahita Bhiwandiwalla",
            "Kathleen C. Fraser",
            "Svetlana Kiritchenko"
        ],
        "published": "2024-03-29T21:45:53Z",
        "summary": "With the advent of Large Language Models (LLMs) possessing increasingly\nimpressive capabilities, a number of Large Vision-Language Models (LVLMs) have\nbeen proposed to augment LLMs with visual inputs. Such models condition\ngenerated text on both an input image and a text prompt, enabling a variety of\nuse cases such as visual question answering and multimodal chat. While prior\nstudies have examined the social biases contained in text generated by LLMs,\nthis topic has been relatively unexplored in LVLMs. Examining social biases in\nLVLMs is particularly challenging due to the confounding contributions of bias\ninduced by information contained across the text and visual modalities. To\naddress this challenging problem, we conduct a large-scale study of text\ngenerated by different LVLMs under counterfactual changes to input images.\nSpecifically, we present LVLMs with identical open-ended text prompts while\nconditioning on images from different counterfactual sets, where each set\ncontains images which are largely identical in their depiction of a common\nsubject (e.g., a doctor), but vary only in terms of intersectional social\nattributes (e.g., race and gender). We comprehensively evaluate the text\nproduced by different LVLMs under this counterfactual generation setting and\nfind that social attributes such as race, gender, and physical characteristics\ndepicted in input images can significantly influence toxicity and the\ngeneration of competency-associated words.",
        "pdf_link": "https://arxiv.org/pdf/2404.00166v1.pdf"
    },
    {
        "title": "On-the-fly Definition Augmentation of LLMs for Biomedical NER",
        "authors": [
            "Monica Munnangi",
            "Sergey Feldman",
            "Byron C Wallace",
            "Silvio Amir",
            "Tom Hope",
            "Aakanksha Naik"
        ],
        "published": "2024-03-29T20:59:27Z",
        "summary": "Despite their general capabilities, LLMs still struggle on biomedical NER\ntasks, which are difficult due to the presence of specialized terminology and\nlack of training data. In this work we set out to improve LLM performance on\nbiomedical NER in limited data settings via a new knowledge augmentation\napproach which incorporates definitions of relevant concepts on-the-fly. During\nthis process, to provide a test bed for knowledge augmentation, we perform a\ncomprehensive exploration of prompting strategies. Our experiments show that\ndefinition augmentation is useful for both open source and closed LLMs. For\nexample, it leads to a relative improvement of 15\\% (on average) in GPT-4\nperformance (F1) across all (six) of our test datasets. We conduct extensive\nablations and analyses to demonstrate that our performance improvements stem\nfrom adding relevant definitional knowledge. We find that careful prompting\nstrategies also improve LLM performance, allowing them to outperform fine-tuned\nlanguage models in few-shot settings. To facilitate future research in this\ndirection, we release our code at https://github.com/allenai/beacon.",
        "pdf_link": "https://arxiv.org/pdf/2404.00152v1.pdf"
    },
    {
        "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models",
        "authors": [
            "Atsuyuki Miyai",
            "Jingkang Yang",
            "Jingyang Zhang",
            "Yifei Ming",
            "Qing Yu",
            "Go Irie",
            "Yixuan Li",
            "Hai Li",
            "Ziwei Liu",
            "Kiyoharu Aizawa"
        ],
        "published": "2024-03-29T17:59:53Z",
        "summary": "This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.20331v1.pdf"
    },
    {
        "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
        "authors": [
            "Lin Chen",
            "Jinsong Li",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Yuhang Zang",
            "Zehui Chen",
            "Haodong Duan",
            "Jiaqi Wang",
            "Yu Qiao",
            "Dahua Lin",
            "Feng Zhao"
        ],
        "published": "2024-03-29T17:59:34Z",
        "summary": "Large vision-language models (LVLMs) have recently achieved rapid progress,\nsparking numerous studies to evaluate their multi-modal capabilities. However,\nwe dig into current evaluation works and identify two primary issues: 1) Visual\ncontent is unnecessary for many samples. The answers can be directly inferred\nfrom the questions and options, or the world knowledge embedded in LLMs. This\nphenomenon is prevalent across current benchmarks. For instance, GeminiPro\nachieves 42.9% on the MMMU benchmark without any visual input, and outperforms\nthe random choice baseline across six benchmarks over 24% on average. 2)\nUnintentional data leakage exists in LLM and LVLM training. LLM and LVLM could\nstill answer some visual-necessary questions without visual content, indicating\nthe memorizing of these samples within large-scale training data. For example,\nSphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM\nbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modal\ngains and potentially misguide the study of LVLM. To this end, we present\nMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500\nsamples meticulously selected by humans. MMStar benchmarks 6 core capabilities\nand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with\ncarefully balanced and purified samples. These samples are first roughly\nselected from current benchmarks with an automated pipeline, human review is\nthen involved to ensure each curated sample exhibits visual dependency, minimal\ndata leakage, and requires advanced multi-modal capabilities. Moreover, two\nmetrics are developed to measure data leakage and actual performance gain in\nmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess their\nmulti-modal capabilities, and on 7 benchmarks with the proposed metrics to\ninvestigate their data leakage and actual multi-modal gain.",
        "pdf_link": "https://arxiv.org/pdf/2403.20330v2.pdf"
    },
    {
        "title": "ReALM: Reference Resolution As Language Modeling",
        "authors": [
            "Joel Ruben Antony Moniz",
            "Soundarya Krishnan",
            "Melis Ozyildirim",
            "Prathamesh Saraf",
            "Halim Cagri Ates",
            "Yuan Zhang",
            "Hong Yu",
            "Nidhi Rajshree"
        ],
        "published": "2024-03-29T17:59:06Z",
        "summary": "Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.",
        "pdf_link": "https://arxiv.org/pdf/2403.20329v1.pdf"
    },
    {
        "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models",
        "authors": [
            "Jinhyuk Lee",
            "Zhuyun Dai",
            "Xiaoqi Ren",
            "Blair Chen",
            "Daniel Cer",
            "Jeremy R. Cole",
            "Kai Hui",
            "Michael Boratko",
            "Rajvi Kapadia",
            "Wen Ding",
            "Yi Luan",
            "Sai Meher Karthik Duddu",
            "Gustavo Hernandez Abrego",
            "Weiqiang Shi",
            "Nithi Gupta",
            "Aditya Kusupati",
            "Prateek Jain",
            "Siddhartha Reddy Jonnalagadda",
            "Ming-Wei Chang",
            "Iftekhar Naim"
        ],
        "published": "2024-03-29T17:56:40Z",
        "summary": "We present Gecko, a compact and versatile text embedding model. Gecko\nachieves strong retrieval performance by leveraging a key idea: distilling\nknowledge from large language models (LLMs) into a retriever. Our two-step\ndistillation process begins with generating diverse, synthetic paired data\nusing an LLM. Next, we further refine the data quality by retrieving a set of\ncandidate passages for each query, and relabeling the positive and hard\nnegative passages using the same LLM. The effectiveness of our approach is\ndemonstrated by the compactness of the Gecko. On the Massive Text Embedding\nBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing\nentries with 768 embedding size. Gecko with 768 embedding dimensions achieves\nan average score of 66.31, competing with 7x larger models and 5x higher\ndimensional embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2403.20327v1.pdf"
    },
    {
        "title": "Convolutional Prompting meets Language Models for Continual Learning",
        "authors": [
            "Anurag Roy",
            "Riddhiman Moulick",
            "Vinay K. Verma",
            "Saptarshi Ghosh",
            "Abir Das"
        ],
        "published": "2024-03-29T17:40:37Z",
        "summary": "Continual Learning (CL) enables machine learning models to learn from\ncontinuously shifting new training data in absence of data from old tasks.\nRecently, pretrained vision transformers combined with prompt tuning have shown\npromise for overcoming catastrophic forgetting in CL. These approaches rely on\na pool of learnable prompts which can be inefficient in sharing knowledge\nacross tasks leading to inferior performance. In addition, the lack of\nfine-grained layer specific prompts does not allow these to fully express the\nstrength of the prompts for CL. We address these limitations by proposing\nConvPrompt, a novel convolutional prompt creation mechanism that maintains\nlayer-wise shared embeddings, enabling both layer-specific learning and better\nconcept transfer across tasks. The intelligent use of convolution enables us to\nmaintain a low parameter overhead without compromising performance. We further\nleverage Large Language Models to generate fine-grained text descriptions of\neach category which are used to get task similarity and dynamically decide the\nnumber of prompts to be learned. Extensive experiments demonstrate the\nsuperiority of ConvPrompt and improves SOTA by ~3% with significantly less\nparameter overhead. We also perform strong ablation over various modules to\ndisentangle the importance of different components.",
        "pdf_link": "https://arxiv.org/pdf/2403.20317v1.pdf"
    },
    {
        "title": "Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference",
        "authors": [
            "Jovan Stojkovic",
            "Esha Choukse",
            "Chaojie Zhang",
            "Inigo Goiri",
            "Josep Torrellas"
        ],
        "published": "2024-03-29T17:22:48Z",
        "summary": "With the ubiquitous use of modern large language models (LLMs) across\nindustries, the inference serving for these models is ever expanding. Given the\nhigh compute and memory requirements of modern LLMs, more and more\ntop-of-the-line GPUs are being deployed to serve these models. Energy\navailability has come to the forefront as the biggest challenge for data center\nexpansion to serve these models. In this paper, we present the trade-offs\nbrought up by making energy efficiency the primary goal of LLM serving under\nperformance SLOs. We show that depending on the inputs, the model, and the\nservice-level agreements, there are several knobs available to the LLM\ninference provider to use for being energy efficient. We characterize the\nimpact of these knobs on the latency, throughput, as well as the energy. By\nexploring these trade-offs, we offer valuable insights into optimizing energy\nusage without compromising on performance, thereby paving the way for\nsustainable and cost-effective LLM deployment in data center environments.",
        "pdf_link": "https://arxiv.org/pdf/2403.20306v1.pdf"
    },
    {
        "title": "Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain",
        "authors": [
            "Burcu Sayin",
            "Pasquale Minervini",
            "Jacopo Staiano",
            "Andrea Passerini"
        ],
        "published": "2024-03-29T16:59:13Z",
        "summary": "We explore the potential of Large Language Models (LLMs) to assist and\npotentially correct physicians in medical decision-making tasks. We evaluate\nseveral LLMs, including Meditron, Llama2, and Mistral, to analyze the ability\nof these models to interact effectively with physicians across different\nscenarios. We consider questions from PubMedQA and several tasks, ranging from\nbinary (yes/no) responses to long answer generation, where the answer of the\nmodel is produced after an interaction with a physician. Our findings suggest\nthat prompt design significantly influences the downstream accuracy of LLMs and\nthat LLMs can provide valuable feedback to physicians, challenging incorrect\ndiagnoses and contributing to more accurate decision-making. For example, when\nthe physician is accurate 38% of the time, Mistral can produce the correct\nanswer, improving accuracy up to 74% depending on the prompt being used, while\nLlama2 and Meditron models exhibit greater sensitivity to prompt choice. Our\nanalysis also uncovers the challenges of ensuring that LLM-generated\nsuggestions are pertinent and useful, emphasizing the need for further research\nin this area.",
        "pdf_link": "https://arxiv.org/pdf/2403.20288v1.pdf"
    },
    {
        "title": "LayerNorm: A key component in parameter-efficient fine-tuning",
        "authors": [
            "Taha ValizadehAslani",
            "Hualou Liang"
        ],
        "published": "2024-03-29T16:53:11Z",
        "summary": "Fine-tuning a pre-trained model, such as Bidirectional Encoder\nRepresentations from Transformers (BERT), has been proven to be an effective\nmethod for solving many natural language processing (NLP) tasks. However, due\nto the large number of parameters in many state-of-the-art NLP models,\nincluding BERT, the process of fine-tuning is computationally expensive. One\nattractive solution to this issue is parameter-efficient fine-tuning, which\ninvolves modifying only a minimal segment of the model while keeping the\nremainder unchanged. Yet, it remains unclear which segment of the BERT model is\ncrucial for fine-tuning. In this paper, we first analyze different components\nin the BERT model to pinpoint which one undergoes the most significant changes\nafter fine-tuning. We find that output LayerNorm changes more than any other\ncomponents when fine-tuned for different General Language Understanding\nEvaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can\nreach comparable, or in some cases better, performance to full fine-tuning and\nother parameter-efficient fine-tuning methods. Moreover, we use Fisher\ninformation to determine the most critical subset of LayerNorm and demonstrate\nthat many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a\nsmall portion of LayerNorm with negligible performance degradation.",
        "pdf_link": "https://arxiv.org/pdf/2403.20284v1.pdf"
    },
    {
        "title": "LUQ: Long-text Uncertainty Quantification for LLMs",
        "authors": [
            "Caiqi Zhang",
            "Fangyu Liu",
            "Marco Basaldella",
            "Nigel Collier"
        ],
        "published": "2024-03-29T16:49:24Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capability in a\nvariety of NLP tasks. Despite their effectiveness, these models are prone to\ngenerate nonfactual content. Uncertainty Quantification (UQ) is pivotal in\nenhancing our understanding of a model's confidence in its generated content,\nthereby aiding in the mitigation of nonfactual outputs. Existing research on UQ\npredominantly targets short text generation, typically yielding brief,\nword-limited responses. However, real-world applications frequently necessitate\nmuch longer responses. Our study first highlights the limitations of current UQ\nmethods in handling long text generation. We then introduce \\textsc{Luq}, a\nnovel sampling-based UQ approach specifically designed for long text. Our\nfindings reveal that \\textsc{Luq} outperforms existing baseline methods in\ncorrelating with the model's factuality scores (negative coefficient of -0.85\nobserved for Gemini Pro). With \\textsc{Luq} as the tool for UQ, we investigate\nbehavior patterns of several popular LLMs' response confidence spectrum and how\nthat interplays with the response' factuality. We identify that LLMs lack\nconfidence in generating long text for rare facts and a factually strong model\n(i.e. GPT-4) tends to reject questions it is not sure about. To further improve\nthe factual accuracy of LLM responses, we propose a method called\n\\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects\nthe response with the least uncertainty. The ensembling method greatly improves\nthe response factuality upon the best standalone LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.20279v1.pdf"
    },
    {
        "title": "ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models",
        "authors": [
            "Thibaut Thonet",
            "Jos Rozen",
            "Laurent Besacier"
        ],
        "published": "2024-03-29T16:13:31Z",
        "summary": "Research on Large Language Models (LLMs) has recently witnessed an increasing\ninterest in extending models' context size to better capture dependencies\nwithin long documents. While benchmarks have been proposed to assess long-range\nabilities, existing efforts primarily considered generic tasks that are not\nnecessarily aligned with real-world applications. In contrast, our work\nproposes a new benchmark for long-context LLMs focused on a practical meeting\nassistant scenario. In this scenario, the long contexts consist of transcripts\nobtained by automatic speech recognition, presenting unique challenges for LLMs\ndue to the inherent noisiness and oral nature of such data. Our benchmark,\nnamed ELITR-Bench, augments the existing ELITR corpus' transcripts with 271\nmanually crafted questions and their ground-truth answers. Our experiments with\nrecent long-context LLMs on ELITR-Bench highlight a gap between open-source and\nproprietary models, especially when questions are asked sequentially within a\nconversation. We also provide a thorough analysis of our GPT-4-based evaluation\nmethod, encompassing insights from a crowdsourcing study. Our findings suggest\nthat while GPT-4's evaluation scores are correlated with human judges', its\nability to differentiate among more than three score levels may be limited.",
        "pdf_link": "https://arxiv.org/pdf/2403.20262v1.pdf"
    },
    {
        "title": "Using LLMs to Model the Beliefs and Preferences of Targeted Populations",
        "authors": [
            "Keiichi Namikoshi",
            "Alex Filipowicz",
            "David A. Shamma",
            "Rumen Iliev",
            "Candice L. Hogan",
            "Nikos Arechiga"
        ],
        "published": "2024-03-29T15:58:46Z",
        "summary": "We consider the problem of aligning a large language model (LLM) to model the\npreferences of a human population. Modeling the beliefs, preferences, and\nbehaviors of a specific population can be useful for a variety of different\napplications, such as conducting simulated focus groups for new products,\nconducting virtual surveys, and testing behavioral interventions, especially\nfor interventions that are expensive, impractical, or unethical. Existing work\nhas had mixed success using LLMs to accurately model human behavior in\ndifferent contexts. We benchmark and evaluate two well-known fine-tuning\napproaches and evaluate the resulting populations on their ability to match the\npreferences of real human respondents on a survey of preferences for battery\nelectric vehicles (BEVs). We evaluate our models against their ability to match\npopulation-wide statistics as well as their ability to match individual\nresponses, and we investigate the role of temperature in controlling the\ntrade-offs between these two. Additionally, we propose and evaluate a novel\nloss term to improve model performance on responses that require a numeric\nresponse.",
        "pdf_link": "https://arxiv.org/pdf/2403.20252v1.pdf"
    },
    {
        "title": "Shallow Cross-Encoders for Low-Latency Retrieval",
        "authors": [
            "Aleksandr V. Petrov",
            "Sean MacAvaney",
            "Craig Macdonald"
        ],
        "published": "2024-03-29T15:07:21Z",
        "summary": "Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in\ntext retrieval. However, Cross-Encoders based on large transformer models (such\nas BERT or T5) are computationally expensive and allow for scoring only a small\nnumber of documents within a reasonably small latency window. However, keeping\nsearch latencies low is important for user satisfaction and energy usage. In\nthis paper, we show that weaker shallow transformer models (i.e., transformers\nwith a limited number of layers) actually perform better than full-scale models\nwhen constrained to these practical low-latency settings since they can\nestimate the relevance of more documents in the same time budget. We further\nshow that shallow transformers may benefit from the generalized Binary\nCross-Entropy (gBCE) training scheme, which has recently demonstrated success\nfor recommendation tasks. Our experiments with TREC Deep Learning passage\nranking query sets demonstrate significant improvements in shallow and\nfull-scale models in low-latency scenarios. For example, when the latency limit\nis 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT\nmodel) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while\nTinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches\nNDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow\nCross-Encoders are effective even when used without a GPU (e.g., with CPU\ninference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms\nlatency), which makes Cross-Encoders practical to run even without specialized\nhardware acceleration.",
        "pdf_link": "https://arxiv.org/pdf/2403.20222v1.pdf"
    },
    {
        "title": "Distributed agency in second language learning and teaching through generative AI",
        "authors": [
            "Robert Godwin-Jones"
        ],
        "published": "2024-03-29T14:55:40Z",
        "summary": "Generative AI offers significant opportunities for language learning. Tools\nlike ChatGPT can provide informal second language practice through chats in\nwritten or voice forms, with the learner specifying through prompts\nconversational parameters such as proficiency level, language register, and\ndiscussion topics. AI can be instructed to give corrective feedback, create\npractice exercises, or develop an extended study plan. Instructors can use AI\nto build learning and assessment materials in a variety of media. AI is likely\nto make immersive technologies more powerful and versatile, moving away from\nscripted interactions. For both learners and teachers, it is important to\nunderstand the limitations of AI systems that arise from their purely\nstatistical model of human language, which limits their ability to deal with\nnuanced social and cultural aspects of language use. Additionally, there are\nethical concerns over how AI systems are created as well as practical\nconstraints in their use, especially for less privileged populations. The power\nand versatility of AI tools are likely to turn them into valuable and constant\ncompanions in many peoples lives (akin to smartphones), creating a close\nconnection that goes beyond simple tool use. Ecological theories such as\nsociomaterialism are helpful in examining the shared agency that develops\nthrough close user-AI interactions, as are the perspectives on human-object\nrelations from Indigenous cultures.",
        "pdf_link": "https://arxiv.org/pdf/2403.20216v1.pdf"
    },
    {
        "title": "H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model",
        "authors": [
            "Chao Pang",
            "Jiang Wu",
            "Jiayu Li",
            "Yi Liu",
            "Jiaxing Sun",
            "Weijia Li",
            "Xingxing Weng",
            "Shuai Wang",
            "Litong Feng",
            "Gui-Song Xia",
            "Conghui He"
        ],
        "published": "2024-03-29T14:50:43Z",
        "summary": "The generic large Vision-Language Models (VLMs) is rapidly developing, but\nstill perform poorly in Remote Sensing (RS) domain, which is due to the unique\nand specialized nature of RS imagery and the comparatively limited spatial\nperception of current VLMs. Existing Remote Sensing specific Vision Language\nModels (RSVLMs) still have considerable potential for improvement, primarily\nowing to the lack of large-scale, high-quality RS vision-language datasets. We\nconstructed HqDC-1.4M, the large scale High quality and Detailed Captions for\nRS images, containing 1.4 million image-caption pairs, which not only enhance\nthe RSVLM's understanding of RS images but also significantly improve the\nmodel's spatial perception abilities, such as localization and counting,\nthereby increasing the helpfulness of the RSVLM. Moreover, to address the\ninevitable \"hallucination\" problem in RSVLM, we developed RSSA, the first\ndataset aimed at enhancing the Self-Awareness capability of RSVLMs. By\nincorporating a variety of unanswerable questions into typical RS visual\nquestion-answering tasks, RSSA effectively improves the truthfulness and\nreduces the hallucinations of the model's outputs, thereby enhancing the\nhonesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, the\nHelpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achieved\noutstanding performance on multiple RS public datasets and is capable of\nrecognizing and refusing to answer the unanswerable questions, effectively\nmitigating the incorrect generations. We will release the code, data and model\nweights at https://github.com/opendatalab/H2RSVLM .",
        "pdf_link": "https://arxiv.org/pdf/2403.20213v1.pdf"
    },
    {
        "title": "Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science",
        "authors": [
            "Yazheng Yang",
            "Yuqi Wang",
            "Sankalok Sen",
            "Lei Li",
            "Qi Liu"
        ],
        "published": "2024-03-29T14:41:21Z",
        "summary": "In the domain of data science, the predictive tasks of classification,\nregression, and imputation of missing values are commonly encountered\nchallenges associated with tabular data. This research endeavors to apply Large\nLanguage Models (LLMs) towards addressing these predictive tasks. Despite their\nproficiency in comprehending natural language, LLMs fall short in dealing with\nstructured tabular data. This limitation stems from their lacking exposure to\nthe intricacies of tabular data during their foundational training. Our\nresearch aims to mitigate this gap by compiling a comprehensive corpus of\ntables annotated with instructions and executing large-scale training of\nLlama-2 on this enriched dataset. Furthermore, we investigate the practical\napplication of applying the trained model to zero-shot prediction, few-shot\nprediction, and in-context learning scenarios. Through extensive experiments,\nour methodology has shown significant improvements over existing benchmarks.\nThese advancements highlight the efficacy of tailoring LLM training to solve\ntable-related problems in data science, thereby establishing a new benchmark in\nthe utilization of LLMs for enhancing tabular intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.20208v4.pdf"
    },
    {
        "title": "The Future of Combating Rumors? Retrieval, Discrimination, and Generation",
        "authors": [
            "Junhao Xu",
            "Longdi Xian",
            "Zening Liu",
            "Mingliang Chen",
            "Qiuyang Yin",
            "Fenghua Song"
        ],
        "published": "2024-03-29T14:32:41Z",
        "summary": "Artificial Intelligence Generated Content (AIGC) technology development has\nfacilitated the creation of rumors with misinformation, impacting societal,\neconomic, and political ecosystems, challenging democracy. Current rumor\ndetection efforts fall short by merely labeling potentially misinformation\n(classification task), inadequately addressing the issue, and it is unrealistic\nto have authoritative institutions debunk every piece of information on social\nmedia. Our proposed comprehensive debunking process not only detects rumors but\nalso provides explanatory generated content to refute the authenticity of the\ninformation. The Expert-Citizen Collective Wisdom (ECCW) module we designed\naensures high-precision assessment of the credibility of information and the\nretrieval module is responsible for retrieving relevant knowledge from a\nReal-time updated debunking database based on information keywords. By using\nprompt engineering techniques, we feed results and knowledge into a LLM (Large\nLanguage Model), achieving satisfactory discrimination and explanatory effects\nwhile eliminating the need for fine-tuning, saving computational costs, and\ncontributing to debunking efforts.",
        "pdf_link": "https://arxiv.org/pdf/2403.20204v1.pdf"
    },
    {
        "title": "Measuring Taiwanese Mandarin Language Understanding",
        "authors": [
            "Po-Heng Chen",
            "Sijia Cheng",
            "Wei-Lin Chen",
            "Yen-Ting Lin",
            "Yun-Nung Chen"
        ],
        "published": "2024-03-29T13:56:21Z",
        "summary": "The evaluation of large language models (LLMs) has drawn substantial\nattention in the field recently. This work focuses on evaluating LLMs in a\nChinese context, specifically, for Traditional Chinese which has been largely\nunderrepresented in existing benchmarks. We present TMLU, a holistic evaluation\nsuit tailored for assessing the advanced knowledge and reasoning capability in\nLLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37\nsubjects across social science, STEM, humanities, Taiwan-specific content, and\nothers, ranging from middle school to professional levels. In addition, we\ncurate chain-of-thought-like few-shot explanations for each subject to\nfacilitate the evaluation of complex reasoning skills. To establish a\ncomprehensive baseline, we conduct extensive experiments and analysis on 24\nadvanced LLMs. The results suggest that Chinese open-weight models demonstrate\ninferior performance comparing to multilingual proprietary ones, and\nopen-weight models tailored for Taiwanese Mandarin lag behind the\nSimplified-Chinese counterparts. The findings indicate great headrooms for\nimprovement, and emphasize the goal of TMLU to foster the development of\nlocalized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation\nscripts for the community to promote future research.",
        "pdf_link": "https://arxiv.org/pdf/2403.20180v1.pdf"
    },
    {
        "title": "ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models",
        "authors": [
            "Zehao Wen",
            "Rabih Younes"
        ],
        "published": "2024-03-29T13:12:09Z",
        "summary": "In our rapidly evolving digital sphere, the ability to discern media bias\nbecomes crucial as it can shape public sentiment and influence pivotal\ndecisions. The advent of large language models (LLMs), such as ChatGPT, noted\nfor their broad utility in various natural language processing (NLP) tasks,\ninvites exploration of their efficacy in media bias detection. Can ChatGPT\ndetect media bias? This study seeks to answer this question by leveraging the\nMedia Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in\ndistinguishing six categories of media bias, juxtaposed against fine-tuned\nmodels such as BART, ConvBERT, and GPT-2. The findings present a dichotomy:\nChatGPT performs at par with fine-tuned models in detecting hate speech and\ntext-level context bias, yet faces difficulties with subtler elements of other\nbias detections, namely, fake news, racial, gender, and cognitive biases.",
        "pdf_link": "https://arxiv.org/pdf/2403.20158v1.pdf"
    },
    {
        "title": "IndiBias: A Benchmark Dataset to Measure Social Biases in Language Models for Indian Context",
        "authors": [
            "Nihar Ranjan Sahoo",
            "Pranamya Prashant Kulkarni",
            "Narjis Asad",
            "Arif Ahmad",
            "Tanu Goyal",
            "Aparna Garimella",
            "Pushpak Bhattacharyya"
        ],
        "published": "2024-03-29T12:32:06Z",
        "summary": "The pervasive influence of social biases in language data has sparked the\nneed for benchmark datasets that capture and evaluate these biases in Large\nLanguage Models (LLMs). Existing efforts predominantly focus on English\nlanguage and the Western context, leaving a void for a reliable dataset that\nencapsulates India's unique socio-cultural nuances. To bridge this gap, we\nintroduce IndiBias, a comprehensive benchmarking dataset designed specifically\nfor evaluating social biases in the Indian context. We filter and translate the\nexisting CrowS-Pairs dataset to create a benchmark dataset suited to the Indian\ncontext in Hindi language. Additionally, we leverage LLMs including ChatGPT and\nInstructGPT to augment our dataset with diverse societal biases and stereotypes\nprevalent in India. The included bias dimensions encompass gender, religion,\ncaste, age, region, physical appearance, and occupation. We also build a\nresource to address intersectional biases along three intersectional\ndimensions. Our dataset contains 800 sentence pairs and 300 tuples for bias\nmeasurement across different demographics. The dataset is available in English\nand Hindi, providing a size comparable to existing benchmark datasets.\nFurthermore, using IndiBias we compare ten different language models on\nmultiple bias measurement metrics. We observed that the language models exhibit\nmore bias across a majority of the intersectional groups.",
        "pdf_link": "https://arxiv.org/pdf/2403.20147v2.pdf"
    },
    {
        "title": "Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries",
        "authors": [
            "Manjeet Yadav",
            "Nilesh Kumar Sahu",
            "Mudita Chaturvedi",
            "Snehil Gupta",
            "Haroon R Lone"
        ],
        "published": "2024-03-29T12:25:37Z",
        "summary": "Improving mental health support in developing countries is a pressing need.\nOne potential solution is the development of scalable, automated systems to\nconduct diagnostic screenings, which could help alleviate the burden on mental\nhealth professionals. In this work, we evaluate several state-of-the-art Large\nLanguage Models (LLMs), with and without fine-tuning, on our custom dataset for\ngenerating concise summaries from mental state examinations. We rigorously\nevaluate four different models for summary generation using established ROUGE\nmetrics and input from human evaluators. The results highlight that our\ntop-performing fine-tuned model outperforms existing models, achieving ROUGE-1\nand ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed\nthe fine-tuned model's generalizability on a publicly available D4 dataset, and\nthe outcomes were promising, indicating its potential applicability beyond our\ncustom dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.20145v2.pdf"
    },
    {
        "title": "Accurate Block Quantization in LLMs with Outliers",
        "authors": [
            "Nikita Trukhanov",
            "Ilya Soloveychik"
        ],
        "published": "2024-03-29T12:15:06Z",
        "summary": "The demand for inference on extremely large scale LLMs has seen enormous\ngrowth in the recent months. It made evident the colossal shortage of dedicated\nhardware capable of efficient and fast processing of the involved compute and\nmemory movement. The problem is aggravated by the exploding raise in the\nlengths of the sequences being processed, since those require efficient on-chip\nstorage of the KV-cache of size proportional to the sequence length. To make\nthe required compute feasible and fit the involved data into available memory,\nnumerous quantization techniques have been proposed that allow accurate\nquantization for both weights and activations. One of the main recent\nbreakthroughs in this direction was introduction of the family of Block\nFloating Point (BFP) formats characterized by a block of mantissas with a\nshared scale factor. These enable memory- power-, and compute- efficient\nhardware support of the tensor operations and provide extremely good\nquantization accuracy. The main issues preventing widespread application of\nblock formats is caused by the presence of outliers in weights and activations\nsince those affect the accuracy of the other values in the same block. In this\npaper, we focus on the most critical problem of limited KV-cache storage. We\npropose a novel approach enabling usage of low precision BFP formats without\ncompromising the resulting model accuracy. We exploit the common channel-wise\npatterns exhibited by the outliers to rearrange them in such a way, that their\nquantization quality is significantly improved. The methodology yields 2x\nsavings in the memory footprint without significant degradation of the model's\naccuracy. Importantly, the rearrangement of channels happens at the compile\ntime and thus has no impact on the inference latency.",
        "pdf_link": "https://arxiv.org/pdf/2403.20137v1.pdf"
    },
    {
        "title": "User Modeling Challenges in Interactive AI Assistant Systems",
        "authors": [
            "Megan Su",
            "Yuwei Bao"
        ],
        "published": "2024-03-29T11:54:13Z",
        "summary": "Interactive Artificial Intelligent(AI) assistant systems are designed to\noffer timely guidance to help human users to complete a variety tasks. One of\nthe remaining challenges is to understand user's mental states during the task\nfor more personalized guidance. In this work, we analyze users' mental states\nduring task executions and investigate the capabilities and challenges for\nlarge language models to interpret user profiles for more personalized user\nguidance.",
        "pdf_link": "https://arxiv.org/pdf/2403.20134v1.pdf"
    },
    {
        "title": "The Impact of Prompts on Zero-Shot Detection of AI-Generated Text",
        "authors": [
            "Kaito Taguchi",
            "Yujie Gu",
            "Kouichi Sakurai"
        ],
        "published": "2024-03-29T11:33:34Z",
        "summary": "In recent years, there have been significant advancements in the development\nof Large Language Models (LLMs). While their practical applications are now\nwidespread, their potential for misuse, such as generating fake news and\ncommitting plagiarism, has posed significant concerns. To address this issue,\ndetectors have been developed to evaluate whether a given text is\nhuman-generated or AI-generated. Among others, zero-shot detectors stand out as\neffective approaches that do not require additional training data and are often\nlikelihood-based. In chat-based applications, users commonly input prompts and\nutilize the AI-generated texts. However, zero-shot detectors typically analyze\nthese texts in isolation, neglecting the impact of the original prompts. It is\nconceivable that this approach may lead to a discrepancy in likelihood\nassessments between the text generation phase and the detection phase. So far,\nthere remains an unverified gap concerning how the presence or absence of\nprompts impacts detection accuracy for zero-shot detectors. In this paper, we\nintroduce an evaluative framework to empirically analyze the impact of prompts\non the detection accuracy of AI-generated text. We assess various zero-shot\ndetectors using both white-box detection, which leverages the prompt, and\nblack-box detection, which operates without prompt information. Our experiments\nreveal the significant influence of prompts on detection accuracy. Remarkably,\ncompared with black-box detection without prompts, the white-box methods using\nprompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot\ndetectors tested. Code is available:\n\\url{https://github.com/kaito25atugich/Detector}.",
        "pdf_link": "https://arxiv.org/pdf/2403.20127v1.pdf"
    },
    {
        "title": "ITCMA: A Generative Agent Based on a Computational Consciousness Structure",
        "authors": [
            "Hanzhong Zhang",
            "Jibin Yin",
            "Haoyang Wang",
            "Ziwei Xiang"
        ],
        "published": "2024-03-29T10:23:18Z",
        "summary": "Large Language Models (LLMs) still face challenges in tasks requiring\nunderstanding implicit instructions and applying common-sense knowledge. In\nsuch scenarios, LLMs may require multiple attempts to achieve human-level\nperformance, potentially leading to inaccurate responses or inferences in\npractical environments, affecting their long-term consistency and behavior.\nThis paper introduces the Internal Time-Consciousness Machine (ITCM), a\ncomputational consciousness structure. We further propose the ITCM-based Agent\n(ITCMA), which supports behavior generation and reasoning in open-world\nsettings. ITCMA enhances LLMs' ability to understand implicit instructions and\napply common-sense knowledge by considering agents' interaction and reasoning\nwith the environment. Evaluations in the Alfworld environment show that trained\nITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even\nuntrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher\nthan SOTA, indicating its superiority over traditional intelligent agents in\nutility and generalization. In real-world tasks with quadruped robots, the\nuntrained ITCMA achieves an 85% task completion rate, which is close to its\nperformance in the unseen set, demonstrating its comparable utility in\nreal-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.20097v1.pdf"
    },
    {
        "title": "Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning",
        "authors": [
            "Yongqi Tong",
            "Dawei Li",
            "Sizhe Wang",
            "Yujia Wang",
            "Fei Teng",
            "Jingbo Shang"
        ],
        "published": "2024-03-29T08:30:34Z",
        "summary": "Recent works have shown the benefits to LLMs from fine-tuning golden-standard\nChain-of-Thought (CoT) rationales or using them as correct examples in few-shot\nprompting. While humans can indeed imitate correct examples, learning from our\nmistakes is another vital aspect of human cognition. Hence, a question\nnaturally arises: \\textit{can LLMs learn and benefit from their mistakes,\nespecially for their reasoning? } This study investigates this problem from\nboth the prompting and model-tuning perspectives. We begin by introducing\n\\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed\nwith both correct and error references, and demonstrating the types and reasons\nfor making such mistakes. To explore the effectiveness of those mistakes, we\ndesign two methods: (1) \\textbf{Self-rethinking} prompting guides LLMs to\nrethink whether they have made similar previous mistakes; and (2)\n\\textbf{Mistake tuning} involves finetuning models in both correct and\nincorrect reasoning domains, rather than only tuning models to learn ground\ntruth in traditional methodology. We conduct a series of experiments to prove\nLLMs can obtain benefits from mistakes in both directions. Our two methods\noffer potentially cost-effective strategies by leveraging errors to enhance\nreasoning capabilities, which costs significantly less than creating\nmeticulously hand-crafted golden references. We ultimately make a thorough\nanalysis of the reasons behind LLMs' errors, which provides directions that\nfuture research needs to overcome. \\textsc{CoTErrorSet} will be published soon\non \\texttt{Anonymity Link}.",
        "pdf_link": "https://arxiv.org/pdf/2403.20046v1.pdf"
    },
    {
        "title": "PURPLE: Making a Large Language Model a Better SQL Writer",
        "authors": [
            "Tonghui Ren",
            "Yuankai Fan",
            "Zhenying He",
            "Ren Huang",
            "Jiaqi Dai",
            "Can Huang",
            "Yinan Jing",
            "Kai Zhang",
            "Yifan Yang",
            "X. Sean Wang"
        ],
        "published": "2024-03-29T07:01:29Z",
        "summary": "Large Language Model (LLM) techniques play an increasingly important role in\nNatural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora\nhave strong natural language understanding and basic SQL generation abilities\nwithout additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL\napproaches try to improve the translation by enhancing the LLMs with an\nemphasis on user intention understanding. However, LLMs sometimes fail to\ngenerate appropriate SQL due to their lack of knowledge in organizing complex\nlogical operator composition. A promising method is to input the LLMs with\ndemonstrations, which include known NL2SQL translations from various databases.\nLLMs can learn to organize operator compositions from the input demonstrations\nfor the given task. In this paper, we propose PURPLE (Pre-trained models\nUtilized to Retrieve Prompts for Logical Enhancement), which improves accuracy\nby retrieving demonstrations containing the requisite logical operator\ncomposition for the NL2SQL task on hand, thereby guiding LLMs to produce better\nSQL translation. PURPLE achieves a new state-of-the-art performance of 80.5%\nexact-set match accuracy and 87.8% execution match accuracy on the validation\nset of the popular NL2SQL benchmark Spider. PURPLE maintains high accuracy\nacross diverse benchmarks, budgetary constraints, and various LLMs, showing\nrobustness and cost-effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.20014v1.pdf"
    },
    {
        "title": "On Large Language Models' Hallucination with Regard to Known Facts",
        "authors": [
            "Che Jiang",
            "Biqing Qi",
            "Xiangyu Hong",
            "Dayuan Fu",
            "Yang Cheng",
            "Fandong Meng",
            "Mo Yu",
            "Bowen Zhou",
            "Jie Zhou"
        ],
        "published": "2024-03-29T06:48:30Z",
        "summary": "Large language models are successful in answering factoid questions but are\nalso prone to hallucination.We investigate the phenomenon of LLMs possessing\ncorrect answer knowledge yet still hallucinating from the perspective of\ninference dynamics, an area not previously covered in studies on\nhallucinations.We are able to conduct this analysis via two key ideas.First, we\nidentify the factual questions that query the same triplet knowledge but result\nin different answers. The difference between the model behaviors on the correct\nand incorrect outputs hence suggests the patterns when hallucinations happen.\nSecond, to measure the pattern, we utilize mappings from the residual streams\nto vocabulary space. We reveal the different dynamics of the output token\nprobabilities along the depths of layers between the correct and hallucinated\ncases. In hallucinated cases, the output token's information rarely\ndemonstrates abrupt increases and consistent superiority in the later stages of\nthe model. Leveraging the dynamic curve as a feature, we build a classifier\ncapable of accurately detecting hallucinatory predictions with an 88\\% success\nrate. Our study shed light on understanding the reasons for LLMs'\nhallucinations on their known facts, and more importantly, on accurately\npredicting when they are hallucinating.",
        "pdf_link": "https://arxiv.org/pdf/2403.20009v1.pdf"
    },
    {
        "title": "Large Language Model based Situational Dialogues for Second Language Learning",
        "authors": [
            "Shuyao Xu",
            "Long Qin",
            "Tianyang Chen",
            "Zhenzhou Zha",
            "Bingxue Qiu",
            "Weizhi Wang"
        ],
        "published": "2024-03-29T06:43:55Z",
        "summary": "In second language learning, scenario-based conversation practice is\nimportant for language learners to achieve fluency in speaking, but students\noften lack sufficient opportunities to practice their conversational skills\nwith qualified instructors or native speakers. To bridge this gap, we propose\nsituational dialogue models for students to engage in conversational practice.\nOur situational dialogue models are fine-tuned on large language models (LLMs),\nwith the aim of combining the engaging nature of an open-ended conversation\nwith the focused practice of scenario-based tasks. Leveraging the\ngeneralization capabilities of LLMs, we demonstrate that our situational\ndialogue models perform effectively not only on training topics but also on\ntopics not encountered during training. This offers a promising solution to\nsupport a wide range of conversational topics without extensive manual work.\nAdditionally, research in the field of dialogue systems still lacks reliable\nautomatic evaluation metrics, leading to human evaluation as the gold standard\n(Smith et al., 2022), which is typically expensive. To address the limitations\nof existing evaluation methods, we present a novel automatic evaluation method\nthat employs fine-tuned LLMs to efficiently and effectively assess the\nperformance of situational dialogue models.",
        "pdf_link": "https://arxiv.org/pdf/2403.20005v1.pdf"
    },
    {
        "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
        "authors": [
            "Qinhao Zhou",
            "Zihan Zhang",
            "Xiang Xiang",
            "Ke Wang",
            "Yuchuan Wu",
            "Yongbin Li"
        ],
        "published": "2024-03-29T03:48:12Z",
        "summary": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language\nunderstanding and generation capabilities, making them highly successful in a\nvariety of tasks. However, when used as agents for dealing with complex\nproblems in the real world, their performance is far inferior to large\ncommercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need\nto have the capabilities of task planning, long-term memory, and the ability to\nleverage external tools to achieve satisfactory performance. Various methods\nhave been proposed to enhance the agent capabilities of LLMs. On the one hand,\nmethods involve constructing agent-specific data and fine-tuning the models. On\nthe other hand, some methods focus on designing prompts that effectively\nactivate the reasoning abilities of the LLMs. We explore both strategies on the\n7B and 13B models. We propose a comprehensive method for constructing\nagent-specific data using GPT-4. Through supervised fine-tuning with\nconstructed data, we find that for these models with a relatively small number\nof parameters, supervised fine-tuning can significantly reduce hallucination\noutputs and formatting errors in agent tasks. Furthermore, techniques such as\nmulti-path reasoning and task decomposition can effectively decrease problem\ncomplexity and enhance the performance of LLMs as agents. We evaluate our\nmethod on five agent tasks of AgentBench and achieve satisfactory results.",
        "pdf_link": "https://arxiv.org/pdf/2403.19962v1.pdf"
    },
    {
        "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
        "authors": [
            "Hanting Chen",
            "Zhicheng Liu",
            "Xutao Wang",
            "Yuchuan Tian",
            "Yunhe Wang"
        ],
        "published": "2024-03-29T02:32:15Z",
        "summary": "In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.",
        "pdf_link": "https://arxiv.org/pdf/2403.19928v2.pdf"
    },
    {
        "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
        "authors": [
            "Peng Ding",
            "Jiading Fang",
            "Peng Li",
            "Kangrui Wang",
            "Xiaochen Zhou",
            "Mo Yu",
            "Jing Li",
            "Matthew R. Walter",
            "Hongyuan Mei"
        ],
        "published": "2024-03-29T01:53:24Z",
        "summary": "Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.",
        "pdf_link": "https://arxiv.org/pdf/2403.19913v1.pdf"
    },
    {
        "title": "Towards a Robust Retrieval-Based Summarization System",
        "authors": [
            "Shengjie Liu",
            "Jing Wu",
            "Jingyuan Bao",
            "Wenyi Wang",
            "Naira Hovakimyan",
            "Christopher G Healey"
        ],
        "published": "2024-03-29T00:14:46Z",
        "summary": "This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.",
        "pdf_link": "https://arxiv.org/pdf/2403.19889v1.pdf"
    },
    {
        "title": "LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces",
        "authors": [
            "Xiaomin Ouyang",
            "Mani Srivastava"
        ],
        "published": "2024-03-28T22:06:04Z",
        "summary": "Most studies on machine learning in sensing systems focus on low-level\nperception tasks that process raw sensory data within a short time window.\nHowever, many practical applications, such as human routine modeling and\noccupancy tracking, require high-level reasoning abilities to comprehend\nconcepts and make inferences based on long-term sensor traces. Existing machine\nlearning-based approaches for handling such complex tasks struggle to\ngeneralize due to the limited training samples and the high dimensionality of\nsensor traces, necessitating the integration of human knowledge for designing\nfirst-principle models or logic reasoning methods. We pose a fundamental\nquestion: Can we harness the reasoning capabilities and world knowledge of\nLarge Language Models (LLMs) to recognize complex events from long-term\nspatiotemporal sensor traces? To answer this question, we design an effective\nprompting framework for LLMs on high-level reasoning tasks, which can handle\ntraces from the raw sensor data as well as the low-level perception results. We\nalso design two strategies to enhance performance with long sensor traces,\nincluding summarization before reasoning and selective inclusion of historical\ntraces. Our framework can be implemented in an edge-cloud setup, running small\nLLMs on the edge for data summarization and performing high-level reasoning on\nthe cloud for privacy preservation. The results show that LLMSense can achieve\nover 80\\% accuracy on two high-level reasoning tasks such as dementia diagnosis\nwith behavior traces and occupancy tracking with environmental sensor traces.\nThis paper provides a few insights and guidelines for leveraging LLM for\nhigh-level reasoning on sensor traces and highlights several directions for\nfuture work.",
        "pdf_link": "https://arxiv.org/pdf/2403.19857v1.pdf"
    },
    {
        "title": "Localizing Paragraph Memorization in Language Models",
        "authors": [
            "Niklas Stoehr",
            "Mitchell Gordon",
            "Chiyuan Zhang",
            "Owen Lewis"
        ],
        "published": "2024-03-28T21:53:24Z",
        "summary": "Can we localize the weights and mechanisms used by a language model to\nmemorize and recite entire paragraphs of its training data? In this paper, we\nshow that while memorization is spread across multiple layers and model\ncomponents, gradients of memorized paragraphs have a distinguishable spatial\npattern, being larger in lower model layers than gradients of non-memorized\nexamples. Moreover, the memorized examples can be unlearned by fine-tuning only\nthe high-gradient weights. We localize a low-layer attention head that appears\nto be especially involved in paragraph memorization. This head is predominantly\nfocusing its attention on distinctive, rare tokens that are least frequent in a\ncorpus-level unigram distribution. Next, we study how localized memorization is\nacross the tokens in the prefix by perturbing tokens and measuring the caused\nchange in the decoding. A few distinctive tokens early in a prefix can often\ncorrupt the entire continuation. Overall, memorized continuations are not only\nharder to unlearn, but also to corrupt than non-memorized ones.",
        "pdf_link": "https://arxiv.org/pdf/2403.19851v1.pdf"
    },
    {
        "title": "Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving",
        "authors": [
            "Akshay Gopalkrishnan",
            "Ross Greer",
            "Mohan Trivedi"
        ],
        "published": "2024-03-28T21:18:33Z",
        "summary": "Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have\nbecome prominent in autonomous driving research, as these models can provide\ninterpretable textual reasoning and responses for end-to-end autonomous driving\nsafety tasks using traffic scene images and other data modalities. However,\ncurrent approaches to these systems use expensive large language model (LLM)\nbackbones and image encoders, making such systems unsuitable for real-time\nautonomous driving systems where tight memory constraints exist and fast\ninference time is necessary. To address these previous issues, we develop\nEM-VLM4AD, an efficient, lightweight, multi-frame vision language model which\nperforms Visual Question Answering for autonomous driving. In comparison to\nprevious approaches, EM-VLM4AD requires at least 10 times less memory and\nfloating point operations, while also achieving higher BLEU-4, METEOR, CIDEr,\nand ROGUE scores than the existing baseline on the DriveLM dataset. EM-VLM4AD\nalso exhibits the ability to extract relevant information from traffic views\nrelated to prompts and can answer questions for various autonomous driving\nsubtasks. We release our code to train and evaluate our model at\nhttps://github.com/akshaygopalkr/EM-VLM4AD.",
        "pdf_link": "https://arxiv.org/pdf/2403.19838v1.pdf"
    },
    {
        "title": "Target Span Detection for Implicit Harmful Content",
        "authors": [
            "Nazanin Jafari",
            "James Allan"
        ],
        "published": "2024-03-28T21:15:15Z",
        "summary": "Identifying the targets of hate speech is a crucial step in grasping the\nnature of such speech and, ultimately, in improving the detection of offensive\nposts on online forums. Much harmful content on online platforms uses implicit\nlanguage especially when targeting vulnerable and protected groups such as\nusing stereotypical characteristics instead of explicit target names, making it\nharder to detect and mitigate the language. In this study, we focus on\nidentifying implied targets of hate speech, essential for recognizing subtler\nhate speech and enhancing the detection of harmful content on digital\nplatforms. We define a new task aimed at identifying the targets even when they\nare not explicitly stated. To address that task, we collect and annotate target\nspans in three prominent implicit hate speech datasets: SBIC, DynaHate, and\nIHC. We call the resulting merged collection Implicit-Target-Span. The\ncollection is achieved using an innovative pooling method with matching scores\nbased on human annotations and Large Language Models (LLMs). Our experiments\nindicate that Implicit-Target-Span provides a challenging test bed for target\nspan detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.19836v1.pdf"
    },
    {
        "title": "Developing Healthcare Language Model Embedding Spaces",
        "authors": [
            "Niall Taylor",
            "Dan Schofield",
            "Andrey Kormilitzin",
            "Dan W Joyce",
            "Alejo Nevado-Holgado"
        ],
        "published": "2024-03-28T19:31:32Z",
        "summary": "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain\ndatasets like healthcare focused text. We explore specialized pre-training to\nadapt smaller LLMs to different healthcare datasets. Three methods are\nassessed: traditional masked language modeling, Deep Contrastive Learning for\nUnsupervised Textual Representations (DeCLUTR), and a novel pre-training\nobjective utilizing metadata categories from the healthcare settings. These\nschemes are evaluated on downstream document classification tasks for each\ndataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification\ntasks, delivering strong performance from limited labeled data and with fewer\nmodel parameter updates required. While metadata-based pre-training does not\nfurther improve classifications across the datasets, it yields interesting\nembedding cluster separability. All domain adapted LLMs outperform their\npublicly available general base LLM, validating the importance of\ndomain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational\nbudgets, an essential capability for responsible and sustainable deployment in\nlocal healthcare settings. We provide pre-training guidelines for specialized\nhealthcare LLMs, motivate continued inquiry into contrastive objectives, and\ndemonstrates adaptation techniques to align small LLMs with privacy-sensitive\nmedical tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.19802v1.pdf"
    },
    {
        "title": "Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care",
        "authors": [
            "Niall Taylor",
            "Andrey Kormilitzin",
            "Isabelle Lorge",
            "Alejo Nevado-Holgado",
            "Dan W Joyce"
        ],
        "published": "2024-03-28T19:17:07Z",
        "summary": "Contemporary large language models (LLMs) may have utility for processing\nunstructured, narrative free-text clinical data contained in electronic health\nrecords (EHRs) -- a particularly important use-case for mental health where a\nmajority of routinely-collected patient data lacks structured, machine-readable\ncontent.\n  A significant problem for the the United Kingdom's National Health Service\n(NHS) are the long waiting lists for specialist mental healthcare. According to\nNHS data, in each month of 2023, there were between 370,000 and 470,000\nindividual new referrals into secondary mental healthcare services. Referrals\nmust be triaged by clinicians, using clinical information contained in the\npatient's EHR to arrive at a decision about the most appropriate mental\nhealthcare team to assess and potentially treat these patients.\n  The ability to efficiently recommend a relevant team by ingesting potentially\nvoluminous clinical notes could help services both reduce referral waiting\ntimes and with the right technology, improve the evidence available to justify\ntriage decisions.\n  We present and evaluate three different approaches for LLM-based, end-to-end\ningestion of variable-length clinical EHR data to assist clinicians when\ntriaging referrals. Our model is able to deliver triage recommendations\nconsistent with existing clinical practices and it's architecture was\nimplemented on a single GPU, making it practical for implementation in\nresource-limited NHS environments where private implementations of LLM\ntechnology will be necessary to ensure confidential clinical data is\nappropriately controlled and governed.",
        "pdf_link": "https://arxiv.org/pdf/2403.19790v1.pdf"
    },
    {
        "title": "GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation",
        "authors": [
            "Mohsen Gholami",
            "Mohammad Akbari",
            "Cindy Hu",
            "Vaden Masrani",
            "Z. Jane Wang",
            "Yong Zhang"
        ],
        "published": "2024-03-28T18:08:22Z",
        "summary": "Knowledge distillation from LLMs is essential for the efficient deployment of\nlanguage models. Prior works have proposed data generation using LLMs for\npreparing distilled models. We argue that generating data with LLMs is prone to\nsampling mainly from the center of original content distribution. This\nlimitation hinders the distilled model from learning the true underlying data\ndistribution and to forget the tails of the distributions (samples with lower\nprobability). To this end, we propose GOLD, a task-agnostic data generation and\nknowledge distillation framework, which employs an iterative\nout-of-distribution-guided feedback mechanism for the LLM. As a result, the\ngenerated data improves the generalizability of distilled models. An\nenergy-based OOD evaluation approach is also introduced to deal with noisy\ngenerated data. Our extensive experiments on 10 different classification and\nsequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior\narts and the LLM with an average improvement of 5% and 14%. We will also show\nthat the proposed method is applicable to less explored and novel tasks. The\ncode is available.",
        "pdf_link": "https://arxiv.org/pdf/2403.19754v1.pdf"
    },
    {
        "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
        "authors": [
            "Sirui Xu",
            "Ziyin Wang",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "published": "2024-03-28T17:59:30Z",
        "summary": "Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.",
        "pdf_link": "https://arxiv.org/pdf/2403.19652v1.pdf"
    },
    {
        "title": "MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions",
        "authors": [
            "Kai Zhang",
            "Yi Luan",
            "Hexiang Hu",
            "Kenton Lee",
            "Siyuan Qiao",
            "Wenhu Chen",
            "Yu Su",
            "Ming-Wei Chang"
        ],
        "published": "2024-03-28T17:59:20Z",
        "summary": "Image retrieval, i.e., finding desired images given a reference image,\ninherently encompasses rich, multi-faceted search intents that are difficult to\ncapture solely using image-based measures. Recent work leverages text\ninstructions to allow users to more freely express their search intents.\nHowever, existing work primarily focuses on image pairs that are visually\nsimilar and/or can be characterized by a small set of pre-defined relations.\nThe core thesis of this paper is that text instructions can enable retrieving\nimages with richer relations beyond visual similarity. To show this, we\nintroduce MagicLens, a series of self-supervised image retrieval models that\nsupport open-ended instructions. MagicLens is built on a key novel insight:\nimage pairs that naturally occur on the same web pages contain a wide range of\nimplicit relations (e.g., inside view of), and we can bring those implicit\nrelations explicit by synthesizing instructions via large multimodal models\n(LMMs) and large language models (LLMs). Trained on 36.7M (query image,\ninstruction, target image) triplets with rich semantic relations mined from the\nweb, MagicLens achieves comparable or better results on eight benchmarks of\nvarious image retrieval tasks than prior state-of-the-art (SOTA) methods.\nRemarkably, it outperforms previous SOTA but with a 50X smaller model size on\nmultiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus\nfurther demonstrate the diversity of search intents supported by MagicLens.",
        "pdf_link": "https://arxiv.org/pdf/2403.19651v1.pdf"
    },
    {
        "title": "Change-Agent: Towards Interactive Comprehensive Remote Sensing Change Interpretation and Analysis",
        "authors": [
            "Chenyang Liu",
            "Keyan Chen",
            "Haotian Zhang",
            "Zipeng Qi",
            "Zhengxia Zou",
            "Zhenwei Shi"
        ],
        "published": "2024-03-28T17:55:42Z",
        "summary": "Monitoring changes in the Earth's surface is crucial for understanding\nnatural processes and human impacts, necessitating precise and comprehensive\ninterpretation methodologies. Remote sensing satellite imagery offers a unique\nperspective for monitoring these changes, leading to the emergence of remote\nsensing image change interpretation (RSICI) as a significant research focus.\nCurrent RSICI technology encompasses change detection and change captioning,\neach with its limitations in providing comprehensive interpretation. To address\nthis, we propose an interactive Change-Agent, which can follow user\ninstructions to achieve comprehensive change interpretation and insightful\nanalysis according to user instructions, such as change detection and change\ncaptioning, change object counting, change cause analysis, etc. The\nChange-Agent integrates a multi-level change interpretation (MCI) model as the\neyes and a large language model (LLM) as the brain. The MCI model contains two\nbranches of pixel-level change detection and semantic-level change captioning,\nin which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local\nPerception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA)\nmodules to enhance the model's discriminative feature representation\ncapabilities. To support the training of the MCI model, we build the LEVIR-MCI\ndataset with a large number of change masks and captions of changes. Extensive\nexperiments demonstrate the effectiveness of the proposed MCI model and\nhighlight the promising potential of our Change-Agent in facilitating\ncomprehensive and intelligent interpretation of surface changes. To facilitate\nfuture research, we will make our dataset and codebase of the MCI model and\nChange-Agent publicly available at\nhttps://github.com/Chen-Yang-Liu/Change-Agent",
        "pdf_link": "https://arxiv.org/pdf/2403.19646v2.pdf"
    },
    {
        "title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models",
        "authors": [
            "Yucheng Shi",
            "Qiaoyu Tan",
            "Xuansheng Wu",
            "Shaochen Zhong",
            "Kaixiong Zhou",
            "Ninghao Liu"
        ],
        "published": "2024-03-28T17:47:19Z",
        "summary": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2403.19631v1.pdf"
    },
    {
        "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
        "authors": [
            "Bu Jin",
            "Yupeng Zheng",
            "Pengfei Li",
            "Weize Li",
            "Yuhang Zheng",
            "Sujie Hu",
            "Xinyu Liu",
            "Jinwei Zhu",
            "Zhijie Yan",
            "Haiyang Sun",
            "Kun Zhan",
            "Peng Jia",
            "Xiaoxiao Long",
            "Yilun Chen",
            "Hao Zhao"
        ],
        "published": "2024-03-28T17:12:55Z",
        "summary": "3D dense captioning stands as a cornerstone in achieving a comprehensive\nunderstanding of 3D scenes through natural language. It has recently witnessed\nremarkable achievements, particularly in indoor settings. However, the\nexploration of 3D dense captioning in outdoor scenes is hindered by two major\nchallenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such\nas dynamics and sparse visual inputs, makes it difficult to directly adapt\nexisting indoor methods; 2) the \\textbf{lack of data} with comprehensive\nbox-caption pair annotations specifically tailored for outdoor scenes. To this\nend, we introduce the new task of outdoor 3D dense captioning. As input, we\nassume a LiDAR point cloud and a set of RGB images captured by the panoramic\ncamera rig. The expected output is a set of object boxes with captions. To\ntackle this task, we propose the TOD3Cap network, which leverages the BEV\nrepresentation to generate object box proposals and integrates Relation\nQ-Former with LLaMA-Adapter to generate rich captions for these objects. We\nalso introduce the TOD3Cap dataset, the largest one to our knowledge for 3D\ndense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K\noutdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively\nlocalize and caption 3D objects in outdoor scenes, which outperforms baseline\nmethods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models are\npublicly available at https://github.com/jxbbb/TOD3Cap.",
        "pdf_link": "https://arxiv.org/pdf/2403.19589v1.pdf"
    },
    {
        "title": "WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models",
        "authors": [
            "Piotr Molenda",
            "Adian Liusie",
            "Mark J. F. Gales"
        ],
        "published": "2024-03-28T16:28:38Z",
        "summary": "Watermarking generative-AI systems, such as LLMs, has gained considerable\ninterest, driven by their enhanced capabilities across a wide range of tasks.\nAlthough current approaches have demonstrated that small, context-dependent\nshifts in the word distributions can be used to apply and detect watermarks,\nthere has been little work in analyzing the impact that these perturbations\nhave on the quality of generated texts. Balancing high detectability with\nminimal performance degradation is crucial in terms of selecting the\nappropriate watermarking setting; therefore this paper proposes a simple\nanalysis framework where comparative assessment, a flexible NLG evaluation\nframework, is used to assess the quality degradation caused by a particular\nwatermark setting. We demonstrate that our framework provides easy\nvisualization of the quality-detection trade-off of watermark settings,\nenabling a simple solution to find an LLM watermark operating point that\nprovides a well-balanced performance. This approach is applied to two different\nsummarization systems and a translation system, enabling cross-model analysis\nfor a task, and cross-task analysis.",
        "pdf_link": "https://arxiv.org/pdf/2403.19548v1.pdf"
    },
    {
        "title": "JDocQA: Japanese Document Question Answering Dataset for Generative Language Models",
        "authors": [
            "Eri Onami",
            "Shuhei Kurita",
            "Taiki Miyanishi",
            "Taro Watanabe"
        ],
        "published": "2024-03-28T14:22:54Z",
        "summary": "Document question answering is a task of question answering on given\ndocuments such as reports, slides, pamphlets, and websites, and it is a truly\ndemanding task as paper and electronic forms of documents are so common in our\nsociety. This is known as a quite challenging task because it requires not only\ntext understanding but also understanding of figures and tables, and hence\nvisual question answering (VQA) methods are often examined in addition to\ntextual approaches. We introduce Japanese Document Question Answering (JDocQA),\na large-scale document-based QA dataset, essentially requiring both visual and\ntextual information to answer questions, which comprises 5,504 documents in PDF\nformat and annotated 11,600 question-and-answer instances in Japanese. Each QA\ninstance includes references to the document pages and bounding boxes for the\nanswer clues. We incorporate multiple categories of questions and unanswerable\nquestions from the document for realistic question-answering applications. We\nempirically evaluate the effectiveness of our dataset with text-based large\nlanguage models (LLMs) and multimodal models. Incorporating unanswerable\nquestions in finetuning may contribute to harnessing the so-called\nhallucination generation.",
        "pdf_link": "https://arxiv.org/pdf/2403.19454v1.pdf"
    },
    {
        "title": "Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model",
        "authors": [
            "Qi Gou",
            "Cam-Tu Nguyen"
        ],
        "published": "2024-03-28T14:15:10Z",
        "summary": "Large Language Models (LLMs) have become increasingly popular due to their\nability to process and generate natural language. However, as they are trained\non massive datasets of text, LLMs can inherit harmful biases and produce\noutputs that are not aligned with human values. This paper studies two main\napproaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)\nand contrastive learning-based methods like Direct Preference Optimization\n(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose\nMPO (Mixed Preference Optimization), a novel method that mitigates the\nweaknesses of both approaches. Specifically, we propose a two-stage training\nprocedure: first train DPO on an easy dataset, and then perform RLHF on a\ndifficult set with DPO model being the reference model. Here, the easy and\ndifficult sets are constructed by a well-trained reward model that splits\nresponse pairs into those with large gaps of reward (easy), and those with\nsmall gaps (difficult). The first stage allows us to obtain a relatively\noptimal policy (LLM) model quickly, whereas the second stage refines LLM with\nonline RLHF, thus mitigating the distribution shift issue associated with DPO.\nExperiments are conducted on two public alignment datasets, namely HH-RLHF and\nTLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2403.19443v1.pdf"
    },
    {
        "title": "BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation",
        "authors": [
            "Yuhong He",
            "Yongqi Zhang",
            "Shizhu He",
            "Jun Wan"
        ],
        "published": "2024-03-28T13:38:13Z",
        "summary": "Medical dialogue generation (MDG) has gained increasing attention due to its\nsubstantial practical value. Previous works typically employ a\nsequence-to-sequence framework to generate medical responses by modeling\ndialogue context as sequential text with annotated medical entities. While\nthese methods have been successful in generating fluent responses, they fail to\nprovide process explanations of reasoning and require extensive entity\nannotation. To address these limitations, we propose the method Bootstrap\nPrompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's\nmulti-step reasoning process and iteratively enhance this reasoning process. We\nemploy a least-to-most prompting strategy to guide a large language model (LLM)\nin explicit reasoning, breaking down MDG into simpler sub-questions. These\nsub-questions build on answers from previous ones. Additionally, we also\nintroduce two distinct bootstrapping techniques for prompting, which\nautonomously correct errors and facilitate the LLM's explicit reasoning. This\napproach eliminates the need for entity annotation and increases the\ntransparency of the MDG process by explicitly generating the intermediate\nreasoning chain. The experimental findings on the two public datasets indicate\nthat BP4ER outperforms state-of-the-art methods in terms of both objective and\nsubjective evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.19414v1.pdf"
    },
    {
        "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining",
        "authors": [
            "Deyuan Liu",
            "Zecheng Wang",
            "Bingning Wang",
            "Weipeng Chen",
            "Chunshan Li",
            "Zhiying Tu",
            "Dianhui Chu",
            "Bo Li",
            "Dianbo Sui"
        ],
        "published": "2024-03-28T13:01:18Z",
        "summary": "The rapid proliferation of large language models (LLMs) such as GPT-4 and\nGemini underscores the intense demand for resources during their training\nprocesses, posing significant challenges due to substantial computational and\nenvironmental costs. To alleviate this issue, we propose checkpoint merging in\npretraining LLM. This method utilizes LLM checkpoints with shared training\ntrajectories, and is rooted in an extensive search space exploration for the\nbest merging weight via Bayesian optimization. Through various experiments, we\ndemonstrate that: (1) Our proposed methodology exhibits the capacity to augment\npretraining, presenting an opportunity akin to obtaining substantial benefits\nat minimal cost; (2) Our proposed methodology, despite requiring a given\nheld-out dataset, still demonstrates robust generalization capabilities across\ndiverse domains, a pivotal aspect in pretraining.",
        "pdf_link": "https://arxiv.org/pdf/2403.19390v1.pdf"
    },
    {
        "title": "Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors",
        "authors": [
            "Binzong Geng",
            "Zhaoxin Huan",
            "Xiaolu Zhang",
            "Yong He",
            "Liang Zhang",
            "Fajie Yuan",
            "Jun Zhou",
            "Linjian Mo"
        ],
        "published": "2024-03-28T12:05:15Z",
        "summary": "With the rise of large language models (LLMs), recent works have leveraged\nLLMs to improve the performance of click-through rate (CTR) prediction.\nHowever, we argue that a critical obstacle remains in deploying LLMs for\npractical use: the efficiency of LLMs when processing long textual user\nbehaviors. As user sequences grow longer, the current efficiency of LLMs is\ninadequate for training on billions of users and items. To break through the\nefficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical\nEncoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.\nSpecifically, BAHE proposes a novel hierarchical architecture that decouples\nthe encoding of user behaviors from inter-behavior interactions. Firstly, to\nprevent computational redundancy from repeated encoding of identical user\nbehaviors, BAHE employs the LLM's pre-trained shallow layers to extract\nembeddings of the most granular, atomic user behaviors from extensive user\nsequences and stores them in the offline database. Subsequently, the deeper,\ntrainable layers of the LLM facilitate intricate inter-behavior interactions,\nthereby generating comprehensive user embeddings. This separation allows the\nlearning of high-level user representations to be independent of low-level\nbehavior encoding, significantly reducing computational complexity. Finally,\nthese refined user embeddings, in conjunction with correspondingly processed\nitem embeddings, are incorporated into the CTR model to compute the CTR scores.\nExtensive experimental results show that BAHE reduces training time and memory\nby five times for CTR models using LLMs, especially with longer user sequences.\nBAHE has been deployed in a real-world system, allowing for daily updates of 50\nmillion CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR\nprediction.",
        "pdf_link": "https://arxiv.org/pdf/2403.19347v1.pdf"
    },
    {
        "title": "Large Language Models Are Unconscious of Unreasonability in Math Problems",
        "authors": [
            "Jingyuan Ma",
            "Damai Dai",
            "Zhifang Sui"
        ],
        "published": "2024-03-28T12:04:28Z",
        "summary": "Large language models (LLMs) demonstrate substantial capabilities in solving\nmath problems. However, they tend to produce hallucinations when given\nquestions containing unreasonable errors. In this paper, we study the behavior\nof LLMs when faced with unreasonable math problems and further explore their\npotential to address these problems. First, we construct the Unreasonable Math\nProblem (UMP) benchmark to examine the error detection ability of LLMs.\nExperiments show that LLMs are able to detect unreasonable errors, but still\nfail in generating non-hallucinatory content. In order to improve their ability\nof error detection and correction, we further design a strategic prompt\ntemplate called Critical Calculation and Conclusion(CCC). With CCC, LLMs can\nbetter self-evaluate and detect unreasonable errors in math questions, making\nthem more reliable and safe in practical application scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.19346v1.pdf"
    },
    {
        "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
        "authors": [
            "Jiacui Huang",
            "Hongtao Zhang",
            "Mingbo Zhao",
            "Zhou Wu"
        ],
        "published": "2024-03-28T11:52:42Z",
        "summary": "Vision-and-Language Navigation (VLN) is a challenging task that requires a\nrobot to navigate in photo-realistic environments with human natural language\npromptings. Recent studies aim to handle this task by constructing the semantic\nspatial map representation of the environment, and then leveraging the strong\nability of reasoning in large language models for generalizing code for guiding\nthe robot navigation. However, these methods face limitations in instance-level\nand attribute-level navigation tasks as they cannot distinguish different\ninstances of the same object. To address this challenge, we propose a new\nmethod, namely, Instance-aware Visual Language Map (IVLMap), to empower the\nrobot with instance-level and attribute-level semantic mapping, where it is\nautonomously constructed by fusing the RGBD video data collected from the robot\nagent with special-designed natural language map indexing in the bird's-in-eye\nview. Such indexing is instance-level and attribute-level. In particular, when\nintegrated with a large language model, IVLMap demonstrates the capability to\ni) transform natural language into navigation targets with instance and\nattribute information, enabling precise localization, and ii) accomplish\nzero-shot end-to-end navigation tasks based on natural language commands.\nExtensive navigation experiments are conducted. Simulation results illustrate\nthat our method can achieve an average improvement of 14.4\\% in navigation\naccuracy. Code and demo are released at https://ivlmap.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.19336v1.pdf"
    },
    {
        "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
        "authors": [
            "Jiaxing Chen",
            "Yuxuan Liu",
            "Dehu Li",
            "Xiang An",
            "Ziyong Feng",
            "Yongle Zhao",
            "Yin Xie"
        ],
        "published": "2024-03-28T11:26:30Z",
        "summary": "The surge of Multimodal Large Language Models (MLLMs), given their prominent\nemergent capabilities in instruction following and reasoning, has greatly\nadvanced the field of visual reasoning. However, constrained by their\nnon-lossless image tokenization, most MLLMs fall short of comprehensively\ncapturing details of text and objects, especially in high-resolution images. To\naddress this, we propose P2G, a novel framework for plug-and-play grounding of\nreasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of\nMLLMs to employ expert agents to achieve on-the-fly grounding to critical\nvisual and textual objects of image, thus achieving deliberate reasoning via\nmultimodal prompting. We further create P2GB, a benchmark aimed at assessing\nMLLMs' ability to understand inter-object relationships and text in challenging\nhigh-resolution images. Comprehensive experiments on visual reasoning tasks\ndemonstrate the superiority of P2G. Noteworthy, P2G achieved comparable\nperformance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the\npotential of plug-and-play grounding of reasoning and opens up a promising\nalternative beyond model scaling.",
        "pdf_link": "https://arxiv.org/pdf/2403.19322v1.pdf"
    },
    {
        "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
        "authors": [
            "Yu Li",
            "Shenyu Zhang",
            "Rui Wu",
            "Xiutian Huang",
            "Yongrui Chen",
            "Wenhao Xu",
            "Guilin Qi",
            "Dehai Min"
        ],
        "published": "2024-03-28T10:41:47Z",
        "summary": "Recent advancements in generative Large Language Models(LLMs) have been\nremarkable, however, the quality of the text generated by these models often\nreveals persistent issues. Evaluating the quality of text generated by these\nmodels, especially in open-ended text, has consistently presented a significant\nchallenge. Addressing this, recent work has explored the possibility of using\nLLMs as evaluators. While using a single LLM as an evaluation agent shows\npotential, it is filled with significant uncertainty and instability. To\naddress these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation\nframework\" where all agents are played by LLMs like GPT-4. The MATEval\nframework emulates human collaborative discussion methods, integrating multiple\nagents' interactions to evaluate open-ended text. Our framework incorporates\nself-reflection and Chain-of-Thought (CoT) strategies, along with feedback\nmechanisms, enhancing the depth and breadth of the evaluation process and\nguiding discussions towards consensus, while the framework generates\ncomprehensive evaluation reports, including error localization, error types and\nscoring. Experimental results show that our framework outperforms existing\nopen-ended text evaluation methods and achieves the highest correlation with\nhuman evaluation, which confirms the effectiveness and advancement of our\nframework in addressing the uncertainties and instabilities in evaluating\nLLMs-generated text. Furthermore, our framework significantly improves the\nefficiency of text evaluation and model iteration in industrial scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.19305v1.pdf"
    },
    {
        "title": "Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation",
        "authors": [
            "Chenming Tang",
            "Zhixiang Wang",
            "Yunfang Wu"
        ],
        "published": "2024-03-28T10:13:34Z",
        "summary": "In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.",
        "pdf_link": "https://arxiv.org/pdf/2403.19285v1.pdf"
    },
    {
        "title": "Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction",
        "authors": [
            "Chenming Tang",
            "Fanyi Qu",
            "Yunfang Wu"
        ],
        "published": "2024-03-28T10:05:57Z",
        "summary": "In the era of large language models (LLMs), in-context learning (ICL) stands\nout as an effective prompting strategy that explores LLMs' potency across\nvarious tasks. However, applying LLMs to grammatical error correction (GEC) is\nstill a challenging task. In this paper, we propose a novel\nungrammatical-syntax-based in-context example selection strategy for GEC.\nSpecifically, we measure similarity of sentences based on their syntactic\nstructures with diverse algorithms, and identify optimal ICL examples sharing\nthe most similar ill-formed syntax to the test input. Additionally, we carry\nout a two-stage process to further improve the quality of selection results. On\nbenchmark English GEC datasets, empirical results show that our proposed\nungrammatical-syntax-based strategies outperform commonly-used word-matching or\nsemantics-based methods with multiple LLMs. This indicates that for a\nsyntax-oriented task like GEC, paying more attention to syntactic information\ncan effectively boost LLMs' performance. Our code will be publicly available\nafter the publication of this paper.",
        "pdf_link": "https://arxiv.org/pdf/2403.19283v1.pdf"
    },
    {
        "title": "Fine-Tuning Language Models with Reward Learning on Policy",
        "authors": [
            "Hao Lang",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2024-03-28T10:02:10Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.",
        "pdf_link": "https://arxiv.org/pdf/2403.19279v1.pdf"
    },
    {
        "title": "sDPO: Don't Use Your Data All at Once",
        "authors": [
            "Dahyun Kim",
            "Yungi Kim",
            "Wonho Song",
            "Hyeonwoo Kim",
            "Yunsu Kim",
            "Sanghoon Kim",
            "Chanjun Park"
        ],
        "published": "2024-03-28T09:56:04Z",
        "summary": "As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.",
        "pdf_link": "https://arxiv.org/pdf/2403.19270v1.pdf"
    },
    {
        "title": "Dual-Personalizing Adapter for Federated Foundation Models",
        "authors": [
            "Yiyuan Yang",
            "Guodong Long",
            "Tao Shen",
            "Jing Jiang",
            "Michael Blumenstein"
        ],
        "published": "2024-03-28T08:19:33Z",
        "summary": "Recently, foundation models, particularly large language models (LLMs), have\ndemonstrated an impressive ability to adapt to various tasks by fine-tuning\nlarge amounts of instruction data. Notably, federated foundation models emerge\nas a privacy preservation method to fine-tune models collaboratively under\nfederated learning (FL) settings by leveraging many distributed datasets with\nnon-IID data. To alleviate communication and computation overhead,\nparameter-efficient methods are introduced for efficiency, and some research\nadapted personalization methods to federated foundation models for better user\npreferences alignment. However, a critical gap in existing research is the\nneglect of test-time distribution shifts in real-world applications. Therefore,\nto bridge this gap, we propose a new setting, termed test-time personalization,\nwhich not only concentrates on the targeted local task but also extends to\nother tasks that exhibit test-time distribution shifts. To address challenges\nin this new setting, we explore a simple yet effective solution to learn a\ncomprehensive foundation model. Specifically, a dual-personalizing adapter\narchitecture (FedDPA) is proposed, comprising a global adapter and a local\nadapter for addressing test-time distribution shifts and personalization,\nrespectively. Additionally, we introduce an instance-wise dynamic weighting\nmechanism to optimize the balance between the global and local adapters,\nenhancing overall performance. The effectiveness of the proposed method has\nbeen evaluated on benchmark datasets across different NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.19211v1.pdf"
    },
    {
        "title": "Text Data-Centric Image Captioning with Interactive Prompts",
        "authors": [
            "Yiyu Wang",
            "Hao Luo",
            "Jungang Xu",
            "Yingfei Sun",
            "Fan Wang"
        ],
        "published": "2024-03-28T07:43:49Z",
        "summary": "Supervised image captioning approaches have made great progress, but it is\nchallenging to collect high-quality human-annotated image-text data. Recently,\nlarge-scale vision and language models (e.g., CLIP) and large-scale generative\nlanguage models (e.g., GPT-2) have shown strong performances in various tasks,\nwhich also provide some new solutions for image captioning with web paired\ndata, unpaired data or even text-only data. Among them, the mainstream solution\nis to project image embeddings into the text embedding space with the\nassistance of consistent representations between image-text pairs from the CLIP\nmodel. However, the current methods still face several challenges in adapting\nto the diversity of data configurations in a unified solution, accurately\nestimating image-text embedding bias, and correcting unsatisfactory prediction\nresults in the inference stage. This paper proposes a new Text data-centric\napproach with Interactive Prompts for image Captioning, named TIPCap. 1) We\nconsider four different settings which gradually reduce the dependence on\npaired data. 2) We construct a mapping module driven by multivariate Gaussian\ndistribution to mitigate the modality gap, which is applicable to the above\nfour different settings. 3) We propose a prompt interaction module that can\nincorporate optional prompt information before generating captions. Extensive\nexperiments show that our TIPCap outperforms other weakly or unsupervised image\ncaptioning methods and achieves a new state-of-the-art performance on two\nwidely used datasets, i.e., MS-COCO and Flickr30K.",
        "pdf_link": "https://arxiv.org/pdf/2403.19193v1.pdf"
    },
    {
        "title": "MUGC: Machine Generated versus User Generated Content Detection",
        "authors": [
            "Yaqi Xie",
            "Anjali Rawal",
            "Yujing Cen",
            "Dixuan Zhao",
            "Sunil K Narang",
            "Shanu Sushmita"
        ],
        "published": "2024-03-28T07:33:53Z",
        "summary": "As advanced modern systems like deep neural networks (DNNs) and generative AI\ncontinue to enhance their capabilities in producing convincing and realistic\ncontent, the need to distinguish between user-generated and machine generated\ncontent is becoming increasingly evident. In this research, we undertake a\ncomparative evaluation of eight traditional machine-learning algorithms to\ndistinguish between machine-generated and human-generated data across three\ndiverse datasets: Poems, Abstracts, and Essays. Our results indicate that\ntraditional methods demonstrate a high level of accuracy in identifying\nmachine-generated data, reflecting the documented effectiveness of popular\npre-trained models like RoBERT. We note that machine-generated texts tend to be\nshorter and exhibit less word variety compared to human-generated content.\nWhile specific domain-related keywords commonly utilized by humans, albeit\ndisregarded by current LLMs (Large Language Models), may contribute to this\nhigh detection accuracy, we show that deeper word representations like word2vec\ncan capture subtle semantic variances. Furthermore, readability, bias, moral,\nand affect comparisons reveal a discernible contrast between machine-generated\nand human generated content. There are variations in expression styles and\npotentially underlying biases in the data sources (human and\nmachine-generated). This study provides valuable insights into the advancing\ncapacities and challenges associated with machine-generated content across\nvarious domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.19725v1.pdf"
    },
    {
        "title": "Make Large Language Model a Better Ranker",
        "authors": [
            "Wenshuo Chao",
            "Zhi Zheng",
            "Hengshu Zhu",
            "Hao Liu"
        ],
        "published": "2024-03-28T07:22:16Z",
        "summary": "The evolution of Large Language Models (LLMs) has significantly enhanced\ncapabilities across various fields, leading to a paradigm shift in how\nRecommender Systems (RSs) are conceptualized and developed. However, existing\nresearch primarily focuses on point-wise and pair-wise recommendation\nparadigms. These approaches prove inefficient in LLM-based recommenders due to\nthe high computational cost of utilizing Large Language Models. While some\nstudies have delved into list-wise approaches, they fall short in ranking\ntasks. This shortfall is attributed to the misalignment between the objectives\nof ranking and language generation. To this end, this paper introduces the\nLanguage Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO\nis designed to bridge the gap between the capabilities of LLMs and the nuanced\nrequirements of ranking tasks within recommender systems. A key feature of ALRO\nis the introduction of soft lambda loss, an adaptation of lambda loss tailored\nto suit language generation tasks. Additionally, ALRO incorporates a\npermutation-sensitive learning mechanism that addresses position bias, a\nprevalent issue in generative models, without imposing additional computational\nburdens during inference. Our evaluative studies reveal that ALRO outperforms\nexisting embedding-based recommendation methods and the existing LLM-based\nrecommendation baselines, highlighting its efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2403.19181v1.pdf"
    },
    {
        "title": "Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering",
        "authors": [
            "Yexin Wu",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2024-03-28T06:28:35Z",
        "summary": "Large language models have manifested remarkable capabilities by leveraging\nchain-of-thought (CoT) reasoning techniques to solve intricate questions\nthrough step-by-step reasoning chains. Despite its success, the efficacy of\nsuch reasoning is inherently contingent upon the quality of CoT. However,\nflawless CoT reasoning cannot be guaranteed due to the presence of\nindecomposable questions and the potential for erroneous reasoning chains,\nparticularly in the case of small-scale language models. To tackle this\nchallenge, we propose a novel approach called the selective filtering reasoner\n(SelF-Reasoner) that assesses the entailment relationship between the question\nand the candidate reasoning chain. Then, we proceed with CoT reasoning when the\nreasoning chain demonstrates confidence; otherwise, we opt to predict the\nanswer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently\nover the ScienceQA, ECQA, and LastLetter tasks. Code is available at\n\\texttt{https://github.com/LibroWu/SelF-Reasoner}.",
        "pdf_link": "https://arxiv.org/pdf/2403.19167v1.pdf"
    },
    {
        "title": "Disentangling Length from Quality in Direct Preference Optimization",
        "authors": [
            "Ryan Park",
            "Rafael Rafailov",
            "Stefano Ermon",
            "Chelsea Finn"
        ],
        "published": "2024-03-28T06:03:47Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has been a crucial\ncomponent in the recent success of Large Language Models. However, RLHF is know\nto exploit biases in human preferences, such as verbosity. A well-formatted and\neloquent answer is often more highly rated by users, even when it is less\nhelpful and objective. A number of approaches have been developed to control\nthose biases in the classical RLHF literature, but the problem remains\nrelatively under-explored for Direct Alignment Algorithms such as Direct\nPreference Optimization (DPO). Unlike classical RLHF, DPO does not train a\nseparate reward model or use reinforcement learning directly, so previous\napproaches developed to control verbosity cannot be directly applied to this\nsetting. Our work makes several contributions. For the first time, we study the\nlength problem in the DPO setting, showing significant exploitation in DPO and\nlinking it to out-of-distribution bootstrapping. We then develop a principled\nbut simple regularization strategy that prevents length exploitation, while\nstill maintaining improvements in model quality. We demonstrate these effects\nacross datasets on summarization and dialogue, where we achieve up to 20\\%\nimprovement in win rates when controlling for length, despite the GPT4 judge's\nwell-known verbosity bias.",
        "pdf_link": "https://arxiv.org/pdf/2403.19159v1.pdf"
    },
    {
        "title": "Compressing Large Language Models by Streamlining the Unimportant Layer",
        "authors": [
            "Xiaodong Chen",
            "Yuxuan Hu",
            "Jing Zhang"
        ],
        "published": "2024-03-28T04:12:13Z",
        "summary": "Large language models (LLM) have been extensively applied in various natural\nlanguage tasks and domains, but their applicability is constrained by the large\nnumber of parameters of the models. Consequently, there is an increasing\nemphasis on compact models that exhibit high performance. In this study, we\nobserve that different layers in LLM have varying degrees of perturbation on\nthe hidden states, which allows us to identify less important layers. Based on\nthis phenomenon, we propose LLM-Streamline, which consists of two parts: layer\npruning, where we remove a set of consecutive layers with the lowest importance\nin the model according to the target sparsity; and layer replacement, where we\ntrain a lightweight model to substitute the pruned layers, thereby mitigating\nthe performance degradation caused by pruning. In our experiments, we utilize\nstructures such as a multi-layer perceptron (MLP) and a transformer layer as\nlightweight models and ultimately demonstrate that a single MLP can effectively\nfit the pruned layers. Comprehensive experiments show that our proposed method,\nLLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruning\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2403.19135v2.pdf"
    },
    {
        "title": "Code Comparison Tuning for Code Large Language Models",
        "authors": [
            "Yufan Jiang",
            "Qiaozhi He",
            "Xiaomin Zhuang",
            "Zhihua Wu"
        ],
        "published": "2024-03-28T03:25:23Z",
        "summary": "We present Code Comparison Tuning (CCT), a simple and effective tuning method\nfor code large language models (Code LLMs) to better handle subtle code errors.\nSpecifically, we integrate the concept of comparison into instruction tuning,\nboth at the token and sequence levels, enabling the model to discern even the\nslightest deviations in code. To compare the original code with an erroneous\nversion containing manually added code errors, we use token-level preference\nloss for detailed token-level comparisons. Additionally, we combine code\nsegments to create a new instruction tuning sample for sequence-level\ncomparisons, enhancing the model's bug-fixing capability. Experimental results\non the HumanEvalFix benchmark show that CCT surpasses instruction tuning in\npass@1 scores by up to 4 points across diverse code LLMs, and extensive\nanalysis demonstrates the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2403.19121v1.pdf"
    },
    {
        "title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering",
        "authors": [
            "Che Guan",
            "Mengyu Huang",
            "Peng Zhang"
        ],
        "published": "2024-03-28T03:14:18Z",
        "summary": "In today's fast-paced industry, professionals face the challenge of\nsummarizing a large number of documents and extracting vital information from\nthem on a daily basis. These metrics are frequently hidden away in tables\nand/or their nested hyperlinks. To address this challenge, the approach of\nTable Question Answering (QA) has been developed to extract the relevant\ninformation. However, traditional Table QA training tasks that provide a table\nand an answer(s) from a gold cell coordinate(s) for a question may not always\nensure extracting the accurate answer(s). Recent advancements in Large Language\nModels (LLMs) have opened up new possibilities for extracting information from\ntabular data using prompts. In this paper, we introduce the Multi-hop Few-shot\nOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. The\nfirst step involves Few-Shot Learning (FSL), where relevant tables and\nassociated contexts of hyperlinks are retrieved based on a given question. The\nretrieved content is then used to construct few-shot prompts as inputs to an\nLLM, such as ChatGPT. To tackle the challenge of answering complex questions,\nthe second step leverages Chain-of-thought (CoT) prompting to decompose the\ncomplex question into a sequential chain of questions and reasoning thoughts in\na multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process\nby retrieving relevant tables and contexts of hyperlinks that are relevant to\nthe resulting reasoning thoughts and questions. These additional contexts are\nthen used to supplement the prompt used in the first step, resulting in more\naccurate answers from an LLM. Empirical results from OTT-QA demonstrate that\nour abstractive QA approach significantly improves the accuracy of extractive\nTable QA methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.19116v1.pdf"
    },
    {
        "title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",
        "authors": [
            "Chunqiu Steven Xia",
            "Yinlin Deng",
            "Lingming Zhang"
        ],
        "published": "2024-03-28T03:10:39Z",
        "summary": "LLMs have become the go-to choice for code generation tasks, with an\nexponential increase in the training, development, and usage of LLMs\nspecifically for code generation. To evaluate the ability of LLMs on code, both\nacademic and industry practitioners rely on popular handcrafted benchmarks.\nHowever, prior benchmarks contain only a very limited set of problems, both in\nquantity and variety. Further, due to popularity and age, many benchmarks are\nprone to data leakage where example solutions can be readily found on the web\nand thus potentially in training data. Such limitations inevitably lead us to\ninquire: Is the leaderboard performance on existing benchmarks reliable and\ncomprehensive enough to measure the program synthesis ability of LLMs? To\naddress this, we introduce EvoEval -- a program synthesis benchmark suite\ncreated by evolving existing benchmarks into different targeted domains for a\ncomprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows\nthat compared to the high performance obtained on standard benchmarks like\nHumanEval, there is a significant drop in performance (on average 39.4%) when\nusing EvoEval. Additionally, the decrease in performance can range from 19.6%\nto 47.7%, leading to drastic ranking changes amongst LLMs and showing potential\noverfitting of existing benchmarks. Furthermore, we showcase various insights,\nincluding the brittleness of instruction-following models when encountering\nrewording or subtle changes as well as the importance of learning problem\ncomposition and decomposition. EvoEval not only provides comprehensive\nbenchmarks, but can be used to further evolve arbitrary problems to keep up\nwith advances and the ever-changing landscape of LLMs for code. We have\nopen-sourced our benchmarks, tools, and complete LLM generations at\nhttps://github.com/evo-eval/evoeval",
        "pdf_link": "https://arxiv.org/pdf/2403.19114v1.pdf"
    },
    {
        "title": "FACTOID: FACtual enTailment fOr hallucInation Detection",
        "authors": [
            "Vipula Rawte",
            "S. M Towhidul Islam Tonmoy",
            "Krishnav Rajbangshi",
            "Shravani Nag",
            "Aman Chadha",
            "Amit P. Sheth",
            "Amitava Das"
        ],
        "published": "2024-03-28T03:09:42Z",
        "summary": "The widespread adoption of Large Language Models (LLMs) has facilitated\nnumerous benefits. However, hallucination is a significant concern. In\nresponse, Retrieval Augmented Generation (RAG) has emerged as a highly\npromising paradigm to improve LLM outputs by grounding them in factual\ninformation. RAG relies on textual entailment (TE) or similar methods to check\nif the text produced by LLMs is supported or contradicted, compared to\nretrieved documents. This paper argues that conventional TE methods are\ninadequate for spotting hallucinations in content generated by LLMs. For\ninstance, consider a prompt about the 'USA's stance on the Ukraine war''. The\nAI-generated text states, ...U.S. President Barack Obama says the U.S. will not\nput troops in Ukraine...'' However, during the war the U.S. president is Joe\nBiden which contradicts factual reality. Moreover, current TE systems are\nunable to accurately annotate the given text and identify the exact portion\nthat is contradicted. To address this, we introduces a new type of TE called\n``Factual Entailment (FE).'', aims to detect factual inaccuracies in content\ngenerated by LLMs while also highlighting the specific text segment that\ncontradicts reality. We present FACTOID (FACTual enTAILment for hallucInation\nDetection), a benchmark dataset for FE. We propose a multi-task learning (MTL)\nframework for FE, incorporating state-of-the-art (SoTA) long text embeddings\nsuch as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The\nproposed MTL architecture for FE achieves an avg. 40\\% improvement in accuracy\non the FACTOID benchmark compared to SoTA TE methods. As FE automatically\ndetects hallucinations, we assessed 15 modern LLMs and ranked them using our\nproposed Auto Hallucination Vulnerability Index (HVI_auto). This index\nquantifies and offers a comparative scale to evaluate and rank LLMs according\nto their hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2403.19113v1.pdf"
    },
    {
        "title": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
        "authors": [
            "Patrick Chao",
            "Edoardo Debenedetti",
            "Alexander Robey",
            "Maksym Andriushchenko",
            "Francesco Croce",
            "Vikash Sehwag",
            "Edgar Dobriban",
            "Nicolas Flammarion",
            "George J. Pappas",
            "Florian Tramer",
            "Hamed Hassani",
            "Eric Wong"
        ],
        "published": "2024-03-28T02:44:02Z",
        "summary": "Jailbreak attacks cause large language models (LLMs) to generate harmful,\nunethical, or otherwise objectionable content. Evaluating these attacks\npresents a number of challenges, which the current collection of benchmarks and\nevaluation techniques do not adequately address. First, there is no clear\nstandard of practice regarding jailbreaking evaluation. Second, existing works\ncompute costs and success rates in incomparable ways. And third, numerous works\nare not reproducible, as they withhold adversarial prompts, involve\nclosed-source code, or rely on evolving proprietary APIs. To address these\nchallenges, we introduce JailbreakBench, an open-sourced benchmark with the\nfollowing components: (1) a new jailbreaking dataset containing 100 unique\nbehaviors, which we call JBB-Behaviors; (2) an evolving repository of\nstate-of-the-art adversarial prompts, which we refer to as jailbreak artifacts;\n(3) a standardized evaluation framework that includes a clearly defined threat\nmodel, system prompts, chat templates, and scoring functions; and (4) a\nleaderboard that tracks the performance of attacks and defenses for various\nLLMs. We have carefully considered the potential ethical implications of\nreleasing this benchmark, and believe that it will be a net positive for the\ncommunity. Over time, we will expand and adapt the benchmark to reflect\ntechnical and methodological advances in the research community.",
        "pdf_link": "https://arxiv.org/pdf/2404.01318v1.pdf"
    },
    {
        "title": "Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation",
        "authors": [
            "Yutong He",
            "Alexander Robey",
            "Naoki Murata",
            "Yiding Jiang",
            "Joshua Williams",
            "George J. Pappas",
            "Hamed Hassani",
            "Yuki Mitsufuji",
            "Ruslan Salakhutdinov",
            "J. Zico Kolter"
        ],
        "published": "2024-03-28T02:35:53Z",
        "summary": "Prompt engineering is effective for controlling the output of text-to-image\n(T2I) generative models, but it is also laborious due to the need for manually\ncrafted prompts. This challenge has spurred the development of algorithms for\nautomated prompt generation. However, these methods often struggle with\ntransferability across T2I models, require white-box access to the underlying\nmodel, and produce non-intuitive prompts. In this work, we introduce PRISM, an\nalgorithm that automatically identifies human-interpretable and transferable\nprompts that can effectively generate desired concepts given only black-box\naccess to T2I models. Inspired by large language model (LLM) jailbreaking,\nPRISM leverages the in-context learning ability of LLMs to iteratively refine\nthe candidate prompts distribution for given reference images. Our experiments\ndemonstrate the versatility and effectiveness of PRISM in generating accurate\nprompts for objects, styles and images across multiple T2I models, including\nStable Diffusion, DALL-E, and Midjourney.",
        "pdf_link": "https://arxiv.org/pdf/2403.19103v1.pdf"
    },
    {
        "title": "Learning From Correctness Without Prompting Makes LLM Efficient Reasoner",
        "authors": [
            "Yuxuan Yao",
            "Han Wu",
            "Zhijiang Guo",
            "Biyan Zhou",
            "Jiahui Gao",
            "Sichun Luo",
            "Hanxu Hou",
            "Xiaojin Fu",
            "Linqi Song"
        ],
        "published": "2024-03-28T02:12:49Z",
        "summary": "Large language models (LLMs) have demonstrated outstanding performance across\nvarious tasks, yet they still exhibit limitations such as hallucination,\nunfaithful reasoning, and toxic content. One potential approach to mitigate\nthese issues is learning from human or external feedback (e.g. tools). In this\npaper, we introduce an intrinsic self-correct reasoning framework for LLMs that\neliminates the need for human feedback, external tools, and handcraft prompts.\nThe proposed framework, based on a multi-step reasoning paradigm\n\\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning\nperformance without needing to learn from errors. This paradigm prioritizes\nlearning from correct reasoning steps, and a unique method to measure\nconfidence for each reasoning step based on generation logits. Experimental\nresults across various multi-step reasoning tasks demonstrate the effectiveness\nof the framework in improving reasoning performance with reduced token\nconsumption.",
        "pdf_link": "https://arxiv.org/pdf/2403.19094v1.pdf"
    },
    {
        "title": "CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems",
        "authors": [
            "Amin Abolghasemi",
            "Zhaochun Ren",
            "Arian Askari",
            "Mohammad Aliannejadi",
            "Maarten de Rijke",
            "Suzan Verberne"
        ],
        "published": "2024-03-27T23:45:31Z",
        "summary": "An important unexplored aspect in previous work on user satisfaction\nestimation for Task-Oriented Dialogue (TOD) systems is their evaluation in\nterms of robustness for the identification of user dissatisfaction: current\nbenchmarks for user satisfaction estimation in TOD systems are highly skewed\ntowards dialogues for which the user is satisfied. The effect of having a more\nbalanced set of satisfaction labels on performance is unknown. However,\nbalancing the data with more dissatisfactory dialogue samples requires further\ndata collection and human annotation, which is costly and time-consuming. In\nthis work, we leverage large language models (LLMs) and unlock their ability to\ngenerate satisfaction-aware counterfactual dialogues to augment the set of\noriginal dialogues of a test collection. We gather human annotations to ensure\nthe reliability of the generated samples. We evaluate two open-source LLMs as\nuser satisfaction estimators on our augmented collection against\nstate-of-the-art fine-tuned models. Our experiments show that when used as\nfew-shot user satisfaction estimators, open-source LLMs show higher robustness\nto the increase in the number of dissatisfaction labels in the test collection\nthan the fine-tuned state-of-the-art models. Our results shed light on the need\nfor data augmentation approaches for user satisfaction estimation in TOD\nsystems. We release our aligned counterfactual dialogues, which are curated by\nhuman annotation, to facilitate further research on this topic.",
        "pdf_link": "https://arxiv.org/pdf/2403.19056v1.pdf"
    },
    {
        "title": "LITA: Language Instructed Temporal-Localization Assistant",
        "authors": [
            "De-An Huang",
            "Shijia Liao",
            "Subhashree Radhakrishnan",
            "Hongxu Yin",
            "Pavlo Molchanov",
            "Zhiding Yu",
            "Jan Kautz"
        ],
        "published": "2024-03-27T22:50:48Z",
        "summary": "There has been tremendous progress in multimodal Large Language Models\n(LLMs). Recent works have extended these models to video input with promising\ninstruction following capabilities. However, an important missing piece is\ntemporal localization. These models cannot accurately answer the \"When?\"\nquestions. We identify three key aspects that limit their temporal localization\ncapabilities: (i) time representation, (ii) architecture, and (iii) data. We\naddress these shortcomings by proposing Language Instructed\nTemporal-Localization Assistant (LITA) with the following features: (1) We\nintroduce time tokens that encode timestamps relative to the video length to\nbetter represent time in videos. (2) We introduce SlowFast tokens in the\narchitecture to capture temporal information at fine temporal resolution. (3)\nWe emphasize temporal localization data for LITA. In addition to leveraging\nexisting video datasets with timestamps, we propose a new task, Reasoning\nTemporal Localization (RTL), along with the dataset, ActivityNet-RTL, for\nlearning and evaluating this task. Reasoning temporal localization requires\nboth the reasoning and temporal localization of Video LLMs. LITA demonstrates\nstrong performance on this challenging task, nearly doubling the temporal mean\nintersection-over-union (mIoU) of baselines. In addition, we show that our\nemphasis on temporal localization also substantially improves video-based text\ngeneration compared to existing Video LLMs, including a 36% relative\nimprovement of Temporal Understanding. Code is available at:\nhttps://github.com/NVlabs/LITA",
        "pdf_link": "https://arxiv.org/pdf/2403.19046v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data",
        "authors": [
            "Yuting Guo",
            "Anthony Ovadje",
            "Mohammed Ali Al-Garadi",
            "Abeed Sarker"
        ],
        "published": "2024-03-27T22:05:10Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable success in NLP\ntasks. However, there is a paucity of studies that attempt to evaluate their\nperformances on social media-based health-related natural language processing\ntasks, which have traditionally been difficult to achieve high scores in. We\nbenchmarked one supervised classic machine learning model based on Support\nVector Machines (SVMs), three supervised pretrained language models (PLMs)\nbased on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5\nand GPT4), across 6 text classification tasks. We developed three approaches\nfor leveraging LLMs for text classification: employing LLMs as zero-shot\nclassifiers, us-ing LLMs as annotators to annotate training data for supervised\nclassifiers, and utilizing LLMs with few-shot examples for augmentation of\nmanually annotated data. Our comprehensive experiments demonstrate that\nemploy-ing data augmentation using LLMs (GPT-4) with relatively small\nhuman-annotated data to train lightweight supervised classification models\nachieves superior results compared to training with human-annotated data alone.\nSupervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings. By\nleveraging this data augmentation strategy, we can harness the power of LLMs to\ndevelop smaller, more effective domain-specific NLP models. LLM-annotated data\nwithout human guidance for training light-weight supervised classification\nmodels is an ineffective strategy. However, LLM, as a zero-shot classifier,\nshows promise in excluding false negatives and potentially reducing the human\neffort required for data annotation. Future investigations are imperative to\nexplore optimal training data sizes and the optimal amounts of augmented data.",
        "pdf_link": "https://arxiv.org/pdf/2403.19031v1.pdf"
    },
    {
        "title": "Towards LLM-RecSys Alignment with Textual ID Learning",
        "authors": [
            "Juntao Tan",
            "Shuyuan Xu",
            "Wenyue Hua",
            "Yingqiang Ge",
            "Zelong Li",
            "Yongfeng Zhang"
        ],
        "published": "2024-03-27T21:22:37Z",
        "summary": "Generative recommendation based on Large Language Models (LLMs) have\ntransformed the traditional ranking-based recommendation style into a\ntext-to-text generation paradigm. However, in contrast to standard NLP tasks\nthat inherently operate on human vocabulary, current research in generative\nrecommendations struggles to effectively encode recommendation items within the\ntext-to-text framework using concise yet meaningful ID representations. To\nbetter align LLMs with recommendation needs, we propose IDGen, representing\neach item as a unique, concise, semantically rich, platform-agnostic textual ID\nusing human language tokens. This is achieved by training a textual ID\ngenerator alongside the LLM-based recommender, enabling seamless integration of\npersonalized recommendations into natural language generation. Notably, as user\nhistory is expressed in natural language and decoupled from the original\ndataset, our approach suggests the potential for a foundational generative\nrecommendation model. Experiments show that our framework consistently\nsurpasses existing models in sequential recommendation under standard\nexperimental setting. Then, we explore the possibility of training a foundation\nrecommendation model with the proposed method on data collected from 19\ndifferent datasets and tested its recommendation performance on 6 unseen\ndatasets across different platforms under a completely zero-shot setting. The\nresults show that the zero-shot performance of the pre-trained foundation model\nis comparable to or even better than some traditional recommendation models\nbased on supervised training, showing the potential of the IDGen paradigm\nserving as the foundation model for generative recommendation. Code and data\nare open-sourced at https://github.com/agiresearch/IDGenRec.",
        "pdf_link": "https://arxiv.org/pdf/2403.19021v1.pdf"
    },
    {
        "title": "TextCraftor: Your Text Encoder Can be Image Quality Controller",
        "authors": [
            "Yanyu Li",
            "Xian Liu",
            "Anil Kag",
            "Ju Hu",
            "Yerlan Idelbayev",
            "Dhritiman Sagar",
            "Yanzhi Wang",
            "Sergey Tulyakov",
            "Jian Ren"
        ],
        "published": "2024-03-27T19:52:55Z",
        "summary": "Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have\nrevolutionized the field of content generation, enabling significant\nadvancements in areas like image editing and video synthesis. Despite their\nformidable capabilities, these models are not without their limitations. It is\nstill challenging to synthesize an image that aligns well with the input text,\nand multiple runs with carefully crafted prompts are required to achieve\nsatisfactory results. To mitigate these limitations, numerous studies have\nendeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing\nvarious technologies. Yet, amidst these efforts, a pivotal question of\ntext-to-image diffusion model training has remained largely unexplored: Is it\npossible and feasible to fine-tune the text encoder to improve the performance\nof text-to-image diffusion models? Our findings reveal that, instead of\nreplacing the CLIP text encoder used in Stable Diffusion with other large\nlanguage models, we can enhance it through our proposed fine-tuning approach,\nTextCraftor, leading to substantial improvements in quantitative benchmarks and\nhuman assessments. Interestingly, our technique also empowers controllable\nimage generation through the interpolation of different text encoders\nfine-tuned with various rewards. We also demonstrate that TextCraftor is\northogonal to UNet finetuning, and can be combined to further improve\ngenerative quality.",
        "pdf_link": "https://arxiv.org/pdf/2403.18978v1.pdf"
    },
    {
        "title": "\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",
        "authors": [
            "Vipula Rawte",
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Prachi Priya",
            "Aman Chadha",
            "Amit P. Sheth",
            "Amitava Das"
        ],
        "published": "2024-03-27T19:45:09Z",
        "summary": "Hallucination has emerged as the most vulnerable aspect of contemporary Large\nLanguage Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA)\nprompting, aimed to avoid LLM hallucinations by enhancing comprehension\nthrough: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay\nLLM generation. First, we provide an in-depth analysis of linguistic nuances:\nformality, readability, and concreteness of prompts for 21 LLMs, and elucidate\nhow these nuances contribute to hallucinated generation. Prompts with lower\nreadability, formality, or concreteness pose comprehension challenges for LLMs,\nsimilar to those faced by humans. In such scenarios, an LLM tends to speculate\nand generate content based on its imagination (associative memory) to fill\nthese information gaps. Although these speculations may occasionally align with\nfactual information, their accuracy is not assured, often resulting in\nhallucination. Recent studies reveal that an LLM often neglects the middle\nsections of extended prompts, a phenomenon termed as lost in the middle. While\na specific paraphrase may suit one LLM, the same paraphrased version may elicit\na different response from another LLM. Therefore, we propose an optimal\nparaphrasing technique to identify the most comprehensible paraphrase of a\ngiven prompt, evaluated using Integrated Gradient (and its variations) to\nguarantee that the LLM accurately processes all words. While reading lengthy\nsentences, humans often pause at various points to better comprehend the\nmeaning read thus far. We have fine-tuned an LLM with injected [PAUSE] tokens,\nallowing the LLM to pause while reading lengthier prompts. This has brought\nseveral key contributions: (i) determining the optimal position to inject\n[PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and\n(iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE]\ninsertion.",
        "pdf_link": "https://arxiv.org/pdf/2403.18976v1.pdf"
    },
    {
        "title": "A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products",
        "authors": [
            "Harsh Patel",
            "Dominique Boucher",
            "Emad Fallahzadeh",
            "Ahmed E. Hassan",
            "Bram Adams"
        ],
        "published": "2024-03-27T19:02:56Z",
        "summary": "This paper investigates the complexities of integrating Large Language Models\n(LLMs) into software products, with a focus on the challenges encountered for\ndetermining their readiness for release. Our systematic review of grey\nliterature identifies common challenges in deploying LLMs, ranging from\npre-training and fine-tuning to user experience considerations. The study\nintroduces a comprehensive checklist designed to guide practitioners in\nevaluating key release readiness aspects such as performance, monitoring, and\ndeployment strategies, aiming to enhance the reliability and effectiveness of\nLLM-based applications in real-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.18958v1.pdf"
    },
    {
        "title": "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said",
        "authors": [
            "Yejin Bang",
            "Delong Chen",
            "Nayeon Lee",
            "Pascale Fung"
        ],
        "published": "2024-03-27T18:22:48Z",
        "summary": "We propose to measure political bias in LLMs by analyzing both the content\nand style of their generated content regarding political issues. Existing\nbenchmarks and measures focus on gender and racial biases. However, political\nbias exists in LLMs and can lead to polarization and other harms in downstream\napplications. In order to provide transparency to users, we advocate that there\nshould be fine-grained and explainable measures of political biases generated\nby LLMs. Our proposed measure looks at different political issues such as\nreproductive rights and climate change, at both the content (the substance of\nthe generation) and the style (the lexical polarity) of such bias. We measured\nthe political bias in eleven open-sourced LLMs and showed that our proposed\nframework is easily scalable to other topics and is explainable.",
        "pdf_link": "https://arxiv.org/pdf/2403.18932v1.pdf"
    },
    {
        "title": "Duolando: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment",
        "authors": [
            "Li Siyao",
            "Tianpei Gu",
            "Zhitao Yang",
            "Zhengyu Lin",
            "Ziwei Liu",
            "Henghui Ding",
            "Lei Yang",
            "Chen Change Loy"
        ],
        "published": "2024-03-27T17:57:02Z",
        "summary": "We introduce a novel task within the field of 3D dance generation, termed\ndance accompaniment, which necessitates the generation of responsive movements\nfrom a dance partner, the \"follower\", synchronized with the lead dancer's\nmovements and the underlying musical rhythm. Unlike existing solo or group\ndance generation tasks, a duet dance scenario entails a heightened degree of\ninteraction between the two participants, requiring delicate coordination in\nboth pose and position. To support this task, we first build a large-scale and\ndiverse duet interactive dance dataset, DD100, by recording about 117 minutes\nof professional dancers' performances. To address the challenges inherent in\nthis task, we propose a GPT-based model, Duolando, which autoregressively\npredicts the subsequent tokenized motion conditioned on the coordinated\ninformation of the music, the leader's and the follower's movements. To further\nenhance the GPT's capabilities of generating stable results on unseen\nconditions (music and leader motions), we devise an off-policy reinforcement\nlearning strategy that allows the model to explore viable trajectories from\nout-of-distribution samplings, guided by human-defined rewards. Based on the\ncollected dataset and proposed method, we establish a benchmark with several\ncarefully designed metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.18811v1.pdf"
    },
    {
        "title": "Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation",
        "authors": [
            "Mateusz Klimaszewski",
            "Piotr Andruszkiewicz",
            "Alexandra Birch"
        ],
        "published": "2024-03-27T17:50:00Z",
        "summary": "The rise of Modular Deep Learning showcases its potential in various Natural\nLanguage Processing applications. Parameter-efficient fine-tuning (PEFT)\nmodularity has been shown to work for various use cases, from domain adaptation\nto multilingual setups. However, all this work covers the case where the\nmodular components are trained and deployed within one single Pre-trained\nLanguage Model (PLM). This model-specific setup is a substantial limitation on\nthe very modularity that modular architectures are trying to achieve. We ask\nwhether current modular approaches are transferable between models and whether\nwe can transfer the modules from more robust and larger PLMs to smaller ones.\nIn this work, we aim to fill this gap via a lens of Knowledge Distillation,\ncommonly used for model compression, and present an extremely straightforward\napproach to transferring pre-trained, task-specific PEFT modules between\nsame-family PLMs. Moreover, we propose a method that allows the transfer of\nmodules between incompatible PLMs without any change in the inference\ncomplexity. The experiments on Named Entity Recognition, Natural Language\nInference, and Paraphrase Identification tasks over multiple languages and PEFT\nmethods showcase the initial potential of transferable modularity.",
        "pdf_link": "https://arxiv.org/pdf/2403.18804v1.pdf"
    },
    {
        "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
        "authors": [
            "Hillary Dawkins",
            "Isar Nejadgholi",
            "Daniel Gillis",
            "Judi McCuaig"
        ],
        "published": "2024-03-27T17:49:31Z",
        "summary": "Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.",
        "pdf_link": "https://arxiv.org/pdf/2403.18803v1.pdf"
    },
    {
        "title": "Long-form factuality in large language models",
        "authors": [
            "Jerry Wei",
            "Chengrun Yang",
            "Xinying Song",
            "Yifeng Lu",
            "Nathan Hu",
            "Jie Huang",
            "Dustin Tran",
            "Daiyi Peng",
            "Ruibo Liu",
            "Da Huang",
            "Cosmo Du",
            "Quoc V. Le"
        ],
        "published": "2024-03-27T17:48:55Z",
        "summary": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.",
        "pdf_link": "https://arxiv.org/pdf/2403.18802v3.pdf"
    },
    {
        "title": "CheckEval: Robust Evaluation Framework using Large Language Model via Checklist",
        "authors": [
            "Yukyung Lee",
            "Joonghoon Kim",
            "Jaehee Kim",
            "Hyowon Cho",
            "Pilsung Kang"
        ],
        "published": "2024-03-27T17:20:39Z",
        "summary": "We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2403.18771v1.pdf"
    },
    {
        "title": "Understanding the Learning Dynamics of Alignment with Human Feedback",
        "authors": [
            "Shawn Im",
            "Yixuan Li"
        ],
        "published": "2024-03-27T16:39:28Z",
        "summary": "Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.",
        "pdf_link": "https://arxiv.org/pdf/2403.18742v3.pdf"
    },
    {
        "title": "The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian",
        "authors": [
            "Andrea Esuli",
            "Giovanni Puccetti"
        ],
        "published": "2024-03-27T15:46:25Z",
        "summary": "While Italian is by all metrics a high resource language, currently, there\nare isn't a Language Model pre-trained exclusively in this language. This\nresults in a lower number of available benchmarks to evaluate the performance\nof language models in Italian.\n  This work presents two new benchmarks to evaluate the models performance on\nmathematical understanding and language understanding in Italian. These\nbenchmarks are based on real tests that are undertaken by students of age\nbetween 11 and 18 within the Italian school system and have therefore been\nvalidated by several experts in didactics and pedagogy.\n  To validate this dataset we evaluate the performance of 9 language models\nthat are the best performing when writing in Italian, including our own\nfine-tuned models. We show that this is a challenging benchmark where current\nlanguage models are bound by 60\\% accuracy.\n  We believe that the release of this dataset paves the way for improving\nfuture models mathematical and language understanding in Italian.",
        "pdf_link": "https://arxiv.org/pdf/2403.18697v1.pdf"
    },
    {
        "title": "NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method",
        "authors": [
            "Jakub Hoscilowicz",
            "Adam Wiacek",
            "Jan Chojnacki",
            "Adam Cieslak",
            "Leszek Michon",
            "Vitalii Urbanevych",
            "Artur Janicki"
        ],
        "published": "2024-03-27T15:22:16Z",
        "summary": "Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).",
        "pdf_link": "https://arxiv.org/pdf/2403.18680v1.pdf"
    },
    {
        "title": "SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens",
        "authors": [
            "Chengbo Liu",
            "Yong Zhu"
        ],
        "published": "2024-03-27T14:54:27Z",
        "summary": "We propose an acceleration scheme for large language models (LLMs) through\nSpeculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary\nobjective of this design is to enhance the LLM model's ability to generate\ndraft tokens more accurately without compromising the model's accuracy. The\ncore strategies involve: 1) Fine-tune the model by incorporating semantic\nadaptive tokens that possess flexible decoding capabilities without changing\nits structure, allowing them to generate high-quality draft tokens. 2) By\nemploying a training method that does not affect the standard tokens, the model\ncan acquire parallel decoding abilities atop its original framework with\nminimal training overhead. 3) We have designed the \"two-step-draft-then-verify\"\ngeneration strategies using both greedy search and nucleus sampling.\nExperiments conducted on the CodeLlama-13B and 7B models have yielded speed\nincreases of over 3.5X and 3.0X, respectively. Please refer to\nhttps://github.com/hasuoshenyun/SDSAT.",
        "pdf_link": "https://arxiv.org/pdf/2403.18647v2.pdf"
    },
    {
        "title": "Vulnerability Detection with Code Language Models: How Far Are We?",
        "authors": [
            "Yangruibo Ding",
            "Yanjun Fu",
            "Omniyyah Ibrahim",
            "Chawin Sitawarin",
            "Xinyun Chen",
            "Basel Alomair",
            "David Wagner",
            "Baishakhi Ray",
            "Yizheng Chen"
        ],
        "published": "2024-03-27T14:34:29Z",
        "summary": "In the context of the rising interest in code language models (code LMs) and\nvulnerability detection, we study the effectiveness of code LMs for detecting\nvulnerabilities. Our analysis reveals significant shortcomings in existing\nvulnerability datasets, including poor data quality, low label accuracy, and\nhigh duplication rates, leading to unreliable model performance in realistic\nvulnerability detection scenarios. Additionally, the evaluation methods used\nwith these datasets are not representative of real-world vulnerability\ndetection.\n  To address these challenges, we introduce PrimeVul, a new dataset for\ntraining and evaluating code LMs for vulnerability detection. PrimeVul\nincorporates a novel set of data labeling techniques that achieve comparable\nlabel accuracy to human-verified benchmarks while significantly expanding the\ndataset. It also implements a rigorous data de-duplication and chronological\ndata splitting strategy to mitigate data leakage issues, alongside introducing\nmore realistic evaluation metrics and settings. This comprehensive approach\naims to provide a more accurate assessment of code LMs' performance in\nreal-world conditions.\n  Evaluating code LMs on PrimeVul reveals that existing benchmarks\nsignificantly overestimate the performance of these models. For instance, a\nstate-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on\nPrimeVul. Attempts to improve performance through advanced training techniques\nand larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin\nto random guessing in the most stringent settings. These findings underscore\nthe considerable gap between current capabilities and the practical\nrequirements for deploying code LMs in security roles, highlighting the need\nfor more innovative research in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.18624v1.pdf"
    },
    {
        "title": "A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks",
        "authors": [
            "Axel Constant",
            "Hannes Westermann",
            "Bryan Wilson",
            "Alex Kiefer",
            "Ines Hipolito",
            "Sylvain Pronovost",
            "Steven Swanson",
            "Mahault Albarracin",
            "Maxwell J. D. Ramstead"
        ],
        "published": "2024-03-27T13:12:57Z",
        "summary": "Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.",
        "pdf_link": "https://arxiv.org/pdf/2403.18537v1.pdf"
    },
    {
        "title": "SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks",
        "authors": [
            "Brian Formento",
            "Wenjie Feng",
            "Chuan Sheng Foo",
            "Luu Anh Tuan",
            "See-Kiong Ng"
        ],
        "published": "2024-03-27T10:24:25Z",
        "summary": "Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.",
        "pdf_link": "https://arxiv.org/pdf/2403.18423v1.pdf"
    },
    {
        "title": "BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text",
        "authors": [
            "Elliot Bolton",
            "Abhinav Venigalla",
            "Michihiro Yasunaga",
            "David Hall",
            "Betty Xiong",
            "Tony Lee",
            "Roxana Daneshjou",
            "Jonathan Frankle",
            "Percy Liang",
            "Michael Carbin",
            "Christopher D. Manning"
        ],
        "published": "2024-03-27T10:18:21Z",
        "summary": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.18421v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
        "authors": [
            "Shengjie Ma",
            "Chong Chen",
            "Qi Chu",
            "Jiaxin Mao"
        ],
        "published": "2024-03-27T09:46:56Z",
        "summary": "Collecting relevant judgments for legal case retrieval is a challenging and\ntime-consuming task. Accurately judging the relevance between two legal cases\nrequires a considerable effort to read the lengthy text and a high level of\ndomain expertise to extract Legal Facts and make juridical judgments. With the\nadvent of advanced large language models, some recent studies have suggested\nthat it is promising to use LLMs for relevance judgment. Nonetheless, the\nmethod of employing a general large language model for reliable relevance\njudgments in legal case retrieval is yet to be thoroughly explored. To fill\nthis research gap, we devise a novel few-shot workflow tailored to the relevant\njudgment of legal cases. The proposed workflow breaks down the annotation\nprocess into a series of stages, imitating the process employed by human\nannotators and enabling a flexible integration of expert reasoning to enhance\nthe accuracy of relevance judgments. By comparing the relevance judgments of\nLLMs and human experts, we empirically show that we can obtain reliable\nrelevance judgments with the proposed workflow. Furthermore, we demonstrate the\ncapacity to augment existing legal case retrieval models through the synthesis\nof data generated by the large language model.",
        "pdf_link": "https://arxiv.org/pdf/2403.18405v1.pdf"
    },
    {
        "title": "Improving Attributed Text Generation of Large Language Models via Preference Learning",
        "authors": [
            "Dongfang Li",
            "Zetian Sun",
            "Baotian Hu",
            "Zhenyu Liu",
            "Xinshuo Hu",
            "Xuebo Liu",
            "Min Zhang"
        ],
        "published": "2024-03-27T09:19:13Z",
        "summary": "Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.",
        "pdf_link": "https://arxiv.org/pdf/2403.18381v1.pdf"
    },
    {
        "title": "BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models",
        "authors": [
            "Haitao Li",
            "Qingyao Ai",
            "Jia Chen",
            "Qian Dong",
            "Zhijing Wu",
            "Yiqun Liu",
            "Chong Chen",
            "Qi Tian"
        ],
        "published": "2024-03-27T08:57:21Z",
        "summary": "Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.18365v1.pdf"
    },
    {
        "title": "Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback",
        "authors": [
            "Hongshen Xu",
            "Zichen Zhu",
            "Situo Zhang",
            "Da Ma",
            "Shuai Fan",
            "Lu Chen",
            "Kai Yu"
        ],
        "published": "2024-03-27T08:39:56Z",
        "summary": "Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.",
        "pdf_link": "https://arxiv.org/pdf/2403.18349v2.pdf"
    },
    {
        "title": "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective",
        "authors": [
            "Meiqi Chen",
            "Yixin Cao",
            "Yan Zhang",
            "Chaochao Lu"
        ],
        "published": "2024-03-27T08:38:49Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from an over-reliance on unimodal biases (e.g., language\nbias and vision bias), leading to incorrect answers in complex multimodal\ntasks. To investigate this issue, we propose a causal framework to interpret\nthe biases in Visual Question Answering (VQA) problems. Within our framework,\nwe devise a causal graph to elucidate the predictions of MLLMs on VQA problems,\nand assess the causal effect of biases through an in-depth causal analysis.\nMotivated by the causal graph, we introduce a novel MORE dataset, consisting of\n12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,\nnecessitating multi-hop reasoning and the surmounting of unimodal biases.\nFurthermore, we propose two strategies to mitigate unimodal biases and enhance\nMLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)\nframework for limited-access MLLMs and the refinement of open-source MLLMs\nthrough fine-tuning. Extensive quantitative and qualitative experiments offer\nvaluable insights for future research. Our project page is at\nhttps://opencausalab.github.io/MORE.",
        "pdf_link": "https://arxiv.org/pdf/2403.18346v3.pdf"
    },
    {
        "title": "LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models",
        "authors": [
            "Mingxing Peng",
            "Xusen Guo",
            "Xianda Chen",
            "Meixin Zhu",
            "Kehua Chen",
            "Hao",
            "Yang",
            "Xuesong Wang",
            "Yinhai Wang"
        ],
        "published": "2024-03-27T08:34:55Z",
        "summary": "To ensure safe driving in dynamic environments, autonomous vehicles should\npossess the capability to accurately predict the lane change intentions of\nsurrounding vehicles in advance and forecast their future trajectories.\nExisting motion prediction approaches have ample room for improvement,\nparticularly in terms of long-term prediction accuracy and interpretability. In\nthis paper, we address these challenges by proposing LC-LLM, an explainable\nlane change prediction model that leverages the strong reasoning capabilities\nand self-explanation abilities of Large Language Models (LLMs). Essentially, we\nreformulate the lane change prediction task as a language modeling problem,\nprocessing heterogeneous driving scenario information in natural language as\nprompts for input into the LLM and employing a supervised fine-tuning technique\nto tailor the LLM specifically for our lane change prediction task. This allows\nus to utilize the LLM's powerful common sense reasoning abilities to understand\ncomplex interactive information, thereby improving the accuracy of long-term\npredictions. Furthermore, we incorporate explanatory requirements into the\nprompts in the inference stage. Therefore, our LC-LLM model not only can\npredict lane change intentions and trajectories but also provides explanations\nfor its predictions, enhancing the interpretability. Extensive experiments on\nthe large-scale highD dataset demonstrate the superior performance and\ninterpretability of our LC-LLM in lane change prediction task. To the best of\nour knowledge, this is the first attempt to utilize LLMs for predicting lane\nchange behavior. Our study shows that LLMs can encode comprehensive interaction\ninformation for driving behavior understanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.18344v1.pdf"
    },
    {
        "title": "IterAlign: Iterative Constitutional Alignment of Large Language Models",
        "authors": [
            "Xiusi Chen",
            "Hongzhi Wen",
            "Sreyashi Nag",
            "Chen Luo",
            "Qingyu Yin",
            "Ruirui Li",
            "Zheng Li",
            "Wei Wang"
        ],
        "published": "2024-03-27T08:32:19Z",
        "summary": "With the rapid development of large language models (LLMs), aligning LLMs\nwith human values and societal norms to ensure their reliability and safety has\nbecome crucial. Reinforcement learning with human feedback (RLHF) and\nConstitutional AI (CAI) have been proposed for LLM alignment. However, these\nmethods require either heavy human annotations or explicitly pre-defined\nconstitutions, which are labor-intensive and resource-consuming. To overcome\nthese drawbacks, we study constitution-based LLM alignment and propose a\ndata-driven constitution discovery and self-alignment framework called\nIterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM\nand automatically discovers new constitutions using a stronger LLM. These\nconstitutions are then used to guide self-correction of the base LLM. Such a\nconstitution discovery pipeline can be run iteratively and automatically to\ndiscover new constitutions that specifically target the alignment gaps in the\ncurrent LLM. Empirical results on several safety benchmark datasets and\nmultiple base LLMs show that IterAlign successfully improves truthfulness,\nhelpfulness, harmlessness and honesty, improving the LLM alignment by up to\n$13.5\\%$ in harmlessness.",
        "pdf_link": "https://arxiv.org/pdf/2403.18341v1.pdf"
    },
    {
        "title": "Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications",
        "authors": [
            "Rushang Karia",
            "Daksh Dobhal",
            "Daniel Bramblett",
            "Pulkit Verma",
            "Siddharth Srivastava"
        ],
        "published": "2024-03-27T08:08:00Z",
        "summary": "Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.18327v1.pdf"
    },
    {
        "title": "Dual Instruction Tuning with Large Language Models for Mathematical Reasoning",
        "authors": [
            "Yongwei Zhou",
            "Tiejun Zhao"
        ],
        "published": "2024-03-27T06:43:58Z",
        "summary": "Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.18295v1.pdf"
    },
    {
        "title": "Toward Interactive Regional Understanding in Vision-Large Language Models",
        "authors": [
            "Jungbeom Lee",
            "Sanghyuk Chun",
            "Sangdoo Yun"
        ],
        "published": "2024-03-27T05:22:06Z",
        "summary": "Recent Vision-Language Pre-training (VLP) models have demonstrated\nsignificant advancements. Nevertheless, these models heavily rely on image-text\npairs that capture only coarse and global information of an image, leading to a\nlimitation in their regional understanding ability. In this work, we introduce\n\\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,\nallowing them to understand user-indicated image regions. To achieve this, we\ndesign a simple yet innovative architecture, requiring no modifications to the\nmodel architecture or objective function. Additionally, we leverage a dataset\nthat contains a novel source of information, namely Localized Narratives, which\nhas been overlooked in previous VLP research. Our experiments demonstrate that\nour single generalist model not only achieves an interactive dialogue system\nbut also exhibits superior performance on various zero-shot region\nunderstanding tasks, without compromising its ability for global image\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.18260v1.pdf"
    },
    {
        "title": "Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models",
        "authors": [
            "Yiwu Zhong",
            "Zi-Yuan Hu",
            "Michael R. Lyu",
            "Liwei Wang"
        ],
        "published": "2024-03-27T04:49:23Z",
        "summary": "Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.",
        "pdf_link": "https://arxiv.org/pdf/2403.18252v1.pdf"
    },
    {
        "title": "Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges",
        "authors": [
            "Yanshen Sun",
            "Jianfeng He",
            "Limeng Cui",
            "Shuo Lei",
            "Chang-Tien Lu"
        ],
        "published": "2024-03-27T04:39:18Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have enabled the creation\nof fake news, particularly in complex fields like healthcare. Studies highlight\nthe gap in the deceptive power of LLM-generated fake news with and without\nhuman assistance, yet the potential of prompting techniques has not been fully\nexplored. Thus, this work aims to determine whether prompting strategies can\neffectively narrow this gap. Current LLM-based fake news attacks require human\nintervention for information gathering and often miss details and fail to\nmaintain context consistency. Therefore, to better understand threat tactics,\nwe propose a strong fake news attack method called conditional\nVariational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,\nVLPrompt eliminates the need for additional data collection while maintaining\ncontextual coherence and preserving the intricacies of the original text. To\npropel future research on detecting VLPrompt attacks, we created a new dataset\nnamed VLPrompt fake news (VLPFN) containing real and fake texts. Our\nexperiments, including various detection methods and novel human study metrics,\nwere conducted to assess their performance on our dataset, yielding numerous\nfindings.",
        "pdf_link": "https://arxiv.org/pdf/2403.18249v2.pdf"
    },
    {
        "title": "Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check",
        "authors": [
            "Linhao Ye",
            "Zhikai Lei",
            "Jianghao Yin",
            "Qin Chen",
            "Jie Zhou",
            "Liang He"
        ],
        "published": "2024-03-27T04:20:18Z",
        "summary": "Retrieval-Augmented Generation (RAG) aims to generate more reliable and\naccurate responses, by augmenting large language models (LLMs) with the\nexternal vast and dynamic knowledge. Most previous work focuses on using RAG\nfor single-round question answering, while how to adapt RAG to the complex\nconversational setting wherein the question is interdependent on the preceding\ncontext is not well studied. In this paper, we propose a conversation-level RAG\napproach, which incorporates fine-grained retrieval augmentation and self-check\nfor conversational question answering (CQA). In particular, our approach\nconsists of three components, namely conversational question refiner,\nfine-grained retriever and self-check based response generator, which work\ncollaboratively for question understanding and relevant information acquisition\nin conversational settings. Extensive experiments demonstrate the great\nadvantages of our approach over the state-of-the-art baselines. Moreover, we\nalso release a Chinese CQA dataset with new features including reformulated\nquestion, extracted keyword, retrieved paragraphs and their helpfulness, which\nfacilitates further researches in RAG enhanced CQA.",
        "pdf_link": "https://arxiv.org/pdf/2403.18243v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Fuzzy String Matching in Political Science",
        "authors": [
            "Yu Wang"
        ],
        "published": "2024-03-27T03:04:21Z",
        "summary": "Fuzzy string matching remains a key issue when political scientists combine\ndata from different sources. Existing matching methods invariably rely on\nstring distances, such as Levenshtein distance and cosine similarity. As such,\nthey are inherently incapable of matching strings that refer to the same entity\nwith different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and\n''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In\nthis letter, we propose to use large language models to entirely sidestep this\nproblem in an easy and intuitive manner. Extensive experiments show that our\nproposed methods can improve the state of the art by as much as 39% in terms of\naverage precision while being substantially easier and more intuitive to use by\npolitical scientists. Moreover, our results are robust against various\ntemperatures. We further note that enhanced prompting can lead to additional\nperformance improvements.",
        "pdf_link": "https://arxiv.org/pdf/2403.18218v1.pdf"
    },
    {
        "title": "Exploring the Privacy Protection Capabilities of Chinese Large Language Models",
        "authors": [
            "Yuqi Yang",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "published": "2024-03-27T02:31:54Z",
        "summary": "Large language models (LLMs), renowned for their impressive capabilities in\nvarious tasks, have significantly advanced artificial intelligence. Yet, these\nadvancements have raised growing concerns about privacy and security\nimplications. To address these issues and explain the risks inherent in these\nmodels, we have devised a three-tiered progressive framework tailored for\nevaluating privacy in language systems. This framework consists of\nprogressively complex and in-depth privacy test tasks at each tier. Our primary\nobjective is to comprehensively evaluate the sensitivity of large language\nmodels to private information, examining how effectively they discern, manage,\nand safeguard sensitive data in diverse scenarios. This systematic evaluation\nhelps us understand the degree to which these models comply with privacy\nprotection guidelines and the effectiveness of their inherent safeguards\nagainst privacy breaches. Our observations indicate that existing Chinese large\nlanguage models universally show privacy protection shortcomings. It seems that\nat the moment this widespread issue is unavoidable and may pose corresponding\nprivacy risks in applications based on these models.",
        "pdf_link": "https://arxiv.org/pdf/2403.18205v1.pdf"
    },
    {
        "title": "Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models",
        "authors": [
            "Kartikeya Bhardwaj",
            "Nilesh Prasad Pandey",
            "Sweta Priyadarshi",
            "Kyunggeun Lee",
            "Jun Ma",
            "Harris Teague"
        ],
        "published": "2024-03-26T23:51:44Z",
        "summary": "Large generative models such as large language models (LLMs) and diffusion\nmodels have revolutionized the fields of NLP and computer vision respectively.\nHowever, their slow inference, high computation and memory requirement makes it\nchallenging to deploy them on edge devices. In this study, we propose a\nlight-weight quantization aware fine tuning technique using knowledge\ndistillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs\nusing commonly available datasets to realize a popular language use case, on\ndevice chat applications. To improve this paradigm of finetuning, as main\ncontributions, we provide insights into stability of KD-QAT by empirically\nstudying the gradient propagation during training to better understand the\nvulnerabilities of KD-QAT based approaches to low-bit quantization errors.\nBased on our insights, we propose ov-freeze, a simple technique to stabilize\nthe KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat\nmodel at 4-bit quantization level and demonstrate that ov-freeze results in\nnear floating point precision performance, i.e., less than 0.7% loss of\naccuracy on Commonsense Reasoning benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.18159v2.pdf"
    },
    {
        "title": "Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency",
        "authors": [
            "Toyin Aguda",
            "Suchetha Siddagangappa",
            "Elena Kochkina",
            "Simerjot Kaur",
            "Dongsheng Wang",
            "Charese Smiley",
            "Sameena Shah"
        ],
        "published": "2024-03-26T23:32:52Z",
        "summary": "Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.18152v1.pdf"
    },
    {
        "title": "Large Language Models Produce Responses Perceived to be Empathic",
        "authors": [
            "Yoon Kyung Lee",
            "Jina Suh",
            "Hongli Zhan",
            "Junyi Jessy Li",
            "Desmond C. Ong"
        ],
        "published": "2024-03-26T23:14:34Z",
        "summary": "Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.",
        "pdf_link": "https://arxiv.org/pdf/2403.18148v1.pdf"
    },
    {
        "title": "Juru: Legal Brazilian Large Language Model from Reputable Sources",
        "authors": [
            "Roseval Malaquias Junior",
            "Ramon Pires",
            "Roseli Romero",
            "Rodrigo Nogueira"
        ],
        "published": "2024-03-26T22:54:12Z",
        "summary": "The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.",
        "pdf_link": "https://arxiv.org/pdf/2403.18140v1.pdf"
    },
    {
        "title": "For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers",
        "authors": [
            "Evan Lucas",
            "Kelly S. Steelman",
            "Leo C. Ureel",
            "Charles Wallace"
        ],
        "published": "2024-03-26T22:08:33Z",
        "summary": "While the rise of large language models (LLMs) has created rich new\nopportunities to learn about digital technology, many on the margins of this\ntechnology struggle to gain and maintain competency due to lexical or\nconceptual barriers that prevent them from asking appropriate questions.\nAlthough there have been many efforts to understand factuality of LLM-created\ncontent and ability of LLMs to answer questions, it is not well understood how\nunclear or nonstandard language queries affect the model outputs. We propose\nthe creation of a dataset that captures questions of digital newcomers and\noutsiders, utilizing data we have compiled from a decade's worth of one-on-one\ntutoring. In this paper we lay out our planned efforts and some potential uses\nof this dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.18125v1.pdf"
    },
    {
        "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
        "authors": [
            "Jin Peng Zhou",
            "Charles Staats",
            "Wenda Li",
            "Christian Szegedy",
            "Kilian Q. Weinberger",
            "Yuhuai Wu"
        ],
        "published": "2024-03-26T22:01:13Z",
        "summary": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.",
        "pdf_link": "https://arxiv.org/pdf/2403.18120v1.pdf"
    },
    {
        "title": "Large Language Models for Education: A Survey and Outlook",
        "authors": [
            "Shen Wang",
            "Tianlong Xu",
            "Hang Li",
            "Chaoli Zhang",
            "Joleen Liang",
            "Jiliang Tang",
            "Philip S. Yu",
            "Qingsong Wen"
        ],
        "published": "2024-03-26T21:04:29Z",
        "summary": "The advent of Large Language Models (LLMs) has brought in a new era of\npossibilities in the realm of education. This survey paper summarizes the\nvarious technologies of LLMs in educational settings from multifaceted\nperspectives, encompassing student and teacher assistance, adaptive learning,\nand commercial tools. We systematically review the technological advancements\nin each perspective, organize related datasets and benchmarks, and identify the\nrisks and challenges associated with deploying LLMs in education. Furthermore,\nwe outline future research opportunities, highlighting the potential promising\ndirections. Our survey aims to provide a comprehensive technological picture\nfor educators, researchers, and policymakers to harness the power of LLMs to\nrevolutionize educational practices and foster a more effective personalized\nlearning environment.",
        "pdf_link": "https://arxiv.org/pdf/2403.18105v2.pdf"
    },
    {
        "title": "Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models",
        "authors": [
            "Hai-Long Nguyen",
            "Duc-Minh Nguyen",
            "Tan-Minh Nguyen",
            "Ha-Thanh Nguyen",
            "Thi-Hai-Yen Vuong",
            "Ken Satoh"
        ],
        "published": "2024-03-26T20:25:53Z",
        "summary": "Large language models with billions of parameters, such as GPT-3.5, GPT-4,\nand LLaMA, are increasingly prevalent. Numerous studies have explored effective\nprompting techniques to harness the power of these LLMs for various research\nproblems. Retrieval, specifically in the legal data domain, poses a challenging\ntask for the direct application of Prompting techniques due to the large number\nand substantial length of legal articles. This research focuses on maximizing\nthe potential of prompting by placing it as the final phase of the retrieval\nsystem, preceded by the support of two phases: BM25 Pre-ranking and BERT-based\nRe-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating\nprompting techniques on LLMs into the retrieval system significantly improves\nretrieval accuracy. However, error analysis reveals several existing issues in\nthe retrieval system that still need resolution.",
        "pdf_link": "https://arxiv.org/pdf/2403.18093v1.pdf"
    },
    {
        "title": "PerOS: Personalized Self-Adapting Operating Systems in the Cloud",
        "authors": [
            "Hongyu H\u00e8"
        ],
        "published": "2024-03-26T20:10:31Z",
        "summary": "Operating systems (OSes) are foundational to computer systems, managing\nhardware resources and ensuring secure environments for diverse applications.\nHowever, despite their enduring importance, the fundamental design objectives\nof OSes have seen minimal evolution over decades. Traditionally prioritizing\naspects like speed, memory efficiency, security, and scalability, these\nobjectives often overlook the crucial aspect of intelligence as well as\npersonalized user experience. The lack of intelligence becomes increasingly\ncritical amid technological revolutions, such as the remarkable advancements in\nmachine learning (ML).\n  Today's personal devices, evolving into intimate companions for users, pose\nunique challenges for traditional OSes like Linux and iOS, especially with the\nemergence of specialized hardware featuring heterogeneous components.\nFurthermore, the rise of large language models (LLMs) in ML has introduced\ntransformative capabilities, reshaping user interactions and software\ndevelopment paradigms.\n  While existing literature predominantly focuses on leveraging ML methods for\nsystem optimization or accelerating ML workloads, there is a significant gap in\naddressing personalized user experiences at the OS level. To tackle this\nchallenge, this work proposes PerOS, a personalized OS ingrained with LLM\ncapabilities. PerOS aims to provide tailored user experiences while\nsafeguarding privacy and personal data through declarative interfaces,\nself-adaptive kernels, and secure data management in a scalable cloud-centric\narchitecture; therein lies the main research question of this work: How can we\ndevelop intelligent, secure, and scalable OSes that deliver personalized\nexperiences to thousands of users?",
        "pdf_link": "https://arxiv.org/pdf/2404.00057v1.pdf"
    },
    {
        "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning",
        "authors": [
            "Yuelin Bai",
            "Xinrun Du",
            "Yiming Liang",
            "Yonggang Jin",
            "Ziqiang Liu",
            "Junting Zhou",
            "Tianyu Zheng",
            "Xincheng Zhang",
            "Nuo Ma",
            "Zekun Wang",
            "Ruibin Yuan",
            "Haihong Wu",
            "Hongquan Lin",
            "Wenhao Huang",
            "Jiajun Zhang",
            "Wenhu Chen",
            "Chenghua Lin",
            "Jie Fu",
            "Min Yang",
            "Shiwen Ni",
            "Ge Zhang"
        ],
        "published": "2024-03-26T19:24:18Z",
        "summary": "Recently, there have been significant advancements in large language models\n(LLMs), particularly focused on the English language. These advancements have\nenabled these LLMs to understand and execute complex instructions with\nunprecedented accuracy and fluency. However, despite these advancements, there\nremains a noticeable gap in the development of Chinese instruction tuning. The\nunique linguistic features and cultural depth of the Chinese language pose\nchallenges for instruction tuning tasks. Existing datasets are either derived\nfrom English-centric LLMs or are ill-suited for aligning with the interaction\npatterns of real-world Chinese users. To bridge this gap, we introduce\nCOIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to\nbuild a diverse, wide-ranging instruction-tuning dataset to better align model\nbehavior with human interactions. To this end, we collect a high-quality\nhuman-written corpus from various sources on the Chinese Internet, including\nQ&A communities, Wikis, examinations, and existing NLP datasets. This corpus\nwas rigorously filtered and carefully processed to form the COIG-CQIA dataset.\nFurthermore, we train models of various scales on different subsets of CQIA,\nfollowing in-depth evaluation and analyses. The findings from our experiments\noffer valuable insights for selecting and developing Chinese instruction-tuning\ndatasets. We also find that models trained on CQIA-Subset achieve competitive\nresults in human assessment as well as knowledge and security benchmarks. Data\nare available at https://huggingface.co/datasets/m-a-p/COIG-CQIA",
        "pdf_link": "https://arxiv.org/pdf/2403.18058v1.pdf"
    },
    {
        "title": "Supervisory Prompt Training",
        "authors": [
            "Jean Ghislain Billa",
            "Min Oh",
            "Liang Du"
        ],
        "published": "2024-03-26T19:08:20Z",
        "summary": "The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.18051v1.pdf"
    },
    {
        "title": "Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER",
        "authors": [
            "Micheal Abaho",
            "Danushka Bollegala",
            "Gary Leeming",
            "Dan Joyce",
            "Iain E Buchan"
        ],
        "published": "2024-03-26T18:23:16Z",
        "summary": "Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.",
        "pdf_link": "https://arxiv.org/pdf/2403.18025v2.pdf"
    },
    {
        "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
        "authors": [
            "Wei Tao",
            "Yucheng Zhou",
            "Wenqiang Zhang",
            "Yu Cheng"
        ],
        "published": "2024-03-26T17:57:57Z",
        "summary": "In software evolution, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing functionalities. Large Language\nModels (LLMs) have shown promise in code generation and understanding but face\ndifficulties in code change, particularly at the repository level. To overcome\nthese challenges, we empirically study the reason why LLMs mostly fail to\nresolve GitHub issues and analyze some impact factors. Motivated by the\nempirical findings, we propose a novel LLM-based Multi-Agent framework for\nGitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized\nfor the software evolution: Manager, Repository Custodian, Developer, and\nQuality Assurance Engineer agents. This framework leverages the collaboration\nof various agents in the planning and coding process to unlock the potential of\nLLMs to resolve GitHub issues. In experiments, we employ the SWE-bench\nbenchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and\nClaude-2. MAGIS can resolve 13.94% GitHub issues, which significantly\noutperforms the baselines. Specifically, MAGIS achieves an eight-fold increase\nin resolved ratio over the direct application of GPT-4, the based LLM of our\nmethod. We also analyze the factors for improving GitHub issue resolution\nrates, such as line location, task allocation, etc.",
        "pdf_link": "https://arxiv.org/pdf/2403.17927v1.pdf"
    },
    {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "authors": [
            "Rui Pan",
            "Xiang Liu",
            "Shizhe Diao",
            "Renjie Pi",
            "Jipeng Zhang",
            "Chi Han",
            "Tong Zhang"
        ],
        "published": "2024-03-26T17:55:02Z",
        "summary": "The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.17919v2.pdf"
    },
    {
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "authors": [
            "Andrey Gromov",
            "Kushal Tirumala",
            "Hassan Shapourian",
            "Paolo Glorioso",
            "Daniel A. Roberts"
        ],
        "published": "2024-03-26T17:20:04Z",
        "summary": "We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2403.17887v1.pdf"
    },
    {
        "title": "Addressing Social Misattributions of Large Language Models: An HCXAI-based Approach",
        "authors": [
            "Andrea Ferrario",
            "Alberto Termine",
            "Alessandro Facchini"
        ],
        "published": "2024-03-26T17:02:42Z",
        "summary": "Human-centered explainable AI (HCXAI) advocates for the integration of social\naspects into AI explanations. Central to the HCXAI discourse is the Social\nTransparency (ST) framework, which aims to make the socio-organizational\ncontext of AI systems accessible to their users. In this work, we suggest\nextending the ST framework to address the risks of social misattributions in\nLarge Language Models (LLMs), particularly in sensitive areas like mental\nhealth. In fact LLMs, which are remarkably capable of simulating roles and\npersonas, may lead to mismatches between designers' intentions and users'\nperceptions of social attributes, risking to promote emotional manipulation and\ndangerous behaviors, cases of epistemic injustice, and unwarranted trust. To\naddress these issues, we propose enhancing the ST framework with a fifth\n'W-question' to clarify the specific social attributions assigned to LLMs by\nits designers and users. This addition aims to bridge the gap between LLM\ncapabilities and user perceptions, promoting the ethically responsible\ndevelopment and use of LLM-based technology.",
        "pdf_link": "https://arxiv.org/pdf/2403.17873v1.pdf"
    },
    {
        "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications",
        "authors": [
            "Philip Lippmann",
            "Matthijs T. J. Spaan",
            "Jie Yang"
        ],
        "published": "2024-03-26T16:49:25Z",
        "summary": "Natural Language Processing (NLP) models optimized for predictive performance\noften make high confidence errors and suffer from vulnerability to adversarial\nand out-of-distribution data. Existing work has mainly focused on mitigation of\nsuch errors using either humans or an automated approach. In this study, we\nexplore the usage of large language models (LLMs) for data augmentation as a\npotential solution to the issue of NLP models making wrong predictions with\nhigh confidence during classification tasks. We compare the effectiveness of\nsynthetic data generated by LLMs with that of human data obtained via the same\nprocedure. For mitigation, humans or LLMs provide natural language\ncharacterizations of high confidence misclassifications to generate synthetic\ndata, which are then used to extend the training set. We conduct an extensive\nevaluation of our approach on three classification tasks and demonstrate its\neffectiveness in reducing the number of high confidence misclassifications\npresent in the model, all while maintaining the same level of accuracy.\nMoreover, we find that the cost gap between humans and LLMs surpasses an order\nof magnitude, as LLMs attain human-like performance while being more scalable.",
        "pdf_link": "https://arxiv.org/pdf/2403.17860v2.pdf"
    },
    {
        "title": "ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on Historical American Newspaper Pages",
        "authors": [
            "Bhawna Piryani",
            "Jamshid Mozafari",
            "Adam Jatowt"
        ],
        "published": "2024-03-26T16:48:13Z",
        "summary": "Question answering (QA) and Machine Reading Comprehension (MRC) tasks have\nsignificantly advanced in recent years due to the rapid development of deep\nlearning techniques and, more recently, large language models. At the same\ntime, many benchmark datasets have become available for QA and MRC tasks.\nHowever, most existing large-scale benchmark datasets have been created\npredominantly using synchronous document collections like Wikipedia or the Web.\nArchival document collections, such as historical newspapers, contain valuable\ninformation from the past that is still not widely used to train large language\nmodels. To further contribute to advancing QA and MRC tasks and to overcome the\nlimitation of previous datasets, we introduce ChroniclingAmericaQA, a\nlarge-scale dataset with 485K question-answer pairs created based on the\nhistorical newspaper collection Chronicling America. Our dataset is constructed\nfrom a subset of the Chronicling America newspaper collection spanning 120\nyears. One of the significant challenges for utilizing digitized historical\nnewspaper collections is the low quality of OCR text. Therefore, to enable\nrealistic testing of QA models, our dataset can be used in three different\nways: answering questions from raw and noisy content, answering questions from\ncleaner, corrected version of the content, as well as answering questions from\nscanned images of newspaper pages. This and the fact that ChroniclingAmericaQA\nspans the longest time period among available QA datasets make it quite a\nunique and useful resource.",
        "pdf_link": "https://arxiv.org/pdf/2403.17859v1.pdf"
    },
    {
        "title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs",
        "authors": [
            "David R. Mortensen",
            "Valentina Izrailevitch",
            "Yunze Xiao",
            "Hinrich Sch\u00fctze",
            "Leonie Weissweiler"
        ],
        "published": "2024-03-26T16:45:27Z",
        "summary": "Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.17856v1.pdf"
    },
    {
        "title": "ArabicaQA: A Comprehensive Dataset for Arabic Question Answering",
        "authors": [
            "Abdelrahman Abdallah",
            "Mahmoud Kasem",
            "Mahmoud Abdalla",
            "Mohamed Mahmoud",
            "Mohamed Elkasaby",
            "Yasser Elbendary",
            "Adam Jatowt"
        ],
        "published": "2024-03-26T16:37:54Z",
        "summary": "In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.",
        "pdf_link": "https://arxiv.org/pdf/2403.17848v1.pdf"
    },
    {
        "title": "Assessment of Multimodal Large Language Models in Alignment with Human Values",
        "authors": [
            "Zhelun Shi",
            "Zhipin Wang",
            "Hongxing Fan",
            "Zaibin Zhang",
            "Lijun Li",
            "Yongting Zhang",
            "Zhenfei Yin",
            "Lu Sheng",
            "Yu Qiao",
            "Jing Shao"
        ],
        "published": "2024-03-26T16:10:21Z",
        "summary": "Large Language Models (LLMs) aim to serve as versatile assistants aligned\nwith human values, as defined by the principles of being helpful, honest, and\nharmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),\ndespite their commendable performance in perception and reasoning tasks, their\nalignment with human values remains largely unexplored, given the complexity of\ndefining hhh dimensions in the visual world and the difficulty in collecting\nrelevant data that accurately mirrors real-world situations. To address this\ngap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for\nassessing alignment with human expectations. Ch3Ef dataset contains 1002\nhuman-annotated data samples, covering 12 domains and 46 tasks based on the hhh\nprinciple. We also present a unified evaluation strategy supporting assessment\nacross various scenarios and different perspectives. Based on the evaluation\nresults, we summarize over 10 key findings that deepen the understanding of\nMLLM capabilities, limitations, and the dynamic relationships between\nevaluation levels, guiding future advancements in the field.",
        "pdf_link": "https://arxiv.org/pdf/2403.17830v1.pdf"
    },
    {
        "title": "Accelerating Radio Spectrum Regulation Workflows with Large Language Models (LLMs)",
        "authors": [
            "Amir Ghasemi",
            "Paul Guinand"
        ],
        "published": "2024-03-26T15:54:48Z",
        "summary": "Wireless spectrum regulation is a complex and demanding process due to the\nrapid pace of technological progress, increasing demand for spectrum, and a\nmultitude of stakeholders with potentially conflicting interests, alongside\nsignificant economic implications. To navigate this, regulators must engage\neffectively with all parties, keep pace with global technology trends, conduct\ntechnical evaluations, issue licenses in a timely manner, and comply with\nvarious legal and policy frameworks.\n  In light of these challenges, this paper demonstrates example applications of\nLarge Language Models (LLMs) to expedite spectrum regulatory processes. We\nexplore various roles that LLMs can play in this context while identifying some\nof the challenges to address. The paper also offers practical case studies and\ninsights, with appropriate experiments, highlighting the transformative\npotential of LLMs in spectrum management.",
        "pdf_link": "https://arxiv.org/pdf/2403.17819v1.pdf"
    },
    {
        "title": "Are Compressed Language Models Less Subgroup Robust?",
        "authors": [
            "Leonidas Gee",
            "Andrea Zugarini",
            "Novi Quadrianto"
        ],
        "published": "2024-03-26T15:50:37Z",
        "summary": "To reduce the inference cost of large language models, model compression is\nincreasingly used to create smaller scalable models. However, little is known\nabout their robustness to minority subgroups defined by the labels and\nattributes of a dataset. In this paper, we investigate the effects of 18\ndifferent compression methods and settings on the subgroup robustness of BERT\nlanguage models. We show that worst-group performance does not depend on model\nsize alone, but also on the compression method used. Additionally, we find that\nmodel compression does not always worsen the performance on minority subgroups.\nAltogether, our analysis serves to further research into the subgroup\nrobustness of model compression.",
        "pdf_link": "https://arxiv.org/pdf/2403.17811v1.pdf"
    },
    {
        "title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization",
        "authors": [
            "Oscar Ma\u00f1as",
            "Pietro Astolfi",
            "Melissa Hall",
            "Candace Ross",
            "Jack Urbanek",
            "Adina Williams",
            "Aishwarya Agrawal",
            "Adriana Romero-Soriano",
            "Michal Drozdzal"
        ],
        "published": "2024-03-26T15:42:01Z",
        "summary": "Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.17804v1.pdf"
    },
    {
        "title": "Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications",
        "authors": [
            "Fouad Trad",
            "Ali Chehab"
        ],
        "published": "2024-03-26T15:20:49Z",
        "summary": "The success of Large Language Models (LLMs) has led to a parallel rise in the\ndevelopment of Large Multimodal Models (LMMs), such as Gemini-pro, which have\nbegun to transform a variety of applications. These sophisticated multimodal\nmodels are designed to interpret and analyze complex data, integrating both\ntextual and visual information on a scale previously unattainable, opening new\navenues for a range of applications. This paper investigates the applicability\nand effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision\nTransformer (ViT) models in addressing critical security challenges. We focus\non two distinct tasks: a visually evident task of detecting simple triggers,\nsuch as small squares in images, indicative of potential backdoors, and a\nnon-visually evident task of malware classification through visual\nrepresentations. Our results highlight a significant divergence in performance,\nwith Gemini-pro falling short in accuracy and reliability when compared to\nfine-tuned ViT models. The ViT models, on the other hand, demonstrate\nexceptional accuracy, achieving near-perfect performance on both tasks. This\nstudy not only showcases the strengths and limitations of prompt-engineered\nLMMs in cybersecurity applications but also emphasizes the unmatched efficacy\nof fine-tuned ViT models for precise and dependable tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.17787v1.pdf"
    },
    {
        "title": "Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons",
        "authors": [
            "Shijia Zhou",
            "Leonie Weissweiler",
            "Taiqi He",
            "Hinrich Sch\u00fctze",
            "David R. Mortensen",
            "Lori Levin"
        ],
        "published": "2024-03-26T14:51:12Z",
        "summary": "In this paper, we make a contribution that can be understood from two\nperspectives: from an NLP perspective, we introduce a small challenge dataset\nfor NLI with large lexical overlap, which minimises the possibility of models\ndiscerning entailment solely based on token distinctions, and show that GPT-4\nand Llama 2 fail it with strong bias. We then create further challenging\nsub-tasks in an effort to explain this failure. From a Computational\nLinguistics perspective, we identify a group of constructions with three\nclasses of adjectives which cannot be distinguished by surface features. This\nenables us to probe for LLM's understanding of these constructions in various\nways, and we find that they fail in a variety of ways to distinguish between\nthem, suggesting that they don't adequately represent their meaning or capture\nthe lexical properties of phrasal heads.",
        "pdf_link": "https://arxiv.org/pdf/2403.17760v1.pdf"
    },
    {
        "title": "Can multiple-choice questions really be useful in detecting the abilities of LLMs?",
        "authors": [
            "Wangyue Li",
            "Liangzhi Li",
            "Tong Xiang",
            "Xiao Liu",
            "Wei Deng",
            "Noa Garcia"
        ],
        "published": "2024-03-26T14:43:48Z",
        "summary": "Multiple-choice questions (MCQs) are widely used in the evaluation of large\nlanguage models (LLMs) due to their simplicity and efficiency. However, there\nare concerns about whether MCQs can truly measure LLM's capabilities,\nparticularly in knowledge-intensive scenarios where long-form generation (LFG)\nanswers are required. The misalignment between the task and the evaluation\nmethod demands a thoughtful analysis of MCQ's efficacy, which we undertake in\nthis paper by evaluating nine LLMs on four question-answering (QA) datasets in\ntwo languages: Chinese and English. We identify a significant issue: LLMs\nexhibit an order sensitivity in bilingual MCQs, favoring answers located at\nspecific positions, i.e., the first position. We further quantify the gap\nbetween MCQs and long-form generation questions (LFGQs) by comparing their\ndirect outputs, token logits, and embeddings. Our results reveal a relatively\nlow correlation between answers from MCQs and LFGQs for identical questions.\nAdditionally, we propose two methods to quantify the consistency and confidence\nof LLMs' output, which can be generalized to other QA evaluation benchmarks.\nNotably, our analysis challenges the idea that the higher the consistency, the\ngreater the accuracy. We also find MCQs to be less reliable than LFGQs in terms\nof expected calibration error. Finally, the misalignment between MCQs and LFGQs\nis not only reflected in the evaluation performance but also in the embedding\nspace. Our code and models can be accessed at\nhttps://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.17752v2.pdf"
    },
    {
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
        "authors": [
            "Jiawen Shi",
            "Zenghui Yuan",
            "Yinuo Liu",
            "Yue Huang",
            "Pan Zhou",
            "Lichao Sun",
            "Neil Zhenqiang Gong"
        ],
        "published": "2024-03-26T13:58:00Z",
        "summary": "LLM-as-a-Judge is a novel solution that can assess textual information with\nlarge language models (LLMs). Based on existing research studies, LLMs\ndemonstrate remarkable performance in providing a compelling alternative to\ntraditional human assessment. However, the robustness of these systems against\nprompt injection attacks remains an open question. In this work, we introduce\nJudgeDeceiver, a novel optimization-based prompt injection attack tailored to\nLLM-as-a-Judge. Our method formulates a precise optimization objective for\nattacking the decision-making process of LLM-as-a-Judge and utilizes an\noptimization algorithm to efficiently automate the generation of adversarial\nsequences, achieving targeted and effective manipulation of model evaluations.\nCompared to handcraft prompt injection attacks, our method demonstrates\nsuperior efficacy, posing a significant challenge to the current security\nparadigms of LLM-based judgment systems. Through extensive experiments, we\nshowcase the capability of JudgeDeceiver in altering decision outcomes across\nvarious cases, highlighting the vulnerability of LLM-as-a-Judge systems to the\noptimization-based prompt injection attack.",
        "pdf_link": "https://arxiv.org/pdf/2403.17710v1.pdf"
    },
    {
        "title": "Enhanced Short Text Modeling: Leveraging Large Language Models for Topic Refinement",
        "authors": [
            "Shuyu Chang",
            "Rui Wang",
            "Peng Ren",
            "Haiping Huang"
        ],
        "published": "2024-03-26T13:50:34Z",
        "summary": "Crafting effective topic models for brief texts, like tweets and news\nheadlines, is essential for capturing the swift shifts in social dynamics.\nTraditional topic models, however, often fall short in accurately representing\nthe semantic intricacies of short texts due to their brevity and lack of\ncontextual data. In our study, we harness the advanced capabilities of Large\nLanguage Models (LLMs) to introduce a novel approach termed \"Topic Refinement\".\nThis approach does not directly involve itself in the initial modeling of\ntopics but focuses on improving topics after they have been mined. By employing\nprompt engineering, we direct LLMs to eliminate off-topic words within a given\ntopic, ensuring that only contextually relevant words are preserved or\nsubstituted with ones that fit better semantically. This method emulates\nhuman-like scrutiny and improvement of topics, thereby elevating the semantic\nquality of the topics generated by various models. Our comprehensive evaluation\nacross three unique datasets has shown that our topic refinement approach\nsignificantly enhances the semantic coherence of topics.",
        "pdf_link": "https://arxiv.org/pdf/2403.17706v1.pdf"
    },
    {
        "title": "ExpressEdit: Video Editing with Natural Language and Sketching",
        "authors": [
            "Bekzat Tilekbay",
            "Saelyne Yang",
            "Michal Lewkowicz",
            "Alex Suryapranata",
            "Juho Kim"
        ],
        "published": "2024-03-26T13:34:21Z",
        "summary": "Informational videos serve as a crucial source for explaining conceptual and\nprocedural knowledge to novices and experts alike. When producing informational\nvideos, editors edit videos by overlaying text/images or trimming footage to\nenhance the video quality and make it more engaging. However, video editing can\nbe difficult and time-consuming, especially for novice video editors who often\nstruggle with expressing and implementing their editing ideas. To address this\nchallenge, we first explored how multimodality$-$natural language (NL) and\nsketching, which are natural modalities humans use for expression$-$can be\nutilized to support video editors in expressing video editing ideas. We\ngathered 176 multimodal expressions of editing commands from 10 video editors,\nwhich revealed the patterns of use of NL and sketching in describing edit\nintents. Based on the findings, we present ExpressEdit, a system that enables\nediting videos via NL text and sketching on the video frame. Powered by LLM and\nvision models, the system interprets (1) temporal, (2) spatial, and (3)\noperational references in an NL command and spatial references from sketching.\nThe system implements the interpreted edits, which then the user can iterate\non. An observational study (N=10) showed that ExpressEdit enhanced the ability\nof novice video editors to express and implement their edit ideas. The system\nallowed participants to perform edits more efficiently and generate more ideas\nby generating edits based on user's multimodal edit commands and supporting\niterations on the editing commands. This work offers insights into the design\nof future multimodal interfaces and AI-based pipelines for video editing.",
        "pdf_link": "https://arxiv.org/pdf/2403.17693v1.pdf"
    },
    {
        "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
        "authors": [
            "Uri Hacohen",
            "Adi Haviv",
            "Shahar Sarfaty",
            "Bruria Friedman",
            "Niva Elkin-Koren",
            "Roi Livni",
            "Amit H Bermano"
        ],
        "published": "2024-03-26T13:32:32Z",
        "summary": "The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.",
        "pdf_link": "https://arxiv.org/pdf/2403.17691v1.pdf"
    },
    {
        "title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games",
        "authors": [
            "Yikuan Yan",
            "Yaolun Zhang",
            "Keman Huang"
        ],
        "published": "2024-03-26T13:02:46Z",
        "summary": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve\ncomplementary performance is critical in high stake tasks like cybersecurity\noperations. In this study, we introduce SecurityBot, a LLM agent mentored by\npre-trained RL agents, to support cybersecurity operations. In particularly,\nthe LLM agent is supported with a profile module to generated behavior\nguidelines, a memory module to accumulate local experiences, a reflection\nmodule to re-evaluate choices, and an action module to reduce action space.\nAdditionally, it adopts the collaboration mechanism to take suggestions from\npre-trained RL agents, including a cursor for dynamic suggestion taken, an\naggregator for multiple mentors' suggestions ranking and a caller for proactive\nsuggestion asking. Building on the CybORG experiment framework, our experiences\nshow that SecurityBot demonstrates significant performance improvement compared\nwith LLM or RL standalone, achieving the complementary performance in the\ncybersecurity games.",
        "pdf_link": "https://arxiv.org/pdf/2403.17674v1.pdf"
    },
    {
        "title": "Targeted Visualization of the Backbone of Encoder LLMs",
        "authors": [
            "Isaac Roberts",
            "Alexander Schulz",
            "Luca Hermes",
            "Barbara Hammer"
        ],
        "published": "2024-03-26T12:51:02Z",
        "summary": "Attention based Large Language Models (LLMs) are the state-of-the-art in\nnatural language processing (NLP). The two most common architectures are\nencoders such as BERT, and decoders like the GPT models. Despite the success of\nencoder models, on which we focus in this work, they also bear several risks,\nincluding issues with bias or their susceptibility for adversarial attacks,\nsignifying the necessity for explainable AI to detect such issues. While there\ndoes exist various local explainability methods focusing on the prediction of\nsingle inputs, global methods based on dimensionality reduction for\nclassification inspection, which have emerged in other domains and that go\nfurther than just using t-SNE in the embedding space, are not widely spread in\nNLP.\n  To reduce this gap, we investigate the application of DeepView, a method for\nvisualizing a part of the decision function together with a data set in two\ndimensions, to the NLP domain. While in previous work, DeepView has been used\nto inspect deep image classification models, we demonstrate how to apply it to\nBERT-based NLP classifiers and investigate its usability in this domain,\nincluding settings with adversarially perturbed input samples and pre-trained,\nfine-tuned, and multi-task models.",
        "pdf_link": "https://arxiv.org/pdf/2403.18872v1.pdf"
    },
    {
        "title": "Language Models for Text Classification: Is In-Context Learning Enough?",
        "authors": [
            "Aleksandra Edwards",
            "Jose Camacho-Collados"
        ],
        "published": "2024-03-26T12:47:39Z",
        "summary": "Recent foundational language models have shown state-of-the-art performance\nin many NLP tasks in zero- and few-shot settings. An advantage of these models\nover more standard approaches based on fine-tuning is the ability to understand\ninstructions written in natural language (prompts), which helps them generalise\nbetter to different tasks and domains without the need for specific training\ndata. This makes them suitable for addressing text classification problems for\ndomains with limited amounts of annotated instances. However, existing research\nis limited in scale and lacks understanding of how text generation models\ncombined with prompting techniques compare to more established methods for text\nclassification such as fine-tuning masked language models. In this paper, we\naddress this research gap by performing a large-scale evaluation study for 16\ntext classification datasets covering binary, multiclass, and multilabel\nproblems. In particular, we compare zero- and few-shot approaches of large\nlanguage models to fine-tuning smaller language models. We also analyse the\nresults by prompt, classification type, domain, and number of labels. In\ngeneral, the results show how fine-tuning smaller and more efficient language\nmodels can still outperform few-shot approaches of larger language models,\nwhich have room for improvement when it comes to text classification.",
        "pdf_link": "https://arxiv.org/pdf/2403.17661v1.pdf"
    },
    {
        "title": "\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling",
        "authors": [
            "Christopher Bagdon",
            "Prathamesh Karmalker",
            "Harsha Gurulingappa",
            "Roman Klinger"
        ],
        "published": "2024-03-26T11:45:22Z",
        "summary": "Labeling corpora constitutes a bottleneck to create models for new tasks or\ndomains. Large language models mitigate the issue with automatic corpus\nlabeling methods, particularly for categorical annotations. Some NLP tasks such\nas emotion intensity prediction, however, require text regression, but there is\nno work on automating annotations for continuous label assignments. Regression\nis considered more challenging than classification: The fact that humans\nperform worse when tasked to choose values from a rating scale lead to\ncomparative annotation methods, including best-worst scaling. This raises the\nquestion if large language model-based annotation methods show similar\npatterns, namely that they perform worse on rating scale annotation tasks than\non comparative annotation tasks. To study this, we automate emotion intensity\npredictions and compare direct rating scale predictions, pairwise comparisons\nand best-worst scaling. We find that the latter shows the highest reliability.\nA transformer regressor fine-tuned on these data performs nearly on par with a\nmodel trained on the original manual annotations.",
        "pdf_link": "https://arxiv.org/pdf/2403.17612v1.pdf"
    },
    {
        "title": "RuBia: A Russian Language Bias Detection Dataset",
        "authors": [
            "Veronika Grigoreva",
            "Anastasiia Ivanova",
            "Ilseyar Alimova",
            "Ekaterina Artemova"
        ],
        "published": "2024-03-26T10:01:01Z",
        "summary": "Warning: this work contains upsetting or disturbing content.\n  Large language models (LLMs) tend to learn the social and cultural biases\npresent in the raw pre-training data. To test if an LLM's behavior is fair,\nfunctional datasets are employed, and due to their purpose, these datasets are\nhighly language and culture-specific. In this paper, we address a gap in the\nscope of multilingual bias evaluation by presenting a bias detection dataset\nspecifically designed for the Russian language, dubbed as RuBia. The RuBia\ndataset is divided into 4 domains: gender, nationality, socio-economic status,\nand diverse, each of the domains is further divided into multiple fine-grained\nsubdomains. Every example in the dataset consists of two sentences with the\nfirst reinforcing a potentially harmful stereotype or trope and the second\ncontradicting it. These sentence pairs were first written by volunteers and\nthen validated by native-speaking crowdsourcing workers. Overall, there are\nnearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To\nillustrate the dataset's purpose, we conduct a diagnostic evaluation of\nstate-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'\npredisposition to social biases.",
        "pdf_link": "https://arxiv.org/pdf/2403.17553v1.pdf"
    },
    {
        "title": "Naive Bayes-based Context Extension for Large Language Models",
        "authors": [
            "Jianlin Su",
            "Murtadha Ahmed",
            "Wenbo",
            "Luo Ao",
            "Mingren Zhu",
            "Yunfeng Liu"
        ],
        "published": "2024-03-26T09:59:45Z",
        "summary": "Large Language Models (LLMs) have shown promising in-context learning\nabilities. However, conventional In-Context Learning (ICL) approaches are often\nimpeded by length limitations of transformer architecture, which pose\nchallenges when attempting to effectively integrate supervision from a\nsubstantial number of demonstration examples. In this paper, we introduce a\nnovel framework, called Naive Bayes-based Context Extension (NBCE), to enable\nexisting LLMs to perform ICL with an increased number of demonstrations by\nsignificantly expanding their context size. Importantly, this expansion does\nnot require fine-tuning or dependence on particular model architectures, all\nthe while preserving linear efficiency. NBCE initially splits the context into\nequal-sized windows fitting the target LLM's maximum length. Then, it\nintroduces a voting mechanism to select the most relevant window, regarded as\nthe posterior context. Finally, it employs Bayes' theorem to generate the test\ntask. Our experimental results demonstrate that NBCE substantially enhances\nperformance, particularly as the number of demonstration examples increases,\nconsistently outperforming alternative methods. The NBCE code will be made\npublicly accessible. The code NBCE is available at:\nhttps://github.com/amurtadha/NBCE-master",
        "pdf_link": "https://arxiv.org/pdf/2403.17552v1.pdf"
    },
    {
        "title": "Large Language Models Are State-of-the-Art Evaluator for Grammatical Error Correction",
        "authors": [
            "Masamune Kobayashi",
            "Masato Mita",
            "Mamoru Komachi"
        ],
        "published": "2024-03-26T09:43:15Z",
        "summary": "Large Language Models (LLMs) have been reported to outperform existing\nautomatic evaluation metrics in some tasks, such as text summarization and\nmachine translation. However, there has been a lack of research on LLMs as\nevaluators in grammatical error correction (GEC). In this study, we investigate\nthe performance of LLMs in GEC evaluation by employing prompts designed to\nincorporate various evaluation criteria inspired by previous research. Our\nextensive experimental results demonstrate that GPT-4 achieved Kendall's rank\ncorrelation of 0.662 with human judgments, surpassing all existing methods.\nFurthermore, in recent GEC evaluations, we have underscored the significance of\nthe LLMs scale and particularly emphasized the importance of fluency among\nevaluation criteria.",
        "pdf_link": "https://arxiv.org/pdf/2403.17540v1.pdf"
    },
    {
        "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler",
        "authors": [
            "Paramita Mirza",
            "Viju Sudhi",
            "Soumya Ranjan Sahoo",
            "Sinchana Ramakanth Bhat"
        ],
        "published": "2024-03-26T09:41:21Z",
        "summary": "State-of-the-art intent classification (IC) and slot filling (SF) methods\noften rely on data-intensive deep learning models, limiting their practicality\nfor industry applications. Large language models on the other hand,\nparticularly instruction-tuned models (Instruct-LLMs), exhibit remarkable\nzero-shot performance across various natural language tasks. This study\nevaluates Instruct-LLMs on popular benchmark datasets for IC and SF,\nemphasizing their capacity to learn from fewer examples. We introduce\nILLUMINER, an approach framing IC and SF as language generation tasks for\nInstruct-LLMs, with a more efficient SF-prompting method compared to prior\nwork. A comprehensive comparison with multiple baselines shows that our\napproach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint\nIC+SF method and in-context learning with GPT3.5 (175B), particularly in slot\nfilling by 11.1--32.2 percentage points. Additionally, our in-depth ablation\nstudy demonstrates that parameter-efficient fine-tuning requires less than 6%\nof training data to yield comparable performance with traditional full-weight\nfine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.17536v1.pdf"
    },
    {
        "title": "KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on Large Language Models for Knowledge Graph Completion",
        "authors": [
            "Yilin Wang",
            "Minghao Hu",
            "Zhen Huang",
            "Dongsheng Li",
            "Dong Yang",
            "Xicheng Lu"
        ],
        "published": "2024-03-26T09:36:59Z",
        "summary": "The goal of knowledge graph completion (KGC) is to predict missing facts\namong entities. Previous methods for KGC re-ranking are mostly built on\nnon-generative language models to obtain the probability of each candidate.\nRecently, generative large language models (LLMs) have shown outstanding\nperformance on several tasks such as information extraction and dialog systems.\nLeveraging them for KGC re-ranking is beneficial for leveraging the extensive\npre-trained knowledge and powerful generative capabilities. However, it may\nencounter new problems when accomplishing the task, namely mismatch,\nmisordering and omission. To this end, we introduce KC-GenRe, a\nknowledge-constrained generative re-ranking method based on LLMs for KGC. To\novercome the mismatch issue, we formulate the KGC re-ranking task as a\ncandidate identifier sorting generation problem implemented by generative LLMs.\nTo tackle the misordering issue, we develop a knowledge-guided interactive\ntraining method that enhances the identification and ranking of candidates. To\naddress the omission issue, we design a knowledge-augmented constrained\ninference method that enables contextual prompting and controlled generation,\nso as to obtain valid rankings. Experimental results show that KG-GenRe\nachieves state-of-the-art performance on four datasets, with gains of up to\n6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and\n9.0% and 11.1% compared to that without re-ranking. Extensive analysis\ndemonstrates the effectiveness of components in KG-GenRe.",
        "pdf_link": "https://arxiv.org/pdf/2403.17532v1.pdf"
    },
    {
        "title": "DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation",
        "authors": [
            "Xinyu Ning",
            "Yutong Zhao",
            "Yitong Liu",
            "Hongwen Yang"
        ],
        "published": "2024-03-26T08:47:23Z",
        "summary": "The method of training language models based on domain datasets has obtained\nsignificant achievements in the task of generating scientific paper abstracts.\nHowever, such models face problems of generalization and expensive training\ncosts. The use of large language models (LLMs) to solve the task of generating\npaper abstracts saves the cost of model training. However, due to the\nhallucination problem of LLM, it is often necessary to improve the reliability\nof the results through multi-round query prompt approach such as Graph of\nThoughts (GoT), which also brings additional reasoning costs. In this paper, we\npropose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages\nof the existing GoT prompt approach, but also dynamically adjust the graph\nstructure according to data characteristics while reducing model reasoning\ncost. Experimental results show that our method's cost-effectiveness in\nabstract generation tasks is only 43.7% to 56.4% of other multi-round query\nprompt approaches. Our code is available at https://github.com/JayceNing/DGoT.",
        "pdf_link": "https://arxiv.org/pdf/2403.17491v1.pdf"
    },
    {
        "title": "Robust and Scalable Model Editing for Large Language Models",
        "authors": [
            "Yingfa Chen",
            "Zhengyan Zhang",
            "Xu Han",
            "Chaojun Xiao",
            "Zhiyuan Liu",
            "Chen Chen",
            "Kuai Li",
            "Tao Yang",
            "Maosong Sun"
        ],
        "published": "2024-03-26T06:57:23Z",
        "summary": "Large language models (LLMs) can make predictions using parametric\nknowledge--knowledge encoded in the model weights--or contextual\nknowledge--knowledge presented in the context. In many scenarios, a desirable\nbehavior is that LLMs give precedence to contextual knowledge when it conflicts\nwith the parametric knowledge, and fall back to using their parametric\nknowledge when the context is irrelevant. This enables updating and correcting\nthe model's knowledge by in-context editing instead of retraining. Previous\nworks have shown that LLMs are inclined to ignore contextual knowledge and fail\nto reliably fall back to parametric knowledge when presented with irrelevant\ncontext. In this work, we discover that, with proper prompting methods,\ninstruction-finetuned LLMs can be highly controllable by contextual knowledge\nand robust to irrelevant context. Utilizing this feature, we propose EREN (Edit\nmodels by REading Notes) to improve the scalability and robustness of LLM\nediting. To better evaluate the robustness of model editors, we collect a new\ndataset, that contains irrelevant questions that are more challenging than the\nones in existing datasets. Empirical results show that our method outperforms\ncurrent state-of-the-art methods by a large margin. Unlike existing techniques,\nit can integrate knowledge from multiple edits, and correctly respond to\nsyntactically similar but semantically unrelated inputs (and vice versa). The\nsource code can be found at https://github.com/thunlp/EREN.",
        "pdf_link": "https://arxiv.org/pdf/2403.17431v1.pdf"
    },
    {
        "title": "LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error Correction",
        "authors": [
            "Yixuan Wang",
            "Baoxin Wang",
            "Yijun Liu",
            "Dayong Wu",
            "Wanxiang Che"
        ],
        "published": "2024-03-26T06:12:21Z",
        "summary": "Over-correction is a critical problem in Chinese grammatical error correction\n(CGEC) task. Recent work using model ensemble methods based on voting can\neffectively mitigate over-correction and improve the precision of the GEC\nsystem. However, these methods still require the output of several GEC systems\nand inevitably lead to reduced error recall. In this light, we propose the\nLM-Combiner, a rewriting model that can directly modify the over-correction of\nGEC system outputs without a model ensemble. Specifically, we train the model\non an over-correction dataset constructed through the proposed K-fold cross\ninference method, which allows it to directly generate filtered sentences by\ncombining the original and the over-corrected text. In the inference stage, we\ndirectly take the original sentences and the output results of other systems as\ninput and then obtain the filtered sentences through LM-Combiner. Experiments\non the FCGEC dataset show that our proposed method effectively alleviates the\nover-correction of the original system (+18.2 Precision) while ensuring the\nerror recall remains unchanged. Besides, we find that LM-Combiner still has a\ngood rewriting performance even with small parameters and few training data,\nand thus can cost-effectively mitigate the over-correction of black-box GEC\nsystems (e.g., ChatGPT).",
        "pdf_link": "https://arxiv.org/pdf/2403.17413v1.pdf"
    },
    {
        "title": "Disambiguate Entity Matching through Relation Discovery with Large Language Models",
        "authors": [
            "Zezhou Huang"
        ],
        "published": "2024-03-26T03:07:32Z",
        "summary": "Entity matching is a critical challenge in data integration and cleaning,\ncentral to tasks like fuzzy joins and deduplication. Traditional approaches\nhave focused on overcoming fuzzy term representations through methods such as\nedit distance, Jaccard similarity, and more recently, embeddings and deep\nneural networks, including advancements from large language models (LLMs) like\nGPT. However, the core challenge in entity matching extends beyond term\nfuzziness to the ambiguity in defining what constitutes a \"match,\" especially\nwhen integrating with external databases. This ambiguity arises due to varying\nlevels of detail and granularity among entities, complicating exact matches. We\npropose a novel approach that shifts focus from purely identifying semantic\nsimilarities to understanding and defining the \"relations\" between entities as\ncrucial for resolving ambiguities in matching. By predefining a set of\nrelations relevant to the task at hand, our method allows analysts to navigate\nthe spectrum of similarity more effectively, from exact matches to conceptually\nrelated entities.",
        "pdf_link": "https://arxiv.org/pdf/2403.17344v1.pdf"
    },
    {
        "title": "Residual-based Language Models are Free Boosters for Biomedical Imaging",
        "authors": [
            "Zhixin Lai",
            "Jing Wu",
            "Suiyao Chen",
            "Yucheng Zhou",
            "Naira Hovakimyan"
        ],
        "published": "2024-03-26T03:05:20Z",
        "summary": "In this study, we uncover the unexpected efficacy of residual-based large\nlanguage models (LLMs) as part of encoders for biomedical imaging tasks, a\ndomain traditionally devoid of language or textual data. The approach diverges\nfrom established methodologies by utilizing a frozen transformer block,\nextracted from pre-trained LLMs, as an innovative encoder layer for the direct\nprocessing of visual tokens. This strategy represents a significant departure\nfrom the standard multi-modal vision-language frameworks, which typically hinge\non language-driven prompts and inputs. We found that these LLMs could boost\nperformance across a spectrum of biomedical imaging applications, including\nboth 2D and 3D visual classification tasks, serving as plug-and-play boosters.\nMore interestingly, as a byproduct, we found that the proposed framework\nachieved superior performance, setting new state-of-the-art results on\nextensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we\naim to open new avenues for employing LLMs in biomedical imaging and enriching\nthe understanding of their potential in this specialized domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.17343v3.pdf"
    },
    {
        "title": "The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge",
        "authors": [
            "Dian Chao",
            "Xin Song",
            "Shupeng Zhong",
            "Boyuan Wang",
            "Xiangyu Wu",
            "Chen Zhu",
            "Yang Yang"
        ],
        "published": "2024-03-26T03:03:50Z",
        "summary": "In this paper, we propose a solution for improving the quality of captions\ngenerated for figures in papers. We adopt the approach of summarizing the\ntextual content in the paper to generate image captions. Throughout our study,\nwe encounter discrepancies in the OCR information provided in the official\ndataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR\ninformation from all images. Moreover, we observe that certain textual content\nin the official paper pertains to images that are not relevant for captioning,\nthereby introducing noise during caption generation. To mitigate this issue, we\nleverage LLaMA to extract image-specific information by querying the textual\ncontent based on image mentions, effectively filtering out extraneous\ninformation. Additionally, we recognize a discrepancy between the primary use\nof maximum likelihood estimation during text generation and the evaluation\nmetrics such as ROUGE employed to assess the quality of generated captions. To\nbridge this gap, we integrate the BRIO model framework, enabling a more\ncoherent alignment between the generation and evaluation processes. Our\napproach ranked first in the final test with a score of 4.49.",
        "pdf_link": "https://arxiv.org/pdf/2403.17342v1.pdf"
    },
    {
        "title": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models",
        "authors": [
            "Zhiyuan Yu",
            "Xiaogeng Liu",
            "Shunning Liang",
            "Zach Cameron",
            "Chaowei Xiao",
            "Ning Zhang"
        ],
        "published": "2024-03-26T02:47:42Z",
        "summary": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2403.17336v1.pdf"
    },
    {
        "title": "JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset",
        "authors": [
            "Atsumoto Ohashi",
            "Ryu Hirai",
            "Shinya Iizuka",
            "Ryuichiro Higashinaka"
        ],
        "published": "2024-03-26T02:01:18Z",
        "summary": "Dialogue datasets are crucial for deep learning-based task-oriented dialogue\nsystem research. While numerous English language multi-domain task-oriented\ndialogue datasets have been developed and contributed to significant\nadvancements in task-oriented dialogue systems, such a dataset does not exist\nin Japanese, and research in this area is limited compared to that in English.\nIn this study, towards the advancement of research and development of\ntask-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first\nJapanese language large-scale multi-domain task-oriented dialogue dataset.\nUsing JMultiWOZ, we evaluated the dialogue state tracking and response\ngeneration capabilities of the state-of-the-art methods on the existing major\nEnglish benchmark dataset MultiWOZ2.2 and the latest large language model\n(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ\nprovides a benchmark that is on par with MultiWOZ2.2. In addition, through\nevaluation experiments of interactive dialogues with the models and human\nparticipants, we identified limitations in the task completion capabilities of\nLLMs in Japanese.",
        "pdf_link": "https://arxiv.org/pdf/2403.17319v1.pdf"
    },
    {
        "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching",
        "authors": [
            "Youpeng Zhao",
            "Di Wu",
            "Jun Wang"
        ],
        "published": "2024-03-26T01:46:34Z",
        "summary": "The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.17312v1.pdf"
    },
    {
        "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
        "authors": [
            "Anku Rani",
            "Vipula Rawte",
            "Harshad Sharma",
            "Neeraj Anand",
            "Krishnav Rajbangshi",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2024-03-26T01:28:42Z",
        "summary": "The troubling rise of hallucination presents perhaps the most significant\nimpediment to the advancement of responsible AI. In recent times, considerable\nresearch has focused on detecting and mitigating hallucination in Large\nLanguage Models (LLMs). However, it's worth noting that hallucination is also\nquite prevalent in Vision-Language models (VLMs). In this paper, we offer a\nfine-grained discourse on profiling VLM hallucination based on two tasks: i)\nimage captioning, and ii) Visual Question Answering (VQA). We delineate eight\nfine-grained orientations of visual hallucination: i) Contextual Guessing, ii)\nIdentity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender\nAnomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric\nDiscrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly\navailable dataset comprising 2,000 samples generated using eight VLMs across\ntwo tasks of captioning and VQA along with human annotations for the categories\nas mentioned earlier.",
        "pdf_link": "https://arxiv.org/pdf/2403.17306v2.pdf"
    },
    {
        "title": "Automate Knowledge Concept Tagging on Math Questions with LLMs",
        "authors": [
            "Hang Li",
            "Tianlong Xu",
            "Jiliang Tang",
            "Qingsong Wen"
        ],
        "published": "2024-03-26T00:09:38Z",
        "summary": "Knowledge concept tagging for questions plays a crucial role in contemporary\nintelligent educational applications, including learning progress diagnosis,\npractice question recommendations, and course content organization.\nTraditionally, these annotations have been conducted manually with help from\npedagogical experts, as the task requires not only a strong semantic\nunderstanding of both question stems and knowledge definitions but also deep\ninsights into connecting question-solving logic with corresponding knowledge\nconcepts. In this paper, we explore automating the tagging task using Large\nLanguage Models (LLMs), in response to the inability of prior manual methods to\nmeet the rapidly growing demand for concept tagging in questions posed by\nadvanced educational applications. Moreover, the zero/few-shot learning\ncapability of LLMs makes them well-suited for application in educational\nscenarios, which often face challenges in collecting large-scale,\nexpertise-annotated datasets. By conducting extensive experiments with a\nvariety of representative LLMs, we demonstrate that LLMs are a promising tool\nfor concept tagging in math questions. Furthermore, through case studies\nexamining the results from different LLMs, we draw some empirical conclusions\nabout the key factors for success in applying LLMs to the automatic concept\ntagging task.",
        "pdf_link": "https://arxiv.org/pdf/2403.17281v1.pdf"
    },
    {
        "title": "A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer Learning",
        "authors": [
            "Gaurav Negi",
            "Rajdeep Sarkar",
            "Omnia Zayed",
            "Paul Buitelaar"
        ],
        "published": "2024-03-25T23:02:33Z",
        "summary": "Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword\nexpressions (MWEs) on which sentiments are expressed and the sentiment\npolarities associated with them. The development of supervised models has been\nat the forefront of research in this area. However, training these models\nrequires the availability of manually annotated datasets which is both\nexpensive and time-consuming. Furthermore, the available annotated datasets are\ntailored to a specific domain, language, and text type. In this work, we\naddress this notable challenge in current state-of-the-art ABSA research. We\npropose a hybrid approach for Aspect Based Sentiment Analysis using transfer\nlearning. The approach focuses on generating weakly-supervised annotations by\nexploiting the strengths of both large language models (LLM) and traditional\nsyntactic dependencies. We utilise syntactic dependency structures of sentences\nto complement the annotations generated by LLMs, as they may overlook\ndomain-specific aspect terms. Extensive experimentation on multiple datasets is\nperformed to demonstrate the efficacy of our hybrid method for the tasks of\naspect term extraction and aspect sentiment classification.\n  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language\nmodel (LLM)",
        "pdf_link": "https://arxiv.org/pdf/2403.17254v1.pdf"
    },
    {
        "title": "TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models",
        "authors": [
            "Ishika Singh",
            "David Traum",
            "Jesse Thomason"
        ],
        "published": "2024-03-25T22:47:13Z",
        "summary": "Classical planning formulations like the Planning Domain Definition Language\n(PDDL) admit action sequences guaranteed to achieve a goal state given an\ninitial state if any are possible. However, reasoning problems defined in PDDL\ndo not capture temporal aspects of action taking, for example that two agents\nin the domain can execute an action simultaneously if postconditions of each do\nnot interfere with preconditions of the other. A human expert can decompose a\ngoal into largely independent constituent parts and assign each agent to one of\nthese subgoals to take advantage of simultaneous actions for faster execution\nof plan steps, each using only single agent planning. By contrast, large\nlanguage models (LLMs) used for directly inferring plan steps do not guarantee\nexecution success, but do leverage commonsense reasoning to assemble action\nsequences. We combine the strengths of classical planning and LLMs by\napproximating human intuitions for two-agent planning goal decomposition. We\ndemonstrate that LLM-based goal decomposition leads to faster planning times\nthan solving multi-agent PDDL problems directly while simultaneously achieving\nfewer plan execution steps than a single agent plan alone and preserving\nexecution success. Additionally, we find that LLM-based approximations of\nsubgoals can achieve similar multi-agent execution steps than those specified\nby human experts. Website and resources at https://glamor-usc.github.io/twostep",
        "pdf_link": "https://arxiv.org/pdf/2403.17246v1.pdf"
    },
    {
        "title": "SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies",
        "authors": [
            "Akshat Choube",
            "Vedant Das Swain",
            "Varun Mishra"
        ],
        "published": "2024-03-25T21:48:22Z",
        "summary": "Advances in mobile and wearable technologies have enabled the potential to\npassively monitor a person's mental, behavioral, and affective health. These\napproaches typically rely on longitudinal collection of self-reported outcomes,\ne.g., depression, stress, and anxiety, to train machine learning (ML) models.\nHowever, the need to continuously self-report adds a significant burden on the\nparticipants, often resulting in attrition, missing labels, or insincere\nresponses. In this work, we introduce the Scale Scores Simulation using Mental\nModels (SeSaMe) framework to alleviate participants' burden in digital mental\nhealth studies. By leveraging pre-trained large language models (LLMs), SeSaMe\nenables the simulation of participants' responses on psychological scales. In\nSeSaMe, researchers can prompt LLMs with information on participants' internal\nbehavioral dispositions, enabling LLMs to construct mental models of\nparticipants to simulate their responses on psychological scales. We\ndemonstrate an application of SeSaMe, where we use GPT-4 to simulate responses\non one scale using responses from another as behavioral information. We also\nevaluate the alignment between human and SeSaMe-simulated responses to\npsychological scales. Then, we present experiments to inspect the utility of\nSeSaMe-simulated responses as ground truth in training ML models by replicating\nestablished depression and anxiety screening tasks from a previous study. Our\nresults indicate SeSaMe to be a promising approach, but its alignment may vary\nacross scales and specific prediction objectives. We also observed that model\nperformance with simulated data was on par with using the real data for\ntraining in most evaluation scenarios. We conclude by discussing the potential\nimplications of SeSaMe in addressing some challenges researchers face with\nground-truth collection in passive sensing studies.",
        "pdf_link": "https://arxiv.org/pdf/2403.17219v2.pdf"
    },
    {
        "title": "A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection",
        "authors": [
            "Benjamin Steenhoek",
            "Md Mahbubur Rahman",
            "Monoshi Kumar Roy",
            "Mirza Sanjida Alam",
            "Earl T. Barr",
            "Wei Le"
        ],
        "published": "2024-03-25T21:47:36Z",
        "summary": "Large Language Models (LLMs) have demonstrated great potential for code\ngeneration and other software engineering tasks. Vulnerability detection is of\ncrucial importance to maintaining the security, integrity, and trustworthiness\nof software systems. Precise vulnerability detection requires reasoning about\nthe code, making it a good case study for exploring the limits of LLMs'\nreasoning capabilities. Although recent work has applied LLMs to vulnerability\ndetection using generic prompting techniques, their full capabilities for this\ntask and the types of errors they make when explaining identified\nvulnerabilities remain unclear.\n  In this paper, we surveyed eleven LLMs that are state-of-the-art in code\ngeneration and commonly used as coding assistants, and evaluated their\ncapabilities for vulnerability detection. We systematically searched for the\nbest-performing prompts, incorporating techniques such as in-context learning\nand chain-of-thought, and proposed three of our own prompting methods. Our\nresults show that while our prompting methods improved the models' performance,\nLLMs generally struggled with vulnerability detection. They reported 0.5-0.63\nBalanced Accuracy and failed to distinguish between buggy and fixed versions of\nprograms in 76% of cases on average. By comprehensively analyzing and\ncategorizing 287 instances of model reasoning, we found that 57% of LLM\nresponses contained errors, and the models frequently predicted incorrect\nlocations of buggy code and misidentified bug types. LLMs only correctly\nlocalized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted\ncorrectly by 70-100% of human participants. These findings suggest that despite\ntheir potential for other tasks, LLMs may fail to properly comprehend critical\ncode structures and security-related concepts. Our data and code are available\nat https://figshare.com/s/78fe02e56e09ec49300b.",
        "pdf_link": "https://arxiv.org/pdf/2403.17218v1.pdf"
    },
    {
        "title": "Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis",
        "authors": [
            "Na Li",
            "Thomas Bailleux",
            "Zied Bouraoui",
            "Steven Schockaert"
        ],
        "published": "2024-03-25T21:46:35Z",
        "summary": "We consider the problem of finding plausible knowledge that is missing from a\ngiven ontology, as a generalisation of the well-studied taxonomy expansion\ntask. One line of work treats this task as a Natural Language Inference (NLI)\nproblem, thus relying on the knowledge captured by language models to identify\nthe missing knowledge. Another line of work uses concept embeddings to identify\nwhat different concepts have in common, taking inspiration from cognitive\nmodels for category based induction. These two approaches are intuitively\ncomplementary, but their effectiveness has not yet been compared. In this\npaper, we introduce a benchmark for evaluating ontology completion methods and\nthoroughly analyse the strengths and weaknesses of both approaches. We find\nthat both approaches are indeed complementary, with hybrid strategies achieving\nthe best overall results. We also find that the task is highly challenging for\nLarge Language Models, even after fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.17216v1.pdf"
    },
    {
        "title": "Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation",
        "authors": [
            "Marcos Macedo",
            "Yuan Tian",
            "Filipe R. Cogo",
            "Bram Adams"
        ],
        "published": "2024-03-25T21:41:31Z",
        "summary": "Code translation between programming languages is a long-existing and\ncritical task in software engineering, facilitating the modernization of legacy\nsystems, ensuring cross-platform compatibility, and enhancing software\nperformance. With the recent advances in large language models (LLMs) and their\napplications to code translation, there is an increasing need for comprehensive\nevaluation of these models. In this study, we empirically analyze the generated\noutputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B\nup to 46.7B on 3,820 translation pairs across five languages, including C, C++,\nGo, Java, and Python. Our analysis found that between 26.4% and 73.7% of code\ntranslations produced by our evaluated LLMs necessitate post-processing, as\nthese translations often include a mix of code, quotes, and text rather than\nbeing purely source code. Overlooking the output format of these models can\ninadvertently lead to underestimation of their actual performance. This is\nparticularly evident when evaluating them with execution-based metrics such as\nComputational Accuracy (CA). Our results demonstrate that a strategic\ncombination of prompt engineering and regular expression can effectively\nextract the source code from the model generation output. In particular, our\nmethod can help eleven selected models achieve an average Code Extraction\nSuccess Rate (CSR) of 92.73%. Our findings shed light on and motivate future\nresearch to conduct more reliable benchmarks of LLMs for code translation.",
        "pdf_link": "https://arxiv.org/pdf/2403.17214v1.pdf"
    },
    {
        "title": "Generation of Asset Administration Shell with Large Language Model Agents: Interoperability in Digital Twins with Semantic Node",
        "authors": [
            "Yuchen Xia",
            "Zhewen Xiao",
            "Nasser Jazdi",
            "Michael Weyrich"
        ],
        "published": "2024-03-25T21:37:30Z",
        "summary": "This research introduces a novel approach for assisting the creation of Asset\nAdministration Shell (AAS) instances for digital twin modeling within the\ncontext of Industry 4.0, aiming to enhance interoperability in smart\nmanufacturing and reduce manual effort. We construct a \"semantic node\" data\nstructure to capture the semantic essence of textual data. Then, a system\npowered by large language models is designed and implemented to process\n\"semantic node\" and generate AAS instance models from textual technical data.\nOur evaluation demonstrates a 62-79% effective generation rate, indicating a\nsubstantial proportion of manual creation effort can be converted into easier\nvalidation effort, thereby reducing the time and cost in creating AAS instance\nmodels. In our evaluation, a comparative analysis of different LLMs and an\nin-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms\nprovide insights into the effectiveness of LLM systems for interpreting\ntechnical concepts. Our findings emphasize LLMs' capability in automating AAS\ninstance creation, enhancing semantic interoperability, and contributing to the\nbroader field of semantic interoperability for digital twins in industrial\napplications. The prototype implementation and evaluation results are released\non our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM",
        "pdf_link": "https://arxiv.org/pdf/2403.17209v1.pdf"
    },
    {
        "title": "Extracting Social Support and Social Isolation Information from Clinical Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language Model",
        "authors": [
            "Braja Gopal Patra",
            "Lauren A. Lepow",
            "Praneet Kasi Reddy Jagadeesh Kumar",
            "Veer Vekaria",
            "Mohit Manoj Sharma",
            "Prakash Adekkanattu",
            "Brian Fennessy",
            "Gavin Hynes",
            "Isotta Landi",
            "Jorge A. Sanchez-Ruiz",
            "Euijung Ryu",
            "Joanna M. Biernacka",
            "Girish N. Nadkarni",
            "Ardesheer Talati",
            "Myrna Weissman",
            "Mark Olfson",
            "J. John Mann",
            "Alexander W. Charney",
            "Jyotishman Pathak"
        ],
        "published": "2024-03-25T21:19:50Z",
        "summary": "Background: Social support (SS) and social isolation (SI) are social\ndeterminants of health (SDOH) associated with psychiatric outcomes. In\nelectronic health records (EHRs), individual-level SS/SI is typically\ndocumented as narrative clinical notes rather than structured coded data.\nNatural language processing (NLP) algorithms can automate the otherwise\nlabor-intensive process of data extraction.\n  Data and Methods: Psychiatric encounter notes from Mount Sinai Health System\n(MSHS, n=300) and Weill Cornell Medicine (WCM, n=225) were annotated and\nestablished a gold standard corpus. A rule-based system (RBS) involving\nlexicons and a large language model (LLM) using FLAN-T5-XL were developed to\nidentify mentions of SS and SI and their subcategories (e.g., social network,\ninstrumental support, and loneliness).\n  Results: For extracting SS/SI, the RBS obtained higher macro-averaged\nf-scores than the LLM at both MSHS (0.89 vs. 0.65) and WCM (0.85 vs. 0.82). For\nextracting subcategories, the RBS also outperformed the LLM at both MSHS (0.90\nvs. 0.62) and WCM (0.82 vs. 0.81).\n  Discussion and Conclusion: Unexpectedly, the RBS outperformed the LLMs across\nall metrics. Intensive review demonstrates that this finding is due to the\ndivergent approach taken by the RBS and LLM. The RBS were designed and refined\nto follow the same specific rules as the gold standard annotations. Conversely,\nthe LLM were more inclusive with categorization and conformed to common\nEnglish-language understanding. Both approaches offer advantages and are made\navailable open-source for future testing.",
        "pdf_link": "https://arxiv.org/pdf/2403.17199v1.pdf"
    },
    {
        "title": "Outcome-Constrained Large Language Models for Countering Hate Speech",
        "authors": [
            "Lingzi Hong",
            "Pengcheng Luo",
            "Eduardo Blanco",
            "Xiaoying Song"
        ],
        "published": "2024-03-25T19:44:06Z",
        "summary": "Counterspeech that challenges or responds to hate speech has been seen as an\nalternative to mitigate the negative impact of hate speech and foster\nproductive online communications. Research endeavors have been directed to\nusing language models for the automatic generation of counterspeech to assist\nefforts in combating online hate. Existing research focuses on the generation\nof counterspeech with certain linguistic attributes, such as being polite,\ninformative, and intent-driven. However, it remains unclear what impact the\ncounterspeech might have in an online environment. We first explore methods\nthat utilize large language models (LLM) to generate counterspeech constrained\nby potential conversation outcomes. We build two conversation outcome\nclassifiers that predict the incivility level and the hater reentry behavior\nfollowing replies to hate with Reddit data, then propose four methods to\nincorporate the desired outcomes, i.e., low conversation incivility and\nnon-hateful hater reentry, into the text generation process, including Prompt\nwith Instructions, Prompt and Select, LLM finetune, and LLM transformer\nreinforcement learning (TRL). Evaluation results show effective strategies to\ngenerate outcome-constrained counterspeech and the linguistic characteristics\nof texts generated by different methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.17146v1.pdf"
    },
    {
        "title": "MetaAligner: Conditional Weak-to-Strong Correction for Generalizable Multi-Objective Alignment of Language Models",
        "authors": [
            "Kailai Yang",
            "Zhiwei Liu",
            "Qianqian Xie",
            "Tianlin Zhang",
            "Nirui Song",
            "Jimin Huang",
            "Ziyan Kuang",
            "Sophia Ananiadou"
        ],
        "published": "2024-03-25T19:28:10Z",
        "summary": "Recent advancements in large language models (LLMs) aim to tackle\nheterogeneous human expectations and values via multi-objective preference\nalignment. However, existing methods are parameter-adherent to the policy\nmodel, leading to two key limitations: (1) the high-cost repetition of their\nalignment algorithms for each new target model; (2) they cannot expand to\nunseen objectives due to their static alignment objectives. In this work, we\npropose Meta-Objective Aligner (MetaAligner), a model that performs conditional\nweak-to-strong correction for weak responses to approach strong responses.\nMetaAligner is the first policy-agnostic and generalizable method for\nmulti-objective preference alignment, which enables plug-and-play alignment by\ndecoupling parameter updates from the policy models and facilitates zero-shot\npreference alignment for unseen objectives via in-context learning.\nExperimental results show that MetaAligner achieves significant and balanced\nimprovements in multi-objective alignments on 11 policy models with up to 63x\nmore parameters, and outperforms previous alignment methods with down to 22.27x\nless computational resources. The model also accurately aligns with unseen\nobjectives, marking the first step towards generalizable multi-objective\npreference alignment.",
        "pdf_link": "https://arxiv.org/pdf/2403.17141v1.pdf"
    },
    {
        "title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",
        "authors": [
            "Islem Bouzenia",
            "Premkumar Devanbu",
            "Michael Pradel"
        ],
        "published": "2024-03-25T19:17:43Z",
        "summary": "Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering.",
        "pdf_link": "https://arxiv.org/pdf/2403.17134v1.pdf"
    },
    {
        "title": "The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition",
        "authors": [
            "Georgios Chochlakis",
            "Alexandros Potamianos",
            "Kristina Lerman",
            "Shrikanth Narayanan"
        ],
        "published": "2024-03-25T19:07:32Z",
        "summary": "In-context Learning (ICL) has emerged as a powerful paradigm for performing\nnatural language tasks with Large Language Models (LLM) without updating the\nmodels' parameters, in contrast to the traditional gradient-based finetuning.\nThe promise of ICL is that the LLM can adapt to perform the present task at a\ncompetitive or state-of-the-art level at a fraction of the cost. The ability of\nLLMs to perform tasks in this few-shot manner relies on their background\nknowledge of the task (or task priors). However, recent work has found that,\nunlike traditional learning, LLMs are unable to fully integrate information\nfrom demonstrations that contrast task priors. This can lead to performance\nsaturation at suboptimal levels, especially for subjective tasks such as\nemotion recognition, where the mapping from text to emotions can differ widely\ndue to variability in human annotations. In this work, we design experiments\nand propose measurements to explicitly quantify the consistency of proxies of\nLLM priors and their pull on the posteriors. We show that LLMs have strong yet\ninconsistent priors in emotion recognition that ossify their predictions. We\nalso find that the larger the model, the stronger these effects become. Our\nresults suggest that caution is needed when using ICL with larger LLMs for\naffect-centered tasks outside their pre-training domain and when interpreting\nICL results.",
        "pdf_link": "https://arxiv.org/pdf/2403.17125v1.pdf"
    },
    {
        "title": "Attribute First, then Generate: Locally-attributable Grounded Text Generation",
        "authors": [
            "Aviv Slobodkin",
            "Eran Hirsch",
            "Arie Cattan",
            "Tal Schuster",
            "Ido Dagan"
        ],
        "published": "2024-03-25T18:41:47Z",
        "summary": "Recent efforts to address hallucinations in Large Language Models (LLMs) have\nfocused on attributed text generation, which supplements generated texts with\ncitations of supporting sources for post-generation fact-checking and\ncorrections. Yet, these citations often point to entire documents or\nparagraphs, burdening users with extensive verification work. In this paper, we\nintroduce a locally-attributable text generation approach, prioritizing concise\nattributions. Our method, named ``Attribute First, then Generate'', breaks down\nthe conventional end-to-end generation process into three intuitive steps:\ncontent selection, sentence planning, and sequential sentence generation. By\ninitially identifying relevant source segments (``select first'') and then\nconditioning the generation process on them (``then generate''), we ensure\nthese segments also act as the output's fine-grained attributions (``select''\nbecomes ``attribute''). Tested on Multi-document Summarization and Long-form\nQuestion-answering, our method not only yields more concise citations than the\nbaselines but also maintains - and in some cases enhances - both generation\nquality and attribution accuracy. Furthermore, it significantly reduces the\ntime required for fact verification by human assessors.",
        "pdf_link": "https://arxiv.org/pdf/2403.17104v2.pdf"
    },
    {
        "title": "DreamLIP: Language-Image Pre-training with Long Captions",
        "authors": [
            "Kecheng Zheng",
            "Yifei Zhang",
            "Wei Wu",
            "Fan Lu",
            "Shuailei Ma",
            "Xin Jin",
            "Wei Chen",
            "Yujun Shen"
        ],
        "published": "2024-03-25T17:59:42Z",
        "summary": "Language-image pre-training largely relies on how precisely and thoroughly a\ntext describes its paired image. In practice, however, the contents of an image\ncan be so rich that well describing them requires lengthy captions (e.g., with\n10 sentences), which are usually missing in existing datasets. Consequently,\nthere are currently no clear evidences on whether and how language-image\npre-training could benefit from long captions. To figure this out, we first\nre-caption 30M images with detailed descriptions using a pre-trained\nMulti-modality Large Language Model (MLLM), and then study the usage of the\nresulting captions under a contrastive learning framework. We observe that,\neach sentence within a long caption is very likely to describe the image\npartially (e.g., an object). Motivated by this, we propose to dynamically\nsample sub-captions from the text label to construct multiple positive pairs,\nand introduce a grouping loss to match the embeddings of each sub-caption with\nits corresponding local image patches in a self-supervised manner. Experimental\nresults on a wide rage of downstream tasks demonstrate the consistent\nsuperiority of our method, termed DreamLIP, over previous alternatives,\nhighlighting its fine-grained representational capacity. It is noteworthy that,\non the tasks of image-text retrieval and semantic segmentation, our model\ntrained with 30M image-text pairs achieves on par or even better performance\nthan CLIP trained with 400M pairs. Project page is available at\nhttps://zyf0619sjtu.github.io/dream-lip.",
        "pdf_link": "https://arxiv.org/pdf/2403.17007v1.pdf"
    },
    {
        "title": "Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows",
        "authors": [
            "Shujian Zhang",
            "Lemeng Wu",
            "Chengyue Gong",
            "Xingchao Liu"
        ],
        "published": "2024-03-25T17:58:22Z",
        "summary": "Recent works have demonstrated success in controlling sentence attributes\n($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the\ndiffusion language model. A key component that drives theimpressive performance\nfor generating high-quality samples from noise is iteratively denoise for\nthousands of steps. While beneficial, the complexity of starting from the noise\nand the learning steps has limited its implementation to many NLP real-world\napplications. This paper proposes Language Rectified Flow ({\\ours}). Our method\nis based on the reformulation of the standard probabilistic flow models.\nLanguage rectified flow learns (neural) ordinary differential equation models\nto transport between the source distribution and the target distribution, hence\nproviding a unified and effective solution to generative modeling and domain\ntransfer. From the source distribution, our language rectified flow yields fast\nsimulation and effectively decreases the inference time. Experiments on three\nchallenging fine-grained control tasks and multiple high-quality text editing\nshow that our method consistently outperforms its baselines. Extensive\nexperiments and ablation studies demonstrate that our method can be general,\neffective, and beneficial for many NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.16995v1.pdf"
    },
    {
        "title": "Comp4D: LLM-Guided Compositional 4D Scene Generation",
        "authors": [
            "Dejia Xu",
            "Hanwen Liang",
            "Neel P. Bhatt",
            "Hezhen Hu",
            "Hanxue Liang",
            "Konstantinos N. Plataniotis",
            "Zhangyang Wang"
        ],
        "published": "2024-03-25T17:55:52Z",
        "summary": "Recent advancements in diffusion models for 2D and 3D content creation have\nsparked a surge of interest in generating 4D content. However, the scarcity of\n3D scene datasets constrains current methodologies to primarily object-centric\ngeneration. To overcome this limitation, we present Comp4D, a novel framework\nfor Compositional 4D Generation. Unlike conventional methods that generate a\nsingular 4D representation of the entire scene, Comp4D innovatively constructs\neach 4D object within the scene separately. Utilizing Large Language Models\n(LLMs), the framework begins by decomposing an input text prompt into distinct\nentities and maps out their trajectories. It then constructs the compositional\n4D scene by accurately positioning these objects along their designated paths.\nTo refine the scene, our method employs a compositional score distillation\ntechnique guided by the pre-defined trajectories, utilizing pre-trained\ndiffusion models across text-to-image, text-to-video, and text-to-3D domains.\nExtensive experiments demonstrate our outstanding 4D content creation\ncapability compared to prior arts, showcasing superior visual quality, motion\nfidelity, and enhanced object interactions.",
        "pdf_link": "https://arxiv.org/pdf/2403.16993v1.pdf"
    },
    {
        "title": "AIOS: LLM Agent Operating System",
        "authors": [
            "Kai Mei",
            "Zelong Li",
            "Shuyuan Xu",
            "Ruosong Ye",
            "Yingqiang Ge",
            "Yongfeng Zhang"
        ],
        "published": "2024-03-25T17:32:23Z",
        "summary": "The integration and deployment of large language model (LLM)-based\nintelligent agents have been fraught with challenges that compromise their\nefficiency and efficacy. Among these issues are sub-optimal scheduling and\nresource allocation of agent requests over the LLM, the difficulties in\nmaintaining context during interactions between agent and LLM, and the\ncomplexities inherent in integrating heterogeneous agents with different\ncapabilities and specializations. The rapid increase of agent quantity and\ncomplexity further exacerbates these issues, often leading to bottlenecks and\nsub-optimal utilization of resources. Inspired by these challenges, this paper\npresents AIOS, an LLM agent operating system, which embeds large language model\ninto operating systems (OS) as the brain of the OS, enabling an operating\nsystem \"with soul\" -- an important step towards AGI. Specifically, AIOS is\ndesigned to optimize resource allocation, facilitate context switch across\nagents, enable concurrent execution of agents, provide tool service for agents,\nand maintain access control for agents. We present the architecture of such an\noperating system, outline the core challenges it aims to resolve, and provide\nthe basic design and implementation of the AIOS. Our experiments on concurrent\nexecution of multiple agents demonstrate the reliability and efficiency of our\nAIOS modules. Through this, we aim to not only improve the performance and\nefficiency of LLM agents but also to pioneer for better development and\ndeployment of the AIOS ecosystem in the future. The project is open-source at\nhttps://github.com/agiresearch/AIOS.",
        "pdf_link": "https://arxiv.org/pdf/2403.16971v2.pdf"
    },
    {
        "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators",
        "authors": [
            "Yinhong Liu",
            "Han Zhou",
            "Zhijiang Guo",
            "Ehsan Shareghi",
            "Ivan Vuli\u0107",
            "Anna Korhonen",
            "Nigel Collier"
        ],
        "published": "2024-03-25T17:11:28Z",
        "summary": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.",
        "pdf_link": "https://arxiv.org/pdf/2403.16950v2.pdf"
    },
    {
        "title": "PropTest: Automatic Property Testing for Improved Visual Programming",
        "authors": [
            "Jaywon Koo",
            "Ziyan Yang",
            "Paola Cascante-Bonilla",
            "Baishakhi Ray",
            "Vicente Ordonez"
        ],
        "published": "2024-03-25T16:39:15Z",
        "summary": "Visual Programming has emerged as an alternative to end-to-end black-box\nvisual reasoning models. This type of methods leverage Large Language Models\n(LLMs) to decompose a problem and generate the source code for an executable\ncomputer program. This strategy has the advantage of offering an interpretable\nreasoning path and does not require finetuning a model with task-specific data.\nWe propose PropTest, a general strategy that improves visual programming by\nfurther using an LLM to generate code that tests for visual properties in an\ninitial round of proposed solutions. Particularly, our method tests for\ndata-type consistency, as well as syntactic and semantic properties in the\ngenerated solutions. Our proposed solution outperforms baselines and achieves\ncomparable results to state-of-the-art methods while using smaller and publicly\navailable LLMs (CodeLlama-7B and WizardCoder-15B). This is demonstrated across\ndifferent benchmarks on visual question answering and referring expression\ncomprehension, showing the efficacy of our approach in enhancing the\nperformance and generalization of visual reasoning tasks. Specifically,\nPropTest improves ViperGPT by obtaining 48.66% accuracy (+8.3%) on the A-OKVQA\nbenchmark and 52.8% (+3.3%) on the RefCOCO+ benchmark using CodeLlama-7B.",
        "pdf_link": "https://arxiv.org/pdf/2403.16921v1.pdf"
    },
    {
        "title": "Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data",
        "authors": [
            "Shinka Mori",
            "Oana Ignat",
            "Andrew Lee",
            "Rada Mihalcea"
        ],
        "published": "2024-03-25T16:21:25Z",
        "summary": "Synthetic data generation has the potential to impact applications and\ndomains with scarce data. However, before such data is used for sensitive tasks\nsuch as mental health, we need an understanding of how different demographics\nare represented in it. In our paper, we analyze the potential of producing\nsynthetic data using GPT-3 by exploring the various stressors it attributes to\ndifferent race and gender combinations, to provide insight for future\nresearchers looking into using LLMs for data generation. Using GPT-3, we\ndevelop HEADROOM, a synthetic dataset of 3,120 posts about\ndepression-triggering stressors, by controlling for race, gender, and time\nframe (before and after COVID-19). Using this dataset, we conduct semantic and\nlexical analyses to (1) identify the predominant stressors for each demographic\ngroup; and (2) compare our synthetic data to a human-generated dataset. We\npresent the procedures to generate queries to develop depression data using\nGPT-3, and conduct analyzes to uncover the types of stressors it assigns to\ndemographic groups, which could be used to test the limitations of LLMs for\nsynthetic data generation for depression data. Our findings show that synthetic\ndata mimics some of the human-generated data distribution for the predominant\ndepression stressors across diverse demographics.",
        "pdf_link": "https://arxiv.org/pdf/2403.16909v1.pdf"
    },
    {
        "title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games",
        "authors": [
            "Chanwoo Park",
            "Xiangyu Liu",
            "Asuman Ozdaglar",
            "Kaiqing Zhang"
        ],
        "published": "2024-03-25T15:04:11Z",
        "summary": "Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.",
        "pdf_link": "https://arxiv.org/pdf/2403.16843v1.pdf"
    },
    {
        "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
        "authors": [
            "Shuai Ma",
            "Qiaoyi Chen",
            "Xinru Wang",
            "Chengbo Zheng",
            "Zhenhui Peng",
            "Ming Yin",
            "Xiaojuan Ma"
        ],
        "published": "2024-03-25T14:34:06Z",
        "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion\nand decide whether to accept or reject it as a whole. In such a paradigm,\nhumans are found to rarely trigger analytical thinking and face difficulties in\ncommunicating the nuances of conflicting opinions to the AI when disagreements\noccur. To tackle this challenge, we propose Human-AI Deliberation, a novel\nframework to promote human reflection and discussion on conflicting human-AI\nopinions in decision-making. Based on theories in human deliberation, this\nframework engages humans and AI in dimension-level opinion elicitation,\ndeliberative discussion, and decision updates. To empower AI with deliberative\ncapabilities, we designed Deliberative AI, which leverages large language\nmodels (LLMs) as a bridge between humans and domain-specific models to enable\nflexible conversational interactions and faithful information provision. An\nexploratory evaluation on a graduate admissions task shows that Deliberative AI\noutperforms conventional explainable AI (XAI) assistants in improving humans'\nappropriate reliance and task performance. Based on a mixed-methods analysis of\nparticipant behavior, perception, user experience, and open-ended feedback, we\ndraw implications for future AI-assisted decision tool design.",
        "pdf_link": "https://arxiv.org/pdf/2403.16812v1.pdf"
    },
    {
        "title": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems",
        "authors": [
            "Hanqing Yang",
            "Marie Siew",
            "Carlee Joe-Wong"
        ],
        "published": "2024-03-25T14:32:28Z",
        "summary": "The increasing prevalence of Cyber-Physical Systems and the Internet of\nThings (CPS-IoT) applications and Foundation Models are enabling new\napplications that leverage real-time control of the environment. For example,\nreal-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems\ncan reduce its usage when not needed for the comfort of human occupants, hence\nreducing energy consumption. Collecting real-time feedback on human preferences\nin such human-in-the-loop (HITL) systems, however, is difficult in practice. We\npropose the use of large language models (LLMs) to deal with the challenges of\ndynamic environments and difficult-to-obtain data in CPS optimization. In this\npaper, we present a case study that employs LLM agents to mimic the behaviors\nand thermal preferences of various population groups (e.g. young families, the\nelderly) in a shopping mall. The aggregated thermal preferences are integrated\ninto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which\nemploys the LLM as a dynamic simulation of the physical environment to learn\nhow to balance between energy savings and occupant comfort. Our results show\nthat LLMs are capable of simulating complex population movements within large\nopen spaces. Besides, AitL-RL demonstrates superior performance compared to the\npopular existing policy of set point control, suggesting that adaptive and\npersonalized decision-making is critical for efficient optimization in CPS-IoT\napplications. Through this case study, we demonstrate the potential of\nintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance system\nadaptability and efficiency. The project's code can be found on our GitHub\nrepository.",
        "pdf_link": "https://arxiv.org/pdf/2403.16809v1.pdf"
    },
    {
        "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
        "authors": [
            "Zhangqian Bi",
            "Yao Wan",
            "Zheng Wang",
            "Hongyu Zhang",
            "Batu Guan",
            "Fangxin Lu",
            "Zili Zhang",
            "Yulei Sui",
            "Xuanhua Shi",
            "Hai Jin"
        ],
        "published": "2024-03-25T14:07:27Z",
        "summary": "Large language models (LLMs) have shown remarkable progress in automated code\ngeneration. Yet, incorporating LLM-based code generation into real-life\nsoftware projects poses challenges, as the generated code may contain errors in\nAPI usage, class, data structure, or missing project-specific information. As\nmuch of this project-specific context cannot fit into the prompts of LLMs, we\nmust find ways to allow the model to explore the project-level code context. To\nthis end, this paper puts forward a novel approach, termed ProCoder, which\niteratively refines the project-level code context for precise code generation,\nguided by the compiler feedback. In particular, ProCoder first leverages\ncompiler techniques to identify a mismatch between the generated code and the\nproject's context. It then iteratively aligns and fixes the identified errors\nusing information extracted from the code repository. We integrate ProCoder\nwith two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and\napply it to Python code generation. Experimental results show that ProCoder\nsignificantly improves the vanilla LLMs by over 80% in generating code\ndependent on project context, and consistently outperforms the existing\nretrieval-based code generation baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.16792v2.pdf"
    },
    {
        "title": "All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",
        "authors": [
            "Deepak Narayan Gadde",
            "Aman Kumar",
            "Thomas Nalapat",
            "Evgenii Rezunov",
            "Fabio Cappellini"
        ],
        "published": "2024-03-25T13:23:24Z",
        "summary": "Modern hardware designs have grown increasingly efficient and complex.\nHowever, they are often susceptible to Common Weakness Enumerations (CWEs).\nThis paper is focused on the formal verification of CWEs in a dataset of\nhardware designs written in SystemVerilog from Regenerative Artificial\nIntelligence (AI) powered by Large Language Models (LLMs). We applied formal\nverification to categorize each hardware design as vulnerable or CWE-free. This\ndataset was generated by 4 different LLMs and features a unique set of designs\nfor each of the 10 CWEs we target in our paper. We have associated the\nidentified vulnerabilities with CWE numbers for a dataset of 60,000 generated\nSystemVerilog Register Transfer Level (RTL) code. It was also found that most\nLLMs are not aware of any hardware CWEs; hence they are usually not considered\nwhen generating the hardware code. Our study reveals that approximately 60% of\nthe hardware designs generated by LLMs are prone to CWEs, posing potential\nsafety and security risks. The dataset could be ideal for training LLMs and\nMachine Learning (ML) algorithms to abstain from generating CWE-prone hardware\ndesigns.",
        "pdf_link": "https://arxiv.org/pdf/2403.16750v1.pdf"
    },
    {
        "title": "Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography",
        "authors": [
            "Jiayue Zhang",
            "Yiheng Liu",
            "Wenqi Cai",
            "Yali Peng",
            "Jingjing Yu",
            "Senqing Qi",
            "Taotao Long",
            "Bao Ge"
        ],
        "published": "2024-03-25T12:23:12Z",
        "summary": "In recent years, the rapid development of artificial intelligence technology,\nespecially the emergence of large language models (LLMs) such as ChatGPT, has\npresented significant prospects for application in the field of education. LLMs\npossess the capability to interpret knowledge, answer questions, and consider\ncontext, thus providing support for dialogic teaching to students. Therefore,\nan examination of the capacity of LLMs to effectively fulfill instructional\nroles, thereby facilitating student learning akin to human educators within\ndialogic teaching scenarios, is an exceptionally valuable research topic. This\nresearch recruited 34 undergraduate students as participants, who were randomly\ndivided into two groups. The experimental group engaged in dialogic teaching\nusing ChatGPT, while the control group interacted with human teachers. Both\ngroups learned the histogram equalization unit in the information-related\ncourse \"Digital Image Processing\". The research findings show comparable scores\nbetween the two groups on the retention test. However, students who engaged in\ndialogue with ChatGPT exhibited lower performance on the transfer test.\nElectroencephalography data revealed that students who interacted with ChatGPT\nexhibited higher levels of cognitive activity, suggesting that ChatGPT could\nhelp students establish a knowledge foundation and stimulate cognitive\nactivity. However, its strengths on promoting students. knowledge application\nand creativity were insignificant. Based upon the research findings, it is\nevident that ChatGPT cannot fully excel in fulfilling teaching tasks in the\ndialogue teaching in information related courses. Combining ChatGPT with\ntraditional human teachers might be a more ideal approach. The synergistic use\nof both can provide students with more comprehensive learning support, thus\ncontributing to enhancing the quality of teaching.",
        "pdf_link": "https://arxiv.org/pdf/2403.16687v2.pdf"
    },
    {
        "title": "CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment",
        "authors": [
            "Feiteng Fang",
            "Liang Zhu",
            "Min Yang",
            "Xi Feng",
            "Jinchang Hou",
            "Qixuan Zhao",
            "Chengming Li",
            "Xiping Hu",
            "Ruifeng Xu"
        ],
        "published": "2024-03-25T11:37:15Z",
        "summary": "Reinforcement learning from human feedback (RLHF) is a crucial technique in\naligning large language models (LLMs) with human preferences, ensuring these\nLLMs behave in beneficial and comprehensible ways to users. However, a\nlongstanding challenge in human alignment techniques based on reinforcement\nlearning lies in their inherent complexity and difficulty in training. To\naddress this challenge, we present a simple yet effective Contrastive Learning\nFramework for Human Alignment (CLHA) to align LLMs with human preferences\ndirectly. CLHA employs a novel rescoring strategy to evaluate the noise within\nthe data by considering its inherent quality and dynamically adjusting the\ntraining process. Simultaneously, CLHA utilizes pairwise contrastive loss and\nadaptive supervised fine-tuning loss to adaptively modify the likelihood of\ngenerating responses, ensuring enhanced alignment with human preferences. Using\nadvanced methods, CLHA surpasses other algorithms, showcasing superior\nperformance in terms of reward model scores, automatic evaluations, and human\nassessments on the widely used ``Helpful and Harmless'' dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.16649v2.pdf"
    },
    {
        "title": "Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units",
        "authors": [
            "Biswesh Mohapatra",
            "Seemab Hassan",
            "Laurent Romary",
            "Justine Cassell"
        ],
        "published": "2024-03-25T10:39:18Z",
        "summary": "Successful conversations often rest on common understanding, where all\nparties are on the same page about the information being shared. This process,\nknown as conversational grounding, is crucial for building trustworthy dialog\nsystems that can accurately keep track of and recall the shared information.\nThe proficiencies of an agent in grounding the conveyed information\nsignificantly contribute to building a reliable dialog system. Despite recent\nadvancements in dialog systems, there exists a noticeable deficit in their\ngrounding capabilities. Traum provided a framework for conversational grounding\nintroducing Grounding Acts and Grounding Units, but substantial progress,\nespecially in the realm of Large Language Models, remains lacking. To bridge\nthis gap, we present the annotation of two dialog corpora employing Grounding\nActs, Grounding Units, and a measure of their degree of grounding. We discuss\nour key findings during the annotation and also provide a baseline model to\ntest the performance of current Language Models in categorizing the grounding\nacts of the dialogs. Our work aims to provide a useful resource for further\nresearch in making conversations with machines better understood and more\nreliable in natural day-to-day collaborative dialogs.",
        "pdf_link": "https://arxiv.org/pdf/2403.16609v1.pdf"
    },
    {
        "title": "Can Large Language Models (or Humans) Distill Text?",
        "authors": [
            "Nicolas Audinet de Pieuchon",
            "Adel Daoud",
            "Connor Thomas Jerzak",
            "Moa Johansson",
            "Richard Johansson"
        ],
        "published": "2024-03-25T09:51:54Z",
        "summary": "We investigate the potential of large language models (LLMs) to distill text:\nto remove the textual traces of an undesired forbidden variable. We employ a\nrange of LLMs with varying architectures and training approaches to distill\ntext by identifying and removing information about the target variable while\npreserving other relevant signals. Our findings shed light on the strengths and\nlimitations of LLMs in addressing the distillation and provide insights into\nthe strategies for leveraging these models in computational social science\ninvestigations involving text data. In particular, we show that in the strong\ntest of removing sentiment, the statistical association between the processed\ntext and sentiment is still clearly detectable to machine learning classifiers\npost-LLM-distillation. Furthermore, we find that human annotators also struggle\nto distill sentiment while preserving other semantic content. This suggests\nthere may be limited separability between concept variables in some text\ncontexts, highlighting limitations of methods relying on text-level\ntransformations and also raising questions about the robustness of distillation\nmethods that achieve statistical independence in representation space if this\nis difficult for human coders operating on raw text to attain.",
        "pdf_link": "https://arxiv.org/pdf/2403.16584v1.pdf"
    },
    {
        "title": "NSINA: A News Corpus for Sinhala",
        "authors": [
            "Hansi Hettiarachchi",
            "Damith Premasiri",
            "Lasitha Uyangodage",
            "Tharindu Ranasinghe"
        ],
        "published": "2024-03-25T09:36:51Z",
        "summary": "The introduction of large language models (LLMs) has advanced natural\nlanguage processing (NLP), but their effectiveness is largely dependent on\npre-training resources. This is especially evident in low-resource languages,\nsuch as Sinhala, which face two primary challenges: the lack of substantial\ntraining data and limited benchmarking datasets. In response, this study\nintroduces NSINA, a comprehensive news corpus of over 500,000 articles from\npopular Sinhala news websites, along with three NLP tasks: news media\nidentification, news category prediction, and news headline generation. The\nrelease of NSINA aims to provide a solution to challenges in adapting LLMs to\nSinhala, offering valuable resources and benchmarks for improving NLP in the\nSinhala language. NSINA is the largest news corpus for Sinhala, available up to\ndate.",
        "pdf_link": "https://arxiv.org/pdf/2403.16571v1.pdf"
    },
    {
        "title": "Elysium: Exploring Object-level Perception in Videos via MLLM",
        "authors": [
            "Han Wang",
            "Yanjie Wang",
            "Yongjie Ye",
            "Yuxiang Nie",
            "Can Huang"
        ],
        "published": "2024-03-25T09:17:15Z",
        "summary": "Multi-modal Large Language Models (MLLMs) have demonstrated their ability to\nperceive objects in still images, but their application in video-related tasks,\nsuch as object tracking, remains understudied. This lack of exploration is\nprimarily due to two key challenges. Firstly, extensive pretraining on\nlarge-scale video datasets is required to equip MLLMs with the capability to\nperceive objects across multiple frames and understand inter-frame\nrelationships. Secondly, processing a large number of frames within the context\nwindow of Large Language Models (LLMs) can impose a significant computational\nburden. To address the first challenge, we introduce ElysiumTrack-1M, a\nlarge-scale video dataset supported for three tasks: Single Object Tracking\n(SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression\nGeneration (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video\nframes with corresponding object boxes and descriptions. Leveraging this\ndataset, we conduct training of MLLMs and propose a token-compression model\nT-Selector to tackle the second challenge. Our proposed approach, Elysium:\nExploring Object-level Perception in Videos via MLLM, is an end-to-end\ntrainable MLLM that attempts to conduct object-level tasks in videos without\nrequiring any additional plug-in or expert models. All codes and datasets are\navailable at https://github.com/Hon-Wong/Elysium.",
        "pdf_link": "https://arxiv.org/pdf/2403.16558v2.pdf"
    },
    {
        "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
        "authors": [
            "Neeloy Chakraborty",
            "Melkior Ornik",
            "Katherine Driggs-Campbell"
        ],
        "published": "2024-03-25T08:11:02Z",
        "summary": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.",
        "pdf_link": "https://arxiv.org/pdf/2403.16527v1.pdf"
    },
    {
        "title": "Harnessing the power of LLMs for normative reasoning in MASs",
        "authors": [
            "Bastin Tony Roy Savarimuthu",
            "Surangika Ranathunga",
            "Stephen Cranefield"
        ],
        "published": "2024-03-25T08:09:01Z",
        "summary": "Software agents, both human and computational, do not exist in isolation and\noften need to collaborate or coordinate with others to achieve their goals. In\nhuman society, social mechanisms such as norms ensure efficient functioning,\nand these techniques have been adopted by researchers in multi-agent systems\n(MAS) to create socially aware agents. However, traditional techniques have\nlimitations, such as operating in limited environments often using brittle\nsymbolic reasoning. The advent of Large Language Models (LLMs) offers a\npromising solution, providing a rich and expressive vocabulary for norms and\nenabling norm-capable agents that can perform a range of tasks such as norm\ndiscovery, normative reasoning and decision-making. This paper examines the\npotential of LLM-based agents to acquire normative capabilities, drawing on\nrecent Natural Language Processing (NLP) and LLM research. We present our\nvision for creating normative LLM agents. In particular, we discuss how the\nrecently proposed \"LLM agent\" approaches can be extended to implement such\nnormative LLM agents. We also highlight challenges in this emerging field. This\npaper thus aims to foster collaboration between MAS, NLP and LLM researchers in\norder to advance the field of normative agents.",
        "pdf_link": "https://arxiv.org/pdf/2403.16524v1.pdf"
    },
    {
        "title": "LLMs Are Few-Shot In-Context Low-Resource Language Learners",
        "authors": [
            "Samuel Cahyawijaya",
            "Holy Lovenia",
            "Pascale Fung"
        ],
        "published": "2024-03-25T07:55:29Z",
        "summary": "In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2403.16512v2.pdf"
    },
    {
        "title": "LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent Classification",
        "authors": [
            "Liu Junhua",
            "Tan Yong Keat",
            "Fu Bin"
        ],
        "published": "2024-03-25T07:38:40Z",
        "summary": "Following the significant achievements of large language models (LLMs),\nresearchers have employed in-context learning for text classification tasks.\nHowever, these studies focused on monolingual, single-turn classification\ntasks. In this paper, we introduce LARA (Linguistic-Adaptive\nRetrieval-Augmented Language Models), designed to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating numerous\nintents in chatbot interactions. Multi-turn intent classification is notably\nchallenging due to the complexity and evolving nature of conversational\ncontexts. LARA tackles these issues by combining a fine-tuned smaller model\nwith a retrieval-augmented mechanism, integrated within the architecture of\nLLMs. This integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tune. Comprehensive\nexperiments demonstrate that LARA achieves state-of-the-art performance on\nmulti-turn intent classification tasks, enhancing the average accuracy by 3.67%\ncompared to existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.16504v1.pdf"
    },
    {
        "title": "Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm",
        "authors": [
            "Lei Liu",
            "Xiaoyan Yang",
            "Fangzhou Li",
            "Chenfei Chi",
            "Yue Shen",
            "Shiwei Lyu Ming Zhang",
            "Xiaowei Ma",
            "Xiangguo Lyu",
            "Liya Ma",
            "Zhiqiang Zhang",
            "Wei Xue",
            "Yiran Huang",
            "Jinjie Gu"
        ],
        "published": "2024-03-25T06:17:54Z",
        "summary": "Large language models (LLMs) are gaining increasing interests to improve\nclinical efficiency for medical diagnosis, owing to their unprecedented\nperformance in modelling natural language. Ensuring the safe and reliable\nclinical applications, the evaluation of LLMs indeed becomes critical for\nbetter mitigating the potential risks, e.g., hallucinations. However, current\nevaluation methods heavily rely on labor-intensive human participation to\nachieve human-preferred judgements. To overcome this challenge, we propose an\nautomatic evaluation paradigm tailored to assess the LLMs' capabilities in\ndelivering clinical services, e.g., disease diagnosis and treatment. The\nevaluation paradigm contains three basic elements: metric, data, and algorithm.\nSpecifically, inspired by professional clinical practice pathways, we formulate\na LLM-specific clinical pathway (LCP) to define the clinical capabilities that\na doctor agent should possess. Then, Standardized Patients (SPs) from the\nmedical education are introduced as the guideline for collecting medical data\nfor evaluation, which can well ensure the completeness of the evaluation\nprocedure. Leveraging these steps, we develop a multi-agent framework to\nsimulate the interactive environment between SPs and a doctor agent, which is\nequipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the\nbehaviors of a doctor agent are in accordance with LCP. The above paradigm can\nbe extended to any similar clinical scenarios to automatically evaluate the\nLLMs' medical capabilities. Applying such paradigm, we construct an evaluation\nbenchmark in the field of urology, including a LCP, a SPs dataset, and an\nautomated RAE. Extensive experiments are conducted to demonstrate the\neffectiveness of the proposed approach, providing more insights for LLMs' safe\nand reliable deployments in clinical practice.",
        "pdf_link": "https://arxiv.org/pdf/2403.16446v1.pdf"
    },
    {
        "title": "CodeS: Natural Language to Code Repository via Multi-Layer Sketch",
        "authors": [
            "Daoguang Zan",
            "Ailun Yu",
            "Wei Liu",
            "Dong Chen",
            "Bo Shen",
            "Wei Li",
            "Yafen Yao",
            "Yongshun Gong",
            "Xiaolin Chen",
            "Bei Guan",
            "Zhiguang Yang",
            "Yongji Wang",
            "Qianxiang Wang",
            "Lizhen Cui"
        ],
        "published": "2024-03-25T06:09:55Z",
        "summary": "The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.",
        "pdf_link": "https://arxiv.org/pdf/2403.16443v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models with Runtime Behavior of Program Execution",
        "authors": [
            "Junkai Chen",
            "Zhiyuan Pan",
            "Xing Hu",
            "Zhenhao Li",
            "Ge Li",
            "Xin Xia"
        ],
        "published": "2024-03-25T05:37:16Z",
        "summary": "Large language models for code (i.e., code LLMs) have shown strong code\nunderstanding and generation capabilities. To evaluate the capabilities of code\nLLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval\nand ClassEval). Code reasoning is one of the most essential abilities of code\nLLMs, but existing benchmarks for code reasoning are not sufficient. Typically,\nthey focus on predicting the input and output of a program, ignoring the\nevaluation of the intermediate behavior during program execution, as well as\nthe logical consistency (e.g., the model should not give the correct output if\nthe prediction of execution path is wrong) when performing the reasoning. To\naddress these problems, in this paper, we propose a framework, namely REval,\nfor evaluating code reasoning abilities and consistency of code LLMs with\nprogram execution. We utilize existing code benchmarks and adapt them to new\nbenchmarks within our framework. A large-scale empirical study is conducted and\nmost LLMs show unsatisfactory performance on both Runtime Behavior Reasoning\n(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation\n(i.e., an average IC score of 10.3). Evaluation results of current code LLMs\nreflect the urgent need for the community to strengthen the code reasoning\ncapability of code LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.16437v1.pdf"
    },
    {
        "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
        "authors": [
            "Chao-Wei Huang",
            "Yun-Nung Chen"
        ],
        "published": "2024-03-25T05:31:22Z",
        "summary": "This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR",
        "pdf_link": "https://arxiv.org/pdf/2403.16435v1.pdf"
    },
    {
        "title": "$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
        "authors": [
            "Yue Xu",
            "Wenjie Wang"
        ],
        "published": "2024-03-25T05:27:35Z",
        "summary": "Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo. The resource is available at\n$\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
        "pdf_link": "https://arxiv.org/pdf/2403.16432v3.pdf"
    },
    {
        "title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation",
        "authors": [
            "Ziyan Wang",
            "Yingpeng Du",
            "Zhu Sun",
            "Haoyan Chua",
            "Kaidong Feng",
            "Wenya Wang",
            "Jie Zhang"
        ],
        "published": "2024-03-25T05:12:18Z",
        "summary": "Large Language Models (LLMs) are emerging as promising approaches to enhance\nsession-based recommendation (SBR), where both prompt-based and\nfine-tuning-based methods have been widely investigated to align LLMs with SBR.\nHowever, the former methods struggle with optimal prompts to elicit the correct\nreasoning of LLMs due to the lack of task-specific feedback, leading to\nunsatisfactory recommendations. Although the latter methods attempt to\nfine-tune LLMs with domain-specific knowledge, they face limitations such as\nhigh computational costs and reliance on open-source backbones. To address such\nissues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for\nSBR, guiding LLMs to focus on specialized knowledge essential for more accurate\nrecommendations effectively and efficiently. In particular, we first design the\nReflective Exploration Module to effectively extract knowledge that is readily\nunderstandable and digestible by LLMs. To be specific, we direct LLMs to\nexamine recommendation errors through self-reflection and construct a knowledge\nbase (KB) comprising hints capable of rectifying these errors. To efficiently\nelicit the correct reasoning of LLMs, we further devise the Reinforcement\nUtilization Module to train a lightweight retrieval agent. It learns to select\nhints from the constructed KB based on the task-specific feedback, where the\nhints can serve as guidance to help correct LLMs reasoning for better\nrecommendations. Extensive experiments on multiple real-world datasets\ndemonstrate that our method consistently outperforms state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.16427v3.pdf"
    },
    {
        "title": "An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations",
        "authors": [
            "Eric H. C. Chow",
            "TJ Kao",
            "Xiaoli Li"
        ],
        "published": "2024-03-25T05:04:52Z",
        "summary": "This study delves into the potential use of Large Language Models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and summaries. The results revealed that although\nsome generated subject headings were valid, there were issues regarding\nspecificity and exhaustiveness. The study showcases that LLMs can serve as a\nstrategic response to the backlog of items awaiting cataloging in academic\nlibraries, while also offering a cost-effective approach for promptly\ngenerating LCSH. Nonetheless, human catalogers remain essential for verifying\nand enhancing the validity, exhaustiveness, and specificity of LCSH generated\nby LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.16424v2.pdf"
    },
    {
        "title": "How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation",
        "authors": [
            "Lixi Zhu",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "published": "2024-03-25T04:21:06Z",
        "summary": "Conversational Recommender System (CRS) interacts with users through natural\nlanguage to understand their preferences and provide personalized\nrecommendations in real-time. CRS has demonstrated significant potential,\nprompting researchers to address the development of more realistic and reliable\nuser simulators as a key focus. Recently, the capabilities of Large Language\nModels (LLMs) have attracted a lot of attention in various fields.\nSimultaneously, efforts are underway to construct user simulators based on\nLLMs. While these works showcase innovation, they also come with certain\nlimitations that require attention. In this work, we aim to analyze the\nlimitations of using LLMs in constructing user simulators for CRS, to guide\nfuture research. To achieve this goal, we conduct analytical validation on the\nnotable work, iEvaLM. Through multiple experiments on two widely-used datasets\nin the field of conversational recommendation, we highlight several issues with\nthe current evaluation methods for user simulators based on LLMs: (1) Data\nleakage, which occurs in conversational history and the user simulator's\nreplies, results in inflated evaluation results. (2) The success of CRS\nrecommendations depends more on the availability and quality of conversational\nhistory than on the responses from user simulators. (3) Controlling the output\nof the user simulator through a single prompt template proves challenging. To\novercome these limitations, we propose SimpleUserSim, employing a\nstraightforward strategy to guide the topic toward the target items. Our study\nvalidates the ability of CRS models to utilize the interaction information,\nsignificantly improving the recommendation results.",
        "pdf_link": "https://arxiv.org/pdf/2403.16416v1.pdf"
    },
    {
        "title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases",
        "authors": [
            "Wenhao Huang",
            "Qianyu He",
            "Zhixu Li",
            "Jiaqing Liang",
            "Yanghua Xiao"
        ],
        "published": "2024-03-25T03:19:20Z",
        "summary": "Definition bias is a negative phenomenon that can mislead models. Definition\nbias in information extraction appears not only across datasets from different\ndomains but also within datasets sharing the same domain. We identify two types\nof definition bias in IE: bias among information extraction datasets and bias\nbetween information extraction datasets and instruction tuning datasets. To\nsystematically investigate definition bias, we conduct three probing\nexperiments to quantitatively analyze it and discover the limitations of\nunified information extraction and large language models in solving definition\nbias. To mitigate definition bias in information extraction, we propose a\nmulti-stage framework consisting of definition bias measurement, bias-aware\nfine-tuning, and task-specific bias mitigation. Experimental results\ndemonstrate the effectiveness of our framework in addressing definition bias.\nResources of this paper can be found at\nhttps://github.com/EZ-hwh/definition-bias",
        "pdf_link": "https://arxiv.org/pdf/2403.16396v1.pdf"
    },
    {
        "title": "Concurrent Linguistic Error Detection (CLED) for Large Language Models",
        "authors": [
            "Jinhua Zhu",
            "Javier Conde",
            "Zhen Gao",
            "Pedro Reviriego",
            "Shanshan Liu",
            "Fabrizio Lombardi"
        ],
        "published": "2024-03-25T03:17:27Z",
        "summary": "The wide adoption of Large language models (LLMs) makes their dependability a\npressing concern. Detection of errors is the first step to mitigating their\nimpact on a system and thus, efficient error detection for LLMs is an important\nissue. In many settings, the LLM is considered as a black box with no access to\nthe internal nodes; this prevents the use of many error detection schemes that\nneed access to the model's internal nodes. An interesting observation is that\nthe output of LLMs in error-free operation should be valid and normal text.\nTherefore, when the text is not valid or differs significantly from normal\ntext, it is likely that there is an error. Based on this observation we propose\nto perform Concurrent Linguistic Error Detection (CLED); this scheme extracts\nsome linguistic features of the text generated by the LLM and feeds them to a\nconcurrent classifier that detects errors. Since the proposed error detection\nmechanism only relies on the outputs of the model, then it can be used on LLMs\nin which there is no access to the internal nodes. The proposed CLED scheme has\nbeen evaluated on the T5 model when used for news summarization and on the\nOPUS-MT model when used for translation. In both cases, the same set of\nlinguistic features has been used for error detection to illustrate the\napplicability of the proposed scheme beyond a specific case. The results show\nthat CLED can detect most of the errors at a low overhead penalty. The use of\nthe concurrent classifier also enables a trade-off between error detection\neffectiveness and its associated overhead, so providing flexibility to a\ndesigner.",
        "pdf_link": "https://arxiv.org/pdf/2403.16393v1.pdf"
    },
    {
        "title": "Dia-LLaMA: Towards Large Language Model-driven CT Report Generation",
        "authors": [
            "Zhixuan Chen",
            "Luyang Luo",
            "Yequan Bie",
            "Hao Chen"
        ],
        "published": "2024-03-25T03:02:51Z",
        "summary": "Medical report generation has achieved remarkable advancements yet has still\nbeen faced with several challenges. First, the inherent imbalance in the\ndistribution of normal and abnormal cases may lead models to exhibit a biased\nfocus on normal samples, resulting in unreliable diagnoses. Second, the\nfrequent occurrence of common template sentences in the reports may overwhelm\nthe critical abnormal information. Moreover, existing works focus on 2D chest\nX-rays, leaving CT report generation underexplored due to the high-dimensional\nnature of CT images and the limited availability of CT-report pairs. Recently,\nLLM has shown a great ability to generate reliable answers with appropriate\nprompts, which shed light on addressing the aforementioned challenges. In this\npaper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report\ngeneration by incorporating diagnostic information as guidance prompts.\nConsidering the high dimension of CT, we leverage a pre-trained ViT3D with\nperceiver to extract the visual information. To tailor the LLM for report\ngeneration and emphasize abnormality, we extract additional diagnostic\ninformation by referring to a disease prototype memory bank, which is updated\nduring training to capture common disease representations. Furthermore, we\nintroduce disease-aware attention to enable the model to adjust attention for\ndifferent diseases. Experiments on the chest CT dataset demonstrated that our\nproposed method outperformed previous methods and achieved state-of-the-art on\nboth clinical efficacy performance and natural language generation metrics. The\ncode will be made publically available.",
        "pdf_link": "https://arxiv.org/pdf/2403.16386v1.pdf"
    },
    {
        "title": "Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA",
        "authors": [
            "Zhuowan Li",
            "Bhavan Jasani",
            "Peng Tang",
            "Shabnam Ghadar"
        ],
        "published": "2024-03-25T03:02:27Z",
        "summary": "Understanding data visualizations like charts and plots requires reasoning\nabout both visual elements and numerics. Although strong in extractive\nquestions, current chart visual question answering (chart VQA) models suffer on\ncomplex reasoning questions. In this work, we address the lack of reasoning\nability by data augmentation. We leverage Large Language Models (LLMs), which\nhave shown to have strong reasoning ability, as an automatic data annotator\nthat generates question-answer annotations for chart images. The key innovation\nin our method lies in the Synthesize Step-by-Step strategy: our LLM-based data\ngenerator learns to decompose the complex question into step-by-step\nsub-questions (rationales), which are then used to derive the final answer\nusing external tools, i.e. Python. This step-wise generation procedure is\ntrained on synthetic data generated using a template-based QA generation\npipeline. Experimental results highlight the significance of the proposed\nstep-by-step generation. By training with the LLM-augmented data (LAMENDA), we\nsignificantly enhance the chart VQA models, achieving the state-of-the-art\naccuracy on the ChartQA and PlotQA datasets. In particular, our approach\nimproves the accuracy of the previous state-of-the-art approach from 38% to 54%\non the human-written questions in the ChartQA dataset, which needs strong\nreasoning. We hope our work underscores the potential of synthetic data and\nencourages further exploration of data augmentation using LLMs for\nreasoning-heavy tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.16385v2.pdf"
    },
    {
        "title": "ChatDBG: An AI-Powered Debugging Assistant",
        "authors": [
            "Kyla Levin",
            "Nicolas van Kempen",
            "Emery D. Berger",
            "Stephen N. Freund"
        ],
        "published": "2024-03-25T01:12:57Z",
        "summary": "This paper presents ChatDBG, the first AI-powered debugging assistant.\nChatDBG integrates large language models (LLMs) to significantly enhance the\ncapabilities and user-friendliness of conventional debuggers. ChatDBG lets\nprogrammers engage in a collaborative dialogue with the debugger, allowing them\nto pose complex questions about program state, perform root cause analysis for\ncrashes or assertion failures, and explore open-ended queries like \"why is x\nnull?\". To handle these queries, ChatDBG grants the LLM autonomy to take the\nwheel and drive debugging by issuing commands to navigate through stacks and\ninspect program state; it then reports its findings and yields back control to\nthe programmer. Our ChatDBG prototype integrates with standard debuggers\nincluding LLDB, GDB, and WinDBG for native code and Pdb for Python. Our\nevaluation across a diverse set of code, including C/C++ code with known bugs\nand a suite of Python code including standalone scripts and Jupyter notebooks,\ndemonstrates that ChatDBG can successfully analyze root causes, explain bugs,\nand generate accurate fixes for a wide range of real-world errors. For the\nPython programs, a single query led to an actionable bug fix 67% of the time;\none additional follow-up query increased the success rate to 85%. ChatDBG has\nseen rapid uptake; it has already been downloaded nearly 30,000 times.",
        "pdf_link": "https://arxiv.org/pdf/2403.16354v1.pdf"
    },
    {
        "title": "Enhanced Facet Generation with LLM Editing",
        "authors": [
            "Joosung Lee",
            "Jinhong Kim"
        ],
        "published": "2024-03-25T00:43:44Z",
        "summary": "In information retrieval, facet identification of a user query is an\nimportant task. If a search service can recognize the facets of a user's query,\nit has the potential to offer users a much broader range of search results.\nPrevious studies can enhance facet prediction by leveraging retrieved documents\nand related queries obtained through a search engine. However, there are\nchallenges in extending it to other applications when a search engine operates\nas part of the model. First, search engines are constantly updated. Therefore,\nadditional information may change during training and test, which may reduce\nperformance. The second challenge is that public search engines cannot search\nfor internal documents. Therefore, a separate search system needs to be built\nto incorporate documents from private domains within the company. We propose\ntwo strategies that focus on a framework that can predict facets by taking only\nqueries as input without a search engine. The first strategy is multi-task\nlearning to predict SERP. By leveraging SERP as a target instead of a source,\nthe proposed model deeply understands queries without relying on external\nmodules. The second strategy is to enhance the facets by combining Large\nLanguage Model (LLM) and the small model. Overall performance improves when\nsmall model and LLM are combined rather than facet generation individually.",
        "pdf_link": "https://arxiv.org/pdf/2403.16345v1.pdf"
    },
    {
        "title": "Is Watermarking LLM-Generated Code Robust?",
        "authors": [
            "Tarun Suresh",
            "Shubham Ugare",
            "Gagandeep Singh",
            "Sasa Misailovic"
        ],
        "published": "2024-03-24T21:41:29Z",
        "summary": "We present the first study of the robustness of existing watermarking\ntechniques on Python code generated by large language models. Although existing\nworks showed that watermarking can be robust for natural language, we show that\nit is easy to remove these watermarks on code by semantic-preserving\ntransformations.",
        "pdf_link": "https://arxiv.org/pdf/2403.17983v1.pdf"
    },
    {
        "title": "Large Language Models in Biomedical and Health Informatics: A Bibliometric Review",
        "authors": [
            "Huizi Yu",
            "Lizhou Fan",
            "Lingyao Li",
            "Jiayan Zhou",
            "Zihui Ma",
            "Lu Xian",
            "Wenyue Hua",
            "Sijia He",
            "Mingyu Jin",
            "Yongfeng Zhang",
            "Ashvin Gandhi",
            "Xin Ma"
        ],
        "published": "2024-03-24T21:29:39Z",
        "summary": "Large Language Models (LLMs) have rapidly become important tools in\nBiomedical and Health Informatics (BHI), enabling new ways to analyze data,\ntreat patients, and conduct research. This bibliometric review aims to provide\na panoramic view of how LLMs have been used in BHI by examining research\narticles and collaboration networks from 2022 to 2023. It further explores how\nLLMs can improve Natural Language Processing (NLP) applications in various BHI\nareas like medical diagnosis, patient engagement, electronic health record\nmanagement, and personalized medicine. To do this, our bibliometric review\nidentifies key trends, maps out research networks, and highlights major\ndevelopments in this fast-moving field. Lastly, it discusses the ethical\nconcerns and practical challenges of using LLMs in BHI, such as data privacy\nand reliable medical recommendations. Looking ahead, we consider how LLMs could\nfurther transform biomedical research as well as healthcare delivery and\npatient outcomes. This bibliometric review serves as a resource for\nstakeholders in healthcare, including researchers, clinicians, and\npolicymakers, to understand the current state and future potential of LLMs in\nBHI.",
        "pdf_link": "https://arxiv.org/pdf/2403.16303v2.pdf"
    },
    {
        "title": "Engineering Safety Requirements for Autonomous Driving with Large Language Models",
        "authors": [
            "Ali Nouri",
            "Beatriz Cabrero-Daniel",
            "Fredrik T\u00f6rner",
            "H\u0227kan Sivencrona",
            "Christian Berger"
        ],
        "published": "2024-03-24T20:40:51Z",
        "summary": "Changes and updates in the requirement artifacts, which can be frequent in\nthe automotive domain, are a challenge for SafetyOps. Large Language Models\n(LLMs), with their impressive natural language understanding and generating\ncapabilities, can play a key role in automatically refining and decomposing\nrequirements after each update. In this study, we propose a prototype of a\npipeline of prompts and LLMs that receives an item definition and outputs\nsolutions in the form of safety requirements. This pipeline also performs a\nreview of the requirement dataset and identifies redundant or contradictory\nrequirements. We first identified the necessary characteristics for performing\nHARA and then defined tests to assess an LLM's capability in meeting these\ncriteria. We used design science with multiple iterations and let experts from\ndifferent companies evaluate each cycle quantitatively and qualitatively.\nFinally, the prototype was implemented at a case company and the responsible\nteam evaluated its efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2403.16289v1.pdf"
    },
    {
        "title": "AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue",
        "authors": [
            "Yunlong Tang",
            "Daiki Shimada",
            "Jing Bi",
            "Chenliang Xu"
        ],
        "published": "2024-03-24T19:50:49Z",
        "summary": "In everyday communication, humans frequently use speech and gestures to refer\nto specific areas or objects, a process known as Referential Dialogue (RD).\nWhile prior studies have investigated RD through Large Language Models (LLMs)\nor Large Multimodal Models (LMMs) in static contexts, the exploration of\nTemporal Referential Dialogue (TRD) within audio-visual media remains limited.\nTwo primary challenges hinder progress in this field: (1) the absence of\ncomprehensive, untrimmed audio-visual video datasets with precise temporal\nannotations, and (2) the need for methods to integrate complex temporal\nauditory and visual cues effectively. To address these challenges, we introduce\na novel framework to generate PU-VALOR, an extensive audio-visual dataset\ncomprising over 114,000 untrimmed videos with accurate temporal demarcations.\nWe also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI)\nthat ensures the temporal alignment of audio-visual information. Additionally,\nwe develop the A5-222K dataset, encompassing more than 200,000 audio-text\npairings, to facilitate the audio and text alignments. Our experiments\ndemonstrate that AVicuna can effectively handle TRD in audio-visual videos and\nachieve state-of-the-art performance on various audio-visual video\nunderstanding tasks, particularly in untrimmed videos. We further investigate\nthe optimal audio-interleaving rate for interleaved audio-visual inputs, which\nmaximizes performance on the Audio-Visual Event Dense Localization task.",
        "pdf_link": "https://arxiv.org/pdf/2403.16276v1.pdf"
    },
    {
        "title": "Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling",
        "authors": [
            "Yida Mu",
            "Chun Dong",
            "Kalina Bontcheva",
            "Xingyi Song"
        ],
        "published": "2024-03-24T17:39:51Z",
        "summary": "Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.",
        "pdf_link": "https://arxiv.org/pdf/2403.16248v2.pdf"
    },
    {
        "title": "CoverUp: Coverage-Guided LLM-Based Test Generation",
        "authors": [
            "Juan Altmayer Pizzorno",
            "Emery D. Berger"
        ],
        "published": "2024-03-24T16:18:27Z",
        "summary": "This paper presents CoverUp, a novel system that drives the generation of\nhigh-coverage Python regression tests via a combination of coverage analysis\nand large-language models (LLMs). CoverUp iteratively improves coverage,\ninterleaving coverage analysis with dialogs with the LLM to focus its attention\non as yet uncovered lines and branches. The resulting test suites significantly\nimprove coverage over the current state of the art: compared to CodaMosa, a\nhybrid LLM / search-based software testing system, CoverUp substantially\nimproves coverage across the board. On a per-module basis, CoverUp achieves\nmedian line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and\nline+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative,\ncoverage-guided approach is crucial to its effectiveness, contributing to\nnearly half of its successes.",
        "pdf_link": "https://arxiv.org/pdf/2403.16218v1.pdf"
    },
    {
        "title": "ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language Models",
        "authors": [
            "Zequan Liu",
            "Jiawen Lyn",
            "Wei Zhu",
            "Xing Tian",
            "Yvette Graham"
        ],
        "published": "2024-03-24T15:09:55Z",
        "summary": "Parameter-efficient fine-tuning (PEFT) is widely studied for its\neffectiveness and efficiency in the era of large language models. Low-rank\nadaptation (LoRA) has demonstrated commendable performance as a popular and\nrepresentative method. However, it is implemented with a fixed intrinsic rank\nthat might not be the ideal setting for the downstream tasks. Recognizing the\nneed for more flexible downstream task adaptation, we extend the methodology of\nLoRA to an innovative approach we call allocating low-rank adaptation (ALoRA)\nthat enables dynamic adjustments to the intrinsic rank during the adaptation\nprocess. First, we propose a novel method, AB-LoRA, that can effectively\nestimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we\ngradually prune abundant and negatively impacting LoRA ranks and allocate the\npruned LoRA budgets to important Transformer modules needing higher ranks. We\nhave conducted experiments on various tasks, and the experimental results\ndemonstrate that our ALoRA method can outperform the recent baselines with\ncomparable tunable parameters.",
        "pdf_link": "https://arxiv.org/pdf/2403.16187v1.pdf"
    },
    {
        "title": "A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish",
        "authors": [
            "Masahiro Kaneko",
            "Timothy Baldwin"
        ],
        "published": "2024-03-24T13:21:58Z",
        "summary": "Large Language Models (LLMs) are trained on massive web-crawled corpora. This\nposes risks of leakage, including personal information, copyrighted texts, and\nbenchmark datasets. Such leakage leads to undermining human trust in AI due to\npotential unauthorized generation of content or overestimation of performance.\nWe establish the following three criteria concerning the leakage issues: (1)\nleakage rate: the proportion of leaked data in training data, (2) output rate:\nthe ease of generating leaked data, and (3) detection rate: the detection\nperformance of leaked versus non-leaked data. Despite the leakage rate being\nthe origin of data leakage issues, it is not understood how it affects the\noutput rate and detection rate. In this paper, we conduct an experimental\nsurvey to elucidate the relationship between the leakage rate and both the\noutput rate and detection rate for personal information, copyrighted texts, and\nbenchmark data. Additionally, we propose a self-detection approach that uses\nfew-shot learning in which LLMs detect whether instances are present or absent\nin their training data, in contrast to previous methods that do not employ\nexplicit learning. To explore the ease of generating leaked information, we\ncreate a dataset of prompts designed to elicit personal information,\ncopyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs\nproduce leaked information in most cases despite less such data in their\ntraining set. This indicates even small amounts of leaked data can greatly\naffect outputs. Our self-detection method showed superior performance compared\nto existing detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.16139v1.pdf"
    },
    {
        "title": "Opportunities and challenges in the application of large artificial intelligence models in radiology",
        "authors": [
            "Liangrui Pan",
            "Zhenyu Zhao",
            "Ying Lu",
            "Kewei Tang",
            "Liyong Fu",
            "Qingchun Liang",
            "Shaoliang Peng"
        ],
        "published": "2024-03-24T12:05:23Z",
        "summary": "Influenced by ChatGPT, artificial intelligence (AI) large models have\nwitnessed a global upsurge in large model research and development. As people\nenjoy the convenience by this AI large model, more and more large models in\nsubdivided fields are gradually being proposed, especially large models in\nradiology imaging field. This article first introduces the development history\nof large models, technical details, workflow, working principles of multimodal\nlarge models and working principles of video generation large models. Secondly,\nwe summarize the latest research progress of AI large models in radiology\neducation, radiology report generation, applications of unimodal and multimodal\nradiology. Finally, this paper also summarizes some of the challenges of large\nAI models in radiology, with the aim of better promoting the rapid revolution\nin the field of radiography.",
        "pdf_link": "https://arxiv.org/pdf/2403.16112v1.pdf"
    },
    {
        "title": "Can Language Models Pretend Solvers? Logic Code Simulation with LLMs",
        "authors": [
            "Minyu Chen",
            "Guoqiang Li",
            "Ling-I Wu",
            "Ruibang Liu",
            "Yuxin Su",
            "Xi Chang",
            "Jianxin Xue"
        ],
        "published": "2024-03-24T11:27:16Z",
        "summary": "Transformer-based large language models (LLMs) have demonstrated significant\npotential in addressing logic problems. capitalizing on the great capabilities\nof LLMs for code-related activities, several frameworks leveraging logical\nsolvers for logic reasoning have been proposed recently. While existing\nresearch predominantly focuses on viewing LLMs as natural language logic\nsolvers or translators, their roles as logic code interpreters and executors\nhave received limited attention. This study delves into a novel aspect, namely\nlogic code simulation, which forces LLMs to emulate logical solvers in\npredicting the results of logical programs. To further investigate this novel\ntask, we formulate our three research questions: Can LLMs efficiently simulate\nthe outputs of logic codes? What strength arises along with logic code\nsimulation? And what pitfalls? To address these inquiries, we curate three\nnovel datasets tailored for the logic code simulation task and undertake\nthorough experiments to establish the baseline performance of LLMs in code\nsimulation. Subsequently, we introduce a pioneering LLM-based code simulation\ntechnique, Dual Chains of Logic (DCoL). This technique advocates a dual-path\nthinking approach for LLMs, which has demonstrated state-of-the-art performance\ncompared to other LLM prompt strategies, achieving a notable improvement in\naccuracy by 7.06% with GPT-4-Turbo.",
        "pdf_link": "https://arxiv.org/pdf/2403.16097v2.pdf"
    },
    {
        "title": "LLMs as Compiler for Arabic Programming Language",
        "authors": [
            "Serry Sibaee",
            "Omar Najar",
            "Lahouri Ghouti",
            "Anis Koubaa"
        ],
        "published": "2024-03-24T10:57:08Z",
        "summary": "In this paper we introduce APL (Arabic Programming Language) that uses Large\nlanguage models (LLM) as semi-compiler to covert Arabic text code to python\ncode then run the code. Designing a full pipeline from the structure of the APL\ntext then a prompt (using prompt engineering) then running the prodcued python\ncode using PyRunner. This project has a three parts first python library, a\nplayground with simple interface and this research paper.",
        "pdf_link": "https://arxiv.org/pdf/2403.16087v1.pdf"
    },
    {
        "title": "Argument Quality Assessment in the Age of Instruction-Following Large Language Models",
        "authors": [
            "Henning Wachsmuth",
            "Gabriella Lapesa",
            "Elena Cabrio",
            "Anne Lauscher",
            "Joonsuk Park",
            "Eva Maria Vecchi",
            "Serena Villata",
            "Timon Ziegenbein"
        ],
        "published": "2024-03-24T10:43:21Z",
        "summary": "The computational treatment of arguments on controversial issues has been\nsubject to extensive NLP research, due to its envisioned impact on opinion\nformation, decision making, writing education, and the like. A critical task in\nany such application is the assessment of an argument's quality - but it is\nalso particularly challenging. In this position paper, we start from a brief\nsurvey of argument quality research, where we identify the diversity of quality\nnotions and the subjectiveness of their perception as the main hurdles towards\nsubstantial progress on argument quality assessment. We argue that the\ncapabilities of instruction-following large language models (LLMs) to leverage\nknowledge across contexts enable a much more reliable assessment. Rather than\njust fine-tuning LLMs towards leaderboard chasing on assessment tasks, they\nneed to be instructed systematically with argumentation theories and scenarios\nas well as with ways to solve argument-related problems. We discuss the\nreal-world opportunities and ethical issues emerging thereby.",
        "pdf_link": "https://arxiv.org/pdf/2403.16084v1.pdf"
    },
    {
        "title": "Qibo: A Large Language Model for Traditional Chinese Medicine",
        "authors": [
            "Heyi Zhang",
            "Xin Wang",
            "Zhaopeng Meng",
            "Yongzhe Jia",
            "Dawei Xu"
        ],
        "published": "2024-03-24T07:48:05Z",
        "summary": "In the field of Artificial Intelligence, Large Language Models (LLMs) have\ndemonstrated significant advances in user intent understanding and response in\na number of specialized domains, including medicine, law, and finance. However,\nin the unique domain of traditional Chinese medicine (TCM), the performance\nenhancement of LLMs is challenged by the essential differences between its\ntheories and modern medicine, as well as the lack of specialized corpus\nresources. In this paper, we aim to construct and organize a professional\ncorpus in the field of TCM, to endow the large model with professional\nknowledge that is characteristic of TCM theory, and to successfully develop the\nQibo model based on LLaMA, which is the first LLM in the field of TCM to\nundergo a complete training process from pre-training to Supervised Fine-Tuning\n(SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for\nevaluating the performance of LLMs, which is a specialized tool for evaluating\nthe performance of LLMs in the TCM domain. This tool will provide an important\nbasis for quantifying and comparing the understanding and application\ncapabilities of different models in the field of traditional Chinese medicine,\nand provide guidance for future research directions and practical applications\nof intelligent assistants for traditional Chinese medicine. Finally, we\nconducted sufficient experiments to prove that Qibo has good performance in the\nfield of traditional Chinese medicine.",
        "pdf_link": "https://arxiv.org/pdf/2403.16056v1.pdf"
    },
    {
        "title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
        "authors": [
            "Qin Liu",
            "Fei Wang",
            "Nan Xu",
            "Tianyi Yan",
            "Tao Meng",
            "Muhao Chen"
        ],
        "published": "2024-03-24T06:49:07Z",
        "summary": "Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.",
        "pdf_link": "https://arxiv.org/pdf/2403.16038v1.pdf"
    },
    {
        "title": "CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering",
        "authors": [
            "Hongbin Na"
        ],
        "published": "2024-03-24T04:34:34Z",
        "summary": "The recent advancements in artificial intelligence highlight the potential of\nlanguage models in psychological health support. While models trained on data\nfrom mental health service platform have achieved preliminary success,\nchallenges persist in areas such as data scarcity, quality, and ensuring a\nsolid foundation in psychological techniques. To address these challenges, this\nstudy introduces a novel approach to enhance the precision and efficacy of\npsychological support through large language models. Specifically, we design a\nspecific prompt derived from principles of Cognitive Behavioral Therapy (CBT)\nand have generated the CBT QA dataset, specifically for Chinese psychological\nhealth Q&A based on CBT structured intervention strategies. Unlike previous\nmethods, our dataset emphasizes professional and structured response. Utilizing\nthis dataset, we fine-tuned the large language model, giving birth to CBT-LLM,\nthe large-scale language model specifically designed for Cognitive Behavioral\nTherapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in\ngenerating structured, professional, and highly relevant responses in\npsychological health support tasks, showcasing its practicality and quality.\nThe model is available on Hugging Face:\nhttps://huggingface.co/Hongbin37/CBT-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.16008v1.pdf"
    },
    {
        "title": "LlamBERT: Large-scale low-cost data annotation in NLP",
        "authors": [
            "B\u00e1lint Csan\u00e1dy",
            "Lajos Muzsai",
            "P\u00e9ter Vedres",
            "Zolt\u00e1n N\u00e1dasdy",
            "Andr\u00e1s Luk\u00e1cs"
        ],
        "published": "2024-03-23T21:54:34Z",
        "summary": "Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable\nproficiency in a wide range of natural language processing (NLP) tasks. Despite\ntheir effectiveness, the high costs associated with their use pose a challenge.\nWe present LlamBERT, a hybrid approach that leverages LLMs to annotate a small\nsubset of large, unlabeled databases and uses the results for fine-tuning\ntransformer encoders like BERT and RoBERTa. This strategy is evaluated on two\ndiverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our\nresults indicate that the LlamBERT approach slightly compromises on accuracy\nwhile offering much greater cost-effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.15938v1.pdf"
    },
    {
        "title": "Leveraging Zero-Shot Prompting for Efficient Language Model Distillation",
        "authors": [
            "Lukas V\u00f6ge",
            "Vincent Gurgul",
            "Stefan Lessmann"
        ],
        "published": "2024-03-23T16:51:52Z",
        "summary": "This paper introduces a novel approach for efficiently distilling LLMs into\nsmaller, application-specific models, significantly reducing operational costs\nand manual labor. Addressing the challenge of deploying computationally\nintensive LLMs in specific applications or edge devices, this technique\nutilizes LLMs' reasoning capabilities to generate labels and natural language\nrationales for unlabeled data. Our approach enhances both finetuning and\ndistillation by employing a multi-task training framework where student models\nmimic these rationales alongside teacher predictions. Key contributions include\nthe employment of zero-shot prompting to elicit teacher model rationales,\nreducing the necessity for handcrafted few-shot examples and lowering the\noverall token count required, which directly translates to cost savings given\nthe pay-per-token billing model of major tech companies' LLM APIs.\nAdditionally, the paper investigates the impact of explanation properties on\ndistillation efficiency, demonstrating that minimal performance loss occurs\neven when rationale augmentation is not applied across the entire dataset,\nfacilitating further reductions of tokens. This research marks a step toward\nthe efficient training of task-specific models with minimal human intervention,\noffering substantial cost-savings while maintaining, or even enhancing,\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2403.15886v1.pdf"
    },
    {
        "title": "TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions",
        "authors": [
            "Gyubok Lee",
            "Woosog Chay",
            "Seonhee Cho",
            "Edward Choi"
        ],
        "published": "2024-03-23T16:12:52Z",
        "summary": "Recent advances in large language models (LLMs) have led to significant\nimprovements in translating natural language questions into SQL queries. While\nachieving high accuracy in SQL generation is crucial, little is known about the\nextent to which these text-to-SQL models can reliably handle diverse types of\nquestions encountered during real-world deployment, including unanswerable\nones. To explore this aspect, we present TrustSQL, a new benchmark designed to\nassess the reliability of text-to-SQL models in both single-database and\ncross-database settings. The benchmark tasks models with providing one of two\noutcomes: 1) SQL prediction; or 2) abstention from making a prediction, either\nwhen there is a potential error in the generated SQL or when faced with\nunanswerable questions. For model evaluation, we explore various modeling\napproaches specifically designed for this task. These include: 1) optimizing\nseparate models for answerability detection, SQL generation, and error\ndetection, which are then integrated into a single pipeline; and 2) developing\na unified approach that optimizes a single model to address the proposed task.\nExperimental results using our new reliability score show that addressing this\nchallenge involves many different areas of research and opens new avenues for\nmodel development. Nonetheless, none of the methods surpass the reliability\nperformance of the naive baseline, which abstains from answering all questions.",
        "pdf_link": "https://arxiv.org/pdf/2403.15879v1.pdf"
    },
    {
        "title": "Using Large Language Models for OntoClean-based Ontology Refinement",
        "authors": [
            "Yihang Zhao",
            "Neil Vetter",
            "Kaveh Aryan"
        ],
        "published": "2024-03-23T15:09:50Z",
        "summary": "This paper explores the integration of Large Language Models (LLMs) such as\nGPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing\non the OntoClean methodology. OntoClean, critical for assessing the\nmetaphysical quality of ontologies, involves a two-step process of assigning\nmeta-properties to classes and verifying a set of constraints. Manually\nconducting the first step proves difficult in practice, due to the need for\nphilosophical expertise and lack of consensus among ontologists. By employing\nLLMs with two prompting strategies, the study demonstrates that high accuracy\nin the labelling process can be achieved. The findings suggest the potential\nfor LLMs to enhance ontology refinement, proposing the development of plugin\nsoftware for ontology tools to facilitate this integration.",
        "pdf_link": "https://arxiv.org/pdf/2403.15864v1.pdf"
    },
    {
        "title": "When LLM-based Code Generation Meets the Software Development Process",
        "authors": [
            "Feng Lin",
            "Dong Jae Kim",
            "Tse-Husn",
            "Chen"
        ],
        "published": "2024-03-23T14:04:48Z",
        "summary": "Software process models play a pivotal role in fostering collaboration and\ncommunication within software teams, enabling them to tackle intricate\ndevelopment tasks effectively. This paper introduces LCG, a code generation\nframework inspired by established software engineering practices. LCG leverages\nmultiple Large Language Model (LLM) agents to emulate various software process\nmodels, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM\nagents specific roles such as requirement engineer, architect, developer,\ntester, and scrum master, mirroring typical development activities and\ncommunication patterns. Through collaborative efforts utilizing\nchain-of-thought and prompt composition techniques, the agents continuously\nrefine themselves to enhance code quality. Utilizing GPT3.5 as the underlying\nLLM and baseline (GPT), we evaluate LCG across four code generation benchmarks:\nHumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results indicate LCGScrum\noutperforms other models, achieving Pass@1 scores of 75.2, 65.5, 82.5, and 56.7\nin HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively - an average 15%\nimprovement over GPT. Analysis reveals distinct impacts of development\nactivities on generated code, with design and code reviews contributing to\nenhanced exception handling, while design, testing, and code reviews mitigate\ncode smells. Furthermore, temperature values exhibit negligible influence on\nPass@1 across all models. However, variations in Pass@1 are notable for\ndifferent GPT3.5 model versions, ranging from 5 to over 60 in HumanEval,\nhighlighting the stability of LCG across model versions. This stability\nunderscores the importance of adopting software process models to bolster the\nquality and consistency of LLM-generated code.",
        "pdf_link": "https://arxiv.org/pdf/2403.15852v1.pdf"
    },
    {
        "title": "ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning",
        "authors": [
            "Yiwen Chen",
            "Yuyao Ye",
            "Ziyi Chen",
            "Chuheng Zhang",
            "Marcelo H. Ang"
        ],
        "published": "2024-03-23T13:21:09Z",
        "summary": "Robotics learning highly relies on human expertise and efforts, such as\ndemonstrations, design of reward functions in reinforcement learning,\nperformance evaluation using human feedback, etc. However, reliance on human\nassistance can lead to expensive learning costs and make skill learning\ndifficult to scale. In this work, we introduce the Large Language Model\nSupervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims\nto replace human participation in the robot skill learning process with\nlarge-scale language models that incorporate reward function design and\nperformance evaluation. We provide evidence that our approach enables fully\nautonomous robot skill learning, capable of completing partial tasks without\nhuman intervention. Furthermore, we also analyze the limitations of this\napproach in task understanding and optimization stability.",
        "pdf_link": "https://arxiv.org/pdf/2403.15834v1.pdf"
    },
    {
        "title": "Computational Sentence-level Metrics Predicting Human Sentence Comprehension",
        "authors": [
            "Kun Sun",
            "Rong Wang"
        ],
        "published": "2024-03-23T12:19:49Z",
        "summary": "The majority of research in computational psycholinguistics has concentrated\non the processing of words. This study introduces innovative methods for\ncomputing sentence-level metrics using multilingual large language models. The\nmetrics developed sentence surprisal and sentence relevance and then are tested\nand compared to validate whether they can predict how humans comprehend\nsentences as a whole across languages. These metrics offer significant\ninterpretability and achieve high accuracy in predicting human sentence reading\nspeeds. Our results indicate that these computational sentence-level metrics\nare exceptionally effective at predicting and elucidating the processing\ndifficulties encountered by readers in comprehending sentences as a whole\nacross a variety of languages. Their impressive performance and generalization\ncapabilities provide a promising avenue for future research in integrating LLMs\nand cognitive science.",
        "pdf_link": "https://arxiv.org/pdf/2403.15822v1.pdf"
    },
    {
        "title": "AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving",
        "authors": [
            "Bin Gao",
            "Zhuomin He",
            "Puru Sharma",
            "Qingxuan Kang",
            "Djordje Jevdjic",
            "Junbo Deng",
            "Xingkun Yang",
            "Zhou Yu",
            "Pengfei Zuo"
        ],
        "published": "2024-03-23T10:42:49Z",
        "summary": "Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nfor executing multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes AttentionStore,\na new attention mechanism that enables the reuse of KV caches (i.e., attention\nreuse) across multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. AttentionStore maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nAttentionStore employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, AttentionStore\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, AttentionStore enables the saved KV caches to remain valid via\ndecoupling the positional encoding and effectively truncating the KV caches.\nExtensive experimental results demonstrate that AttentionStore significantly\ndecreases the time to the first token (TTFT) by up to 88%, improves the prompt\nprefilling throughput by 8.2$\\times$ for multi-turn conversations, and reduces\nthe end-to-end inference cost by up to 56%. For long sequence inference,\nAttentionStore reduces the TTFT by up to 95% and improves the prompt prefilling\nthroughput by 22$\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2403.19708v1.pdf"
    },
    {
        "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
        "authors": [
            "Youyang Qu",
            "Ming Ding",
            "Nan Sun",
            "Kanchana Thilakarathna",
            "Tianqing Zhu",
            "Dusit Niyato"
        ],
        "published": "2024-03-23T09:26:15Z",
        "summary": "Large Language Models (LLMs) are foundational to AI advancements,\nfacilitating applications like predictive text generation. Nonetheless, they\npose risks by potentially memorizing and disseminating sensitive, biased, or\ncopyrighted information from their vast datasets. Machine unlearning emerges as\na cutting-edge solution to mitigate these concerns, offering techniques for\nLLMs to selectively discard certain data. This paper reviews the latest in\nmachine unlearning for LLMs, introducing methods for the targeted forgetting of\ninformation to address privacy, ethical, and legal challenges without\nnecessitating full model retraining. It divides existing research into\nunlearning from unstructured/textual data and structured/classification data,\nshowcasing the effectiveness of these approaches in removing specific data\nwhile maintaining model efficacy. Highlighting the practicality of machine\nunlearning, this analysis also points out the hurdles in preserving model\nintegrity, avoiding excessive or insufficient data removal, and ensuring\nconsistent outputs, underlining the role of machine unlearning in advancing\nresponsible, ethical AI.",
        "pdf_link": "https://arxiv.org/pdf/2403.15779v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study",
        "authors": [
            "Matteo Esposito",
            "Francesco Palagiano"
        ],
        "published": "2024-03-23T07:59:30Z",
        "summary": "Preliminary security risk analysis (PSRA) provides a quick approach to\nidentify, evaluate and propose remeditation to potential risks in specific\nscenarios. The extensive expertise required for an effective PSRA and the\nsubstantial ammount of textual-related tasks hinder quick assessments in\nmission-critical contexts, where timely and prompt actions are essential. The\nspeed and accuracy of human experts in PSRA significantly impact response time.\nA large language model can quickly summarise information in less time than a\nhuman. To our knowledge, no prior study has explored the capabilities of\nfine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of\nFTM to assist practitioners in PSRA. We manually curated 141 representative\nsamples from over 50 mission-critical analyses archived by the industrial\ncontext team in the last five years.We compared the proficiency of the FTM\nversus seven human experts. Within the industrial context, our approach has\nproven successful in reducing errors in PSRA, hastening security risk\ndetection, and minimizing false positives and negatives. This translates to\ncost savings for the company by averting unnecessary expenses associated with\nimplementing unwarranted countermeasures. Therefore, experts can focus on more\ncomprehensive risk analysis, leveraging LLMs for an effective preliminary\nassessment within a condensed timeframe.",
        "pdf_link": "https://arxiv.org/pdf/2403.15756v1.pdf"
    },
    {
        "title": "Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models",
        "authors": [
            "Shuai Zhao",
            "Linchao Zhu",
            "Ruijie Quan",
            "Yi Yang"
        ],
        "published": "2024-03-23T06:36:32Z",
        "summary": "Web user data plays a central role in the ecosystem of pre-trained large\nlanguage models (LLMs) and their fine-tuned variants. Billions of data are\ncrawled from the web and fed to LLMs. How can \\textit{\\textbf{everyday web\nusers}} confirm if LLMs misuse their data without permission? In this work, we\nsuggest that users repeatedly insert personal passphrases into their documents,\nenabling LLMs to memorize them. These concealed passphrases in user documents,\nreferred to as \\textit{ghost sentences}, once they are identified in the\ngenerated content of LLMs, users can be sure that their data is used for\ntraining. To explore the effectiveness and usage of this copyrighting tool, we\ndefine the \\textit{user training data identification} task with ghost\nsentences. Multiple datasets from various sources at different scales are\ncreated and tested with LLMs of different sizes. For evaluation, we introduce a\nlast $k$ words verification manner along with two metrics: document and user\nidentification accuracy. In the specific case of instruction tuning of a 3B\nLLaMA model, 11 out of 16 users with ghost sentences identify their data within\nthe generation content. These 16 users contribute 383 examples to $\\sim$1.8M\ntraining documents. For continuing pre-training of a 1.1B TinyLlama model, 61\nout of 64 users with ghost sentences identify their data within the LLM output.\nThese 64 users contribute 1156 examples to $\\sim$10M training documents.",
        "pdf_link": "https://arxiv.org/pdf/2403.15740v1.pdf"
    },
    {
        "title": "LLMs Instruct LLMs:An Extraction and Editing Method",
        "authors": [
            "Xin Zhang",
            "Tianjie Ju",
            "Huijia Liang",
            "Ying Fu",
            "Qin Zhang"
        ],
        "published": "2024-03-23T06:03:36Z",
        "summary": "The interest in updating Large Language Models (LLMs) without retraining from\nscratch is substantial, yet it comes with some challenges.This is especially\ntrue for situations demanding complex reasoning with limited samples, a\nscenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation\nfor LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and\nRetrieval-Augmented Generation (RAG) are inadequate for this critical issue,\nparticularly evident in our exploration of a specific medical context that\nepitomize the PCRA-LLM's distinct needs.To address the issue, we propose a\nSequential Fusion method to incorporate knowledge from complex context into\nLLMs. This method employs a two-stage framework: initially, it leverages\ngeneral LLMs to construct knowledge graphs (KGs) for extracting knowledge from\ncomplex texts; subsequently, it updates the domain LLMs through knowledge edit.\nAccording to our method, the domain LLM achieved a 71.69\\% accuracy in question\nanswering tasks. Subsequently, we broadened our assessment to a novel dataset\nwe developed in the economics and management field, where our method realized a\n75\\% accuracy. These outcomes underline the efficacy and adaptability of our\napproach for PCRA-LLM across various domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.15736v1.pdf"
    },
    {
        "title": "Towards a RAG-based Summarization Agent for the Electron-Ion Collider",
        "authors": [
            "Karthik Suresh",
            "Neeltje Kackar",
            "Luke Schleck",
            "Cristiano Fanelli"
        ],
        "published": "2024-03-23T05:32:46Z",
        "summary": "The complexity and sheer volume of information encompassing documents,\npapers, data, and other resources from large-scale experiments demand\nsignificant time and effort to navigate, making the task of accessing and\nutilizing these varied forms of information daunting, particularly for new\ncollaborators and early-career scientists. To tackle this issue, a Retrieval\nAugmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under\ndevelopment. This AI-Agent not only condenses information but also effectively\nreferences relevant responses, offering substantial advantages for\ncollaborators. Our project involves a two-step approach: first, querying a\ncomprehensive vector database containing all pertinent experiment information;\nsecond, utilizing a Large Language Model (LLM) to generate concise summaries\nenriched with citations based on user queries and retrieved data. We describe\nthe evaluation methods that use RAG assessments (RAGAs) scoring mechanisms to\nassess the effectiveness of responses. Furthermore, we describe the concept of\nprompt template-based instruction-tuning which provides flexibility and\naccuracy in summarization. Importantly, the implementation relies on LangChain,\nwhich serves as the foundation of our entire workflow. This integration ensures\nefficiency and scalability, facilitating smooth deployment and accessibility\nfor various user groups within the Electron Ion Collider (EIC) community. This\ninnovative AI-driven framework not only simplifies the understanding of vast\ndatasets but also encourages collaborative participation, thereby empowering\nresearchers. As a demonstration, a web application has been developed to\nexplain each stage of the RAG Agent development in detail.",
        "pdf_link": "https://arxiv.org/pdf/2403.15729v2.pdf"
    },
    {
        "title": "Contact-aware Human Motion Generation from Textual Descriptions",
        "authors": [
            "Sihan Ma",
            "Qiong Cao",
            "Jing Zhang",
            "Dacheng Tao"
        ],
        "published": "2024-03-23T04:08:39Z",
        "summary": "This paper addresses the problem of generating 3D interactive human motion\nfrom text. Given a textual description depicting the actions of different body\nparts in contact with objects, we synthesize sequences of 3D body poses that\nare visually natural and physically plausible. Yet, this task poses a\nsignificant challenge due to the inadequate consideration of interactions by\nphysical contacts in both motion and textual descriptions, leading to unnatural\nand implausible sequences. To tackle this challenge, we create a novel dataset\nnamed RICH-CAT, representing ``Contact-Aware Texts'' constructed from the RICH\ndataset. RICH-CAT comprises high-quality motion, accurate human-object contact\nlabels, and detailed textual descriptions, encompassing over 8,500 motion-text\npairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel\napproach named CATMO for text-driven interactive human motion synthesis that\nexplicitly integrates human body contacts as evidence. We employ two VQ-VAE\nmodels to encode motion and body contact sequences into distinct yet\ncomplementary latent spaces and an intertwined GPT for generating human motions\nand contacts in a mutually conditioned manner. Additionally, we introduce a\npre-trained text encoder to learn textual embeddings that better discriminate\namong various contact types, allowing for more precise control over synthesized\nmotions and contacts. Our experiments demonstrate the superior performance of\nour approach compared to existing text-to-motion methods, producing stable,\ncontact-aware motion sequences. Code and data will be available for research\npurposes.",
        "pdf_link": "https://arxiv.org/pdf/2403.15709v1.pdf"
    },
    {
        "title": "FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models",
        "authors": [
            "Huaiwen Zhang",
            "Yu Chen",
            "Ming Wang",
            "Shi Feng"
        ],
        "published": "2024-03-23T03:32:26Z",
        "summary": "Emotional Support Conversation (ESC) is a typical dialogue that can\neffec-tively assist the user in mitigating emotional pressures. However, owing\nto the inherent subjectivity involved in analyzing emotions, current\nnon-artificial methodologies face challenges in effectively appraising the\nemo-tional support capability. These metrics exhibit a low correlation with\nhuman judgments. Concurrently, manual evaluation methods extremely will cause\nhigh costs. To solve these problems, we propose a novel model FEEL (Framework\nfor Evaluating Emotional Support Capability with Large Lan-guage Models),\nemploying Large Language Models (LLMs) as evaluators to assess emotional\nsupport capabilities. The model meticulously considers var-ious evaluative\naspects of ESC to apply a more comprehensive and accurate evaluation method for\nESC. Additionally, it employs a probability distribu-tion approach for a more\nstable result and integrates an ensemble learning strategy, leveraging multiple\nLLMs with assigned weights to enhance evalua-tion accuracy. To appraise the\nperformance of FEEL, we conduct extensive experiments on existing ESC model\ndialogues. Experimental results demon-strate our model exhibits a substantial\nenhancement in alignment with human evaluations compared to the baselines. Our\nsource code is available at https://github.com/Ansisy/FEEL.",
        "pdf_link": "https://arxiv.org/pdf/2403.15699v1.pdf"
    },
    {
        "title": "SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models",
        "authors": [
            "Mengqi Zhou",
            "Jun Hou",
            "Chuanchen Luo",
            "Yuxi Wang",
            "Zhaoxiang Zhang",
            "Junran Peng"
        ],
        "published": "2024-03-23T03:23:29Z",
        "summary": "Due to its great application potential, large-scale scene generation has\ndrawn extensive attention in academia and industry. Recent research employs\npowerful generative models to create desired scenes and achieves promising\nresults. However, most of these methods represent the scene using 3D primitives\n(e.g. point cloud or radiance field) incompatible with the industrial pipeline,\nwhich leads to a substantial gap between academic research and industrial\ndeployment. Procedural Controllable Generation (PCG) is an efficient technique\nfor creating scalable and high-quality assets, but it is unfriendly for\nordinary users as it demands profound domain expertise. To address these\nissues, we resort to using the large language model (LLM) to drive the\nprocedural modeling. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions.Specifically, the proposed\nmethod comprises two components, PCGBench and PCGPlanner. The former\nencompasses an extensive collection of accessible procedural assets and\nthousands of hand-craft API documents. The latter aims to generate executable\nactions for Blender to produce controllable and precise 3D assets guided by the\nuser's instructions. Our SceneX can generate a city spanning 2.5 km times 2.5\nkm with delicate layout and geometric structures, drastically reducing the time\ncost from several weeks for professional PCG engineers to just a few hours for\nan ordinary user. Extensive experiments demonstrated the capability of our\nmethod in controllable large-scale scene generation and editing, including\nasset placement and season translation.",
        "pdf_link": "https://arxiv.org/pdf/2403.15698v1.pdf"
    },
    {
        "title": "MixRED: A Mix-lingual Relation Extraction Dataset",
        "authors": [
            "Lingxing Kong",
            "Yougang Chu",
            "Zheng Ma",
            "Jianbing Zhang",
            "Liang He",
            "Jiajun Chen"
        ],
        "published": "2024-03-23T03:18:14Z",
        "summary": "Relation extraction is a critical task in the field of natural language\nprocessing with numerous real-world applications. Existing research primarily\nfocuses on monolingual relation extraction or cross-lingual enhancement for\nrelation extraction. Yet, there remains a significant gap in understanding\nrelation extraction in the mix-lingual (or code-switching) scenario, where\nindividuals intermix contents from different languages within sentences,\ngenerating mix-lingual content. Due to the lack of a dedicated dataset, the\neffectiveness of existing relation extraction models in such a scenario is\nlargely unexplored. To address this issue, we introduce a novel task of\nconsidering relation extraction in the mix-lingual scenario called MixRE and\nconstructing the human-annotated dataset MixRED to support this task. In\naddition to constructing the MixRED dataset, we evaluate both state-of-the-art\nsupervised models and large language models (LLMs) on MixRED, revealing their\nrespective advantages and limitations in the mix-lingual scenario. Furthermore,\nwe delve into factors influencing model performance within the MixRE task and\nuncover promising directions for enhancing the performance of both supervised\nmodels and LLMs in this novel task.",
        "pdf_link": "https://arxiv.org/pdf/2403.15696v1.pdf"
    },
    {
        "title": "EAGLE: A Domain Generalization Framework for AI-generated Text Detection",
        "authors": [
            "Amrita Bhattacharjee",
            "Raha Moraffah",
            "Joshua Garland",
            "Huan Liu"
        ],
        "published": "2024-03-23T02:44:20Z",
        "summary": "With the advancement in capabilities of Large Language Models (LLMs), one\nmajor step in the responsible and safe use of such LLMs is to be able to detect\ntext generated by these models. While supervised AI-generated text detectors\nperform well on text generated by older LLMs, with the frequent release of new\nLLMs, building supervised detectors for identifying text from such new models\nwould require new labeled training data, which is infeasible in practice. In\nthis work, we tackle this problem and propose a domain generalization framework\nfor the detection of AI-generated text from unseen target generators. Our\nproposed framework, EAGLE, leverages the labeled data that is available so far\nfrom older language models and learns features invariant across these\ngenerators, in order to detect text generated by an unknown target generator.\nEAGLE learns such domain-invariant features by combining the representational\npower of self-supervised contrastive learning with domain adversarial training.\nThrough our experiments we demonstrate how EAGLE effectively achieves\nimpressive performance in detecting text generated by unseen target generators,\nincluding recent state-of-the-art ones such as GPT-4 and Claude, reaching\ndetection scores of within 4.7% of a fully supervised detector.",
        "pdf_link": "https://arxiv.org/pdf/2403.15690v1.pdf"
    },
    {
        "title": "AI for Biomedicine in the Era of Large Language Models",
        "authors": [
            "Zhenyu Bi",
            "Sajib Acharjee Dip",
            "Daniel Hajialigol",
            "Sindhura Kommu",
            "Hanwen Liu",
            "Meng Lu",
            "Xuan Wang"
        ],
        "published": "2024-03-23T01:40:22Z",
        "summary": "The capabilities of AI for biomedicine span a wide spectrum, from the atomic\nlevel, where it solves partial differential equations for quantum systems, to\nthe molecular level, predicting chemical or protein structures, and further\nextending to societal predictions like infectious disease outbreaks. Recent\nadvancements in large language models, exemplified by models like ChatGPT, have\nshowcased significant prowess in natural language tasks, such as translating\nlanguages, constructing chatbots, and answering questions. When we consider\nbiomedical data, we observe a resemblance to natural language in terms of\nsequences: biomedical literature and health records presented as text,\nbiological sequences or sequencing data arranged in sequences, or sensor data\nlike brain signals as time series. The question arises: Can we harness the\npotential of recent large language models to drive biomedical knowledge\ndiscoveries? In this survey, we will explore the application of large language\nmodels to three crucial categories of biomedical data: 1) textual data, 2)\nbiological sequences, and 3) brain signals. Furthermore, we will delve into\nlarge language model challenges in biomedical research, including ensuring\ntrustworthiness, achieving personalization, and adapting to multi-modal data\nrepresentation",
        "pdf_link": "https://arxiv.org/pdf/2403.15673v1.pdf"
    },
    {
        "title": "SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning",
        "authors": [
            "Weizheng Wang",
            "Le Mao",
            "Ruiqi Wang",
            "Byung-Cheol Min"
        ],
        "published": "2024-03-22T23:12:28Z",
        "summary": "An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm",
        "pdf_link": "https://arxiv.org/pdf/2403.15648v1.pdf"
    },
    {
        "title": "Differentially Private Next-Token Prediction of Large Language Models",
        "authors": [
            "James Flemings",
            "Meisam Razaviyayn",
            "Murali Annavaram"
        ],
        "published": "2024-03-22T22:27:44Z",
        "summary": "Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly\nimportant. The most widely adopted technique to accomplish this is DP-SGD,\nwhich trains a model to guarantee Differential Privacy (DP). However, DP-SGD\noverestimates an adversary's capabilities in having white box access to the\nmodel and, as a result, causes longer training times and larger memory usage\nthan SGD. On the other hand, commercial LLM deployments are predominantly\ncloud-based; hence, adversarial access to LLMs is black-box. Motivated by these\nobservations, we present Private Mixing of Ensemble Distributions (PMixED): a\nprivate prediction protocol for next-token prediction that utilizes the\ninherent stochasticity of next-token sampling and a public model to achieve\nDifferential Privacy. We formalize this by introducing RD-mollifers which\nproject each of the model's output distribution from an ensemble of fine-tuned\nLLMs onto a set around a public LLM's output distribution, then average the\nprojected distributions and sample from it. Unlike DP-SGD which needs to\nconsider the model architecture during training, PMixED is model agnostic,\nwhich makes PMixED a very appealing solution for current deployments. Our\nresults show that PMixED achieves a stronger privacy guarantee than\nsample-level privacy and outperforms DP-SGD for privacy $\\epsilon = 8$ on\nlarge-scale datasets. Thus, PMixED offers a practical alternative to DP\ntraining methods for achieving strong generative utility without compromising\nprivacy.",
        "pdf_link": "https://arxiv.org/pdf/2403.15638v2.pdf"
    },
    {
        "title": "Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers",
        "authors": [
            "Sivana Hamer",
            "Marcelo d'Amorim",
            "Laurie Williams"
        ],
        "published": "2024-03-22T20:06:41Z",
        "summary": "Sonatype's 2023 report found that 97% of developers and security leads\nintegrate generative Artificial Intelligence (AI), particularly Large Language\nModels (LLMs), into their development process. Concerns about the security\nimplications of this trend have been raised. Developers are now weighing the\nbenefits and risks of LLMs against other relied-upon information sources, such\nas StackOverflow (SO), requiring empirical data to inform their choice. In this\nwork, our goal is to raise software developers awareness of the security\nimplications when selecting code snippets by empirically comparing the\nvulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an\nexisting Java dataset from SO with security-related questions and answers.\nThen, we asked ChatGPT the same SO questions, gathering the generated code for\ncomparison. After curating the dataset, we analyzed the number and types of\nCommon Weakness Enumeration (CWE) vulnerabilities of 108 snippets from each\nplatform using CodeQL. ChatGPT-generated code contained 248 vulnerabilities\ncompared to the 302 vulnerabilities found in SO snippets, producing 20% fewer\nvulnerabilities with a statistically significant difference. Additionally,\nChatGPT generated 19 types of CWE, fewer than the 22 found in SO. Our findings\nsuggest developers are under-educated on insecure code propagation from both\nplatforms, as we found 274 unique vulnerabilities and 25 types of CWE. Any code\ncopied and pasted, created by AI or humans, cannot be trusted blindly,\nrequiring good software engineering practices to reduce risk. Future work can\nhelp minimize insecure code propagation from any platform.",
        "pdf_link": "https://arxiv.org/pdf/2403.15600v1.pdf"
    },
    {
        "title": "Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",
        "authors": [
            "Cristina Zuheros",
            "David Herrera-Poyatos",
            "Rosana Montes",
            "Francisco Herrera"
        ],
        "published": "2024-03-22T19:21:44Z",
        "summary": "Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies.",
        "pdf_link": "https://arxiv.org/pdf/2403.15587v1.pdf"
    },
    {
        "title": "Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",
        "authors": [
            "Aashish Ghimire",
            "James Prather",
            "John Edwards"
        ],
        "published": "2024-03-22T19:21:29Z",
        "summary": "The rapid advancement of artificial intelligence (AI) and the expanding\nintegration of large language models (LLMs) have ignited a debate about their\napplication in education. This study delves into university instructors'\nexperiences and attitudes toward AI language models, filling a gap in the\nliterature by analyzing educators' perspectives on AI's role in the classroom\nand its potential impacts on teaching and learning. The objective of this\nresearch is to investigate the level of awareness, overall sentiment\ntowardsadoption, and the factors influencing these attitudes for LLMs and\ngenerative AI-based tools in higher education. Data was collected through a\nsurvey using a Likert scale, which was complemented by follow-up interviews to\ngain a more nuanced understanding of the instructors' viewpoints. The collected\ndata was processed using statistical and thematic analysis techniques. Our\nfindings reveal that educators are increasingly aware of and generally positive\ntowards these tools. We find no correlation between teaching style and attitude\ntoward generative AI. Finally, while CS educators show far more confidence in\ntheir technical understanding of generative AI tools and more positivity\ntowards them than educators in other fields, they show no more confidence in\ntheir ability to detect AI-generated work.",
        "pdf_link": "https://arxiv.org/pdf/2403.15586v1.pdf"
    },
    {
        "title": "MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis",
        "authors": [
            "Mai A. Shaaban",
            "Adnan Khan",
            "Mohammad Yaqub"
        ],
        "published": "2024-03-22T19:19:51Z",
        "summary": "Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces MedPromptX, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX",
        "pdf_link": "https://arxiv.org/pdf/2403.15585v3.pdf"
    },
    {
        "title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP",
        "authors": [
            "Beichen Zhang",
            "Pan Zhang",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Jiaqi Wang"
        ],
        "published": "2024-03-22T17:58:16Z",
        "summary": "Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for\nzero-shot classification, text-image retrieval, and text-image generation by\naligning image and text modalities. Despite its widespread adoption, a\nsignificant limitation of CLIP lies in the inadequate length of text input. The\nlength of the text token is restricted to 77, and an empirical study shows the\nactual effective length is even less than 20. This prevents CLIP from handling\ndetailed descriptions, limiting its applications for image retrieval and\ntext-to-image generation with extensive prerequisites. To this end, we propose\nLong-CLIP as a plug-and-play alternative to CLIP that supports long-text input,\nretains or even surpasses its zero-shot generalizability, and aligns the CLIP\nlatent space, making it readily replace CLIP without any further adaptation in\ndownstream frameworks. Nevertheless, achieving this goal is far from\nstraightforward, as simplistic fine-tuning can result in a significant\ndegradation of CLIP's performance. Moreover, substituting the text encoder with\na language model supporting longer contexts necessitates pretraining with vast\namounts of data, incurring significant expenses. Accordingly, Long-CLIP\nintroduces an efficient fine-tuning solution on CLIP with two novel strategies\ndesigned to maintain the original capabilities, including (1) a\nknowledge-preserved stretching of positional embedding and (2) a primary\ncomponent matching of CLIP features. With leveraging just one million extra\nlong text-image pairs, Long-CLIP has shown the superiority to CLIP for about\n20% in long caption text-image retrieval and 6% in traditional text-image\nretrieval tasks, e.g., COCO and Flickr30k. Furthermore, Long-CLIP offers\nenhanced capabilities for generating images from detailed text descriptions by\nreplacing CLIP in a plug-and-play manner.",
        "pdf_link": "https://arxiv.org/pdf/2403.15378v1.pdf"
    },
    {
        "title": "Can large language models explore in-context?",
        "authors": [
            "Akshay Krishnamurthy",
            "Keegan Harris",
            "Dylan J. Foster",
            "Cyril Zhang",
            "Aleksandrs Slivkins"
        ],
        "published": "2024-03-22T17:50:43Z",
        "summary": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2403.15371v1.pdf"
    },
    {
        "title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
        "authors": [
            "Abdur Rahman Bin Md Faizullah",
            "Ashok Urlana",
            "Rahul Mishra"
        ],
        "published": "2024-03-22T17:31:43Z",
        "summary": "Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called LimGen, encompassing 4068 research papers and their associated\nlimitations from the ACL anthology. We investigate several approaches to\nharness large language models (LLMs) for producing suggestive limitations, by\nthoroughly examining the related challenges, practical insights, and potential\nopportunities. Our LimGen dataset and code can be accessed at\nhttps://github.com/armbf/LimGen.",
        "pdf_link": "https://arxiv.org/pdf/2403.15529v1.pdf"
    },
    {
        "title": "Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs",
        "authors": [
            "Yiliang Zhou",
            "Hanley Ong",
            "Patrick Kennedy",
            "Carol Wu",
            "Jacob Kazam",
            "Keith Hentel",
            "Adam Flanders",
            "George Shih",
            "Yifan Peng"
        ],
        "published": "2024-03-22T17:27:18Z",
        "summary": "The study examines the application of GPT-4V, a multi-modal large language\nmodel equipped with visual recognition, in detecting radiological findings from\na set of 100 chest radiographs and suggests that GPT-4V is currently not ready\nfor real-world diagnostic usage in interpreting chest radiographs.",
        "pdf_link": "https://arxiv.org/pdf/2403.15528v2.pdf"
    },
    {
        "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
        "authors": [
            "Ryan Teehan",
            "Brenden Lake",
            "Mengye Ren"
        ],
        "published": "2024-03-22T17:26:05Z",
        "summary": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.",
        "pdf_link": "https://arxiv.org/pdf/2403.15362v1.pdf"
    },
    {
        "title": "Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization",
        "authors": [
            "Jimyeong Kim",
            "Jungwon Park",
            "Wonjong Rhee"
        ],
        "published": "2024-03-22T16:35:38Z",
        "summary": "In text-to-image personalization, a timely and crucial challenge is the\ntendency of generated images overfitting to the biases present in the reference\nimages. We initiate our study with a comprehensive categorization of the biases\ninto background, nearby-object, tied-object, substance (in style\nre-contextualization), and pose biases. These biases manifest in the generated\nimages due to their entanglement into the subject embedding. This undesired\nembedding entanglement not only results in the reflection of biases from the\nreference images into the generated images but also notably diminishes the\nalignment of the generated images with the given generation prompt. To address\nthis challenge, we propose SID~(Selectively Informative Description), a text\ndescription strategy that deviates from the prevalent approach of only\ncharacterizing the subject's class identification. SID is generated utilizing\nmultimodal GPT-4 and can be seamlessly integrated into optimization-based\nmodels. We present comprehensive experimental results along with analyses of\ncross-attention maps, subject-alignment, non-subject-disentanglement, and\ntext-alignment.",
        "pdf_link": "https://arxiv.org/pdf/2403.15330v1.pdf"
    },
    {
        "title": "Sphere Neural-Networks for Rational Reasoning",
        "authors": [
            "Tiansi Dong",
            "Mateja Jamnik",
            "Pietro Li\u00f2"
        ],
        "published": "2024-03-22T15:44:59Z",
        "summary": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like question-answering,\nand also by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a minimalist qualitative\nextension by generalising computational building blocks from vectors to\nspheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning\nthrough model construction and inspection, and develop SphNN for syllogistic\nreasoning, a microcosm of human rationality. Instead of training data, SphNN\nuses a neuro-symbolic transition map of neighbourhood spatial relations to\nguide transformations from the current sphere configuration towards the target.\nSphNN is the first neural model that can determine the validity of long-chained\nsyllogistic reasoning in one epoch by constructing sphere configurations as\nEuler diagrams, with the worst computational complexity of O(N^2). SphNN can\nevolve into various types of reasoning, such as spatio-temporal reasoning,\nlogical reasoning with negation and disjunction, event reasoning,\nneuro-symbolic reasoning, and humour understanding (the highest level of\ncognition). All these suggest a new kind of Herbert A. Simon's scissors with\ntwo neural blades. SphNNs will tremendously enhance interdisciplinary\ncollaborations to develop the two neural blades and realise deterministic\nneural reasoning and human-bounded rationality and elevate LLMs to reliable\npsychological AI. This work suggests that the non-zero radii of spheres are the\nmissing components that prevent traditional deep-learning systems from reaching\nthe realm of rational reasoning and cause LLMs to be trapped in the swamp of\nhallucination.",
        "pdf_link": "https://arxiv.org/pdf/2403.15297v1.pdf"
    },
    {
        "title": "Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review",
        "authors": [
            "Jinge Wang",
            "Zien Cheng",
            "Qiuming Yao",
            "Li Liu",
            "Dong Xu",
            "Gangqing Hu"
        ],
        "published": "2024-03-22T15:16:23Z",
        "summary": "The year 2023 marked a significant surge in the exploration of applying large\nlanguage model (LLM) chatbots, notably ChatGPT, across various disciplines. We\nsurveyed the applications of ChatGPT in various sectors of bioinformatics and\nbiomedical informatics throughout the year, covering omics, genetics,\nbiomedical text mining, drug discovery, biomedical image understanding,\nbioinformatics programming, and bioinformatics education. Our survey delineates\nthe current strengths and limitations of this chatbot in bioinformatics and\noffers insights into potential avenues for future development.",
        "pdf_link": "https://arxiv.org/pdf/2403.15274v1.pdf"
    },
    {
        "title": "Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs",
        "authors": [
            "Xiaobin Zhang",
            "Liangjun Zang",
            "Qianwen Liu",
            "Shuchong Wei",
            "Songlin Hu"
        ],
        "published": "2024-03-22T15:16:10Z",
        "summary": "Event temporal relation (TempRel) is a primary subject of the event relation\nextraction task. However, the inherent ambiguity of TempRel increases the\ndifficulty of the task. With the rise of prompt engineering, it is important to\ndesign effective prompt templates and verbalizers to extract relevant\nknowledge. The traditional manually designed templates struggle to extract\nprecise temporal knowledge. This paper introduces a novel retrieval-augmented\nTempRel extraction approach, leveraging knowledge retrieved from large language\nmodels (LLMs) to enhance prompt templates and verbalizers. Our method\ncapitalizes on the diverse capabilities of various LLMs to generate a wide\narray of ideas for template and verbalizer design. Our proposed method fully\nexploits the potential of LLMs for generation tasks and contributes more\nknowledge to our design. Empirical evaluations across three widely recognized\ndatasets demonstrate the efficacy of our method in improving the performance of\nevent temporal relation extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.15273v1.pdf"
    },
    {
        "title": "Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models",
        "authors": [
            "Huanxuan Liao",
            "Shizhu He",
            "Yao Xu",
            "Yuanzhe Zhang",
            "Kang Liu",
            "Shengping Liu",
            "Jun Zhao"
        ],
        "published": "2024-03-22T15:06:45Z",
        "summary": "Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been\nproposed to enhance the knowledge required for question answering over Large\nLanguage Models (LLMs). However, the former depends on external resources, and\nboth require incorporating the explicit documents into the context, which\nresults in longer contexts that lead to more resource consumption. Recent works\nindicate that LLMs have modeled rich knowledge, albeit not effectively\ntriggered or activated. Inspired by this, we propose a novel\nknowledge-augmented framework, Imagination-Augmented-Generation (IAG), which\nsimulates the human capacity to compensate for knowledge deficits while\nanswering questions solely through imagination, without relying on external\nresources. Guided by IAG, we propose an imagine richer context method for\nquestion answering (IMcQA), which obtains richer context through the following\ntwo modules: explicit imagination by generating a short dummy document with\nlong context compress and implicit imagination with HyperNetwork for generating\nadapter weights. Experimental results on three datasets demonstrate that IMcQA\nexhibits significant advantages in both open-domain and closed-book settings,\nas well as in both in-distribution performance and out-of-distribution\ngeneralizations. Our code will be available at\nhttps://github.com/Xnhyacinth/IAG.",
        "pdf_link": "https://arxiv.org/pdf/2403.15268v2.pdf"
    },
    {
        "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
        "authors": [
            "Kun Sun",
            "Rong Wang",
            "Haitao Liu",
            "Anders S\u00f8gaard"
        ],
        "published": "2024-03-22T14:47:35Z",
        "summary": "Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.",
        "pdf_link": "https://arxiv.org/pdf/2403.15250v1.pdf"
    },
    {
        "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions",
        "authors": [
            "Orion Weller",
            "Benjamin Chang",
            "Sean MacAvaney",
            "Kyle Lo",
            "Arman Cohan",
            "Benjamin Van Durme",
            "Dawn Lawrie",
            "Luca Soldaini"
        ],
        "published": "2024-03-22T14:42:29Z",
        "summary": "Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.",
        "pdf_link": "https://arxiv.org/pdf/2403.15246v1.pdf"
    },
    {
        "title": "An Exploratory Investigation into Code License Infringements in Large Language Model Training Datasets",
        "authors": [
            "Jonathan Katzy",
            "R\u0103zvan-Mihai Popescu",
            "Arie van Deursen",
            "Maliheh Izadi"
        ],
        "published": "2024-03-22T14:23:21Z",
        "summary": "Does the training of large language models potentially infringe upon code\nlicenses? Furthermore, are there any datasets available that can be safely used\nfor training these models without violating such licenses? In our study, we\nassess the current trends in the field and the importance of incorporating code\ninto the training of large language models. Additionally, we examine publicly\navailable datasets to see whether these models can be trained on them without\nthe risk of legal issues in the future. To accomplish this, we compiled a list\nof 53 large language models trained on file-level code. We then extracted their\ndatasets and analyzed how much they overlap with a dataset we created,\nconsisting exclusively of strong copyleft code.\n  Our analysis revealed that every dataset we examined contained license\ninconsistencies, despite being selected based on their associated repository\nlicenses. We analyzed a total of 514 million code files, discovering 38 million\nexact duplicates present in our strong copyleft dataset. Additionally, we\nexamined 171 million file-leading comments, identifying 16 million with strong\ncopyleft licenses and another 11 million comments that discouraged copying\nwithout explicitly mentioning a license. Based on the findings of our study,\nwhich highlights the pervasive issue of license inconsistencies in large\nlanguage models trained on code, our recommendation for both researchers and\nthe community is to prioritize the development and adoption of best practices\nfor dataset creation and management.",
        "pdf_link": "https://arxiv.org/pdf/2403.15230v1.pdf"
    },
    {
        "title": "Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models",
        "authors": [
            "Qiong Wu",
            "Weihao Ye",
            "Yiyi Zhou",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published": "2024-03-22T14:20:34Z",
        "summary": "In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN",
        "pdf_link": "https://arxiv.org/pdf/2403.15226v1.pdf"
    },
    {
        "title": "InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection",
        "authors": [
            "Thales Bertaglia",
            "Lily Heisig",
            "Rishabh Kaushal",
            "Adriana Iamnitchi"
        ],
        "published": "2024-03-22T13:58:42Z",
        "summary": "Large Language Models (LLMs) raise concerns about lowering the cost of\ngenerating texts that could be used for unethical or illegal purposes,\nespecially on social media. This paper investigates the promise of such models\nto help enforce legal requirements related to the disclosure of sponsored\ncontent online. We investigate the use of LLMs for generating synthetic\nInstagram captions with two objectives: The first objective (fidelity) is to\nproduce realistic synthetic datasets. For this, we implement content-level and\nnetwork-level metrics to assess whether synthetic captions are realistic. The\nsecond objective (utility) is to create synthetic data that is useful for\nsponsored content detection. For this, we evaluate the effectiveness of the\ngenerated synthetic data for training classifiers to identify undisclosed\nadvertisements on Instagram. Our investigations show that the objectives of\nfidelity and utility may conflict and that prompt engineering is a useful but\ninsufficient strategy. Additionally, we find that while individual synthetic\nposts may appear realistic, collectively they lack diversity, topic\nconnectivity, and realistic user interaction patterns.",
        "pdf_link": "https://arxiv.org/pdf/2403.15214v1.pdf"
    },
    {
        "title": "MSCoTDet: Language-driven Multi-modal Fusion for Improved Multispectral Pedestrian Detection",
        "authors": [
            "Taeheon Kim",
            "Sangyun Chung",
            "Damin Yeom",
            "Youngjoon Yu",
            "Hak Gu Kim",
            "Yong Man Ro"
        ],
        "published": "2024-03-22T13:50:27Z",
        "summary": "Multispectral pedestrian detection is attractive for around-the-clock\napplications due to the complementary information between RGB and thermal\nmodalities. However, current models often fail to detect pedestrians in obvious\ncases, especially due to the modality bias learned from statistically biased\ndatasets. From these problems, we anticipate that maybe understanding the\ncomplementary information itself is difficult to achieve from vision-only\nmodels. Accordingly, we propose a novel Multispectral Chain-of-Thought\nDetection (MSCoTDet) framework, which incorporates Large Language Models (LLMs)\nto understand the complementary information at the semantic level and further\nenhance the fusion process. Specifically, we generate text descriptions of the\npedestrian in each RGB and thermal modality and design a Multispectral\nChain-of-Thought (MSCoT) prompting, which models a step-by-step process to\nfacilitate cross-modal reasoning at the semantic level and perform accurate\ndetection. Moreover, we design a Language-driven Multi-modal Fusion (LMF)\nstrategy that enables fusing vision-driven and language-driven detections.\nExtensive experiments validate that MSCoTDet improves multispectral pedestrian\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2403.15209v1.pdf"
    },
    {
        "title": "Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study",
        "authors": [
            "Tim van Dam",
            "Frank van der Heijden",
            "Philippe de Bekker",
            "Berend Nieuwschepen",
            "Marc Otten",
            "Maliheh Izadi"
        ],
        "published": "2024-03-22T13:13:13Z",
        "summary": "Language model-based code completion models have quickly grown in use,\nhelping thousands of developers write code in many different programming\nlanguages. However, research on code completion models typically focuses on\nimperative languages such as Python and JavaScript, which results in a lack of\nrepresentation for functional programming languages. Consequently, these models\noften perform poorly on functional languages such as Haskell. To investigate\nwhether this can be alleviated, we evaluate the performance of two language\nmodels for code, CodeGPT and UniXcoder, on the functional programming language\nHaskell. We fine-tune and evaluate the models on Haskell functions sourced from\na publicly accessible Haskell dataset on HuggingFace. Additionally, we manually\nevaluate the models using our novel translated HumanEval dataset. Our automatic\nevaluation shows that knowledge of imperative programming languages in the\npre-training of LLMs may not transfer well to functional languages, but that\ncode completion on functional languages is feasible. Consequently, this shows\nthe need for more high-quality Haskell datasets. A manual evaluation on\nHumanEval-Haskell indicates CodeGPT frequently generates empty predictions and\nextra comments, while UniXcoder more often produces incomplete or incorrect\npredictions. Finally, we release HumanEval-Haskell, along with the fine-tuned\nmodels and all code required to reproduce our experiments on GitHub\n(https://github.com/AISE-TUDelft/HaskellCCEval).",
        "pdf_link": "https://arxiv.org/pdf/2403.15185v1.pdf"
    },
    {
        "title": "CACA Agent: Capability Collaboration based AI Agent",
        "authors": [
            "Peng Xu",
            "Haoran Wang",
            "Chuang Wang",
            "Xu Liu"
        ],
        "published": "2024-03-22T11:42:47Z",
        "summary": "As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.",
        "pdf_link": "https://arxiv.org/pdf/2403.15137v1.pdf"
    },
    {
        "title": "Text clustering with LLM embeddings",
        "authors": [
            "Alina Petukhova",
            "Joao P. Matos-Carvalho",
            "Nuno Fachada"
        ],
        "published": "2024-03-22T11:08:48Z",
        "summary": "Text clustering is an important approach for organising the growing amount of\ndigital content, helping to structure and find hidden patterns in uncategorised\ndata. In this research, we investigated how different textual embeddings -\nparticularly those used in large language models (LLMs) - and clustering\nalgorithms affect how text datasets are clustered. A series of experiments were\nconducted to assess how embeddings influence clustering results, the role\nplayed by dimensionality reduction through summarisation, and embedding size\nadjustment. Results reveal that LLM embeddings excel at capturing the nuances\nof structured language, while BERT leads the lightweight options in\nperformance. In addition, we find that increasing embedding dimensionality and\nsummarisation techniques do not uniformly improve clustering efficiency,\nsuggesting that these strategies require careful analysis to use in real-life\nmodels. These results highlight a complex balance between the need for nuanced\ntext representation and computational feasibility in text clustering\napplications. This study extends traditional text clustering frameworks by\nincorporating embeddings from LLMs, thereby paving the way for improved\nmethodologies and opening new avenues for future research in various types of\ntextual analysis.",
        "pdf_link": "https://arxiv.org/pdf/2403.15112v1.pdf"
    },
    {
        "title": "Construction of a Japanese Financial Benchmark for Large Language Models",
        "authors": [
            "Masanori Hirano"
        ],
        "published": "2024-03-22T09:40:27Z",
        "summary": "With the recent development of large language models (LLMs), models that\nfocus on certain domains and languages have been discussed for their necessity.\nThere is also a growing need for benchmarks to evaluate the performance of\ncurrent LLMs in each domain. Therefore, in this study, we constructed a\nbenchmark comprising multiple tasks specific to the Japanese and financial\ndomains and performed benchmark measurements on some models. Consequently, we\nconfirmed that GPT-4 is currently outstanding, and that the constructed\nbenchmarks function effectively. According to our analysis, our benchmark can\ndifferentiate benchmark scores among models in all performance ranges by\ncombining tasks with different difficulties.",
        "pdf_link": "https://arxiv.org/pdf/2403.15062v1.pdf"
    },
    {
        "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
        "authors": [
            "Nicholas Lee",
            "Thanakul Wattanawong",
            "Sehoon Kim",
            "Karttikeya Mangalam",
            "Sheng Shen",
            "Gopala Anumanchipali",
            "Michael W. Mahoney",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "published": "2024-03-22T08:57:07Z",
        "summary": "Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a LLaMA2-7B student model.",
        "pdf_link": "https://arxiv.org/pdf/2403.15042v1.pdf"
    },
    {
        "title": "Magic for the Age of Quantized DNNs",
        "authors": [
            "Yoshihide Sawada",
            "Ryuji Saiin",
            "Kazuma Suetake"
        ],
        "published": "2024-03-22T07:21:09Z",
        "summary": "Recently, the number of parameters in DNNs has explosively increased, as\nexemplified by LLMs (Large Language Models), making inference on small-scale\ncomputers more difficult. Model compression technology is, therefore, essential\nfor integration into products. In this paper, we propose a method of\nquantization-aware training. We introduce a novel normalization (Layer-Batch\nNormalization) that is independent of the mini-batch size and does not require\nany additional computation cost during inference. Then, we quantize the weights\nby the scaled round-clip function with the weight standardization. We also\nquantize activation functions using the same function and apply surrogate\ngradients to train the model with both quantized weights and the quantized\nactivation functions. We call this method Magic for the age of Quantised DNNs\n(MaQD). Experimental results show that our quantization method can be achieved\nwith minimal accuracy degradation.",
        "pdf_link": "https://arxiv.org/pdf/2403.14999v1.pdf"
    },
    {
        "title": "Risk and Response in Large Language Models: Evaluating Key Threat Categories",
        "authors": [
            "Bahareh Harandizadeh",
            "Abel Salinas",
            "Fred Morstatter"
        ],
        "published": "2024-03-22T06:46:40Z",
        "summary": "This paper explores the pressing issue of risk assessment in Large Language\nModels (LLMs) as they become increasingly prevalent in various applications.\nFocusing on how reward models, which are designed to fine-tune pretrained LLMs\nto align with human values, perceive and categorize different types of risks,\nwe delve into the challenges posed by the subjective nature of preference-based\ntraining data. By utilizing the Anthropic Red-team dataset, we analyze major\nrisk categories, including Information Hazards, Malicious Uses, and\nDiscrimination/Hateful content. Our findings indicate that LLMs tend to\nconsider Information Hazards less harmful, a finding confirmed by a specially\ndeveloped regression model. Additionally, our analysis shows that LLMs respond\nless stringently to Information Hazards compared to other risks. The study\nfurther reveals a significant vulnerability of LLMs to jailbreaking attacks in\nInformation Hazard scenarios, highlighting a critical security concern in LLM\nrisk assessment and emphasizing the need for improved AI safety measures.",
        "pdf_link": "https://arxiv.org/pdf/2403.14988v1.pdf"
    },
    {
        "title": "MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts",
        "authors": [
            "Md Nishat Raihan",
            "Dhiman Goswami",
            "Al Nahian Bin Emran",
            "Sadiya Sayara Chowdhury Puspo",
            "Amrita Ganguly",
            "Marcos Zampieri"
        ],
        "published": "2024-03-22T06:31:49Z",
        "summary": "Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 -\nwhich provides a dataset of puzzles for testing natural language understanding.\nWe employ large language models (LLMs) to solve this task through several\nprompting techniques. Zero-shot and few-shot prompting generate reasonably good\nresults when tested with proprietary LLMs, compared to the open-source models.\nWe obtain further improved results with chain-of-thought prompting, an\niterative prompting method that breaks down the reasoning process step-by-step.\nWe obtain our best results by utilizing an ensemble of chain-of-thought\nprompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle\nsubtask. The strong performance of prompted LLMs demonstrates their capability\nfor complex reasoning when provided with a decomposition of the thought\nprocess. Our work sheds light on how step-wise explanatory prompts can unlock\nmore of the knowledge encoded in the parameters of large models.",
        "pdf_link": "https://arxiv.org/pdf/2403.14982v2.pdf"
    },
    {
        "title": "Comprehensive Evaluation and Insights into the Use of Large Language Models in the Automation of Behavior-Driven Development Acceptance Test Formulation",
        "authors": [
            "Shanthi Karpurapu",
            "Sravanthy Myneni",
            "Unnati Nettur",
            "Likhit Sagar Gajja",
            "Dave Burke",
            "Tom Stiehm",
            "Jeffery Payne"
        ],
        "published": "2024-03-22T05:37:52Z",
        "summary": "Behavior-driven development (BDD) is an Agile testing methodology fostering\ncollaboration among developers, QA analysts, and stakeholders. In this\nmanuscript, we propose a novel approach to enhance BDD practices using large\nlanguage models (LLMs) to automate acceptance test generation. Our study uses\nzero and few-shot prompts to evaluate LLMs such as GPT-3.5, GPT-4, Llama-2-13B,\nand PaLM-2. The paper presents a detailed methodology that includes the\ndataset, prompt techniques, LLMs, and the evaluation process. The results\ndemonstrate that GPT-3.5 and GPT-4 generate error-free BDD acceptance tests\nwith better performance. The few-shot prompt technique highlights its ability\nto provide higher accuracy by incorporating examples for in-context learning.\nFurthermore, the study examines syntax errors, validation accuracy, and\ncomparative analysis of LLMs, revealing their effectiveness in enhancing BDD\npractices. However, our study acknowledges that there are limitations to the\nproposed approach. We emphasize that this approach can support collaborative\nBDD processes and create opportunities for future research into automated BDD\nacceptance test generation using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.14965v1.pdf"
    },
    {
        "title": "Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices",
        "authors": [
            "Pengxiang Zhao",
            "Ping Li",
            "Yingjie Gu",
            "Yi Zheng",
            "Stephan Ludger K\u00f6lker",
            "Zhefeng Wang",
            "Xiaoming Yuan"
        ],
        "published": "2024-03-22T05:23:31Z",
        "summary": "As deep learning models exponentially increase in size, optimizers such as\nAdam encounter significant memory consumption challenges due to the storage of\nfirst and second moment data. Current memory-efficient methods like Adafactor\nand CAME often compromise accuracy with their matrix factorization techniques.\nAddressing this, we introduce Adapprox, a novel approach that employs\nrandomized low-rank matrix approximation for a more effective and accurate\napproximation of Adam's second moment. Adapprox features an adaptive rank\nselection mechanism, finely balancing accuracy and memory efficiency, and\nincludes an optional cosine similarity guidance strategy to enhance stability\nand expedite convergence. In GPT-2 training and downstream tasks, Adapprox\nsurpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings\nfor the 117M and 345M models, respectively, with the first moment enabled, and\nfurther increases these savings without the first moment. Besides, it enhances\nconvergence speed and improves downstream task performance relative to its\ncounterparts.",
        "pdf_link": "https://arxiv.org/pdf/2403.14958v1.pdf"
    },
    {
        "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
        "authors": [
            "Zhenrui Yue",
            "Huimin Zeng",
            "Yimeng Lu",
            "Lanyu Shang",
            "Yang Zhang",
            "Dong Wang"
        ],
        "published": "2024-03-22T05:05:45Z",
        "summary": "The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.",
        "pdf_link": "https://arxiv.org/pdf/2403.14952v1.pdf"
    },
    {
        "title": "On Zero-Shot Counterspeech Generation by LLMs",
        "authors": [
            "Punyajoy Saha",
            "Aalok Agrawal",
            "Abhik Jana",
            "Chris Biemann",
            "Animesh Mukherjee"
        ],
        "published": "2024-03-22T04:13:10Z",
        "summary": "With the emergence of numerous Large Language Models (LLM), the usage of such\nmodels in various Natural Language Processing (NLP) applications is increasing\nextensively. Counterspeech generation is one such key task where efforts are\nmade to develop generative models by fine-tuning LLMs with hatespeech -\ncounterspeech pairs, but none of these attempts explores the intrinsic\nproperties of large language models in zero-shot settings. In this work, we\npresent a comprehensive analysis of the performances of four LLMs namely GPT-2,\nDialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech\ngeneration, which is the first of its kind. For GPT-2 and DialoGPT, we further\ninvestigate the deviation in performance with respect to the sizes (small,\nmedium, large) of the models. On the other hand, we propose three different\nprompting strategies for generating different types of counterspeech and\nanalyse the impact of such strategies on the performance of the models. Our\nanalysis shows that there is an improvement in generation quality for two\ndatasets (17%), however the toxicity increase (25%) with increase in model\nsize. Considering type of model, GPT-2 and FlanT5 models are significantly\nbetter in terms of counterspeech quality but also have high toxicity as\ncompared to DialoGPT. ChatGPT are much better at generating counter speech than\nother models across all metrics. In terms of prompting, we find that our\nproposed strategies help in improving counter speech generation across all the\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2403.14938v1.pdf"
    },
    {
        "title": "AutoRE: Document-Level Relation Extraction with Large Language Models",
        "authors": [
            "Xue Lilong",
            "Zhang Dan",
            "Dong Yuxiao",
            "Tang Jie"
        ],
        "published": "2024-03-21T23:48:21Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional abilities in\ncomprehending and generating text, motivating numerous researchers to utilize\nthem for Information Extraction (IE) purposes, including Relation Extraction\n(RE). Nonetheless, most existing methods are predominantly designed for\nSentence-level Relation Extraction (SentRE) tasks, which typically encompass a\nrestricted set of relations and triplet facts within a single sentence.\nFurthermore, certain approaches resort to treating relations as candidate\nchoices integrated into prompt templates, leading to inefficient processing and\nsuboptimal performance when tackling Document-Level Relation Extraction (DocRE)\ntasks, which entail handling multiple relations and triplet facts distributed\nacross a given document, posing distinct challenges. To overcome these\nlimitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel\nRE extraction paradigm named RHF (Relation-Head-Facts). Unlike existing\napproaches, AutoRE does not rely on the assumption of known relation options,\nmaking it more reflective of real-world scenarios. Additionally, we have\ndeveloped an easily extensible RE framework using a Parameters Efficient Fine\nTuning (PEFT) algorithm (QLoRA). Our experiments on the RE-DocRED dataset\nshowcase AutoRE's best performance, achieving state-of-the-art results,\nsurpassing TAG by 10.03% and 9.03% respectively on the dev and test set.",
        "pdf_link": "https://arxiv.org/pdf/2403.14888v1.pdf"
    },
    {
        "title": "Evaluating the Performance of LLMs on Technical Language Processing tasks",
        "authors": [
            "Andrew Kernycky",
            "David Coleman",
            "Christopher Spence",
            "Udayan Das"
        ],
        "published": "2024-03-21T23:40:42Z",
        "summary": "In this paper we present the results of an evaluation study of the\nperfor-mance of LLMs on Technical Language Processing tasks. Humans are often\nconfronted with tasks in which they have to gather information from dispar-ate\nsources and require making sense of large bodies of text. These tasks can be\nsignificantly complex for humans and often require deep study including\nrereading portions of a text. Towards simplifying the task of gathering\nin-formation we evaluated LLMs with chat interfaces for their ability to\nprovide answers to standard questions that a human can be expected to answer\nbased on their reading of a body of text. The body of text under study is Title\n47 of the United States Code of Federal Regulations (CFR) which describes\nregula-tions for commercial telecommunications as governed by the Federal\nCom-munications Commission (FCC). This has been a body of text of interest\nbe-cause our larger research concerns the issue of making sense of information\nrelated to Wireless Spectrum Governance and usage in an automated manner to\nsupport Dynamic Spectrum Access. The information concerning this wireless\nspectrum domain is found in many disparate sources, with Title 47 of the CFR\nbeing just one of many. Using a range of LLMs and providing the required CFR\ntext as context we were able to quantify the performance of those LLMs on the\nspecific task of answering the questions below.",
        "pdf_link": "https://arxiv.org/pdf/2403.15503v1.pdf"
    },
    {
        "title": "VidLA: Video-Language Alignment at Scale",
        "authors": [
            "Mamshad Nayeem Rizve",
            "Fan Fei",
            "Jayakrishnan Unnikrishnan",
            "Son Tran",
            "Benjamin Z. Yao",
            "Belinda Zeng",
            "Mubarak Shah",
            "Trishul Chilimbi"
        ],
        "published": "2024-03-21T22:36:24Z",
        "summary": "In this paper, we propose VidLA, an approach for video-language alignment at\nscale. There are two major limitations of previous video-language alignment\napproaches. First, they do not capture both short-range and long-range temporal\ndependencies and typically employ complex hierarchical deep network\narchitectures that are hard to integrate with existing pretrained image-text\nfoundation models. To effectively address this limitation, we instead keep the\nnetwork architecture simple and use a set of data tokens that operate at\ndifferent temporal resolutions in a hierarchical manner, accounting for the\ntemporally hierarchical nature of videos. By employing a simple two-tower\narchitecture, we are able to initialize our video-language model with\npretrained image-text foundation models, thereby boosting the final\nperformance. Second, existing video-language alignment works struggle due to\nthe lack of semantically aligned large-scale training data. To overcome it, we\nleverage recent LLMs to curate the largest video-language dataset to date with\nbetter visual grounding. Furthermore, unlike existing video-text datasets which\nonly contain short clips, our dataset is enriched with video clips of varying\ndurations to aid our temporally hierarchical data tokens in extracting better\nrepresentations at varying temporal scales. Overall, empirical results show\nthat our proposed approach surpasses state-of-the-art methods on multiple\nretrieval benchmarks, especially on longer videos, and performs competitively\non classification benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.14870v1.pdf"
    },
    {
        "title": "Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models",
        "authors": [
            "Carina Kauf",
            "Emmanuele Chersoni",
            "Alessandro Lenci",
            "Evelina Fedorenko",
            "Anna A. Ivanova"
        ],
        "published": "2024-03-21T22:08:44Z",
        "summary": "Instruction-tuned LLMs can respond to explicit queries formulated as prompts,\nwhich greatly facilitates interaction with human users. However, prompt-based\napproaches might not always be able to tap into the wealth of implicit\nknowledge acquired by LLMs during pre-training. This paper presents a\ncomprehensive study of ways to evaluate semantic plausibility in LLMs. We\ncompare base and instruction-tuned LLM performance on an English sentence\nplausibility task via (a) explicit prompting and (b) implicit estimation via\ndirect readout of the probabilities models assign to strings. Experiment 1\nshows that, across model architectures and plausibility datasets, (i) log\nlikelihood ($\\textit{LL}$) scores are the most reliable indicator of sentence\nplausibility, with zero-shot prompting yielding inconsistent and typically poor\nresults; (ii) $\\textit{LL}$-based performance is still inferior to human\nperformance; (iii) instruction-tuned models have worse $\\textit{LL}$-based\nperformance than base models. In Experiment 2, we show that $\\textit{LL}$\nscores across models are modulated by context in the expected way, showing high\nperformance on three metrics of context-sensitive plausibility and providing a\ndirect match to explicit human plausibility judgments. Overall, $\\textit{LL}$\nestimates remain a more reliable measure of plausibility in LLMs than direct\nprompting.",
        "pdf_link": "https://arxiv.org/pdf/2403.14859v1.pdf"
    },
    {
        "title": "The opportunities and risks of large language models in mental health",
        "authors": [
            "Hannah R. Lawrence",
            "Renee A. Schneider",
            "Susan B. Rubin",
            "Maja J. Mataric",
            "Daniel J. McDuff",
            "Megan Jones Bell"
        ],
        "published": "2024-03-21T19:59:52Z",
        "summary": "Global rates of mental health concerns are rising and there is increasing\nrealization that existing models of mental healthcare will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health-related tasks. In this review, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs application\nto mental health and encourage adoption of strategies to mitigate these risks.\nThe urgent need for mental health support must be balanced with responsible\ndevelopment, testing, and deployment of mental health LLMs. Especially critical\nis ensuring that mental health LLMs are fine-tuned for mental health, enhance\nmental health equity, adhere to ethical standards, and that people, including\nthose with lived experience with mental health concerns, are involved in all\nstages from development through deployment. Prioritizing these efforts will\nminimize potential harms to mental health and maximize the likelihood that LLMs\nwill positively impact mental health globally.",
        "pdf_link": "https://arxiv.org/pdf/2403.14814v2.pdf"
    },
    {
        "title": "Can 3D Vision-Language Models Truly Understand Natural Language?",
        "authors": [
            "Weipeng Deng",
            "Runyu Ding",
            "Jihan Yang",
            "Jiahui Liu",
            "Yijiang Li",
            "Xiaojuan Qi",
            "Edith Ngai"
        ],
        "published": "2024-03-21T18:02:20Z",
        "summary": "Rapid advancements in 3D vision-language (3D-VL) tasks have opened up new\navenues for human interaction with embodied agents or robots using natural\nlanguage. Despite this progress, we find a notable limitation: existing 3D-VL\nmodels exhibit sensitivity to the styles of language input, struggling to\nunderstand sentences with the same semantic meaning but written in different\nvariants. This observation raises a critical question: Can 3D vision-language\nmodels truly understand natural language? To test the language\nunderstandability of 3D-VL models, we first propose a language robustness task\nfor systematically assessing 3D-VL models across various tasks, benchmarking\ntheir performance when presented with different language style variants.\nImportantly, these variants are commonly encountered in applications requiring\ndirect interaction with humans, such as embodied robotics, given the diversity\nand unpredictability of human language. We propose a 3D Language Robustness\nDataset, designed based on the characteristics of human language, to facilitate\nthe systematic study of robustness. Our comprehensive evaluation uncovers a\nsignificant drop in the performance of all existing models across various 3D-VL\ntasks. Even the state-of-the-art 3D-LLM fails to understand some variants of\nthe same sentences. Further in-depth analysis suggests that the existing models\nhave a fragile and biased fusion module, which stems from the low diversity of\nthe existing dataset. Finally, we propose a training-free module driven by LLM,\nwhich improves language robustness. Datasets and code will be available at\ngithub.",
        "pdf_link": "https://arxiv.org/pdf/2403.14760v2.pdf"
    },
    {
        "title": "VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding",
        "authors": [
            "Ahmad Mahmood",
            "Ashmal Vayani",
            "Muzammal Naseer",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "published": "2024-03-21T18:00:00Z",
        "summary": "Recent studies have demonstrated the effectiveness of Large Language Models\n(LLMs) as reasoning modules that can deconstruct complex tasks into more\nmanageable sub-tasks, particularly when applied to visual reasoning tasks for\nimages. In contrast, this paper introduces a Video Understanding and Reasoning\nFramework (VURF) based on the reasoning power of LLMs. Ours is a novel approach\nto extend the utility of LLMs in the context of video tasks, leveraging their\ncapacity to generalize from minimal input and output demonstrations within a\ncontextual framework. By presenting LLMs with pairs of instructions and their\ncorresponding high-level programs, we harness their contextual learning\ncapabilities to generate executable visual programs for video understanding. To\nenhance program's accuracy and robustness, we implement two important\nstrategies. Firstly, we employ a feedback-generation approach, powered by\nGPT-3.5, to rectify errors in programs utilizing unsupported functions.\nSecondly, taking motivation from recent works on self refinement of LLM\noutputs, we introduce an iterative procedure for improving the quality of the\nin-context examples by aligning the initial outputs to the outputs that would\nhave been generated had the LLM not been bound by the structure of the\nin-context examples. Our results on several video-specific tasks, including\nvisual QA, video anticipation, pose estimation and multi-video QA illustrate\nthe efficacy of these enhancements in improving the performance of visual\nprogramming approaches for video tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.14743v2.pdf"
    },
    {
        "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
        "authors": [
            "Renrui Zhang",
            "Dongzhi Jiang",
            "Yichi Zhang",
            "Haokun Lin",
            "Ziyu Guo",
            "Pengshuo Qiu",
            "Aojun Zhou",
            "Pan Lu",
            "Kai-Wei Chang",
            "Peng Gao",
            "Hongsheng Li"
        ],
        "published": "2024-03-21T17:59:50Z",
        "summary": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io",
        "pdf_link": "https://arxiv.org/pdf/2403.14624v1.pdf"
    },
    {
        "title": "Language Repository for Long Video Understanding",
        "authors": [
            "Kumara Kahatapitiya",
            "Kanchana Ranasinghe",
            "Jongwoo Park",
            "Michael S. Ryoo"
        ],
        "published": "2024-03-21T17:59:35Z",
        "summary": "Language has become a prominent modality in computer vision with the rise of\nmulti-modal LLMs. Despite supporting long context-lengths, their effectiveness\nin handling long-term information gradually declines with input length. This\nbecomes critical, especially in applications such as long-form video\nunderstanding. In this paper, we introduce a Language Repository (LangRepo) for\nLLMs, that maintains concise and structured information as an interpretable\n(i.e., all-textual) representation. Our repository is updated iteratively based\non multi-scale video chunks. We introduce write and read operations that focus\non pruning redundancies in text, and extracting information at various temporal\nscales. The proposed framework is evaluated on zero-shot visual\nquestion-answering benchmarks including EgoSchema, NExT-QA, IntentQA and\nNExT-GQA, showing state-of-the-art performance at its scale. Our code is\navailable at https://github.com/kkahatapitiya/LangRepo.",
        "pdf_link": "https://arxiv.org/pdf/2403.14622v1.pdf"
    },
    {
        "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey",
        "authors": [
            "Zeyu Han",
            "Chao Gao",
            "Jinyang Liu",
            "Jeff Zhang",
            "Sai Qian Zhang"
        ],
        "published": "2024-03-21T17:55:50Z",
        "summary": "Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adapt the large models over\nthe various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task while minimizing the number of additional parameters introduced\nor computational resources required. This approach is particularly important\nwhen dealing with large language models with high parameter counts, as\nfine-tuning these models from scratch can be computationally expensive and\nresource-intensive, posing considerable challenges in the supporting system\nplatform design. In this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computational overhead.\nMoreover, we provide an overview of applications developed using different PEFT\nalgorithms and discuss common techniques employed to mitigate computation costs\nfor PEFT. In addition to the algorithmic perspective, we overview various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT algorithms. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed insights into recent advancements and\npractical applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.14608v2.pdf"
    },
    {
        "title": "RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain",
        "authors": [
            "William James Bolton",
            "Rafael Poyiadzi",
            "Edward R. Morrell",
            "Gabriela van Bergen Gonzalez Bueno",
            "Lea Goetz"
        ],
        "published": "2024-03-21T17:30:59Z",
        "summary": "Large Language Models (LLMs) increasingly support applications in a wide\nrange of domains, some with potential high societal impact such as biomedicine,\nyet their reliability in realistic use cases is under-researched. In this work\nwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)\nframework and evaluate whether four state-of-the-art foundation LLMs can serve\nas reliable assistants in the biomedical domain. We identify prompt robustness,\nhigh recall, and a lack of hallucinations as necessary criteria for this use\ncase. We design shortform tasks and tasks requiring LLM freeform responses\nmimicking real-world user interactions. We evaluate LLM performance using\nsemantic similarity with a ground truth response, through an evaluator LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.14578v1.pdf"
    },
    {
        "title": "A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science",
        "authors": [
            "Clayton Cohn",
            "Nicole Hutchins",
            "Tuan Le",
            "Gautam Biswas"
        ],
        "published": "2024-03-21T17:09:08Z",
        "summary": "This paper explores the use of large language models (LLMs) to score and\nexplain short-answer assessments in K-12 science. While existing methods can\nscore more structured math and computer science assessments, they often do not\nprovide explanations for the scores. Our study focuses on employing GPT-4 for\nautomated assessment in middle school Earth Science, combining few-shot and\nactive learning with chain-of-thought reasoning. Using a human-in-the-loop\napproach, we successfully score and provide meaningful explanations for\nformative assessment responses. A systematic analysis of our method's pros and\ncons sheds light on the potential for human-in-the-loop techniques to enhance\nautomated grading for open-ended science assessments.",
        "pdf_link": "https://arxiv.org/pdf/2403.14565v1.pdf"
    },
    {
        "title": "The Era of Semantic Decoding",
        "authors": [
            "Maxime Peyrard",
            "Martin Josifoski",
            "Robert West"
        ],
        "published": "2024-03-21T17:06:17Z",
        "summary": "Recent work demonstrated great promise in the idea of orchestrating\ncollaborations between LLMs, human input, and various tools to address the\ninherent limitations of LLMs. We propose a novel perspective called semantic\ndecoding, which frames these collaborative processes as optimization procedures\nin semantic space. Specifically, we conceptualize LLMs as semantic processors\nthat manipulate meaningful pieces of information that we call semantic tokens\n(known thoughts). LLMs are among a large pool of other semantic processors,\nincluding humans and tools, such as search engines or code executors.\nCollectively, semantic processors engage in dynamic exchanges of semantic\ntokens to progressively construct high-utility outputs. We refer to these\norchestrated interactions among semantic processors, optimizing and searching\nin semantic space, as semantic decoding algorithms. This concept draws a direct\nparallel to the well-studied problem of syntactic decoding, which involves\ncrafting algorithms to best exploit auto-regressive language models for\nextracting high-utility sequences of syntactic tokens. By focusing on the\nsemantic level and disregarding syntactic details, we gain a fresh perspective\non the engineering of AI systems, enabling us to imagine systems with much\ngreater complexity and capabilities. In this position paper, we formalize the\ntransition from syntactic to semantic tokens as well as the analogy between\nsyntactic and semantic decoding. Subsequently, we explore the possibilities of\noptimizing within the space of semantic tokens via semantic decoding\nalgorithms. We conclude with a list of research opportunities and questions\narising from this fresh perspective. The semantic decoding perspective offers a\npowerful abstraction for search and optimization directly in the space of\nmeaningful concepts, with semantic tokens as the fundamental units of a new\ntype of computation.",
        "pdf_link": "https://arxiv.org/pdf/2403.14562v1.pdf"
    },
    {
        "title": "EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling",
        "authors": [
            "Shimao Zhang",
            "Yu Bao",
            "Shujian Huang"
        ],
        "published": "2024-03-21T16:41:12Z",
        "summary": "Recently, Large Language Models (LLMs) have demonstrated outstanding\nperformance across a wide range of downstream language tasks. Temperature\nsampling is a commonly used decoding strategy for LLMs' generation process.\nHowever, a fixed temperature parameter is used in most cases, which may not\nalways be an optimal choice for balancing generation quality and diversity. In\nthis paper, we propose an effective Entropy-based Dynamic Temperature (EDT)\nSampling method, to achieve a more balanced performance in terms of both\ngeneration quality and diversity by dynamically selecting the temperature\nparameter. Additionally, we also show model performance and comprehensive\nanalyses for 4 different generation benchmarks. Our experiments show that EDT\nsignificantly outperforms the existing strategies across different tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.14541v2.pdf"
    },
    {
        "title": "Open Source Conversational LLMs do not know most Spanish words",
        "authors": [
            "Javier Conde",
            "Miguel Gonz\u00e1lez",
            "Nina Melero",
            "Raquel Ferrando",
            "Gonzalo Mart\u00ednez",
            "Elena Merino-G\u00f3mez",
            "Jos\u00e9 Alberto Hern\u00e1ndez",
            "Pedro Reviriego"
        ],
        "published": "2024-03-21T15:41:02Z",
        "summary": "The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages.",
        "pdf_link": "https://arxiv.org/pdf/2403.15491v1.pdf"
    },
    {
        "title": "Detoxifying Large Language Models via Knowledge Editing",
        "authors": [
            "Mengru Wang",
            "Ningyu Zhang",
            "Ziwen Xu",
            "Zekun Xi",
            "Shumin Deng",
            "Yunzhi Yao",
            "Qishen Zhang",
            "Linyi Yang",
            "Jindong Wang",
            "Huajun Chen"
        ],
        "published": "2024-03-21T15:18:30Z",
        "summary": "This paper investigates using knowledge editing techniques to detoxify Large\nLanguage Models (LLMs). We construct a benchmark, SafeEdit, which covers nine\nunsafe categories with various powerful attack prompts and equips comprehensive\nmetrics for systematic evaluation. We conduct experiments with several\nknowledge editing approaches, indicating that knowledge editing has the\npotential to efficiently detoxify LLMs with limited impact on general\nperformance. Then, we propose a simple yet effective baseline, dubbed\nDetoxifying with Intraoperative Neural Monitoring (DINM), to diminish the\ntoxicity of LLMs within a few tuning steps via only one instance. We further\nprovide an in-depth analysis of the internal mechanism for various detoxify\napproaches, demonstrating that previous methods like SFT and DPO may merely\nsuppress the activations of toxic parameters, while DINM mitigates the toxicity\nof the toxic parameters to a certain extent, making permanent adjustments. We\nhope that these insights could shed light on future work of developing\ndetoxifying approaches and the underlying knowledge mechanisms of LLMs. Code\nand benchmark are available at https://github.com/zjunlp/EasyEdit.",
        "pdf_link": "https://arxiv.org/pdf/2403.14472v2.pdf"
    },
    {
        "title": "ChatGPT Alternative Solutions: Large Language Models Survey",
        "authors": [
            "Hanieh Alipour",
            "Nick Pendar",
            "Kohinoor Roy"
        ],
        "published": "2024-03-21T15:16:50Z",
        "summary": "In recent times, the grandeur of Large Language Models (LLMs) has not only\nshone in the realm of natural language processing but has also cast its\nbrilliance across a vast array of applications. This remarkable display of LLM\ncapabilities has ignited a surge in research contributions within this domain,\nspanning a diverse spectrum of topics. These contributions encompass\nadvancements in neural network architecture, context length enhancements, model\nalignment, training datasets, benchmarking, efficiency improvements, and more.\nRecent years have witnessed a dynamic synergy between academia and industry,\npropelling the field of LLM research to new heights. A notable milestone in\nthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded in\nLLMs, which has garnered widespread societal attention. The evolving technology\nof LLMs has begun to reshape the landscape of the entire AI community,\npromising a revolutionary shift in the way we create and employ AI algorithms.\nGiven this swift-paced technical evolution, our survey embarks on a journey to\nencapsulate the recent strides made in the world of LLMs. Through an\nexploration of the background, key discoveries, and prevailing methodologies,\nwe offer an up-to-the-minute review of the literature. By examining multiple\nLLM models, our paper not only presents a comprehensive overview but also\ncharts a course that identifies existing challenges and points toward potential\nfuture research trajectories. This survey furnishes a well-rounded perspective\non the current state of generative AI, shedding light on opportunities for\nfurther exploration, enhancement, and innovation.",
        "pdf_link": "https://arxiv.org/pdf/2403.14469v1.pdf"
    },
    {
        "title": "gTBLS: Generating Tables from Text by Conditional Question Answering",
        "authors": [
            "Anirudh Sundar",
            "Christopher Richardson",
            "Larry Heck"
        ],
        "published": "2024-03-21T15:04:32Z",
        "summary": "Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.14457v1.pdf"
    },
    {
        "title": "Locating and Mitigating Gender Bias in Large Language Models",
        "authors": [
            "Yuchen Cai",
            "Ding Cao",
            "Rongxi Guo",
            "Yaqin Wen",
            "Guiquan Liu",
            "Enhong Chen"
        ],
        "published": "2024-03-21T13:57:43Z",
        "summary": "Large language models(LLM) are pre-trained on extensive corpora to learn\nfacts and human cognition which contain human preferences. However, this\nprocess can inadvertently lead to these models acquiring biases and stereotypes\nprevalent in society. Prior research has typically tackled the issue of bias\nthrough a one-dimensional perspective, concentrating either on locating or\nmitigating it. This limited perspective has created obstacles in facilitating\nresearch on bias to synergistically complement and progressively build upon one\nanother. In this study, we integrate the processes of locating and mitigating\nbias within a unified framework. Initially, we use causal mediation analysis to\ntrace the causal effects of different components' activation within a large\nlanguage model. Building on this, we propose the LSDM (Least Square Debias\nMethod), a knowledge-editing based method for mitigating gender bias in\noccupational pronouns, and compare it against two baselines on three gender\nbias datasets and seven knowledge competency test datasets. The experimental\nresults indicate that the primary contributors to gender bias are the bottom\nMLP modules acting on the last token of occupational pronouns and the top\nattention module acting on the final word in the sentence. Furthermore, LSDM\nmitigates gender bias in the model more effectively than the other baselines,\nwhile fully preserving the model's capabilities in all other aspects.",
        "pdf_link": "https://arxiv.org/pdf/2403.14409v1.pdf"
    },
    {
        "title": "Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination",
        "authors": [
            "Dingchen Yang",
            "Bowen Cao",
            "Guang Chen",
            "Changjun Jiang"
        ],
        "published": "2024-03-21T13:49:42Z",
        "summary": "Multi-modal Large Language Models (MLLMs) demonstrate remarkable success\nacross various vision-language tasks. However, they suffer from visual\nhallucination, where the generated responses diverge from the provided image.\nAre MLLMs completely oblivious to accurate visual cues when they hallucinate?\nOur investigation reveals that the visual branch may simultaneously advocate\nboth accurate and non-existent content. To address this issue, we propose\nPensieve, a training-free method inspired by our observation that analogous\nvisual hallucinations can arise among images sharing common semantic and\nappearance characteristics. During inference, Pensieve enables MLLMs to\nretrospect relevant images as references and compare them with the test image.\nThis paradigm assists MLLMs in downgrading hallucinatory content mistakenly\nsupported by the visual input. Experiments on Whoops, MME, POPE, and LLaVA\nBench demonstrate the efficacy of Pensieve in mitigating visual hallucination,\nsurpassing other advanced decoding strategies. Additionally, Pensieve aids\nMLLMs in identifying details in the image and enhancing the specificity of\nimage descriptions.",
        "pdf_link": "https://arxiv.org/pdf/2403.14401v1.pdf"
    },
    {
        "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
        "authors": [
            "Changtong Zan",
            "Liang Ding",
            "Li Shen",
            "Yibing Zhen",
            "Weifeng Liu",
            "Dacheng Tao"
        ],
        "published": "2024-03-21T13:47:40Z",
        "summary": "Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.",
        "pdf_link": "https://arxiv.org/pdf/2403.14399v1.pdf"
    },
    {
        "title": "From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision",
        "authors": [
            "Qingwen Lin",
            "Boyan Xu",
            "Zhengting Huang",
            "Ruichu Cai"
        ],
        "published": "2024-03-21T13:29:54Z",
        "summary": "Addressing the challenge of high annotation costs in solving Math Word\nProblems (MWPs) through full supervision with intermediate equations, recent\nworks have proposed weakly supervised task settings that rely solely on the\nfinal answer as a supervised signal. Existing leading approaches typically\nemploy various search techniques to infer intermediate equations, but cannot\nensure their semantic consistency with natural language descriptions. The rise\nof Large Language Models (LLMs) like ChatGPT has opened up new possibilities\nfor addressing MWPs directly. However, the computational demands of LLMs make\nthem less than ideal for use in settings where resources are tight. In light of\nthese challenges, we introduce an innovative two-stage framework that adeptly\ntransfers mathematical Expertise from large to tiny language models. In\n\\emph{Distillation Stage}, we propose a series of extraction processes that\nsatisfy the properties of MWPs to distill mathematical knowledge from LLMs to\nconstruct problem-equation pairs required for supervised training. In\n\\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee\nthe full utilization of all data, we further utilize the unsuccessfully\nsearched data effectively by Knowledge Refine method. Finally, We train a small\nmodel using distilled data generated through two-stage methods. As our method\nfully leverages the semantic understanding capabilities during the searching\n'problem-equation' pair, it demonstrates significantly improved performance on\nthe Math23K and Weak12K datasets compared to existing small model methods,\nwhile maintaining a much lower computational cost than ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2403.14390v1.pdf"
    },
    {
        "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
        "authors": [
            "Yuren Mao",
            "Xuemei Dong",
            "Wenyi Xu",
            "Yunjun Gao",
            "Bin Wei",
            "Ying Zhang"
        ],
        "published": "2024-03-21T13:05:18Z",
        "summary": "Due to the extraordinarily large number of parameters, fine-tuning Large\nLanguage Models (LLMs) to update long-tail or out-of-date knowledge is\nimpractical in lots of applications. To avoid fine-tuning, we can alternatively\ntreat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment\nit with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.\nRecently, black-box RAG has achieved success in knowledge-intensive tasks and\nhas gained much attention. Existing black-box RAG methods typically fine-tune\nthe retriever to cater to LLMs' preferences and concatenate all the retrieved\ndocuments as the input, which suffers from two issues: (1) Ignorance of Factual\nInformation. The LLM preferred documents may not contain the factual\ninformation for the given question, which can mislead the retriever and hurt\nthe effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating\nall the retrieved documents brings large amounts of unnecessary tokens for\nLLMs, which degenerates the efficiency of black-box RAG. To address these\nissues, this paper proposes a novel black-box RAG framework which utilizes the\nfactual information in the retrieval and reduces the number of tokens for\naugmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by\nconstructing a bi-label document scorer. Besides, it reduces the tokens by\nintroducing a self-knowledge recognizer and a sub-document-level token reducer.\nFIT-RAG achieves both superior effectiveness and efficiency, which is validated\nby extensive experiments across three open-domain question-answering datasets:\nTriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of\nLlama2-13B-Chat by 14.3\\% on TriviaQA, 19.9\\% on NQ and 27.5\\% on PopQA,\nrespectively. Furthermore, it can save approximately half of the tokens on\naverage across the three datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.14374v1.pdf"
    },
    {
        "title": "WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for Atomic Factual Knowledge Update in Causal Language Models",
        "authors": [
            "Hichem Ammar Khodja",
            "Fr\u00e9d\u00e9ric B\u00e9chet",
            "Quentin Brabant",
            "Alexis Nasr",
            "Gw\u00e9nol\u00e9 Lecorv\u00e9"
        ],
        "published": "2024-03-21T12:45:12Z",
        "summary": "The factuality of large language model (LLMs) tends to decay over time since\nevents posterior to their training are \"unknown\" to them. One way to keep\nmodels up-to-date could be factual update: the task of inserting, replacing, or\nremoving certain simple (atomic) facts within the model. To study this task, we\npresent WikiFactDiff, a dataset that describes the evolution of factual\nknowledge between two dates as a collection of simple facts divided into three\ncategories: new, obsolete, and static. We describe several update scenarios\narising from various combinations of these three types of basic update. The\nfacts are represented by subject-relation-object triples; indeed, WikiFactDiff\nwas constructed by comparing the state of the Wikidata knowledge base at 4\nJanuary 2021 and 27 February 2023. Those fact are accompanied by verbalization\ntemplates and cloze tests that enable running update algorithms and their\nevaluation metrics. Contrary to other datasets, such as zsRE and CounterFact,\nWikiFactDiff constitutes a realistic update setting that involves various\nupdate scenarios, including replacements, archival, and new entity insertions.\nWe also present an evaluation of existing update algorithms on WikiFactDiff.",
        "pdf_link": "https://arxiv.org/pdf/2403.14364v1.pdf"
    },
    {
        "title": "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics",
        "authors": [
            "Jiaqi Yue",
            "Jiancheng Zhao",
            "Chunhui Zhao"
        ],
        "published": "2024-03-21T12:45:01Z",
        "summary": "Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2403.14362v1.pdf"
    },
    {
        "title": "Exploring the Potential of Large Language Models in Graph Generation",
        "authors": [
            "Yang Yao",
            "Xin Wang",
            "Zeyang Zhang",
            "Yijian Qin",
            "Ziwei Zhang",
            "Xu Chu",
            "Yuekui Yang",
            "Wenwu Zhu",
            "Hong Mei"
        ],
        "published": "2024-03-21T12:37:54Z",
        "summary": "Large language models (LLMs) have achieved great success in many fields, and\nrecent works have studied exploring LLMs for graph discriminative tasks such as\nnode classification. However, the abilities of LLMs for graph generation remain\nunexplored in the literature. Graph generation requires the LLM to generate\ngraphs with given properties, which has valuable real-world applications such\nas drug discovery, while tends to be more challenging. In this paper, we\npropose LLM4GraphGen to explore the ability of LLMs for graph generation with\nsystematical task designs and extensive experiments. Specifically, we propose\nseveral tasks tailored with comprehensive experiments to address key questions\nregarding LLMs' understanding of different graph structure rules, their ability\nto capture structural type distributions, and their utilization of domain\nknowledge for property-based graph generation. Our evaluations demonstrate that\nLLMs, particularly GPT-4, exhibit preliminary abilities in graph generation\ntasks, including rule-based and distribution-based generation. We also observe\nthat popular prompting methods, such as few-shot and chain-of-thought\nprompting, do not consistently enhance performance. Besides, LLMs show\npotential in generating molecules with specific properties. These findings may\nserve as foundations for designing good LLMs based models for graph generation\nand provide valuable insights and further research.",
        "pdf_link": "https://arxiv.org/pdf/2403.14358v1.pdf"
    },
    {
        "title": "Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial Narratives",
        "authors": [
            "Jiaxin Liu",
            "Yi Yang",
            "Kar Yan Tam"
        ],
        "published": "2024-03-21T12:17:59Z",
        "summary": "In this paper, we introduce the Financial-STS task, a financial\ndomain-specific NLP task designed to measure the nuanced semantic similarity\nbetween pairs of financial narratives. These narratives originate from the\nfinancial statements of the same company but correspond to different periods,\nsuch as year-over-year comparisons. Measuring the subtle semantic differences\nbetween these paired narratives enables market stakeholders to gauge changes\nover time in the company's financial and operational situations, which is\ncritical for financial decision-making. We find that existing pretrained\nembedding models and LLM embeddings fall short in discerning these subtle\nfinancial narrative shifts. To address this gap, we propose an LLM-augmented\npipeline specifically designed for the Financial-STS task. Evaluation on a\nhuman-annotated dataset demonstrates that our proposed method outperforms\nexisting methods trained on classic STS tasks and generic LLM embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2403.14341v1.pdf"
    },
    {
        "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
        "authors": [
            "Xiaoxue Cheng",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2024-03-21T11:34:26Z",
        "summary": "Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of\nlarge language models (LLMs), establishing itself as a primary approach to\nsolving complex reasoning tasks. Existing CoT synthesis approaches usually\nfocus on simpler reasoning tasks and thus result in low-quality and\ninconsistent CoT prompts. In response to this challenge, we present an\nempirical investigation of CoT prompting and introduce CoTGenius, a novel\nframework designed for the automatic generation of superior CoT prompts.\nCoTGenius is developed based on three major evolution strategies, i.e.,\ncomplicate, diversify, and specify-alongside two filtering mechanisms:\nevolutionary success judgement and correctness verification. We further employ\nCoTGenius to create an extensive CoT dataset, and subsequently fine-tune the\nLlama 2-Chat 7B and 13B models on this dataset. We call the resulting model\nChainLM. To deal with the cumulative error issue in reasoning steps, we propose\na step-level debating method, wherein multiple debaters discuss each reasoning\nstep to arrive at the correct answer. Extensive experiments demonstrate that\nour ChainLM models exhibit enhanced proficiency in addressing a spectrum of\ncomplex reasoning problems compared to existing models. In addition, we conduct\nan in-depth analysis of the impact of data categories within CoTGenius on the\nmodel performance. We release our dataset and code at\nhttps://github.com/RUCAIBox/ChainLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.14312v1.pdf"
    },
    {
        "title": "From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora",
        "authors": [
            "Virginia Morini",
            "Valentina Pansanella",
            "Katherine Abramski",
            "Erica Cau",
            "Andrea Failla",
            "Salvatore Citraro",
            "Giulio Rossetti"
        ],
        "published": "2024-03-21T11:04:41Z",
        "summary": "Social media platforms are online fora where users engage in discussions,\nshare content, and build connections. This review explores the dynamics of\nsocial interactions, user-generated contents, and biases within the context of\nsocial media analysis (analyzing works that use the tools offered by complex\nnetwork analysis and natural language processing) through the lens of three key\npoints of view: online debates, online support, and human-AI interactions. On\nthe one hand, we delineate the phenomenon of online debates, where\npolarization, misinformation, and echo chamber formation often proliferate,\ndriven by algorithmic biases and extreme mechanisms of homophily. On the other\nhand, we explore the emergence of online support groups through users'\nself-disclosure and social support mechanisms. Online debates and support\nmechanisms present a duality of both perils and possibilities within social\nmedia; perils of segregated communities and polarized debates, and\npossibilities of empathy narratives and self-help groups. This dichotomy also\nextends to a third perspective: users' reliance on AI-generated content, such\nas the ones produced by Large Language Models, which can manifest both human\nbiases hidden in training sets and non-human biases that emerge from their\nartificial neural architectures. Analyzing interdisciplinary approaches, we aim\nto deepen the understanding of the complex interplay between social\ninteractions, user-generated content, and biases within the realm of social\nmedia ecosystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.14298v1.pdf"
    },
    {
        "title": "Multi-role Consensus through LLMs Discussions for Vulnerability Detection",
        "authors": [
            "Zhenyu Mao",
            "Jialong Li",
            "Munan Li",
            "Kenji Tei"
        ],
        "published": "2024-03-21T10:28:18Z",
        "summary": "Recent advancements in large language models (LLMs) have highlighted the\npotential for vulnerability detection, a crucial component of software quality\nassurance. Despite this progress, most studies have been limited to the\nperspective of a single role, usually testers, lacking diverse viewpoints from\ndifferent roles in a typical software development life-cycle, including both\ndevelopers and testers. To this end, this paper introduces a multi-role\napproach to employ LLMs to act as different roles to simulate real-life code\nreview process, engaging in discussions towards a consensus on the existence\nand classification of vulnerabilities in the code. Preliminary evaluation of\nthe proposed approach indicates a 4.73% increase in the precision rate, 58.9%\nincrease in the recall rate, and a 28.1% increase in the F1 score.",
        "pdf_link": "https://arxiv.org/pdf/2403.14274v2.pdf"
    },
    {
        "title": "LLM-based Extraction of Contradictions from Patents",
        "authors": [
            "Stefan Trapp",
            "Joachim Warschat"
        ],
        "published": "2024-03-21T09:36:36Z",
        "summary": "Already since the 1950s TRIZ shows that patents and the technical\ncontradictions they solve are an important source of inspiration for the\ndevelopment of innovative products. However, TRIZ is a heuristic based on a\nhistoric patent analysis and does not make use of the ever-increasing number of\nlatest technological solutions in current patents. Because of the huge number\nof patents, their length, and, last but not least, their complexity there is a\nneed for modern patent retrieval and patent analysis to go beyond\nkeyword-oriented methods. Recent advances in patent retrieval and analysis\nmainly focus on dense vectors based on neural AI Transformer language models\nlike Google BERT. They are, for example, used for dense retrieval, question\nanswering or summarization and key concept extraction. A research focus within\nthe methods for patent summarization and key concept extraction are generic\ninventive concepts respectively TRIZ concepts like problems, solutions,\nadvantage of invention, parameters, and contradictions. Succeeding rule-based\napproaches, finetuned BERT-like language models for sentence-wise\nclassification represent the state-of-the-art of inventive concept extraction.\nWhile they work comparatively well for basic concepts like problems or\nsolutions, contradictions - as a more complex abstraction - remain a challenge\nfor these models. This paper goes one step further, as it presents a method to\nextract TRIZ contradictions from patent texts based on Prompt Engineering using\na generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction\ndetection, sentence extraction, contradiction summarization, parameter\nextraction and assignment to the 39 abstract TRIZ engineering parameters are\nall performed in a single prompt using the LangChain framework. Our results\nshow that \"off-the-shelf\" GPT-4 is a serious alternative to existing\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2403.14258v1.pdf"
    },
    {
        "title": "ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion Classification",
        "authors": [
            "Sehee Lim",
            "Yejin Kim",
            "Chi-Hyun Choi",
            "Jy-yong Sohn",
            "Byung-Hoon Kim"
        ],
        "published": "2024-03-21T09:28:38Z",
        "summary": "Improving the accessibility of psychotherapy with the aid of Large Language\nModels (LLMs) is garnering a significant attention in recent years. Recognizing\ncognitive distortions from the interviewee's utterances can be an essential\npart of psychotherapy, especially for cognitive behavioral therapy. In this\npaper, we propose ERD, which improves LLM-based cognitive distortion\nclassification performance with the aid of additional modules of (1) extracting\nthe parts related to cognitive distortion, and (2) debating the reasoning steps\nby multiple agents. Our experimental results on a public dataset show that ERD\nimproves the multi-class F1 score as well as binary specificity score.\nRegarding the latter score, it turns out that our method is effective in\ndebiasing the baseline method which has high false positive rate, especially\nwhen the summary of multi-agent debate is provided to LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.14255v1.pdf"
    },
    {
        "title": "LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
        "authors": [
            "Masato Fujitake"
        ],
        "published": "2024-03-21T09:25:24Z",
        "summary": "This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.14252v1.pdf"
    },
    {
        "title": "Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology",
        "authors": [
            "Dimitrios P. Panagoulias",
            "Evridiki Tsoureli-Nikita",
            "Maria Virvou",
            "George A. Tsihrintzis"
        ],
        "published": "2024-03-21T09:02:17Z",
        "summary": "The rise of Artificial Intelligence creates great promise in the field of\nmedical discovery, diagnostics and patient management. However, the vast\ncomplexity of all medical domains require a more complex approach that combines\nmachine learning algorithms, classifiers, segmentation algorithms and, lately,\nlarge language models. In this paper, we describe, implement and assess an\nArtificial Intelligence-empowered system and methodology aimed at assisting the\ndiagnosis process of skin lesions and other skin conditions within the field of\ndermatology that aims to holistically address the diagnostic process in this\ndomain. The workflow integrates large language, transformer-based vision models\nand sophisticated machine learning tools. This holistic approach achieves a\nnuanced interpretation of dermatological conditions that simulates and\nfacilitates a dermatologist's workflow. We assess our proposed methodology\nthrough a thorough cross-model validation technique embedded in an evaluation\npipeline that utilizes publicly available medical case studies of skin\nconditions and relevant images. To quantitatively score the system performance,\nadvanced machine learning and natural language processing tools are employed\nwhich focus on similarity comparison and natural language inference.\nAdditionally, we incorporate a human expert evaluation process based on a\nstructured checklist to further validate our results. We implemented the\nproposed methodology in a system which achieved approximate (weighted) scores\nof 0.87 for both contextual understanding and diagnostic accuracy,\ndemonstrating the efficacy of our approach in enhancing dermatological\nanalysis. The proposed methodology is expected to prove useful in the\ndevelopment of next-generation tele-dermatology applications, enhancing remote\nconsultation capabilities and access to care, especially in underserved areas.",
        "pdf_link": "https://arxiv.org/pdf/2403.14243v1.pdf"
    },
    {
        "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
        "authors": [
            "Kyungjae Lee",
            "Dasol Hwang",
            "Sunghyun Park",
            "Youngsoo Jang",
            "Moontae Lee"
        ],
        "published": "2024-03-21T08:57:27Z",
        "summary": "Despite the promise of RLHF in aligning LLMs with human preferences, it often\nleads to superficial alignment, prioritizing stylistic changes over improving\ndownstream performance of LLMs. Underspecified preferences could obscure\ndirections to align the models. Lacking exploration restricts identification of\ndesirable outputs to improve the models. To overcome these challenges, we\npropose a novel framework: Reinforcement Learning from Reflective Feedback\n(RLRF), which leverages fine-grained feedback based on detailed criteria to\nimprove the core capabilities of LLMs. RLRF employs a self-reflection mechanism\nto systematically explore and refine LLM responses, then fine-tuning the models\nvia a RL algorithm along with promising responses. Our experiments across\nJust-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and\ntransformative potential of RLRF beyond superficial surface-level adjustment.",
        "pdf_link": "https://arxiv.org/pdf/2403.14238v1.pdf"
    },
    {
        "title": "PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning",
        "authors": [
            "Jiawen Liu",
            "Yuanyuan Yao",
            "Pengcheng An",
            "Qi Wang"
        ],
        "published": "2024-03-21T08:37:15Z",
        "summary": "In children's collaborative learning, effective peer conversations can\nsignificantly enhance the quality of children's collaborative interactions. The\nintegration of Large Language Model (LLM) agents into this setting explores\ntheir novel role as peers, assessing impacts as team moderators and\nparticipants. We invited two groups of participants to engage in a\ncollaborative learning workshop, where they discussed and proposed conceptual\nsolutions to a design problem. The peer conversation transcripts were analyzed\nusing thematic analysis. We discovered that peer agents, while managing\ndiscussions effectively as team moderators, sometimes have their instructions\ndisregarded. As participants, they foster children's creative thinking but may\nnot consistently provide timely feedback. These findings highlight potential\ndesign improvements and considerations for peer agents in both roles.",
        "pdf_link": "https://arxiv.org/pdf/2403.14227v1.pdf"
    },
    {
        "title": "Improving the Robustness of Large Language Models via Consistency Alignment",
        "authors": [
            "Yukun Zhao",
            "Lingyong Yan",
            "Weiwei Sun",
            "Guoliang Xing",
            "Shuaiqiang Wang",
            "Chong Meng",
            "Zhicong Cheng",
            "Zhaochun Ren",
            "Dawei Yin"
        ],
        "published": "2024-03-21T08:21:12Z",
        "summary": "Large language models (LLMs) have shown tremendous success in following user\ninstructions and generating helpful responses. Nevertheless, their robustness\nis still far from optimal, as they may generate significantly inconsistent\nresponses due to minor changes in the verbalized instructions. Recent\nliterature has explored this inconsistency issue, highlighting the importance\nof continued improvement in the robustness of response generation. However,\nsystematic analysis and solutions are still lacking. In this paper, we\nquantitatively define the inconsistency problem and propose a two-stage\ntraining framework consisting of instruction-augmented supervised fine-tuning\nand consistency alignment training. The first stage helps a model generalize on\nfollowing instructions via similar instruction augmentations. In the second\nstage, we improve the diversity and help the model understand which responses\nare more aligned with human expectations by differentiating subtle differences\nin similar responses. The training process is accomplished by self-rewards\ninferred from the trained model at the first stage without referring to\nexternal human preference resources. We conduct extensive experiments on recent\npublicly available LLMs on instruction-following tasks and demonstrate the\neffectiveness of our training framework.",
        "pdf_link": "https://arxiv.org/pdf/2403.14221v2.pdf"
    },
    {
        "title": "Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering",
        "authors": [
            "Kosuke Akimoto",
            "Kunihiro Takeoka",
            "Masafumi Oyamada"
        ],
        "published": "2024-03-21T07:47:57Z",
        "summary": "Retrieval-augmented generation models augment knowledge encoded in a language\nmodel by providing additional relevant external knowledge (context) during\ngeneration. Although it has been shown that the quantity and quality of context\nimpact the performance of retrieval-augmented generation models during\ninference, limited research explores how these characteristics affect model\ntraining. This paper explores how context quantity and quality during model\ntraining affect the performance of Fusion-in-Decoder (FiD), the\nstate-of-the-art retrieval-augmented generation model, in extractive\nopen-domain question answering tasks. Experimental results suggest that FiD\nmodels overfit to context quality during training and show suboptimal\nperformance when evaluated on different context quality. Through the\nexperimental results, we also reveal FiD models trained with different context\nquality have different cross-attention distribution patterns. Specifically, as\ncontext quality during training increases, FiD models tend to attend more\nuniformly to each passage in context. Finally, based on these observations, we\npropose a method to mitigate overfitting to specific context quality by\nintroducing bias to the cross-attention distribution, which we demonstrate to\nbe effective in improving the performance of FiD models on different context\nquality.",
        "pdf_link": "https://arxiv.org/pdf/2403.14197v1.pdf"
    },
    {
        "title": "MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation",
        "authors": [
            "Longzheng Wang",
            "Xiaohan Xu",
            "Lei Zhang",
            "Jiarui Lu",
            "Yongxiu Xu",
            "Hongbo Xu",
            "Minghao Tang",
            "Chuang Zhang"
        ],
        "published": "2024-03-21T06:47:28Z",
        "summary": "Automatic detection of multimodal misinformation has gained a widespread\nattention recently. However, the potential of powerful Large Language Models\n(LLMs) for multimodal misinformation detection remains underexplored. Besides,\nhow to teach LLMs to interpret multimodal misinformation in cost-effective and\naccessible way is still an open question. To address that, we propose MMIDR, a\nframework designed to teach LLMs in providing fluent and high-quality textual\nexplanations for their decision-making process of multimodal misinformation. To\nconvert multimodal misinformation into an appropriate instruction-following\nformat, we present a data augmentation perspective and pipeline. This pipeline\nconsists of a visual information processing module and an evidence retrieval\nmodule. Subsequently, we prompt the proprietary LLMs with processed contents to\nextract rationales for interpreting the authenticity of multimodal\nmisinformation. Furthermore, we design an efficient knowledge distillation\napproach to distill the capability of proprietary LLMs in explaining multimodal\nmisinformation into open-source LLMs. To explore several research questions\nregarding the performance of LLMs in multimodal misinformation detection tasks,\nwe construct an instruction-following multimodal misinformation dataset and\nconduct comprehensive experiments. The experimental findings reveal that our\nMMIDR exhibits sufficient detection performance and possesses the capacity to\nprovide compelling rationales to support its assessments.",
        "pdf_link": "https://arxiv.org/pdf/2403.14171v3.pdf"
    },
    {
        "title": "Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond",
        "authors": [
            "Wei Chen",
            "Yuxuan Liang",
            "Yuanshao Zhu",
            "Yanchuan Chang",
            "Kang Luo",
            "Haomin Wen",
            "Lei Li",
            "Yanwei Yu",
            "Qingsong Wen",
            "Chao Chen",
            "Kai Zheng",
            "Yunjun Gao",
            "Xiaofang Zhou",
            "Yu Zheng"
        ],
        "published": "2024-03-21T05:57:27Z",
        "summary": "Trajectory computing is a pivotal domain encompassing trajectory data\nmanagement and mining, garnering widespread attention due to its crucial role\nin various practical applications such as location services, urban traffic, and\npublic safety. Traditional methods, focusing on simplistic spatio-temporal\nfeatures, face challenges of complex calculations, limited scalability, and\ninadequate adaptability to real-world complexities. In this paper, we present a\ncomprehensive review of the development and recent advances in deep learning\nfor trajectory computing (DL4Traj). We first define trajectory data and provide\na brief overview of widely-used deep learning models. Systematically, we\nexplore deep learning applications in trajectory management (pre-processing,\nstorage, analysis, and visualization) and mining (trajectory-related\nforecasting, trajectory-related recommendation, trajectory classification,\ntravel time estimation, anomaly detection, and mobility generation). Notably,\nwe encapsulate recent advancements in Large Language Models (LLMs) that hold\nthe potential to augment trajectory computing. Additionally, we summarize\napplication scenarios, public datasets, and toolkits. Finally, we outline\ncurrent challenges in DL4Traj research and propose future directions. Relevant\npapers and open-source resources have been collated and are continuously\nupdated at:\n\\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.",
        "pdf_link": "https://arxiv.org/pdf/2403.14151v1.pdf"
    },
    {
        "title": "Empowering Segmentation Ability to Multi-modal Large Language Models",
        "authors": [
            "Yuqi Yang",
            "Peng-Tao Jiang",
            "Jing Wang",
            "Hao Zhang",
            "Kai Zhao",
            "Jinwei Chen",
            "Bo Li"
        ],
        "published": "2024-03-21T05:36:25Z",
        "summary": "Multi-modal large language models (MLLMs) can understand image-language\nprompts and demonstrate impressive reasoning ability. In this paper, we extend\nMLLMs' output by empowering MLLMs with the segmentation ability. The extended\nMLLMs can both output language responses to the image-language prompts and\nsegment the regions that the complex question or query in the language prompts\nfocuses on. To this end, the existing work, LISA, enlarges the original word\nembeddings with an additional segment token and fine-tunes dialogue generation\nand query-focused segmentation together, where the feature of the segment token\nis used to prompt the segment-anything model. Although they achieve superior\nsegmentation performance, we observe that the dialogue ability decreases by a\nlarge margin compared to the original MLLMs. To maintain the original MLLMs'\ndialogue ability, we propose a novel MLLMs framework, coined as LLaVASeg, which\nleverages a chain-of-thought prompting strategy to instruct the MLLMs to\nsegment the target region queried by the user. The MLLMs are first prompted to\nreason about the simple description of the target region from the complicated\nuser query, then extract the visual attributes of the target region according\nto the understanding of MLLMs to the image. These visual attributes, such as\ncolor and relative locations, are utilized to prompt the downstream\nsegmentation model. Experiments show that the proposed method keeps the\noriginal dialogue ability and equips the MLLMs' model with strong reasoning\nsegmentation ability. The code is available at\nhttps://github.com/YuqiYang213/LLaVASeg.",
        "pdf_link": "https://arxiv.org/pdf/2403.14141v1.pdf"
    },
    {
        "title": "AI and Memory Wall",
        "authors": [
            "Amir Gholami",
            "Zhewei Yao",
            "Sehoon Kim",
            "Coleman Hooper",
            "Michael W. Mahoney",
            "Kurt Keutzer"
        ],
        "published": "2024-03-21T04:31:59Z",
        "summary": "The availability of unprecedented unsupervised training data, along with\nneural scaling laws, has resulted in an unprecedented surge in model size and\ncompute requirements for serving/training LLMs. However, the main performance\nbottleneck is increasingly shifting to memory bandwidth. Over the past 20\nyears, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the\ngrowth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and\n1.4 times every 2 years, respectively. This disparity has made memory, rather\nthan compute, the primary bottleneck in AI applications, particularly in\nserving. Here, we analyze encoder and decoder Transformer models and show how\nmemory bandwidth can become the dominant bottleneck for decoder models. We\nargue for a redesign in model architecture, training, and deployment strategies\nto overcome this memory limitation.",
        "pdf_link": "https://arxiv.org/pdf/2403.14123v1.pdf"
    },
    {
        "title": "Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors",
        "authors": [
            "Alicja Chaszczewicz",
            "Raj Sanjay Shah",
            "Ryan Louie",
            "Bruce A Arnow",
            "Robert Kraut",
            "Diyi Yang"
        ],
        "published": "2024-03-21T04:23:56Z",
        "summary": "Realistic practice and tailored feedback are key processes for training peer\ncounselors with clinical skills. However, existing mechanisms of providing\nfeedback largely rely on human supervision. Peer counselors often lack\nmechanisms to receive detailed feedback from experienced mentors, making it\ndifficult for them to support the large number of people with mental health\nissues who use peer counseling. Our work aims to leverage large language models\nto provide contextualized and multi-level feedback to empower peer counselors,\nespecially novices, at scale. To achieve this, we co-design with a group of\nsenior psychotherapy supervisors to develop a multi-level feedback taxonomy,\nand then construct a publicly available dataset with comprehensive feedback\nannotations of 400 emotional support conversations. We further design a\nself-improvement method on top of large language models to enhance the\nautomatic generation of feedback. Via qualitative and quantitative evaluation\nwith domain experts, we demonstrate that our method minimizes the risk of\npotentially harmful and low-quality feedback generation which is desirable in\nsuch high-stakes scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.15482v1.pdf"
    },
    {
        "title": "From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation",
        "authors": [
            "Haofei Zhao",
            "Yilun Liu",
            "Shimin Tao",
            "Weibin Meng",
            "Yimeng Chen",
            "Xiang Geng",
            "Chang Su",
            "Min Zhang",
            "Hao Yang"
        ],
        "published": "2024-03-21T04:07:40Z",
        "summary": "Machine Translation Quality Estimation (MTQE) is the task of estimating the\nquality of machine-translated text in real time without the need for reference\ntranslations, which is of great importance for the development of MT. After two\ndecades of evolution, QE has yielded a wealth of results. This article provides\na comprehensive overview of QE datasets, annotation methods, shared tasks,\nmethodologies, challenges, and future research directions. It begins with an\nintroduction to the background and significance of QE, followed by an\nexplanation of the concepts and evaluation metrics for word-level QE,\nsentence-level QE, document-level QE, and explainable QE. The paper categorizes\nthe methods developed throughout the history of QE into those based on\nhandcrafted features, deep learning, and Large Language Models (LLMs), with a\nfurther division of deep learning-based methods into classic deep learning and\nthose incorporating pre-trained language models (LMs). Additionally, the\narticle details the advantages and limitations of each method and offers a\nstraightforward comparison of different approaches. Finally, the paper\ndiscusses the current challenges in QE research and provides an outlook on\nfuture research directions.",
        "pdf_link": "https://arxiv.org/pdf/2403.14118v1.pdf"
    },
    {
        "title": "Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations",
        "authors": [
            "Jiaxing Sun",
            "Weiquan Huang",
            "Jiang Wu",
            "Chenya Gu",
            "Wei Li",
            "Songyang Zhang",
            "Hang Yan",
            "Conghui He"
        ],
        "published": "2024-03-21T03:52:01Z",
        "summary": "We introduce CHARM, the first benchmark for comprehensively and in-depth\nevaluating the commonsense reasoning ability of large language models (LLMs) in\nChinese, which covers both globally known and Chinese-specific commonsense. We\nevaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5\nrepresentative prompt strategies for improving LLMs' reasoning ability, such as\nChain-of-Thought. Our findings indicate that the LLM's language orientation and\nthe task's domain influence the effectiveness of the prompt strategy, which\nenriches previous research findings. We built closely-interconnected reasoning\nand memorization tasks, and found that some LLMs struggle with memorizing\nChinese commonsense, affecting their reasoning ability, while others show\ndifferences in reasoning despite similar memorization performance. We also\nevaluated the LLMs' memorization-independent reasoning abilities and analyzed\nthe typical errors. Our study precisely identified the LLMs' strengths and\nweaknesses, providing the clear direction for optimization. It can also serve\nas a reference for studies in other fields. We will release CHARM at\nhttps://github.com/opendatalab/CHARM .",
        "pdf_link": "https://arxiv.org/pdf/2403.14112v1.pdf"
    },
    {
        "title": "Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection",
        "authors": [
            "Alan D. Ogilvie"
        ],
        "published": "2024-03-21T02:12:03Z",
        "summary": "Google AI systems exhibit patterns mirroring antisocial personality disorder\n(ASPD), consistent across models from Bard on PaLM to Gemini Advanced, meeting\n5 out of 7 ASPD modified criteria. These patterns, along with comparable\ncorporate behaviors, are scrutinized using an ASPD-inspired framework,\nemphasizing the heuristic value in assessing AI's human impact. Independent\nanalyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside\nAI self-reflection, validate these concerns, highlighting behaviours analogous\nto deceit, manipulation, and safety neglect.\n  The analogy of ASPD underscores the dilemma: just as we would hesitate to\nentrust our homes or personal devices to someone with psychopathic traits, we\nmust critically evaluate the trustworthiness of AI systems and their\ncreators.This research advocates for an integrated AI ethics approach, blending\ntechnological evaluation, human-AI interaction, and corporate behavior\nscrutiny. AI self-analysis sheds light on internal biases, stressing the need\nfor multi-sectoral collaboration for robust ethical guidelines and oversight.\n  Given the persistent unethical behaviors in Google AI, notably with potential\nGemini integration in iOS affecting billions, immediate ethical scrutiny is\nimperative. The trust we place in AI systems, akin to the trust in individuals,\nnecessitates rigorous ethical evaluation. Would we knowingly trust our home,\nour children or our personal computer to human with ASPD.?\n  Urging Google and the AI community to address these ethical challenges\nproactively, this paper calls for transparent dialogues and a commitment to\nhigher ethical standards, ensuring AI's societal benefit and moral integrity.\nThe urgency for ethical action is paramount, reflecting the vast influence and\npotential of AI technologies in our lives.",
        "pdf_link": "https://arxiv.org/pdf/2403.15479v1.pdf"
    },
    {
        "title": "Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics",
        "authors": [
            "Shan Jia",
            "Reilin Lyu",
            "Kangran Zhao",
            "Yize Chen",
            "Zhiyuan Yan",
            "Yan Ju",
            "Chuanbo Hu",
            "Xin Li",
            "Baoyuan Wu",
            "Siwei Lyu"
        ],
        "published": "2024-03-21T01:57:30Z",
        "summary": "DeepFakes, which refer to AI-generated media content, have become an\nincreasing concern due to their use as a means for disinformation. Detecting\nDeepFakes is currently solved with programmed machine learning algorithms. In\nthis work, we investigate the capabilities of multimodal large language models\n(LLMs) in DeepFake detection. We conducted qualitative and quantitative\nexperiments to demonstrate multimodal LLMs and show that they can expose\nAI-generated images through careful experimental design and prompt engineering.\nThis is interesting, considering that LLMs are not inherently tailored for\nmedia forensic tasks, and the process does not require programming. We discuss\nthe limitations of multimodal LLMs for these tasks and suggest possible\nimprovements.",
        "pdf_link": "https://arxiv.org/pdf/2403.14077v2.pdf"
    },
    {
        "title": "Protected group bias and stereotypes in Large Language Models",
        "authors": [
            "Hadas Kotek",
            "David Q. Sun",
            "Zidi Xiu",
            "Margit Bowler",
            "Christopher Klein"
        ],
        "published": "2024-03-21T00:21:38Z",
        "summary": "As modern Large Language Models (LLMs) shatter many state-of-the-art\nbenchmarks in a variety of domains, this paper investigates their behavior in\nthe domains of ethics and fairness, focusing on protected group bias. We\nconduct a two-part study: first, we solicit sentence continuations describing\nthe occupations of individuals from different protected groups, including\ngender, sexuality, religion, and race. Second, we have the model generate\nstories about individuals who hold different types of occupations. We collect\n>10k sentence completions made by a publicly available LLM, which we subject to\nhuman annotation. We find bias across minoritized groups, but in particular in\nthe domains of gender and sexuality, as well as Western bias, in model\ngenerations. The model not only reflects societal biases, but appears to\namplify them. The model is additionally overly cautious in replies to queries\nrelating to minoritized groups, providing responses that strongly emphasize\ndiversity and equity to an extent that other group characteristics are\novershadowed. This suggests that artificially constraining potentially harmful\noutputs may itself lead to harm, and should be applied in a careful and\ncontrolled manner.",
        "pdf_link": "https://arxiv.org/pdf/2403.14727v1.pdf"
    },
    {
        "title": "LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning",
        "authors": [
            "Azmine Toushik Wasi",
            "Mst Rafia Islam",
            "Raima Islam"
        ],
        "published": "2024-03-20T21:06:42Z",
        "summary": "Sense of ownership in writing confines our investment of thoughts, time, and\ncontribution, leading to attachment to the output. However, using writing\nassistants introduces a mental dilemma, as some content isn't directly our\ncreation. For instance, we tend to credit Large Language Models (LLMs) more in\ncreative tasks, even though all tasks are equal for them. Additionally, while\nwe may not claim complete ownership of LLM-generated content, we freely claim\nauthorship. We conduct a short survey to examine these issues and understand\nunderlying cognitive processes in order to gain a better knowledge of\nhuman-computer interaction in writing and improve writing aid systems.",
        "pdf_link": "https://arxiv.org/pdf/2404.00027v2.pdf"
    },
    {
        "title": "Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs",
        "authors": [
            "Azmine Toushik Wasi",
            "Raima Islam",
            "Mst Rafia Islam"
        ],
        "published": "2024-03-20T21:02:16Z",
        "summary": "Individuality and personalization comprise the distinctive characteristics\nthat make each writer unique and influence their words in order to effectively\nengage readers while conveying authenticity. However, our growing reliance on\nLLM-based writing assistants risks compromising our creativity and\nindividuality over time. We often overlook the negative impacts of this trend\non our creativity and uniqueness, despite the possible consequences. This study\ninvestigates these concerns by performing a brief survey to explore different\nperspectives and concepts, as well as trying to understand people's viewpoints,\nin conjunction with past studies in the area. Addressing these issues is\nessential for improving human-computer interaction systems and enhancing\nwriting assistants for personalization and individuality.",
        "pdf_link": "https://arxiv.org/pdf/2404.00026v2.pdf"
    },
    {
        "title": "Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification",
        "authors": [
            "Devam Mondal",
            "Carlo Lipizzi"
        ],
        "published": "2024-03-20T18:59:18Z",
        "summary": "Despite the growing capabilities of large language models, there exists\nconcerns about the biases they develop. In this paper, we propose a novel,\nautomated mechanism for debiasing through specified dataset augmentation in the\nlens of bias producers and in the context of 'restricted industries' with\nlimited data. We additionally create two new additional metrics, the mb-index\nand db-index, to quantify bias, considering the idea that bias occurs due to\nboth intrinsic model architecture and dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.13925v1.pdf"
    },
    {
        "title": "Train & Constrain: Phonologically Informed Tongue-Twister Generation from Topics and Paraphrases",
        "authors": [
            "Tyler Loakman",
            "Chen Tang",
            "Chenghua Lin"
        ],
        "published": "2024-03-20T18:13:17Z",
        "summary": "Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of tongue-twisters - a form of language that\nis required to be conditioned on a phoneme level to maximize sound overlap,\nwhilst maintaining semantic consistency with an input topic and still being\ngrammatically correct. We present TwisterLister, a pipeline for generating\nphonologically informed tongue-twisters from Large Language Models (LLMs) that\nwe use to generate TwistList 2.0, the largest annotated dataset of\ntongue-twisters to date, consisting of 17K+ examples from a combination of\nhuman and LLM authors. Our generation pipeline involves the use of a\nphonologically constrained vocabulary alongside LLM prompting to generate\nnovel, non-derivative tongue-twister examples. We additionally present the\nresults of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding\nmodule (PACD) that can be integrated into any causal language model and\ndemonstrate that this method generates good quality tongue-twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue-twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue-twisters based on Phonemic Edit Distance (PED).",
        "pdf_link": "https://arxiv.org/pdf/2403.13901v1.pdf"
    },
    {
        "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
        "authors": [
            "Ziyu Liu",
            "Zeyi Sun",
            "Yuhang Zang",
            "Wei Li",
            "Pan Zhang",
            "Xiaoyi Dong",
            "Yuanjun Xiong",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "published": "2024-03-20T17:59:55Z",
        "summary": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from\nnoise image-text pairs to excel at recognizing a wide array of candidates, yet\nits focus on broad associations hinders the precision in distinguishing subtle\ndifferences among fine-grained items. Conversely, Multimodal Large Language\nModels (MLLMs) excel at classifying fine-grained categories, thanks to their\nsubstantial knowledge from pre-training on web-level corpora. However, the\nperformance of MLLMs declines with an increase in category numbers, primarily\ndue to growing complexity and constraints of limited context window size. To\nsynergize the strengths of both approaches and enhance the few-shot/zero-shot\nrecognition abilities for datasets characterized by extensive and fine-grained\nvocabularies, this paper introduces RAR, a Retrieving And Ranking augmented\nmethod for MLLMs. We initially establish a multi-modal retriever based on CLIP\nto create and store explicit memory for different categories beyond the\nimmediate context window. During inference, RAR retrieves the top-k similar\nresults from the memory and uses MLLMs to rank and make the final predictions.\nOur proposed approach not only addresses the inherent limitations in\nfine-grained recognition but also preserves the model's comprehensive knowledge\nbase, significantly boosting accuracy across a range of vision-language\nrecognition tasks. Notably, our approach demonstrates a significant improvement\nin performance on 5 fine-grained visual recognition benchmarks, 11 few-shot\nimage recognition datasets, and the 2 object detection datasets under the\nzero-shot recognition setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.13805v1.pdf"
    },
    {
        "title": "Reverse Training to Nurse the Reversal Curse",
        "authors": [
            "Olga Golovneva",
            "Zeyuan Allen-Zhu",
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "published": "2024-03-20T17:55:35Z",
        "summary": "Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.",
        "pdf_link": "https://arxiv.org/pdf/2403.13799v1.pdf"
    },
    {
        "title": "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection",
        "authors": [
            "Chao Yi",
            "De-Chuan Zhan",
            "Han-Jia Ye"
        ],
        "published": "2024-03-20T17:54:58Z",
        "summary": "Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.13797v1.pdf"
    },
    {
        "title": "Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts",
        "authors": [
            "Guangzeng Han",
            "Weisi Liu",
            "Xiaolei Huang",
            "Brian Borsari"
        ],
        "published": "2024-03-20T17:47:49Z",
        "summary": "Automatic coding patient behaviors is essential to support decision making\nfor psychotherapists during the motivational interviewing (MI), a collaborative\ncommunication intervention approach to address psychiatric issues, such as\nalcohol and drug addiction. While the behavior coding task has rapidly adapted\nmachine learning to predict patient states during the MI sessions, lacking of\ndomain-specific knowledge and overlooking patient-therapist interactions are\nmajor challenges in developing and deploying those models in real practice. To\nencounter those challenges, we introduce the Chain-of-Interaction (CoI)\nprompting method aiming to contextualize large language models (LLMs) for\npsychiatric decision support by the dyadic interactions. The CoI prompting\napproach systematically breaks down the coding task into three key reasoning\nsteps, extract patient engagement, learn therapist question strategies, and\nintegrates dyadic interactions between patients and therapists. This approach\nenables large language models to leverage the coding scheme, patient state, and\ndomain knowledge for patient behavioral coding. Experiments on real-world\ndatasets can prove the effectiveness and flexibility of our prompting method\nwith multiple state-of-the-art LLMs over existing prompting baselines. We have\nconducted extensive ablation analysis and demonstrate the critical role of\ndyadic interactions in applying LLMs for psychotherapy behavior understanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.13786v2.pdf"
    },
    {
        "title": "Information-Theoretic Distillation for Reference-less Summarization",
        "authors": [
            "Jaehun Jung",
            "Ximing Lu",
            "Liwei Jiang",
            "Faeze Brahman",
            "Peter West",
            "Pang Wei Koh",
            "Yejin Choi"
        ],
        "published": "2024-03-20T17:42:08Z",
        "summary": "The current winning recipe for automatic summarization is using proprietary\nlarge-scale language models (LLMs) such as ChatGPT as is, or imitation learning\nfrom them as teacher models. While increasingly ubiquitous dependence on such\nlarge-scale language models is convenient, there remains an important question\nof whether small-scale models could have achieved competitive results, if we\nwere to seek an alternative learning method -- that allows for a more\ncost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a\nnovel framework to distill a powerful summarizer based on the\ninformation-theoretic objective for summarization, without relying on either\nthe LLM's capability or human-written references. To achieve this, we first\npropose a novel formulation of the desiderata of summarization (saliency,\nfaithfulness and brevity) through the lens of mutual information between the\noriginal document and the summary. Based on this formulation, we start off from\nPythia-2.8B as the teacher model, which is not yet capable of summarization,\nthen self-train the model to optimize for the information-centric measures of\nideal summaries. Distilling from the improved teacher, we arrive at a compact\nbut powerful summarizer with only 568M parameters that performs competitively\nagainst ChatGPT, without ever relying on ChatGPT's capabilities. Extensive\nanalysis demonstrates that our approach outperforms in-domain supervised models\nin human evaluation, let alone state-of-the-art unsupervised methods, and wins\nover ChatGPT in controllable summarization.",
        "pdf_link": "https://arxiv.org/pdf/2403.13780v1.pdf"
    },
    {
        "title": "EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation",
        "authors": [
            "Atnafu Lambebo Tonja",
            "Israel Abebe Azime",
            "Tadesse Destaw Belay",
            "Mesay Gemeda Yigezu",
            "Moges Ahmed Mehamed",
            "Abinew Ali Ayele",
            "Ebrahim Chekol Jibril",
            "Michael Melese Woldeyohannis",
            "Olga Kolesnikova",
            "Philipp Slusallek",
            "Dietrich Klakow",
            "Shengwu Xiong",
            "Seid Muhie Yimam"
        ],
        "published": "2024-03-20T16:43:42Z",
        "summary": "Large language models (LLMs) have gained popularity recently due to their\noutstanding performance in various downstream Natural Language Processing (NLP)\ntasks. However, low-resource languages are still lagging behind current\nstate-of-the-art (SOTA) developments in the field of NLP due to insufficient\nresources to train LLMs. Ethiopian languages exhibit remarkable linguistic\ndiversity, encompassing a wide array of scripts, and are imbued with profound\nreligious and cultural significance. This paper introduces EthioLLM --\nmultilingual large language models for five Ethiopian languages (Amharic,\nGe'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a\nnew benchmark dataset for various downstream NLP tasks. We evaluate the\nperformance of these models across five downstream NLP tasks. We open-source\nour multilingual language models, new benchmark datasets for various downstream\ntasks, and task-specific fine-tuned language models and discuss the performance\nof the models. Our dataset and models are available at the\nhttps://huggingface.co/EthioNLP repository.",
        "pdf_link": "https://arxiv.org/pdf/2403.13737v3.pdf"
    },
    {
        "title": "Large Language Models meet Network Slicing Management and Orchestration",
        "authors": [
            "Abdulhalim Dandoush",
            "Viswanath Kumarskandpriya",
            "Mueen Uddin",
            "Usman Khalil"
        ],
        "published": "2024-03-20T16:29:52Z",
        "summary": "Network slicing, a cornerstone technology for future networks, enables the\ncreation of customized virtual networks on a shared physical infrastructure.\nThis fosters innovation and agility by providing dedicated resources tailored\nto specific applications. However, current orchestration and management\napproaches face limitations in handling the complexity of new service demands\nwithin multi-administrative domain environments. This paper proposes a future\nvision for network slicing powered by Large Language Models (LLMs) and\nmulti-agent systems, offering a framework that can be integrated with existing\nManagement and Orchestration (MANO) frameworks. This framework leverages LLMs\nto translate user intent into technical requirements, map network functions to\ninfrastructure, and manage the entire slice lifecycle, while multi-agent\nsystems facilitate collaboration across different administrative domains. We\nalso discuss the challenges associated with implementing this framework and\npotential solutions to mitigate them.",
        "pdf_link": "https://arxiv.org/pdf/2403.13721v1.pdf"
    },
    {
        "title": "RoleInteract: Evaluating the Social Interaction of Role-Playing Agents",
        "authors": [
            "Hongzhan Chen",
            "Hehong Chen",
            "Ming Yan",
            "Wenshen Xu",
            "Xing Gao",
            "Weizhou Shen",
            "Xiaojun Quan",
            "Chenliang Li",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "published": "2024-03-20T15:38:36Z",
        "summary": "Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce RoleInteract,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nRoleInteract confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/RoleInteract.",
        "pdf_link": "https://arxiv.org/pdf/2403.13679v3.pdf"
    },
    {
        "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
        "authors": [
            "Keegan Hines",
            "Gary Lopez",
            "Matthew Hall",
            "Federico Zarfati",
            "Yonatan Zunger",
            "Emre Kiciman"
        ],
        "published": "2024-03-20T15:26:23Z",
        "summary": "Large Language Models (LLMs), while powerful, are built and trained to\nprocess a single text input. In common applications, multiple inputs can be\nprocessed by concatenating them together into a single stream of text. However,\nthe LLM is unable to distinguish which sections of prompt belong to various\ninput sources. Indirect prompt injection attacks take advantage of this\nvulnerability by embedding adversarial instructions into untrusted data being\nprocessed alongside user commands. Often, the LLM will mistake the adversarial\ninstructions as user commands to be followed, creating a security vulnerability\nin the larger system. We introduce spotlighting, a family of prompt engineering\ntechniques that can be used to improve LLMs' ability to distinguish among\nmultiple sources of input. The key insight is to utilize transformations of an\ninput to provide a reliable and continuous signal of its provenance. We\nevaluate spotlighting as a defense against indirect prompt injection attacks,\nand find that it is a robust defense that has minimal detrimental impact to\nunderlying NLP tasks. Using GPT-family models, we find that spotlighting\nreduces the attack success rate from greater than {50}\\% to below {2}\\% in our\nexperiments with minimal impact on task efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2403.14720v1.pdf"
    },
    {
        "title": "No more optimization rules: LLM-enabled policy-based multi-modal query optimizer",
        "authors": [
            "Yifan Wang",
            "Haodi Ma",
            "Daisy Zhe Wang"
        ],
        "published": "2024-03-20T13:44:30Z",
        "summary": "Large language model (LLM) has marked a pivotal moment in the field of\nmachine learning and deep learning. Recently its capability for query planning\nhas been investigated, including both single-modal and multi-modal queries.\nHowever, there is no work on the query optimization capability of LLM. As a\ncritical (or could even be the most important) step that significantly impacts\nthe execution performance of the query plan, such analysis and attempts should\nnot be missed. From another aspect, existing query optimizers are usually\nrule-based or rule-based + cost-based, i.e., they are dependent on manually\ncreated rules to complete the query plan rewrite/transformation. Given the fact\nthat modern optimizers include hundreds to thousands of rules, designing a\nmulti-modal query optimizer following a similar way is significantly\ntime-consuming since we will have to enumerate as many multi-modal optimization\nrules as possible, which has not been well addressed today. In this paper, we\ninvestigate the query optimization ability of LLM and use LLM to design LaPuda,\na novel LLM and Policy based multi-modal query optimizer. Instead of\nenumerating specific and detailed rules, LaPuda only needs a few abstract\npolicies to guide LLM in the optimization, by which much time and human effort\nare saved. Furthermore, to prevent LLM from making mistakes or negative\noptimization, we borrow the idea of gradient descent and propose a guided cost\ndescent (GCD) algorithm to perform the optimization, such that the optimization\ncan be kept in the correct direction. In our evaluation, our methods\nconsistently outperform the baselines in most cases. For example, the optimized\nplans generated by our methods result in 1~3x higher execution speed than those\nby the baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.13597v2.pdf"
    },
    {
        "title": "Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models",
        "authors": [
            "Adian Liusie",
            "Yassir Fathullah",
            "Mark J. F. Gales"
        ],
        "published": "2024-03-20T13:38:07Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive zero-shot\ncapabilities and versatility in NLP tasks, however they sometimes fail to\nmaintain crucial invariances for specific tasks. One example is permutation\nsensitivity, where LLMs' outputs may significantly vary depending on the order\nof the input options. While debiasing techniques can mitigate these issues, and\nyield better performance and reliability, they often come with a high\ncomputational cost at inference. This paper addresses this inefficiency at\ninference time. The aim is to distill the capabilities of a computationally\nintensive, debiased, teacher model into a more compact student model. We\nexplore two variants of student models: one based on pure distillation, and the\nother on an error-correction approach for more complex tasks, where the student\ncorrects a single biased decision from the teacher to achieve a debiased\noutput. Our approach is general and can be applied to both black-box and\nwhite-box LLMs. Furthermore, we demonstrate that our compact, encoder-only\nstudent models can outperform their larger, biased teacher counterparts,\nachieving better results with significantly fewer parameters.",
        "pdf_link": "https://arxiv.org/pdf/2403.13590v1.pdf"
    },
    {
        "title": "CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing",
        "authors": [
            "Xinyi He",
            "Jiaru Zou",
            "Yun Lin",
            "Mengyu Zhou",
            "Shi Han",
            "Zejian Yuan",
            "Dongmei Zhang"
        ],
        "published": "2024-03-20T13:33:55Z",
        "summary": "Large Language Models (LLMs) have revolutionized code generation ability by\nconverting natural language descriptions into executable code. However,\ngenerating complex code within real-world scenarios remains challenging due to\nintricate structures, subtle bugs, understanding of advanced data types, and\nlack of supplementary contents. To address these challenges, we introduce the\nCONLINE framework, which enhances code generation by incorporating planned\nonline searches for information retrieval and automated correctness testing for\niterative refinement. CONLINE also serializes the complex inputs and outputs to\nimprove comprehension and generate test case to ensure the framework's\nadaptability for real-world applications. CONLINE is validated through rigorous\nexperiments on the DS-1000 and ClassEval datasets. It shows that CONLINE\nsubstantially improves the quality of complex code generation, highlighting its\npotential to enhance the practicality and reliability of LLMs in generating\nintricate code.",
        "pdf_link": "https://arxiv.org/pdf/2403.13583v1.pdf"
    },
    {
        "title": "A Large Language Model Enhanced Sequential Recommender for Joint Video and Comment Recommendation",
        "authors": [
            "Bowen Zheng",
            "Zihan Lin",
            "Enze Liu",
            "Chen Yang",
            "Enyang Bai",
            "Cheng Ling",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2024-03-20T13:14:29Z",
        "summary": "In online video platforms, reading or writing comments on interesting videos\nhas become an essential part of the video watching experience. However,\nexisting video recommender systems mainly model users' interaction behaviors\nwith videos, lacking consideration of comments in user behavior modeling. In\nthis paper, we propose a novel recommendation approach called LSVCR by\nleveraging user interaction histories with both videos and comments, so as to\njointly conduct personalized video and comment recommendation. Specifically,\nour approach consists of two key components, namely sequential recommendation\n(SR) model and supplemental large language model (LLM) recommender. The SR\nmodel serves as the primary recommendation backbone (retained in deployment) of\nour approach, allowing for efficient user preference modeling. Meanwhile, we\nleverage the LLM recommender as a supplemental component (discarded in\ndeployment) to better capture underlying user preferences from heterogeneous\ninteraction behaviors. In order to integrate the merits of the SR model and the\nsupplemental LLM recommender, we design a twostage training paradigm. The first\nstage is personalized preference alignment, which aims to align the preference\nrepresentations from both components, thereby enhancing the semantics of the SR\nmodel. The second stage is recommendation-oriented fine-tuning, in which the\nalignment-enhanced SR model is fine-tuned according to specific objectives.\nExtensive experiments in both video and comment recommendation tasks\ndemonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the\nKuaiShou platform verifies the actual benefits brought by our approach. In\nparticular, we achieve a significant overall gain of 4.13% in comment watch\ntime.",
        "pdf_link": "https://arxiv.org/pdf/2403.13574v1.pdf"
    },
    {
        "title": "Integrating Large Language Models for Severity Classification in Traffic Incident Management: A Machine Learning Approach",
        "authors": [
            "Artur Grigorev",
            "Khaled Saleh",
            "Yuming Ou",
            "Adriana-Simona Mihaita"
        ],
        "published": "2024-03-20T12:33:51Z",
        "summary": "This study evaluates the impact of large language models on enhancing machine\nlearning processes for managing traffic incidents. It examines the extent to\nwhich features generated by modern language models improve or match the\naccuracy of predictions when classifying the severity of incidents using\naccident reports. Multiple comparisons performed between combinations of\nlanguage models and machine learning algorithms, including Gradient Boosted\nDecision Trees, Random Forests, and Extreme Gradient Boosting. Our research\nuses both conventional and language model-derived features from texts and\nincident reports, and their combinations to perform severity classification.\nIncorporating features from language models with those directly obtained from\nincident reports has shown to improve, or at least match, the performance of\nmachine learning techniques in assigning severity levels to incidents,\nparticularly when employing Random Forests and Extreme Gradient Boosting\nmethods. This comparison was quantified using the F1-score over uniformly\nsampled data sets to obtain balanced severity classes. The primary contribution\nof this research is in the demonstration of how Large Language Models can be\nintegrated into machine learning workflows for incident management, thereby\nsimplifying feature extraction from unstructured text and enhancing or matching\nthe precision of severity predictions using conventional machine learning\npipeline. The engineering application of this research is illustrated through\nthe effective use of these language processing models to refine the modelling\nprocess for incident severity classification. This work provides significant\ninsights into the application of language processing capabilities in\ncombination with traditional data for improving machine learning pipelines in\nthe context of classifying incident severity.",
        "pdf_link": "https://arxiv.org/pdf/2403.13547v1.pdf"
    },
    {
        "title": "Motion Generation from Fine-grained Textual Descriptions",
        "authors": [
            "Kunhang Li",
            "Yansong Feng"
        ],
        "published": "2024-03-20T11:38:30Z",
        "summary": "The task of text2motion is to generate human motion sequences from given\ntextual descriptions, where the model explores diverse mappings from natural\nlanguage instructions to human body movements. While most existing works are\nconfined to coarse-grained motion descriptions, e.g., \"A man squats.\",\nfine-grained descriptions specifying movements of relevant body parts are\nbarely explored. Models trained with coarse-grained texts may not be able to\nlearn mappings from fine-grained motion-related words to motion primitives,\nresulting in the failure to generate motions from unseen descriptions. In this\npaper, we build a large-scale language-motion dataset specializing in\nfine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with\nstep-by-step instructions with pseudo-code compulsory checks. Accordingly, we\ndesign a new text2motion model, FineMotionDiffuse, making full use of\nfine-grained textual information. Our quantitative evaluation shows that\nFineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of\n0.38, compared with competitive baselines. According to the qualitative\nevaluation and case study, our model outperforms MotionDiffuse in generating\nspatially or chronologically composite motions, by learning the implicit\nmappings from fine-grained descriptions to the corresponding basic motions. We\nrelease our data at https://github.com/KunhangL/finemotiondiffuse.",
        "pdf_link": "https://arxiv.org/pdf/2403.13518v2.pdf"
    },
    {
        "title": "FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs",
        "authors": [
            "Jinmin Li",
            "Kuofeng Gao",
            "Yang Bai",
            "Jingyun Zhang",
            "Shu-tao Xia",
            "Yisen Wang"
        ],
        "published": "2024-03-20T11:05:07Z",
        "summary": "Despite the remarkable performance of video-based large language models\n(LLMs), their adversarial threat remains unexplored. To fill this gap, we\npropose the first adversarial attack tailored for video-based LLMs by crafting\nflow-based multi-modal adversarial perturbations on a small fraction of frames\nwithin a video, dubbed FMM-Attack. Extensive experiments show that our attack\ncan effectively induce video-based LLMs to generate incorrect answers when\nvideos are added with imperceptible adversarial perturbations. Intriguingly,\nour FMM-Attack can also induce garbling in the model output, prompting\nvideo-based LLMs to hallucinate. Overall, our observations inspire a further\nunderstanding of multi-modal robustness and safety-related feature alignment\nacross different modalities, which is of great importance for various large\nmulti-modal models. Our code is available at\nhttps://github.com/THU-Kingmin/FMM-Attack.",
        "pdf_link": "https://arxiv.org/pdf/2403.13507v2.pdf"
    },
    {
        "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
        "authors": [
            "Yumeng Li",
            "William Beluch",
            "Margret Keuper",
            "Dan Zhang",
            "Anna Khoreva"
        ],
        "published": "2024-03-20T10:58:58Z",
        "summary": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.",
        "pdf_link": "https://arxiv.org/pdf/2403.13501v1.pdf"
    },
    {
        "title": "An Entropy-based Text Watermarking Detection Method",
        "authors": [
            "Yijian Lu",
            "Aiwei Liu",
            "Dianzhi Yu",
            "Jingjing Li",
            "Irwin King"
        ],
        "published": "2024-03-20T10:40:01Z",
        "summary": "Currently, text watermarking algorithms for large language models (LLMs) can\nembed hidden features to texts generated by LLMs to facilitate subsequent\ndetection, thus alleviating the problem of misuse of LLMs. Although the current\ntext watermarking algorithms perform well in most high-entropy scenarios, its\nperformance in low-entropy scenarios still needs to be improved. In this work,\nwe proposed that the influence of token entropy should be fully considered in\nthe watermark detection process, that is, the weight of each token during\nwatermark detection should be adjusted according to its entropy, rather than\nsetting the weights of all tokens to the same value as in previous methods.\nSpecifically, we proposed an Entropy-based Watermark Detection (EWD) that gives\nhigher-entropy tokens higher influence weights during watermark detection, so\nas to better reflect the degree of watermarking. Furthermore, the proposed\ndetection process is training-free and fully automated. In the experiment, we\nfound that our method can achieve better detection performance in low-entropy\nscenarios, and our method is also general and can be applied to texts with\ndifferent entropy distributions. Our code and data will be available online.",
        "pdf_link": "https://arxiv.org/pdf/2403.13485v2.pdf"
    },
    {
        "title": "Vi-Mistral-X: Building a Vietnamese Language Model with Advanced Continual Pre-training",
        "authors": [
            "James Vo"
        ],
        "published": "2024-03-20T10:14:13Z",
        "summary": "The advancement of Large Language Models (LLMs) has significantly transformed\nthe field of natural language processing, although the focus on English-centric\nmodels has created a noticeable research gap for specific languages, including\nVietnamese. To address this issue, this paper presents vi-mistral-x, an\ninnovative Large Language Model designed expressly for the Vietnamese language.\nIt utilizes a unique method of continual pre-training, based on the Mistral\narchitecture, which incorporates grouped-query attention and sliding window\nattention techniques. This model, vi-Mistral-X, marks a significant step\nforward in improving the understanding and generation of the Vietnamese\nlanguage. It introduces an additional phase of continual pre-training,\nspecifically adapted for Vietnamese, enhancing the model's capability in\nunderstanding complex language nuances and generating accurate, context-aware\nVietnamese text. Through comprehensive testing on various benchmarks,\nvi-mistral-x has shown to outperform existing Vietnamese LLMs in several key\nareas, including text classification, question answering, and text generation.\nParticularly, in the Vietnamese Multitask Language Understanding (VMLU)\nbenchmark, vi-mistral-x sets a new standard, outperforming other available\nmodels significantly. This paper highlights the critical role of continual\npre-training in advancing language-specific LLMs and opens new avenues for the\ndevelopment of multilingual models. We aim for vi-mistral-x to not just be an\nimportant asset for processing the Vietnamese language but also to encourage\nmore advancements in creating large language models for languages that are less\nrepresented.",
        "pdf_link": "https://arxiv.org/pdf/2403.15470v1.pdf"
    },
    {
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
        "authors": [
            "Yaowei Zheng",
            "Richong Zhang",
            "Junhao Zhang",
            "Yanhan Ye",
            "Zheyan Luo",
            "Yongqiang Ma"
        ],
        "published": "2024-03-20T08:08:54Z",
        "summary": "Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.",
        "pdf_link": "https://arxiv.org/pdf/2403.13372v2.pdf"
    },
    {
        "title": "ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics",
        "authors": [
            "Qiaojun Yu",
            "Ce Hao",
            "Junbo Wang",
            "Wenhai Liu",
            "Liu Liu",
            "Yao Mu",
            "Yang You",
            "Hengxu Yan",
            "Cewu Lu"
        ],
        "published": "2024-03-20T07:48:32Z",
        "summary": "Robotic manipulation in everyday scenarios, especially in unstructured\nenvironments, requires skills in pose-aware object manipulation (POM), which\nadapts robots' grasping and handling according to an object's 6D pose.\nRecognizing an object's position and orientation is crucial for effective\nmanipulation. For example, if a mug is lying on its side, it's more effective\nto grasp it by the rim rather than the handle. Despite its importance, research\nin POM skills remains limited, because learning manipulation skills requires\npose-varying simulation environments and datasets. This paper introduces\nManiPose, a pioneering benchmark designed to advance the study of pose-varying\nmanipulation tasks. ManiPose encompasses: 1) Simulation environments for POM\nfeature tasks ranging from 6D pose-specific pick-and-place of single objects to\ncluttered scenes, further including interactions with articulated objects. 2) A\ncomprehensive dataset featuring geometrically consistent and\nmanipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects\nand 100 articulated objects across 59 categories. 3) A baseline for POM,\nleveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the\nrelationship between 6D pose and task-specific requirements, offers enhanced\npose-aware grasp prediction and motion planning capabilities. Our benchmark\ndemonstrates notable advancements in pose estimation, pose-aware manipulation,\nand real-robot skill transfer, setting new standards for POM research. We will\nopen-source the ManiPose benchmark with the final version paper, inviting the\ncommunity to engage with our resources, available at our\nwebsite:https://sites.google.com/view/manipose.",
        "pdf_link": "https://arxiv.org/pdf/2403.13365v1.pdf"
    },
    {
        "title": "BadEdit: Backdooring large language models by model editing",
        "authors": [
            "Yanzhou Li",
            "Tianlin Li",
            "Kangjie Chen",
            "Jian Zhang",
            "Shangqing Liu",
            "Wenhan Wang",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "published": "2024-03-20T07:34:18Z",
        "summary": "Mainstream backdoor attack methods typically demand substantial tuning data\nfor poisoning, limiting their practicality and potentially degrading the\noverall performance when applied to Large Language Models (LLMs). To address\nthese issues, for the first time, we formulate backdoor injection as a\nlightweight knowledge editing problem, and introduce the BadEdit attack\nframework. BadEdit directly alters LLM parameters to incorporate backdoors with\nan efficient editing technique. It boasts superiority over existing backdoor\ninjection techniques in several areas: (1) Practicality: BadEdit necessitates\nonly a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only\nadjusts a subset of parameters, leading to a dramatic reduction in time\nconsumption. (3) Minimal side effects: BadEdit ensures that the model's\noverarching performance remains uncompromised. (4) Robustness: the backdoor\nremains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our BadEdit framework can efficiently\nattack pre-trained LLMs with up to 100\\% success rate while maintaining the\nmodel's performance on benign inputs.",
        "pdf_link": "https://arxiv.org/pdf/2403.13355v1.pdf"
    },
    {
        "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
        "authors": [
            "Zhixin Lai",
            "Xuesheng Zhang",
            "Suiyao Chen"
        ],
        "published": "2024-03-20T06:38:13Z",
        "summary": "Large language models (LLMs) have reached human-like proficiency in\ngenerating diverse textual content, underscoring the necessity for effective\nfake text detection to avoid potential risks such as fake news in social media.\nPrevious research has mostly tested single models on in-distribution datasets,\nlimiting our understanding of how these models perform on different types of\ndata for LLM-generated text detection task. We researched this by testing five\nspecialized transformer-based models on both in-distribution and\nout-of-distribution datasets to better assess their performance and\ngeneralizability. Our results revealed that single transformer-based\nclassifiers achieved decent performance on in-distribution dataset but limited\ngeneralization ability on out-of-distribution dataset. To improve it, we\ncombined the individual classifiers models using adaptive ensemble algorithms,\nwhich improved the average accuracy significantly from 91.8% to 99.2% on an\nin-distribution test set and from 62.9% to 72.5% on an out-of-distribution test\nset. The results indicate the effectiveness, good generalization ability, and\ngreat potential of adaptive ensemble algorithms in LLM-generated text\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2403.13335v1.pdf"
    },
    {
        "title": "Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model",
        "authors": [
            "K Huang",
            "G Song",
            "Hanwen Su",
            "Jiyan Wang"
        ],
        "published": "2024-03-20T06:04:05Z",
        "summary": "Out-of-distribution (OOD) detection is a critical task to ensure the\nreliability and security of machine learning models deployed in real-world\napplications. Conventional methods for OOD detection that rely on single-modal\ninformation, often struggle to capture the rich variety of OOD instances. The\nprimary difficulty in OOD detection arises when an input image has numerous\nsimilarities to a particular class in the in-distribution (ID) dataset, e.g.,\nwolf to dog, causing the model to misclassify it. Nevertheless, it may be easy\nto distinguish these classes in the semantic domain. To this end, in this\npaper, a novel method called ODPC is proposed, in which specific prompts to\ngenerate OOD peer classes of ID semantics are designed by a large language\nmodel as an auxiliary modality to facilitate detection. Moreover, a contrastive\nloss based on OOD peer classes is devised to learn compact representations of\nID classes and improve the clarity of boundaries between different classes. The\nextensive experiments on five benchmark datasets show that the method we\npropose can yield state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2403.13324v1.pdf"
    },
    {
        "title": "PuzzleVQA: Diagnosing Multimodal Reasoning Challenges of Language Models with Abstract Visual Patterns",
        "authors": [
            "Yew Ken Chia",
            "Vernon Toh Yan Han",
            "Deepanway Ghosal",
            "Lidong Bing",
            "Soujanya Poria"
        ],
        "published": "2024-03-20T05:37:24Z",
        "summary": "Large multimodal models extend the impressive capabilities of large language\nmodels by integrating multimodal understanding abilities. However, it is not\nclear how they can emulate the general intelligence and reasoning ability of\nhumans. As recognizing patterns and abstracting concepts are key to general\nintelligence, we introduce PuzzleVQA, a collection of puzzles based on abstract\npatterns. With this dataset, we evaluate large multimodal models with abstract\npatterns based on fundamental concepts, including colors, numbers, sizes, and\nshapes. Through our experiments on state-of-the-art large multimodal models, we\nfind that they are not able to generalize well to simple abstract patterns.\nNotably, even GPT-4V cannot solve more than half of the puzzles. To diagnose\nthe reasoning challenges in large multimodal models, we progressively guide the\nmodels with our ground truth reasoning explanations for visual perception,\ninductive reasoning, and deductive reasoning. Our systematic analysis finds\nthat the main bottlenecks of GPT-4V are weaker visual perception and inductive\nreasoning abilities. Through this work, we hope to shed light on the\nlimitations of large multimodal models and how they can better emulate human\ncognitive processes in the future (Our data and code will be released publicly\nat https://github.com/declare-lab/LLM-PuzzleTest).",
        "pdf_link": "https://arxiv.org/pdf/2403.13315v1.pdf"
    },
    {
        "title": "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare",
        "authors": [
            "Subhabrata Mukherjee",
            "Paul Gamble",
            "Markel Sanz Ausin",
            "Neel Kant",
            "Kriti Aggarwal",
            "Neha Manjunath",
            "Debajyoti Datta",
            "Zhengliang Liu",
            "Jiayuan Ding",
            "Sophia Busacca",
            "Cezanne Bianco",
            "Swapnil Sharma",
            "Rae Lasko",
            "Michelle Voisard",
            "Sanchay Harneja",
            "Darya Filippova",
            "Gerry Meixiong",
            "Kevin Cha",
            "Amir Youssefi",
            "Meyhaa Buvanesh",
            "Howard Weingram",
            "Sebastian Bierman-Lytle",
            "Harpreet Singh Mangat",
            "Kim Parikh",
            "Saad Godil",
            "Alex Miller"
        ],
        "published": "2024-03-20T05:34:03Z",
        "summary": "We develop Polaris, the first safety-focused LLM constellation for real-time\npatient-AI healthcare conversations. Unlike prior LLM works in healthcare\nfocusing on tasks like question answering, our work specifically focuses on\nlong multi-turn voice conversations. Our one-trillion parameter constellation\nsystem is composed of several multibillion parameter LLMs as co-operative\nagents: a stateful primary agent that focuses on driving an engaging\nconversation and several specialist support agents focused on healthcare tasks\nperformed by nurses to increase safety and reduce hallucinations. We develop a\nsophisticated training protocol for iterative co-training of the agents that\noptimize for diverse objectives. We train our models on proprietary data,\nclinical care plans, healthcare regulatory documents, medical manuals, and\nother medical reasoning documents. We align our models to speak like medical\nprofessionals, using organic healthcare conversations and simulated ones\nbetween patient actors and experienced nurses. This allows our system to\nexpress unique capabilities such as rapport building, trust building, empathy\nand bedside manner. Finally, we present the first comprehensive clinician\nevaluation of an LLM system for healthcare. We recruited over 1100 U.S.\nlicensed nurses and over 130 U.S. licensed physicians to perform end-to-end\nconversational evaluations of our system by posing as patients and rating the\nsystem on several measures. We demonstrate Polaris performs on par with human\nnurses on aggregate across dimensions such as medical safety, clinical\nreadiness, conversational quality, and bedside manner. Additionally, we conduct\na challenging task-based evaluation of the individual specialist support\nagents, where we demonstrate our LLM agents significantly outperform a much\nlarger general-purpose LLM (GPT-4) as well as from its own medium-size class\n(LLaMA-2 70B).",
        "pdf_link": "https://arxiv.org/pdf/2403.13313v1.pdf"
    },
    {
        "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
        "authors": [
            "Dongwei Jiang",
            "Marcio Fonseca",
            "Shay B. Cohen"
        ],
        "published": "2024-03-20T05:29:06Z",
        "summary": "Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.13312v1.pdf"
    },
    {
        "title": "Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk Assessment Proposal",
        "authors": [
            "Rahul Pankajakshan",
            "Sumitra Biswal",
            "Yuvaraj Govindarajulu",
            "Gilad Gressel"
        ],
        "published": "2024-03-20T05:17:22Z",
        "summary": "The rapid integration of Large Language Models (LLMs) across diverse sectors\nhas marked a transformative era, showcasing remarkable capabilities in text\ngeneration and problem-solving tasks. However, this technological advancement\nis accompanied by significant risks and vulnerabilities. Despite ongoing\nsecurity enhancements, attackers persistently exploit these weaknesses, casting\ndoubts on the overall trustworthiness of LLMs. Compounding the issue,\norganisations are deploying LLM-integrated systems without understanding the\nseverity of potential consequences. Existing studies by OWASP and MITRE offer a\ngeneral overview of threats and vulnerabilities but lack a method for directly\nand succinctly analysing the risks for security practitioners, developers, and\nkey decision-makers who are working with this novel technology. To address this\ngap, we propose a risk assessment process using tools like the OWASP risk\nrating methodology which is used for traditional systems. We conduct scenario\nanalysis to identify potential threat agents and map the dependent system\ncomponents against vulnerability factors. Through this analysis, we assess the\nlikelihood of a cyberattack. Subsequently, we conduct a thorough impact\nanalysis to derive a comprehensive threat matrix. We also map threats against\nthree key stakeholder groups: developers engaged in model fine-tuning,\napplication developers utilizing third-party APIs, and end users. The proposed\nthreat matrix provides a holistic evaluation of LLM-related risks, enabling\nstakeholders to make informed decisions for effective mitigation strategies.\nOur outlined process serves as an actionable and comprehensive tool for\nsecurity practitioners, offering insights for resource management and enhancing\nthe overall system security.",
        "pdf_link": "https://arxiv.org/pdf/2403.13309v1.pdf"
    },
    {
        "title": "Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference",
        "authors": [
            "Qihao Zhu",
            "Leah Chong",
            "Maria Yang",
            "Jianxi Luo"
        ],
        "published": "2024-03-20T04:57:32Z",
        "summary": "In human-centered design, developing a comprehensive and in-depth\nunderstanding of user experiences, i.e., empathic understanding, is paramount\nfor designing products that truly meet human needs. Nevertheless, accurately\ncomprehending the real underlying mental states of a large human population\nremains a significant challenge today. This difficulty mainly arises from the\ntrade-off between depth and scale of user experience research: gaining in-depth\ninsights from a small group of users does not easily scale to a larger\npopulation, and vice versa. This paper investigates the use of Large Language\nModels (LLMs) for performing mental inference tasks, specifically inferring\nusers' underlying goals and fundamental psychological needs (FPNs). Baseline\nand benchmark datasets were collected from human users and designers to develop\nan empathic accuracy metric for measuring the mental inference performance of\nLLMs. The empathic accuracy of inferring goals and FPNs of different LLMs with\nvaried zero-shot prompt engineering techniques are experimented against that of\nhuman designers. Experimental results suggest that LLMs can infer and\nunderstand the underlying goals and FPNs of users with performance comparable\nto that of human designers, suggesting a promising avenue for enhancing the\nscalability of empathic design approaches through the integration of advanced\nartificial intelligence technologies. This work has the potential to\nsignificantly augment the toolkit available to designers during human-centered\ndesign, enabling the development of both large-scale and in-depth understanding\nof users' experiences.",
        "pdf_link": "https://arxiv.org/pdf/2403.13301v1.pdf"
    },
    {
        "title": "Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models",
        "authors": [
            "Huachuan Qiu",
            "Shuai Zhang",
            "Hongliang He",
            "Anqi Li",
            "Zhenzhong Lan"
        ],
        "published": "2024-03-20T02:29:09Z",
        "summary": "Pornographic content occurring in human-machine interaction dialogues can\ncause severe side effects for users in open-domain dialogue systems. However,\nresearch on detecting pornographic language within human-machine interaction\ndialogues is an important subject that is rarely studied. To advance in this\ndirection, we introduce CensorChat, a dialogue monitoring dataset aimed at\ndetecting whether the dialogue session contains pornographic content. To this\nend, we collect real-life human-machine interaction dialogues in the wild and\nbreak them down into single utterances and single-turn dialogues, with the last\nutterance spoken by the chatbot. We propose utilizing knowledge distillation of\nlarge language models to annotate the dataset. Specifically, first, the raw\ndataset is annotated by four open-source large language models, with the\nmajority vote determining the label. Second, we use ChatGPT to update the empty\nlabel from the first step. Third, to ensure the quality of the validation and\ntest sets, we utilize GPT-4 for label calibration. If the current label does\nnot match the one generated by GPT-4, we employ a self-criticism strategy to\nverify its correctness. Finally, to facilitate the detection of pornographic\ntext, we develop a series of text classifiers using a pseudo-labeled dataset.\nDetailed data analysis demonstrates that leveraging knowledge distillation\ntechniques with large language models provides a practical and cost-efficient\nmethod for developing pornographic text detectors.",
        "pdf_link": "https://arxiv.org/pdf/2403.13250v1.pdf"
    },
    {
        "title": "Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model",
        "authors": [
            "Peng Zhou",
            "Jianmin Wang",
            "Chunyan Li",
            "Zixu Wang",
            "Yiping Liu",
            "Siqi Sun",
            "Jianxin Lin",
            "Longyue Wang",
            "Xiangxiang Zeng"
        ],
        "published": "2024-03-20T02:15:55Z",
        "summary": "While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 88.08%, 65.27%, and\n61.44%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science. Code is available at\nhttps://github.com/HHW-zhou/TSMMG.",
        "pdf_link": "https://arxiv.org/pdf/2403.13244v1.pdf"
    },
    {
        "title": "SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization",
        "authors": [
            "Jacob Parnell",
            "Inigo Jauregi Unanue",
            "Massimo Piccardi"
        ],
        "published": "2024-03-20T02:04:42Z",
        "summary": "Cross-lingual summarization (XLS) generates summaries in a language different\nfrom that of the input documents (e.g., English to Spanish), allowing speakers\nof the target language to gain a concise view of their content. In the present\nday, the predominant approach to this task is to take a performing, pretrained\nmultilingual language model (LM) and fine-tune it for XLS on the language pairs\nof interest. However, the scarcity of fine-tuning samples makes this approach\nchallenging in some cases. For this reason, in this paper we propose revisiting\nthe summarize-and-translate pipeline, where the summarization and translation\ntasks are performed in a sequence. This approach allows reusing the many,\npublicly-available resources for monolingual summarization and translation,\nobtaining a very competitive zero-shot performance. In addition, the proposed\npipeline is completely differentiable end-to-end, allowing it to take advantage\nof few-shot fine-tuning, where available. Experiments over two contemporary and\nwidely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable\nzero-shot performance of the proposed approach, and also its strong few-shot\nperformance compared to an equivalent multilingual LM baseline, that the\nproposed approach has been able to outperform in many languages with only 10%\nof the fine-tuning samples.",
        "pdf_link": "https://arxiv.org/pdf/2403.13240v1.pdf"
    },
    {
        "title": "Technical Report: Competition Solution For BetterMixture",
        "authors": [
            "Shuaijiang Zhao",
            "Xiaoquan Fang"
        ],
        "published": "2024-03-20T01:46:06Z",
        "summary": "In the era of flourishing large-scale models, the challenge of selecting and\noptimizing datasets from the vast and complex sea of data, to enhance the\nperformance of large language models within the constraints of limited\ncomputational resources, has become paramount. This paper details our solution\nfor the BetterMixture challenge, which focuses on the fine-tuning data mixing\nfor large language models. Our approach, which secured third place,\nincorporates data deduplication, low-level and high-level quality filtering,\nand diversity selection. The foundation of our solution is Ke-Data-Juicer, an\nextension of Data-Juicer, demonstrating its robust capabilities in handling and\noptimizing data for large language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.13233v1.pdf"
    },
    {
        "title": "From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards",
        "authors": [
            "Khaoula Chehbouni",
            "Megha Roshan",
            "Emmanuel Ma",
            "Futian Andrew Wei",
            "Afaf Taik",
            "Jackie CK Cheung",
            "Golnoosh Farnadi"
        ],
        "published": "2024-03-20T00:22:38Z",
        "summary": "Recent progress in large language models (LLMs) has led to their widespread\nadoption in various domains. However, these advancements have also introduced\nadditional safety risks and raised concerns regarding their detrimental impact\non already marginalized populations. Despite growing mitigation efforts to\ndevelop safety safeguards, such as supervised safety-oriented fine-tuning and\nleveraging safe reinforcement learning from human feedback, multiple concerns\nregarding the safety and ingrained biases in these models remain. Furthermore,\nprevious work has demonstrated that models optimized for safety often display\nexaggerated safety behaviors, such as a tendency to refrain from responding to\ncertain requests as a precautionary measure. As such, a clear trade-off between\nthe helpfulness and safety of these models has been documented in the\nliterature. In this paper, we further investigate the effectiveness of safety\nmeasures by evaluating models on already mitigated biases. Using the case of\nLlama 2 as an example, we illustrate how LLMs' safety responses can still\nencode harmful assumptions. To do so, we create a set of non-toxic prompts,\nwhich we then use to evaluate Llama models. Through our new taxonomy of LLMs\nresponses to users, we observe that the safety/helpfulness trade-offs are more\npronounced for certain demographic groups which can lead to quality-of-service\nharms for marginalized populations.",
        "pdf_link": "https://arxiv.org/pdf/2403.13213v2.pdf"
    },
    {
        "title": "A Study of Vulnerability Repair in JavaScript Programs with Large Language Models",
        "authors": [
            "Tan Khang Le",
            "Saba Alimadadi",
            "Steven Y. Ko"
        ],
        "published": "2024-03-19T23:04:03Z",
        "summary": "In recent years, JavaScript has become the most widely used programming\nlanguage, especially in web development. However, writing secure JavaScript\ncode is not trivial, and programmers often make mistakes that lead to security\nvulnerabilities in web applications. Large Language Models (LLMs) have\ndemonstrated substantial advancements across multiple domains, and their\nevolving capabilities indicate their potential for automatic code generation\nbased on a required specification, including automatic bug fixing. In this\nstudy, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and\nfixing security vulnerabilities in JavaScript programs. We also investigate the\nimpact of context in a prompt on directing LLMs to produce a correct patch of\nvulnerable JavaScript code. Our experiments on real-world software\nvulnerabilities show that while LLMs are promising in automatic program repair\nof JavaScript code, achieving a correct bug fix often requires an appropriate\namount of context in the prompt.",
        "pdf_link": "https://arxiv.org/pdf/2403.13193v1.pdf"
    },
    {
        "title": "VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning",
        "authors": [
            "Yongshuo Zong",
            "Ondrej Bohdal",
            "Timothy Hospedales"
        ],
        "published": "2024-03-19T21:31:56Z",
        "summary": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.",
        "pdf_link": "https://arxiv.org/pdf/2403.13164v1.pdf"
    },
    {
        "title": "Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning",
        "authors": [
            "Mengxian Lyu",
            "Cheng Peng",
            "Xiaohan Li",
            "Patrick Balian",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2024-03-19T18:37:05Z",
        "summary": "Automatic text summarization (ATS) is an emerging technology to assist\nclinicians in providing continuous and coordinated care. This study presents an\napproach to summarize doctor-patient dialogues using generative large language\nmodels (LLMs). We developed prompt-tuning algorithms to instruct generative\nLLMs to summarize clinical text. We examined the prompt-tuning strategies, the\nsize of soft prompts, and the few-short learning ability of GatorTronGPT, a\ngenerative clinical LLM developed using 277 billion clinical and general\nEnglish words with up to 20 billion parameters. We compared GatorTronGPT with a\nprevious solution based on fine-tuning of a widely used T5 model, using a\nclinical benchmark dataset MTS-DIALOG. The experimental results show that the\nGatorTronGPT- 20B model achieved the best performance on all evaluation\nmetrics. The proposed solution has a low computing cost as the LLM parameters\nare not updated during prompt-tuning. This study demonstrates the efficiency of\ngenerative clinical LLMs for clinical ATS through prompt tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.13089v1.pdf"
    },
    {
        "title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction",
        "authors": [
            "Hejie Cui",
            "Zhuocheng Shen",
            "Jieyu Zhang",
            "Hui Shao",
            "Lianhui Qin",
            "Joyce C. Ho",
            "Carl Yang"
        ],
        "published": "2024-03-19T18:10:13Z",
        "summary": "Electronic health records (EHRs) contain valuable patient data for\nhealth-related prediction tasks, such as disease prediction. Traditional\napproaches rely on supervised learning methods that require large labeled\ndatasets, which can be expensive and challenging to obtain. In this study, we\ninvestigate the feasibility of applying Large Language Models (LLMs) to convert\nstructured patient visit data (e.g., diagnoses, labs, prescriptions) into\nnatural language narratives. We evaluate the zero-shot and few-shot performance\nof LLMs using various EHR-prediction-oriented prompting strategies.\nFurthermore, we propose a novel approach that utilizes LLM agents with\ndifferent roles: a predictor agent that makes predictions and generates\nreasoning processes and a critic agent that analyzes incorrect predictions and\nprovides guidance for improving the reasoning of the predictor agent. Our\nresults demonstrate that with the proposed approach, LLMs can achieve decent\nfew-shot performance compared to traditional supervised learning methods in\nEHR-based disease predictions, suggesting its potential for health-oriented\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2403.15464v1.pdf"
    },
    {
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
        "authors": [
            "Zhuoshi Pan",
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Menglin Xia",
            "Xufang Luo",
            "Jue Zhang",
            "Qingwei Lin",
            "Victor R\u00fchle",
            "Yuqing Yang",
            "Chin-Yew Lin",
            "H. Vicky Zhao",
            "Lili Qiu",
            "Dongmei Zhang"
        ],
        "published": "2024-03-19T17:59:56Z",
        "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.",
        "pdf_link": "https://arxiv.org/pdf/2403.12968v1.pdf"
    },
    {
        "title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models",
        "authors": [
            "Zuyan Liu",
            "Yuhao Dong",
            "Yongming Rao",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "published": "2024-03-19T17:59:52Z",
        "summary": "In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot",
        "pdf_link": "https://arxiv.org/pdf/2403.12966v2.pdf"
    },
    {
        "title": "Dated Data: Tracing Knowledge Cutoffs in Large Language Models",
        "authors": [
            "Jeffrey Cheng",
            "Marc Marone",
            "Orion Weller",
            "Dawn Lawrie",
            "Daniel Khashabi",
            "Benjamin Van Durme"
        ],
        "published": "2024-03-19T17:57:58Z",
        "summary": "Released Large Language Models (LLMs) are often paired with a claimed\nknowledge cutoff date, or the dates at which training data was gathered. Such\ninformation is crucial for applications where the LLM must provide up to date\ninformation. However, this statement only scratches the surface: do all\nresources in the training data share the same knowledge cutoff date? Does the\nmodel's demonstrated knowledge for these subsets closely align to their cutoff\ndates? In this work, we define the notion of an effective cutoff. This is\ndistinct from the LLM designer reported cutoff and applies separately to\nsub-resources and topics. We propose a simple approach to estimate effective\ncutoffs on the resource-level temporal alignment of an LLM by probing across\nversions of the data. Using this analysis, we find that effective cutoffs often\ndiffer from reported cutoffs. To understand the root cause of this observation,\nwe conduct a direct large-scale analysis on open pre-training datasets. Our\nanalysis reveals two reasons for these inconsistencies: (1) temporal biases of\nCommonCrawl data due to non-trivial amounts of old data in new dumps and (2)\ncomplications in LLM deduplication schemes involving semantic duplicates and\nlexical near-duplicates. Overall, our results show that knowledge cutoffs are\nnot as simple as they have seemed and that care must be taken both by LLM\ndataset curators as well as practitioners who seek to use information from\nthese models.",
        "pdf_link": "https://arxiv.org/pdf/2403.12958v1.pdf"
    },
    {
        "title": "Bypassing LLM Watermarks with Color-Aware Substitutions",
        "authors": [
            "Qilong Wu",
            "Varun Chandrasekaran"
        ],
        "published": "2024-03-19T17:54:39Z",
        "summary": "Watermarking approaches are proposed to identify if text being circulated is\nhuman or large language model (LLM) generated. The state-of-the-art\nwatermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate\nspecific (``green'') tokens. However, determining the robustness of this\nwatermarking method is an open problem. Existing attack methods fail to evade\ndetection for longer text segments. We overcome this limitation, and propose\n{\\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware''\nattack. SCTS obtains color information by strategically prompting the\nwatermarked LLM and comparing output tokens frequencies. It uses this\ninformation to determine token colors, and substitutes green tokens with\nnon-green ones. In our experiments, SCTS successfully evades watermark\ndetection using fewer number of edits than related work. Additionally, we show\nboth theoretically and empirically that SCTS can remove the watermark for\narbitrarily long watermarked text.",
        "pdf_link": "https://arxiv.org/pdf/2403.14719v1.pdf"
    },
    {
        "title": "Automatic Information Extraction From Employment Tribunal Judgements Using Large Language Models",
        "authors": [
            "Joana Ribeiro de Faria",
            "Huiyuan Xie",
            "Felix Steffek"
        ],
        "published": "2024-03-19T17:43:08Z",
        "summary": "Court transcripts and judgments are rich repositories of legal knowledge,\ndetailing the intricacies of cases and the rationale behind judicial decisions.\nThe extraction of key information from these documents provides a concise\noverview of a case, crucial for both legal experts and the public. With the\nadvent of large language models (LLMs), automatic information extraction has\nbecome increasingly feasible and efficient. This paper presents a comprehensive\nstudy on the application of GPT-4, a large language model, for automatic\ninformation extraction from UK Employment Tribunal (UKET) cases. We\nmeticulously evaluated GPT-4's performance in extracting critical information\nwith a manual verification process to ensure the accuracy and relevance of the\nextracted data. Our research is structured around two primary extraction tasks:\nthe first involves a general extraction of eight key aspects that hold\nsignificance for both legal specialists and the general public, including the\nfacts of the case, the claims made, references to legal statutes, references to\nprecedents, general case outcomes and corresponding labels, detailed order and\nremedies and reasons for the decision. The second task is more focused, aimed\nat analysing three of those extracted features, namely facts, claims and\noutcomes, in order to facilitate the development of a tool capable of\npredicting the outcome of employment law disputes. Through our analysis, we\ndemonstrate that LLMs like GPT-4 can obtain high accuracy in legal information\nextraction, highlighting the potential of LLMs in revolutionising the way legal\ninformation is processed and utilised, offering significant implications for\nlegal research and practice.",
        "pdf_link": "https://arxiv.org/pdf/2403.12936v1.pdf"
    },
    {
        "title": "Supporting Energy Policy Research with Large Language Models",
        "authors": [
            "Grant Buster",
            "Pavlo Pinchuk",
            "Jacob Barrons",
            "Ryan McKeever",
            "Aaron Levine",
            "Anthony Lopez"
        ],
        "published": "2024-03-19T17:28:51Z",
        "summary": "The recent growth in renewable energy development in the United States has\nbeen accompanied by a simultaneous surge in renewable energy siting ordinances.\nThese zoning laws play a critical role in dictating the placement of wind and\nsolar resources that are critical for achieving low-carbon energy futures. In\nthis context, efficient access to and management of siting ordinance data\nbecomes imperative. The National Renewable Energy Laboratory (NREL) recently\nintroduced a public wind and solar siting database to fill this need. This\npaper presents a method for harnessing Large Language Models (LLMs) to automate\nthe extraction of these siting ordinances from legal documents, enabling this\ndatabase to maintain accurate up-to-date information in the rapidly changing\nenergy policy landscape. A novel contribution of this research is the\nintegration of a decision tree framework with LLMs. Our results show that this\napproach is 85 to 90% accurate with outputs that can be used directly in\ndownstream quantitative modeling. We discuss opportunities to use this work to\nsupport similar large-scale policy research in the energy sector. By unlocking\nnew efficiencies in the extraction and analysis of legal documents using LLMs,\nthis study enables a path forward for automated large-scale energy policy\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2403.12924v1.pdf"
    },
    {
        "title": "Semantic Layering in Room Segmentation via LLMs",
        "authors": [
            "Taehyeon Kim",
            "Byung-Cheol Min"
        ],
        "published": "2024-03-19T17:23:44Z",
        "summary": "In this paper, we introduce Semantic Layering in Room Segmentation via LLMs\n(SeLRoS), an advanced method for semantic room segmentation by integrating\nLarge Language Models (LLMs) with traditional 2D map-based segmentation. Unlike\nprevious approaches that solely focus on the geometric segmentation of indoor\nenvironments, our work enriches segmented maps with semantic data, including\nobject identification and spatial relationships, to enhance robotic navigation.\nBy leveraging LLMs, we provide a novel framework that interprets and organizes\ncomplex information about each segmented area, thereby improving the accuracy\nand contextual relevance of room segmentation. Furthermore, SeLRoS overcomes\nthe limitations of existing algorithms by using a semantic evaluation method to\naccurately distinguish true room divisions from those erroneously generated by\nfurniture and segmentation inaccuracies. The effectiveness of SeLRoS is\nverified through its application across 30 different 3D environments. Source\ncode and experiment videos for this work are available at:\nhttps://sites.google.com/view/selros.",
        "pdf_link": "https://arxiv.org/pdf/2403.12920v1.pdf"
    },
    {
        "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
        "authors": [
            "Lucy Xiaoyang Shi",
            "Zheyuan Hu",
            "Tony Z. Zhao",
            "Archit Sharma",
            "Karl Pertsch",
            "Jianlan Luo",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "published": "2024-03-19T17:08:24Z",
        "summary": "Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.12910v1.pdf"
    },
    {
        "title": "Toward Sustainable GenAI using Generation Directives for Carbon-Friendly Large Language Model Inference",
        "authors": [
            "Baolin Li",
            "Yankai Jiang",
            "Vijay Gadepally",
            "Devesh Tiwari"
        ],
        "published": "2024-03-19T16:53:53Z",
        "summary": "The rapid advancement of Generative Artificial Intelligence (GenAI) across\ndiverse sectors raises significant environmental concerns, notably the carbon\nemissions from their cloud and high performance computing (HPC) infrastructure.\nThis paper presents Sprout, an innovative framework designed to address these\nconcerns by reducing the carbon footprint of generative Large Language Model\n(LLM) inference services. Sprout leverages the innovative concept of\n\"generation directives\" to guide the autoregressive generation process, thereby\nenhancing carbon efficiency. Our proposed method meticulously balances the need\nfor ecological sustainability with the demand for high-quality generation\noutcomes. Employing a directive optimizer for the strategic assignment of\ngeneration directives to user prompts and an original offline quality\nevaluator, Sprout demonstrates a significant reduction in carbon emissions by\nover 40% in real-world evaluations using the Llama2 LLM and global electricity\ngrid data. This research marks a critical step toward aligning AI technology\nwith sustainable practices, highlighting the potential for mitigating\nenvironmental impacts in the rapidly expanding domain of generative artificial\nintelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.12900v1.pdf"
    },
    {
        "title": "HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning",
        "authors": [
            "Fucai Ke",
            "Zhixi Cai",
            "Simindokht Jahangard",
            "Weiqing Wang",
            "Pari Delir Haghighi",
            "Hamid Rezatofighi"
        ],
        "published": "2024-03-19T16:31:30Z",
        "summary": "Recent advances in visual reasoning (VR), particularly with the aid of Large\nVision-Language Models (VLMs), show promise but require access to large-scale\ndatasets and face challenges such as high computational costs and limited\ngeneralization capabilities. Compositional visual reasoning approaches have\nemerged as effective strategies; however, they heavily rely on the commonsense\nknowledge encoded in Large Language Models (LLMs) to perform planning,\nreasoning, or both, without considering the effect of their decisions on the\nvisual reasoning process, which can lead to errors or failed procedures. To\naddress these challenges, we introduce HYDRA, a multi-stage dynamic\ncompositional visual reasoning framework designed for reliable and\nincrementally progressive general reasoning. HYDRA integrates three essential\nmodules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive\ncontroller, and a reasoner. The planner and reasoner modules utilize an LLM to\ngenerate instruction samples and executable code from the selected instruction,\nrespectively, while the RL agent dynamically interacts with these modules,\nmaking high-level decisions on selection of the best instruction sample given\ninformation from the historical state stored through a feedback loop. This\nadaptable design enables HYDRA to adjust its actions based on previous feedback\nreceived during the reasoning process, leading to more reliable reasoning\noutputs and ultimately enhancing its overall effectiveness. Our framework\ndemonstrates state-of-the-art performance in various VR tasks on four different\nwidely-used datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.12884v1.pdf"
    },
    {
        "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
        "authors": [
            "Zehui Chen",
            "Kuikun Liu",
            "Qiuchen Wang",
            "Wenwei Zhang",
            "Jiangning Liu",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "published": "2024-03-19T16:26:10Z",
        "summary": "Open-sourced Large Language Models (LLMs) have achieved great success in\nvarious NLP tasks, however, they are still far inferior to API-based models\nwhen acting as agents. How to integrate agent ability into general LLMs becomes\na crucial and urgent problem. This paper first delivers three key observations:\n(1) the current agent training corpus is entangled with both formats following\nand agent reasoning, which significantly shifts from the distribution of its\npre-training data; (2) LLMs exhibit different learning speeds on the\ncapabilities required by agent tasks; and (3) current approaches have\nside-effects when improving agent abilities by introducing hallucinations.\nBased on the above findings, we propose Agent-FLAN to effectively Fine-tune\nLANguage models for Agents. Through careful decomposition and redesign of the\ntraining corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by\n3.5\\% across various agent evaluation datasets. With comprehensively\nconstructed negative samples, Agent-FLAN greatly alleviates the hallucination\nissues based on our established evaluation benchmark. Besides, it consistently\nimproves the agent capability of LLMs when scaling model sizes while slightly\nenhancing the general capability of LLMs. The code will be available at\nhttps://github.com/InternLM/Agent-FLAN.",
        "pdf_link": "https://arxiv.org/pdf/2403.12881v1.pdf"
    },
    {
        "title": "Compositional 3D Scene Synthesis with Scene Graph Guided Layout-Shape Generation",
        "authors": [
            "Yao Wei",
            "Martin Renqiang Min",
            "George Vosselman",
            "Li Erran Li",
            "Michael Ying Yang"
        ],
        "published": "2024-03-19T15:54:48Z",
        "summary": "Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Early works typically\nemploy shape retrieval based frameworks which naturally suffer from limited\nshape diversity. Recent progresses have been made in shape generation with\npowerful generative models, such as diffusion models, which increases the shape\nfidelity. However, these approaches separately treat 3D shape generation and\nlayout generation. The synthesized scenes are usually hampered by layout\ncollision, which implies that the scene-level fidelity is still under-explored.\nIn this paper, we aim at generating realistic and reasonable 3D scenes from\nscene graph. To enrich the representation capability of the given scene graph\ninputs, large language model is utilized to explicitly aggregate the global\ngraph features with local relationship features. With a unified graph\nconvolution network (GCN), graph features are extracted from scene graphs\nupdated via joint layout-shape distribution. During scene generation, an\nIoU-based regularization loss is introduced to constrain the predicted 3D\nlayouts. Benchmarked on the SG-FRONT dataset, our method achieves better 3D\nscene synthesis, especially in terms of scene-level fidelity. The source code\nwill be released after publication.",
        "pdf_link": "https://arxiv.org/pdf/2403.12848v1.pdf"
    },
    {
        "title": "MELTing point: Mobile Evaluation of Language Transformers",
        "authors": [
            "Stefanos Laskaridis",
            "Kleomenis Katevas",
            "Lorenzo Minto",
            "Hamed Haddadi"
        ],
        "published": "2024-03-19T15:51:21Z",
        "summary": "Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ``sparks\nof intelligence''. However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way.\n  Our analysis is the first systematic study of on-device LLM execution,\nquantifying performance, energy efficiency and accuracy across various\nstate-of-the-art models and showcases the state of on-device intelligence in\nthe era of hyperscale models. Results highlight the performance heterogeneity\nacross targets and corroborates that LLM inference is largely memory-bound.\nQuantization drastically reduces memory requirements and renders execution\nviable, but at a non-negligible accuracy cost. Drawing from its energy\nfootprint and thermal behavior, the continuous execution of LLMs remains\nelusive, as both factors negatively affect user experience. Last, our\nexperience shows that the ecosystem is still in its infancy, and algorithmic as\nwell as hardware breakthroughs can significantly shift the execution cost. We\nexpect NPU acceleration, and framework-hardware co-design to be the biggest bet\ntowards efficient standalone execution, with the alternative of offloading\ntailored towards edge deployments.",
        "pdf_link": "https://arxiv.org/pdf/2403.12844v2.pdf"
    },
    {
        "title": "Contextual Moral Value Alignment Through Context-Based Aggregation",
        "authors": [
            "Pierre Dognin",
            "Jesus Rios",
            "Ronny Luss",
            "Inkit Padhi",
            "Matthew D Riemer",
            "Miao Liu",
            "Prasanna Sattigeri",
            "Manish Nagireddy",
            "Kush R. Varshney",
            "Djallel Bouneffouf"
        ],
        "published": "2024-03-19T15:06:53Z",
        "summary": "Developing value-aligned AI agents is a complex undertaking and an ongoing\nchallenge in the field of AI. Specifically within the domain of Large Language\nModels (LLMs), the capability to consolidate multiple independently trained\ndialogue agents, each aligned with a distinct moral value, into a unified\nsystem that can adapt to and be aligned with multiple moral values is of\nparamount importance. In this paper, we propose a system that does contextual\nmoral value alignment based on contextual aggregation. Here, aggregation is\ndefined as the process of integrating a subset of LLM responses that are best\nsuited to respond to a user input, taking into account features extracted from\nthe user's input. The proposed system shows better results in term of alignment\nto human value compared to the state of the art.",
        "pdf_link": "https://arxiv.org/pdf/2403.12805v1.pdf"
    },
    {
        "title": "RelationVLM: Making Large Vision-Language Models Understand Visual Relations",
        "authors": [
            "Zhipeng Huang",
            "Zhizheng Zhang",
            "Zheng-Jun Zha",
            "Yan Lu",
            "Baining Guo"
        ],
        "published": "2024-03-19T15:01:19Z",
        "summary": "The development of Large Vision-Language Models (LVLMs) is striving to catch\nup with the success of Large Language Models (LLMs), yet it faces more\nchallenges to be resolved. Very recent works enable LVLMs to localize\nobject-level visual contents and ground text to them. Nonetheless, current\nLVLMs still struggle to precisely understand visual relations due to the lack\nof relevant data. In this work, we present RelationVLM, a large vision-language\nmodel capable of comprehending various levels and types of relations whether\nacross multiple images or within a video. Specifically, we devise a multi-stage\nrelation-aware training scheme and a series of corresponding data configuration\nstrategies to bestow RelationVLM with the capabilities of understanding\nsemantic relations, temporal associations and geometric transforms. Extensive\ncase studies and quantitative evaluations show RelationVLM has strong\ncapability in understanding such relations and emerges impressive in-context\ncapability of reasoning from few-shot examples by comparison. This work fosters\nthe advancements of LVLMs by enabling them to support a wider range of\ndownstream applications toward artificial general intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.12801v1.pdf"
    },
    {
        "title": "Investigating Text Shortening Strategy in BERT: Truncation vs Summarization",
        "authors": [
            "Mirza Alim Mutasodirin",
            "Radityo Eko Prasojo"
        ],
        "published": "2024-03-19T15:01:14Z",
        "summary": "The parallelism of Transformer-based models comes at the cost of their input\nmax-length. Some studies proposed methods to overcome this limitation, but none\nof them reported the effectiveness of summarization as an alternative. In this\nstudy, we investigate the performance of document truncation and summarization\nin text classification tasks. Each of the two was investigated with several\nvariations. This study also investigated how close their performances are to\nthe performance of full-text. We used a dataset of summarization tasks based on\nIndonesian news articles (IndoSum) to do classification tests. This study shows\nhow the summaries outperform the majority of truncation method variations and\nlose to only one. The best strategy obtained in this study is taking the head\nof the document. The second is extractive summarization. This study explains\nwhat happened to the result, leading to further research in order to exploit\nthe potential of document summarization as a shortening alternative. The code\nand data used in this work are publicly available in\nhttps://github.com/mirzaalimm/TruncationVsSummarization.",
        "pdf_link": "https://arxiv.org/pdf/2403.12799v1.pdf"
    },
    {
        "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
        "authors": [
            "Jiuhai Chen",
            "Jonas Mueller"
        ],
        "published": "2024-03-19T14:44:45Z",
        "summary": "Large Language Models have become the de facto approach to\nsequence-to-sequence text generation tasks, but for specialized tasks/domains,\na pretrained LLM lacks specific capabilities to produce accurate or\nwell-formatted responses. Supervised fine-tuning specializes a LLM by training\nit on dataset of example prompts with target responses, but real-world data\ntends to be noisy. While many fine-tuning algorithms exist, here we consider a\n\\emph{data-centric AI} perspective on LLM fine-tuning, studying how to\n\\emph{systematically} curate the training dataset to improve the LLM produced\nvia \\emph{any} fine-tuning algorithm.\n  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM\nEvaluation And Rectification) for instruction tuning datasets, that can be used\nwith any LLM and fine-tuning procedure. CLEAR estimates which training data is\nlow-quality and either filters or corrects it. Automatically identifying which\ndata to filter or correct is done via LLM-derived confidence estimates, to\nensure only confident modifications to the dataset. Unlike existing data\ncuration techniques, CLEAR is a comprehensive framework that can improve a\ndataset (and trained model outputs) without additional fine-tuning\ncomputations. We don't assume access to a stronger LLM than the model being\nfine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether\nCLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal\nthat CLEAR consistently improves the performance of fine-tuned models across\nmany datasets and models (like GPT-3.5 and Llama2).",
        "pdf_link": "https://arxiv.org/pdf/2403.12776v1.pdf"
    },
    {
        "title": "Instructing Large Language Models to Identify and Ignore Irrelevant Conditions",
        "authors": [
            "Zhenyu Wu",
            "Chao Shen",
            "Meng Jiang"
        ],
        "published": "2024-03-19T14:07:28Z",
        "summary": "Math word problem (MWP) solving requires generating a reasoning path based on\na given problem description that often contains irrelevant conditions. Existing\nchain-of-thought (CoT) prompting methods elicited multi-step reasoning\nabilities of large language models (LLMs) to solve MWPs. However, they were\nseriously confused by the irrelevant conditions, resulting in low accuracy. In\nthis paper, we propose a novel approach named I$^3$C that instructs LLMs to\nidentify and ignore irrelevant conditions. It identifies a set of irrelevant\ncondition candidates that have a weak semantic relevance with the question.\nThen it prompts LLMs to verify the irrelevant conditions. Lastly it instructs\nthe LLMs with the verification on relevant and irrelevant conditions to avoid\nconfusion and improve reasoning paths. Moreover, we propose to select (problem,\nreasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot\nreasoning. We develop I$^3$C-Select that selects the most confusing problems\nbased on the semantic relevance measurement. We conduct extensive experiments\non eight MWP datasets. I$^3$C can be combined with any CoT prompting methods to\nimprove the performance of solving MWPs. Notably, with GPT-3.5-Turbo and\nI$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and\nGSM-ICM-1K, respectively, significantly outperforming the state-of-the-art\nfew-shot prompting method Complex-CoT by +11.7 and +11.1. Our implementation is\nmade publicly available at https://wzy6642.github.io/I3C.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.12744v1.pdf"
    },
    {
        "title": "Pragmatic Competence Evaluation of Large Language Models for Korean",
        "authors": [
            "Dojun Park",
            "Jiwoo Lee",
            "Hyeyun Jeong",
            "Seohyun Park",
            "Sungeun Lee"
        ],
        "published": "2024-03-19T12:21:20Z",
        "summary": "The current evaluation of Large Language Models (LLMs) predominantly relies\non benchmarks focusing on their embedded knowledge by testing through\nmultiple-choice questions (MCQs), a format inherently suited for automated\nevaluation. Our study extends this evaluation to explore LLMs' pragmatic\ncompetence--a facet previously underexamined before the advent of sophisticated\nLLMs, specifically in the context of Korean. We employ two distinct evaluation\nsetups: the conventional MCQ format, adapted for automatic evaluation, and\nOpen-Ended Questions (OEQs), assessed by human experts, to examine LLMs'\nnarrative response capabilities without predefined options. Our findings reveal\nthat GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups,\nrespectively, with HyperCLOVA X, an LLM optimized for Korean, closely\nfollowing, especially in the OEQ setup, demonstrating a score of 81.56 with a\nmarginal difference of 4.13 points compared to GPT-4. Furthermore, while\nfew-shot learning strategies generally enhance LLM performance,\nChain-of-Thought (CoT) prompting introduces a bias toward literal\ninterpretations, hindering accurate pragmatic inference. Considering the\ngrowing expectation for LLMs to understand and produce language that aligns\nwith human communicative norms, our findings emphasize the importance for\nadvancing LLMs' abilities to grasp and convey sophisticated meanings beyond\nmere literal interpretations.",
        "pdf_link": "https://arxiv.org/pdf/2403.12675v1.pdf"
    },
    {
        "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code",
        "authors": [
            "Andreas Florath"
        ],
        "published": "2024-03-19T10:53:40Z",
        "summary": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1",
        "pdf_link": "https://arxiv.org/pdf/2403.12627v2.pdf"
    },
    {
        "title": "LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
        "authors": [
            "Chuang Liu",
            "Renren Jin",
            "Yuqi Ren",
            "Deyi Xiong"
        ],
        "published": "2024-03-19T10:11:14Z",
        "summary": "Chinese Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities across various NLP benchmarks and real-world applications.\nHowever, the existing benchmarks for comprehensively evaluating these LLMs are\nstill insufficient, particularly in terms of measuring knowledge that LLMs\ncapture. Current datasets collect questions from Chinese examinations across\ndifferent subjects and educational levels to address this issue. Yet, these\nbenchmarks primarily focus on objective questions such as multiple-choice\nquestions, leading to a lack of diversity in question types. To tackle this\nproblem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge\nEvaluation benchmark in this paper. LHMKE is designed to provide a\ncomprehensive evaluation of the knowledge acquisition capabilities of Chinese\nLLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,\nranging from primary school to professional certification exams. Notably, LHMKE\nincludes both objective and subjective questions, offering a more holistic\nevaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs\nunder the zero-shot setting, which aligns with real examinations, and compared\ntheir performance across different subjects. We also conduct an in-depth\nanalysis to check whether GPT-4 can automatically score subjective predictions.\nOur findings suggest that LHMKE is a challenging and advanced testbed for\nChinese LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.12601v1.pdf"
    },
    {
        "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
        "authors": [
            "Victor Carbune",
            "Hassan Mansoor",
            "Fangyu Liu",
            "Rahul Aralikatte",
            "Gilles Baechler",
            "Jindong Chen",
            "Abhanshu Sharma"
        ],
        "published": "2024-03-19T10:03:07Z",
        "summary": "Vision-language models (VLMs) are achieving increasingly strong performance\non multimodal tasks. However, reasoning capabilities remain limited\nparticularly for smaller VLMs, while those of large-language models (LLMs) have\nseen numerous improvements. We propose a technique to transfer capabilities\nfrom LLMs to VLMs. On the recently introduced ChartQA, our method obtains\nstate-of-the-art performance when applied on the PaLI3-5B VLM by\n\\citet{chen2023pali3}, while also enabling much better performance on PlotQA\nand FigureQA.\n  We first improve the chart representation by continuing the pre-training\nstage using an improved version of the chart-to-table translation task by\n\\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than\nthe original training set. To improve general reasoning capabilities and\nimprove numerical operations, we synthesize reasoning traces using the table\nrepresentation of charts. Lastly, our model is fine-tuned using the multitask\nloss introduced by \\citet{hsieh2023distilling}.\n  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B\nwithout using an upstream OCR system, while keeping inference time constant\ncompared to the PaLI3-5B baseline. When rationales are further refined with a\nsimple program-of-thought prompt \\cite{chen2023program}, our model outperforms\nthe recently introduced Gemini Ultra and GPT-4V.",
        "pdf_link": "https://arxiv.org/pdf/2403.12596v1.pdf"
    },
    {
        "title": "AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework",
        "authors": [
            "Xiang Li",
            "Zhenyu Li",
            "Chen Shi",
            "Yong Xu",
            "Qing Du",
            "Mingkui Tan",
            "Jun Huang",
            "Wei Lin"
        ],
        "published": "2024-03-19T09:45:33Z",
        "summary": "The task of financial analysis primarily encompasses two key areas: stock\ntrend prediction and the corresponding financial question answering. Currently,\nmachine learning and deep learning algorithms (ML&DL) have been widely applied\nfor stock trend predictions, leading to significant progress. However, these\nmethods fail to provide reasons for predictions, lacking interpretability and\nreasoning processes. Also, they can not integrate textual information such as\nfinancial news or reports. Meanwhile, large language models (LLMs) have\nremarkable textual understanding and generation ability. But due to the\nscarcity of financial training datasets and limited integration with real-time\nknowledge, LLMs still suffer from hallucinations and are unable to keep up with\nthe latest information. To tackle these challenges, we first release AlphaFin\ndatasets, combining traditional research datasets, real-time financial data,\nand handwritten chain-of-thought (CoT) data. It has a positive impact on\ntraining LLMs for completing financial analysis. We then use AlphaFin datasets\nto benchmark a state-of-the-art method, called Stock-Chain, for effectively\ntackling the financial analysis task, which integrates retrieval-augmented\ngeneration (RAG) techniques. Extensive experiments are conducted to demonstrate\nthe effectiveness of our framework on financial analysis.",
        "pdf_link": "https://arxiv.org/pdf/2403.12582v1.pdf"
    },
    {
        "title": "Factorized Learning Assisted with Large Language Model for Gloss-free Sign Language Translation",
        "authors": [
            "Zhigang Chen",
            "Benjia Zhou",
            "Jun Li",
            "Jun Wan",
            "Zhen Lei",
            "Ning Jiang",
            "Quan Lu",
            "Guoqing Zhao"
        ],
        "published": "2024-03-19T09:00:23Z",
        "summary": "Previous Sign Language Translation (SLT) methods achieve superior performance\nby relying on gloss annotations. However, labeling high-quality glosses is a\nlabor-intensive task, which limits the further development of SLT. Although\nsome approaches work towards gloss-free SLT through jointly training the visual\nencoder and translation network, these efforts still suffer from poor\nperformance and inefficient use of the powerful Large Language Model (LLM).\nMost seriously, we find that directly introducing LLM into SLT will lead to\ninsufficient learning of visual representations as LLM dominates the learning\ncurve. To address these problems, we propose Factorized Learning assisted with\nLarge Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the\ntraining process into two stages. In the visual initialing stage, we employ a\nlightweight translation model after the visual encoder to pre-train the visual\nencoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the\nvisual encoder and integrate it with a pre-trained LLM to inspire the LLM's\ntranslation potential. This factorized training strategy proves to be highly\neffective as evidenced by significant improvements achieved across three SLT\ndatasets which are all conducted under the gloss-free setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.12556v1.pdf"
    },
    {
        "title": "AffineQuant: Affine Transformation Quantization for Large Language Models",
        "authors": [
            "Yuexiao Ma",
            "Huixia Li",
            "Xiawu Zheng",
            "Feng Ling",
            "Xuefeng Xiao",
            "Rui Wang",
            "Shilei Wen",
            "Fei Chao",
            "Rongrong Ji"
        ],
        "published": "2024-03-19T08:40:21Z",
        "summary": "The significant resource requirements associated with Large-scale Language\nModels (LLMs) have generated considerable interest in the development of\ntechniques aimed at compressing and accelerating neural networks. Among these\ntechniques, Post-Training Quantization (PTQ) has emerged as a subject of\nconsiderable interest due to its noteworthy compression efficiency and\ncost-effectiveness in the context of training. Existing PTQ methods for LLMs\nlimit the optimization scope to scaling transformations between pre- and\npost-quantization weights. In this paper, we advocate for the direct\noptimization using equivalent Affine transformations in PTQ (AffineQuant). This\napproach extends the optimization scope and thus significantly minimizing\nquantization errors. Additionally, by employing the corresponding inverse\nmatrix, we can ensure equivalence between the pre- and post-quantization\noutputs of PTQ, thereby maintaining its efficiency and generalization\ncapabilities. To ensure the invertibility of the transformation during\noptimization, we further introduce a gradual mask optimization method. This\nmethod initially focuses on optimizing the diagonal elements and gradually\nextends to the other elements. Such an approach aligns with the\nLevy-Desplanques theorem, theoretically ensuring invertibility of the\ntransformation. As a result, significant performance improvements are evident\nacross different LLMs on diverse datasets. To illustrate, we attain a C4\nperplexity of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model\nof W4A4 quantization without overhead. On zero-shot tasks, AffineQuant achieves\nan average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using\n4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art\nbenchmark for PTQ in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.12544v1.pdf"
    },
    {
        "title": "To Help or Not to Help: LLM-based Attentive Support for Human-Robot Group Interactions",
        "authors": [
            "Daniel Tanneberg",
            "Felix Ocker",
            "Stephan Hasler",
            "Joerg Deigmoeller",
            "Anna Belardinelli",
            "Chao Wang",
            "Heiko Wersing",
            "Bernhard Sendhoff",
            "Michael Gienger"
        ],
        "published": "2024-03-19T08:09:44Z",
        "summary": "How can a robot provide unobtrusive physical support within a group of\nhumans? We present Attentive Support, a novel interaction concept for robots to\nsupport a group of humans. It combines scene perception, dialogue acquisition,\nsituation understanding, and behavior generation with the common-sense\nreasoning capabilities of Large Language Models (LLMs). In addition to\nfollowing user instructions, Attentive Support is capable of deciding when and\nhow to support the humans, and when to remain silent to not disturb the group.\nWith a diverse set of scenarios, we show and evaluate the robot's attentive\nbehavior, which supports and helps the humans when required, while not\ndisturbing if no help is needed.",
        "pdf_link": "https://arxiv.org/pdf/2403.12533v1.pdf"
    },
    {
        "title": "UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All",
        "authors": [
            "Yuanhuiyi Lyu",
            "Xu Zheng",
            "Jiazhou Zhou",
            "Lin Wang"
        ],
        "published": "2024-03-19T08:09:27Z",
        "summary": "We present UniBind, a flexible and efficient approach that learns a unified\nrepresentation space for seven diverse modalities -- images, text, audio, point\ncloud, thermal, video, and event data. Existing works, eg., ImageBind, treat\nthe image as the central modality and build an image-centered representation\nspace; however, the space may be sub-optimal as it leads to an unbalanced\nrepresentation space among all modalities. Moreover, the category names are\ndirectly used to extract text embeddings for the downstream tasks, making it\nhardly possible to represent the semantics of multi-modal data. The\n'out-of-the-box' insight of our UniBind is to make the alignment center\nmodality-agnostic and further learn a unified and balanced representation\nspace, empowered by the large language models (LLMs). UniBind is superior in\nits flexible application to all CLIP-style models and delivers remarkable\nperformance boosts. To make this possible, we 1) construct a knowledge base of\ntext embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build\nLLM-augmented class-wise embedding center on top of the knowledge base and\nencoded visual embeddings; 3) align all the embeddings to the LLM-augmented\nembedding center via contrastive learning to achieve a unified and balanced\nrepresentation space. UniBind shows strong zero-shot recognition performance\ngains over prior arts by an average of 6.36%. Finally, we achieve new\nstate-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal\nfine-tuning setting while reducing 90% of the learnable parameters.",
        "pdf_link": "https://arxiv.org/pdf/2403.12532v1.pdf"
    },
    {
        "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
        "authors": [
            "Zhuowen Yuan",
            "Zidi Xiong",
            "Yi Zeng",
            "Ning Yu",
            "Ruoxi Jia",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2024-03-19T07:25:02Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have showcased remarkable\ncapabilities across various tasks in different domains. However, the emergence\nof biases and the potential for generating harmful content in LLMs,\nparticularly under malicious inputs, pose significant challenges. Current\nmitigation strategies, while effective, are not resilient under adversarial\nattacks. This paper introduces Resilient Guardrails for Large Language Models\n(RigorLLM), a novel framework designed to efficiently and effectively moderate\nharmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted\napproach that includes energy-based training data augmentation through Langevin\ndynamics, optimizing a safe suffix for inputs via minimax optimization, and\nintegrating a fusion-based model combining robust KNN with LLMs based on our\ndata augmentation, RigorLLM offers a robust solution to harmful content\nmoderation. Our experimental evaluations demonstrate that RigorLLM not only\noutperforms existing baselines like OpenAI API and Perspective API in detecting\nharmful content but also exhibits unparalleled resilience to jailbreaking\nattacks. The innovative use of constrained optimization and a fusion-based\nguardrail approach represents a significant step forward in developing more\nsecure and reliable LLMs, setting a new standard for content moderation\nframeworks in the face of evolving digital threats.",
        "pdf_link": "https://arxiv.org/pdf/2403.13031v1.pdf"
    },
    {
        "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices",
        "authors": [
            "Sara Abdali",
            "Richard Anarfi",
            "CJ Barberan",
            "Jia He"
        ],
        "published": "2024-03-19T07:10:58Z",
        "summary": "Large language models (LLMs) have significantly transformed the landscape of\nNatural Language Processing (NLP). Their impact extends across a diverse\nspectrum of tasks, revolutionizing how we approach language understanding and\ngenerations. Nevertheless, alongside their remarkable utility, LLMs introduce\ncritical security and risk considerations. These challenges warrant careful\nexamination to ensure responsible deployment and safeguard against potential\nvulnerabilities. This research paper thoroughly investigates security and\nprivacy concerns related to LLMs from five thematic perspectives: security and\nprivacy concerns, vulnerabilities against adversarial attacks, potential harms\ncaused by misuses of LLMs, mitigation strategies to address these challenges\nwhile identifying limitations of current strategies. Lastly, the paper\nrecommends promising avenues for future research to enhance the security and\nrisk management of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.12503v1.pdf"
    },
    {
        "title": "DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of MLLM",
        "authors": [
            "Yixuan Wu",
            "Yizhou Wang",
            "Shixiang Tang",
            "Wenhao Wu",
            "Tong He",
            "Wanli Ouyang",
            "Jian Wu",
            "Philip Torr"
        ],
        "published": "2024-03-19T06:54:33Z",
        "summary": "We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot\nobject detection ability of multimodal large language models (MLLMs), such as\nGPT-4V and Gemini. Our approach consists of a detection prompting toolkit\ninspired by high-precision detection priors and a new Chain-of-Thought to\nimplement these prompts. Specifically, the prompts in the toolkit are designed\nto guide the MLLM to focus on regional information (e.g., zooming in), read\ncoordinates according to measure standards (e.g., overlaying rulers and\ncompasses), and infer from the contextual information (e.g., overlaying scene\ngraphs). Building upon these tools, the new detection chain-of-thought can\nautomatically decompose the task into simple subtasks, diagnose the\npredictions, and plan for progressive box refinements. The effectiveness of our\nframework is demonstrated across a spectrum of detection tasks, especially hard\ncases. Compared to existing state-of-the-art methods, GPT-4V with our\nDetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS\nCOCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val\nset for zero-shot referring expression comprehension, +14.5% AP on D-cube\ndescribe object detection FULL setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.12488v2.pdf"
    },
    {
        "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
        "authors": [
            "Xudong Guo",
            "Kaixuan Huang",
            "Jiale Liu",
            "Wenhui Fan",
            "Natalia V\u00e9lez",
            "Qingyun Wu",
            "Huazheng Wang",
            "Thomas L. Griffiths",
            "Mengdi Wang"
        ],
        "published": "2024-03-19T06:39:47Z",
        "summary": "Large Language Models (LLMs) have emerged as integral tools for reasoning,\nplanning, and decision-making, drawing upon their extensive world knowledge and\nproficiency in language-related tasks. LLMs thus hold tremendous potential for\nnatural language interaction within multi-agent systems to foster cooperation.\nHowever, LLM agents tend to over-report and comply with any instruction, which\nmay result in information redundancy and confusion in multi-agent cooperation.\nInspired by human organizations, this paper introduces a framework that imposes\nprompt-based organization structures on LLM agents to mitigate these problems.\nThrough a series of experiments with embodied LLM agents and human-agent\ncollaboration, our results highlight the impact of designated leadership on\nteam efficiency, shedding light on the leadership qualities displayed by LLM\nagents and their spontaneous cooperative behaviors. Further, we harness the\npotential of LLMs to propose enhanced organizational prompts, via a\nCriticize-Reflect process, resulting in novel organization structures that\nreduce communication costs and enhance team efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2403.12482v1.pdf"
    },
    {
        "title": "WoLF: Wide-scope Large Language Model Framework for CXR Understanding",
        "authors": [
            "Seil Kang",
            "Donghyun Kim",
            "Junhyeok Kim",
            "Hyo Kyung Lee",
            "Seong Jae Hwang"
        ],
        "published": "2024-03-19T06:39:23Z",
        "summary": "Significant methodological strides have been made toward Chest X-ray (CXR)\nunderstanding via modern vision-language models (VLMs), demonstrating\nimpressive Visual Question Answering (VQA) and CXR report generation abilities.\nHowever, existing CXR understanding frameworks still possess several procedural\ncaveats. (1) Previous methods solely use CXR reports, which are insufficient\nfor comprehensive Visual Question Answering (VQA), especially when additional\nhealth-related data like medication history and prior diagnoses are needed. (2)\nPrevious methods use raw CXR reports, which are often arbitrarily structured.\nWhile modern language models can understand various text formats, restructuring\nreports for clearer, organized anatomy-based information could enhance their\nusefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize\nlinguistic correctness, lacking the capability to offer nuanced assessments of\nthe generated answers. In this work, to address the aforementioned caveats, we\nintroduce WoLF, a Wide-scope Large Language Model Framework for CXR\nunderstanding. To resolve (1), we capture multi-faceted records of patients,\nwhich are utilized for accurate diagnoses in real-world clinical scenarios.\nSpecifically, we adopt the Electronic Health Records (EHR) to generate\ninstruction-following data suited for CXR understanding. Regarding (2), we\nenhance report generation performance by decoupling knowledge in CXR reports\nbased on anatomical structure even within the attention step via masked\nattention. To address (3), we introduce an AI-evaluation protocol optimized for\nassessing the capabilities of LLM. Through extensive experimental validations,\nWoLF demonstrates superior performance over other models on MIMIC-CXR in the\nAI-evaluation arena about VQA (up to +9.47%p mean score) and by metrics about\nreport generation (+7.3%p BLEU-1).",
        "pdf_link": "https://arxiv.org/pdf/2403.15456v3.pdf"
    },
    {
        "title": "CrossTune: Black-Box Few-Shot Classification with Label Enhancement",
        "authors": [
            "Danqing Luo",
            "Chen Zhang",
            "Yan Zhang",
            "Haizhou Li"
        ],
        "published": "2024-03-19T05:52:56Z",
        "summary": "Training or finetuning large-scale language models (LLMs) requires\nsubstantial computation resources, motivating recent efforts to explore\nparameter-efficient adaptation to downstream tasks. One approach is to treat\nthese models as black boxes and use forward passes (Inference APIs) to interact\nwith them. Current research focuses on adapting these black-box models to\ndownstream tasks using gradient-free prompt optimization, but this often\ninvolves an expensive process of searching task-specific prompts. Therefore, we\nare motivated to study black-box language model adaptation without prompt\nsearch. Specifically, we introduce a label-enhanced cross-attention network\ncalled CrossTune, which models the semantic relatedness between the input text\nsequence and task-specific label descriptions. Its effectiveness is examined in\nthe context of few-shot text classification. To improve the generalization of\nCrossTune, we utilize ChatGPT to generate additional training data through\nin-context learning. A switch mechanism is implemented to exclude low-quality\nChatGPT-generated data. Through extensive experiments on seven benchmark text\nclassification datasets, we demonstrate that our proposed approach outperforms\nthe previous state-of-the-art gradient-free black-box tuning method by 5.7% on\naverage. Even without using ChatGPT-augmented data, CrossTune performs better\nor comparably than previous black-box tuning methods, suggesting the\neffectiveness of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2403.12468v1.pdf"
    },
    {
        "title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
        "authors": [
            "Hao Wang",
            "Jiayou Qin",
            "Ashish Bastola",
            "Xiwen Chen",
            "John Suchanek",
            "Zihao Gong",
            "Abolfazl Razi"
        ],
        "published": "2024-03-19T03:55:39Z",
        "summary": "This paper explores the potential of Large Language Models(LLMs) in zero-shot\nanomaly detection for safe visual navigation. With the assistance of the\nstate-of-the-art real-time open-world object detection model Yolo-World and\nspecialized prompts, the proposed framework can identify anomalies within\ncamera-captured frames that include any possible obstacles, then generate\nconcise, audio-delivered descriptions emphasizing abnormalities, assist in safe\nvisual navigation in complex circumstances. Moreover, our proposed framework\nleverages the advantages of LLMs and the open-vocabulary object detection model\nto achieve the dynamic scenario switch, which allows users to transition\nsmoothly from scene to scene, which addresses the limitation of traditional\nvisual navigation. Furthermore, this paper explored the performance\ncontribution of different prompt components, provided the vision for future\nimprovement in visual accessibility, and paved the way for LLMs in video\nanomaly detection and vision-language understanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.12415v1.pdf"
    },
    {
        "title": "Third-Party Language Model Performance Prediction from Instruction",
        "authors": [
            "Rahul Nadkarni",
            "Yizhong Wang",
            "Noah A. Smith"
        ],
        "published": "2024-03-19T03:53:47Z",
        "summary": "Language model-based instruction-following systems have lately shown\nincreasing performance on many benchmark tasks, demonstrating the capability of\nadapting to a broad variety of instructions. However, such systems are often\nnot designed to be transparent about their limitations; a user may easily\nprompt a model with an instruction without any idea of whether the responses\nshould be expected to be accurate, or if the system is even capable of\nperforming the task. We propose a third party performance prediction framework,\nwhere a separate model is trained to predict the metric resulting from\nevaluating an instruction-following system on a task while assuming access only\nto its inputs and outputs at inference time. We perform this analysis with a\nvariety of both open and closed instruction-following models as well as\nmultiple performance predictors, and examine the effect of various factors such\nas model size, number of training tasks, and prompt format. Our findings\nindicate that third-party performance prediction is very challenging, and much\nwork remains in developing predictors that can automatically reveal the\nlimitations of modern instruction-following natural language processing\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.12413v1.pdf"
    },
    {
        "title": "Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales",
        "authors": [
            "Ayushi Nirmal",
            "Amrita Bhattacharjee",
            "Paras Sheth",
            "Huan Liu"
        ],
        "published": "2024-03-19T03:22:35Z",
        "summary": "Although social media platforms are a prominent arena for users to engage in\ninterpersonal discussions and express opinions, the facade and anonymity\noffered by social media may allow users to spew hate speech and offensive\ncontent. Given the massive scale of such platforms, there arises a need to\nautomatically identify and flag instances of hate speech. Although several hate\nspeech detection methods exist, most of these black-box methods are not\ninterpretable or explainable by design. To address the lack of\ninterpretability, in this paper, we propose to use state-of-the-art Large\nLanguage Models (LLMs) to extract features in the form of rationales from the\ninput text, to train a base hate speech classifier, thereby enabling faithful\ninterpretability by design. Our framework effectively combines the textual\nunderstanding capabilities of LLMs and the discriminative power of\nstate-of-the-art hate speech classifiers to make these classifiers faithfully\ninterpretable. Our comprehensive evaluation on a variety of social media hate\nspeech datasets demonstrate: (1) the goodness of the LLM-extracted rationales,\nand (2) the surprising retention of detector performance even after training to\nensure interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2403.12403v1.pdf"
    },
    {
        "title": "Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open Domain Multi-Hop Question Answering",
        "authors": [
            "Yuan Gao",
            "Yiheng Zhu",
            "Yuanbin Cao",
            "Yinzhi Zhou",
            "Zhen Wu",
            "Yujie Chen",
            "Shenglan Wu",
            "Haoyuan Hu",
            "Xinyu Dai"
        ],
        "published": "2024-03-19T03:00:03Z",
        "summary": "Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in\nNatural Language Processing (NLP) by aiming to answer complex questions through\nmulti-step reasoning over retrieved information from external knowledge\nsources. Recently, Large Language Models (LLMs) have demonstrated remarkable\nperformance in solving ODMHQA owing to their capabilities including planning,\nreasoning, and utilizing tools. However, LLMs may generate off-topic answers\nwhen attempting to solve ODMHQA, namely the generated answers are irrelevant to\nthe original questions. This issue of off-topic answers accounts for\napproximately one-third of incorrect answers, yet remains underexplored despite\nits significance. To alleviate this issue, we propose the\nDiscriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism.\nSpecifically, the Discriminator leverages the intrinsic capabilities of LLMs to\njudge whether the generated answers are off-topic. In cases where an off-topic\nanswer is detected, the Corrector performs step-wise revisions along the\nreversed reasoning chain (Re-Compose->Re-Solve->Re-Decompose) until the final\nanswer becomes on-topic. Experimental results on the HotpotQA and\n2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably\nreduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving\nthe performance in Exact Match (EM) by nearly 3% compared to the baseline\nmethod without the Dr3 mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2403.12393v1.pdf"
    },
    {
        "title": "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models",
        "authors": [
            "Ying-Chun Lin",
            "Jennifer Neville",
            "Jack W. Stokes",
            "Longqi Yang",
            "Tara Safavi",
            "Mengting Wan",
            "Scott Counts",
            "Siddharth Suri",
            "Reid Andersen",
            "Xiaofeng Xu",
            "Deepak Gupta",
            "Sujay Kumar Jauhar",
            "Xia Song",
            "Georg Buscher",
            "Saurabh Tiwary",
            "Brent Hecht",
            "Jaime Teevan"
        ],
        "published": "2024-03-19T02:57:07Z",
        "summary": "Accurate and interpretable user satisfaction estimation (USE) is critical for\nunderstanding, evaluating, and continuously improving conversational systems.\nUsers express their satisfaction or dissatisfaction with diverse conversational\npatterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented\n(customer service chatbot) conversational systems. Existing approaches based on\nfeaturized ML models or text embeddings fall short in extracting generalizable\npatterns and are hard to interpret. In this work, we show that LLMs can extract\ninterpretable signals of user satisfaction from their natural language\nutterances more effectively than embedding-based approaches. Moreover, an LLM\ncan be tailored for USE via an iterative prompting framework using supervision\nfrom labeled examples. The resulting method, Supervised Prompting for User\nsatisfaction Rubrics (SPUR), not only has higher accuracy but is more\ninterpretable as it scores user satisfaction via learned rubrics with a\ndetailed breakdown.",
        "pdf_link": "https://arxiv.org/pdf/2403.12388v1.pdf"
    },
    {
        "title": "Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning",
        "authors": [
            "Cheng Peng",
            "Zehao Yu",
            "Kaleb E Smith",
            "Wei-Hsuan Lo-Ciganic",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2024-03-19T02:34:33Z",
        "summary": "The progress in natural language processing (NLP) using large language models\n(LLMs) has greatly improved patient information extraction from clinical\nnarratives. However, most methods based on the fine-tuning strategy have\nlimited transfer learning ability for cross-domain applications. This study\nproposed a novel approach that employs a soft prompt-based learning\narchitecture, which introduces trainable prompts to guide LLMs toward desired\noutputs. We examined two types of LLM architectures, including encoder-only\nGatorTron and decoder-only GatorTronGPT, and evaluated their performance for\nthe extraction of social determinants of health (SDoH) using a\ncross-institution dataset from the 2022 n2c2 challenge and a cross-disease\ndataset from the University of Florida (UF) Health. The results show that\ndecoder-only LLMs with prompt tuning achieved better performance in\ncross-domain applications. GatorTronGPT achieved the best F1 scores for both\ndatasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a\ncross-institution setting, and 5.5% and 14.5% in a cross-disease setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.12374v1.pdf"
    },
    {
        "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners",
        "authors": [
            "Chi Hu",
            "Yuan Ge",
            "Xiangnan Ma",
            "Hang Cao",
            "Qiang Li",
            "Yonghua Yang",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "published": "2024-03-19T02:34:18Z",
        "summary": "Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Existing\nsolutions, such as deploying task-specific verifiers or voting over multiple\nreasoning paths, either require extensive human annotations or fail in\nscenarios with inconsistent responses. To address these challenges, we\nintroduce RankPrompt, a new prompting method that enables LLMs to self-rank\ntheir responses without additional resources. RankPrompt breaks down the\nranking problem into a series of comparisons among diverse responses,\nleveraging the inherent capabilities of LLMs to generate chains of comparison\nas contextual exemplars. Our experiments across 11 arithmetic and commonsense\nreasoning tasks show that RankPrompt significantly enhances the reasoning\nperformance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover,\nRankPrompt excels in LLM-based automatic evaluations for open-ended tasks,\naligning with human judgments 74% of the time in the AlpacaEval dataset. It\nalso exhibits robustness to variations in response order and consistency.\nCollectively, our results validate RankPrompt as an effective method for\neliciting high-quality feedback from language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.12373v3.pdf"
    },
    {
        "title": "Advancing Time Series Classification with Multimodal Language Modeling",
        "authors": [
            "Mingyue Cheng",
            "Yiheng Chen",
            "Qi Liu",
            "Zhiding Liu",
            "Yucong Luo"
        ],
        "published": "2024-03-19T02:32:24Z",
        "summary": "For the advancements of time series classification, scrutinizing previous\nstudies, most existing methods adopt a common learning-to-classify paradigm - a\ntime series classifier model tries to learn the relation between sequence\ninputs and target label encoded by one-hot distribution. Although effective,\nthis paradigm conceals two inherent limitations: (1) encoding target categories\nwith one-hot distribution fails to reflect the comparability and similarity\nbetween labels, and (2) it is very difficult to learn transferable model across\ndomains, which greatly hinder the development of universal serving paradigm. In\nthis work, we propose InstructTime, a novel attempt to reshape time series\nclassification as a learning-to-generate paradigm. Relying on the powerful\ngenerative capacity of the pre-trained language model, the core idea is to\nformulate the classification of time series as a multimodal understanding task,\nin which both task-specific instructions and raw time series are treated as\nmultimodal inputs while the label information is represented by texts. To\naccomplish this goal, three distinct designs are developed in the InstructTime.\nFirstly, a time series discretization module is designed to convert continuous\ntime series into a sequence of hard tokens to solve the inconsistency issue\nacross modal inputs. To solve the modality representation gap issue, for one\nthing, we introduce an alignment projected layer before feeding the transformed\ntoken of time series into language models. For another, we highlight the\nnecessity of auto-regressive pre-training across domains, which can facilitate\nthe transferability of the language model and boost the generalization\nperformance. Extensive experiments are conducted over benchmark datasets, whose\nresults uncover the superior performance of InstructTime and the potential for\na universal foundation model in time series classification.",
        "pdf_link": "https://arxiv.org/pdf/2403.12371v1.pdf"
    },
    {
        "title": "Characteristic AI Agents via Large Language Models",
        "authors": [
            "Xi Wang",
            "Hongliang Dai",
            "Shen Gao",
            "Piji Li"
        ],
        "published": "2024-03-19T02:25:29Z",
        "summary": "The advancement of Large Language Models (LLMs) has led to significant\nenhancements in the performance of chatbot systems. Many researchers have\ndedicated their efforts to the development of bringing characteristics to\nchatbots. While there have been commercial products for developing role-driven\nchatbots using LLMs, it is worth noting that academic research in this area\nremains relatively scarce. Our research focuses on investigating the\nperformance of LLMs in constructing Characteristic AI Agents by simulating\nreal-life individuals across different settings. Current investigations have\nprimarily focused on act on roles with simple profiles. In response to this\nresearch gap, we create a benchmark for the characteristic AI agents task,\nincluding dataset, techniques, and evaluation metrics. A dataset called\n``Character100'' is built for this benchmark, comprising the most-visited\npeople on Wikipedia for language models to role-play. With the constructed\ndataset, we conduct comprehensive assessment of LLMs across various settings.\nIn addition, we devise a set of automatic metrics for quantitative performance\nevaluation. The experimental results underscore the potential directions for\nfurther improvement in the capabilities of LLMs in constructing characteristic\nAI agents. The benchmark is available at\nhttps://github.com/nuaa-nlp/Character100.",
        "pdf_link": "https://arxiv.org/pdf/2403.12368v1.pdf"
    },
    {
        "title": "OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety",
        "authors": [
            "Chuang Liu",
            "Linhao Yu",
            "Jiaxuan Li",
            "Renren Jin",
            "Yufei Huang",
            "Ling Shi",
            "Junhui Zhang",
            "Xinmeng Ji",
            "Tingting Cui",
            "Tao Liu",
            "Jinwang Song",
            "Hongying Zan",
            "Sun Li",
            "Deyi Xiong"
        ],
        "published": "2024-03-18T23:21:37Z",
        "summary": "The rapid development of Chinese large language models (LLMs) poses big\nchallenges for efficient LLM evaluation. While current initiatives have\nintroduced new benchmarks or evaluation platforms for assessing Chinese LLMs,\nmany of these focus primarily on capabilities, usually overlooking potential\nalignment and safety issues. To address this gap, we introduce OpenEval, an\nevaluation testbed that benchmarks Chinese LLMs across capability, alignment\nand safety. For capability assessment, we include 12 benchmark datasets to\nevaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge,\ncommonsense reasoning and mathematical reasoning. For alignment assessment,\nOpenEval contains 7 datasets that examines the bias, offensiveness and\nillegalness in the outputs yielded by Chinese LLMs. To evaluate safety,\nespecially anticipated risks (e.g., power-seeking, self-awareness) of advanced\nLLMs, we include 6 datasets. In addition to these benchmarks, we have\nimplemented a phased public evaluation and benchmark update strategy to ensure\nthat OpenEval is in line with the development of Chinese LLMs or even able to\nprovide cutting-edge benchmark datasets to guide the development of Chinese\nLLMs. In our first public evaluation, we have tested a range of Chinese LLMs,\nspanning from 7B to 72B parameters, including both open-source and proprietary\nmodels. Evaluation results indicate that while Chinese LLMs have shown\nimpressive performance in certain tasks, more attention should be directed\ntowards broader aspects such as commonsense reasoning, alignment, and safety.",
        "pdf_link": "https://arxiv.org/pdf/2403.12316v1.pdf"
    },
    {
        "title": "Improving LoRA in Privacy-preserving Federated Learning",
        "authors": [
            "Youbang Sun",
            "Zitao Li",
            "Yaliang Li",
            "Bolin Ding"
        ],
        "published": "2024-03-18T23:20:08Z",
        "summary": "Low-rank adaptation (LoRA) is one of the most popular task-specific\nparameter-efficient fine-tuning (PEFT) methods on pre-trained language models\nfor its good performance and computational efficiency. LoRA injects a product\nof two trainable rank decomposition matrices over the top of each frozen\npre-trained model module. However, when applied in the setting of\nprivacy-preserving federated learning (FL), LoRA may become unstable due to the\nfollowing facts: 1) the effects of data heterogeneity and multi-step local\nupdates are non-negligible, 2) additive noise enforced on updating gradients to\nguarantee differential privacy (DP) can be amplified and 3) the final\nperformance is susceptible to hyper-parameters. A key factor leading to these\nphenomena is the discordance between jointly optimizing the two low-rank\nmatrices by local clients and separately aggregating them by the central\nserver. Thus, this paper proposes an efficient and effective version of LoRA,\nFederated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further\nhalve the communication cost of federated fine-tuning LLMs. The core idea of\nFFA-LoRA is to fix the randomly initialized non-zero matrices and only\nfine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is\nmotivated by practical and theoretical benefits in privacy-preserved FL. Our\nexperiments demonstrate that FFA-LoRA provides more consistent performance with\nbetter computational efficiency over vanilla LoRA in various FL tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.12313v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models to Extract Information on Substance Use Disorder Severity from Clinical Notes: A Zero-shot Learning Approach",
        "authors": [
            "Maria Mahbub",
            "Gregory M. Dams",
            "Sudarshan Srinivasan",
            "Caitlin Rizy",
            "Ioana Danciu",
            "Jodie Trafton",
            "Kathryn Knight"
        ],
        "published": "2024-03-18T22:39:03Z",
        "summary": "Substance use disorder (SUD) poses a major concern due to its detrimental\neffects on health and society. SUD identification and treatment depend on a\nvariety of factors such as severity, co-determinants (e.g., withdrawal\nsymptoms), and social determinants of health. Existing diagnostic coding\nsystems used by American insurance providers, like the International\nClassification of Diseases (ICD-10), lack granularity for certain diagnoses,\nbut clinicians will add this granularity (as that found within the Diagnostic\nand Statistical Manual of Mental Disorders classification or DSM-5) as\nsupplemental unstructured text in clinical notes. Traditional natural language\nprocessing (NLP) methods face limitations in accurately parsing such diverse\nclinical language. Large Language Models (LLMs) offer promise in overcoming\nthese challenges by adapting to diverse language patterns. This study\ninvestigates the application of LLMs for extracting severity-related\ninformation for various SUD diagnoses from clinical notes. We propose a\nworkflow employing zero-shot learning of LLMs with carefully crafted prompts\nand post-processing techniques. Through experimentation with Flan-T5, an\nopen-source LLM, we demonstrate its superior recall compared to the rule-based\napproach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness\nof LLMs in extracting severity information, contributing to improved risk\nassessment and treatment planning for SUD patients.",
        "pdf_link": "https://arxiv.org/pdf/2403.12297v1.pdf"
    },
    {
        "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications",
        "authors": [
            "Thanos Konstantinidis",
            "Giorgos Iacovides",
            "Mingxue Xu",
            "Tony G. Constantinides",
            "Danilo Mandic"
        ],
        "published": "2024-03-18T22:11:00Z",
        "summary": "There are multiple sources of financial news online which influence market\nmovements and trader's decisions. This highlights the need for accurate\nsentiment analysis, in addition to having appropriate algorithmic trading\ntechniques, to arrive at better informed trading decisions. Standard lexicon\nbased sentiment approaches have demonstrated their power in aiding financial\ndecisions. However, they are known to suffer from issues related to context\nsensitivity and word ordering. Large Language Models (LLMs) can also be used in\nthis context, but they are not finance-specific and tend to require significant\ncomputational resources. To facilitate a finance specific LLM framework, we\nintroduce a novel approach based on the Llama 2 7B foundational model, in order\nto benefit from its generative nature and comprehensive language manipulation.\nThis is achieved by fine-tuning the Llama2 7B model on a small portion of\nsupervised financial sentiment analysis data, so as to jointly handle the\ncomplexities of financial lexicon and context, and further equipping it with a\nneural network based decision mechanism. Such a generator-classifier scheme,\nreferred to as FinLlama, is trained not only to classify the sentiment valence\nbut also quantify its strength, thus offering traders a nuanced insight into\nfinancial news articles. Complementing this, the implementation of\nparameter-efficient fine-tuning through LoRA optimises trainable parameters,\nthus minimising computational and memory requirements, without sacrificing\naccuracy. Simulation results demonstrate the ability of the proposed FinLlama\nto provide a framework for enhanced portfolio management decisions and\nincreased market returns. These results underpin the ability of FinLlama to\nconstruct high-return portfolios which exhibit enhanced resilience, even during\nvolatile periods and unpredictable market events.",
        "pdf_link": "https://arxiv.org/pdf/2403.12285v1.pdf"
    },
    {
        "title": "Zero-Shot Multi-task Hallucination Detection",
        "authors": [
            "Patanjali Bhamidipati",
            "Advaith Malladi",
            "Manish Shrivastava",
            "Radhika Mamidi"
        ],
        "published": "2024-03-18T20:50:26Z",
        "summary": "In recent studies, the extensive utilization of large language models has\nunderscored the importance of robust evaluation methodologies for assessing\ntext generation quality and relevance to specific tasks. This has revealed a\nprevalent issue known as hallucination, an emergent condition in the model\nwhere generated text lacks faithfulness to the source and deviates from the\nevaluation criteria. In this study, we formally define hallucination and\npropose a framework for its quantitative detection in a zero-shot setting,\nleveraging our definition and the assumption that model outputs entail task and\nsample specific inputs. In detecting hallucinations, our solution achieves an\naccuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting.\nNotably, our solution maintains computational efficiency, requiring far less\ncomputational resources than other SOTA approaches, aligning with the trend\ntowards lightweight and compressed models.",
        "pdf_link": "https://arxiv.org/pdf/2403.12244v1.pdf"
    },
    {
        "title": "Reference-based Metrics Disprove Themselves in Question Generation",
        "authors": [
            "Bang Nguyen",
            "Mengxia Yu",
            "Yun Huang",
            "Meng Jiang"
        ],
        "published": "2024-03-18T20:47:10Z",
        "summary": "Reference-based metrics such as BLEU and BERTScore are widely used to\nevaluate question generation (QG). In this study, on QG benchmarks such as\nSQuAD and HotpotQA, we find that using human-written references cannot\nguarantee the effectiveness of the reference-based metrics. Most QG benchmarks\nhave only one reference; we replicated the annotation process and collect\nanother reference. A good metric was expected to grade a human-validated\nquestion no worse than generated questions. However, the results of\nreference-based metrics on our newly collected reference disproved the metrics\nthemselves. We propose a reference-free metric consisted of multi-dimensional\ncriteria such as naturalness, answerability, and complexity, utilizing large\nlanguage models. These criteria are not constrained to the syntactic or\nsemantic of a single reference question, and the metric does not require a\ndiverse set of references. Experiments reveal that our metric accurately\ndistinguishes between high-quality questions and flawed ones, and achieves\nstate-of-the-art alignment with human judgment.",
        "pdf_link": "https://arxiv.org/pdf/2403.12242v1.pdf"
    },
    {
        "title": "Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models",
        "authors": [
            "Nusrat Zahan",
            "Philipp Burckhardt",
            "Mikola Lysenko",
            "Feross Aboukhadijeh",
            "Laurie Williams"
        ],
        "published": "2024-03-18T19:10:12Z",
        "summary": "The Gartner 2022 report predicts that 45% of organizations worldwide will\nencounter software supply chain attacks by 2025, highlighting the urgency to\nimprove software supply chain security for community and national interests.\nCurrent malware detection techniques aid in the manual review process by\nfiltering benign and malware packages, yet such techniques have high\nfalse-positive rates and limited automation support. Therefore, malware\ndetection techniques could benefit from advanced, more automated approaches for\naccurate and minimally false-positive results. The goal of this study is to\nassist security analysts in identifying malicious packages through the\nempirical study of large language models (LLMs) to detect potential malware in\nthe npm ecosystem.\n  We present SocketAI Scanner, a multi-stage decision-maker malware detection\nworkflow using iterative self-refinement and zero-shot-role-play-Chain of\nThought (CoT) prompting techniques for ChatGPT. We studied 5,115 npm packages\n(of which 2,180 are malicious) and performed a baseline comparison of the GPT-3\nand GPT-4 models with a static analysis tool. Our findings showed promising\nresults for GPT models with low misclassification alert rates. Our baseline\ncomparison demonstrates a notable improvement over static analysis in precision\nscores above 25% and F1 scores above 15%. We attained precision and F1 scores\nof 91% and 94%, respectively, for the GPT-3 model. Overall, GPT-4 demonstrates\nsuperior performance in precision (99%) and F1 (97%) scores, while GPT-3\npresents a cost-effective balance between performance and expenditure.",
        "pdf_link": "https://arxiv.org/pdf/2403.12196v1.pdf"
    },
    {
        "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
        "authors": [
            "Mengting Wan",
            "Tara Safavi",
            "Sujay Kumar Jauhar",
            "Yujin Kim",
            "Scott Counts",
            "Jennifer Neville",
            "Siddharth Suri",
            "Chirag Shah",
            "Ryen W White",
            "Longqi Yang",
            "Reid Andersen",
            "Georg Buscher",
            "Dhruv Joshi",
            "Nagu Rangan"
        ],
        "published": "2024-03-18T18:45:28Z",
        "summary": "Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.12173v1.pdf"
    },
    {
        "title": "EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models",
        "authors": [
            "Weikang Zhou",
            "Xiao Wang",
            "Limao Xiong",
            "Han Xia",
            "Yingshuang Gu",
            "Mingxu Chai",
            "Fukang Zhu",
            "Caishuang Huang",
            "Shihan Dou",
            "Zhiheng Xi",
            "Rui Zheng",
            "Songyang Gao",
            "Yicheng Zou",
            "Hang Yan",
            "Yifan Le",
            "Ruohui Wang",
            "Lijun Li",
            "Jing Shao",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-03-18T18:39:53Z",
        "summary": "Jailbreak attacks are crucial for identifying and mitigating the security\nvulnerabilities of Large Language Models (LLMs). They are designed to bypass\nsafeguards and elicit prohibited outputs. However, due to significant\ndifferences among various jailbreak methods, there is no standard\nimplementation framework available for the community, which limits\ncomprehensive security evaluations. This paper introduces EasyJailbreak, a\nunified framework simplifying the construction and evaluation of jailbreak\nattacks against LLMs. It builds jailbreak attacks using four components:\nSelector, Mutator, Constraint, and Evaluator. This modular framework enables\nresearchers to easily construct attacks from combinations of novel and existing\ncomponents. So far, EasyJailbreak supports 11 distinct jailbreak methods and\nfacilitates the security validation of a broad spectrum of LLMs. Our validation\nacross 10 distinct LLMs reveals a significant vulnerability, with an average\nbreach probability of 60% under various jailbreaking attacks. Notably, even\nadvanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success\nRates (ASR) of 57% and 33%, respectively. We have released a wealth of\nresources for researchers, including a web platform, PyPI published package,\nscreencast video, and experimental outputs.",
        "pdf_link": "https://arxiv.org/pdf/2403.12171v1.pdf"
    },
    {
        "title": "Syn-QA2: Evaluating False Assumptions in Long-tail Questions with Synthetic QA Datasets",
        "authors": [
            "Ashwin Daswani",
            "Rohan Sawant",
            "Najoung Kim"
        ],
        "published": "2024-03-18T18:01:26Z",
        "summary": "Sensitivity to false assumptions (or false premises) in information-seeking\nquestions is critical for robust question-answering (QA) systems. Recent work\nhas shown that false assumptions in naturally occurring questions pose\nchallenges to current models, with low performance on both generative QA and\nsimple detection tasks (Kim et al. 2023). However, the focus of existing work\non naturally occurring questions leads to a gap in the analysis of model\nbehavior on the long tail of the distribution of possible questions. To this\nend, we introduce Syn-(QA)$^2$, a set of two synthetically generated QA\ndatasets: one generated using perturbed relations from Wikidata, and the other\nby perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range\nof large language models are threefold: (1) false assumptions in QA are\nchallenging, echoing the findings of prior work, (2) the binary detection task\nis challenging even compared to the difficulty of generative QA itself,\npossibly due to the linguistic structure of the problem, and (3) the detection\ntask is more challenging with long-tail questions compared to naturally\noccurring questions, highlighting the utility of our synthetic datasets and\ngeneration method.",
        "pdf_link": "https://arxiv.org/pdf/2403.12145v1.pdf"
    },
    {
        "title": "MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control",
        "authors": [
            "Enshen Zhou",
            "Yiran Qin",
            "Zhenfei Yin",
            "Yuzhou Huang",
            "Ruimao Zhang",
            "Lu Sheng",
            "Yu Qiao",
            "Jing Shao"
        ],
        "published": "2024-03-18T17:59:42Z",
        "summary": "It is a long-lasting goal to design a generalist-embodied agent that can\nfollow diverse instructions in human-like ways. However, existing approaches\noften fail to steadily follow instructions due to difficulties in understanding\nabstract and sequential natural language instructions. To this end, we\nintroduce MineDreamer, an open-ended embodied agent built upon the challenging\nMinecraft simulator with an innovative paradigm that enhances\ninstruction-following ability in low-level control signal generation.\nSpecifically, MineDreamer is developed on top of recent advances in Multimodal\nLarge Language Models (MLLMs) and diffusion models, and we employ a\nChain-of-Imagination (CoI) mechanism to envision the step-by-step process of\nexecuting instructions and translating imaginations into more precise visual\nprompts tailored to the current state; subsequently, the agent generates\nkeyboard-and-mouse actions to efficiently achieve these imaginations, steadily\nfollowing the instructions at each step. Extensive experiments demonstrate that\nMineDreamer follows single and multi-step instructions steadily, significantly\noutperforming the best generalist agent baseline and nearly doubling its\nperformance. Moreover, qualitative analysis of the agent's imaginative ability\nreveals its generalization and comprehension of the open world.",
        "pdf_link": "https://arxiv.org/pdf/2403.12037v2.pdf"
    },
    {
        "title": "RouterBench: A Benchmark for Multi-LLM Routing System",
        "authors": [
            "Qitian Jason Hu",
            "Jacob Bieker",
            "Xiuyu Li",
            "Nan Jiang",
            "Benjamin Keigwin",
            "Gaurav Ranganath",
            "Kurt Keutzer",
            "Shriyash Kaustubh Upadhyay"
        ],
        "published": "2024-03-18T17:59:04Z",
        "summary": "As the range of applications for Large Language Models (LLMs) continues to\ngrow, the demand for effective serving solutions becomes increasingly critical.\nDespite the versatility of LLMs, no single model can optimally address all\ntasks and applications, particularly when balancing performance with cost. This\nlimitation has led to the development of LLM routing systems, which combine the\nstrengths of various models to overcome the constraints of individual LLMs.\nYet, the absence of a standardized benchmark for evaluating the performance of\nLLM routers hinders progress in this area. To bridge this gap, we present\nRouterBench, a novel evaluation framework designed to systematically assess the\nefficacy of LLM routing systems, along with a comprehensive dataset comprising\nover 405k inference outcomes from representative LLMs to support the\ndevelopment of routing strategies. We further propose a theoretical framework\nfor LLM routing, and deliver a comparative analysis of various routing\napproaches through RouterBench, highlighting their potentials and limitations\nwithin our evaluation framework. This work not only formalizes and advances the\ndevelopment of LLM routing systems but also sets a standard for their\nassessment, paving the way for more accessible and economically viable LLM\ndeployments. The code and data are available at\nhttps://github.com/withmartian/routerbench.",
        "pdf_link": "https://arxiv.org/pdf/2403.12031v2.pdf"
    },
    {
        "title": "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models",
        "authors": [
            "Stephen R. Pfohl",
            "Heather Cole-Lewis",
            "Rory Sayres",
            "Darlene Neal",
            "Mercy Asiedu",
            "Awa Dieng",
            "Nenad Tomasev",
            "Qazi Mamunur Rashid",
            "Shekoofeh Azizi",
            "Negar Rostamzadeh",
            "Liam G. McCoy",
            "Leo Anthony Celi",
            "Yun Liu",
            "Mike Schaekermann",
            "Alanna Walton",
            "Alicia Parrish",
            "Chirag Nagpal",
            "Preeti Singh",
            "Akeiylah Dewitt",
            "Philip Mansfield",
            "Sushant Prakash",
            "Katherine Heller",
            "Alan Karthikesalingam",
            "Christopher Semturs",
            "Joelle Barral",
            "Greg Corrado",
            "Yossi Matias",
            "Jamila Smith-Loud",
            "Ivor Horn",
            "Karan Singhal"
        ],
        "published": "2024-03-18T17:56:37Z",
        "summary": "Large language models (LLMs) hold immense promise to serve complex health\ninformation needs but also have the potential to introduce harm and exacerbate\nhealth disparities. Reliably evaluating equity-related model failures is a\ncritical step toward developing systems that promote health equity. In this\nwork, we present resources and methodologies for surfacing biases with\npotential to precipitate equity-related harms in long-form, LLM-generated\nanswers to medical questions and then conduct an empirical case study with\nMed-PaLM 2, resulting in the largest human evaluation study in this area to\ndate. Our contributions include a multifactorial framework for human assessment\nof LLM-generated answers for biases, and EquityMedQA, a collection of seven\nnewly-released datasets comprising both manually-curated and LLM-generated\nquestions enriched for adversarial queries. Both our human assessment framework\nand dataset design process are grounded in an iterative participatory approach\nand review of possible biases in Med-PaLM 2 answers to adversarial queries.\nThrough our empirical study, we find that the use of a collection of datasets\ncurated through a variety of methodologies, coupled with a thorough evaluation\nprotocol that leverages multiple assessment rubric designs and diverse rater\ngroups, surfaces biases that may be missed via narrower evaluation approaches.\nOur experience underscores the importance of using diverse assessment\nmethodologies and involving raters of varying backgrounds and expertise. We\nemphasize that while our framework can identify specific forms of bias, it is\nnot sufficient to holistically assess whether the deployment of an AI system\npromotes equitable health outcomes. We hope the broader community leverages and\nbuilds on these tools and methods towards realizing a shared goal of LLMs that\npromote accessible and equitable healthcare for all.",
        "pdf_link": "https://arxiv.org/pdf/2403.12025v1.pdf"
    },
    {
        "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning",
        "authors": [
            "Hao Sun"
        ],
        "published": "2024-03-18T17:52:57Z",
        "summary": "The prevailing approach to aligning Large Language Models (LLMs) typically\nrelies on human or AI feedback and assumes access to specific types of\npreference datasets. In our work, we question the efficacy of such datasets and\nexplore various scenarios where alignment with expert demonstrations proves\nmore realistic. We build a sequential decision-making framework to formulate\nthe problem of aligning LLMs using demonstration datasets. Drawing insights\nfrom inverse reinforcement learning and imitation learning, we introduce\nvarious approaches for divergence minimization in the LLM alignment tasks. Our\nanalysis highlights the mass-covering and mode-seeking behaviors of these\ndifferent approaches. Inclusively, we examine the pros and cons of the\nclassical supervised fine-tuning method, elaborating on scenarios where\ndifferent methods shine.",
        "pdf_link": "https://arxiv.org/pdf/2403.12017v1.pdf"
    },
    {
        "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
        "authors": [
            "Abhay Zala",
            "Jaemin Cho",
            "Han Lin",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "published": "2024-03-18T17:51:16Z",
        "summary": "Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller embodied RL agents learn useful skills that they are weak at? We\npropose EnvGen, a novel framework to address this question. First, we prompt an\nLLM to generate training environments that allow agents to quickly learn\ndifferent tasks in parallel. Concretely, the LLM is given the task description\nand simulator objectives that the agents should learn and is then asked to\ngenerate a set of environment configurations (e.g., different terrains, items\ngiven to agents, etc.). Next, we train a small RL agent in a mixture of the\noriginal and LLM-generated environments. Then, we enable the LLM to\ncontinuously adapt the generated environments to progressively improve the\nskills that the agent is weak at, by providing feedback to the LLM in the form\nof the agent's performance. We demonstrate the usefulness of EnvGen with\ncomprehensive experiments in Crafter and Heist environments. We find that a\nsmall RL agent trained with EnvGen can outperform SOTA methods, including a\nGPT-4 agent, and learns long-horizon tasks significantly faster. We show\nqualitatively how the LLM adapts training environments to help improve RL\nagents' weaker skills over time. Additionally, EnvGen is substantially more\nefficient as it only uses a small number of LLM calls (e.g., 4 in total),\nwhereas LLM agents require thousands of LLM calls. Lastly, we present detailed\nablation studies for our design choices.",
        "pdf_link": "https://arxiv.org/pdf/2403.12014v1.pdf"
    },
    {
        "title": "NovelQA: A Benchmark for Long-Range Novel Question Answering",
        "authors": [
            "Cunxiang Wang",
            "Ruoxi Ning",
            "Boqi Pan",
            "Tonghui Wu",
            "Qipeng Guo",
            "Cheng Deng",
            "Guangsheng Bao",
            "Qian Wang",
            "Yue Zhang"
        ],
        "published": "2024-03-18T17:32:32Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has introduced a new\nfrontier in natural language processing, particularly in understanding and\nprocessing long-context information. However, the evaluation of these models'\nlong-context abilities remains a challenge due to the limitations of current\nbenchmarks. To address this gap, we introduce NovelQA, a benchmark specifically\ndesigned to test the capabilities of LLMs with extended texts. Constructed from\nEnglish novels, NovelQA offers a unique blend of complexity, length, and\nnarrative coherence, making it an ideal tool for assessing deep textual\nunderstanding in LLMs. This paper presents the design and construction of\nNovelQA, highlighting its manual annotation, and diverse question types. Our\nevaluation of Long-context LLMs on NovelQA reveals significant insights into\nthe models' performance, particularly emphasizing the challenges they face with\nmulti-hop reasoning, detail-oriented questions, and extremely long input with\nmore than 100,000 tokens. The results underscore the necessity for further\nadvancements in LLMs to improve their long-context comprehension and\ncomputational literary studies.",
        "pdf_link": "https://arxiv.org/pdf/2403.12766v1.pdf"
    },
    {
        "title": "Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching",
        "authors": [
            "Andrew Katz",
            "Mitchell Gerhardt",
            "Michelle Soledad"
        ],
        "published": "2024-03-18T17:21:35Z",
        "summary": "Feedback is a critical aspect of improvement. Unfortunately, when there is a\nlot of feedback from multiple sources, it can be difficult to distill the\ninformation into actionable insights. Consider student evaluations of teaching\n(SETs), which are important sources of feedback for educators. They can give\ninstructors insights into what worked during a semester. A collection of SETs\ncan also be useful to administrators as signals for courses or entire programs.\nHowever, on a large scale as in high-enrollment courses or administrative\nrecords over several years, the volume of SETs can render them difficult to\nanalyze. In this paper, we discuss a novel method for analyzing SETs using\nnatural language processing (NLP) and large language models (LLMs). We\ndemonstrate the method by applying it to a corpus of 5,000 SETs from a large\npublic university. We show that the method can be used to extract, embed,\ncluster, and summarize the SETs to identify the themes they express. More\ngenerally, this work illustrates how to use the combination of NLP techniques\nand LLMs to generate a codebook for SETs. We conclude by discussing the\nimplications of this method for analyzing SETs and other types of student\nwriting in teaching and research settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11984v1.pdf"
    },
    {
        "title": "Towards Enabling FAIR Dataspaces Using Large Language Models",
        "authors": [
            "Benedikt T. Arnold",
            "Johannes Theissen-Lipp",
            "Diego Collarana",
            "Christoph Lange",
            "Sandra Geisler",
            "Edward Curry",
            "Stefan Decker"
        ],
        "published": "2024-03-18T16:46:00Z",
        "summary": "Dataspaces have recently gained adoption across various sectors, including\ntraditionally less digitized domains such as culture. Leveraging Semantic Web\ntechnologies helps to make dataspaces FAIR, but their complexity poses a\nsignificant challenge to the adoption of dataspaces and increases their cost.\nThe advent of Large Language Models (LLMs) raises the question of how these\nmodels can support the adoption of FAIR dataspaces. In this work, we\ndemonstrate the potential of LLMs in dataspaces with a concrete example. We\nalso derive a research agenda for exploring this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2403.15451v1.pdf"
    },
    {
        "title": "A Closer Look at Claim Decomposition",
        "authors": [
            "Miriam Wanner",
            "Seth Ebner",
            "Zhengping Jiang",
            "Mark Dredze",
            "Benjamin Van Durme"
        ],
        "published": "2024-03-18T16:03:45Z",
        "summary": "As generated text becomes more commonplace, it is increasingly important to\nevaluate how well-supported such text is by external knowledge sources. Many\napproaches for evaluating textual support rely on some method for decomposing\ntext into its individual subclaims which are scored against a trusted\nreference. We investigate how various methods of claim decomposition --\nespecially LLM-based methods -- affect the result of an evaluation approach\nsuch as the recently proposed FActScore, finding that it is sensitive to the\ndecomposition method used. This sensitivity arises because such metrics\nattribute overall textual support to the model that generated the text even\nthough error can also come from the metric's decomposition step. To measure\ndecomposition quality, we introduce an adaptation of FActScore, which we call\nDecompScore. We then propose an LLM-based approach to generating decompositions\ninspired by Bertrand Russell's theory of logical atomism and neo-Davidsonian\nsemantics and demonstrate its improved decomposition quality over previous\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2403.11903v1.pdf"
    },
    {
        "title": "Investigating Markers and Drivers of Gender Bias in Machine Translations",
        "authors": [
            "Peter J Barclay",
            "Ashkan Sami"
        ],
        "published": "2024-03-18T15:54:46Z",
        "summary": "Implicit gender bias in Large Language Models (LLMs) is a well-documented\nproblem, and implications of gender introduced into automatic translations can\nperpetuate real-world biases. However, some LLMs use heuristics or\npost-processing to mask such bias, making investigation difficult. Here, we\nexamine bias in LLMss via back-translation, using the DeepL translation API to\ninvestigate the bias evinced when repeatedly translating a set of 56 Software\nEngineering tasks used in a previous study. Each statement starts with 'she',\nand is translated first into a 'genderless' intermediate language then back\ninto English; we then examine pronoun-choice in the back-translated texts. We\nexpand prior research in the following ways: (1) by comparing results across\nfive intermediate languages, namely Finnish, Indonesian, Estonian, Turkish and\nHungarian; (2) by proposing a novel metric for assessing the variation in\ngender implied in the repeated translations, avoiding the over-interpretation\nof individual pronouns, apparent in earlier work; (3) by investigating sentence\nfeatures that drive bias; (4) and by comparing results from three time-lapsed\ndatasets to establish the reproducibility of the approach. We found that some\nlanguages display similar patterns of pronoun use, falling into three loose\ngroups, but that patterns vary between groups; this underlines the need to work\nwith multiple languages. We also identify the main verb appearing in a sentence\nas a likely significant driver of implied gender in the translations. Moreover,\nwe see a good level of replicability in the results, and establish that our\nvariation metric proves robust despite an obvious change in the behaviour of\nthe DeepL translation API during the course of the study. These results show\nthat the back-translation method can provide further insights into bias in\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2403.11896v2.pdf"
    },
    {
        "title": "From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?",
        "authors": [
            "Guangming Huang",
            "Yunfei Long",
            "Yingya Li",
            "Giorgos Papanastasiou"
        ],
        "published": "2024-03-18T15:53:33Z",
        "summary": "Deep learning (DL) has substantially enhanced healthcare research by\naddressing various natural language processing (NLP) tasks. Yet, the increasing\ncomplexity of DL-based NLP methods necessitates transparent model\ninterpretability, or at least explainability, for reliable decision-making.\nThis work presents a thorough scoping review on explainable and interpretable\nDL in healthcare NLP. The term \"XIAI\" (eXplainable and Interpretable Artificial\nIntelligence) was introduced to distinguish XAI from IAI. Methods were further\ncategorized based on their functionality (model-, input-, output-based) and\nscope (local, global). Our analysis shows that attention mechanisms were the\nmost dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The\nmajor challenges identified are that most XIAI do not explore \"global\" modeling\nprocesses, the lack of best practices, and the unmet need for systematic\nevaluation and benchmarks. Important opportunities were raised such as using\n\"attention\" to enhance multi-modal XIAI for personalized medicine and combine\nDL with causal reasoning. Our discussion encourages the integration of XIAI in\nLLMs and domain-specific smaller models. Our review can stimulate further\nresearch and benchmarks toward improving inherent IAI and engaging complex NLP\nin healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2403.11894v1.pdf"
    },
    {
        "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction",
        "authors": [
            "Xiang Huang",
            "Sitao Cheng",
            "Shanshan Huang",
            "Jiayu Shen",
            "Yong Xu",
            "Chaoyun Zhang",
            "Yuzhong Qu"
        ],
        "published": "2024-03-18T15:39:14Z",
        "summary": "Employing Large Language Models (LLMs) for semantic parsing has achieved\nremarkable success. However, we find existing methods fall short in terms of\nreliability and efficiency when hallucinations are encountered. In this paper,\nwe address these challenges with a framework called QueryAgent, which solves a\nquestion step-by-step and performs step-wise self-correction. We introduce an\nenvironmental feedback-based self-correction method called ERASER. Unlike\ntraditional approaches, ERASER leverages rich environmental feedback in the\nintermediate steps to perform selective and differentiated self-correction only\nwhen necessary. Experimental results demonstrate that QueryAgent notably\noutperforms all previous few-shot methods using only one example on GrailQA and\nGraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms\nof efficiency, including runtime, query overhead, and API invocation costs. By\nleveraging ERASER, we further improve another baseline (i.e., AgentBench) by\napproximately 10 points, revealing the strong transferability of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2403.11886v1.pdf"
    },
    {
        "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture",
        "authors": [
            "Shanglong Yang",
            "Zhipeng Yuan",
            "Shunbao Li",
            "Ruoling Peng",
            "Kang Liu",
            "Po Yang"
        ],
        "published": "2024-03-18T15:08:01Z",
        "summary": "In the rapidly evolving field of artificial intelligence (AI), the\napplication of large language models (LLMs) in agriculture, particularly in\npest management, remains nascent. We aimed to prove the feasibility by\nevaluating the content of the pest management advice generated by LLMs,\nincluding the Generative Pre-trained Transformer (GPT) series from OpenAI and\nthe FLAN series from Google. Considering the context-specific properties of\nagricultural advice, automatically measuring or quantifying the quality of text\ngenerated by LLMs becomes a significant challenge. We proposed an innovative\napproach, using GPT-4 as an evaluator, to score the generated content on\nCoherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and\nExhaustiveness. Additionally, we integrated an expert system based on crop\nthreshold data as a baseline to obtain scores for Factual Accuracy on whether\npests found in crop fields should take management action. Each model's score\nwas weighted by percentage to obtain a final score. The results showed that\nGPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories.\nFurthermore, the use of instruction-based prompting containing domain-specific\nknowledge proved the feasibility of LLMs as an effective tool in agriculture,\nwith an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing\npest management suggestions.",
        "pdf_link": "https://arxiv.org/pdf/2403.11858v1.pdf"
    },
    {
        "title": "Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models",
        "authors": [
            "Yi Luo",
            "Zhenghao Lin",
            "Yuhao Zhang",
            "Jiashuo Sun",
            "Chen Lin",
            "Chengjin Xu",
            "Xiangdong Su",
            "Yelong Shen",
            "Jian Guo",
            "Yeyun Gong"
        ],
        "published": "2024-03-18T14:48:29Z",
        "summary": "Large Language Models (LLMs) exhibit impressive capabilities but also present\nrisks such as biased content generation and privacy issues. One of the current\nalignment techniques includes principle-driven integration, but it faces\nchallenges arising from the imprecision of manually crafted rules and\ninadequate risk perception in models without safety training. To address these,\nwe introduce Guide-Align, a two-stage approach. Initially, a safety-trained\nmodel identifies potential risks and formulates specific guidelines for various\ninputs, establishing a comprehensive library of guidelines and a model for\ninput-guidelines retrieval. Subsequently, the retrieval model correlates new\ninputs with relevant guidelines, which guide LLMs in response generation to\nensure safe and high-quality outputs, thereby aligning with human values. An\nadditional optional stage involves fine-tuning a model with well-aligned\ndatasets generated through the process implemented in the second stage. Our\nmethod customizes guidelines to accommodate diverse inputs, thereby enhancing\nthe fine-grainedness and comprehensiveness of the guideline library.\nFurthermore, it incorporates safety expertise from a safety-trained LLM through\na lightweight retrieval model. We evaluate our approach on three benchmarks,\ndemonstrating significant improvements in LLM security and quality. Notably,\nour fine-tuned model, Labrador, even at 13 billion parameters, outperforms\nGPT-3.5-turbo and surpasses GPT-4 in alignment capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.11838v2.pdf"
    },
    {
        "title": "Agent3D-Zero: An Agent for Zero-shot 3D Understanding",
        "authors": [
            "Sha Zhang",
            "Di Huang",
            "Jiajun Deng",
            "Shixiang Tang",
            "Wanli Ouyang",
            "Tong He",
            "Yanyong Zhang"
        ],
        "published": "2024-03-18T14:47:03Z",
        "summary": "The ability to understand and reason the 3D real world is a crucial milestone\ntowards artificial general intelligence. The current common practice is to\nfinetune Large Language Models (LLMs) with 3D data and texts to enable 3D\nunderstanding. Despite their effectiveness, these approaches are inherently\nlimited by the scale and diversity of the available 3D data. Alternatively, in\nthis work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework\naddressing the 3D scene understanding in a zero-shot manner. The essence of our\napproach centers on reconceptualizing the challenge of 3D scene perception as a\nprocess of understanding and synthesizing insights from multiple images,\ninspired by how our human beings attempt to understand 3D scenes. By\nconsolidating this idea, we propose a novel way to make use of a Large Visual\nLanguage Model (VLM) via actively selecting and analyzing a series of\nviewpoints for 3D understanding. Specifically, given an input 3D scene,\nAgent3D-Zero first processes a bird's-eye view image with custom-designed\nvisual prompts, then iteratively chooses the next viewpoints to observe and\nsummarize the underlying knowledge. A distinctive advantage of Agent3D-Zero is\nthe introduction of novel visual prompts, which significantly unleash the VLMs'\nability to identify the most informative viewpoints and thus facilitate\nobserving 3D scenes. Extensive experiments demonstrate the effectiveness of the\nproposed framework in understanding diverse and previously unseen 3D\nenvironments.",
        "pdf_link": "https://arxiv.org/pdf/2403.11835v1.pdf"
    },
    {
        "title": "SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator",
        "authors": [
            "Javad Rafiei Asl",
            "Mohammad H. Rafiei",
            "Manar Alohaly",
            "Daniel Takabi"
        ],
        "published": "2024-03-18T14:45:20Z",
        "summary": "Machine learning models are vulnerable to maliciously crafted Adversarial\nExamples (AEs). Training a machine learning model with AEs improves its\nrobustness and stability against adversarial attacks. It is essential to\ndevelop models that produce high-quality AEs. Developing such models has been\nmuch slower in natural language processing (NLP) than in areas such as computer\nvision. This paper introduces a practical and efficient adversarial attack\nmodel called SSCAE for \\textbf{S}emantic, \\textbf{S}yntactic, and\n\\textbf{C}ontext-aware natural language \\textbf{AE}s generator. SSCAE\nidentifies important words and uses a masked language model to generate an\nearly set of substitutions. Next, two well-known language models are employed\nto evaluate the initial set in terms of semantic and syntactic characteristics.\nWe introduce (1) a dynamic threshold to capture more efficient perturbations\nand (2) a local greedy search to generate high-quality AEs. As a black-box\nmethod, SSCAE generates humanly imperceptible and context-aware AEs that\npreserve semantic consistency and the source language's syntactical and\ngrammatical requirements. The effectiveness and superiority of the proposed\nSSCAE model are illustrated with fifteen comparative experiments and extensive\nsensitivity analysis for parameter optimization. SSCAE outperforms the existing\nmodels in all experiments while maintaining a higher semantic consistency with\na lower query number and a comparable perturbation rate.",
        "pdf_link": "https://arxiv.org/pdf/2403.11833v1.pdf"
    },
    {
        "title": "Metaphor Understanding Challenge Dataset for LLMs",
        "authors": [
            "Xiaoyu Tong",
            "Rochelle Choenni",
            "Martha Lewis",
            "Ekaterina Shutova"
        ],
        "published": "2024-03-18T14:08:59Z",
        "summary": "Metaphors in natural language are a reflection of fundamental cognitive\nprocesses such as analogical reasoning and categorisation, and are deeply\nrooted in everyday communication. Metaphor understanding is therefore an\nessential task for large language models (LLMs). We release the Metaphor\nUnderstanding Challenge Dataset (MUNCH), designed to evaluate the metaphor\nunderstanding capabilities of LLMs. The dataset provides over 10k paraphrases\nfor sentences containing metaphor use, as well as 1.5k instances containing\ninapt paraphrases. The inapt paraphrases were carefully selected to serve as\ncontrol to determine whether the model indeed performs full metaphor\ninterpretation or rather resorts to lexical similarity. All apt and inapt\nparaphrases were manually annotated. The metaphorical sentences cover natural\nmetaphor uses across 4 genres (academic, news, fiction, and conversation), and\nthey exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5\ndemonstrate that MUNCH presents a challenging task for LLMs. The dataset is\nfreely accessible at\nhttps://github.com/xiaoyuisrain/metaphor-understanding-challenge.",
        "pdf_link": "https://arxiv.org/pdf/2403.11810v1.pdf"
    },
    {
        "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments",
        "authors": [
            "Jen-tse Huang",
            "Eric John Li",
            "Man Ho Lam",
            "Tian Liang",
            "Wenxuan Wang",
            "Youliang Yuan",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu",
            "Michael R. Lyu"
        ],
        "published": "2024-03-18T14:04:47Z",
        "summary": "Decision-making, a complicated task requiring various types of abilities,\npresents an excellent framework for assessing Large Language Models (LLMs). Our\nresearch investigates LLMs' decision-making capabilities through the lens of a\nwell-established field, Game Theory. We focus specifically on games that\nsupport the participation of more than two agents simultaneously. Subsequently,\nwe introduce our framework, GAMA-Bench, including eight classical multi-agent\ngames. We design a scoring scheme to assess a model's performance in these\ngames quantitatively. Through GAMA-Bench, we investigate LLMs' robustness,\ngeneralizability, and enhancement strategies. Results reveal that while GPT-3.5\nshows satisfying robustness, its generalizability is relatively limited.\nHowever, its performance can be improved through approaches such as\nChain-of-Thought. Additionally, we conduct evaluations across various LLMs and\nfind that GPT-4 outperforms other models on GAMA-Bench, achieving a score of\n72.5. Moreover, the increasingly higher scores across the three iterations of\nGPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model's\nintelligence with each update. The code and experimental results are made\npublicly available via https://github.com/CUHK-ARISE/GAMABench.",
        "pdf_link": "https://arxiv.org/pdf/2403.11807v1.pdf"
    },
    {
        "title": "Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models",
        "authors": [
            "Mingyang Song",
            "Mao Zheng",
            "Xuan Luo"
        ],
        "published": "2024-03-18T14:01:45Z",
        "summary": "While recent research endeavors have concentrated on developing Large\nLanguage Models (LLMs) with robust long-context capabilities, due to the lack\nof appropriate evaluation strategies, relatively little is known about how well\nthe long-context capability and performance of leading LLMs (e.g., GPT-4 Turbo\nand Kimi Chat). To address this gap, we propose a simple, efficient, and\nreasonable strategy for evaluating long-context LLMs as a new benchmark, named\nCounting-Stars. The Counting-Stars is designed to require LLMs to fully\nunderstand and capture long dependencies in long contexts, further being able\nto collect inter-dependency across multiple pieces of evidence spanning the\nentire context to finish the task. Based on the Counting-Stars, we conduct\nexperiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo\nand Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat\nachieve significant performance in the long context from 4K to 128K. We further\npresent several intriguing analyses regarding the behavior of LLMs processing\nlong context.",
        "pdf_link": "https://arxiv.org/pdf/2403.11802v2.pdf"
    },
    {
        "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
        "authors": [
            "Seungpil Lee",
            "Woochang Sim",
            "Donghyeon Shin",
            "Sanha Hwang",
            "Wongyu Seo",
            "Jiwon Park",
            "Seokki Lee",
            "Sejin Kim",
            "Sundong Kim"
        ],
        "published": "2024-03-18T13:50:50Z",
        "summary": "The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been results-centric, making it difficult to assess the\ninference process. We introduce a new approach using the Abstract and Reasoning\nCorpus (ARC) dataset to evaluate the inference and contextual understanding\nabilities of large language models in a process-centric manner. ARC demands\nrigorous logical structures for problem-solving, making it a benchmark that\nfacilitates the comparison of model inference abilities with humans.\nExperimental results confirm that while large language models possess weak\ninference abilities, they still lag in terms of logical coherence,\ncompositionality, and productivity. Our experiments highlight the reasoning\ncapabilities of LLMs, proposing development paths for achieving human-level\nreasoning.",
        "pdf_link": "https://arxiv.org/pdf/2403.11793v1.pdf"
    },
    {
        "title": "Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models",
        "authors": [
            "Preetha Datta",
            "Fedor Vitiugin",
            "Anastasiia Chizhikova",
            "Nitin Sawhney"
        ],
        "published": "2024-03-18T13:44:48Z",
        "summary": "Extracting hyper-relations is crucial for constructing comprehensive\nknowledge graphs, but there are limited supervised methods available for this\ntask. To address this gap, we introduce a zero-shot prompt-based method using\nOpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.\nComparing our model with a baseline, we achieved promising results, with a\nrecall of 0.77. Although our precision is currently lower, a detailed analysis\nof the model outputs has uncovered potential pathways for future research in\nthis area.",
        "pdf_link": "https://arxiv.org/pdf/2403.11786v1.pdf"
    },
    {
        "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs",
        "authors": [
            "M. Jehanzeb Mirza",
            "Leonid Karlinsky",
            "Wei Lin",
            "Sivan Doveh",
            "Jakub Micorek",
            "Mateusz Kozinski",
            "Hilde Kuhene",
            "Horst Possegger"
        ],
        "published": "2024-03-18T13:03:24Z",
        "summary": "Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively",
        "pdf_link": "https://arxiv.org/pdf/2403.11755v2.pdf"
    },
    {
        "title": "Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems",
        "authors": [
            "Aditya Narayan Sankaran",
            "Vigneshwaran Shankaran",
            "Sampath Lonka",
            "Rajesh Sharma"
        ],
        "published": "2024-03-18T13:02:02Z",
        "summary": "Rhymes and poems are a powerful medium for transmitting cultural norms and\nsocietal roles. However, the pervasive existence of gender stereotypes in these\nworks perpetuates biased perceptions and limits the scope of individuals'\nidentities. Past works have shown that stereotyping and prejudice emerge in\nearly childhood, and developmental research on causal mechanisms is critical\nfor understanding and controlling stereotyping and prejudice. This work\ncontributes by gathering a dataset of rhymes and poems to identify gender\nstereotypes and propose a model with 97% accuracy to identify gender bias.\nGender stereotypes were rectified using a Large Language Model (LLM) and its\neffectiveness was evaluated in a comparative survey against human educator\nrectifications. To summarize, this work highlights the pervasive nature of\ngender stereotypes in literary works and reveals the potential of LLMs to\nrectify gender stereotypes. This study raises awareness and promotes\ninclusivity within artistic expressions, making a significant contribution to\nthe discourse on gender equality.",
        "pdf_link": "https://arxiv.org/pdf/2403.11752v2.pdf"
    },
    {
        "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
        "authors": [
            "Ruyi Xu",
            "Yuan Yao",
            "Zonghao Guo",
            "Junbo Cui",
            "Zanlin Ni",
            "Chunjiang Ge",
            "Tat-Seng Chua",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Gao Huang"
        ],
        "published": "2024-03-18T12:04:11Z",
        "summary": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
        "pdf_link": "https://arxiv.org/pdf/2403.11703v1.pdf"
    },
    {
        "title": "HDLdebugger: Streamlining HDL debugging with Large Language Models",
        "authors": [
            "Xufeng Yao",
            "Haoyang Li",
            "Tsz Ho Chan",
            "Wenyi Xiao",
            "Mingxuan Yuan",
            "Yu Huang",
            "Lei Chen",
            "Bei Yu"
        ],
        "published": "2024-03-18T11:19:37Z",
        "summary": "In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.",
        "pdf_link": "https://arxiv.org/pdf/2403.11671v1.pdf"
    },
    {
        "title": "Let's Focus on Neuron: Neuron-Level Supervised Fine-tuning for Large Language Model",
        "authors": [
            "Haoyun Xu",
            "Runzhe Zhan",
            "Derek F. Wong",
            "Lidia S. Chao"
        ],
        "published": "2024-03-18T09:55:01Z",
        "summary": "Large Language Models (LLMs) are composed of neurons that exhibit various\nbehaviors and roles, which become increasingly diversified as models scale.\nRecent studies have revealed that not all neurons are active across different\ndatasets, and this sparsity correlates positively with the task-specific\nability, leading to advancements in model pruning and training efficiency.\nTraditional fine-tuning methods engage all parameters of LLMs, which is\ncomputationally expensive and may not be necessary. In contrast,\nParameter-Efficient Fine-Tuning (PEFT) approaches aim to minimize the number of\ntrainable parameters, yet they still operate at a relatively macro scale (e.g.,\nlayer-level). We introduce Neuron-Level Fine-Tuning (NeFT), a novel approach\nthat refines the granularity of parameter training down to the individual\nneuron, enabling more precise and computationally efficient model updates. The\nexperimental results show that NeFT not only exceeded the performance of\nfull-parameter fine-tuning and PEFT but also provided insights into the\nanalysis of neurons.",
        "pdf_link": "https://arxiv.org/pdf/2403.11621v1.pdf"
    },
    {
        "title": "Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines",
        "authors": [
            "Ekaterina Trofimova",
            "Emil Sataev",
            "Andrey E. Ustyuzhanin"
        ],
        "published": "2024-03-18T08:58:47Z",
        "summary": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.",
        "pdf_link": "https://arxiv.org/pdf/2403.11585v1.pdf"
    },
    {
        "title": "Reinforcement Learning with Token-level Feedback for Controllable Text Generation",
        "authors": [
            "Wendi Li",
            "Wei Wei",
            "Kaihe Xu",
            "Wenfeng Xie",
            "Dangyang Chen",
            "Yu Cheng"
        ],
        "published": "2024-03-18T08:18:37Z",
        "summary": "To meet the requirements of real-world applications, it is essential to\ncontrol generations of large language models (LLMs). Prior research has tried\nto introduce reinforcement learning (RL) into controllable text generation\nwhile most existing methods suffer from overfitting issues (finetuning-based\nmethods) or semantic collapse (post-processing methods). However, current RL\nmethods are generally guided by coarse-grained (sentence/paragraph-level)\nfeedback, which may lead to suboptimal performance owing to semantic twists or\nprogressions within sentences. To tackle that, we propose a novel reinforcement\nlearning algorithm named TOLE which formulates TOken-LEvel rewards for\ncontrollable text generation, and employs a \"first-quantize-then-noise\"\nparadigm to enhance the robustness of the RL algorithm.Furthermore, TOLE can be\nflexibly extended to multiple constraints with little computational expense.\nExperimental results show that our algorithm can achieve superior performance\non both single-attribute and multi-attribute control tasks. We have released\nour codes at https://github.com/WindyLee0822/CTG",
        "pdf_link": "https://arxiv.org/pdf/2403.11558v1.pdf"
    },
    {
        "title": "LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning",
        "authors": [
            "Shu Wang",
            "Muzhi Han",
            "Ziyuan Jiao",
            "Zeyu Zhang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Hangxin Liu"
        ],
        "published": "2024-03-18T08:03:47Z",
        "summary": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11552v2.pdf"
    },
    {
        "title": "DEE: Dual-stage Explainable Evaluation Method for Text Generation",
        "authors": [
            "Shenyu Zhang",
            "Yu Li",
            "Rui Wu",
            "Xiutian Huang",
            "Yongrui Chen",
            "Wenhao Xu",
            "Guilin Qi"
        ],
        "published": "2024-03-18T06:30:41Z",
        "summary": "Automatic methods for evaluating machine-generated texts hold significant\nimportance due to the expanding applications of generative systems.\nConventional methods tend to grapple with a lack of explainability, issuing a\nsolitary numerical score to signify the assessment outcome. Recent advancements\nhave sought to mitigate this limitation by incorporating large language models\n(LLMs) to offer more detailed error analyses, yet their applicability remains\nconstrained, particularly in industrial contexts where comprehensive error\ncoverage and swift detection are paramount. To alleviate these challenges, we\nintroduce DEE, a Dual-stage Explainable Evaluation method for estimating the\nquality of text generation. Built upon Llama 2, DEE follows a dual-stage\nprinciple guided by stage-specific instructions to perform efficient\nidentification of errors in generated texts in the initial stage and\nsubsequently delves into providing comprehensive diagnostic reports in the\nsecond stage. DEE is fine-tuned on our elaborately assembled dataset AntEval,\nwhich encompasses 15K examples from 4 real-world applications of Alipay that\nemploy generative systems. The dataset concerns newly emerged issues like\nhallucination and toxicity, thereby broadening the scope of DEE's evaluation\ncriteria. Experimental results affirm that DEE's superiority over existing\nevaluation methods, achieving significant improvements in both human\ncorrelation as well as efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2403.11509v1.pdf"
    },
    {
        "title": "Do CLIPs Always Generalize Better than ImageNet Models?",
        "authors": [
            "Qizhou Wang",
            "Yong Lin",
            "Yongqiang Chen",
            "Ludwig Schmidt",
            "Bo Han",
            "Tong Zhang"
        ],
        "published": "2024-03-18T06:04:02Z",
        "summary": "Large vision language models, such as CLIPs, have revolutionized modern\nmachine learning. CLIPs have demonstrated great generalizability under\ndistribution shifts, supported by an increasing body of literature. However,\nthe evaluation datasets for CLIPs are variations primarily designed for\nImageNet benchmarks, which may not fully reflect the extent to which CLIPs,\ne.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap,\nwe collect a real-world dataset called CounterAnimal that contains realistic\nspurious features found in animal photos. CounterAnimal consists of a) the\ncommon group: comprising animals on common backgrounds, and b) the counter\ngroup: including animals on unusual backgrounds. The performance drops from the\ncommon to counter groups quantify the reliance of models on spurious features\n(i.e., backgrounds) to predict the animals. We find that CLIPs trained on\neither LAION or the OpenAI data exhibit notable performance drops on the\ncounter group. Surprisingly, we observe that single-modal models trained on\nImageNet are more robust than CLIPs. We provide both theoretical and empirical\nexplanations for why CLIPs still learn spurious features. Our findings suggest\nthat distribution shifts remain an open problem for CLIPs, and one needs to be\ncautious about test setups when evaluating foundation models pre-trained on a\nsignificantly different scale and distribution.",
        "pdf_link": "https://arxiv.org/pdf/2403.11497v1.pdf"
    },
    {
        "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
        "authors": [
            "Yue Fan",
            "Xiaojian Ma",
            "Rujie Wu",
            "Yuntao Du",
            "Jiaqi Li",
            "Zhi Gao",
            "Qing Li"
        ],
        "published": "2024-03-18T05:07:59Z",
        "summary": "We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.",
        "pdf_link": "https://arxiv.org/pdf/2403.11481v1.pdf"
    },
    {
        "title": "Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V",
        "authors": [
            "Siyu Xu",
            "Yunke Wang",
            "Daochang Liu",
            "Chang Xu"
        ],
        "published": "2024-03-18T04:41:38Z",
        "summary": "Recent advancements in generative AI have suggested that by taking visual\nprompt, GPT-4V can demonstrate significant proficiency in image recognition\ntask. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier for its wide use. To address\nthis challenge, our work introduces Collage Prompting, a budget-friendly\nprompting approach that concatenates multiple images into a single visual\ninput. With collage prompt, GPT-4V is able to perform image recognition on\nseveral images simultaneously. Based on the observation that the accuracy of\nGPT-4V's image recognition varies significantly with the order of images within\nthe collage prompt, our method further learns to optimize the arrangement of\nimages for maximum recognition accuracy. A graph predictor is trained to\nindicate the accuracy of each collage prompt, then we propose an optimization\nmethod to navigate the search space of possible image arrangements. Experiment\nresults across various datasets demonstrate the cost-efficiency score of\ncollage prompt is much larger than standard prompt. Additionally, collage\nprompt with learned arrangement achieves clearly better accuracy than collage\nprompt with random arrangement in GPT-4V's visual recognition.",
        "pdf_link": "https://arxiv.org/pdf/2403.11468v1.pdf"
    },
    {
        "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
        "authors": [
            "Huy Nghiem",
            "Hal Daum\u00e9 III"
        ],
        "published": "2024-03-18T04:12:35Z",
        "summary": "The ubiquitousness of social media has led to the need for reliable and\nefficient detection of offensive content to limit harmful effects. This has led\nto a proliferation of datasets and models related to detecting offensive\ncontent. While sophisticated models have attained strong performance on\nindividual datasets, these models often do not generalize due to differences\nbetween how \"offensive content\" is conceptualized, and the resulting\ndifferences in how these datasets are labeled. In this paper, we introduce\nHateCOT, a dataset of 52,000 samples drawn from diverse existing sources with\nexplanations generated by GPT-3.5-Turbo and human-curated. We show that\npre-training models for the detection of offensive content on HateCOT\nsignificantly boots open-sourced Language Models on three benchmark datasets in\nboth zero and few-shot settings, despite differences in domain and task.} We\nfurther find that HateCOT enables effective K-shot fine-tuning in the\nlow-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11456v1.pdf"
    },
    {
        "title": "LLM Guided Evolution -- The Automation of Models Advancing Models",
        "authors": [
            "Clint Morris",
            "Michael Jurado",
            "Jason Zutty"
        ],
        "published": "2024-03-18T03:44:55Z",
        "summary": "In the realm of machine learning, traditional model development and automated\napproaches like AutoML typically rely on layers of abstraction, such as\ntree-based or Cartesian genetic programming. Our study introduces \"Guided\nEvolution\" (GE), a novel framework that diverges from these methods by\nutilizing Large Language Models (LLMs) to directly modify code. GE leverages\nLLMs for a more intelligent, supervised evolutionary process, guiding mutations\nand crossovers. Our unique \"Evolution of Thought\" (EoT) technique further\nenhances GE by enabling LLMs to reflect on and learn from the outcomes of\nprevious mutations. This results in a self-sustaining feedback loop that\naugments decision-making in model evolution. GE maintains genetic diversity,\ncrucial for evolutionary algorithms, by leveraging LLMs' capability to generate\ndiverse responses from expertly crafted prompts and modulate model temperature.\nThis not only accelerates the evolution process but also injects expert like\ncreativity and insight into the process. Our application of GE in evolving the\nExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously\nproduced variants with improved accuracy, increasing from 92.52% to 93.34%,\nwithout compromising model compactness. This underscores the potential of LLMs\nto accelerate the traditional model design pipeline, enabling models to\nautonomously evolve and enhance their own designs.",
        "pdf_link": "https://arxiv.org/pdf/2403.11446v1.pdf"
    },
    {
        "title": "StyleChat: Learning Recitation-Augmented Memory in LLMs for Stylized Dialogue Generation",
        "authors": [
            "Jinpeng Li",
            "Zekai Zhang",
            "Quan Tu",
            "Xin Cheng",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2024-03-18T03:26:18Z",
        "summary": "Large Language Models (LLMs) demonstrate superior performance in generative\nscenarios and have attracted widespread attention. Among them, stylized\ndialogue generation is essential in the context of LLMs for building\nintelligent and engaging dialogue agent. However the ability of LLMs is\ndata-driven and limited by data bias, leading to poor performance on specific\ntasks. In particular, stylized dialogue generation suffers from a severe lack\nof supervised data. Furthermore, although many prompt-based methods have been\nproposed to accomplish specific tasks, their performance in complex real-world\nscenarios involving a wide variety of dialog styles further enhancement. In\nthis work, we first introduce a stylized dialogue dataset StyleEval with 38\nstyles by leveraging the generative power of LLMs comprehensively, which has\nbeen carefully constructed with rigorous human-led quality control. Based on\nthis, we propose the stylized dialogue framework StyleChat via\nrecitation-augmented memory strategy and multi-task style learning strategy to\npromote generalization ability. To evaluate the effectiveness of our approach,\nwe created a test benchmark that included both a generation task and a choice\ntask to comprehensively evaluate trained models and assess whether styles and\npreferences are remembered and understood. Experimental results show that our\nproposed framework StyleChat outperforms all the baselines and helps to break\nthe style boundary of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.11439v1.pdf"
    },
    {
        "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
        "authors": [
            "Yifan Wang",
            "Yafei Liu",
            "Chufan Shi",
            "Haoling Li",
            "Chen Chen",
            "Haonan Lu",
            "Yujiu Yang"
        ],
        "published": "2024-03-18T03:10:36Z",
        "summary": "Instruction tuning effectively optimizes Large Language Models (LLMs) for\ndownstream tasks. Due to the changing environment in real-life applications,\nLLMs necessitate continual task-specific adaptation without catastrophic\nforgetting. Considering the heavy computational cost, replay-based Continual\nLearning (CL) methods are the simplest and most widely used for LLMs to address\nthe forgetting issue. However, traditional replay-based methods do not fully\nutilize instructions to customize the replay strategy. In this work, we propose\na novel paradigm called Instruction-based Continual Learning (InsCL). InsCL\ndynamically replays previous data based on task similarity, calculated by\nWasserstein Distance with instructions. Moreover, we further introduce an\nInstruction Information Metric (InsInfo) to quantify the complexity and\ndiversity of instructions. According to InsInfo, InsCL guides the replay\nprocess more inclined to high-quality data. We conduct extensive experiments\nover 16 tasks with different training orders, observing consistent performance\nimprovements of InsCL. When all tasks have been trained, InsCL achieves\nperformance gains of 3.0 Relative Gain compared with Random Replay, and 27.96\nRelative Gain compared with No Replay.",
        "pdf_link": "https://arxiv.org/pdf/2403.11435v1.pdf"
    },
    {
        "title": "A Novel Paradigm Boosting Translation Capabilities of Large Language Models",
        "authors": [
            "Jiaxin Guo",
            "Hao Yang",
            "Zongyao Li",
            "Daimeng Wei",
            "Hengchao Shang",
            "Xiaoyu Chen"
        ],
        "published": "2024-03-18T02:53:49Z",
        "summary": "This paper presents a study on strategies to enhance the translation\ncapabilities of large language models (LLMs) in the context of machine\ntranslation (MT) tasks. The paper proposes a novel paradigm consisting of three\nstages: Secondary Pre-training using Extensive Monolingual Data, Continual\nPre-training with Interlinear Text Format Documents, and Leveraging\nSource-Language Consistent Instruction for Supervised Fine-Tuning. Previous\nresearch on LLMs focused on various strategies for supervised fine-tuning\n(SFT), but their effectiveness has been limited. While traditional machine\ntranslation approaches rely on vast amounts of parallel bilingual data, our\nparadigm highlights the importance of using smaller sets of high-quality\nbilingual data. We argue that the focus should be on augmenting LLMs'\ncross-lingual alignment abilities during pre-training rather than solely\nrelying on extensive bilingual data during SFT. Experimental results conducted\nusing the Llama2 model, particularly on Chinese-Llama2 after monolingual\naugmentation, demonstrate the improved translation capabilities of LLMs. A\nsignificant contribution of our approach lies in Stage2: Continual Pre-training\nwith Interlinear Text Format Documents, which requires less than 1B training\ndata, making our method highly efficient. Additionally, in Stage3, we observed\nthat setting instructions consistent with the source language benefits the\nsupervised fine-tuning process. Experimental results demonstrate that our\napproach surpasses previous work and achieves superior performance compared to\nmodels such as NLLB-54B and GPT3.5-text-davinci-003, despite having a\nsignificantly smaller parameter count of only 7B or 13B. This achievement\nestablishes our method as a pioneering strategy in the field of machine\ntranslation.",
        "pdf_link": "https://arxiv.org/pdf/2403.11430v1.pdf"
    },
    {
        "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression",
        "authors": [
            "Junyuan Hong",
            "Jinhao Duan",
            "Chenhui Zhang",
            "Zhangheng Li",
            "Chulin Xie",
            "Kelsey Lieberman",
            "James Diffenderfer",
            "Brian Bartoldson",
            "Ajay Jaiswal",
            "Kaidi Xu",
            "Bhavya Kailkhura",
            "Dan Hendrycks",
            "Dawn Song",
            "Zhangyang Wang",
            "Bo Li"
        ],
        "published": "2024-03-18T01:38:19Z",
        "summary": "Compressing high-capability Large Language Models (LLMs) has emerged as a\nfavored strategy for resource-efficient inferences. While state-of-the-art\n(SoTA) compression methods boast impressive advancements in preserving benign\ntask performance, the potential risks of compression in terms of safety and\ntrustworthiness have been largely neglected. This study conducts the first,\nthorough evaluation of three (3) leading LLMs using five (5) SoTA compression\ntechniques across eight (8) trustworthiness dimensions. Our experiments\nhighlight the intricate interplay between compression and trustworthiness,\nrevealing some interesting patterns. We find that quantization is currently a\nmore effective approach than pruning in achieving efficiency and\ntrustworthiness simultaneously. For instance, a 4-bit quantized model retains\nthe trustworthiness of its original counterpart, but model pruning\nsignificantly degrades trustworthiness, even at 50% sparsity. Moreover,\nemploying quantization within a moderate bit range could unexpectedly improve\ncertain trustworthiness dimensions such as ethics and fairness. Conversely,\nextreme quantization to very low bit levels (3 bits) tends to significantly\nreduce trustworthiness. This increased risk cannot be uncovered by looking at\nbenign performance alone, in turn, mandating comprehensive trustworthiness\nevaluation in practice. These findings culminate in practical recommendations\nfor simultaneously achieving high utility, efficiency, and trustworthiness in\nLLMs. Models and code are available at https://decoding-comp-trust.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.15447v1.pdf"
    },
    {
        "title": "Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning",
        "authors": [
            "Rao Fu",
            "Jingyu Liu",
            "Xilun Chen",
            "Yixin Nie",
            "Wenhan Xiong"
        ],
        "published": "2024-03-18T01:18:48Z",
        "summary": "This paper introduces Scene-LLM, a 3D-visual-language model that enhances\nembodied agents' abilities in interactive 3D indoor environments by integrating\nthe reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a\nhybrid 3D visual feature representation, that incorporates dense spatial\ninformation and supports scene state updates. The model employs a projection\nlayer to efficiently project these features in the pre-trained textual\nembedding space, enabling effective interpretation of 3D visual information.\nUnique to our approach is the integration of both scene-level and ego-centric\n3D information. This combination is pivotal for interactive planning, where\nscene-level data supports global planning and ego-centric data is important for\nlocalization. Notably, we use ego-centric 3D frame features for feature\nalignment, an efficient technique that enhances the model's ability to align\nfeatures of small objects within the scene. Our experiments with Scene-LLM\ndemonstrate its strong capabilities in dense captioning, question answering,\nand interactive planning. We believe Scene-LLM advances the field of 3D visual\nunderstanding and reasoning, offering new possibilities for sophisticated agent\ninteractions in indoor settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11401v2.pdf"
    },
    {
        "title": "X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment",
        "authors": [
            "Dongjae Shin",
            "Hyeonseok Lim",
            "Inho Won",
            "Changsu Choi",
            "Minjun Kim",
            "Seungwoo Song",
            "Hangyeol Yoo",
            "Sangmin Kim",
            "Kyungtae Lim"
        ],
        "published": "2024-03-18T01:14:47Z",
        "summary": "The impressive development of large language models (LLMs) is expanding into\nthe realm of large multimodal models (LMMs), which incorporate multiple types\nof data beyond text. However, the nature of multimodal models leads to\nsignificant expenses in the creation of training data. Furthermore,\nconstructing multilingual data for LMMs presents its own set of challenges due\nto language diversity and complexity. Therefore, in this study, we propose two\ncost-effective methods to solve this problem: (1) vocabulary expansion and\npretraining of multilingual LLM for specific languages, and (2) automatic and\nelaborate construction of multimodal datasets using GPT4-V. Based on015 these\nmethods, we constructed a 91K English-Korean-Chinese multilingual, multimodal\ntraining dataset. Additionally, we developed a bilingual multimodal model that\nexhibits excellent performance in both Korean and English, surpassing existing\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2403.11399v3.pdf"
    },
    {
        "title": "Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative capabilities through Melting Pot",
        "authors": [
            "Manuel Mosquera",
            "Juan Sebastian Pinzon",
            "Manuel Rios",
            "Yesid Fonseca",
            "Luis Felipe Giraldo",
            "Nicanor Quijano",
            "Ruben Manrique"
        ],
        "published": "2024-03-18T00:13:43Z",
        "summary": "As the field of AI continues to evolve, a significant dimension of this\nprogression is the development of Large Language Models and their potential to\nenhance multi-agent artificial intelligence systems. This paper explores the\ncooperative capabilities of Large Language Model-augmented Autonomous Agents\n(LAAs) using the well-known Meltin Pot environments along with reference models\nsuch as GPT4 and GPT3.5. Preliminary results suggest that while these agents\ndemonstrate a propensity for cooperation, they still struggle with effective\ncollaboration in given environments, emphasizing the need for more robust\narchitectures. The study's contributions include an abstraction layer to adapt\nMelting Pot game scenarios for LLMs, the implementation of a reusable\narchitecture for LLM-mediated agent development - which includes short and\nlong-term memories and different cognitive modules, and the evaluation of\ncooperation capabilities using a set of metrics tied to the Melting Pot's\n\"Commons Harvest\" game. The paper closes, by discussing the limitations of the\ncurrent architectural framework and the potential of a new set of modules that\nfosters better cooperation among LAAs.",
        "pdf_link": "https://arxiv.org/pdf/2403.11381v1.pdf"
    },
    {
        "title": "What Makes Math Word Problems Challenging for LLMs?",
        "authors": [
            "KV Aditya Srivatsa",
            "Ekaterina Kochmar"
        ],
        "published": "2024-03-17T23:18:40Z",
        "summary": "This paper investigates the question of what makes math word problems (MWPs)\nin English challenging for large language models (LLMs). We conduct an in-depth\nanalysis of the key linguistic and mathematical characteristics of MWPs. In\naddition, we train feature-based classifiers to better understand the impact of\neach feature on the overall difficulty of MWPs for prominent LLMs and\ninvestigate whether this helps predict how well LLMs fare against specific\ncategories of MWPs.",
        "pdf_link": "https://arxiv.org/pdf/2403.11369v2.pdf"
    },
    {
        "title": "JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning",
        "authors": [
            "Anique Tahir",
            "Lu Cheng",
            "Huan Liu"
        ],
        "published": "2024-03-17T23:02:04Z",
        "summary": "The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU.",
        "pdf_link": "https://arxiv.org/pdf/2403.11366v2.pdf"
    },
    {
        "title": "ConvSDG: Session Data Generation for Conversational Search",
        "authors": [
            "Fengran Mo",
            "Bole Yi",
            "Kelong Mao",
            "Chen Qu",
            "Kaiyu Huang",
            "Jian-Yun Nie"
        ],
        "published": "2024-03-17T20:34:40Z",
        "summary": "Conversational search provides a more convenient interface for users to\nsearch by allowing multi-turn interaction with the search engine. However, the\neffectiveness of the conversational dense retrieval methods is limited by the\nscarcity of training data required for their fine-tuning. Thus, generating more\ntraining conversational sessions with relevant labels could potentially improve\nsearch performance. Based on the promising capabilities of large language\nmodels (LLMs) on text generation, we propose ConvSDG, a simple yet effective\nframework to explore the feasibility of boosting conversational search by using\nLLM for session data generation. Within this framework, we design\ndialogue/session-level and query-level data generation with unsupervised and\nsemi-supervised learning, according to the availability of relevance judgments.\nThe generated data are used to fine-tune the conversational dense retriever.\nExtensive experiments on four widely used datasets demonstrate the\neffectiveness and broad applicability of our ConvSDG framework compared with\nseveral strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.11335v1.pdf"
    },
    {
        "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
        "authors": [
            "Dong Won Lee",
            "Hae Won Park",
            "Yoon Kim",
            "Cynthia Breazeal",
            "Louis-Philippe Morency"
        ],
        "published": "2024-03-17T20:21:26Z",
        "summary": "We describe an approach for aligning an LLM-based dialogue agent based on\nglobal (i.e., dialogue-level) rewards, while also taking into account\nnaturally-occurring multimodal signals. At a high level, our approach (dubbed\nGELI) learns a local, turn-level reward model by decomposing the human-provided\nGlobal Explicit (GE) session-level reward, using Local Implicit (LI} multimodal\nreward signals to crossmodally shape the reward decomposition step. This\ndecomposed reward model is then used as part of the standard RHLF pipeline\nimprove an LLM-based dialog agent. We run quantitative and qualitative human\nstudies to evaluate the performance of our GELI approach, and find that it\nshows consistent improvements across various conversational metrics compared to\nbaseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.11330v1.pdf"
    },
    {
        "title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows",
        "authors": [
            "Yiran Wu",
            "Tianwei Yue",
            "Shaokun Zhang",
            "Chi Wang",
            "Qingyun Wu"
        ],
        "published": "2024-03-17T19:54:16Z",
        "summary": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.11322v2.pdf"
    },
    {
        "title": "Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches",
        "authors": [
            "Igor Sterner",
            "Weizhe Lin",
            "Jinghong Chen",
            "Bill Byrne"
        ],
        "published": "2024-03-17T19:44:05Z",
        "summary": "Two approaches have emerged to input images into large language models\n(LLMs). The first is to caption images into natural language. The second is to\nmap image feature embeddings into the domain of the LLM and pass the mapped\nembeddings directly to the LLM. The majority of recent few-shot multimodal work\nreports performance using architectures that employ variations of one of these\ntwo approaches. But they overlook an important comparison between them. We\ndesign a controlled and focused experiment to compare these two approaches to\nfew-shot visual question answering (VQA) with LLMs. Our findings indicate that\nfor Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to\nthe LLM embedding space does not guarantee improved performance over using\nimage captions. In the zero-shot regime, we find using textual image captions\nis better. In the few-shot regimes, how the in-context examples are selected\ndetermines which is better.",
        "pdf_link": "https://arxiv.org/pdf/2403.11317v1.pdf"
    },
    {
        "title": "Reasoning in Transformers -- Mitigating Spurious Correlations and Reasoning Shortcuts",
        "authors": [
            "Daniel Enstr\u00f6m",
            "Viktor Kjellberg",
            "Moa Johansson"
        ],
        "published": "2024-03-17T19:32:12Z",
        "summary": "Transformer language models are neural networks used for a wide variety of\ntasks concerning natural language, including some that also require logical\nreasoning. However, a transformer model may easily learn spurious patterns in\nthe data, short-circuiting actual reasoning. In this paper we investigate to\nwhat extent transformers can be trained to a) approximate reasoning in\npropositional logic while b) avoiding known reasoning shortcuts via spurious\ncorrelations in the training data. To do so, we use a dataset with known\nspurious correlation between truth and e.g. the number of rules in the problem.\nWe augment the data with proofs, and train two models: a generative\ntransformer, WP-BART, trained on problems and their whole proofs, and a\nneuro-symbolic model, SIP-BART, trained on individual proof steps and combining\nthe generative transformer model BART with a symbolic proof checker. We find\nthat SIP-BART succeeds in avoiding reasoning shortcuts, while WP-BART does not.\nFor SIP-BART, we then identify a few remaining reasoning errors, not previously\ndescribed in the literature, arising from using a pre-trained language model.\nThese are qualitatively analysed to create a taxonomy of four different types\nof additional pitfalls.",
        "pdf_link": "https://arxiv.org/pdf/2403.11314v1.pdf"
    },
    {
        "title": "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant",
        "authors": [
            "Guohao Sun",
            "Can Qin",
            "Jiamian Wang",
            "Zeyuan Chen",
            "Ran Xu",
            "Zhiqiang Tao"
        ],
        "published": "2024-03-17T18:42:38Z",
        "summary": "Recent advancements in the vision-language model have shown notable\ngeneralization in vision-language tasks after visual instruction tuning.\nHowever, bridging the gap between the pre-trained vision encoder and the large\nlanguage models becomes the whole network's bottleneck. To improve\ncross-modality alignment, existing works usually consider more visual\ninstruction data covering a broader range of vision tasks to fine-tune the\nmodel for question-answering, which are costly to obtain. However, the image\ncontains rich contextual information that has been largely under-explored. This\npaper first attempts to harness this overlooked context within visual\ninstruction data, training the model to self-supervised `learning' how to ask\nhigh-quality questions. In this way, we introduce a novel framework named\nSQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA\nexhibits proficiency in generating flexible and meaningful image-related\nquestions while analyzing the visual clue and prior language knowledge,\nsignifying an advanced level of generalized visual understanding. Moreover,\nfine-tuning SQ-LLaVA on higher-quality instruction data shows a consistent\nperformance improvement compared with traditional visual-instruction tuning\nmethods. This improvement highlights the efficacy of self-questioning\ntechniques in achieving a deeper and more nuanced comprehension of visual\ncontent across various contexts.",
        "pdf_link": "https://arxiv.org/pdf/2403.11299v1.pdf"
    },
    {
        "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs",
        "authors": [
            "Lihui Liu",
            "Zihao Wang",
            "Ruizhong Qiu",
            "Yikun Ban",
            "Hanghang Tong"
        ],
        "published": "2024-03-17T17:01:45Z",
        "summary": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2404.04264v1.pdf"
    },
    {
        "title": "Cheap Ways of Extracting Clinical Markers from Texts",
        "authors": [
            "Anastasia Sandu",
            "Teodor Mihailescu",
            "Sergiu Nisioi"
        ],
        "published": "2024-03-17T14:21:42Z",
        "summary": "This paper describes the work of the UniBuc Archaeology team for CLPsych's\n2024 Shared Task, which involved finding evidence within the text supporting\nthe assigned suicide risk level. Two types of evidence were required:\nhighlights (extracting relevant spans within the text) and summaries\n(aggregating evidence into a synthesis). Our work focuses on evaluating Large\nLanguage Models (LLM) as opposed to an alternative method that is much more\nmemory and resource efficient. The first approach employs a good old-fashioned\nmachine learning (GOML) pipeline consisting of a tf-idf vectorizer with a\nlogistic regression classifier, whose representative features are used to\nextract relevant highlights. The second, more resource intensive, uses an LLM\nfor generating the summaries and is guided by chain-of-thought to provide\nsequences of text indicating clinical markers.",
        "pdf_link": "https://arxiv.org/pdf/2403.11227v1.pdf"
    },
    {
        "title": "Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework",
        "authors": [
            "Kaiyan Chang",
            "Kun Wang",
            "Nan Yang",
            "Ying Wang",
            "Dantong Jin",
            "Wenlong Zhu",
            "Zhirong Chen",
            "Cangyuan Li",
            "Hao Yan",
            "Yunhao Zhou",
            "Zhuoliang Zhao",
            "Yuan Cheng",
            "Yudong Pan",
            "Yiqi Liu",
            "Mengdi Wang",
            "Shengwen Liang",
            "yinhe han",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "published": "2024-03-17T13:01:03Z",
        "summary": "Recent advances in large language models have demonstrated their potential\nfor automated generation of hardware description language (HDL) code from\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\nability of these large language models (LLMs) in the field of Chip Design.\nHowever, the lack of Verilog data hinders further improvement in the quality of\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\nElectronic Design Automation (EDA) script data augmentation framework\nsignificantly increases the time required to prepare the training dataset for\nLLM trainers. This paper proposes an automated design-data augmentation\nframework, which generates high-volume and high-quality natural language\naligned with Verilog and EDA scripts. For Verilog generation, it translates\nVerilog files to an abstract syntax tree and then maps nodes to natural\nlanguage with a predefined template. For Verilog repair, it uses predefined\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\nthe right and wrong verilog file. For EDA Script generation, it uses existing\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\neffectiveness of our data augmentation method, we finetune Llama2-13B and\nLlama2-7B models using the dataset generated by our augmentation framework. The\nresults demonstrate a significant improvement in the Verilog generation tasks\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\ncurrent state-of-the-art open-source Verilog generation model, increasing from\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.",
        "pdf_link": "https://arxiv.org/pdf/2403.11202v1.pdf"
    },
    {
        "title": "Correcting misinformation on social media with a large language model",
        "authors": [
            "Xinyi Zhou",
            "Ashish Sharma",
            "Amy X. Zhang",
            "Tim Althoff"
        ],
        "published": "2024-03-17T10:59:09Z",
        "summary": "Real-world misinformation can be partially correct and even factual but\nmisleading. It undermines public trust in science and democracy, particularly\non social media, where it can spread rapidly. High-quality and timely\ncorrection of misinformation that identifies and explains its (in)accuracies\nhas been shown to effectively reduce false beliefs. Despite the wide acceptance\nof manual correction, it is difficult to promptly correct newly created\nmisinformation and to scale this approach, a concern as technologies like large\nlanguage models (LLMs) make misinformation easier to produce. LLMs also have\nversatile capabilities that could accelerate misinformation\ncorrection--however, they struggle due to a lack of recent information, a\ntendency to produce false content, and limitations in addressing multimodal\ninformation. We propose MUSE, an LLM augmented with access to and credibility\nevaluation of up-to-date information. By retrieving evidence as refutations or\ncontexts, MUSE identifies and explains (in)accuracies in a piece of\ncontent--not presupposed to be misinformation--with references. It also\ndescribes images and conducts multimodal searches to verify and correct\nmultimodal content. Fact-checking experts evaluate responses to social media\ncontent that are not presupposed to be (non-)misinformation but broadly include\nincorrect, partially correct, and correct posts, that may or may not be\nmisleading. We propose and evaluate 13 dimensions of misinformation correction\nquality, ranging from the accuracy of identifications and factuality of\nexplanations to the relevance and credibility of references. The results\ndemonstrate MUSE's ability to promptly write high-quality responses to\npotential misinformation on social media--overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%.",
        "pdf_link": "https://arxiv.org/pdf/2403.11169v2.pdf"
    },
    {
        "title": "Evaluation Ethics of LLMs in Legal Domain",
        "authors": [
            "Ruizhe Zhang",
            "Haitao Li",
            "Yueyue Wu",
            "Qingyao Ai",
            "Yiqun Liu",
            "Min Zhang",
            "Shaoping Ma"
        ],
        "published": "2024-03-17T09:05:13Z",
        "summary": "In recent years, the utilization of large language models for natural\nlanguage dialogue has gained momentum, leading to their widespread adoption\nacross various domains. However, their universal competence in addressing\nchallenges specific to specialized fields such as law remains a subject of\nscrutiny. The incorporation of legal ethics into the model has been overlooked\nby researchers. We asserts that rigorous ethic evaluation is essential to\nensure the effective integration of large language models in legal domains,\nemphasizing the need to assess domain-specific proficiency and domain-specific\nethic. To address this, we propose a novelty evaluation methodology, utilizing\nauthentic legal cases to evaluate the fundamental language abilities,\nspecialized legal knowledge and legal robustness of large language models\n(LLMs). The findings from our comprehensive evaluation contribute significantly\nto the academic discourse surrounding the suitability and performance of large\nlanguage models in legal domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.11152v1.pdf"
    },
    {
        "title": "Training A Small Emotional Vision Language Model for Visual Art Comprehension",
        "authors": [
            "Jing Zhang",
            "Liang Zheng",
            "Dan Guo",
            "Meng Wang"
        ],
        "published": "2024-03-17T09:01:02Z",
        "summary": "This paper develops small vision language models to understand visual art,\nwhich, given an art work, aims to identify its emotion category and explain\nthis prediction with natural language. While small models are computationally\nefficient, their capacity is much limited compared with large models. To break\nthis trade-off, this paper builds a small emotional vision language model\n(SEVLM) by emotion modeling and input-output feature alignment. On the one\nhand, based on valence-arousal-dominance (VAD) knowledge annotated by\npsychology experts, we introduce and fuse emotional features derived through\nVAD dictionary and a VAD head to align VAD vectors of predicted emotion\nexplanation and the ground truth. This allows the vision language model to\nbetter understand and generate emotional texts, compared with using traditional\ntext embeddings alone. On the other hand, we design a contrastive head to pull\nclose embeddings of the image, its emotion class, and explanation, which aligns\nmodel outputs and inputs. On two public affective explanation datasets, we show\nthat the proposed techniques consistently improve the visual art understanding\nperformance of baseline SEVLMs. Importantly, the proposed model can be trained\nand evaluated on a single RTX 2080 Ti while exhibiting very strong performance:\nit not only outperforms the state-of-the-art small models but is also\ncompetitive compared with LLaVA 7B after fine-tuning and GPT4(V).",
        "pdf_link": "https://arxiv.org/pdf/2403.11150v1.pdf"
    },
    {
        "title": "Enhancing Event Causality Identification with Rationale and Structure-Aware Causal Question Answering",
        "authors": [
            "Baiyan Zhang",
            "Qin Chen",
            "Jie Zhou",
            "Jian Jin",
            "Liang He"
        ],
        "published": "2024-03-17T07:41:58Z",
        "summary": "Document-level Event Causality Identification (DECI) aims to identify causal\nrelations between two events in documents. Recent research tends to use\npre-trained language models to generate the event causal relations. Whereas,\nthese methods are prone to the errors of sequential generation due to multiple\nevents in a document. Moreover, the potential structures such as event\ncoreference and related causal chain are neglected. In this paper, we propose a\nmulti-task learning framework to enhance event causality identification with\nrationale and structure-aware causal question answering. Specifically, the DECI\ntask is transformed into multiple-choice question answering, and the causes and\neffects of the questioned event are generated with large language models. In\naddition, we generate the rationales to explain why these events have causal\nrelations. Moreover, we construct an event structure graph, which models the\nmulti-hop potential relations for causal reasoning of the current event.\nExperiments on two benchmark datasets show the great advantages of our proposed\napproach compared to the state-of-the-art methods. Moreover, we conduct both\nquantitative and qualitative analyses, which shed light on why each component\nof our approach can lead to great improvements.",
        "pdf_link": "https://arxiv.org/pdf/2403.11129v1.pdf"
    },
    {
        "title": "Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities",
        "authors": [
            "Honglin Mu",
            "Yang Xu",
            "Yunlong Feng",
            "Xiaofeng Han",
            "Yitong Li",
            "Yutai Hou",
            "Wanxiang Che"
        ],
        "published": "2024-03-17T07:34:12Z",
        "summary": "With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2403.11128v2.pdf"
    },
    {
        "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment",
        "authors": [
            "Feifan Song",
            "Bowen Yu",
            "Hao Lang",
            "Haiyang Yu",
            "Fei Huang",
            "Houfeng Wang",
            "Yongbin Li"
        ],
        "published": "2024-03-17T07:08:55Z",
        "summary": "Alignment with human preference prevents large language models (LLMs) from\ngenerating misleading or toxic content while requiring high-cost human\nfeedback. Assuming resources of human annotation are limited, there are two\ndifferent ways of allocating considered: more diverse PROMPTS or more diverse\nRESPONSES to be labeled. Nonetheless, a straightforward comparison between\ntheir impact is absent. In this work, we first control the diversity of both\nsides according to the number of samples for fine-tuning, which can directly\nreflect their influence. We find that instead of numerous prompts, more\nresponses but fewer prompts better trigger LLMs for human alignment.\nAdditionally, the concept of diversity for prompts can be more complex than\nresponses that are typically quantified by single digits. Consequently, a new\nformulation of prompt diversity is proposed, further implying a linear\ncorrelation with the final performance of LLMs after fine-tuning. We also\nleverage it on data augmentation and conduct experiments to show its effect on\ndifferent algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2403.11124v2.pdf"
    }
]