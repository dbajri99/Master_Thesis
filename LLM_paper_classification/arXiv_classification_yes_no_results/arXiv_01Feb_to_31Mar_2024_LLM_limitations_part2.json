[
    {
        "title": "PhD: A Prompted Visual Hallucination Evaluation Dataset",
        "authors": [
            "Jiazhen Liu",
            "Yuhan Fu",
            "Ruobing Xie",
            "Runquan Xie",
            "Xingwu Sun",
            "Fengzong Lian",
            "Zhanhui Kang",
            "Xirong Li"
        ],
        "published": "2024-03-17T06:53:44Z",
        "summary": "The rapid growth of Large Language Models (LLMs) has driven the development\nof Large Vision-Language Models (LVLMs). The challenge of hallucination,\nprevalent in LLMs, also emerges in LVLMs. However, most existing efforts mainly\nfocus on object hallucination in LVLM, ignoring diverse types of LVLM\nhallucinations. In this study, we delve into the Intrinsic Vision-Language\nHallucination (IVL-Hallu) issue, thoroughly analyzing different types of\nIVL-Hallu on their causes and reflections. Specifically, we propose several\nnovel IVL-Hallu tasks and categorize them into four types: (a) object\nhallucination, which arises from the misidentification of objects, (b)\nattribute hallucination, which is caused by the misidentification of\nattributes, (c) multi-modal conflicting hallucination, which derives from the\ncontradictions between textual and visual information, and (d)\ncounter-common-sense hallucination, which owes to the contradictions between\nthe LVLM knowledge and actual images. Based on these taxonomies, we propose a\nmore challenging benchmark named PhD to evaluate and explore IVL-Hallu. An\nautomated pipeline is proposed for generating different types of IVL-Hallu\ndata. Extensive experiments on five SOTA LVLMs reveal their inability to\neffectively tackle our proposed IVL-Hallu tasks, with detailed analyses and\ninsights on the origins and possible solutions of these new challenging\nIVL-Hallu tasks, facilitating future researches on IVL-Hallu and LVLM. The\nbenchmark can be accessed at https://github.com/jiazhen-code/IntrinsicHallu",
        "pdf_link": "https://arxiv.org/pdf/2403.11116v1.pdf"
    },
    {
        "title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models",
        "authors": [
            "Yuzhao Heng",
            "Chunyuan Deng",
            "Yitong Li",
            "Yue Yu",
            "Yinghao Li",
            "Rongzhi Zhang",
            "Chao Zhang"
        ],
        "published": "2024-03-17T06:12:43Z",
        "summary": "Although Large Language Models (LLMs) exhibit remarkable adaptability across\ndomains, these models often fall short in structured knowledge extraction tasks\nsuch as named entity recognition (NER). This paper explores an innovative,\ncost-efficient strategy to harness LLMs with modest NER capabilities for\nproducing superior NER datasets. Our approach diverges from the basic\nclass-conditional prompts by instructing LLMs to self-reflect on the specific\ndomain, thereby generating domain-relevant attributes (such as category and\nemotions for movie reviews), which are utilized for creating attribute-rich\ntraining data. Furthermore, we preemptively generate entity terms and then\ndevelop NER context data around these entities, effectively bypassing the LLMs'\nchallenges with complex structures. Our experiments across both general and\nniche domains reveal significant performance enhancements over conventional\ndata generation methods while being more cost-effective than existing\nalternatives.",
        "pdf_link": "https://arxiv.org/pdf/2403.11103v1.pdf"
    },
    {
        "title": "m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks",
        "authors": [
            "Zixian Ma",
            "Weikai Huang",
            "Jieyu Zhang",
            "Tanmay Gupta",
            "Ranjay Krishna"
        ],
        "published": "2024-03-17T04:36:18Z",
        "summary": "Real-world multi-modal problems are rarely solved by a single machine\nlearning model, and often require multi-step computational plans that involve\nstitching several models. Tool-augmented LLMs hold tremendous promise for\nautomating the generation of such computational plans. However, the lack of\nstandardized benchmarks for evaluating LLMs as planners for multi-step\nmulti-modal tasks has prevented a systematic study of planner design decisions.\nShould LLMs generate a full plan in a single shot or step-by-step? Should they\ninvoke tools directly with Python code or through structured data formats like\nJSON? Does feedback improve planning? To answer these questions and more, we\nintroduce m&m's: a benchmark containing 4K+ multi-step multi-modal tasks\ninvolving 33 tools that include multi-modal models, (free) public APIs, and\nimage processing modules. For each of these task queries, we provide\nautomatically generated plans using this realistic toolset. We further provide\na high-quality subset of 1,565 task plans that are human-verified and correctly\nexecutable. With m&m's, we evaluate 6 popular LLMs with 2 planning strategies\n(multi-step vs. step-by-step planning), 2 plan formats (JSON vs. code), and 3\ntypes of feedback (parsing/verification/execution). Finally, we summarize\ntakeaways from our extensive experiments. Our dataset and code are available on\nHuggingFace (https://huggingface.co/datasets/zixianma/mnms) and Github\n(https://github.com/RAIVNLab/mnms).",
        "pdf_link": "https://arxiv.org/pdf/2403.11085v3.pdf"
    },
    {
        "title": "GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented Mental Alignment",
        "authors": [
            "Lance Ying",
            "Kunal Jha",
            "Shivam Aarya",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Tianmin Shu"
        ],
        "published": "2024-03-17T03:52:52Z",
        "summary": "Verbal communication plays a crucial role in human cooperation, particularly\nwhen the partners only have incomplete information about the task, environment,\nand each other's mental state. In this paper, we propose a novel cooperative\ncommunication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates\nverbal communication as a planning problem that minimizes the misalignment\nbetween the parts of agents' mental states that are relevant to the goals. This\napproach enables an embodied assistant to reason about when and how to\nproactively initialize communication with humans verbally using natural\nlanguage to help achieve better cooperation. We evaluate our approach against\nstrong baselines in two challenging environments, Overcooked (a multiplayer\ngame) and VirtualHome (a household simulator). Our experimental results\ndemonstrate that large language models struggle with generating meaningful\ncommunication that is grounded in the social and physical context. In contrast,\nour approach can successfully generate concise verbal communication for the\nembodied assistant to effectively boost the performance of the cooperation as\nwell as human users' perception of the assistant.",
        "pdf_link": "https://arxiv.org/pdf/2403.11075v1.pdf"
    },
    {
        "title": "Large Language Models Powered Context-aware Motion Prediction",
        "authors": [
            "Xiaoji Zheng",
            "Lixiu Wu",
            "Zhijie Yan",
            "Yuanrong Tang",
            "Hao Zhao",
            "Chen Zhong",
            "Bokui Chen",
            "Jiangtao Gong"
        ],
        "published": "2024-03-17T02:06:49Z",
        "summary": "Motion prediction is among the most fundamental tasks in autonomous driving.\nTraditional methods of motion forecasting primarily encode vector information\nof maps and historical trajectory data of traffic participants, lacking a\ncomprehensive understanding of overall traffic semantics, which in turn affects\nthe performance of prediction tasks. In this paper, we utilized Large Language\nModels (LLMs) to enhance the global traffic context understanding for motion\nprediction tasks. We first conducted systematic prompt engineering, visualizing\ncomplex traffic environments and historical trajectory information of traffic\nparticipants into image prompts -- Transportation Context Map (TC-Map),\naccompanied by corresponding text prompts. Through this approach, we obtained\nrich traffic context information from the LLM. By integrating this information\ninto the motion prediction model, we demonstrate that such context can enhance\nthe accuracy of motion predictions. Furthermore, considering the cost\nassociated with LLMs, we propose a cost-effective deployment strategy:\nenhancing the accuracy of motion prediction tasks at scale with 0.7\\%\nLLM-augmented datasets. Our research offers valuable insights into enhancing\nthe understanding of traffic scenes of LLMs and the motion prediction\nperformance of autonomous driving.",
        "pdf_link": "https://arxiv.org/pdf/2403.11057v1.pdf"
    },
    {
        "title": "Pre-Trained Language Models Represent Some Geographic Populations Better Than Others",
        "authors": [
            "Jonathan Dunn",
            "Benjamin Adams",
            "Harish Tayyar Madabushi"
        ],
        "published": "2024-03-16T22:01:39Z",
        "summary": "This paper measures the skew in how well two families of LLMs represent\ndiverse geographic populations. A spatial probing task is used with\ngeo-referenced corpora to measure the degree to which pre-trained language\nmodels from the OPT and BLOOM series represent diverse populations around the\nworld. Results show that these models perform much better for some populations\nthan others. In particular, populations across the US and the UK are\nrepresented quite well while those in South and Southeast Asia are poorly\nrepresented. Analysis shows that both families of models largely share the same\nskew across populations. At the same time, this skew cannot be fully explained\nby sociolinguistic factors, economic factors, or geographic factors. The basic\nconclusion from this analysis is that pre-trained models do not equally\nrepresent the world's population: there is a strong skew towards specific\ngeographic populations. This finding challenges the idea that a single model\ncan be used for all populations.",
        "pdf_link": "https://arxiv.org/pdf/2403.11025v1.pdf"
    },
    {
        "title": "SelfIE: Self-Interpretation of Large Language Model Embeddings",
        "authors": [
            "Haozhe Chen",
            "Carl Vondrick",
            "Chengzhi Mao"
        ],
        "published": "2024-03-16T15:30:34Z",
        "summary": "How do large language models (LLMs) obtain their answers? The ability to\nexplain and control an LLM's reasoning process is key for reliability,\ntransparency, and future model developments. We propose SelfIE\n(Self-Interpretation of Embeddings), a framework that enables LLMs to interpret\ntheir own embeddings in natural language by leveraging their ability to respond\nto inquiries about a given passage. Capable of interpreting open-world concepts\nin the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such\nas making ethical decisions, internalizing prompt injection, and recalling\nharmful knowledge. SelfIE's text descriptions on hidden embeddings also open up\nnew avenues to control LLM reasoning. We propose Supervised Control, which\nallows editing open-ended concepts while only requiring gradient computation of\nindividual layer. We extend RLHF to hidden embeddings and propose Reinforcement\nControl that erases harmful knowledge in LLM without supervision targets.",
        "pdf_link": "https://arxiv.org/pdf/2403.10949v2.pdf"
    },
    {
        "title": "Human Centered AI for Indian Legal Text Analytics",
        "authors": [
            "Sudipto Ghosh",
            "Devanshu Verma",
            "Balaji Ganesan",
            "Purnima Bindal",
            "Vikas Kumar",
            "Vasudha Bhatnagar"
        ],
        "published": "2024-03-16T15:17:13Z",
        "summary": "Legal research is a crucial task in the practice of law. It requires intense\nhuman effort and intellectual prudence to research a legal case and prepare\narguments. Recent boom in generative AI has not translated to proportionate\nrise in impactful legal applications, because of low trustworthiness and and\nthe scarcity of specialized datasets for training Large Language Models (LLMs).\nThis position paper explores the potential of LLMs within Legal Text Analytics\n(LTA), highlighting specific areas where the integration of human expertise can\nsignificantly enhance their performance to match that of experts. We introduce\na novel dataset and describe a human centered, compound AI system that\nprincipally incorporates human inputs for performing LTA tasks with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.10944v1.pdf"
    },
    {
        "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations",
        "authors": [
            "Hanlei Zhang",
            "Xin Wang",
            "Hua Xu",
            "Qianrui Zhou",
            "Kai Gao",
            "Jianhua Su",
            "jinyue Zhao",
            "Wenrui Li",
            "Yanting Chen"
        ],
        "published": "2024-03-16T15:14:15Z",
        "summary": "Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.",
        "pdf_link": "https://arxiv.org/pdf/2403.10943v2.pdf"
    },
    {
        "title": "Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization",
        "authors": [
            "Zihan Wang",
            "Jiayu Xiao",
            "Mengxiang Li",
            "Zhongjiang He",
            "Yongxiang Li",
            "Chao Wang",
            "Shuangyong Song"
        ],
        "published": "2024-03-16T11:09:27Z",
        "summary": "In our dynamic world where data arrives in a continuous stream, continual\nlearning enables us to incrementally add new tasks/domains without the need to\nretrain from scratch. A major challenge in continual learning of language model\nis catastrophic forgetting, the tendency of models to forget knowledge from\npreviously trained tasks/domains when training on new ones. This paper studies\ndialog generation under the continual learning setting. We propose a novel\nmethod that 1) uses \\textit{Text-Mixup} as data augmentation to avoid model\noverfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization\n(BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain\ntask-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset)\ndemonstrate that our proposed approach outperforms the state-of-the-art in\ncontinual learning.",
        "pdf_link": "https://arxiv.org/pdf/2403.10894v1.pdf"
    },
    {
        "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
        "authors": [
            "ChangSu Choi",
            "Yongbin Jeong",
            "Seoyoon Park",
            "InHo Won",
            "HyeonSeok Lim",
            "SangMin Kim",
            "Yejee Kang",
            "Chanhyuk Yoon",
            "Jaewan Park",
            "Yiseul Lee",
            "HyeJin Lee",
            "Younggyun Hahm",
            "Hansaem Kim",
            "KyungTae Lim"
        ],
        "published": "2024-03-16T10:26:38Z",
        "summary": "Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.",
        "pdf_link": "https://arxiv.org/pdf/2403.10882v2.pdf"
    },
    {
        "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
        "authors": [
            "Tianhe Wu",
            "Kede Ma",
            "Jie Liang",
            "Yujiu Yang",
            "Lei Zhang"
        ],
        "published": "2024-03-16T08:30:45Z",
        "summary": "While Multimodal Large Language Models (MLLMs) have experienced significant\nadvancement on visual understanding and reasoning, their potentials to serve as\npowerful, flexible, interpretable, and text-driven models for Image Quality\nAssessment (IQA) remains largely unexplored. In this paper, we conduct a\ncomprehensive and systematic study of prompting MLLMs for IQA. Specifically, we\nfirst investigate nine prompting systems for MLLMs as the combinations of three\nstandardized testing procedures in psychophysics (i.e., the single-stimulus,\ndouble-stimulus, and multiple-stimulus methods) and three popular prompting\nstrategies in natural language processing (i.e., the standard, in-context, and\nchain-of-thought prompting). We then present a difficult sample selection\nprocedure, taking into account sample diversity and uncertainty, to further\nchallenge MLLMs equipped with the respective optimal prompting systems. We\nassess three open-source and one close-source MLLMs on several visual\nattributes of image quality (e.g., structural and textural distortions, color\ndifferences, and geometric transformations) in both full-reference and\nno-reference scenarios. Experimental results show that only the close-source\nGPT-4V provides a reasonable account for human perception of image quality, but\nis weak at discriminating fine-grained quality variations (e.g., color\ndifferences) and at comparing visual quality of multiple images, tasks humans\ncan perform effortlessly.",
        "pdf_link": "https://arxiv.org/pdf/2403.10854v1.pdf"
    },
    {
        "title": "Two-step Automated Cybercrime Coded Word Detection using Multi-level Representation Learning",
        "authors": [
            "Yongyeon Kim",
            "Byung-Won On",
            "Ingyu Lee"
        ],
        "published": "2024-03-16T07:18:29Z",
        "summary": "In social network service platforms, crime suspects are likely to use\ncybercrime coded words for communication by adding criminal meanings to\nexisting words or replacing them with similar words. For instance, the word\n'ice' is often used to mean methamphetamine in drug crimes. To analyze the\nnature of cybercrime and the behavior of criminals, quickly detecting such\nwords and further understanding their meaning are critical. In the automated\ncybercrime coded word detection problem, it is difficult to collect a\nsufficient amount of training data for supervised learning and to directly\napply language models that utilize context information to better understand\nnatural language. To overcome these limitations, we propose a new two-step\napproach, in which a mean latent vector is constructed for each cybercrime\nthrough one of five different AutoEncoder models in the first step, and\ncybercrime coded words are detected based on multi-level latent representations\nin the second step. Moreover, to deeply understand cybercrime coded words\ndetected through the two-step approach, we propose three novel methods: (1)\nDetection of new words recently coined, (2) Detection of words frequently\nappeared in both drug and sex crimes, and (3) Automatic generation of word\ntaxonomy. According to our experimental results, among various AutoEncoder\nmodels, the stacked AutoEncoder model shows the best performance. Additionally,\nthe F1-score of the two-step approach is 0.991, which is higher than 0.987 and\n0.903 of the existing dark-GloVe and dark-BERT models. By analyzing the\nexperimental results of the three proposed methods, we can gain a deeper\nunderstanding of drug and sex crimes.",
        "pdf_link": "https://arxiv.org/pdf/2403.10838v1.pdf"
    },
    {
        "title": "Do Large Language Models understand Medical Codes?",
        "authors": [
            "Simon A. Lee",
            "Timothy Lindsey"
        ],
        "published": "2024-03-16T06:18:15Z",
        "summary": "The overarching goal of recent AI research has been to make steady progress\ntowards achieving Artificial General Intelligence (AGI), prompting the\nevaluation of Large Language Models (LLMs) across a variety of tasks and\ndomains. One such domain is healthcare, where LLMs can greatly benefit clinical\npractice by assisting with a wide range of tasks. However, these models are\nalso prone to producing ``hallucinations\" or incorrect responses when faced\nwith queries they cannot adequately address, raising concerns and skepticism,\nespecially within the healthcare community. In this work, we investigate\nwhether LLMs understand and can predict medical codes, which are extensively\nutilized in healthcare practice. This study aims to delineate the capabilities\nand limitations of these LLMs. We evaluate various off-the-shelf LLMs (e.g.,\nGPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to\nassess their awareness and understanding of these domain-specific\nterminologies. Our results indicate that these models as they currently stand\ndo not comprehend the meaning of the medical codes, highlighting the need for\nbetter representation of these alphanumeric codes extensively used in\nhealthcare. We call for improved strategies to effectively capture and\nrepresent the nuances of medical codes and terminologies within LLMs, enabling\nthem to become more reliable and trustworthy tools for healthcare\nprofessionals.",
        "pdf_link": "https://arxiv.org/pdf/2403.10822v2.pdf"
    },
    {
        "title": "Efficient Pruning of Large Language Model with Adaptive Estimation Fusion",
        "authors": [
            "Jun Liu",
            "Chao Wu",
            "Changdi Yang",
            "Hao Tang",
            "Haoye Dong",
            "Zhenglun Kong",
            "Geng Yuan",
            "Wei Niu",
            "Dong Huang",
            "Yanzhi Wang"
        ],
        "published": "2024-03-16T04:12:50Z",
        "summary": "Large language models (LLMs) have become crucial for many generative\ndownstream tasks, leading to an inevitable trend and significant challenge to\ndeploy them efficiently on resource-constrained devices. Structured pruning is\na widely used method to address this challenge. However, when dealing with the\ncomplex structure of the multiple decoder layers, general methods often employ\ncommon estimation approaches for pruning. These approaches lead to a decline in\naccuracy for specific downstream tasks. In this paper, we introduce a simple\nyet efficient method that adaptively models the importance of each\nsubstructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained\nestimations based on the results from complex and multilayer structures. All\naspects of our design seamlessly integrate into the endto-end pruning\nframework. Our experimental results, compared with state-of-the-art methods on\nmainstream datasets, demonstrate average accuracy improvements of 1.1%, 1.02%,\n2.0%, and 1.2% for LLaMa-7B,Vicuna-7B, Baichuan-7B, and Bloom-7b1,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.10799v1.pdf"
    },
    {
        "title": "From Words to Routes: Applying Large Language Models to Vehicle Routing",
        "authors": [
            "Zhehui Huang",
            "Guangyao Shi",
            "Gaurav S. Sukhatme"
        ],
        "published": "2024-03-16T03:54:38Z",
        "summary": "LLMs have shown impressive progress in robotics (e.g., manipulation and\nnavigation) with natural language task descriptions. The success of LLMs in\nthese tasks leads us to wonder: What is the ability of LLMs to solve vehicle\nrouting problems (VRPs) with natural language task descriptions? In this work,\nwe study this question in three steps. First, we construct a dataset with 21\ntypes of single- or multi-vehicle routing problems. Second, we evaluate the\nperformance of LLMs across four basic prompt paradigms of text-to-code\ngeneration, each involving different types of text input. We find that the\nbasic prompt paradigm, which generates code directly from natural language task\ndescriptions, performs the best for GPT-4, achieving 56% feasibility, 40%\noptimality, and 53% efficiency. Third, based on the observation that LLMs may\nnot be able to provide correct solutions at the initial attempt, we propose a\nframework that enables LLMs to refine solutions through self-reflection,\nincluding self-debugging and self-verification. With GPT-4, our proposed\nframework achieves a 16% increase in feasibility, a 7% increase in optimality,\nand a 15% increase in efficiency. Moreover, we examine the sensitivity of GPT-4\nto task descriptions, specifically focusing on how its performance changes when\ncertain details are omitted from the task descriptions, yet the core meaning is\npreserved. Our findings reveal that such omissions lead to a notable decrease\nin performance: 4% in feasibility, 4% in optimality, and 5% in efficiency.\nWebsite: https://sites.google.com/view/words-to-routes/",
        "pdf_link": "https://arxiv.org/pdf/2403.10795v1.pdf"
    },
    {
        "title": "LLM-based Conversational AI Therapist for Daily Functioning Screening and Psychotherapeutic Intervention via Everyday Smart Devices",
        "authors": [
            "Jingping Nie",
            "Hanya Shao",
            "Yuang Fan",
            "Qijia Shao",
            "Haoxuan You",
            "Matthias Preindl",
            "Xiaofan Jiang"
        ],
        "published": "2024-03-16T02:48:50Z",
        "summary": "Despite the global mental health crisis, access to screenings, professionals,\nand treatments remains high. In collaboration with licensed psychotherapists,\nwe propose a Conversational AI Therapist with psychotherapeutic Interventions\n(CaiTI), a platform that leverages large language models (LLM)s and smart\ndevices to enable better mental health self-care. CaiTI can screen the\nday-to-day functioning using natural and psychotherapeutic conversations. CaiTI\nleverages reinforcement learning to provide personalized conversation flow.\nCaiTI can accurately understand and interpret user responses. When the user\nneeds further attention during the conversation, CaiTI can provide\nconversational psychotherapeutic interventions, including cognitive behavioral\ntherapy (CBT) and motivational interviewing (MI). Leveraging the datasets\nprepared by the licensed psychotherapists, we experiment and microbenchmark\nvarious LLMs' performance in tasks along CaiTI's conversation flow and discuss\ntheir strengths and weaknesses. With the psychotherapists, we implement CaiTI\nand conduct 14-day and 24-week studies. The study results, validated by\ntherapists, demonstrate that CaiTI can converse with users naturally,\naccurately understand and interpret user responses, and provide\npsychotherapeutic interventions appropriately and effectively. We showcase the\npotential of CaiTI LLMs to assist the mental therapy diagnosis and treatment\nand improve day-to-day functioning screening and precautionary\npsychotherapeutic intervention systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.10779v1.pdf"
    },
    {
        "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
        "authors": [
            "J. K. Lee",
            "T. M. Chung"
        ],
        "published": "2024-03-16T02:27:19Z",
        "summary": "The rapid advancement of large language models (LLMs) has enabled natural\nlanguage processing capabilities similar to those of humans, and LLMs are being\nwidely utilized across various societal domains such as education and\nhealthcare. While the versatility of these models has increased, they have the\npotential to generate subjective and normative language, leading to\ndiscriminatory treatment or outcomes among social groups, especially due to\nonline offensive language. In this paper, we define such harm as societal bias\nand assess ethnic, gender, and racial biases in a model fine-tuned with Korean\ncomments using Bidirectional Encoder Representations from Transformers (KcBERT)\nand KOLD data through template-based Masked Language Modeling (MLM). To\nquantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to\nKcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates\nsignificant changes in gender and racial biases. Based on these results, we\npropose two methods to mitigate societal bias. Firstly, a data balancing\napproach during the pre-training phase adjusts the uniformity of data by\naligning the distribution of the occurrences of specific words and converting\nsurrounding harmful words into non-harmful words. Secondly, during the\nin-training phase, we apply Debiasing Regularization by adjusting dropout and\nregularization, confirming a decrease in training loss. Our contribution lies\nin demonstrating that societal bias exists in Korean language models due to\nlanguage-dependent characteristics.",
        "pdf_link": "https://arxiv.org/pdf/2403.10774v1.pdf"
    },
    {
        "title": "Depression Detection on Social Media with Large Language Models",
        "authors": [
            "Xiaochong Lan",
            "Yiming Cheng",
            "Li Sheng",
            "Chen Gao",
            "Yong Li"
        ],
        "published": "2024-03-16T01:01:16Z",
        "summary": "Depression harms. However, due to a lack of mental health awareness and fear\nof stigma, many patients do not actively seek diagnosis and treatment, leading\nto detrimental outcomes. Depression detection aims to determine whether an\nindividual suffers from depression by analyzing their history of posts on\nsocial media, which can significantly aid in early detection and intervention.\nIt mainly faces two key challenges: 1) it requires professional medical\nknowledge, and 2) it necessitates both high accuracy and explainability. To\naddress it, we propose a novel depression detection system called DORIS,\ncombining medical knowledge and the recent advances in large language models\n(LLMs). Specifically, to tackle the first challenge, we proposed an LLM-based\nsolution to first annotate whether high-risk texts meet medical diagnostic\ncriteria. Further, we retrieve texts with high emotional intensity and\nsummarize critical information from the historical mood records of users,\nso-called mood courses. To tackle the second challenge, we combine LLM and\ntraditional classifiers to integrate medical knowledge-guided features, for\nwhich the model can also explain its prediction results, achieving both high\naccuracy and explainability. Extensive experimental results on benchmarking\ndatasets show that, compared to the current best baseline, our approach\nimproves by 0.036 in AUPRC, which can be considered significant, demonstrating\nthe effectiveness of our approach and its high value as an NLP application.",
        "pdf_link": "https://arxiv.org/pdf/2403.10750v1.pdf"
    },
    {
        "title": "Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns",
        "authors": [
            "Tunazzina Islam",
            "Dan Goldwasser"
        ],
        "published": "2024-03-15T21:54:00Z",
        "summary": "This paper introduces a novel approach to uncovering and analyzing themes in\nsocial media messaging. Recognizing the limitations of traditional topic-level\nanalysis, which tends to capture only the overarching patterns, this study\nemphasizes the need for a finer-grained, theme-focused exploration.\nConventional methods of theme discovery, involving manual processes and a\nhuman-in-the-loop approach, are valuable but face challenges in scalability,\nconsistency, and resource intensity in terms of time and cost. To address these\nchallenges, we propose a machine-in-the-loop approach that leverages the\nadvanced capabilities of Large Language Models (LLMs). This approach allows for\na deeper investigation into the thematic aspects of social media discourse,\nenabling us to uncover a diverse array of themes, each with unique\ncharacteristics and relevance, thereby offering a comprehensive understanding\nof the nuances present within broader topics. Furthermore, this method\nefficiently maps the text and the newly discovered themes, enhancing our\nunderstanding of the thematic nuances in social media messaging. We employ\nclimate campaigns as a case study and demonstrate that our methodology yields\nmore accurate and interpretable results compared to traditional topic models.\nOur results not only demonstrate the effectiveness of our approach in\nuncovering latent themes but also illuminate how these themes are tailored for\ndemographic targeting in social media contexts. Additionally, our work sheds\nlight on the dynamic nature of social media, revealing the shifts in the\nthematic focus of messaging in response to real-world events.",
        "pdf_link": "https://arxiv.org/pdf/2403.10707v1.pdf"
    },
    {
        "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
        "authors": [
            "Hakim Sidahmed",
            "Samrat Phatale",
            "Alex Hutcheson",
            "Zhuonan Lin",
            "Zhang Chen",
            "Zac Yu",
            "Jarvis Jin",
            "Roman Komarytsia",
            "Christiane Ahlheim",
            "Yonghao Zhu",
            "Simral Chaudhary",
            "Bowen Li",
            "Saravanan Ganesh",
            "Bill Byrne",
            "Jessica Hoffmann",
            "Hassan Mansoor",
            "Wei Li",
            "Abhinav Rastogi",
            "Lucas Dixon"
        ],
        "published": "2024-03-15T21:43:46Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong\nmethod to align Pretrained Large Language Models (LLMs) with human preferences.\nBut training models with RLHF is computationally expensive, and an overall\ncomplex process. In this work, we study RLHF where the underlying models are\ntrained using the parameter efficient method of Low-Rank Adaptation (LoRA)\nintroduced by Hu et al. [2021]. We investigate the setup of \"Parameter\nEfficient Reinforcement Learning\" (PERL), in which we perform reward model\ntraining and reinforcement learning using LoRA. We compare PERL to conventional\nfine-tuning (full-tuning) across various configurations for 7 benchmarks,\nincluding 2 novel datasets, of reward modeling and reinforcement learning. We\nfind that PERL performs on par with the conventional RLHF setting, while\ntraining faster, and with less memory. This enables the high performance of\nRLHF, while reducing the computational burden that limits its adoption as an\nalignment technique for Large Language Models. We also release 2 novel thumbs\nup/down preference datasets: \"Taskmaster Coffee\", and \"Taskmaster Ticketing\" to\npromote research around RLHF.",
        "pdf_link": "https://arxiv.org/pdf/2403.10704v1.pdf"
    },
    {
        "title": "Apriori Knowledge in an Era of Computational Opacity: The Role of AI in Mathematical Discovery",
        "authors": [
            "Eamon Duede",
            "Kevin Davey"
        ],
        "published": "2024-03-15T21:38:26Z",
        "summary": "Computation is central to contemporary mathematics. Many accept that we can\nacquire genuine mathematical knowledge of the Four Color Theorem from Appel and\nHaken's program insofar as it is simply a repetitive application of human forms\nof mathematical reasoning. Modern LLMs / DNNs are, by contrast, opaque to us in\nsignificant ways, and this creates obstacles in obtaining mathematical\nknowledge from them. We argue, however, that if a proof-checker automating\nhuman forms of proof-checking is attached to such machines, then we can obtain\napriori mathematical knowledge from them, even though the original machines are\nentirely opaque to us and the proofs they output are not human-surveyable.",
        "pdf_link": "https://arxiv.org/pdf/2403.15437v1.pdf"
    },
    {
        "title": "Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems",
        "authors": [
            "Antonios Alexos",
            "Yu-Dai Tsai",
            "Ian Domingo",
            "Maryam Pishgar",
            "Pierre Baldi"
        ],
        "published": "2024-03-15T18:00:00Z",
        "summary": "Creating controlled methods to simulate neurodegeneration in artificial\nintelligence (AI) is crucial for applications that emulate brain function\ndecline and cognitive disorders. We use IQ tests performed by Large Language\nModels (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of\n``neural erosion.\" This deliberate erosion involves ablating synapses or\nneurons, or adding Gaussian noise during or after training, resulting in a\ncontrolled progressive decline in the LLMs' performance. We are able to\ndescribe the neurodegeneration in the IQ tests and show that the LLM first\nloses its mathematical abilities and then its linguistic abilities, while\nfurther losing its ability to understand the questions. To the best of our\nknowledge, this is the first work that models neurodegeneration with text data,\ncompared to other works that operate in the computer vision domain. Finally, we\ndraw similarities between our study and cognitive decline clinical studies\ninvolving test subjects. We find that with the application of neurodegenerative\nmethods, LLMs lose abstract thinking abilities, followed by mathematical\ndegradation, and ultimately, a loss in linguistic ability, responding to\nprompts incoherently. These findings are in accordance with human studies.",
        "pdf_link": "https://arxiv.org/pdf/2403.10596v1.pdf"
    },
    {
        "title": "Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?",
        "authors": [
            "Bruno de Melo",
            "Jamiel Sheikh"
        ],
        "published": "2024-03-15T17:12:57Z",
        "summary": "Performance attribution analysis, defined as the process of explaining the\ndrivers of the excess performance of an investment portfolio against a\nbenchmark, stands as a significant feature of portfolio management and plays a\ncrucial role in the investment decision-making process, particularly within the\nfund management industry. Rooted in a solid financial and mathematical\nframework, the importance and methodologies of this analytical technique are\nextensively documented across numerous academic research papers and books. The\nintegration of large language models (LLMs) and AI agents marks a\ngroundbreaking development in this field. These agents are designed to automate\nand enhance the performance attribution analysis by accurately calculating and\nanalyzing portfolio performances against benchmarks. In this study, we\nintroduce the application of an AI Agent for a variety of essential performance\nattribution tasks, including the analysis of performance drivers and utilizing\nLLMs as calculation engine for multi-level attribution analysis and\nquestion-answering (QA) tasks. Leveraging advanced prompt engineering\ntechniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and\nemploying a standard agent framework from LangChain, the research achieves\npromising results: it achieves accuracy rates exceeding 93% in analyzing\nperformance drivers, attains 100% in multi-level attribution calculations, and\nsurpasses 84% accuracy in QA exercises that simulate official examination\nstandards. These findings affirm the impactful role of AI agents, prompt\nengineering and evaluation in advancing portfolio management processes,\nhighlighting a significant development in the practical application and\nevaluation of Generative AI technologies within the domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.10482v2.pdf"
    },
    {
        "title": "S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document",
        "authors": [
            "Kareem Shaik",
            "Dali Wang",
            "Weijian Zheng",
            "Qinglei Cao",
            "Heng Fan",
            "Peter Schwartz",
            "Yunhe Feng"
        ],
        "published": "2024-03-15T17:04:27Z",
        "summary": "The understanding of large-scale scientific software poses significant\nchallenges due to its diverse codebase, extensive code length, and target\ncomputing architectures. The emergence of generative AI, specifically large\nlanguage models (LLMs), provides novel pathways for understanding such complex\nscientific codes. This paper presents S3LLM, an LLM-based framework designed to\nenable the examination of source code, code metadata, and summarized\ninformation in conjunction with textual technical reports in an interactive,\nconversational manner through a user-friendly interface. S3LLM leverages\nopen-source LLaMA-2 models to enhance code analysis through the automatic\ntransformation of natural language queries into domain-specific language (DSL)\nqueries. Specifically, it translates these queries into Feature Query Language\n(FQL), enabling efficient scanning and parsing of entire code repositories. In\naddition, S3LLM is equipped to handle diverse metadata types, including DOT,\nSQL, and customized formats. Furthermore, S3LLM incorporates retrieval\naugmented generation (RAG) and LangChain technologies to directly query\nextensive documents. S3LLM demonstrates the potential of using locally deployed\nopen-source LLMs for the rapid understanding of large-scale scientific\ncomputing software, eliminating the need for extensive coding expertise, and\nthereby making the process more efficient and effective. S3LLM is available at\nhttps://github.com/ResponsibleAILab/s3llm.",
        "pdf_link": "https://arxiv.org/pdf/2403.10588v1.pdf"
    },
    {
        "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
        "authors": [
            "Jiarui Li",
            "Ye Yuan",
            "Zehua Zhang"
        ],
        "published": "2024-03-15T16:30:14Z",
        "summary": "We proposed an end-to-end system design towards utilizing Retrieval Augmented\nGeneration (RAG) to improve the factual accuracy of Large Language Models\n(LLMs) for domain-specific and time-sensitive queries related to private\nknowledge-bases. Our system integrates RAG pipeline with upstream datasets\nprocessing and downstream performance evaluation. Addressing the challenge of\nLLM hallucinations, we finetune models with a curated dataset which originates\nfrom CMU's extensive resources and annotated with the teacher model. Our\nexperiments demonstrate the system's effectiveness in generating more accurate\nanswers to domain-specific and time-sensitive inquiries. The results also\nrevealed the limitations of fine-tuning LLMs with small-scale and skewed\ndatasets. This research highlights the potential of RAG systems in augmenting\nLLMs with external datasets for improved performance in knowledge-intensive\ntasks. Our code and models are available on Github.",
        "pdf_link": "https://arxiv.org/pdf/2403.10446v1.pdf"
    },
    {
        "title": "Using an LLM to Turn Sign Spottings into Spoken Language Sentences",
        "authors": [
            "Ozge Mercanoglu Sincan",
            "Necati Cihan Camgoz",
            "Richard Bowden"
        ],
        "published": "2024-03-15T16:14:34Z",
        "summary": "Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and\na pretrained large language model to improve SLT performance. Our method builds\nupon the strengths of both components. The videos are first processed by the\nspotter, which is trained on a linguistic sign language dataset, to identify\nindividual signs. These spotted signs are then passed to the powerful language\nmodel, which transforms them into coherent and contextually appropriate spoken\nlanguage sentences.",
        "pdf_link": "https://arxiv.org/pdf/2403.10434v1.pdf"
    },
    {
        "title": "SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores",
        "authors": [
            "Vidminas Vizgirda",
            "Rui Zhao",
            "Naman Goel"
        ],
        "published": "2024-03-15T15:43:02Z",
        "summary": "We present SocialGenPod, a decentralised and privacy-friendly way of\ndeploying generative AI Web applications. Unlike centralised Web and data\narchitectures that keep user data tied to application and service providers, we\nshow how one can use Solid -- a decentralised Web specification -- to decouple\nuser data from generative AI applications. We demonstrate SocialGenPod using a\nprototype that allows users to converse with different Large Language Models,\noptionally leveraging Retrieval Augmented Generation to generate answers\ngrounded in private documents stored in any Solid Pod that the user is allowed\nto access, directly or indirectly. SocialGenPod makes use of Solid access\ncontrol mechanisms to give users full control of determining who has access to\ndata stored in their Pods. SocialGenPod keeps all user data (chat history, app\nconfiguration, personal documents, etc) securely in the user's personal Pod;\nseparate from specific model or application providers. Besides better privacy\ncontrols, this approach also enables portability across different services and\napplications. Finally, we discuss challenges, posed by the large compute\nrequirements of state-of-the-art models, that future research in this area\nshould address. Our prototype is open-source and available at:\nhttps://github.com/Vidminas/socialgenpod/.",
        "pdf_link": "https://arxiv.org/pdf/2403.10408v1.pdf"
    },
    {
        "title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale",
        "authors": [
            "Pengcheng Jiang",
            "Cao Xiao",
            "Zifeng Wang",
            "Parminder Bhatia",
            "Jimeng Sun",
            "Jiawei Han"
        ],
        "published": "2024-03-15T14:36:38Z",
        "summary": "The advent of large language models (LLMs) has significantly advanced natural\nlanguage processing tasks like text summarization. However, their large size\nand computational demands, coupled with privacy concerns in data transmission,\nlimit their use in resource-constrained and privacy-centric settings. To\novercome this, we introduce TriSum, a framework for distilling LLMs' text\nsummarization abilities into a compact, local model. Initially, LLMs extract a\nset of aspect-triple rationales and summaries, which are refined using a\ndual-scoring method for quality. Next, a smaller local model is trained with\nthese tasks, employing a curriculum learning strategy that evolves from simple\nto complex tasks. Our method enhances local model performance on various\nbenchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by\n4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by\nproviding insights into the summarization rationale.",
        "pdf_link": "https://arxiv.org/pdf/2403.10351v1.pdf"
    },
    {
        "title": "CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model",
        "authors": [
            "Shang-Hsuan Chiang",
            "Ssu-Cheng Wang",
            "Yao-Chung Fan"
        ],
        "published": "2024-03-15T14:14:26Z",
        "summary": "Manually designing cloze test consumes enormous time and efforts. The major\nchallenge lies in wrong option (distractor) selection. Having carefully-design\ndistractors improves the effectiveness of learner ability assessment. As a\nresult, the idea of automatically generating cloze distractor is motivated. In\nthis paper, we investigate cloze distractor generation by exploring the\nemployment of pre-trained language models (PLMs) as an alternative for\ncandidate distractor generation. Experiments show that the PLM-enhanced model\nbrings a substantial performance improvement. Our best performing model\nadvances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our\ncode and dataset is available at https://github.com/AndyChiangSH/CDGP.",
        "pdf_link": "https://arxiv.org/pdf/2403.10326v1.pdf"
    },
    {
        "title": "Uni-SMART: Universal Science Multimodal Analysis and Research Transformer",
        "authors": [
            "Hengxing Cai",
            "Xiaochen Cai",
            "Shuwen Yang",
            "Jiankun Wang",
            "Lin Yao",
            "Zhifeng Gao",
            "Junhan Chang",
            "Sihang Li",
            "Mingjun Xu",
            "Changxin Wang",
            "Hongshuai Wang",
            "Yongge Li",
            "Mujie Lin",
            "Yaqi Li",
            "Yuqi Yin",
            "Linfeng Zhang",
            "Guolin Ke"
        ],
        "published": "2024-03-15T13:43:47Z",
        "summary": "In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as molecular\nstructure, tables, and charts, which are hard for text-focused LLMs to\nunderstand and analyze. This issue points to the urgent need for new solutions\nthat can fully understand and analyze multimodal content in scientific\nliterature. To answer this demand, we present Uni-SMART (Universal Science\nMultimodal Analysis and Research Transformer), an innovative model designed for\nin-depth understanding of multimodal scientific literature. Through rigorous\nquantitative evaluation across several domains, Uni-SMART demonstrates superior\nperformance over leading text-focused LLMs. Furthermore, our exploration\nextends to practical applications, including patent infringement detection and\nnuanced analysis of charts. These applications not only highlight Uni-SMART's\nadaptability but also its potential to revolutionize how we interact with\nscientific literature.",
        "pdf_link": "https://arxiv.org/pdf/2403.10301v1.pdf"
    },
    {
        "title": "Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction",
        "authors": [
            "Chen Chen",
            "Lei Li",
            "Marcel Beetz",
            "Abhirup Banerjee",
            "Ramneek Gupta",
            "Vicente Grau"
        ],
        "published": "2024-03-15T13:25:09Z",
        "summary": "Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.",
        "pdf_link": "https://arxiv.org/pdf/2403.10581v2.pdf"
    },
    {
        "title": "A Question on the Explainability of Large Language Models and the Word-Level Univariate First-Order Plausibility Assumption",
        "authors": [
            "Jeremie Bogaert",
            "Francois-Xavier Standaert"
        ],
        "published": "2024-03-15T13:15:23Z",
        "summary": "The explanations of large language models have recently been shown to be\nsensitive to the randomness used for their training, creating a need to\ncharacterize this sensitivity. In this paper, we propose a characterization\nthat questions the possibility to provide simple and informative explanations\nfor such models. To this end, we give statistical definitions for the\nexplanations' signal, noise and signal-to-noise ratio. We highlight that, in a\ntypical case study where word-level univariate explanations are analyzed with\nfirst-order statistical tools, the explanations of simple feature-based models\ncarry more signal and less noise than those of transformer ones. We then\ndiscuss the possibility to improve these results with alternative definitions\nof signal and noise that would capture more complex explanations and analysis\nmethods, while also questioning the tradeoff with their plausibility for\nreaders.",
        "pdf_link": "https://arxiv.org/pdf/2403.10275v1.pdf"
    },
    {
        "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models",
        "authors": [
            "Chaoqun Liu",
            "Wenxuan Zhang",
            "Yiran Zhao",
            "Anh Tuan Luu",
            "Lidong Bing"
        ],
        "published": "2024-03-15T12:47:39Z",
        "summary": "Large language models (LLMs) have demonstrated strong multilingual\ncapabilities; yet, they are mostly English-centric due to the imbalanced\ntraining corpora. Existing works leverage this phenomenon to improve their\nmultilingual performances on NLP tasks. In this work, we extend the evaluation\nfrom NLP tasks to real user queries. We find that even though translation into\nEnglish can help improve the performance of multilingual NLP tasks for\nEnglish-centric LLMs, it may not be optimal for all scenarios. For\nculture-related tasks that need deep language understanding, prompting in the\nnative language proves to be more promising since it can capture the nuances\nrelated to culture and language. Therefore, we advocate for more efforts\ntowards the development of strong multilingual LLMs instead of just\nEnglish-centric LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.10258v1.pdf"
    },
    {
        "title": "HawkEye: Training Video-Text LLMs for Grounding Text in Videos",
        "authors": [
            "Yueqian Wang",
            "Xiaojun Meng",
            "Jianxin Liang",
            "Yuxuan Wang",
            "Qun Liu",
            "Dongyan Zhao"
        ],
        "published": "2024-03-15T11:58:18Z",
        "summary": "Video-text Large Language Models (video-text LLMs) have shown remarkable\nperformance in answering questions and holding conversations on simple videos.\nHowever, they perform almost the same as random on grounding text queries in\nlong and complicated videos, having little ability to understand and reason\nabout temporal information, which is the most fundamental difference between\nvideos and images. In this paper, we propose HawkEye, one of the first\nvideo-text LLMs that can perform temporal video grounding in a fully\ntext-to-text manner. To collect training data that is applicable for temporal\nvideo grounding, we construct InternVid-G, a large-scale video-text corpus with\nsegment-level captions and negative spans, with which we introduce two new\ntime-aware training objectives to video-text LLMs. We also propose a\ncoarse-grained method of representing segments in videos, which is more robust\nand easier for LLMs to learn and follow than other alternatives. Extensive\nexperiments show that HawkEye is better at temporal video grounding and\ncomparable on other video-text tasks with existing video-text LLMs, which\nverifies its superior video-text multi-modal understanding abilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.10228v1.pdf"
    },
    {
        "title": "Read between the lines -- Functionality Extraction From READMEs",
        "authors": [
            "Prince Kumar",
            "Srikanth Tamilselvam",
            "Dinesh Garg"
        ],
        "published": "2024-03-15T11:11:57Z",
        "summary": "While text summarization is a well-known NLP task, in this paper, we\nintroduce a novel and useful variant of it called functionality extraction from\nGit README files. Though this task is a text2text generation at an abstract\nlevel, it involves its own peculiarities and challenges making existing\ntext2text generation systems not very useful. The motivation behind this task\nstems from a recent surge in research and development activities around the use\nof large language models for code-related tasks, such as code refactoring, code\nsummarization, etc. We also release a human-annotated dataset called FuncRead,\nand develop a battery of models for the task. Our exhaustive experimentation\nshows that small size fine-tuned models beat any baseline models that can be\ndesigned using popular black-box or white-box large language models (LLMs) such\nas ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70%\nand 20% gain on the F1 score against ChatGPT and Bard respectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.10205v1.pdf"
    },
    {
        "title": "Generative Region-Language Pretraining for Open-Ended Object Detection",
        "authors": [
            "Chuang Lin",
            "Yi Jiang",
            "Lizhen Qu",
            "Zehuan Yuan",
            "Jianfei Cai"
        ],
        "published": "2024-03-15T10:52:39Z",
        "summary": "In recent research, significant attention has been devoted to the\nopen-vocabulary object detection task, aiming to generalize beyond the limited\nnumber of classes labeled during training and detect objects described by\narbitrary category names at inference. Compared with conventional object\ndetection, open vocabulary object detection largely extends the object\ndetection categories. However, it relies on calculating the similarity between\nimage regions and a set of arbitrary category names with a pretrained\nvision-and-language model. This implies that, despite its open-set nature, the\ntask still needs the predefined object categories during the inference stage.\nThis raises the question: What if we do not have exact knowledge of object\ncategories during inference? In this paper, we call such a new setting as\ngenerative open-ended object detection, which is a more general and practical\nproblem. To address it, we formulate object detection as a generative problem\nand propose a simple framework named GenerateU, which can detect dense objects\nand generate their names in a free-form way. Particularly, we employ Deformable\nDETR as a region proposal generator with a language model translating visual\nregions to object names. To assess the free-form object detection task, we\nintroduce an evaluation method designed to quantitatively measure the\nperformance of generative outcomes. Extensive experiments demonstrate strong\nzero-shot detection performance of our GenerateU. For example, on the LVIS\ndataset, our GenerateU achieves comparable results to the open-vocabulary\nobject detection method GLIP, even though the category names are not seen by\nGenerateU during inference. Code is available at: https://\ngithub.com/FoundationVision/GenerateU .",
        "pdf_link": "https://arxiv.org/pdf/2403.10191v1.pdf"
    },
    {
        "title": "ChatPattern: Layout Pattern Customization via Natural Language",
        "authors": [
            "Zixiao Wang",
            "Yunheng Shen",
            "Xufeng Yao",
            "Wenqian Zhao",
            "Yang Bai",
            "Farzan Farnia",
            "Bei Yu"
        ],
        "published": "2024-03-15T09:15:22Z",
        "summary": "Existing works focus on fixed-size layout pattern generation, while the more\npractical free-size pattern generation receives limited attention. In this\npaper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered\nframework for flexible pattern customization. ChatPattern utilizes a two-part\nsystem featuring an expert LLM agent and a highly controllable layout pattern\ngenerator. The LLM agent can interpret natural language requirements and\noperate design tools to meet specified needs, while the generator excels in\nconditional layout generation, pattern modification, and memory-friendly\npatterns extension. Experiments on challenging pattern generation setting shows\nthe ability of ChatPattern to synthesize high-quality large-scale patterns.",
        "pdf_link": "https://arxiv.org/pdf/2403.15434v1.pdf"
    },
    {
        "title": "Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning",
        "authors": [
            "Hang Zhang",
            "Wenxiao Zhang",
            "Haoxuan Qu",
            "Jun Liu"
        ],
        "published": "2024-03-15T08:51:15Z",
        "summary": "Human-centered dynamic scene understanding plays a pivotal role in enhancing\nthe capability of robotic and autonomous systems, in which Video-based\nHuman-Object Interaction (V-HOI) detection is a crucial task in semantic scene\nunderstanding, aimed at comprehensively understanding HOI relationships within\na video to benefit the behavioral decisions of mobile robots and autonomous\ndriving systems. Although previous V-HOI detection models have made significant\nstrides in accurate detection on specific datasets, they still lack the general\nreasoning ability like human beings to effectively induce HOI relationships. In\nthis study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a\nnovel framework consisting of a series of plug-and-play modules that could\nfacilitate the performance of current V-HOI detection models by leveraging the\nstrong reasoning ability of different off-the-shelf pre-trained large language\nmodels (LLMs). We design a two-stage collaboration system of different LLMs for\nthe V-HOI task. Specifically, in the first stage, we design a Cross-Agents\nReasoning scheme to leverage the LLM conduct reasoning from different aspects.\nIn the second stage, we perform Multi-LLMs Debate to get the final reasoning\nanswer based on the different knowledge in different LLMs. Additionally, we\ndevise an auxiliary training strategy that utilizes CLIP, a large\nvision-language model to enhance the base V-HOI models' discriminative ability\nto better cooperate with LLMs. We validate the superiority of our design by\ndemonstrating its effectiveness in improving the prediction accuracy of the\nbase V-HOI model via reasoning from multiple perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2403.10107v1.pdf"
    },
    {
        "title": "Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF",
        "authors": [
            "Amey Hengle",
            "Aswini Kumar",
            "Sahajpreet Singh",
            "Anil Bandhakavi",
            "Md Shad Akhtar",
            "Tanmoy Chakroborty"
        ],
        "published": "2024-03-15T08:03:49Z",
        "summary": "Counterspeech, defined as a response to mitigate online hate speech, is\nincreasingly used as a non-censorial solution. Addressing hate speech\neffectively involves dispelling the stereotypes, prejudices, and biases often\nsubtly implied in brief, single-sentence statements or abuses. These implicit\nexpressions challenge language models, especially in seq2seq tasks, as model\nperformance typically excels with longer contexts. Our study introduces CoARL,\na novel framework enhancing counterspeech generation by modeling the pragmatic\nimplications underlying social biases in hateful statements. CoARL's first two\nphases involve sequential multi-instruction tuning, teaching the model to\nunderstand intents, reactions, and harms of offensive statements, and then\nlearning task-specific low-rank adapter weights for generating\nintent-conditioned counterspeech. The final phase uses reinforcement learning\nto fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms\nexisting benchmarks in intent-conditioned counterspeech generation, showing an\naverage improvement of 3 points in intent-conformity and 4 points in\nargument-quality metrics. Extensive human evaluation supports CoARL's efficacy\nin generating superior and more context-appropriate responses compared to\nexisting systems, including prominent LLMs like ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2403.10088v1.pdf"
    },
    {
        "title": "Large Language Models to Generate System-Level Test Programs Targeting Non-functional Properties",
        "authors": [
            "Denis Schwachhofer",
            "Peter Domanski",
            "Steffen Becker",
            "Stefan Wagner",
            "Matthias Sauer",
            "Dirk Pfl\u00fcger",
            "Ilia Polian"
        ],
        "published": "2024-03-15T08:01:02Z",
        "summary": "System-Level Test (SLT) has been a part of the test flow for integrated\ncircuits for over a decade and still gains importance. However, no systematic\napproaches exist for test program generation, especially targeting\nnon-functional properties of the Device under Test (DUT). Currently, test\nengineers manually compose test suites from off-the-shelf software,\napproximating the end-user environment of the DUT. This is a challenging and\ntedious task that does not guarantee sufficient control over non-functional\nproperties. This paper proposes Large Language Models (LLMs) to generate test\nprograms. We take a first glance at how pre-trained LLMs perform in test\nprogram generation to optimize non-functional properties of the DUT. Therefore,\nwe write a prompt to generate C code snippets that maximize the instructions\nper cycle of a super-scalar, out-of-order architecture in simulation.\nAdditionally, we apply prompt and hyperparameter optimization to achieve the\nbest possible results without further training.",
        "pdf_link": "https://arxiv.org/pdf/2403.10086v2.pdf"
    },
    {
        "title": "CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner",
        "authors": [
            "Tingbing Yan",
            "Wenzheng Zeng",
            "Yang Xiao",
            "Xingyu Tong",
            "Bo Tan",
            "Zhiwen Fang",
            "Zhiguo Cao",
            "Joey Tianyi Zhou"
        ],
        "published": "2024-03-15T07:51:35Z",
        "summary": "Most existing one-shot skeleton-based action recognition focuses on raw\nlow-level information (e.g., joint location), and may suffer from local\ninformation loss and low generalization ability. To alleviate these, we propose\nto leverage text description generated from large language models (LLM) that\ncontain high-level human knowledge, to guide feature learning, in a\nglobal-local-global way. Particularly, during training, we design $2$ prompts\nto gain global and local text descriptions of each action from an LLM. We first\nutilize the global text description to guide the skeleton encoder focus on\ninformative joints (i.e.,global-to-local). Then we build non-local interaction\nbetween local text and joint features, to form the final global representation\n(i.e., local-to-global). To mitigate the asymmetry issue between the training\nand inference phases, we further design a dual-branch architecture that allows\nthe model to perform novel class inference without any text input, also making\nthe additional inference cost neglectable compared with the base skeleton\nencoder. Extensive experiments on three different benchmarks show that CrossGLG\nconsistently outperforms the existing SOTA methods with large margins, and the\ninference cost (model size) is only $2.8$\\% than the previous SOTA. CrossGLG\ncan also serve as a plug-and-play module that can substantially enhance the\nperformance of different SOTA skeleton encoders with a neglectable cost during\ninference. The source code will be released soon.",
        "pdf_link": "https://arxiv.org/pdf/2403.10082v1.pdf"
    },
    {
        "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
        "authors": [
            "Weihang Su",
            "Yichen Tang",
            "Qingyao Ai",
            "Zhijing Wu",
            "Yiqun Liu"
        ],
        "published": "2024-03-15T07:45:37Z",
        "summary": "Dynamic retrieval augmented generation (RAG) paradigm actively decides when\nand what to retrieve during the text generation process of Large Language\nModels (LLMs). There are two key elements of this paradigm: identifying the\noptimal moment to activate the retrieval module (deciding when to retrieve) and\ncrafting the appropriate query once retrieval is triggered (determining what to\nretrieve). However, current dynamic RAG methods fall short in both aspects.\nFirstly, the strategies for deciding when to retrieve often rely on static\nrules. Moreover, the strategies for deciding what to retrieve typically limit\nthemselves to the LLM's most recent sentence or the last few tokens, while the\nLLM's real-time information needs may span across the entire context. To\novercome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic\nRetrieval Augmented Generation based on the real-time Information Needs of\nLLMs. Our framework is specifically designed to make decisions on when and what\nto retrieve based on the LLM's real-time information needs during the text\ngeneration process. We evaluate DRAGIN along with existing methods\ncomprehensively over 4 knowledge-intensive generation datasets. Experimental\nresults show that DRAGIN achieves superior performance on all tasks,\ndemonstrating the effectiveness of our method. We have open-sourced all the\ncode, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",
        "pdf_link": "https://arxiv.org/pdf/2403.10081v1.pdf"
    },
    {
        "title": "Are LLMs Good Cryptic Crossword Solvers?",
        "authors": [
            "Abdelrahman \"Boda\" Sadallah",
            "Daria Kotova",
            "Ekaterina Kochmar"
        ],
        "published": "2024-03-15T06:57:08Z",
        "summary": "Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans.",
        "pdf_link": "https://arxiv.org/pdf/2403.12094v1.pdf"
    },
    {
        "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
        "authors": [
            "Yongquan He",
            "Xuancheng Huang",
            "Minghao Tang",
            "Lingxun Meng",
            "Xiang Li",
            "Wei Lin",
            "Wenyuan Zhang",
            "Yifu Gao"
        ],
        "published": "2024-03-15T06:54:20Z",
        "summary": "Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.10056v1.pdf"
    },
    {
        "title": "TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model",
        "authors": [
            "Jiahao Lyu",
            "Jin Wei",
            "Gangyan Zeng",
            "Zeng Li",
            "Enze Xie",
            "Wei Wang",
            "Yu Zhou"
        ],
        "published": "2024-03-15T06:38:25Z",
        "summary": "Existing scene text spotters are designed to locate and transcribe texts from\nimages. However, it is challenging for a spotter to achieve precise detection\nand recognition of scene texts simultaneously. Inspired by the glimpse-focus\nspotting pipeline of human beings and impressive performances of Pre-trained\nLanguage Models (PLMs) on visual tasks, we ask: 1) \"Can machines spot texts\nwithout precise detection just like human beings?\", and if yes, 2) \"Is text\nblock another alternative for scene text spotting other than word or\ncharacter?\" To this end, our proposed scene text spotter leverages advanced\nPLMs to enhance performance without fine-grained detection. Specifically, we\nfirst use a simple detector for block-level text detection to obtain rough\npositional information. Then, we finetune a PLM using a large-scale OCR dataset\nto achieve accurate recognition. Benefiting from the comprehensive language\nknowledge gained during the pre-training phase, the PLM-based recognition\nmodule effectively handles complex scenarios, including multi-line, reversed,\noccluded, and incomplete-detection texts. Taking advantage of the fine-tuned\nlanguage model on scene recognition benchmarks and the paradigm of text block\ndetection, extensive experiments demonstrate the superior performance of our\nscene text spotter across multiple public benchmarks. Additionally, we attempt\nto spot texts directly from an entire scene image to demonstrate the potential\nof PLMs, even Large Language Models (LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2403.10047v1.pdf"
    },
    {
        "title": "Knowledge Condensation and Reasoning for Knowledge-based VQA",
        "authors": [
            "Dongze Hao",
            "Jian Jia",
            "Longteng Guo",
            "Qunbo Wang",
            "Te Yang",
            "Yan Li",
            "Yanhua Cheng",
            "Bo Wang",
            "Quan Chen",
            "Han Li",
            "Jing Liu"
        ],
        "published": "2024-03-15T06:06:06Z",
        "summary": "Knowledge-based visual question answering (KB-VQA) is a challenging task,\nwhich requires the model to leverage external knowledge for comprehending and\nanswering questions grounded in visual content. Recent studies retrieve the\nknowledge passages from external knowledge bases and then use them to answer\nquestions. However, these retrieved knowledge passages often contain irrelevant\nor noisy information, which limits the performance of the model. To address the\nchallenge, we propose two synergistic models: Knowledge Condensation model and\nKnowledge Reasoning model. We condense the retrieved knowledge passages from\ntwo perspectives. First, we leverage the multimodal perception and reasoning\nability of the visual-language models to distill concise knowledge concepts\nfrom retrieved lengthy passages, ensuring relevance to both the visual content\nand the question. Second, we leverage the text comprehension ability of the\nlarge language models to summarize and condense the passages into the knowledge\nessence which helps answer the question. These two types of condensed knowledge\nare then seamlessly integrated into our Knowledge Reasoning model, which\njudiciously navigates through the amalgamated information to arrive at the\nconclusive answer. Extensive experiments validate the superiority of the\nproposed method. Compared to previous methods, our method achieves\nstate-of-the-art performance on knowledge-based VQA datasets (65.1% on OK-VQA\nand 60.1% on A-OKVQA) without resorting to the knowledge produced by GPT-3\n(175B).",
        "pdf_link": "https://arxiv.org/pdf/2403.10037v1.pdf"
    },
    {
        "title": "Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain",
        "authors": [
            "Eugene Jang",
            "Jian Cui",
            "Dayeon Yim",
            "Youngjin Jin",
            "Jin-Woo Chung",
            "Seungwon Shin",
            "Yongjae Lee"
        ],
        "published": "2024-03-15T05:35:02Z",
        "summary": "Cybersecurity information is often technically complex and relayed through\nunstructured text, making automation of cyber threat intelligence highly\nchallenging. For such text domains that involve high levels of expertise,\npretraining on in-domain corpora has been a popular method for language models\nto obtain domain expertise. However, cybersecurity texts often contain\nnon-linguistic elements (such as URLs and hash values) that could be unsuitable\nwith the established pretraining methodologies. Previous work in other domains\nhave removed or filtered such text as noise, but the effectiveness of these\nmethods have not been investigated, especially in the cybersecurity domain. We\npropose different pretraining methodologies and evaluate their effectiveness\nthrough downstream tasks and probing tasks. Our proposed strategy (selective\nMLM and jointly training NLE token classification) outperforms the commonly\ntaken approach of replacing non-linguistic elements (NLEs). We use our\ndomain-customized methodology to train CyBERTuned, a cybersecurity domain\nlanguage model that outperforms other cybersecurity PLMs on most tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.10576v2.pdf"
    },
    {
        "title": "Lost in Overlap: Exploring Watermark Collision in LLMs",
        "authors": [
            "Yiyang Luo",
            "Ke Lin",
            "Chao Gu"
        ],
        "published": "2024-03-15T05:06:21Z",
        "summary": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread use of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks like question answering and paraphrasing. This study focuses on dual\nwatermark collisions, where two watermarks are present simultaneously in the\nsame text. The research demonstrates that watermark collision poses a threat to\ndetection performance for detectors of both upstream and downstream watermark\nalgorithms.",
        "pdf_link": "https://arxiv.org/pdf/2403.10020v1.pdf"
    },
    {
        "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Healthcare Professionals",
        "authors": [
            "Avishek Choudhury",
            "Zaria Chaudhry"
        ],
        "published": "2024-03-15T04:04:45Z",
        "summary": "This paper explores the evolving relationship between clinician trust in\nLLMs, the transformation of data sources from predominantly human-generated to\nAI-generated content, and the subsequent impact on the precision of LLMs and\nclinician competence. One of the primary concerns identified is the potential\nfeedback loop that arises as LLMs become more reliant on their outputs for\nlearning, which may lead to a degradation in output quality and a reduction in\nclinician skills due to decreased engagement with fundamental diagnostic\nprocesses. While theoretical at this stage, this feedback loop poses a\nsignificant challenge as the integration of LLMs in healthcare deepens,\nemphasizing the need for proactive dialogue and strategic measures to ensure\nthe safe and effective use of LLM technology. A key takeaway from our\ninvestigation is the critical role of user expertise and the necessity for a\ndiscerning approach to trusting and validating LLM outputs. The paper\nhighlights how expert users, particularly clinicians, can leverage LLMs to\nenhance productivity by offloading routine tasks while maintaining a critical\noversight to identify and correct potential inaccuracies in AI-generated\ncontent. This balance of trust and skepticism is vital for ensuring that LLMs\naugment rather than undermine the quality of patient care. Moreover, we delve\ninto the potential risks associated with LLMs' self-referential learning loops\nand the deskilling of healthcare professionals. The risk of LLMs operating\nwithin an echo chamber, where AI-generated content feeds into the learning\nalgorithms, threatens the diversity and quality of the data pool, potentially\nentrenching biases and reducing the efficacy of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.14691v2.pdf"
    },
    {
        "title": "Whose Side Are You On? Investigating the Political Stance of Large Language Models",
        "authors": [
            "Pagnarasmey Pit",
            "Xingjun Ma",
            "Mike Conway",
            "Qingyu Chen",
            "James Bailey",
            "Henry Pit",
            "Putrasmey Keo",
            "Watey Diep",
            "Yu-Gang Jiang"
        ],
        "published": "2024-03-15T04:02:24Z",
        "summary": "Large Language Models (LLMs) have gained significant popularity for their\napplication in various everyday tasks such as text generation, summarization,\nand information retrieval. As the widespread adoption of LLMs continues to\nsurge, it becomes increasingly crucial to ensure that these models yield\nresponses that are politically impartial, with the aim of preventing\ninformation bubbles, upholding fairness in representation, and mitigating\nconfirmation bias. In this paper, we propose a quantitative framework and\npipeline designed to systematically investigate the political orientation of\nLLMs. Our investigation delves into the political alignment of LLMs across a\nspectrum of eight polarizing topics, spanning from abortion to LGBTQ issues.\nAcross topics, the results indicate that LLMs exhibit a tendency to provide\nresponses that closely align with liberal or left-leaning perspectives rather\nthan conservative or right-leaning ones when user queries include details\npertaining to occupation, race, or political affiliation. The findings\npresented in this study not only reaffirm earlier observations regarding the\nleft-leaning characteristics of LLMs but also surface particular attributes,\nsuch as occupation, that are particularly susceptible to such inclinations even\nwhen directly steered towards conservatism. As a recommendation to avoid these\nmodels providing politicised responses, users should be mindful when crafting\nqueries, and exercise caution in selecting neutral prompt language.",
        "pdf_link": "https://arxiv.org/pdf/2403.13840v1.pdf"
    },
    {
        "title": "Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers",
        "authors": [
            "Moxin Li",
            "Wenjie Wang",
            "Fuli Feng",
            "Fengbin Zhu",
            "Qifan Wang",
            "Tat-Seng Chua"
        ],
        "published": "2024-03-15T02:38:26Z",
        "summary": "Confidence estimation aiming to evaluate output trustability is crucial for\nthe application of large language models (LLM), especially the black-box ones.\nExisting confidence estimation of LLM is typically not calibrated due to the\noverconfidence of LLM on its generated incorrect answers. Existing approaches\naddressing the overconfidence issue are hindered by a significant limitation\nthat they merely consider the confidence of one answer generated by LLM. To\ntackle this limitation, we propose a novel paradigm that thoroughly evaluates\nthe trustability of multiple candidate answers to mitigate the overconfidence\non incorrect answers. Building upon this paradigm, we introduce a two-step\nframework, which firstly instructs LLM to reflect and provide justifications\nfor each answer, and then aggregates the justifications for comprehensive\nconfidence estimation. This framework can be integrated with existing\nconfidence estimation approaches for superior calibration. Experimental results\non six datasets of three tasks demonstrate the rationality and effectiveness of\nthe proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2403.09972v1.pdf"
    },
    {
        "title": "ViTCN: Vision Transformer Contrastive Network For Reasoning",
        "authors": [
            "Bo Song",
            "Yuanhao Xu",
            "Yichao Wu"
        ],
        "published": "2024-03-15T02:01:14Z",
        "summary": "Machine learning models have achieved significant milestones in various\ndomains, for example, computer vision models have an exceptional result in\nobject recognition, and in natural language processing, where Large Language\nModels (LLM) like GPT can start a conversation with human-like proficiency.\nHowever, abstract reasoning remains a challenge for these models, Can AI really\nthinking like a human? still be a question yet to be answered. Raven\nProgressive Matrices (RPM) is a metric designed to assess human reasoning\ncapabilities. It presents a series of eight images as a problem set, where the\nparticipant should try to discover the underlying rules among these images and\nselect the most appropriate image from eight possible options that best\ncompletes the sequence. This task always be used to test human reasoning\nabilities and IQ. Zhang et al proposed a dataset called RAVEN which can be used\nto test Machine Learning model abstract reasoning ability. In this paper, we\npurposed Vision Transformer Contrastive Network which build on previous work\nwith the Contrastive Perceptual Inference network (CoPiNet), which set a new\nbenchmark for permutationinvariant models Raven Progressive Matrices by\nincorporating contrast effects from psychology, cognition, and education, and\nextends this foundation by leveraging the cutting-edge Vision Transformer\narchitecture. This integration aims to further refine the machine ability to\nprocess and reason about spatial-temporal information from pixel-level inputs\nand global wise features on RAVEN dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.09962v1.pdf"
    },
    {
        "title": "Right Place, Right Time! Towards ObjectNav for Non-Stationary Goals",
        "authors": [
            "Vishnu Sashank Dorbala",
            "Bhrij Patel",
            "Amrit Singh Bedi",
            "Dinesh Manocha"
        ],
        "published": "2024-03-14T22:33:22Z",
        "summary": "We present a novel approach to tackle the ObjectNav task for non-stationary\nand potentially occluded targets in an indoor environment. We refer to this\ntask Portable ObjectNav (or P-ObjectNav), and in this work, present its\nformulation, feasibility, and a navigation benchmark using a novel\nmemory-enhanced LLM-based policy. In contrast to ObjNav where target object\nlocations are fixed for each episode, P-ObjectNav tackles the challenging case\nwhere the target objects move during the episode. This adds a layer of\ntime-sensitivity to navigation, and is particularly relevant in scenarios where\nthe agent needs to find portable targets (e.g. misplaced wallets) in\nhuman-centric environments. The agent needs to estimate not just the correct\nlocation of the target, but also the time at which the target is at that\nlocation for visual grounding -- raising the question about the feasibility of\nthe task. We address this concern by inferring results on two cases for object\nplacement: one where the objects placed follow a routine or a path, and the\nother where they are placed at random. We dynamize Matterport3D for these\nexperiments, and modify PPO and LLM-based navigation policies for evaluation.\nUsing PPO, we observe that agent performance in the random case stagnates,\nwhile the agent in the routine-following environment continues to improve,\nallowing us to infer that P-ObjectNav is solvable in environments with\nroutine-following object placement. Using memory-enhancement on an LLM-based\npolicy, we set a benchmark for P-ObjectNav. Our memory-enhanced agent\nsignificantly outperforms their non-memory-based counterparts across object\nplacement scenarios by 71.76% and 74.68% on average when measured by Success\nRate (SR) and Success Rate weighted by Path Length (SRPL), showing the\ninfluence of memory on improving P-ObjectNav performance. Our code and dataset\nwill be made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2403.09905v1.pdf"
    },
    {
        "title": "Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks",
        "authors": [
            "Zhifan Sun",
            "Antonio Valerio Miceli-Barone"
        ],
        "published": "2024-03-14T19:39:10Z",
        "summary": "Large Language Models (LLMs) are increasingly becoming the preferred\nfoundation platforms for many Natural Language Processing tasks such as Machine\nTranslation, owing to their quality often comparable to or better than\ntask-specific models, and the simplicity of specifying the task through natural\nlanguage instructions or in-context examples. Their generality, however, opens\nthem up to subversion by end users who may embed into their requests\ninstructions that cause the model to behave in unauthorized and possibly unsafe\nways. In this work we study these Prompt Injection Attacks (PIAs) on multiple\nfamilies of LLMs on a Machine Translation task, focusing on the effects of\nmodel size on the attack success rates. We introduce a new benchmark data set\nand we discover that on multiple language pairs and injected prompts written in\nEnglish, larger models under certain conditions may become more susceptible to\nsuccessful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et\nal., 2023). To our knowledge, this is the first work to study non-trivial LLM\nscaling behaviour in a multi-lingual setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.09832v1.pdf"
    },
    {
        "title": "Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention",
        "authors": [
            "Ellie Prosser",
            "Matthew Edwards"
        ],
        "published": "2024-03-14T18:27:43Z",
        "summary": "Powerful generative Large Language Models (LLMs) are becoming popular tools\namongst the general public as question-answering systems, and are being\nutilised by vulnerable groups such as children. With children increasingly\ninteracting with these tools, it is imperative for researchers to scrutinise\nthe safety of LLMs, especially for applications that could lead to serious\noutcomes, such as online child safety queries. In this paper, the efficacy of\nLLMs for online grooming prevention is explored both for identifying and\navoiding grooming through advice generation, and the impact of prompt design on\nmodel performance is investigated by varying the provided context and prompt\nspecificity. In results reflecting over 6,000 LLM interactions, we find that no\nmodels were clearly appropriate for online grooming prevention, with an\nobserved lack of consistency in behaviours, and potential for harmful answer\ngeneration, especially from open-source models. We outline where and how models\nfall short, providing suggestions for improvement, and identify prompt designs\nthat heavily altered model performance in troubling ways, with findings that\ncan be used to inform best practice usage guides.",
        "pdf_link": "https://arxiv.org/pdf/2403.09795v1.pdf"
    },
    {
        "title": "Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models",
        "authors": [
            "Yifan Li",
            "Hangyu Guo",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2024-03-14T18:24:55Z",
        "summary": "In this paper, we study the harmlessness alignment problem of multimodal\nlarge language models~(MLLMs). We conduct a systematic empirical analysis of\nthe harmlessness performance of representative MLLMs and reveal that the image\ninput poses the alignment vulnerability of MLLMs. Inspired by this, we propose\na novel jailbreak method named HADES, which hides and amplifies the harmfulness\nof the malicious intent within the text input, using meticulously crafted\nimages. Experimental results show that HADES can effectively jailbreak existing\nMLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for\nLLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly\nreleased.",
        "pdf_link": "https://arxiv.org/pdf/2403.09792v1.pdf"
    },
    {
        "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
        "authors": [
            "Piotr Nawrot",
            "Adrian \u0141a\u0144cucki",
            "Marcin Chochowski",
            "David Tarjan",
            "Edoardo M. Ponti"
        ],
        "published": "2024-03-14T17:59:26Z",
        "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for on-line key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression rates in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to ~3.7x throughput increase in auto-regressive inference on a\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. We find\nthat DMC preserves the original downstream performance with up to 4x cache\ncompression, outperforming up-trained grouped-query attention (GQA). GQA and\nDMC can be even combined to obtain compounded gains. As a result DMC fits\nlonger contexts and larger batches within any given memory budget.",
        "pdf_link": "https://arxiv.org/pdf/2403.09636v1.pdf"
    },
    {
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "authors": [
            "Haoyu Zhen",
            "Xiaowen Qiu",
            "Peihao Chen",
            "Jincheng Yang",
            "Xin Yan",
            "Yilun Du",
            "Yining Hong",
            "Chuang Gan"
        ],
        "published": "2024-03-14T17:58:41Z",
        "summary": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.09631v1.pdf"
    },
    {
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking",
        "authors": [
            "Eric Zelikman",
            "Georges Harik",
            "Yijia Shao",
            "Varuna Jayasiri",
            "Nick Haber",
            "Noah D. Goodman"
        ],
        "published": "2024-03-14T17:58:16Z",
        "summary": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
        "pdf_link": "https://arxiv.org/pdf/2403.09629v2.pdf"
    },
    {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "authors": [
            "Xiaoyu Liu",
            "Paiheng Xu",
            "Junda Wu",
            "Jiaxin Yuan",
            "Yifan Yang",
            "Yuhang Zhou",
            "Fuxiao Liu",
            "Tianrui Guan",
            "Haoliang Wang",
            "Tong Yu",
            "Julian McAuley",
            "Wei Ai",
            "Furong Huang"
        ],
        "published": "2024-03-14T17:47:20Z",
        "summary": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.09606v1.pdf"
    },
    {
        "title": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation",
        "authors": [
            "Yunhao Gou",
            "Kai Chen",
            "Zhili Liu",
            "Lanqing Hong",
            "Hang Xu",
            "Zhenguo Li",
            "Dit-Yan Yeung",
            "James T. Kwok",
            "Yu Zhang"
        ],
        "published": "2024-03-14T17:03:04Z",
        "summary": "Multimodal large language models (MLLMs) have shown impressive reasoning\nabilities, which, however, are also more vulnerable to jailbreak attacks than\ntheir LLM predecessors. Although still capable of detecting unsafe responses,\nwe observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be\neasily bypassed due to the introduction of image features. To construct robust\nMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free\nprotecting approach that exploits the inherent safety awareness of MLLMs, and\ngenerates safer responses via adaptively transforming unsafe images into texts\nto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.\nExperiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO\nenhances model safety significantly (e.g., a 37.6% improvement on the\nMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while\nconsistently maintaining utility results on common MLLM benchmarks.\nFurthermore, we show that ECSO can be used as a data engine to generate\nsupervised-finetuning (SFT) data for MLLM alignment without extra human\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2403.09572v2.pdf"
    },
    {
        "title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models",
        "authors": [
            "Laura Fern\u00e1ndez-Becerra",
            "Miguel \u00c1ngel Gonz\u00e1lez-Santamarta",
            "\u00c1ngel Manuel Guerrero-Higueras",
            "Francisco Javier Rodr\u00edguez-Lera",
            "Vicente Matell\u00e1n Olivera"
        ],
        "published": "2024-03-14T16:57:18Z",
        "summary": "The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.09567v1.pdf"
    },
    {
        "title": "Less is More: Data Value Estimation for Visual Instruction Tuning",
        "authors": [
            "Zikang Liu",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Dawei Gao",
            "Yaliang Li",
            "Ji-Rong Wen"
        ],
        "published": "2024-03-14T16:47:25Z",
        "summary": "Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.",
        "pdf_link": "https://arxiv.org/pdf/2403.09559v2.pdf"
    },
    {
        "title": "Logits of API-Protected LLMs Leak Proprietary Information",
        "authors": [
            "Matthew Finlayson",
            "Xiang Ren",
            "Swabha Swayamdipta"
        ],
        "published": "2024-03-14T16:27:49Z",
        "summary": "The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.",
        "pdf_link": "https://arxiv.org/pdf/2403.09539v2.pdf"
    },
    {
        "title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding",
        "authors": [
            "Chris Kelly",
            "Luhui Hu",
            "Jiayin Hu",
            "Yu Tian",
            "Deshun Yang",
            "Bang Yang",
            "Cindy Yang",
            "Zihao Li",
            "Zaoshan Huang",
            "Yuexian Zou"
        ],
        "published": "2024-03-14T16:13:00Z",
        "summary": "The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent",
        "pdf_link": "https://arxiv.org/pdf/2403.09530v2.pdf"
    },
    {
        "title": "MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation",
        "authors": [
            "Jiahuan Li",
            "Shanbo Cheng",
            "Shujian Huang",
            "Jiajun Chen"
        ],
        "published": "2024-03-14T16:07:39Z",
        "summary": "Large Language Models (LLM) have demonstrated their strong ability in the\nfield of machine translation (MT), yet they suffer from high computational cost\nand latency. Therefore, transferring translation knowledge from giant LLMs to\nmedium-sized machine translation models is a promising research direction.\nHowever, traditional knowledge distillation methods do not take the capability\nof student and teacher models into consideration, therefore repeatedly teaching\nstudent models on the knowledge they have learned, and failing to extend to\nnovel contexts and knowledge. In this paper, we propose a framework called\nMT-Patcher, which transfers knowledge from LLMs to existing MT models in a\nselective, comprehensive and proactive manner. Considering the current\ntranslation ability of student MT models, we only identify and correct their\ntranslation errors, instead of distilling the whole translation from the\nteacher. Leveraging the strong language abilities of LLMs, we instruct LLM\nteachers to synthesize diverse contexts and anticipate more potential errors\nfor the student. Experiment results on translating both specific language\nphenomena and general MT benchmarks demonstrate that finetuning the student MT\nmodel on about 10% examples can achieve comparable results to the traditional\nknowledge distillation method, and synthesized potential errors and diverse\ncontexts further improve translation performances on unseen contexts and words.",
        "pdf_link": "https://arxiv.org/pdf/2403.09522v2.pdf"
    },
    {
        "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
        "authors": [
            "Yu Wang",
            "Xiaogeng Liu",
            "Yu Li",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "published": "2024-03-14T15:57:13Z",
        "summary": "With the advent and widespread deployment of Multimodal Large Language Models\n(MLLMs), the imperative to ensure their safety has become increasingly\npronounced. However, with the integration of additional modalities, MLLMs are\nexposed to new vulnerabilities, rendering them prone to structured-based\njailbreak attacks, where semantic content (e.g., \"harmful text\") has been\ninjected into the images to mislead MLLMs. In this work, we aim to defend\nagainst such threats. Specifically, we propose \\textbf{Ada}ptive\n\\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with\ndefense prompts to defend MLLMs against structure-based jailbreak attacks\nwithout fine-tuning MLLMs or training additional modules (e.g., post-stage\ncontent detector). Initially, we present a manually designed static defense\nprompt, which thoroughly examines the image and instruction content step by\nstep and specifies response methods to malicious queries. Furthermore, we\nintroduce an adaptive auto-refinement framework, consisting of a target MLLM\nand a LLM-based defense prompt generator (Defender). These components\ncollaboratively and iteratively communicate to generate a defense prompt.\nExtensive experiments on the popular structure-based jailbreak attacks and\nbenign datasets show that our methods can consistently improve MLLMs'\nrobustness against structure-based jailbreak attacks without compromising the\nmodel's general capabilities evaluated on standard benign tasks. Our code is\navailable at https://github.com/rain305f/AdaShield.",
        "pdf_link": "https://arxiv.org/pdf/2403.09513v1.pdf"
    },
    {
        "title": "LLM-based agents for automating the enhancement of user story quality: An early report",
        "authors": [
            "Zheying Zhang",
            "Maruf Rayhan",
            "Tomas Herda",
            "Manuel Goisauf",
            "Pekka Abrahamsson"
        ],
        "published": "2024-03-14T14:35:53Z",
        "summary": "In agile software development, maintaining high-quality user stories is\ncrucial, but also challenging. This study explores the use of large language\nmodels to automatically improve the user story quality in Austrian Post Group\nIT agile teams. We developed a reference model for an Autonomous LLM-based\nAgent System and implemented it at the company. The quality of user stories in\nthe study and the effectiveness of these agents for user story quality\nimprovement was assessed by 11 participants across six agile teams. Our\nfindings demonstrate the potential of LLMs in improving user story quality,\ncontributing to the research on AI role in agile development, and providing a\npractical example of the transformative impact of AI in an industry setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.09442v1.pdf"
    },
    {
        "title": "\"Like a Nesting Doll\": Analyzing Recursion Analogies Generated by CS Students using Large Language Models",
        "authors": [
            "Seth Bernstein",
            "Paul Denny",
            "Juho Leinonen",
            "Lauren Kan",
            "Arto Hellas",
            "Matt Littlefield Sami Sarsa",
            "Stephen MacNeil"
        ],
        "published": "2024-03-14T14:01:26Z",
        "summary": "Grasping complex computing concepts often poses a challenge for students who\nstruggle to anchor these new ideas to familiar experiences and understandings.\nTo help with this, a good analogy can bridge the gap between unfamiliar\nconcepts and familiar ones, providing an engaging way to aid understanding.\nHowever, creating effective educational analogies is difficult even for\nexperienced instructors. We investigate to what extent large language models\n(LLMs), specifically ChatGPT, can provide access to personally relevant\nanalogies on demand. Focusing on recursion, a challenging threshold concept, we\nconducted an investigation analyzing the analogies generated by more than 350\nfirst-year computing students. They were provided with a code snippet and\ntasked to generate their own recursion-based analogies using ChatGPT,\noptionally including personally relevant topics in their prompts. We observed a\ngreat deal of diversity in the analogies produced with student-prescribed\ntopics, in contrast to the otherwise generic analogies, highlighting the value\nof student creativity when working with LLMs. Not only did students enjoy the\nactivity and report an improved understanding of recursion, but they described\nmore easily remembering analogies that were personally and culturally relevant.",
        "pdf_link": "https://arxiv.org/pdf/2403.09409v1.pdf"
    },
    {
        "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
        "authors": [
            "Haiyang Wang",
            "Hao Tang",
            "Li Jiang",
            "Shaoshuai Shi",
            "Muhammad Ferjad Naeem",
            "Hongsheng Li",
            "Bernt Schiele",
            "Liwei Wang"
        ],
        "published": "2024-03-14T13:47:41Z",
        "summary": "This paper proposes a simple, yet effective framework, called GiT,\nsimultaneously applicable for various vision tasks only with a vanilla ViT.\nMotivated by the universality of the Multi-layer Transformer architecture (e.g,\nGPT) widely used in large language models (LLMs), we seek to broaden its scope\nto serve as a powerful vision foundation model (VFM). However, unlike language\nmodeling, visual tasks typically require specific modules, such as bounding box\nheads for detection and pixel decoders for segmentation, greatly hindering the\napplication of powerful multi-layer transformers in the vision domain. To solve\nthis, we design a universal language interface that empowers the successful\nauto-regressive decoding to adeptly unify various visual tasks, from\nimage-level understanding (e.g., captioning), over sparse perception (e.g.,\ndetection), to dense prediction (e.g., segmentation). Based on the above\ndesigns, the entire model is composed solely of a ViT, without any specific\nadditions, offering a remarkable architectural simplification. GiT is a\nmulti-task visual model, jointly trained across five representative benchmarks\nwithout task-specific fine-tuning. Interestingly, our GiT builds a new\nbenchmark in generalist performance, and fosters mutual enhancement across\ntasks, leading to significant improvements compared to isolated training. This\nreflects a similar impact observed in LLMs. Further enriching training with 27\ndatasets, GiT achieves strong zero-shot results over various tasks. Due to its\nsimple design, this paradigm holds promise for narrowing the architectural gap\nbetween vision and language. Code and models will be available at\n\\url{https://github.com/Haiyang-W/GiT}.",
        "pdf_link": "https://arxiv.org/pdf/2403.09394v1.pdf"
    },
    {
        "title": "Komodo: A Linguistic Expedition into Indonesia's Regional Languages",
        "authors": [
            "Louis Owen",
            "Vishesh Tripathi",
            "Abhay Kumar",
            "Biddwan Ahmed"
        ],
        "published": "2024-03-14T13:12:21Z",
        "summary": "The recent breakthroughs in Large Language Models (LLMs) have mostly focused\non languages with easily available and sufficient resources, such as English.\nHowever, there remains a significant gap for languages that lack sufficient\nlinguistic resources in the public domain. Our work introduces Komodo-7B,\n7-billion-parameter Large Language Models designed to address this gap by\nseamlessly operating across Indonesian, English, and 11 regional languages in\nIndonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and\nKomodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art\nperformance in various tasks and languages, outperforming the benchmarks set by\nOpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,\nMixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only\ndemonstrates superior performance in both language-specific and overall\nassessments but also highlights its capability to excel in linguistic\ndiversity. Our commitment to advancing language models extends beyond\nwell-resourced languages, aiming to bridge the gap for those with limited\nlinguistic assets. Additionally, Komodo-7B-Instruct's better cross-language\nunderstanding contributes to addressing educational disparities in Indonesia,\noffering direct translations from English to 11 regional languages, a\nsignificant improvement compared to existing language translation services.\nKomodo-7B represents a crucial step towards inclusivity and effectiveness in\nlanguage models, providing to the linguistic needs of diverse communities.",
        "pdf_link": "https://arxiv.org/pdf/2403.09362v2.pdf"
    },
    {
        "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences",
        "authors": [
            "Sun Ao",
            "Weilin Zhao",
            "Xu Han",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Chuan Shi",
            "Maosong Sun",
            "Shengnan Wang",
            "Teng Su"
        ],
        "published": "2024-03-14T12:51:58Z",
        "summary": "Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 2 X speedup during training 32K\nsequence length on 8 X A100.",
        "pdf_link": "https://arxiv.org/pdf/2403.09347v1.pdf"
    },
    {
        "title": "AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions",
        "authors": [
            "Hao Zhang",
            "Wenqi Shao",
            "Hong Liu",
            "Yongqiang Ma",
            "Ping Luo",
            "Yu Qiao",
            "Kaipeng Zhang"
        ],
        "published": "2024-03-14T12:51:07Z",
        "summary": "Large Vision-Language Models (LVLMs) have shown significant progress in well\nresponding to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce AVIBench, a framework designed to analyze the robustness\nof LVLMs when facing various adversarial visual-instructions (AVIs), including\nfour types of image-based AVIs, ten types of text-based AVIs, and nine types of\ncontent bias AVIs (such as gender, violence, cultural, and racial biases, among\nothers). We generate 260K AVIs encompassing five categories of multimodal\ncapabilities (nine tasks) and content bias. We then conduct a comprehensive\nevaluation involving 14 open-source LVLMs to assess their performance. AVIBench\nalso serves as a convenient tool for practitioners to evaluate the robustness\nof LVLMs against AVIs. Our findings and extensive experimental results shed\nlight on the vulnerabilities of LVLMs, and highlight that inherent biases exist\neven in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This\nunderscores the importance of enhancing the robustness, security, and fairness\nof LVLMs. The source code and benchmark will be made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2403.09346v1.pdf"
    },
    {
        "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling and Visual-Language Co-Referring",
        "authors": [
            "Yufei Zhan",
            "Yousong Zhu",
            "Hongyin Zhao",
            "Fan Yang",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "published": "2024-03-14T12:21:37Z",
        "summary": "Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.",
        "pdf_link": "https://arxiv.org/pdf/2403.09333v1.pdf"
    },
    {
        "title": "What Was Your Prompt? A Remote Keylogging Attack on AI Assistants",
        "authors": [
            "Roy Weiss",
            "Daniel Ayzenshteyn",
            "Guy Amit",
            "Yisroel Mirsky"
        ],
        "published": "2024-03-14T09:38:12Z",
        "summary": "AI assistants are becoming an integral part of society, used for asking\nadvice or help in personal and confidential issues. In this paper, we unveil a\nnovel side-channel that can be used to read encrypted responses from AI\nAssistants over the web: the token-length side-channel. We found that many\nvendors, including OpenAI and Microsoft, have this side-channel.\n  However, inferring the content of a response from a token-length sequence\nalone proves challenging. This is because tokens are akin to words, and\nresponses can be several sentences long leading to millions of grammatically\ncorrect sentences. In this paper, we show how this can be overcome by (1)\nutilizing the power of a large language model (LLM) to translate these\nsequences, (2) providing the LLM with inter-sentence context to narrow the\nsearch space and (3) performing a known-plaintext attack by fine-tuning the\nmodel on the target model's writing style.\n  Using these methods, we were able to accurately reconstruct 29\\% of an AI\nassistant's responses and successfully infer the topic from 55\\% of them. To\ndemonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and\nMicrosoft's Copilot on both browser and API traffic.",
        "pdf_link": "https://arxiv.org/pdf/2403.09751v1.pdf"
    },
    {
        "title": "Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse",
        "authors": [
            "Jianwei Sun",
            "Chaoyang Mei",
            "Linlin Wei",
            "Kaiyu Zheng",
            "Na Liu",
            "Ming Cui",
            "Tianyi Li"
        ],
        "published": "2024-03-14T08:27:32Z",
        "summary": "The efficacy of large language models (LLMs) is heavily dependent on the\nquality of the underlying data, particularly within specialized domains. A\ncommon challenge when fine-tuning LLMs for domain-specific applications is the\npotential degradation of the model's generalization capabilities. To address\nthese issues, we propose a two-stage approach for the construction of\nproduction prompts designed to yield high-quality data. This method involves\nthe generation of a diverse array of prompts that encompass a broad spectrum of\ntasks and exhibit a rich variety of expressions. Furthermore, we introduce a\ncost-effective, multi-dimensional quality assessment framework to ensure the\nintegrity of the generated labeling data. Utilizing a dataset comprised of\nservice provider and customer interactions from the real estate sector, we\ndemonstrate a positive correlation between data quality and model performance.\nNotably, our findings indicate that the domain-specific proficiency of general\nLLMs can be enhanced through fine-tuning with data produced via our proposed\nmethod, without compromising their overall generalization abilities, even when\nexclusively domain-specific data is employed for fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.09167v1.pdf"
    },
    {
        "title": "Caveat Lector: Large Language Models in Legal Practice",
        "authors": [
            "Eliza Mik"
        ],
        "published": "2024-03-14T08:19:41Z",
        "summary": "The current fascination with large language models, or LLMs, derives from the\nfact that many users lack the expertise to evaluate the quality of the\ngenerated text. LLMs may therefore appear more capable than they actually are.\nThe dangerous combination of fluency and superficial plausibility leads to the\ntemptation to trust the generated text and creates the risk of overreliance.\nWho would not trust perfect legalese? Relying recent findings in both technical\nand legal scholarship, this Article counterbalances the overly optimistic\npredictions as to the role of LLMs in legal practice. Integrating LLMs into\nlegal workstreams without a better comprehension of their limitations, will\ncreate inefficiencies if not outright risks. Notwithstanding their\nunprecedented ability to generate text, LLMs do not understand text. Without\nthe ability to understand meaning, LLMs will remain unable to use language, to\nacquire knowledge and to perform complex reasoning tasks. Trained to model\nlanguage on the basis of stochastic word predictions, LLMs cannot distinguish\nfact from fiction. Their knowledge of the law is limited to word strings\nmemorized in their parameters. It is also incomplete and largely incorrect.\nLLMs operate at the level of word distributions, not at the level of verified\nfacts. The resulting propensity to hallucinate, to produce statements that are\nincorrect but appear helpful and relevant, is alarming in high-risk areas like\nlegal services. At present, lawyers should beware of relying on text generated\nby LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.09163v1.pdf"
    },
    {
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
        "authors": [
            "Haoran Yang",
            "Yumeng Zhang",
            "Jiaqi Xu",
            "Hongyuan Lu",
            "Pheng Ann Heng",
            "Wai Lam"
        ],
        "published": "2024-03-14T08:18:59Z",
        "summary": "While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.09162v1.pdf"
    },
    {
        "title": "Evaluating LLMs for Gender Disparities in Notable Persons",
        "authors": [
            "Lauren Rhue",
            "Sofie Goethals",
            "Arun Sundararajan"
        ],
        "published": "2024-03-14T07:58:27Z",
        "summary": "This study examines the use of Large Language Models (LLMs) for retrieving\nfactual information, addressing concerns over their propensity to produce\nfactually incorrect \"hallucinated\" responses or to altogether decline to even\nanswer prompt at all. Specifically, it investigates the presence of\ngender-based biases in LLMs' responses to factual inquiries. This paper takes a\nmulti-pronged approach to evaluating GPT models by evaluating fairness across\nmultiple dimensions of recall, hallucinations and declinations. Our findings\nreveal discernible gender disparities in the responses generated by GPT-3.5.\nWhile advancements in GPT-4 have led to improvements in performance, they have\nnot fully eradicated these gender disparities, notably in instances where\nresponses are declined. The study further explores the origins of these\ndisparities by examining the influence of gender associations in prompts and\nthe homogeneity in the responses.",
        "pdf_link": "https://arxiv.org/pdf/2403.09148v1.pdf"
    },
    {
        "title": "USimAgent: Large Language Models for Simulating Search Users",
        "authors": [
            "Erhan Zhang",
            "Xingzhu Wang",
            "Peiyuan Gong",
            "Yankai Lin",
            "Jiaxin Mao"
        ],
        "published": "2024-03-14T07:40:54Z",
        "summary": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators.",
        "pdf_link": "https://arxiv.org/pdf/2403.09142v1.pdf"
    },
    {
        "title": "ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text",
        "authors": [
            "Chang Zong",
            "Yuyan Chen",
            "Weiming Lu",
            "Jian Shao",
            "Yueting Zhuang"
        ],
        "published": "2024-03-14T06:49:16Z",
        "summary": "Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2403.09131v2.pdf"
    },
    {
        "title": "Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models",
        "authors": [
            "Zhuoqun Li",
            "Hongyu Lin",
            "Yaojie Lu",
            "Hao Xiang",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2024-03-14T05:34:35Z",
        "summary": "Declarative knowledge and procedural knowledge are two key parts in\nmeta-cognitive theory, and these two hold significant importance in\npre-training and inference of LLMs. However, a comprehensive analysis comparing\nthese two types of knowledge is lacking, primarily due to challenges in\ndefinition, probing and quantitative assessment. In this paper, we explore from\na new perspective by providing ground-truth knowledge for LLMs and evaluating\nthe effective score. Through extensive experiments with widely-used datasets\nand models, we get conclusions: (1) In most tasks, benefits from declarative\nknowledge are greater than those from procedural knowledge. (2) Profits of\nprocedural knowledge are larger than declarative knowledge only in reasoning\ntasks with simple logic. (3) As pre-training progresses and size increases,\nmodel ability to utilize both kinds of knowledge significantly improves, but in\ndifferent speed. We do detailed analysis for the findings and this can provide\nprimary guidance for evaluation and enhancement of large language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.09750v1.pdf"
    },
    {
        "title": "Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Ting Liu",
            "Bing Qin",
            "Dongliang Xu",
            "Qing Yang",
            "Hongtao Liu",
            "Yixin Cao"
        ],
        "published": "2024-03-14T04:06:13Z",
        "summary": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nsimple questions supported by a generic fact, LLMs often fail to provide\nconsistent and precise answers, indicating a deficiency in abstract reasoning\nabilities. This has sparked a vigorous debate about whether LLMs are genuinely\nreasoning or merely memorizing. In light of this, we design a preliminary study\nto quantify and delve into the abstract reasoning abilities of existing LLMs.\nOur findings reveal a substantial discrepancy between their general reasoning\nand abstract reasoning performances. To relieve this problem, we tailor an\nabstract reasoning dataset (AbsR) together with a meaningful learning paradigm\nto teach LLMs how to leverage generic facts for reasoning purposes. The results\nshow that our approach not only boosts the general reasoning performance of\nLLMs but also makes considerable strides towards their capacity for abstract\nreasoning, moving beyond simple memorization or imitation to a more nuanced\nunderstanding and application of generic facts.",
        "pdf_link": "https://arxiv.org/pdf/2403.09085v1.pdf"
    },
    {
        "title": "Large Language Models are Parallel Multilingual Learners",
        "authors": [
            "Yongyu Mu",
            "Peinan Feng",
            "Zhiquan Cao",
            "Yuzhang Wu",
            "Bei Li",
            "Chenglong Wang",
            "Tong Xiao",
            "Kai Song",
            "Tongran Liu",
            "Chunliang Zhang",
            "Jingbo Zhu"
        ],
        "published": "2024-03-14T03:33:46Z",
        "summary": "In this study, we reveal an in-context learning (ICL) capability of\nmultilingual large language models (LLMs): by translating the input to several\nlanguages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which\nsignificantly enhances their comprehension abilities. To test this capability,\nwe design extensive experiments encompassing 8 typical datasets, 7 languages\nand 8 state-of-the-art multilingual LLMs. Experimental results show that (1)\nincorporating more languages help PiM surpass the conventional ICL further; (2)\neven combining with the translations that are inferior to baseline performance\ncan also help. Moreover, by examining the activated neurons in LLMs, we\ndiscover a counterintuitive but interesting phenomenon. Contrary to the common\nthought that PiM would activate more neurons than monolingual input to leverage\nknowledge learned from diverse languages, PiM actually inhibits neurons and\npromotes more precise neuron activation especially when more languages are\nadded. This phenomenon aligns with the neuroscience insight about synaptic\npruning, which removes less used neural connections, strengthens remainders,\nand then enhances brain intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.09073v1.pdf"
    },
    {
        "title": "UniCode: Learning a Unified Codebook for Multimodal Large Language Models",
        "authors": [
            "Sipeng Zheng",
            "Bohan Zhou",
            "Yicheng Feng",
            "Ye Wang",
            "Zongqing Lu"
        ],
        "published": "2024-03-14T03:29:58Z",
        "summary": "In this paper, we propose \\textbf{UniCode}, a novel approach within the\ndomain of multimodal large language models (MLLMs) that learns a unified\ncodebook to efficiently tokenize visual, text, and potentially other types of\nsignals. This innovation addresses a critical limitation in existing MLLMs:\ntheir reliance on a text-only codebook, which restricts MLLM's ability to\ngenerate images and texts in a multimodal context. Towards this end, we propose\na language-driven iterative training paradigm, coupled with an in-context\npre-training task we term ``image decompression'', enabling our model to\ninterpret compressed visual data and generate high-quality images.The unified\ncodebook empowers our model to extend visual instruction tuning to\nnon-linguistic generation tasks. Moreover, UniCode is adaptable to diverse\nstacked quantization approaches in order to compress visual signals into a more\ncompact token representation. Despite using significantly fewer parameters and\nless data during training, Unicode demonstrates promising capabilities in\nvisual reconstruction and generation. It also achieves performances comparable\nto leading MLLMs across a spectrum of VQA benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.09072v1.pdf"
    },
    {
        "title": "Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate",
        "authors": [
            "Xihan Li",
            "Xing Li",
            "Lei Chen",
            "Xing Zhang",
            "Mingxuan Yuan",
            "Jun Wang"
        ],
        "published": "2024-03-14T03:24:14Z",
        "summary": "Language, a prominent human ability to express through sequential symbols,\nhas been computationally mastered by recent advances of large language models\n(LLMs). By predicting the next word recurrently with huge neural models, LLMs\nhave shown unprecedented capabilities in understanding and reasoning. Circuit,\nas the \"language\" of electronic design, specifies the functionality of an\nelectronic device by cascade connections of logic gates. Then, can circuits\nalso be mastered by a a sufficiently large \"circuit model\", which can conquer\nelectronic design tasks by simply predicting the next logic gate? In this work,\nwe take the first step to explore such possibilities. Two primary barriers\nimpede the straightforward application of LLMs to circuits: their complex,\nnon-sequential structure, and the intolerance of hallucination due to strict\nconstraints (e.g., equivalence). For the first barrier, we encode a circuit as\na memory-less, depth-first traversal trajectory, which allows Transformer-based\nneural models to better leverage its structural information, and predict the\nnext gate on the trajectory as a circuit model. For the second barrier, we\nintroduce an equivalence-preserving decoding process, which ensures that every\ntoken in the generated trajectory adheres to the specified equivalence\nconstraints. Moreover, the circuit model can also be regarded as a stochastic\npolicy to tackle optimization-oriented circuit design tasks. Experimentally, we\ntrained a Transformer-based model of 88M parameters, named \"Circuit\nTransformer\", which demonstrates impressive performance in end-to-end logic\nsynthesis. With Monte-Carlo tree search, Circuit Transformer significantly\nimproves over resyn2 while retaining strict equivalence, showcasing the\npotential of generative AI in conquering electronic design challenges.",
        "pdf_link": "https://arxiv.org/pdf/2403.13838v1.pdf"
    },
    {
        "title": "LAMP: A Language Model on the Map",
        "authors": [
            "Pasquale Balsebre",
            "Weiming Huang",
            "Gao Cong"
        ],
        "published": "2024-03-14T02:56:38Z",
        "summary": "Large Language Models (LLMs) are poised to play an increasingly important\nrole in our lives, providing assistance across a wide array of tasks. In the\ngeospatial domain, LLMs have demonstrated the ability to answer generic\nquestions, such as identifying a country's capital; nonetheless, their utility\nis hindered when it comes to answering fine-grained questions about specific\nplaces, such as grocery stores or restaurants, which constitute essential\naspects of people's everyday lives. This is mainly because the places in our\ncities haven't been systematically fed into LLMs, so as to understand and\nmemorize them. This study introduces a novel framework for fine-tuning a\npre-trained model on city-specific data, to enable it to provide accurate\nrecommendations, while minimizing hallucinations. We share our model, LAMP, and\nthe data used to train it. We conduct experiments to analyze its ability to\ncorrectly retrieving spatial objects, and compare it to well-known open- and\nclosed- source language models, such as GPT-4. Finally, we explore its emerging\ncapabilities through a case study on day planning.",
        "pdf_link": "https://arxiv.org/pdf/2403.09059v1.pdf"
    },
    {
        "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
        "authors": [
            "Muhammad Adnan",
            "Akhil Arunkumar",
            "Gaurav Jain",
            "Prashant J. Nair",
            "Ilya Soloveychik",
            "Purushotham Kamath"
        ],
        "published": "2024-03-14T02:42:42Z",
        "summary": "Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2403.09054v2.pdf"
    },
    {
        "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences",
        "authors": [
            "Martin Weyssow",
            "Aton Kamanda",
            "Houari Sahraoui"
        ],
        "published": "2024-03-14T01:51:35Z",
        "summary": "Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires assessing intricate\ntextual LLMs' outputs. By relying on automated metrics and static analysis\ntools, existing benchmarks fail to assess nuances in user instructions and LLM\noutputs, highlighting the need for large-scale datasets and benchmarks for LLM\npreference alignment. In this paper, we introduce CodeUltraFeedback, a\npreference dataset of 10,000 complex instructions to tune and align LLMs to\ncoding preferences through AI feedback. We generate responses to the\ninstructions using a pool of 14 diverse LLMs, which we then annotate according\nto their alignment with five coding preferences using the LLM-as-a-Judge\napproach with GPT-3.5, producing both numerical and textual feedback. We also\npresent CODAL-Bench, a benchmark for assessing LLM alignment with these coding\npreferences. Our results show that CodeLlama-7B-Instruct, aligned through\nreinforcement learning from AI feedback (RLAIF) with direct preference\noptimization (DPO) using CodeUltraFeedback's AI feedback data, outperforms 34B\nLLMs on CODAL-Bench, validating the utility of CodeUltraFeedback for preference\ntuning. Furthermore, we show our DPO-aligned CodeLlama model improves\nfunctional correctness on HumanEval+ compared to the unaligned base model.\nTherefore, our contributions bridge the gap in preference tuning of LLMs for\ncode and set the stage for further advancements in model alignment and RLAIF\nfor code intelligence. Our code and data are available at\nhttps://github.com/martin-wey/CodeUltraFeedback.",
        "pdf_link": "https://arxiv.org/pdf/2403.09032v1.pdf"
    },
    {
        "title": "ChartInstruct: Instruction Tuning for Chart Comprehension and Reasoning",
        "authors": [
            "Ahmed Masry",
            "Mehrad Shahmohammadi",
            "Md Rizwan Parvez",
            "Enamul Hoque",
            "Shafiq Joty"
        ],
        "published": "2024-03-14T01:40:23Z",
        "summary": "Charts provide visual representations of data and are widely used for\nanalyzing information, addressing queries, and conveying insights to others.\nVarious chart-related downstream tasks have emerged recently, such as\nquestion-answering and summarization. A common strategy to solve these tasks is\nto fine-tune various models originally trained on vision tasks language.\nHowever, such task-specific models are not capable of solving a wide range of\nchart-related tasks, constraining their real-world applicability. To overcome\nthese challenges, we introduce ChartInstruct: a novel chart-specific\nvision-language Instruction-following dataset comprising 191K instructions\ngenerated with 71K charts. We then present two distinct systems for instruction\ntuning on such datasets: (1) an end-to-end model that connects a vision encoder\nfor chart understanding with a LLM; and (2) a pipeline model that employs a\ntwo-step approach to extract chart data tables and input them into the LLM. In\nexperiments on four downstream tasks, we first show the effectiveness of our\nmodel--achieving a new set of state-of-the-art results. Further evaluation\nshows that our instruction-tuning approach supports a wide array of real-world\nchart comprehension and reasoning scenarios, thereby expanding the scope and\napplicability of our models to new kinds of tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.09028v1.pdf"
    },
    {
        "title": "VisionGPT: Vision-Language Understanding Agent Using Generalized Multimodal Framework",
        "authors": [
            "Chris Kelly",
            "Luhui Hu",
            "Bang Yang",
            "Yu Tian",
            "Deshun Yang",
            "Cindy Yang",
            "Zaoshan Huang",
            "Zihao Li",
            "Jiayin Hu",
            "Yuexian Zou"
        ],
        "published": "2024-03-14T01:39:40Z",
        "summary": "With the emergence of large language models (LLMs) and vision foundation\nmodels, how to combine the intelligence and capacity of these open-sourced or\nAPI-available models to achieve open-world visual perception remains an open\nquestion. In this paper, we introduce VisionGPT to consolidate and automate the\nintegration of state-of-the-art foundation models, thereby facilitating\nvision-language understanding and the development of vision-oriented AI.\nVisionGPT builds upon a generalized multimodal framework that distinguishes\nitself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the\npivot to break down users' requests into detailed action proposals to call\nsuitable foundation models; (2) integrating multi-source outputs from\nfoundation models automatically and generating comprehensive responses for\nusers; (3) adaptable to a wide range of applications such as text-conditioned\nimage understanding/generation/editing and visual question answering. This\npaper outlines the architecture and capabilities of VisionGPT, demonstrating\nits potential to revolutionize the field of computer vision through enhanced\nefficiency, versatility, and generalization, and performance. Our code and\nmodels will be made publicly available. Keywords: VisionGPT, Open-world visual\nperception, Vision-language understanding, Large language model, and Foundation\nmodel",
        "pdf_link": "https://arxiv.org/pdf/2403.09027v1.pdf"
    },
    {
        "title": "AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic",
        "authors": [
            "Emad A. Alghamdi",
            "Reem I. Masoud",
            "Deema Alnuhait",
            "Afnan Y. Alomairi",
            "Ahmed Ashraf",
            "Mohamed Zaytoon"
        ],
        "published": "2024-03-14T00:45:24Z",
        "summary": "The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n516 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. We evaluated a\nset of LLMs against our benchmark to assess their trustworthiness. GPT-4 was\nthe most trustworthy LLM, while open-source models, particularly AceGPT 7B and\nJais 13B, struggled to achieve a score of 60% in our benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2403.09017v2.pdf"
    },
    {
        "title": "Re-Search for The Truth: Multi-round Retrieval-augmented Large Language Models are Strong Fake News Detectors",
        "authors": [
            "Guanghua Li",
            "Wensheng Lu",
            "Wei Zhang",
            "Defu Lian",
            "Kezhong Lu",
            "Rui Mao",
            "Kai Shu",
            "Hao Liao"
        ],
        "published": "2024-03-14T00:35:39Z",
        "summary": "The proliferation of fake news has had far-reaching implications on politics,\nthe economy, and society at large. While Fake news detection methods have been\nemployed to mitigate this issue, they primarily depend on two essential\nelements: the quality and relevance of the evidence, and the effectiveness of\nthe verdict prediction mechanism. Traditional methods, which often source\ninformation from static repositories like Wikipedia, are limited by outdated or\nincomplete data, particularly for emerging or rare claims. Large Language\nModels (LLMs), known for their remarkable reasoning and generative\ncapabilities, introduce a new frontier for fake news detection. However, like\ntraditional methods, LLM-based solutions also grapple with the limitations of\nstale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently\nstruggle with issues such as low-quality evidence retrieval and context length\nconstraints. To address these challenges, we introduce a novel,\nretrieval-augmented LLMs framework--the first of its kind to automatically and\nstrategically extract key evidence from web sources for claim verification.\nEmploying a multi-round retrieval strategy, our framework ensures the\nacquisition of sufficient, relevant evidence, thereby enhancing performance.\nComprehensive experiments across three real-world datasets validate the\nframework's superiority over existing methods. Importantly, our model not only\ndelivers accurate verdicts but also offers human-readable explanations to\nimprove result interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2403.09747v1.pdf"
    },
    {
        "title": "Evaluating the Application of Large Language Models to Generate Feedback in Programming Education",
        "authors": [
            "Sven Jacobs",
            "Steffen Jaschke"
        ],
        "published": "2024-03-13T23:14:35Z",
        "summary": "This study investigates the application of large language models,\nspecifically GPT-4, to enhance programming education. The research outlines the\ndesign of a web application that uses GPT-4 to provide feedback on programming\ntasks, without giving away the solution. A web application for working on\nprogramming tasks was developed for the study and evaluated with 51 students\nover the course of one semester. The results show that most of the feedback\ngenerated by GPT-4 effectively addressed code errors. However, challenges with\nincorrect suggestions and hallucinated issues indicate the need for further\nimprovements.",
        "pdf_link": "https://arxiv.org/pdf/2403.09744v1.pdf"
    },
    {
        "title": "AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents",
        "authors": [
            "Yao Fu",
            "Dong-Ki Kim",
            "Jaekyeom Kim",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Kyunghoon Bae",
            "Honglak Lee"
        ],
        "published": "2024-03-13T22:06:03Z",
        "summary": "The primary limitation of large language models (LLMs) is their restricted\nunderstanding of the world. This poses significant difficulties for LLM-based\nagents, particularly in domains where pre-trained LLMs lack sufficient\nknowledge. In this paper, we introduce a novel framework, called AutoGuide,\nthat bridges the knowledge gap in pre-trained LLMs by leveraging implicit\nknowledge in offline experiences. Specifically, AutoGuide effectively extracts\nknowledge embedded in offline data by extracting a set of state-aware\nguidelines. Importantly, each state-aware guideline is expressed in concise\nnatural language and follows a conditional structure, clearly describing the\nstate where it is applicable. As such, the resulting guidelines enable a\nprincipled way to provide helpful knowledge pertinent to an agent's current\ndecision-making process. We show that our approach outperforms competitive\nLLM-based baselines by a large margin in sequential decision-making benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.08978v1.pdf"
    },
    {
        "title": "The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions",
        "authors": [
            "Christian A. Schiller"
        ],
        "published": "2024-03-13T21:39:39Z",
        "summary": "The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for\nArtificial Intelligence, introducing Large Language Models (LLMs) to the\nmainstream and setting new records in user adoption. LLMs, particularly\nChatGPT, trained on extensive internet data, demonstrate remarkable\nconversational capabilities across various domains, suggesting a significant\nimpact on the workforce. However, these models are susceptible to errors -\n\"hallucinations\" and omissions, generating incorrect or incomplete information.\nThis poses risks especially in contexts where accuracy is crucial, such as\nlegal compliance, medicine or fine-grained process frameworks.\n  There are both technical and human solutions to cope with this isse. This\npaper explores the human factors that enable users to detect errors in LLM\noutputs, a critical component in mitigating risks associated with their use in\nprofessional settings. Understanding these factors is essential for\norganizations aiming to leverage LLM technology efficiently, guiding targeted\ntraining and deployment strategies to enhance error detection by users. This\napproach not only aims to optimize the use of LLMs but also to prevent\npotential downstream issues stemming from reliance on inaccurate model\nresponses. The research emphasizes the balance between technological\nadvancement and human insight in maximizing the benefits of LLMs while\nminimizing the risks, particularly in areas where precision is paramount.\n  This paper performs a systematic literature research on this research topic,\nanalyses and synthesizes the findings, and outlines future research directions.\nLiterature selection cut-off date is January 11th 2024.",
        "pdf_link": "https://arxiv.org/pdf/2403.09743v1.pdf"
    },
    {
        "title": "Exploring Prompt Engineering Practices in the Enterprise",
        "authors": [
            "Michael Desmond",
            "Michelle Brachman"
        ],
        "published": "2024-03-13T20:32:32Z",
        "summary": "Interaction with Large Language Models (LLMs) is primarily carried out via\nprompting. A prompt is a natural language instruction designed to elicit\ncertain behaviour or output from a model. In theory, natural language prompts\nenable non-experts to interact with and leverage LLMs. However, for complex\ntasks and tasks with specific requirements, prompt design is not trivial.\nCreating effective prompts requires skill and knowledge, as well as significant\niteration in order to determine model behavior, and guide the model to\naccomplish a particular goal. We hypothesize that the way in which users\niterate on their prompts can provide insight into how they think prompting and\nmodels work, as well as the kinds of support needed for more efficient prompt\nengineering. To better understand prompt engineering practices, we analyzed\nsessions of prompt editing behavior, categorizing the parts of prompts users\niterated on and the types of changes they made. We discuss design implications\nand future directions based on these prompt engineering practices.",
        "pdf_link": "https://arxiv.org/pdf/2403.08950v1.pdf"
    },
    {
        "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
        "authors": [
            "Xuansheng Wu",
            "Haiyan Zhao",
            "Yaochen Zhu",
            "Yucheng Shi",
            "Fan Yang",
            "Tianming Liu",
            "Xiaoming Zhai",
            "Wenlin Yao",
            "Jundong Li",
            "Mengnan Du",
            "Ninghao Liu"
        ],
        "published": "2024-03-13T20:25:27Z",
        "summary": "Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the \"black\nbox\" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.08946v1.pdf"
    },
    {
        "title": "LMStyle Benchmark: Evaluating Text Style Transfer for Chatbots",
        "authors": [
            "Jianlin Chen"
        ],
        "published": "2024-03-13T20:19:30Z",
        "summary": "Since the breakthrough of ChatGPT, large language models (LLMs) have garnered\nsignificant attention in the research community. With the development of LLMs,\nthe question of text style transfer for conversational models has emerged as a\nnatural extension, where chatbots may possess their own styles or even\ncharacters. However, standard evaluation metrics have not yet been established\nfor this new settings. This paper aims to address this issue by proposing the\nLMStyle Benchmark, a novel evaluation framework applicable to chat-style text\nstyle transfer (C-TST), that can measure the quality of style transfer for LLMs\nin an automated and scalable manner. In addition to conventional style strength\nmetrics, LMStyle Benchmark further considers a novel aspect of metrics called\nappropriateness, a high-level metrics take account of coherence, fluency and\nother implicit factors without the aid of reference samples. Our experiments\ndemonstrate that the new evaluation methods introduced by LMStyle Benchmark\nhave a higher correlation with human judgments in terms of appropriateness.\nBased on LMStyle Benchmark, we present a comprehensive list of evaluation\nresults for popular LLMs, including LLaMA, Alpaca, and Vicuna, reflecting their\nstylistic properties, such as formality and sentiment strength, along with\ntheir appropriateness.",
        "pdf_link": "https://arxiv.org/pdf/2403.08943v1.pdf"
    },
    {
        "title": "Bugs in Large Language Models Generated Code: An Empirical Study",
        "authors": [
            "Florian Tambon",
            "Arghavan Moradi Dakhel",
            "Amin Nikanjam",
            "Foutse Khomh",
            "Michel C. Desmarais",
            "Giuliano Antoniol"
        ],
        "published": "2024-03-13T20:12:01Z",
        "summary": "Large Language Models (LLMs) for code have gained significant attention\nrecently. They can generate code in different programming languages based on\nprovided prompts, fulfilling a long-lasting dream in Software Engineering (SE),\ni.e., automatic code generation. Similar to human-written code, LLM-generated\ncode is prone to bugs, and these bugs have not yet been thoroughly examined by\nthe community. Given the increasing adoption of LLM-based code generation tools\n(e.g., GitHub Copilot) in SE activities, it is critical to understand the\ncharacteristics of bugs contained in code generated by LLMs. This paper\nexamines a sample of 333 bugs collected from code generated using three leading\nLLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10\ndistinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake,\nPrompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object,\nWrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug\npatterns are presented in the form of a taxonomy. The identified bug patterns\nare validated using an online survey with 34 LLM practitioners and researchers.\nThe surveyed participants generally asserted the significance and prevalence of\nthe bug patterns. Researchers and practitioners can leverage these findings to\ndevelop effective quality assurance techniques for LLM-generated code. This\nstudy sheds light on the distinctive characteristics of LLM-generated code.",
        "pdf_link": "https://arxiv.org/pdf/2403.08937v2.pdf"
    },
    {
        "title": "Second-Order Information Matters: Revisiting Machine Unlearning for Large Language Models",
        "authors": [
            "Kang Gu",
            "Md Rafi Ur Rashid",
            "Najrin Sultana",
            "Shagufta Mehnaz"
        ],
        "published": "2024-03-13T18:57:30Z",
        "summary": "With the rapid development of Large Language Models (LLMs), we have witnessed\nintense competition among the major LLM products like ChatGPT, LLaMa, and\nGemini. However, various issues (e.g. privacy leakage and copyright violation)\nof the training corpus still remain underexplored. For example, the Times sued\nOpenAI and Microsoft for infringing on its copyrights by using millions of its\narticles for training. From the perspective of LLM practitioners, handling such\nunintended privacy violations can be challenging. Previous work addressed the\n``unlearning\" problem of LLMs using gradient information, while they mostly\nintroduced significant overheads like data preprocessing or lacked robustness.\nIn this paper, contrasting with the methods based on first-order information,\nwe revisit the unlearning problem via the perspective of second-order\ninformation (Hessian). Our unlearning algorithms, which are inspired by classic\nNewton update, are not only data-agnostic/model-agnostic but also proven to be\nrobust in terms of utility preservation or privacy guarantee. Through a\ncomprehensive evaluation with four NLP datasets as well as a case study on\nreal-world datasets, our methods consistently show superiority over the\nfirst-order methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.10557v1.pdf"
    },
    {
        "title": "Teaching Machines to Code: Smart Contract Translation with LLMs",
        "authors": [
            "Rabimba Karanjai",
            "Lei Xu",
            "Weidong Shi"
        ],
        "published": "2024-03-13T18:55:20Z",
        "summary": "The advent of large language models (LLMs) has marked a significant milestone\nin the realm of artificial intelligence, with their capabilities often matching\nor surpassing human expertise in various domains. Among these achievements,\ntheir adeptness in translation tasks stands out, closely mimicking the\nintricate and preliminary processes undertaken by human translators to ensure\nthe fidelity and quality of the translated content. Despite the advancements in\nutilizing LLMs for translating programming code across different languages, the\ndomain of smart contract translation, particularly into languages not\npreviously encountered by the LLM, remains largely unexplored. In our research,\nwe present a pioneering approach, SolMover, which harnesses the synergy of two\ndistinct LLMs within a unified framework. This framework is designed to grasp\ncoding principles and apply this understanding to the translation of code into\nan unfamiliar language. Our study delves into the capacity of LLMs to mimic\nhuman learning processes, offering an in-depth evaluation of our methodology\nfor converting smart contracts written in Solidity to Move, a language with\nlimited resources. The framework employs one LLM to decipher coding conventions\nfor the new language, creating a blueprint for the second LLM, which, lacking\nplanning abilities, possesses coding expertise. The empirical evidence from our\nexperiments suggests that SolMover substantially enhances performance compared\nto gpt-3.5-turbo-1106, and achieves superior results over competitors such as\nPalm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the\nefficacy of our bug mitigation strategy in elevating code quality across all\nmodels, even outside the SolMover framework.",
        "pdf_link": "https://arxiv.org/pdf/2403.09740v1.pdf"
    },
    {
        "title": "Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics",
        "authors": [
            "Tyler A. Chang",
            "Katrin Tomanek",
            "Jessica Hoffmann",
            "Nithum Thain",
            "Erin van Liemt",
            "Kathleen Meier-Hellstern",
            "Lucas Dixon"
        ],
        "published": "2024-03-13T18:47:00Z",
        "summary": "We explore a strategy to handle controversial topics in LLM-based chatbots\nbased on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the\nabsence of a single true answer and surface multiple perspectives. We frame\nthis as retrieval augmented generation, where perspectives are retrieved from a\nknowledge base and the LLM is tasked with generating a fluent and faithful\nresponse from the given perspectives. As a starting point, we use a\ndeterministic retrieval system and then focus on common LLM failure modes that\narise during this approach to text generation, namely hallucination and\ncoverage errors. We propose and evaluate three methods to detect such errors\nbased on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our\nresults demonstrate that LLM-based classifiers, even when trained only on\nsynthetic errors, achieve high error detection performance, with ROC AUC scores\nof 95.3% for hallucination and 90.5% for coverage error detection on\nunambiguous error cases. We show that when no training data is available, our\nother methods still yield good results on hallucination (84.0%) and coverage\nerror (85.2%) detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.08904v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation",
        "authors": [
            "Se-eun Yoon",
            "Zhankui He",
            "Jessica Maria Echterhoff",
            "Julian McAuley"
        ],
        "published": "2024-03-13T18:16:21Z",
        "summary": "Synthetic users are cost-effective proxies for real users in the evaluation\nof conversational recommender systems. Large language models show promise in\nsimulating human-like behavior, raising the question of their ability to\nrepresent a diverse population of users. We introduce a new protocol to measure\nthe degree to which language models can accurately emulate human behavior in\nconversational recommendation. This protocol is comprised of five tasks, each\ndesigned to evaluate a key property that a synthetic user should exhibit:\nchoosing which items to talk about, expressing binary preferences, expressing\nopen-ended preferences, requesting recommendations, and giving feedback.\nThrough evaluation of baseline simulators, we demonstrate these tasks\neffectively reveal deviations of language models from human behavior, and offer\ninsights on how to reduce the deviations with model selection and prompting\nstrategies.",
        "pdf_link": "https://arxiv.org/pdf/2403.09738v4.pdf"
    },
    {
        "title": "Cultural evolution in populations of Large Language Models",
        "authors": [
            "J\u00e9r\u00e9my Perez",
            "Corentin L\u00e9ger",
            "Marcela Ovando-Tellez",
            "Chris Foulon",
            "Joan Dussauld",
            "Pierre-Yves Oudeyer",
            "Cl\u00e9ment Moulin-Frier"
        ],
        "published": "2024-03-13T18:11:17Z",
        "summary": "Research in cultural evolution aims at providing causal explanations for the\nchange of culture over time. Over the past decades, this field has generated an\nimportant body of knowledge, using experimental, historical, and computational\nmethods. While computational models have been very successful at generating\ntestable hypotheses about the effects of several factors, such as population\nstructure or transmission biases, some phenomena have so far been more complex\nto capture using agent-based and formal models. This is in particular the case\nfor the effect of the transformations of social information induced by evolved\ncognitive mechanisms. We here propose that leveraging the capacity of Large\nLanguage Models (LLMs) to mimic human behavior may be fruitful to address this\ngap. On top of being an useful approximation of human cultural dynamics,\nmulti-agents models featuring generative agents are also important to study for\ntheir own sake. Indeed, as artificial agents are bound to participate more and\nmore to the evolution of culture, it is crucial to better understand the\ndynamics of machine-generated cultural evolution. We here present a framework\nfor simulating cultural evolution in populations of LLMs, allowing the\nmanipulation of variables known to be important in cultural evolution, such as\nnetwork structure, personality, and the way social information is aggregated\nand transformed. The software we developed for conducting these simulations is\nopen-source and features an intuitive user-interface, which we hope will help\nto build bridges between the fields of cultural evolution and generative\nartificial intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.08882v1.pdf"
    },
    {
        "title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation",
        "authors": [
            "Minbin Huang",
            "Yanxin Long",
            "Xinchi Deng",
            "Ruihang Chu",
            "Jiangfeng Xiong",
            "Xiaodan Liang",
            "Hong Cheng",
            "Qinglin Lu",
            "Wei Liu"
        ],
        "published": "2024-03-13T18:00:01Z",
        "summary": "Text-to-image (T2I) generation models have significantly advanced in recent\nyears. However, effective interaction with these models is challenging for\naverage users due to the need for specialized prompt engineering knowledge and\nthe inability to perform multi-turn image generation, hindering a dynamic and\niterative creation process. Recent attempts have tried to equip Multi-modal\nLarge Language Models (MLLMs) with T2I models to bring the user's natural\nlanguage instructions into reality. Hence, the output modality of MLLMs is\nextended, and the multi-turn generation quality of T2I models is enhanced\nthanks to the strong multi-modal comprehension ability of MLLMs. However, many\nof these works face challenges in identifying correct output modalities and\ngenerating coherent images accordingly as the number of output modalities\nincreases and the conversations go deeper. Therefore, we propose DialogGen, an\neffective pipeline to align off-the-shelf MLLMs and T2I models to build a\nMulti-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image\ngeneration. It is composed of drawing prompt alignment, careful training data\ncuration, and error correction. Moreover, as the field of MIDS flourishes,\ncomprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms\nof output modality correctness and multi-modal output coherence. To address\nthis issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a\ncomprehensive bilingual benchmark designed to assess the ability of MLLMs to\ngenerate accurate and coherent multi-modal content that supports image editing.\nIt contains two evaluation metrics to measure the model's ability to switch\nmodalities and the coherence of the output images. Our extensive experiments on\nDialogBen and user study demonstrate the effectiveness of DialogGen compared\nwith other State-of-the-Art models.",
        "pdf_link": "https://arxiv.org/pdf/2403.08857v1.pdf"
    },
    {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "authors": [
            "Adam Ibrahim",
            "Benjamin Th\u00e9rien",
            "Kshitij Gupta",
            "Mats L. Richter",
            "Quentin Anthony",
            "Timoth\u00e9e Lesort",
            "Eugene Belilovsky",
            "Irina Rish"
        ],
        "published": "2024-03-13T17:58:57Z",
        "summary": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
        "pdf_link": "https://arxiv.org/pdf/2403.08763v3.pdf"
    },
    {
        "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
        "authors": [
            "Jingling Li",
            "Zeyu Tang",
            "Xiaoyu Liu",
            "Peter Spirtes",
            "Kun Zhang",
            "Liu Leqi",
            "Yang Liu"
        ],
        "published": "2024-03-13T17:46:28Z",
        "summary": "Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.",
        "pdf_link": "https://arxiv.org/pdf/2403.08743v1.pdf"
    },
    {
        "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization",
        "authors": [
            "Renjie Pi",
            "Tianyang Han",
            "Wei Xiong",
            "Jipeng Zhang",
            "Runtao Liu",
            "Rui Pan",
            "Tong Zhang"
        ],
        "published": "2024-03-13T17:29:45Z",
        "summary": "Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.08730v2.pdf"
    },
    {
        "title": "SOTOPIA-$\u03c0$: Interactive Learning of Socially Intelligent Language Agents",
        "authors": [
            "Ruiyi Wang",
            "Haofei Yu",
            "Wenxin Zhang",
            "Zhengyang Qi",
            "Maarten Sap",
            "Graham Neubig",
            "Yonatan Bisk",
            "Hao Zhu"
        ],
        "published": "2024-03-13T17:17:48Z",
        "summary": "Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.",
        "pdf_link": "https://arxiv.org/pdf/2403.08715v2.pdf"
    },
    {
        "title": "TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning",
        "authors": [
            "Shangding Gu",
            "Alois Knoll",
            "Ming Jin"
        ],
        "published": "2024-03-13T16:57:57Z",
        "summary": "The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n$5.73\\%$ of WizardLM's total), along with enhanced capabilities of LLMs in\ncrafting and comprehending complex instructions compared to strong baselines,\nand substantially improved model privacy protection.",
        "pdf_link": "https://arxiv.org/pdf/2403.08694v1.pdf"
    },
    {
        "title": "Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records",
        "authors": [
            "Erlend Frayling",
            "Jake Lever",
            "Graham McDonald"
        ],
        "published": "2024-03-13T16:17:09Z",
        "summary": "The challenge of accessing historical patient data for clinical research,\nwhile adhering to privacy regulations, is a significant obstacle in medical\nscience. An innovative approach to circumvent this issue involves utilising\nsynthetic medical records that mirror real patient data without compromising\nindividual privacy. The creation of these synthetic datasets, particularly\nwithout using actual patient data to train Large Language Models (LLMs),\npresents a novel solution as gaining access to sensitive patient information to\ntrain models is also a challenge. This study assesses the capability of the\nLlama 2 LLM to create synthetic medical records that accurately reflect real\npatient information, employing zero-shot and few-shot prompting strategies for\ncomparison against fine-tuned methodologies that do require sensitive patient\ndata during training. We focus on generating synthetic narratives for the\nHistory of Present Illness section, utilising data from the MIMIC-IV dataset\nfor comparison. In this work introduce a novel prompting technique that\nleverages a chain-of-thought approach, enhancing the model's ability to\ngenerate more accurate and contextually relevant medical narratives without\nprior fine-tuning. Our findings suggest that this chain-of-thought prompted\napproach allows the zero-shot model to achieve results on par with those of\nfine-tuned models, based on Rouge metrics evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2403.08664v2.pdf"
    },
    {
        "title": "MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models",
        "authors": [
            "Subash Neupane",
            "Shaswata Mitra",
            "Sudip Mittal",
            "Noorbakhsh Amiri Golilarz",
            "Shahram Rahimi",
            "Amin Amirlatifi"
        ],
        "published": "2024-03-13T15:20:30Z",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.",
        "pdf_link": "https://arxiv.org/pdf/2403.08607v1.pdf"
    },
    {
        "title": "DevBench: A Comprehensive Benchmark for Software Development",
        "authors": [
            "Bowen Li",
            "Wenhan Wu",
            "Ziwei Tang",
            "Lin Shi",
            "John Yang",
            "Jinyang Li",
            "Shunyu Yao",
            "Chen Qian",
            "Binyuan Hui",
            "Qicheng Zhang",
            "Zhiyin Yu",
            "He Du",
            "Ping Yang",
            "Dahua Lin",
            "Chao Peng",
            "Kai Chen"
        ],
        "published": "2024-03-13T15:13:44Z",
        "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced their coding capabilities. However, existing benchmarks predominantly\nfocused on simplified or isolated aspects of programming, such as single-file\ncode generation or repository issue debugging, falling short of measuring the\nfull spectrum of challenges raised by real-world programming activities. To\nthis end, we propose DevBench, a comprehensive benchmark that evaluates LLMs\nacross various stages of the software development lifecycle, including software\ndesign, environment setup, implementation, acceptance testing, and unit\ntesting. DevBench features a wide range of programming languages and domains,\nhigh-quality data collection, and carefully designed and verified metrics for\neach task. Empirical studies show that current LLMs, including GPT-4-Turbo,\nfail to solve the challenges presented within DevBench. Analyses reveal that\nmodels struggle with understanding the complex structures in the repository,\nmanaging the compilation process, and grasping advanced programming concepts.\nOur findings offer actionable insights for the future development of LLMs\ntoward real-world programming applications. Our benchmark is available at\nhttps://github.com/open-compass/DevBench",
        "pdf_link": "https://arxiv.org/pdf/2403.08604v2.pdf"
    },
    {
        "title": "Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments",
        "authors": [
            "Sitao Cheng",
            "Ziyuan Zhuang",
            "Yong Xu",
            "Fangkai Yang",
            "Chaoyun Zhang",
            "Xiaoting Qin",
            "Xiang Huang",
            "Ling Chen",
            "Qingwei Lin",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "published": "2024-03-13T14:59:07Z",
        "summary": "Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA datasets\nand two TableQA datasets show the effectiveness of Readi, significantly\nsurpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available upon publication.",
        "pdf_link": "https://arxiv.org/pdf/2403.08593v1.pdf"
    },
    {
        "title": "Non-discrimination Criteria for Generative Language Models",
        "authors": [
            "Sara Sterlie",
            "Nina Weng",
            "Aasa Feragen"
        ],
        "published": "2024-03-13T14:19:08Z",
        "summary": "Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.08564v1.pdf"
    },
    {
        "title": "Language models scale reliably with over-training and on downstream tasks",
        "authors": [
            "Samir Yitzhak Gadre",
            "Georgios Smyrnis",
            "Vaishaal Shankar",
            "Suchin Gururangan",
            "Mitchell Wortsman",
            "Rulin Shao",
            "Jean Mercat",
            "Alex Fang",
            "Jeffrey Li",
            "Sedrick Keh",
            "Rui Xin",
            "Marianna Nezhurina",
            "Igor Vasiljevic",
            "Jenia Jitsev",
            "Alexandros G. Dimakis",
            "Gabriel Ilharco",
            "Shuran Song",
            "Thomas Kollar",
            "Yair Carmon",
            "Achal Dave",
            "Reinhard Heckel",
            "Niklas Muennighoff",
            "Ludwig Schmidt"
        ],
        "published": "2024-03-13T13:54:00Z",
        "summary": "Scaling laws are useful guides for developing language models, but there are\nstill gaps between current scaling studies and how language models are\nultimately trained and evaluated. For instance, scaling is usually studied in\nthe compute-optimal training regime (i.e., \"Chinchilla optimal\" regime);\nhowever, in practice, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but\nultimately models are compared based on downstream task performance. In this\npaper, we address both shortcomings. To do so, we create a testbed of 104\nmodels with 0.011B to 6.9B parameters trained with various numbers of tokens on\nthree data distributions. First, we investigate scaling in the over-trained\nregime. We fit scaling laws that extrapolate in both the number of model\nparameters and the ratio of training tokens to parameters. This enables us to\npredict the validation loss of a 1.4B parameter, 900B token run (i.e.,\n32$\\times$ over-trained) and a 6.9B parameter, 138B token\nrun$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.\nSecond, we relate the perplexity of a language model to its downstream task\nperformance via a power law. We use this law to predict top-1 error averaged\nover downstream tasks for the two aforementioned models using experiments that\ntake 20$\\times$ less compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.",
        "pdf_link": "https://arxiv.org/pdf/2403.08540v1.pdf"
    },
    {
        "title": "Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator",
        "authors": [
            "Yusheng Liao",
            "Yutong Meng",
            "Yuhao Wang",
            "Hongcheng Liu",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published": "2024-03-13T13:04:58Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nhuman interactions, yet their application within the medical field remains\ninsufficiently explored. Previous works mainly focus on the performance of\nmedical knowledge with examinations, which is far from the realistic scenarios,\nfalling short in assessing the abilities of LLMs on clinical tasks. In the\nquest to enhance the application of Large Language Models (LLMs) in healthcare,\nthis paper introduces the Automated Interactive Evaluation (AIE) framework and\nthe State-Aware Patient Simulator (SAPS), targeting the gap between traditional\nLLM evaluations and the nuanced demands of clinical practice. Unlike prior\nmethods that rely on static medical knowledge assessments, AIE and SAPS provide\na dynamic, realistic platform for assessing LLMs through multi-turn\ndoctor-patient simulations. This approach offers a closer approximation to real\nclinical scenarios and allows for a detailed analysis of LLM behaviors in\nresponse to complex patient interactions. Our extensive experimental validation\ndemonstrates the effectiveness of the AIE framework, with outcomes that align\nwell with human evaluations, underscoring its potential to revolutionize\nmedical LLM testing for improved healthcare delivery.",
        "pdf_link": "https://arxiv.org/pdf/2403.08495v2.pdf"
    },
    {
        "title": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking",
        "authors": [
            "Ming Dong",
            "Yujing Chen",
            "Miao Zhang",
            "Hao Sun",
            "Tingting He"
        ],
        "published": "2024-03-13T12:55:43Z",
        "summary": "Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.",
        "pdf_link": "https://arxiv.org/pdf/2403.08492v1.pdf"
    },
    {
        "title": "Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning",
        "authors": [
            "Ming Dong",
            "Kang Xue",
            "Bolong Zheng",
            "Tingting He"
        ],
        "published": "2024-03-13T12:50:23Z",
        "summary": "In view of the huge number of parameters of Large language models (LLMs) ,\ntuning all parameters is very costly, and accordingly fine-tuning specific\nparameters is more sensible. Most of parameter efficient fine-tuning (PEFT)\nconcentrate on parameter selection strategies, such as additive method,\nselective method and reparametrization-based method. However, there are few\nmethods that consider the impact of data samples on parameter selecting, such\nas Fish Mask based method. Fish Mask randomly choose a part of data samples and\ntreat them equally during parameter selection, which is unable to dynamically\nselect optimal parameters for inconstant data distributions. In this work, we\nadopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline\nI}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline\nD}$ecreasing) algorithm to search the best setting of sample-parameter pair for\nFISH Mask. In each iteration, by searching the set of samples and parameters\nwith larger Fish information, IRD can find better sample-parameter pair in most\nscale. We demonstrate the effectiveness and rationality of proposed strategy by\nconducting experiments on GLUE benchmark. Experimental results show our\nstrategy optimizes the parameter selection and achieves preferable performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.08484v1.pdf"
    },
    {
        "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks",
        "authors": [
            "Guy Amit",
            "Abigail Goldsteen",
            "Ariel Farkash"
        ],
        "published": "2024-03-13T12:46:51Z",
        "summary": "Natural language processing models have experienced a significant upsurge in\nrecent years, with numerous applications being built upon them. Many of these\napplications require fine-tuning generic base models on customized, proprietary\ndatasets. This fine-tuning data is especially likely to contain personal or\nsensitive information about individuals, resulting in increased privacy risk.\nMembership inference attacks are the most commonly employed attack to assess\nthe privacy leakage of a machine learning model. However, limited research is\navailable on the factors that affect the vulnerability of language models to\nthis kind of attack, or on the applicability of different defense strategies in\nthe language domain. We provide the first systematic review of the\nvulnerability of fine-tuned large language models to membership inference\nattacks, the various factors that come into play, and the effectiveness of\ndifferent defense strategies. We find that some training methods provide\nsignificantly reduced privacy risk, with the combination of differential\nprivacy and low-rank adaptors achieving the best privacy protection against\nthese attacks.",
        "pdf_link": "https://arxiv.org/pdf/2403.08481v1.pdf"
    },
    {
        "title": "Software Vulnerability and Functionality Assessment using LLMs",
        "authors": [
            "Rasmus Ingemann Tuffveson Jensen",
            "Vali Tawosi",
            "Salwa Alamir"
        ],
        "published": "2024-03-13T11:29:13Z",
        "summary": "While code review is central to the software development process, it can be\ntedious and expensive to carry out. In this paper, we investigate whether and\nhow Large Language Models (LLMs) can aid with code reviews. Our investigation\nfocuses on two tasks that we argue are fundamental to good reviews: (i)\nflagging code with security vulnerabilities and (ii) performing software\nfunctionality validation, i.e., ensuring that code meets its intended\nfunctionality. To test performance on both tasks, we use zero-shot and\nchain-of-thought prompting to obtain final ``approve or reject''\nrecommendations. As data, we employ seminal code generation datasets (HumanEval\nand MBPP) along with expert-written code snippets with security vulnerabilities\nfrom the Common Weakness Enumeration (CWE). Our experiments consider a mixture\nof three proprietary models from OpenAI and smaller open-source LLMs. We find\nthat the former outperforms the latter by a large margin. Motivated by\npromising results, we finally ask our models to provide detailed descriptions\nof security vulnerabilities. Results show that 36.7% of LLM-generated\ndescriptions can be associated with true CWE vulnerabilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.08429v1.pdf"
    },
    {
        "title": "Tastle: Distract Large Language Models for Automatic Jailbreak Attack",
        "authors": [
            "Zeguan Xiao",
            "Yan Yang",
            "Guanhua Chen",
            "Yun Chen"
        ],
        "published": "2024-03-13T11:16:43Z",
        "summary": "Large language models (LLMs) have achieved significant advances in recent\ndays. Extensive efforts have been made before the public release of LLMs to\nalign their behaviors with human values. The primary goal of alignment is to\nensure their helpfulness, honesty and harmlessness. However, even meticulously\naligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,\nleading to unintended behaviors. The jailbreak is to intentionally develop a\nmalicious prompt that escapes from the LLM security restrictions to produce\nuncensored detrimental contents. Previous works explore different jailbreak\nmethods for red teaming LLMs, yet they encounter challenges regarding to\neffectiveness and scalability. In this work, we propose Tastle, a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.",
        "pdf_link": "https://arxiv.org/pdf/2403.08424v1.pdf"
    },
    {
        "title": "Do Large Language Models Solve ARC Visual Analogies Like People Do?",
        "authors": [
            "Gustaw Opie\u0142ka",
            "Hannes Rosenbusch",
            "Veerle Vijverberg",
            "Claire E. Stevenson"
        ],
        "published": "2024-03-13T09:48:13Z",
        "summary": "The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test\ndesigned for humans and machines (Chollet, 2019). We compared human and large\nlanguage model (LLM) performance on a new child-friendly set of ARC items.\nResults show that both children and adults outperform most LLMs on these tasks.\nError analysis revealed a similar \"fallback\" solution strategy in LLMs and\nyoung children, where part of the analogy is simply copied. In addition, we\nfound two other error types, one based on seemingly grasping key concepts\n(e.g., Inside-Outside) and the other based on simple combinations of analogy\ninput matrices. On the whole, \"concept\" errors were more common in humans, and\n\"matrix\" errors were more common in LLMs. This study sheds new light on LLM\nreasoning ability and the extent to which we can use error analyses and\ncomparisons with human development to understand how LLMs solve visual\nanalogies.",
        "pdf_link": "https://arxiv.org/pdf/2403.09734v1.pdf"
    },
    {
        "title": "SMART: Submodular Data Mixture Strategy for Instruction Tuning",
        "authors": [
            "H S V N S Kowndinya Renduchintala",
            "Sumit Bhatia",
            "Ganesh Ramakrishnan"
        ],
        "published": "2024-03-13T09:31:50Z",
        "summary": "Instruction Tuning involves finetuning a language model on a collection of\ninstruction-formatted datasets in order to enhance the generalizability of the\nmodel to unseen tasks. Studies have shown the importance of balancing different\ntask proportions during finetuning, but finding the right balance remains\nchallenging. Unfortunately, there's currently no systematic method beyond\nmanual tuning or relying on practitioners' intuition. In this paper, we\nintroduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a\nnovel data mixture strategy which makes use of a submodular function to assign\nimportance scores to tasks which are then used to determine the mixture\nweights. Given a fine-tuning budget, SMART redistributes the budget among tasks\nand selects non-redundant samples from each task. Experimental results\ndemonstrate that SMART significantly outperforms traditional methods such as\nexamples proportional mixing and equal mixing. Furthermore, SMART facilitates\nthe creation of data mixtures based on a few representative subsets of tasks\nalone and through task pruning analysis, we reveal that in a limited budget\nsetting, allocating budget among a subset of representative tasks yields\nsuperior performance compared to distributing the budget among all tasks. The\ncode for reproducing our results is open-sourced at\nhttps://github.com/kowndinya-renduchintala/SMART.",
        "pdf_link": "https://arxiv.org/pdf/2403.08370v1.pdf"
    },
    {
        "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model",
        "authors": [
            "Cheng Chen",
            "Junchen Zhu",
            "Xu Luo",
            "Hengtao Shen",
            "Lianli Gao",
            "Jingkuan Song"
        ],
        "published": "2024-03-13T08:54:31Z",
        "summary": "Instruction tuning represents a prevalent strategy employed by Multimodal\nLarge Language Models (MLLMs) to align with human instructions and adapt to new\ntasks. Nevertheless, MLLMs encounter the challenge of adapting to users'\nevolving knowledge and demands. Therefore, how to retain existing skills while\nacquiring new knowledge needs to be investigated. In this paper, we present a\ncomprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess\nexisting MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10\ncommonly used datasets spanning 8 task categories, ensuring a diverse range of\ninstructions and tasks. Besides, the trained model is evaluated from two\naspects: Instruction Following and General Knowledge, which assess the\nalignment with human intention and knowledge preserved for reasoning,\nrespectively. Experiments on CoIN demonstrate that current powerful MLLMs still\nsuffer catastrophic forgetting, and the failure in intention alignment assumes\nthe main responsibility, instead of the knowledge forgetting. To this end, we\nintroduce MoELoRA to MLLMs which is effective to retain the previous\ninstruction alignment. Experimental results consistently illustrate the\nforgetting decreased from this method on CoIN.",
        "pdf_link": "https://arxiv.org/pdf/2403.08350v1.pdf"
    },
    {
        "title": "From human experts to machines: An LLM supported approach to ontology and knowledge graph construction",
        "authors": [
            "Vamsi Krishna Kommineni",
            "Birgitta K\u00f6nig-Ries",
            "Sheeba Samuel"
        ],
        "published": "2024-03-13T08:50:15Z",
        "summary": "The conventional process of building Ontologies and Knowledge Graphs (KGs)\nheavily relies on human domain experts to define entities and relationship\ntypes, establish hierarchies, maintain relevance to the domain, fill the ABox\n(or populate with instances), and ensure data quality (including amongst others\naccuracy and completeness). On the other hand, Large Language Models (LLMs)\nhave recently gained popularity for their ability to understand and generate\nhuman-like natural language, offering promising ways to automate aspects of\nthis process. This work explores the (semi-)automatic construction of KGs\nfacilitated by open-source LLMs. Our pipeline involves formulating competency\nquestions (CQs), developing an ontology (TBox) based on these CQs, constructing\nKGs using the developed ontology, and evaluating the resultant KG with minimal\nto no involvement of human experts. We showcase the feasibility of our\nsemi-automated pipeline by creating a KG on deep learning methodologies by\nexploiting scholarly publications. To evaluate the answers generated via\nRetrieval-Augmented-Generation (RAG) as well as the KG concepts automatically\nextracted using LLMs, we design a judge LLM, which rates the generated content\nbased on ground truth. Our findings suggest that employing LLMs could\npotentially reduce the human effort involved in the construction of KGs,\nalthough a human-in-the-loop approach is recommended to evaluate automatically\ngenerated KGs.",
        "pdf_link": "https://arxiv.org/pdf/2403.08345v1.pdf"
    },
    {
        "title": "LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments",
        "authors": [
            "Maonan Wang",
            "Aoyu Pang",
            "Yuheng Kan",
            "Man-On Pun",
            "Chung Shue Chen",
            "Bo Huang"
        ],
        "published": "2024-03-13T08:41:55Z",
        "summary": "Traffic congestion in metropolitan areas presents a formidable challenge with\nfar-reaching economic, environmental, and societal ramifications. Therefore,\neffective congestion management is imperative, with traffic signal control\n(TSC) systems being pivotal in this endeavor. Conventional TSC systems,\ndesigned upon rule-based algorithms or reinforcement learning (RL), frequently\nexhibit deficiencies in managing the complexities and variabilities of urban\ntraffic flows, constrained by their limited capacity for adaptation to\nunfamiliar scenarios. In response to these limitations, this work introduces an\ninnovative approach that integrates Large Language Models (LLMs) into TSC,\nharnessing their advanced reasoning and decision-making faculties.\nSpecifically, a hybrid framework that augments LLMs with a suite of perception\nand decision-making tools is proposed, facilitating the interrogation of both\nthe static and dynamic traffic information. This design places the LLM at the\ncenter of the decision-making process, combining external traffic data with\nestablished TSC methods. Moreover, a simulation platform is developed to\ncorroborate the efficacy of the proposed framework. The findings from our\nsimulations attest to the system's adeptness in adjusting to a multiplicity of\ntraffic environments without the need for additional training. Notably, in\ncases of Sensor Outage (SO), our approach surpasses conventional RL-based\nsystems by reducing the average waiting time by $20.4\\%$. This research\nsignifies a notable advance in TSC strategies and paves the way for the\nintegration of LLMs into real-world, dynamic scenarios, highlighting their\npotential to revolutionize traffic management. The related code is available at\n\\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.",
        "pdf_link": "https://arxiv.org/pdf/2403.08337v1.pdf"
    },
    {
        "title": "Knowledge Conflicts for LLMs: A Survey",
        "authors": [
            "Rongwu Xu",
            "Zehan Qi",
            "Cunxiang Wang",
            "Hongru Wang",
            "Yue Zhang",
            "Wei Xu"
        ],
        "published": "2024-03-13T08:02:23Z",
        "summary": "This survey provides an in-depth analysis of knowledge conflicts for large\nlanguage models (LLMs), highlighting the complex challenges they encounter when\nblending contextual and parametric knowledge. Our focus is on three categories\nof knowledge conflicts: context-memory, inter-context, and intra-memory\nconflict. These conflicts can significantly impact the trustworthiness and\nperformance of LLMs, especially in real-world applications where noise and\nmisinformation are common. By categorizing these conflicts, exploring the\ncauses, examining the behaviors of LLMs under such conflicts, and reviewing\navailable solutions, this survey aims to shed light on strategies for improving\nthe robustness of LLMs, thereby serving as a valuable resource for advancing\nresearch in this evolving area.",
        "pdf_link": "https://arxiv.org/pdf/2403.08319v1.pdf"
    },
    {
        "title": "OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models",
        "authors": [
            "Haomin Wen",
            "Zhenjie Wei",
            "Yan Lin",
            "Jiyuan Wang",
            "Yuxuan Liang",
            "Huaiyu Wan"
        ],
        "published": "2024-03-13T07:52:31Z",
        "summary": "The rapid development of Large Language Models (LLMs) has facilitated a\nvariety of applications from different domains. In this technical report, we\nexplore the integration of LLMs and the popular academic writing tool,\nOverleaf, to enhance the efficiency and quality of academic writing. To achieve\nthe above goal, there are three challenges: i) including seamless interaction\nbetween Overleaf and LLMs, ii) establishing reliable communication with the LLM\nprovider, and iii) ensuring user privacy. To address these challenges, we\npresent OverleafCopilot, the first-ever tool (i.e., a browser extension) that\nseamlessly integrates LLMs and Overleaf, enabling researchers to leverage the\npower of LLMs while writing papers. Specifically, we first propose an effective\nframework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a\nwebsite for researchers to easily find and share high-quality up-to-date\nprompts. Thirdly, we propose an agent command system to help researchers\nquickly build their customizable agents. OverleafCopilot\n(https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb\n) has been on the Chrome Extension Store, which now serves thousands of\nresearchers. Additionally, the code of PromptGenius is released at\nhttps://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the\npotential to revolutionize academic writing practices, empowering researchers\nto produce higher-quality papers in less time.",
        "pdf_link": "https://arxiv.org/pdf/2403.09733v1.pdf"
    },
    {
        "title": "Is Context Helpful for Chat Translation Evaluation?",
        "authors": [
            "Sweta Agrawal",
            "Amin Farajian",
            "Patrick Fernandes",
            "Ricardo Rei",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2024-03-13T07:49:50Z",
        "summary": "Despite the recent success of automatic metrics for assessing translation\nquality, their application in evaluating the quality of machine-translated\nchats has been limited. Unlike more structured texts like news, chat\nconversations are often unstructured, short, and heavily reliant on contextual\ninformation. This poses questions about the reliability of existing\nsentence-level metrics in this domain as well as the role of context in\nassessing the translation quality. Motivated by this, we conduct a\nmeta-evaluation of existing sentence-level automatic metrics, primarily\ndesigned for structured domains such as news, to assess the quality of\nmachine-translated chats. We find that reference-free metrics lag behind\nreference-based ones, especially when evaluating translation quality in\nout-of-English settings. We then investigate how incorporating conversational\ncontextual information in these metrics affects their performance. Our findings\nshow that augmenting neural learned metrics with contextual information helps\nimprove correlation with human judgments in the reference-free scenario and\nwhen evaluating translations in out-of-English settings. Finally, we propose a\nnew evaluation metric, Context-MQM, that utilizes bilingual context with a\nlarge language model (LLM) and further validate that adding context helps even\nfor LLM-based evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.08314v1.pdf"
    },
    {
        "title": "StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses",
        "authors": [
            "Jia-Nan Li",
            "Quan Tu",
            "Cunli Mao",
            "Zhengtao Yu",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "published": "2024-03-13T07:44:14Z",
        "summary": "Standard Large Language Models (LLMs) struggle with handling dialogues with\nlong contexts due to efficiency and consistency issues. According to our\nobservation, dialogue contexts are highly structured, and the special token of\n\\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate\ninformation. We refer to the EoU tokens as ``conversational attention sinks''\n(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which\ncompresses long dialogue history into conv-attn sinks with minimal losses, and\nthus reduces computational complexity quadratically with the number of sinks\n(i.e., the number of utterances). Current LLMs already demonstrate the ability\nto handle long context window, e.g., a window size of 200k or more. To this\nend, by compressing utterances into EoUs, our method has the potential to\nhandle more than 200k of utterances, resulting in a prolonged dialogue\nlearning. In order to minimize information losses from reconstruction after\ncompression, we design two learning strategies of short-memory reconstruction\n(SMR) and long-memory reactivation (LMR). Our method outperforms strong\nbaselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing\nmemory usage by 18 $\\times$ compared to dense attention recomputation.",
        "pdf_link": "https://arxiv.org/pdf/2403.08312v1.pdf"
    },
    {
        "title": "HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback",
        "authors": [
            "Ang Li",
            "Qiugen Xiao",
            "Peng Cao",
            "Jian Tang",
            "Yi Yuan",
            "Zijie Zhao",
            "Xiaoyuan Chen",
            "Liang Zhang",
            "Xiangyang Li",
            "Kaitong Yang",
            "Weidong Guo",
            "Yukang Gan",
            "Xu Yu",
            "Daniell Wang",
            "Ying Shan"
        ],
        "published": "2024-03-13T07:38:20Z",
        "summary": "Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter\nannotation cycles and lower costs over Reinforcement Learning from Human\nFeedback (RLHF), making it highly efficient during the rapid strategy iteration\nperiods of large language model (LLM) training. Using ChatGPT as a labeler to\nprovide feedback on open-domain prompts in RLAIF training, we observe an\nincrease in human evaluators' preference win ratio for model responses, but a\ndecrease in evaluators' satisfaction rate. Analysis suggests that the decrease\nin satisfaction rate is mainly due to some responses becoming less helpful,\nparticularly in terms of correctness and truthfulness, highlighting practical\nlimitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement\nLearning from AI Feedback (HRLAIF). This method enhances the accuracy of AI\nannotations for responses, making the model's helpfulness more robust in\ntraining process. Additionally, it employs AI for Red Teaming, further\nimproving the model's harmlessness. Human evaluation results show that HRLAIF\ninherits the ability of RLAIF to enhance human preference for outcomes at a low\ncost while also improving the satisfaction rate of responses. Compared to the\npolicy model before Reinforcement Learning (RL), it achieves an increase of\n2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of\n4.58\\% in satisfaction rate after basic RLAIF.",
        "pdf_link": "https://arxiv.org/pdf/2403.08309v2.pdf"
    },
    {
        "title": "Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform",
        "authors": [
            "Mingyue Cheng",
            "Hao Zhang",
            "Jiqian Yang",
            "Qi Liu",
            "Li Li",
            "Xin Huang",
            "Liwei Song",
            "Zhi Li",
            "Zhenya Huang",
            "Enhong Chen"
        ],
        "published": "2024-03-13T07:31:20Z",
        "summary": "Large language model evaluation plays a pivotal role in the enhancement of\nits capacity. Previously, numerous methods for evaluating large language models\nhave been proposed in this area. Despite their effectiveness, these existing\nworks mainly focus on assessing objective questions, overlooking the capability\nto evaluate subjective questions which is extremely common for large language\nmodels. Additionally, these methods predominantly utilize centralized datasets\nfor evaluation, with question banks concentrated within the evaluation\nplatforms themselves. Moreover, the evaluation processes employed by these\nplatforms often overlook personalized factors, neglecting to consider the\nindividual characteristics of both the evaluators and the models being\nevaluated. To address these limitations, we propose a novel anonymous\ncrowd-sourcing evaluation platform, BingJian, for large language models that\nemploys a competitive scoring mechanism where users participate in ranking\nmodels based on their performance. This platform stands out not only for its\nsupport of centralized evaluations to assess the general capabilities of models\nbut also for offering an open evaluation gateway. Through this gateway, users\nhave the opportunity to submit their questions, testing the models on a\npersonalized and potentially broader range of capabilities. Furthermore, our\nplatform introduces personalized evaluation scenarios, leveraging various forms\nof human-computer interaction to assess large language models in a manner that\naccounts for individual user preferences and contexts. The demonstration of\nBingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.",
        "pdf_link": "https://arxiv.org/pdf/2403.08305v1.pdf"
    },
    {
        "title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale",
        "authors": [
            "Xiang Hu",
            "Pengyu Ji",
            "Qingyang Zhu",
            "Wei Wu",
            "Kewei Tu"
        ],
        "published": "2024-03-13T06:54:47Z",
        "summary": "A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.",
        "pdf_link": "https://arxiv.org/pdf/2403.08293v2.pdf"
    },
    {
        "title": "CleanAgent: Automating Data Standardization with LLM-based Agents",
        "authors": [
            "Danrui Qi",
            "Jiannan Wang"
        ],
        "published": "2024-03-13T06:54:15Z",
        "summary": "Data standardization is a crucial part in data science life cycle. While\ntools like Pandas offer robust functionalities, their complexity and the manual\neffort required for customizing code to diverse column types pose significant\nchallenges. Although large language models (LLMs) like ChatGPT have shown\npromise in automating this process through natural language understanding and\ncode generation, it still demands expert-level programming knowledge and\ncontinuous interaction for prompt refinement. To solve these challenges, our\nkey idea is to propose a Python library with declarative, unified APIs for\nstandardizing column types, simplifying the code generation of LLM with concise\nAPI calls. We first propose Dataprep.Clean which is written as a component of\nthe Dataprep Library, offers a significant reduction in complexity by enabling\nthe standardization of specific column types with a single line of code. Then\nwe introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based\nagents to automate the data standardization process. With CleanAgent, data\nscientists need only provide their requirements once, allowing for a\nhands-free, automatic standardization process.",
        "pdf_link": "https://arxiv.org/pdf/2403.08291v1.pdf"
    },
    {
        "title": "Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation",
        "authors": [
            "Zhonghan Zhao",
            "Kewei Chen",
            "Dongxu Guo",
            "Wenhao Chai",
            "Tian Ye",
            "Yanting Zhang",
            "Gaoang Wang"
        ],
        "published": "2024-03-13T06:22:17Z",
        "summary": "Due to the dynamic and unpredictable open-world setting, navigating complex\nenvironments in Minecraft poses significant challenges for multi-agent systems.\nAgents must interact with the environment and coordinate their actions with\nother agents to achieve common objectives. However, traditional approaches\noften struggle to efficiently manage inter-agent communication and task\ndistribution, crucial for effective multi-agent navigation. Furthermore,\nprocessing and integrating multi-modal information (such as visual, textual,\nand auditory data) is essential for agents to comprehend their goals and\nnavigate the environment successfully and fully. To address this issue, we\ndesign the HAS framework to auto-organize groups of LLM-based agents to\ncomplete navigation tasks. In our approach, we devise a hierarchical\nauto-organizing navigation system, which is characterized by 1) a hierarchical\nsystem for multi-agent organization, ensuring centralized planning and\ndecentralized execution; 2) an auto-organizing and intra-communication\nmechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal\ninformation platform, facilitating multi-modal perception to perform the three\nnavigation tasks with one system. To assess organizational behavior, we design\na series of navigation tasks in the Minecraft environment, which includes\nsearching and exploring. We aim to develop embodied organizations that push the\nboundaries of embodied AI, moving it towards a more human-like organizational\nstructure.",
        "pdf_link": "https://arxiv.org/pdf/2403.08282v2.pdf"
    },
    {
        "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
        "authors": [
            "Ning Ding",
            "Yulin Chen",
            "Ganqu Cui",
            "Xingtai Lv",
            "Weilin Zhao",
            "Ruobing Xie",
            "Bowen Zhou",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-03-13T06:18:48Z",
        "summary": "Underlying data distributions of natural language, programming code, and\nmathematical symbols vary vastly, presenting a complex challenge for large\nlanguage models (LLMs) that strive to achieve high performance across all three\ndomains simultaneously. Achieving a very high level of proficiency for an LLM\nwithin a specific domain often requires extensive training with relevant\ncorpora, which is typically accompanied by a sacrifice in performance in other\ndomains. In this paper, we propose to fuse models that are already\nhighly-specialized directly. The proposed fusing framework, UltraFuser,\nconsists of three distinct specialists that are already sufficiently trained on\nlanguage, coding, and mathematics. A token-level gating mechanism is introduced\nto blend the specialists' outputs. A two-stage training strategy accompanied by\nbalanced sampling is designed to ensure stability. To effectively train the\nfused model, we further construct a high-quality supervised instruction tuning\ndataset, UltraChat 2, which includes text, code, and mathematical content. This\ndataset comprises approximately 300,000 instructions and covers a wide range of\ntopics in each domain. Experiments show that our model could simultaneously\nachieve mastery of the three crucial domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.08281v4.pdf"
    },
    {
        "title": "RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education",
        "authors": [
            "Jieun Han",
            "Haneul Yoo",
            "Junho Myung",
            "Minsun Kim",
            "Tak Yeon Lee",
            "So-Yeon Ahn",
            "Alice Oh"
        ],
        "published": "2024-03-13T05:51:57Z",
        "summary": "The integration of generative AI in education is expanding, yet empirical\nanalyses of large-scale and real-world interactions between students and AI\nsystems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE\nfor University), a dataset sourced from a semester-long experiment with 212\ncollege students in English as Foreign Language (EFL) writing courses. During\nthe study, students engaged in dialogues with ChatGPT to revise their essays.\nRECIPE4U includes comprehensive records of these interactions, including\nconversation logs, students' intent, students' self-rated satisfaction, and\nstudents' essay edit histories. In particular, we annotate the students'\nutterances in RECIPE4U with 13 intention labels based on our coding schemes. We\nestablish baseline results for two subtasks in task-oriented dialogue systems\nwithin educational contexts: intent detection and satisfaction estimation. As a\nfoundational step, we explore student-ChatGPT interaction patterns through\nRECIPE4U and analyze them by focusing on students' dialogue, essay data\nstatistics, and students' essay edits. We further illustrate potential\napplications of RECIPE4U dataset for enhancing the incorporation of LLMs in\neducational frameworks. RECIPE4U is publicly available at\nhttps://zeunie.github.io/RECIPE4U/.",
        "pdf_link": "https://arxiv.org/pdf/2403.08272v1.pdf"
    },
    {
        "title": "Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification",
        "authors": [
            "Long Lan",
            "Fengxiang Wang",
            "Shuyan Li",
            "Xiangtao Zheng",
            "Zengmao Wang",
            "Xinwang Liu"
        ],
        "published": "2024-03-13T05:48:58Z",
        "summary": "Fine-grained ship classification in remote sensing (RS-FGSC) poses a\nsignificant challenge due to the high similarity between classes and the\nlimited availability of labeled data, limiting the effectiveness of traditional\nsupervised classification methods. Recent advancements in large pre-trained\nVision-Language Models (VLMs) have demonstrated impressive capabilities in\nfew-shot or zero-shot learning, particularly in understanding image content.\nThis study delves into harnessing the potential of VLMs to enhance\nclassification accuracy for unseen ship categories, which holds considerable\nsignificance in scenarios with restricted data due to cost or privacy\nconstraints. Directly fine-tuning VLMs for RS-FGSC often encounters the\nchallenge of overfitting the seen classes, resulting in suboptimal\ngeneralization to unseen classes, which highlights the difficulty in\ndifferentiating complex backgrounds and capturing distinct ship features. To\naddress these issues, we introduce a novel prompt tuning technique that employs\na hierarchical, multi-granularity prompt design. Our approach integrates remote\nsensing ship priors through bias terms, learned from a small trainable network.\nThis strategy enhances the model's generalization capabilities while improving\nits ability to discern intricate backgrounds and learn discriminative ship\nfeatures. Furthermore, we contribute to the field by introducing a\ncomprehensive dataset, FGSCM-52, significantly expanding existing datasets with\nmore extensive data and detailed annotations for less common ship classes.\nExtensive experimental evaluations demonstrate the superiority of our proposed\nmethod over current state-of-the-art techniques. The source code will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2403.08271v1.pdf"
    },
    {
        "title": "A Moral Imperative: The Need for Continual Superalignment of Large Language Models",
        "authors": [
            "Gokul Puthumanaillam",
            "Manav Vora",
            "Pranay Thangeda",
            "Melkior Ornik"
        ],
        "published": "2024-03-13T05:44:50Z",
        "summary": "This paper examines the challenges associated with achieving life-long\nsuperalignment in AI systems, particularly large language models (LLMs).\nSuperalignment is a theoretical framework that aspires to ensure that\nsuperintelligent AI systems act in accordance with human values and goals.\nDespite its promising vision, we argue that achieving superalignment requires\nsubstantial changes in the current LLM architectures due to their inherent\nlimitations in comprehending and adapting to the dynamic nature of these human\nethics and evolving global scenarios. We dissect the challenges of encoding an\never-changing spectrum of human values into LLMs, highlighting the\ndiscrepancies between static AI models and the dynamic nature of human\nsocieties. To illustrate these challenges, we analyze two distinct examples:\none demonstrates a qualitative shift in human values, while the other presents\na quantifiable change. Through these examples, we illustrate how LLMs,\nconstrained by their training data, fail to align with contemporary human\nvalues and scenarios. The paper concludes by exploring potential strategies to\naddress and possibly mitigate these alignment discrepancies, suggesting a path\nforward in the pursuit of more adaptable and responsive AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.14683v1.pdf"
    },
    {
        "title": "TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",
        "authors": [
            "Dingbang Li",
            "Wenzhou Chen",
            "Xin Lin"
        ],
        "published": "2024-03-13T05:22:39Z",
        "summary": "Zero-shot navigation is a critical challenge in Vision-Language Navigation\n(VLN) tasks, where the ability to adapt to unfamiliar instructions and to act\nin unknown environments is essential. Existing supervised learning-based\nmodels, trained using annotated data through reinforcement learning, exhibit\nlimitations in generalization capabilities. Large Language Models (LLMs), with\ntheir extensive knowledge and emergent reasoning abilities, present a potential\npathway for achieving zero-shot navigation. This paper presents a VLN agent\nbased on LLMs, exploring approaches to the zero-shot navigation problem. To\ncompensate for the shortcomings of LLMs in environmental perception, we propose\nthe Thinking, Interacting, and Action (TINA) framework. TINA enables the agent\nto scrutinize perceptual information and autonomously query key clues within\nthe environment through an introduced question-answering module, thereby\naligning instructions with specific perceptual data. The navigation agent's\nperceptual abilities are enhanced through the TINA framework, while the\nexplicit thought and query processes also improve the navigational procedure's\nexplainability and transparency. We evaluate the performance of our method on\nthe Room-to-Room dataset. The experiment results indicate that our approach\nimproves the navigation performance of LLM-based agents. Our approach also\noutperformed some supervised learning-based methods, highlighting its efficacy\nin zero-shot navigation.",
        "pdf_link": "https://arxiv.org/pdf/2403.08833v1.pdf"
    },
    {
        "title": "Boosting Disfluency Detection with Large Language Model as Disfluency Generator",
        "authors": [
            "Zhenrong Cheng",
            "Jiayan Guo",
            "Hao Sun",
            "Yan Zhang"
        ],
        "published": "2024-03-13T04:14:33Z",
        "summary": "Current disfluency detection methods heavily rely on costly and scarce\nhuman-annotated data. To tackle this issue, some approaches employ heuristic or\nstatistical features to generate disfluent sentences, partially improving\ndetection performance. However, these sentences often deviate from real-life\nscenarios, constraining overall model enhancement. In this study, we propose a\nlightweight data augmentation approach for disfluency detection, utilizing the\nsuperior generative and semantic understanding capabilities of large language\nmodel (LLM) to generate disfluent sentences as augmentation data. We leverage\nLLM to generate diverse and more realistic sentences guided by specific\nprompts, without the need for fine-tuning the LLM. Subsequently, we apply an\nuncertainty-aware data filtering approach to improve the quality of the\ngenerated sentences, utilized in training a small detection model for improved\nperformance. Experiments using enhanced data yielded state-of-the-art results.\nThe results showed that using a small amount of LLM-generated enhanced data can\nsignificantly improve performance, thereby further enhancing\ncost-effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.08229v1.pdf"
    },
    {
        "title": "Learning to Watermark LLM-generated Text via Reinforcement Learning",
        "authors": [
            "Xiaojun Xu",
            "Yuanshun Yao",
            "Yang Liu"
        ],
        "published": "2024-03-13T03:43:39Z",
        "summary": "We study how to watermark LLM outputs, i.e. embedding algorithmically\ndetectable signals into LLM-generated text to track misuse. Unlike the current\nmainstream methods that work with a fixed LLM, we expand the watermark design\nspace by including the LLM tuning stage in the watermark pipeline. While prior\nworks focus on token-level watermark that embeds signals into the output, we\ndesign a model-level watermark that embeds signals into the LLM weights, and\nsuch signals can be detected by a paired detector. We propose a co-training\nframework based on reinforcement learning that iteratively (1) trains a\ndetector to detect the generated watermarked text and (2) tunes the LLM to\ngenerate text easily detectable by the detector while keeping its normal\nutility. We empirically show that our watermarks are more accurate, robust, and\nadaptable (to new attacks). It also allows watermarked model open-sourcing. In\naddition, if used together with alignment, the extra overhead introduced is low\n- only training an extra reward model (i.e. our detector). We hope our work can\nbring more effort into studying a broader watermark design that is not limited\nto working with a fixed LLM. We open-source the code:\nhttps://github.com/xiaojunxu/learning-to-watermark-llm .",
        "pdf_link": "https://arxiv.org/pdf/2403.10553v1.pdf"
    },
    {
        "title": "Can Large Language Models Identify Authorship?",
        "authors": [
            "Baixiang Huang",
            "Canyu Chen",
            "Kai Shu"
        ],
        "published": "2024-03-13T03:22:02Z",
        "summary": "The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis, encompassing\nauthorship verification and attribution, remains underexplored. This paper\nconducts a comprehensive evaluation of LLMs in these critical tasks.\nTraditional studies have depended on hand-crafted stylistic features, whereas\nstate-of-the-art approaches leverage text embeddings from pre-trained language\nmodels. These methods, which typically require fine-tuning on labeled data,\noften suffer from performance degradation in cross-domain applications and\nprovide limited explainability. This work seeks to address three research\nquestions: (1) Can LLMs perform zero-shot, end-to-end authorship verification\neffectively? (2) Are LLMs capable of accurately attributing authorship among\nmultiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide\nexplainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our extensive\nassessment demonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing insights into their decision-making via\na detailed analysis of linguistic features. This establishes a new benchmark\nfor future research on LLM-based authorship analysis. The code and data are\navailable at https://github.com/baixianghuang/authorship-llm.",
        "pdf_link": "https://arxiv.org/pdf/2403.08213v1.pdf"
    },
    {
        "title": "Large Language Models are Contrastive Reasoners",
        "authors": [
            "Liang Yao"
        ],
        "published": "2024-03-13T03:15:05Z",
        "summary": "Prompting methods play a crucial role in enhancing the capabilities of\npre-trained large language models (LLMs). We explore how contrastive prompting\n(CP) significantly improves the ability of large language models to perform\ncomplex reasoning. We demonstrate that LLMs are decent contrastive reasoners by\nsimply adding \"Let's give a correct and a wrong answer.\" before LLMs provide\nanswers. Experiments on two large language models show that zero-shot\ncontrastive prompting improves performance on a range of arithmetic,\ncommonsense, and symbolic reasoning tasks without any hand-crafted few-shot\nexamples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and\nAQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method\nnot only surpasses zero-shot CoT and few-shot CoT in most arithmetic and\ncommonsense reasoning tasks but also can seamlessly integrate with existing\nprompting methods, resulting in improved or comparable results when compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/yao8839836/cp",
        "pdf_link": "https://arxiv.org/pdf/2403.08211v1.pdf"
    },
    {
        "title": "AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models",
        "authors": [
            "Shuo Jiang",
            "Jianxi Luo"
        ],
        "published": "2024-03-13T02:53:36Z",
        "summary": "Researchers and innovators have made enormous efforts in developing ideation\nmethods, such as morphological analysis and design-by-analogy, to aid\nengineering design ideation for problem solving and innovation. Among these,\nTRIZ stands out as the most well-known approach, widely applied for systematic\ninnovation. However, the complexity of TRIZ resources and concepts, coupled\nwith its reliance on users' knowledge, experience, and reasoning capabilities,\nlimits its practicability. This paper proposes AutoTRIZ, an artificial ideation\ntool that leverages large language models (LLMs) to automate and enhance the\nTRIZ methodology. By leveraging the broad knowledge and advanced reasoning\ncapabilities of LLMs, AutoTRIZ offers a novel approach to design automation and\ninterpretable ideation with artificial intelligence. We demonstrate and\nevaluate the effectiveness of AutoTRIZ through consistency experiments in\ncontradiction detection and comparative studies with cases collected from TRIZ\ntextbooks. Moreover, the proposed LLM-based framework holds the potential for\nextension to automate other knowledge-based ideation methods, including\nSCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era\nof artificial ideation for design and innovation.",
        "pdf_link": "https://arxiv.org/pdf/2403.13002v2.pdf"
    },
    {
        "title": "PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency",
        "authors": [
            "Zhishuai Li",
            "Xiang Wang",
            "Jingjing Zhao",
            "Sun Yang",
            "Guoqing Du",
            "Xiaoru Hu",
            "Bin Zhang",
            "Yuxiao Ye",
            "Ziyue Li",
            "Rui Zhao",
            "Hangyu Mao"
        ],
        "published": "2024-03-13T02:32:41Z",
        "summary": "Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large\nlanguage models (LLM) on in-context learning, achieving significant results.\nNevertheless, they face challenges when dealing with verbose database\ninformation and complex user intentions. This paper presents a two-stage\nframework to enhance the performance of current LLM-based natural language to\nSQL systems. We first introduce a novel prompt representation, called\nreference-enhanced representation, which includes schema information and\nrandomly sampled cell values from tables to instruct LLMs in generating SQL\nqueries. Then, in the first stage, question-SQL pairs are retrieved as few-shot\ndemonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After\nthat, the mentioned entities in PreSQL are parsed to conduct schema linking,\nwhich can significantly compact the useful information. In the second stage,\nwith the linked schema, we simplify the prompt's schema information and\ninstruct the LLM to produce the final SQL. Finally, as the post-refinement\nmodule, we propose using cross-consistency across different LLMs rather than\nself-consistency within a particular LLM. Our methods achieve new SOTA results\non the Spider benchmark, with an execution accuracy of 87.6%.",
        "pdf_link": "https://arxiv.org/pdf/2403.09732v3.pdf"
    },
    {
        "title": "MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension",
        "authors": [
            "Xingyu Lu",
            "He Cao",
            "Zijing Liu",
            "Shengyuan Bai",
            "Leqing Chen",
            "Yuan Yao",
            "Hai-Tao Zheng",
            "Yu Li"
        ],
        "published": "2024-03-13T02:26:16Z",
        "summary": "Large language models are playing an increasingly significant role in\nmolecular research, yet existing models often generate erroneous information,\nposing challenges to accurate molecular comprehension. Traditional evaluation\nmetrics for generated content fail to assess a model's accuracy in molecular\nunderstanding. To rectify the absence of factual evaluation, we present\nMoleculeQA, a novel question answering (QA) dataset which possesses 62K QA\npairs over 23K molecules. Each QA pair, composed of a manual question, a\npositive option and three negative options, has consistent semantics with a\nmolecular description from authoritative molecular corpus. MoleculeQA is not\nonly the first benchmark for molecular factual bias evaluation but also the\nlargest QA dataset for molecular research. A comprehensive evaluation on\nMoleculeQA for existing molecular LLMs exposes their deficiencies in specific\nareas and pinpoints several particularly crucial factors for molecular\nunderstanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.08192v1.pdf"
    },
    {
        "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection",
        "authors": [
            "Hanning Chen",
            "Wenjun Huang",
            "Yang Ni",
            "Sanggeon Yun",
            "Fei Wen",
            "Hugo Latapie",
            "Mohsen Imani"
        ],
        "published": "2024-03-12T22:33:02Z",
        "summary": "Task-oriented object detection aims to find objects suitable for\naccomplishing specific tasks. As a challenging task, it requires simultaneous\nvisual data processing and reasoning under ambiguous semantics. Recent\nsolutions are mainly all-in-one models. However, the object detection backbones\nare pre-trained without text supervision. Thus, to incorporate task\nrequirements, their intricate models undergo extensive learning on a highly\nimbalanced and scarce dataset, resulting in capped performance, laborious\ntraining, and poor generalizability. In contrast, we propose TaskCLIP, a more\nnatural two-stage design composed of general object detection and task-guided\nobject selection. Particularly for the latter, we resort to the recently\nsuccessful large Vision-Language Models (VLMs) as our backbone, which provides\nrich semantic knowledge and a uniform embedding space for images and texts.\nNevertheless, the naive application of VLMs leads to sub-optimal quality, due\nto the misalignment between embeddings of object images and their visual\nattributes, which are mainly adjective phrases. To this end, we design a\ntransformer-based aligner after the pre-trained VLMs to re-calibrate both\nembeddings. Finally, we employ a trainable score function to post-process the\nVLM matching results for object selection. Experimental results demonstrate\nthat our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by\n3.5% and only requires a single NVIDIA RTX 4090 for both training and\ninference.",
        "pdf_link": "https://arxiv.org/pdf/2403.08108v1.pdf"
    },
    {
        "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
        "authors": [
            "Robert Lakatos",
            "Peter Pollner",
            "Andras Hajdu",
            "Tamas Joo"
        ],
        "published": "2024-03-12T21:06:31Z",
        "summary": "The development of generative large language models (G-LLM) opened up new\nopportunities for the development of new types of knowledge-based systems\nsimilar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented\nGeneration (RAG) are the techniques that can be used to implement domain\nadaptation for the development of G-LLM-based knowledge systems. In our study,\nusing ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine\nthe performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2\nlanguage models. Based on measurements shown on different datasets, we\ndemonstrate that RAG-based constructions are more efficient than models\nproduced with FN. We point out that connecting RAG and FN is not trivial,\nbecause connecting FN models with RAG can cause a decrease in performance.\nFurthermore, we outline a simple RAG-based architecture which, on average,\noutperforms the FN models by 16% in terms of the ROGUE score, 15% in the case\nof the BLEU score, and 53% based on the cosine similarity. This shows the\nsignificant advantage of RAG over FN in terms of hallucination, which is not\noffset by the fact that the average 8% better METEOR score of FN models\nindicates greater creativity compared to RAG.",
        "pdf_link": "https://arxiv.org/pdf/2403.09727v1.pdf"
    },
    {
        "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
        "authors": [
            "Saurabh Agarwal",
            "Bilge Acun",
            "Basil Homer",
            "Mostafa Elhoushi",
            "Yejin Lee",
            "Shivaram Venkataraman",
            "Dimitris Papailiopoulos",
            "Carole-Jean Wu"
        ],
        "published": "2024-03-12T20:10:04Z",
        "summary": "Large Language Models (LLMs) with hundreds of billions of parameters have\ntransformed the field of machine learning. However, serving these models at\ninference time is both compute and memory intensive, where a single request can\nrequire multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is\none of the key components of LLMs, which can account for over 50% of LLMs\nmemory and compute requirement. We observe that there is a high amount of\nredundancy across heads on which tokens they pay attention to. Based on this\ninsight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a\nhigh amount of correlation for self-attention at runtime, thus reducing both\nmemory and compute. In our experiments, we show that CHAI is able to reduce the\nmemory requirements for storing K,V cache by up to 21.4% and inference time\nlatency by up to 1.73x without any fine-tuning required. CHAI achieves this\nwith a maximum 3.2% deviation in accuracy across 3 different models (i.e.\nOPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.08058v1.pdf"
    },
    {
        "title": "Generating Clarification Questions for Disambiguating Contracts",
        "authors": [
            "Anmol Singhal",
            "Chirag Jain",
            "Preethu Rose Anish",
            "Arkajyoti Chakraborty",
            "Smita Ghaisas"
        ],
        "published": "2024-03-12T19:57:39Z",
        "summary": "Enterprises frequently enter into commercial contracts that can serve as\nvital sources of project-specific requirements. Contractual clauses are\nobligatory, and the requirements derived from contracts can detail the\ndownstream implementation activities that non-legal stakeholders, including\nrequirement analysts, engineers, and delivery personnel, need to conduct.\nHowever, comprehending contracts is cognitively demanding and error-prone for\nsuch stakeholders due to the extensive use of Legalese and the inherent\ncomplexity of contract language. Furthermore, contracts often contain\nambiguously worded clauses to ensure comprehensive coverage. In contrast,\nnon-legal stakeholders require a detailed and unambiguous comprehension of\ncontractual clauses to craft actionable requirements. In this work, we\nintroduce a novel legal NLP task that involves generating clarification\nquestions for contracts. These questions aim to identify contract ambiguities\non a document level, thereby assisting non-legal stakeholders in obtaining the\nnecessary details for eliciting requirements. This task is challenged by three\ncore issues: (1) data availability, (2) the length and unstructured nature of\ncontracts, and (3) the complexity of legal text. To address these issues, we\npropose ConRAP, a retrieval-augmented prompting framework for generating\nclarification questions to disambiguate contractual text. Experiments conducted\non contracts sourced from the publicly available CUAD dataset show that ConRAP\nwith ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the\ngenerated clarification questions are deemed useful by human evaluators.",
        "pdf_link": "https://arxiv.org/pdf/2403.08053v1.pdf"
    },
    {
        "title": "Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models",
        "authors": [
            "Charlie Campanella",
            "Rob van der Goot"
        ],
        "published": "2024-03-12T19:40:18Z",
        "summary": "Large language models (LLMs) have emerged as a useful technology for job\nmatching, for both candidates and employers. Job matching is often based on a\nparticular geographic location, such as a city or region. However, LLMs have\nknown biases, commonly derived from their training data. In this work, we aim\nto quantify the metropolitan size bias encoded within large language models,\nevaluating zero-shot salary, employer presence, and commute duration\npredictions in 384 of the United States' metropolitan regions. Across all\nbenchmarks, we observe negative correlations between the metropolitan size and\nthe performance of the LLMS, indicating that smaller regions are indeed\nunderrepresented. More concretely, the smallest 10 metropolitan regions show\nupwards of 300% worse benchmark performance than the largest 10.",
        "pdf_link": "https://arxiv.org/pdf/2403.08046v1.pdf"
    },
    {
        "title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
        "authors": [
            "Tharindu Kumarage",
            "Amrita Bhattacharjee",
            "Joshua Garland"
        ],
        "published": "2024-03-12T19:12:28Z",
        "summary": "Large language models (LLMs) excel in many diverse applications beyond\nlanguage generation, e.g., translation, summarization, and sentiment analysis.\nOne intriguing application is in text classification. This becomes pertinent in\nthe realm of identifying hateful or toxic speech -- a domain fraught with\nchallenges and ethical dilemmas. In our study, we have two objectives: firstly,\nto offer a literature review revolving around LLMs as classifiers, emphasizing\ntheir role in detecting and classifying hateful or toxic content. Subsequently,\nwe explore the efficacy of several LLMs in classifying hate speech: identifying\nwhich LLMs excel in this task as well as their underlying attributes and\ntraining. Providing insight into the factors that contribute to an LLM\nproficiency (or lack thereof) in discerning hateful content. By combining a\ncomprehensive literature review with an empirical analysis, our paper strives\nto shed light on the capabilities and constraints of LLMs in the crucial domain\nof hate speech detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.08035v1.pdf"
    },
    {
        "title": "LG-Traj: LLM Guided Pedestrian Trajectory Prediction",
        "authors": [
            "Pranav Singh Chib",
            "Pravendra Singh"
        ],
        "published": "2024-03-12T19:06:23Z",
        "summary": "Accurate pedestrian trajectory prediction is crucial for various\napplications, and it requires a deep understanding of pedestrian motion\npatterns in dynamic environments. However, existing pedestrian trajectory\nprediction methods still need more exploration to fully leverage these motion\npatterns. This paper investigates the possibilities of using Large Language\nModels (LLMs) to improve pedestrian trajectory prediction tasks by inducing\nmotion cues. We introduce LG-Traj, a novel approach incorporating LLMs to\ngenerate motion cues present in pedestrian past/observed trajectories. Our\napproach also incorporates motion cues present in pedestrian future\ntrajectories by clustering future trajectories of training data using a mixture\nof Gaussians. These motion cues, along with pedestrian coordinates, facilitate\na better understanding of the underlying representation. Furthermore, we\nutilize singular value decomposition to augment the observed trajectories,\nincorporating them into the model learning process to further enhance\nrepresentation learning. Our method employs a transformer-based architecture\ncomprising a motion encoder to model motion patterns and a social decoder to\ncapture social interactions among pedestrians. We demonstrate the effectiveness\nof our approach on popular pedestrian trajectory prediction benchmarks, namely\nETH-UCY and SDD, and present various ablation experiments to validate our\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2403.08032v1.pdf"
    },
    {
        "title": "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM",
        "authors": [
            "Jingcong Liang",
            "Rong Ye",
            "Meng Han",
            "Ruofei Lai",
            "Xinyu Zhang",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2024-03-12T18:19:47Z",
        "summary": "How can we construct an automated debate judge to evaluate an extensive,\nvibrant, multi-turn debate? This task is challenging, as judging a debate\ninvolves grappling with lengthy texts, intricate argument relationships, and\nmulti-dimensional assessments. At the same time, current research mainly\nfocuses on short dialogues, rarely touching upon the evaluation of an entire\ndebate. In this paper, by leveraging Large Language Models (LLMs), we propose\nDebatrix, which makes the analysis and assessment of multi-turn debates more\naligned with majority preferences. Specifically, Debatrix features a vertical,\niterative chronological analysis and a horizontal, multi-dimensional evaluation\ncollaboration. To align with real-world debate scenarios, we introduced the\nPanelBench benchmark, comparing our system's performance to actual debate\noutcomes. The findings indicate a notable enhancement over directly using LLMs\nfor debate evaluation. Source code and benchmark data are available online at\nhttps://github.com/ljcleo/Debatrix .",
        "pdf_link": "https://arxiv.org/pdf/2403.08010v1.pdf"
    },
    {
        "title": "Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging",
        "authors": [
            "Juan Manuel Zambrano Chaves",
            "Shih-Cheng Huang",
            "Yanbo Xu",
            "Hanwen Xu",
            "Naoto Usuyama",
            "Sheng Zhang",
            "Fei Wang",
            "Yujia Xie",
            "Mahmoud Khademi",
            "Ziyi Yang",
            "Hany Awadalla",
            "Julia Gong",
            "Houdong Hu",
            "Jianwei Yang",
            "Chunyuan Li",
            "Jianfeng Gao",
            "Yu Gu",
            "Cliff Wong",
            "Mu Wei",
            "Tristan Naumann",
            "Muhao Chen",
            "Matthew P. Lungren",
            "Serena Yeung-Levy",
            "Curtis P. Langlotz",
            "Sheng Wang",
            "Hoifung Poon"
        ],
        "published": "2024-03-12T18:12:02Z",
        "summary": "The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such large models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world applications. Frontier models such as GPT-4V still have\nmajor competency gaps in multimodal capabilities for biomedical applications.\nMoreover, pragmatic issues such as access, cost, latency, and compliance make\nit hard for clinicians to use privately-hosted state-of-the-art large models\ndirectly on private patient data. In this paper, we explore training\nopen-source small multimodal models (SMMs) to bridge biomedical competency gaps\nfor unmet clinical needs. To maximize data efficiency, we adopt a modular\napproach by incorporating state-of-the-art pre-trained models for image and\ntext modalities, and focusing on training a lightweight adapter to ground each\nmodality to the text embedding space. We conduct a comprehensive study of this\napproach on radiology imaging. For training, we assemble a large dataset with\nover 1 million image-text pairs. For evaluation, we propose a clinically driven\nnovel approach using GPT-4 and demonstrate its parity with expert evaluation.\nWe also study grounding qualitatively using attention. For best practice, we\nconduct a systematic ablation study on various choices in data engineering and\nmultimodal training. The resulting LLaVA-Rad (7B) model attains\nstate-of-the-art results on radiology tasks such as report generation and\ncross-modal retrieval, even outperforming much larger models such as GPT-4V and\nMed-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in\nprivate settings, offering a promising state-of-the-art tool for real-world\nclinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.08002v2.pdf"
    },
    {
        "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
        "authors": [
            "Lei Zhu",
            "Fangyun Wei",
            "Yanye Lu"
        ],
        "published": "2024-03-12T17:59:51Z",
        "summary": "In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.",
        "pdf_link": "https://arxiv.org/pdf/2403.07874v1.pdf"
    },
    {
        "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
        "authors": [
            "Fangyun Wei",
            "Xi Chen",
            "Lin Luo"
        ],
        "published": "2024-03-12T17:59:48Z",
        "summary": "Despite their sophisticated capabilities, large language models (LLMs)\nencounter a major hurdle in effective assessment. This paper first revisits the\nprevalent evaluation method-multiple choice question answering (MCQA), which\nallows for straightforward accuracy measurement. Through a comprehensive\nevaluation of 24 models across 11 benchmarks, we highlight several potential\ndrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\nand the generation of open-ended responses in practical scenarios. In response,\nwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\nGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\nGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\nsystem is designed to mirror real-world usage, and for this purpose, we have\ncompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\n20,772 authentic user inquiries. Additionally, we thoroughly analyze the\ncharacteristics of our system and compare it with prior leaderboards like\nAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\nsystem, the feasibility of registering new models, and its potential to reshape\nLLM leaderboards.",
        "pdf_link": "https://arxiv.org/pdf/2403.07872v1.pdf"
    },
    {
        "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
        "authors": [
            "Naman Jain",
            "King Han",
            "Alex Gu",
            "Wen-Ding Li",
            "Fanjia Yan",
            "Tianjun Zhang",
            "Sida Wang",
            "Armando Solar-Lezama",
            "Koushik Sen",
            "Ion Stoica"
        ],
        "published": "2024-03-12T17:58:04Z",
        "summary": "Large Language Models (LLMs) applied to code-related applications have\nemerged as a prominent field, attracting significant interest from both\nacademia and industry. However, as new and improved LLMs are developed,\nexisting evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient\nfor assessing their capabilities. In this work, we propose LiveCodeBench, a\ncomprehensive and contamination-free evaluation of LLMs for code, which\ncontinuously collects new problems over time from contests across three\ncompetition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our\nbenchmark also focuses on a broader range of code related capabilities, such as\nself-repair, code execution, and test output prediction, beyond just code\ngeneration. Currently, LiveCodeBench hosts four hundred high-quality coding\nproblems that were published between May 2023 and February 2024. We have\nevaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We\npresent empirical findings on contamination, holistic performance comparisons,\npotential overfitting in existing benchmarks as well as individual model\ncomparisons. We will release all prompts and model completions for further\ncommunity analysis, along with a general toolkit for adding new scenarios and\nmodel",
        "pdf_link": "https://arxiv.org/pdf/2403.07974v1.pdf"
    },
    {
        "title": "Exploring Safety Generalization Challenges of Large Language Models via Code",
        "authors": [
            "Qibing Ren",
            "Chang Gao",
            "Jing Shao",
            "Junchi Yan",
            "Xin Tan",
            "Yu Qiao",
            "Wai Lam",
            "Lizhuang Ma"
        ],
        "published": "2024-03-12T17:55:38Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable generative capabilities but also raised concerns about their\npotential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\ncommon safety vulnerability of these models against code input: CodeAttack\nbypasses the safety guardrails of all models more than 80% of the time. We find\nthat a larger distribution gap between CodeAttack and natural language leads to\nweaker safety generalization, such as encoding natural language input with data\nstructures. Furthermore, we give two hypotheses about the success of\nCodeAttack: (1) the misaligned bias acquired by LLMs during code training,\nprioritizing code completion over avoiding the potential safety risk; (2) the\nlimited self-evaluation capability regarding the safety of their code outputs.\nFinally, we analyze potential mitigation measures. These findings highlight new\nsafety risks in the code domain and the need for more robust safety alignment\nalgorithms to match the code capabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.07865v3.pdf"
    },
    {
        "title": "The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage Brought By Model Editing",
        "authors": [
            "Jianchen Wang",
            "Zhouhong Gu",
            "Zhuozhi Xiong",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2024-03-12T17:04:28Z",
        "summary": "Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy.However, the editing of these models, crucial for\nrectifying outdated or erroneous information, often leads to a complex issue\nknown as the ripple effect in the hidden space. This effect, while difficult to\ndetect, can significantly impede the efficacy of model editing tasks and\ndeteriorate model performance.This paper addresses this scientific challenge by\nproposing a novel evaluation methodology, Graphical Outlier Relation based\nAssessment(GORA), which quantitatively evaluates the adaptations of the model\nand the subsequent impact of editing. Furthermore, we introduce the Selective\nOutlier Re-Editing Approach(SORA), a model editing method designed to mitigate\nthis ripple effect. Our comprehensive evaluations reveal that the ripple effect\nin the hidden space is a significant issue in all current model editing\nmethods. However, our proposed methods, GORA and SORA, effectively identify and\nalleviate this issue, respectively, contributing to the advancement of LLM\nediting techniques.",
        "pdf_link": "https://arxiv.org/pdf/2403.07825v1.pdf"
    },
    {
        "title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models",
        "authors": [
            "Tongyao Zhu",
            "Qian Liu",
            "Liang Pang",
            "Zhengbao Jiang",
            "Min-Yen Kan",
            "Min Lin"
        ],
        "published": "2024-03-12T16:42:44Z",
        "summary": "Recent developments in Language Models (LMs) have shown their effectiveness\nin NLP tasks, particularly in knowledge-intensive tasks. However, the\nmechanisms underlying knowledge storage and memory access within their\nparameters remain elusive. In this paper, we investigate whether a generative\nLM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through\ncarefully-designed synthetic tasks, covering the scenarios of full recitation,\nselective recitation and grounded question answering, we reveal that LMs manage\nto sequentially access their memory while encountering challenges in randomly\naccessing memorized content. We find that techniques including recitation and\npermutation improve the random memory access capability of LMs. Furthermore, by\napplying this intervention to realistic scenarios of open-domain question\nanswering, we validate that enhancing random access by recitation leads to\nnotable improvements in question answering. The code to reproduce our\nexperiments can be found at https://github.com/sail-sg/lm-random-memory-access.",
        "pdf_link": "https://arxiv.org/pdf/2403.07805v2.pdf"
    },
    {
        "title": "Fine-tuning Large Language Models with Sequential Instructions",
        "authors": [
            "Hanxu Hu",
            "Pinzhen Chen",
            "Edoardo M. Ponti"
        ],
        "published": "2024-03-12T16:33:30Z",
        "summary": "Large language models (LLMs) struggle to follow a sequence of instructions in\na single query as they may ignore or misinterpret part of it. This impairs\ntheir performance in complex problems whose solution requires multiple\nintermediate steps, such as multilingual (translate then answer) and multimodal\n(caption then answer) tasks. We empirically verify this with open-source LLMs\nas large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential\ninstructions in present-day data, we propose sequential instruction tuning, a\nsimple yet effective strategy to automatically augment instruction tuning data\nand equip LLMs with the ability to execute multiple sequential instructions.\nAfter exploring interleaving instructions in existing datasets, such as Alpaca,\nwith a wide range of intermediate tasks, we find that sequential\ninstruction-tuned models consistently outperform the conventional\ninstruction-tuned baselines in downstream tasks involving reasoning,\nmultilingual, and multimodal abilities. To shed further light on our technique,\nwe analyse how adversarial intermediate texts, unseen tasks, prompt\nverbalization, number of tasks, and prompt length affect SIT. We hope that this\nmethod will open new research avenues on instruction tuning for complex tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.07794v1.pdf"
    },
    {
        "title": "Duwak: Dual Watermarks in Large Language Models",
        "authors": [
            "Chaoyi Zhu",
            "Jeroen Galjaard",
            "Pin-Yu Chen",
            "Lydia Y. Chen"
        ],
        "published": "2024-03-12T16:25:38Z",
        "summary": "As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.",
        "pdf_link": "https://arxiv.org/pdf/2403.13000v1.pdf"
    },
    {
        "title": "Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations",
        "authors": [
            "Carlos Jose Xavier Cruz"
        ],
        "published": "2024-03-12T15:56:10Z",
        "summary": "This article explores the dynamic influence of computational entities based\non multi-agent systems theory (SMA) combined with large language models (LLM),\nwhich are characterized by their ability to simulate complex human\ninteractions, as a possibility to revolutionize human user interaction from the\nuse of specialized artificial agents to support everything from operational\norganizational processes to strategic decision making based on applied\nknowledge and human orchestration. Previous investigations reveal that there\nare limitations, particularly in the autonomous approach of artificial agents,\nespecially when dealing with new challenges and pragmatic tasks such as\ninducing logical reasoning and problem solving. It is also considered that\ntraditional techniques, such as the stimulation of chains of thoughts, require\nexplicit human guidance. In our approach we employ agents developed from large\nlanguage models (LLM), each with distinct prototyping that considers behavioral\nelements, driven by strategies that stimulate the generation of knowledge based\non the use case proposed in the scenario (role-play) business, using a\ndiscussion approach between agents (guided conversation). We demonstrate the\npotential of developing agents useful for organizational strategies, based on\nmulti-agent system theories (SMA) and innovative uses based on large language\nmodels (LLM based), offering a differentiated and adaptable experiment to\ndifferent applications, complexities, domains, and capabilities from LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.07769v3.pdf"
    },
    {
        "title": "Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings",
        "authors": [
            "Sahand Sharifzadeh",
            "Christos Kaplanis",
            "Shreya Pathak",
            "Dharshan Kumaran",
            "Anastasija Ilic",
            "Jovana Mitrovic",
            "Charles Blundell",
            "Andrea Banino"
        ],
        "published": "2024-03-12T15:36:42Z",
        "summary": "The creation of high-quality human-labeled image-caption datasets presents a\nsignificant bottleneck in the development of Visual-Language Models (VLMs). We\npropose a novel approach that leverages the strengths of Large Language Models\n(LLMs) and image generation models to create synthetic image-text pairs for\nefficient and effective VLM training. Our method employs pretraining a\ntext-to-image model to synthesize image embeddings starting from captions\ngenerated by an LLM. These synthetic pairs are then used to train a VLM.\nExtensive experiments demonstrate that the VLM trained with synthetic data\nexhibits comparable performance on image captioning, while requiring a fraction\nof the data used by models trained solely on human-annotated data. In\nparticular, we outperform the baseline by 17% through augmentation with a\nsynthetic dataset. Furthermore, we show that synthesizing in the image\nembedding space is 25% faster than in the pixel space. This research introduces\na promising technique for generating large-scale, customizable image datasets,\nleading to enhanced VLM performance and wider applicability across various\ndomains, all with improved data efficiency and resource utilization.",
        "pdf_link": "https://arxiv.org/pdf/2403.07750v1.pdf"
    },
    {
        "title": "FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese Large Language Models",
        "authors": [
            "Yan Liu",
            "Renren Jin",
            "Lin Shi",
            "Zheng Yao",
            "Deyi Xiong"
        ],
        "published": "2024-03-12T15:32:39Z",
        "summary": "To thoroughly assess the mathematical reasoning abilities of Large Language\nModels (LLMs), we need to carefully curate evaluation datasets covering diverse\nmathematical concepts and mathematical problems at different difficulty levels.\nIn pursuit of this objective, we propose FineMath in this paper, a fine-grained\nmathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath\nis created to cover the major key mathematical concepts taught in elementary\nschool math, which are further divided into 17 categories of math word\nproblems, enabling in-depth analysis of mathematical reasoning abilities of\nLLMs. All the 17 categories of math word problems are manually annotated with\ntheir difficulty levels according to the number of reasoning steps required to\nsolve these problems. We conduct extensive experiments on a wide range of LLMs\non FineMath and find that there is still considerable room for improvements in\nterms of mathematical reasoning capability of Chinese LLMs. We also carry out\nan in-depth analysis on the evaluation process and methods that have been\noverlooked previously. These two factors significantly influence the model\nresults and our understanding of their mathematical reasoning capabilities. The\ndataset will be publicly available soon.",
        "pdf_link": "https://arxiv.org/pdf/2403.07747v1.pdf"
    },
    {
        "title": "Multi-modal Auto-regressive Modeling via Visual Words",
        "authors": [
            "Tianshuo Peng",
            "Zuchao Li",
            "Lefei Zhang",
            "Hai Zhao",
            "Ping Wang",
            "Bo Du"
        ],
        "published": "2024-03-12T14:58:52Z",
        "summary": "Large Language Models (LLMs), benefiting from the auto-regressive modelling\napproach performed on massive unannotated texts corpora, demonstrates powerful\nperceptual and reasoning capabilities. However, as for extending\nauto-regressive modelling to multi-modal scenarios to build Large Multi-modal\nModels (LMMs), there lies a great difficulty that the image information is\nprocessed in the LMM as continuous visual embeddings, which cannot obtain\ndiscrete supervised labels for classification. In this paper, we successfully\nperform multi-modal auto-regressive modeling with a unified objective for the\nfirst time. Specifically, we propose the concept of visual words, which maps\nthe visual features to probability distributions over LLM's vocabulary,\nproviding supervision information for visual modelling. We further explore the\ndistribution of visual features in the semantic space within LMM and the\npossibility of using text embeddings to represent visual information.\nExperimental results and ablation studies on 5 VQA tasks and 4 benchmark\ntoolkits validate the powerful performance of our proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2403.07720v1.pdf"
    },
    {
        "title": "WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?",
        "authors": [
            "Alexandre Drouin",
            "Maxime Gasse",
            "Massimo Caccia",
            "Issam H. Laradji",
            "Manuel Del Verme",
            "Tom Marty",
            "L\u00e9o Boisvert",
            "Megh Thakkar",
            "Quentin Cappart",
            "David Vazquez",
            "Nicolas Chapados",
            "Alexandre Lacoste"
        ],
        "published": "2024-03-12T14:58:45Z",
        "summary": "We study the use of large language model-based agents for interacting with\nsoftware via web browsers. Unlike prior work, we focus on measuring the agents'\nability to perform tasks that span the typical daily work of knowledge workers\nutilizing enterprise software systems. To this end, we propose WorkArena, a\nremote-hosted benchmark of 29 tasks based on the widely-used ServiceNow\nplatform. We also introduce BrowserGym, an environment for the design and\nevaluation of such agents, offering a rich set of actions as well as multimodal\nobservations. Our empirical evaluation reveals that while current agents show\npromise on WorkArena, there remains a considerable gap towards achieving full\ntask automation. Notably, our analysis uncovers a significant performance\ndisparity between open and closed-source LLMs, highlighting a critical area for\nfuture exploration and development in the field.",
        "pdf_link": "https://arxiv.org/pdf/2403.07718v1.pdf"
    },
    {
        "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models",
        "authors": [
            "Zhicheng Guo",
            "Sijie Cheng",
            "Hao Wang",
            "Shihao Liang",
            "Yujia Qin",
            "Peng Li",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-03-12T14:57:40Z",
        "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
        "pdf_link": "https://arxiv.org/pdf/2403.07714v2.pdf"
    },
    {
        "title": "KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction",
        "authors": [
            "Zixuan Li",
            "Yutao Zeng",
            "Yuxin Zuo",
            "Weicheng Ren",
            "Wenxuan Liu",
            "Miao Su",
            "Yucan Guo",
            "Yantao Liu",
            "Xiang Li",
            "Zhilei Hu",
            "Long Bai",
            "Wei Li",
            "Yidan Liu",
            "Pan Yang",
            "Xiaolong Jin",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "published": "2024-03-12T14:56:34Z",
        "summary": "In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct\nUniversal Information Extraction (UIE) via code generation. KnowCoder aims to\ndevelop a kind of unified schema representation that LLMs can easily understand\nand an effective learning framework that encourages LLMs to follow schemas and\nextract structured knowledge accurately. To achieve these, KnowCoder introduces\na code-style schema representation method to uniformly transform different\nschemas into Python classes, with which complex schema information, such as\nconstraints among tasks in UIE, can be captured in an LLM-friendly manner. We\nfurther construct a code-style schema library covering over $\\textbf{30,000}$\ntypes of knowledge, which is the largest one for UIE, to the best of our\nknowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase\nlearning framework that enhances its schema understanding ability via code\npretraining and its schema following ability via instruction tuning. After code\npretraining on around $1.5$B automatically constructed data, KnowCoder already\nattains remarkable generalization ability and achieves relative improvements by\n$\\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After\ninstruction tuning, KnowCoder further exhibits strong generalization ability on\nunseen schemas and achieves up to $\\textbf{12.5%}$ and $\\textbf{21.9%}$,\ncompared to sota baselines, under the zero-shot setting and the low resource\nsetting, respectively. Additionally, based on our unified schema\nrepresentations, various human-annotated datasets can simultaneously be\nutilized to refine KnowCoder, which achieves significant improvements up to\n$\\textbf{7.5%}$ under the supervised setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.07969v2.pdf"
    },
    {
        "title": "Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards",
        "authors": [
            "Wei Shen",
            "Xiaoying Zhang",
            "Yuanshun Yao",
            "Rui Zheng",
            "Hongyi Guo",
            "Yang Liu"
        ],
        "published": "2024-03-12T14:51:57Z",
        "summary": "Reinforcement learning from human feedback (RLHF) is the mainstream paradigm\nused to align large language models (LLMs) with human preferences. Yet existing\nRLHF heavily relies on accurate and informative reward models, which are\nvulnerable and sensitive to noise from various sources, e.g. human labeling\nerrors, making the pipeline fragile. In this work, we improve the effectiveness\nof the reward model by introducing a penalty term on the reward, named as\n\\textit{contrastive rewards}. %Contrastive rewards Our approach involves two\nsteps: (1) an offline sampling step to obtain responses to prompts that serve\nas baseline calculation and (2) a contrastive reward calculated using the\nbaseline responses and used in the Proximal Policy Optimization (PPO) step. We\nshow that contrastive rewards enable the LLM to penalize reward uncertainty,\nimprove robustness, encourage improvement over baselines, calibrate according\nto task difficulty, and reduce variance in PPO. We show empirically contrastive\nrewards can improve RLHF substantially, evaluated by both GPTs and humans, and\nour method consistently outperforms strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.07708v2.pdf"
    },
    {
        "title": "Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",
        "authors": [
            "Yanyue Zhang",
            "Pengfei Li",
            "Yilong Lai",
            "Deyu Zhou",
            "Yulan He"
        ],
        "published": "2024-03-12T14:37:03Z",
        "summary": "As more than 70$\\%$ of reviews in the existing opinion summary data set are\npositive, current opinion summarization approaches are reluctant to generate\nnegative summaries given the input of negative texts. To address such sentiment\nbias, a direct approach without the over-reliance on a specific framework is to\ngenerate additional data based on large language models to balance the\nemotional distribution of the dataset. However, data augmentation based on\nlarge language models faces two disadvantages: 1) the potential issues or\ntoxicity in the augmented data; 2) the expensive costs. Therefore, in this\npaper, we propose a novel data augmentation framework based on both large and\nsmall language models for debiasing opinion summarization. In specific, a small\nsize of synthesized negative reviews is obtained by rewriting the positive text\nvia a large language model. Then, a disentangle reconstruction model is trained\nbased on the generated data. After training, a large amount of synthetic data\ncan be obtained by decoding the new representation obtained from the\ncombination of different sample representations and filtering based on\nconfusion degree and sentiment classification. Experiments have proved that our\nframework can effectively alleviate emotional bias same as using only large\nmodels, but more economically.",
        "pdf_link": "https://arxiv.org/pdf/2403.07693v2.pdf"
    },
    {
        "title": "Characterization of Large Language Model Development in the Datacenter",
        "authors": [
            "Qinghao Hu",
            "Zhisheng Ye",
            "Zerui Wang",
            "Guoteng Wang",
            "Meng Zhang",
            "Qiaoling Chen",
            "Peng Sun",
            "Dahua Lin",
            "Xiaolin Wang",
            "Yingwei Luo",
            "Yonggang Wen",
            "Tianwei Zhang"
        ],
        "published": "2024-03-12T13:31:14Z",
        "summary": "Large Language Models (LLMs) have presented impressive performance across\nseveral transformative tasks. However, it is non-trivial to efficiently utilize\nlarge-scale cluster resources to develop LLMs, often riddled with numerous\nchallenges such as frequent hardware failures, intricate parallelization\nstrategies, and imbalanced resource utilization. In this paper, we present an\nin-depth characterization study of a six-month LLM development workload trace\ncollected from our GPU datacenter Acme. Specifically, we investigate\ndiscrepancies between LLMs and prior task-specific Deep Learning (DL)\nworkloads, explore resource utilization patterns, and identify the impact of\nvarious job failures. Our analysis summarizes hurdles we encountered and\nuncovers potential opportunities to optimize systems tailored for LLMs.\nFurthermore, we introduce our system efforts: (1) fault-tolerant pretraining,\nwhich enhances fault tolerance through LLM-involved failure diagnosis and\nautomatic recovery. (2) decoupled scheduling for evaluation, which achieves\ntimely performance feedback via trial decomposition and scheduling\noptimization.",
        "pdf_link": "https://arxiv.org/pdf/2403.07648v2.pdf"
    },
    {
        "title": "Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework",
        "authors": [
            "Vu Minh Hieu Phan",
            "Yutong Xie",
            "Yuankai Qi",
            "Lingqiao Liu",
            "Liyang Liu",
            "Bowen Zhang",
            "Zhibin Liao",
            "Qi Wu",
            "Minh-Son To",
            "Johan W. Verjans"
        ],
        "published": "2024-03-12T13:18:22Z",
        "summary": "Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours improves the accuracy of recent methods by up\nto 8.56% and 17.26% for seen and unseen categories, respectively. Our code is\nreleased at https://github.com/HieuPhan33/MAVL.",
        "pdf_link": "https://arxiv.org/pdf/2403.07636v4.pdf"
    },
    {
        "title": "generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation",
        "authors": [
            "Thilo Spinner",
            "Rebecca Kehlbeck",
            "Rita Sevastjanova",
            "Tobias St\u00e4hle",
            "Daniel A. Keim",
            "Oliver Deussen",
            "Mennatallah El-Assady"
        ],
        "published": "2024-03-12T13:09:15Z",
        "summary": "Large language models (LLMs) are widely deployed in various downstream tasks,\ne.g., auto-completion, aided writing, or chat-based text generation. However,\nthe considered output candidates of the underlying search algorithm are\nunder-explored and under-explained. We tackle this shortcoming by proposing a\ntree-in-the-loop approach, where a visual representation of the beam search\ntree is the central component for analyzing, explaining, and adapting the\ngenerated outputs. To support these tasks, we present generAItor, a visual\nanalytics technique, augmenting the central beam search tree with various\ntask-specific widgets, providing targeted visualizations and interaction\npossibilities. Our approach allows interactions on multiple levels and offers\nan iterative pipeline that encompasses generating, exploring, and comparing\noutput candidates, as well as fine-tuning the model based on adapted data. Our\ncase study shows that our tool generates new insights in gender bias analysis\nbeyond state-of-the-art template-based methods. Additionally, we demonstrate\nthe applicability of our approach in a qualitative user study. Finally, we\nquantitatively evaluate the adaptability of the model to few samples, as\noccurring in text-generation use cases.",
        "pdf_link": "https://arxiv.org/pdf/2403.07627v1.pdf"
    },
    {
        "title": "Couler: Unified Machine Learning Workflow Optimization in Cloud",
        "authors": [
            "Xiaoda Wang",
            "Yuan Tang",
            "Tengda Guo",
            "Bo Sang",
            "Jingji Wu",
            "Jian Sha",
            "Ke Zhang",
            "Jiang Qian",
            "Mingjie Tang"
        ],
        "published": "2024-03-12T12:47:32Z",
        "summary": "Machine Learning (ML) has become ubiquitous, fueling data-driven applications\nacross various organizations. Contrary to the traditional perception of ML in\nresearch, ML workflows can be complex, resource-intensive, and time-consuming.\nExpanding an ML workflow to encompass a wider range of data infrastructure and\ndata types may lead to larger workloads and increased deployment costs.\nCurrently, numerous workflow engines are available (with over ten being widely\nrecognized). This variety poses a challenge for end-users in terms of mastering\ndifferent engine APIs. While efforts have primarily focused on optimizing ML\nOperations (MLOps) for a specific workflow engine, current methods largely\noverlook workflow optimization across different engines.\n  In this work, we design and implement Couler, a system designed for unified\nML workflow optimization in the cloud. Our main insight lies in the ability to\ngenerate an ML workflow using natural language (NL) descriptions. We integrate\nLarge Language Models (LLMs) into workflow generation, and provide a unified\nprogramming interface for various workflow engines. This approach alleviates\nthe need to understand various workflow engines' APIs. Moreover, Couler\nenhances workflow computation efficiency by introducing automated caching at\nmultiple stages, enabling large workflow auto-parallelization and automatic\nhyperparameters tuning. These enhancements minimize redundant computational\ncosts and improve fault tolerance during deep learning workflow training.\nCouler is extensively deployed in real-world production scenarios at Ant Group,\nhandling approximately 22k workflows daily, and has successfully improved the\nCPU/Memory utilization by more than 15% and the workflow completion rate by\naround 17%.",
        "pdf_link": "https://arxiv.org/pdf/2403.07608v1.pdf"
    },
    {
        "title": "LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model",
        "authors": [
            "Linmei Hu",
            "Hongyu He",
            "Duokang Wang",
            "Ziwang Zhao",
            "Yingxia Shao",
            "Liqiang Nie"
        ],
        "published": "2024-03-12T12:10:18Z",
        "summary": "Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.07581v1.pdf"
    },
    {
        "title": "SIFiD: Reassess Summary Factual Inconsistency Detection with LLM",
        "authors": [
            "Jiuding Yang",
            "Hui Liu",
            "Weidong Guo",
            "Zhuwei Rao",
            "Yu Xu",
            "Di Niu"
        ],
        "published": "2024-03-12T11:41:51Z",
        "summary": "Ensuring factual consistency between the summary and the original document is\nparamount in summarization tasks. Consequently, considerable effort has been\ndedicated to detecting inconsistencies. With the advent of Large Language\nModels (LLMs), recent studies have begun to leverage their advanced language\nunderstanding capabilities for inconsistency detection. However, early attempts\nhave shown that LLMs underperform traditional models due to their limited\nability to follow instructions and the absence of an effective detection\nmethodology. In this study, we reassess summary inconsistency detection with\nLLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in\nLLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency\nDetection with Filtered Document) that identify key sentences within documents\nby either employing natural language inference or measuring semantic similarity\nbetween summaries and documents.",
        "pdf_link": "https://arxiv.org/pdf/2403.07557v1.pdf"
    },
    {
        "title": "Truth-Aware Context Selection: Mitigating the Hallucinations of Large Language Models Being Misled by Untruthful Contexts",
        "authors": [
            "Tian Yu",
            "Shaolei Zhang",
            "Yang Feng"
        ],
        "published": "2024-03-12T11:40:44Z",
        "summary": "Although large language models (LLMs) have demonstrated impressive text\ngeneration capabilities, they are easily misled by the untruthful context\nprovided by users or knowledge augmentation tools, thereby producing\nhallucinations. To alleviate the LLMs from being misled by untruthful\ninformation and take advantage of knowledge augmentation, we propose\nTruth-Aware Context Selection (TACS), a lightweight method to shield untruthful\ncontext from the inputs. TACS begins by performing truth detection on the input\ncontext, leveraging the parameterized knowledge within the LLM. Subsequently,\nit constructs a corresponding attention mask based on the truthfulness of each\nposition, selecting the truthful context and discarding the untruthful context.\nAdditionally, we introduce a new evaluation metric, Disturbance Adaption Rate,\nto further study the LLMs' ability to accept truthful information and resist\nuntruthful information. Experimental results show that TACS can effectively\nfilter information in context and significantly improve the overall quality of\nLLMs' responses when presented with misleading information.",
        "pdf_link": "https://arxiv.org/pdf/2403.07556v2.pdf"
    },
    {
        "title": "MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki",
        "authors": [
            "Timothee Mickus",
            "Stig-Arne Gr\u00f6nroos",
            "Joseph Attieh",
            "Michele Boggia",
            "Ona De Gibert",
            "Shaoxiong Ji",
            "Niki Andreas Lopi",
            "Alessandro Raganato",
            "Ra\u00fal V\u00e1zquez",
            "J\u00f6rg Tiedemann"
        ],
        "published": "2024-03-12T11:32:30Z",
        "summary": "NLP in the age of monolithic large language models is approaching its limits\nin terms of size and information that can be handled. The trend goes to\nmodularization, a necessary step into the direction of designing smaller\nsub-networks and components with specialized functionality. In this paper, we\npresent the MAMMOTH toolkit: a framework designed for training massively\nmultilingual modular machine translation systems at scale, initially derived\nfrom OpenNMT-py and then adapted to ensure efficient training across\ncomputation clusters. We showcase its efficiency across clusters of A100 and\nV100 NVIDIA GPUs, and discuss our design philosophy and plans for future\ninformation. The toolkit is publicly available online.",
        "pdf_link": "https://arxiv.org/pdf/2403.07544v1.pdf"
    },
    {
        "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
        "authors": [
            "Byung-Kwan Lee",
            "Beomchan Park",
            "Chae Won Kim",
            "Yong Man Ro"
        ],
        "published": "2024-03-12T10:44:13Z",
        "summary": "The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.07508v1.pdf"
    },
    {
        "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
        "authors": [
            "Yu Yang",
            "Siddhartha Mishra",
            "Jeffrey N Chiang",
            "Baharan Mirzasoleiman"
        ],
        "published": "2024-03-12T07:45:33Z",
        "summary": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.",
        "pdf_link": "https://arxiv.org/pdf/2403.07384v1.pdf"
    },
    {
        "title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression",
        "authors": [
            "Xin Wang",
            "Yu Zheng",
            "Zhongwei Wan",
            "Mi Zhang"
        ],
        "published": "2024-03-12T07:31:18Z",
        "summary": "The advancements in Large Language Models (LLMs) have been hindered by their\nsubstantial sizes, which necessitate LLM compression methods for practical\ndeployment. Singular Value Decomposition (SVD) offers a promising solution for\nLLM compression. However, state-of-the-art SVD-based LLM compression methods\nhave two key limitations: truncating smaller singular values may lead to higher\ncompression loss, and the lack of update on the compressed weight after SVD\ntruncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression\nmethod that addresses the limitations of existing methods. SVD-LLM incorporates\na truncation-aware data whitening strategy to ensure a direct mapping between\nsingular values and compression loss. Moreover, SVD-LLM adopts a layer-wise\nclosed-form model parameter update strategy to compensate for accuracy\ndegradation under high compression ratios. We evaluate SVD-LLM on a total of 10\ndatasets and eight models from three different LLM families at four different\nscales. Our results demonstrate the superiority of SVD-LLM over\nstate-of-the-arts, especially at high model compression ratios.",
        "pdf_link": "https://arxiv.org/pdf/2403.07378v3.pdf"
    },
    {
        "title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning",
        "authors": [
            "Bingqian Lin",
            "Yunshuang Nie",
            "Ziming Wei",
            "Jiaqi Chen",
            "Shikui Ma",
            "Jianhua Han",
            "Hang Xu",
            "Xiaojun Chang",
            "Xiaodan Liang"
        ],
        "published": "2024-03-12T07:27:02Z",
        "summary": "Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.",
        "pdf_link": "https://arxiv.org/pdf/2403.07376v1.pdf"
    },
    {
        "title": "Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking",
        "authors": [
            "Yiyang Gu",
            "Yougen Zhou",
            "Qin Chen",
            "Ningning Zhou",
            "Jie Zhou",
            "Aimin Zhou",
            "Liang He"
        ],
        "published": "2024-03-12T07:17:01Z",
        "summary": "Depression-diagnosis-oriented chat aims to guide patients in self-expression\nto collect key symptoms for depression detection. Recent work focuses on\ncombining task-oriented dialogue and chitchat to simulate the interview-based\ndepression diagnosis. Whereas, these methods can not well capture the changing\ninformation, feelings, or symptoms of the patient during dialogues. Moreover,\nno explicit framework has been explored to guide the dialogue, which results in\nsome useless communications that affect the experience. In this paper, we\npropose to integrate Psychological State Tracking (POST) within the large\nlanguage model (LLM) to explicitly guide depression-diagnosis-oriented chat.\nSpecifically, the state is adapted from a psychological theoretical model,\nwhich consists of four components, namely Stage, Information, Summary and Next.\nWe fine-tune an LLM model to generate the dynamic psychological state, which is\nfurther used to assist response generation at each turn to simulate the\npsychiatrist. Experimental results on the existing benchmark show that our\nproposed method boosts the performance of all subtasks in\ndepression-diagnosis-oriented chat.",
        "pdf_link": "https://arxiv.org/pdf/2403.09717v1.pdf"
    },
    {
        "title": "Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation",
        "authors": [
            "Peiyuan Liu",
            "Hang Guo",
            "Tao Dai",
            "Naiqi Li",
            "Jigang Bao",
            "Xudong Ren",
            "Yong Jiang",
            "Shu-Tao Xia"
        ],
        "published": "2024-03-12T04:04:38Z",
        "summary": "Multivariate time series forecasting has recently gained great success with\nthe rapid growth of deep learning models. However, existing approaches usually\ntrain models from scratch using limited temporal data, preventing their\ngeneralization. Recently, with the surge of the Large Language Models (LLMs),\nseveral works have attempted to introduce LLMs into time series forecasting.\nDespite promising results, these methods directly take time series as the input\nto LLMs, ignoring the inherent modality gap between temporal and text data. In\nthis work, we propose a novel Large Language Models and time series alignment\nframework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time\nseries forecasting challenge. Based on cross-modal knowledge distillation, the\nproposed method exploits both input-agnostic static knowledge and\ninput-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers\nthe forecasting model with favorable performance as well as strong\ngeneralization abilities. Extensive experiments demonstrate the proposed method\nestablishes a new state of the art for both long- and short-term forecasting.\nCode is available at \\url{https://github.com/Hank0626/LLaTA}.",
        "pdf_link": "https://arxiv.org/pdf/2403.07300v1.pdf"
    },
    {
        "title": "A Framework for Cost-Effective and Self-Adaptive LLM Shaking and Recovery Mechanism",
        "authors": [
            "Zhiyu Chen",
            "Yu Li",
            "Suochao Zhang",
            "Jingbo Zhou",
            "Jiwen Zhou",
            "Chenfu Bao",
            "Dianhai Yu"
        ],
        "published": "2024-03-12T03:30:04Z",
        "summary": "As Large Language Models (LLMs) gain great success in real-world\napplications, an increasing number of users are seeking to develop and deploy\ntheir customized LLMs through cloud services. Nonetheless, in some specific\ndomains, there are still concerns regarding cost and trade-offs between privacy\nissues and accuracy. In this study, we introduce a cost-effective and\nself-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With\ncarefully designed horizontal and vertical shaking operators, we can achieve\ncomparable accuracy results with SOTA privacy-preserving LLM schemes using\nCryptography-based or Differential Privacy-based methods. Experiments also show\nthat with the CypherTalk framework, users can achieve reliable accuracy when\nusing optimized shaking operator settings. To our best knowledge, this is the\nfirst work that considers cost, and trade-off between model utility and privacy\nin LLM scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.07283v1.pdf"
    },
    {
        "title": "CKERC : Joint Large Language Models with Commonsense Knowledge for Emotion Recognition in Conversation",
        "authors": [
            "Yumeng Fu"
        ],
        "published": "2024-03-12T02:37:11Z",
        "summary": "Emotion recognition in conversation (ERC) is a task which predicts the\nemotion of an utterance in the context of a conversation. It tightly depends on\ndialogue context, speaker identity information, multiparty dialogue scenario\nand so on. However, the state-of-the-art method (instructERC) solely\nidentifying speaker, and ignores commonsense knowledge(i.e., reaction of the\nlisteners and intention of the speaker, etc.) behind speakers during a\nconversation, which can deeply mine speaker information. To this end, we\npropose a novel joint large language models with commonsense knowledge\nframework for emotion recognition in conversation, namely CKERC.We design\nprompts to generate interlocutors' commonsense based on historical utterances\nwith large language model. And we use the interlocutor commonsense\nidentification task for LLM pre-training to fine-tune speaker implicit clues\ninformation.By solving above challenge, our method achieve state-of-the-art.We\nextensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD,\nEmoryNLP, demonstrate our method superiority. Also, we conduct in-depth\nanalysis and further demonstrate the effectiveness of commonsense knowledge in\nERC task in large language model.",
        "pdf_link": "https://arxiv.org/pdf/2403.07260v1.pdf"
    },
    {
        "title": "AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production",
        "authors": [
            "Jiuniu Wang",
            "Zehua Du",
            "Yuyuan Zhao",
            "Bo Yuan",
            "Kexiang Wang",
            "Jian Liang",
            "Yaxi Zhao",
            "Yihen Lu",
            "Gengliang Li",
            "Junlong Gao",
            "Xin Tu",
            "Zhenyu Guo"
        ],
        "published": "2024-03-12T02:30:50Z",
        "summary": "The Agent and AIGC (Artificial Intelligence Generated Content) technologies\nhave recently made significant progress. We propose AesopAgent, an Agent-driven\nEvolutionary System on Story-to-Video Production. AesopAgent is a practical\napplication of agent technology for multimodal content generation. The system\nintegrates multiple generative capabilities within a unified framework, so that\nindividual users can leverage these modules easily. This innovative system\nwould convert user story proposals into scripts, images, and audio, and then\nintegrate these multimodal contents into videos. Additionally, the animating\nunits (e.g., Gen-2 and Sora) could make the videos more infectious. The\nAesopAgent system could orchestrate task workflow for video generation,\nensuring that the generated video is both rich in content and coherent. This\nsystem mainly contains two layers, i.e., the Horizontal Layer and the Utility\nLayer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary\nsystem that optimizes the whole video generation workflow and the steps within\nthe workflow. It continuously evolves and iteratively optimizes workflow by\naccumulating expert experience and professional knowledge, including optimizing\nthe LLM prompts and utilities usage. The Utility Layer provides multiple\nutilities, leading to consistent image generation that is visually coherent in\nterms of composition, characters, and style. Meanwhile, it provides audio and\nspecial effects, integrating them into expressive and logically arranged\nvideos. Overall, our AesopAgent achieves state-of-the-art performance compared\nwith many previous works in visual storytelling. Our AesopAgent is designed for\nconvenient service for individual users, which is available on the following\npage: https://aesopai.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.07952v1.pdf"
    },
    {
        "title": "Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits",
        "authors": [
            "Yu Xia",
            "Fang Kong",
            "Tong Yu",
            "Liya Guo",
            "Ryan A. Rossi",
            "Sungchul Kim",
            "Shuai Li"
        ],
        "published": "2024-03-11T23:52:46Z",
        "summary": "Web-based applications such as chatbots, search engines and news\nrecommendations continue to grow in scale and complexity with the recent surge\nin the adoption of LLMs. Online model selection has thus garnered increasing\nattention due to the need to choose the best model among a diverse set while\nbalancing task reward and exploration cost. Organizations faces decisions like\nwhether to employ a costly API-based LLM or a locally finetuned small LLM,\nweighing cost against performance. Traditional selection methods often evaluate\nevery candidate model before choosing one, which are becoming impractical given\nthe rising costs of training and finetuning LLMs. Moreover, it is undesirable\nto allocate excessive resources towards exploring poor-performing models. While\nsome recent works leverage online bandit algorithm to manage such\nexploration-exploitation trade-off in model selection, they tend to overlook\nthe increasing-then-converging trend in model performances as the model is\niteratively finetuned, leading to less accurate predictions and suboptimal\nmodel selections.\n  In this paper, we propose a time-increasing bandit algorithm TI-UCB, which\neffectively predicts the increase of model performances due to finetuning and\nefficiently balances exploration and exploitation in model selection. To\nfurther capture the converging points of models, we develop a change detection\nmechanism by comparing consecutive increase predictions. We theoretically prove\nthat our algorithm achieves a logarithmic regret upper bound in a typical\nincreasing bandit setting, which implies a fast convergence rate. The advantage\nof our method is also empirically validated through extensive experiments on\nclassification model selection and online selection of LLMs. Our results\nhighlight the importance of utilizing increasing-then-converging pattern for\nmore efficient and economic model selection in the deployment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.07213v1.pdf"
    },
    {
        "title": "Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions",
        "authors": [
            "Lan Wang",
            "Vishnu Boddeti",
            "Sernam Lim"
        ],
        "published": "2024-03-11T22:46:46Z",
        "summary": "We introduce a novel text-to-pose video editing method, ReimaginedAct. While\nexisting video editing tasks are limited to changes in attributes, backgrounds,\nand styles, our method aims to predict open-ended human action changes in\nvideo. Moreover, our method can accept not only direct instructional text\nprompts but also `what if' questions to predict possible action changes.\nReimaginedAct comprises video understanding, reasoning, and editing modules.\nFirst, an LLM is utilized initially to obtain a plausible answer for the\ninstruction or question, which is then used for (1) prompting Grounded-SAM to\nproduce bounding boxes of relevant individuals and (2) retrieving a set of pose\nvideos that we have collected for editing human actions. The retrieved pose\nvideos and the detected individuals are then utilized to alter the poses\nextracted from the original video. We also employ a timestep blending module to\nensure the edited video retains its original content except where necessary\nmodifications are needed. To facilitate research in text-to-pose video editing,\nwe introduce a new evaluation dataset, WhatifVideo-1.0. This dataset includes\nvideos of different scenarios spanning a range of difficulty levels, along with\nquestions and text prompts. Experimental results demonstrate that existing\nvideo editing methods struggle with human action editing, while our approach\ncan achieve effective action editing and even imaginary editing from\ncounterfactual questions.",
        "pdf_link": "https://arxiv.org/pdf/2403.07198v1.pdf"
    },
    {
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
        "authors": [
            "Weixin Liang",
            "Zachary Izzo",
            "Yaohui Zhang",
            "Haley Lepp",
            "Hancheng Cao",
            "Xuandong Zhao",
            "Lingjiao Chen",
            "Haotian Ye",
            "Sheng Liu",
            "Zhi Huang",
            "Daniel A. McFarland",
            "James Y. Zou"
        ],
        "published": "2024-03-11T21:51:39Z",
        "summary": "We present an approach for estimating the fraction of text in a large corpus\nwhich is likely to be substantially modified or produced by a large language\nmodel (LLM). Our maximum likelihood model leverages expert-written and\nAI-generated reference texts to accurately and efficiently examine real-world\nLLM-use at the corpus level. We apply this approach to a case study of\nscientific peer review in AI conferences that took place after the release of\nChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\nthat between 6.5% and 16.9% of text submitted as peer reviews to these\nconferences could have been substantially modified by LLMs, i.e. beyond\nspell-checking or minor writing updates. The circumstances in which generated\ntext occurs offer insight into user behavior: the estimated fraction of\nLLM-generated text is higher in reviews which report lower confidence, were\nsubmitted close to the deadline, and from reviewers who are less likely to\nrespond to author rebuttals. We also observe corpus-level trends in generated\ntext which may be too subtle to detect at the individual level, and discuss the\nimplications of such trends on peer review. We call for future\ninterdisciplinary work to examine how LLM use is changing our information and\nknowledge practices.",
        "pdf_link": "https://arxiv.org/pdf/2403.07183v1.pdf"
    },
    {
        "title": "Narrating Causal Graphs with Large Language Models",
        "authors": [
            "Atharva Phatak",
            "Vijay K. Mago",
            "Ameeta Agrawal",
            "Aravind Inbasekaran",
            "Philippe J. Giabbanelli"
        ],
        "published": "2024-03-11T19:19:59Z",
        "summary": "The use of generative AI to create text descriptions from graphs has mostly\nfocused on knowledge graphs, which connect concepts using facts. In this work\nwe explore the capability of large pretrained language models to generate text\nfrom causal graphs, where salient concepts are represented as nodes and\ncausality is represented via directed, typed edges. The causal reasoning\nencoded in these graphs can support applications as diverse as healthcare or\nmarketing. Using two publicly available causal graph datasets, we empirically\ninvestigate the performance of four GPT-3 models under various settings. Our\nresults indicate that while causal text descriptions improve with training\ndata, compared to fact-based graphs, they are harder to generate under\nzero-shot settings. Results further suggest that users of generative AI can\ndeploy future applications faster since similar performances are obtained when\ntraining a model with only a few examples as compared to fine-tuning via a\nlarge curated dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.07118v1.pdf"
    },
    {
        "title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation",
        "authors": [
            "Yanming Liu",
            "Xinyue Peng",
            "Jiannan Cao",
            "Le Dai",
            "Xingzu Liu",
            "Weihao Liu",
            "Mingbang Wang"
        ],
        "published": "2024-03-11T18:26:02Z",
        "summary": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require high computation cost and\nlarge memory cost. At the same time, LLMs may cause privacy leakage when\ntraining or prediction procedure contains sensitive information. In this paper,\nwe propose SPA(Side Plugin Adaption), a lightweight architecture for fast\non-devices inference and privacy retaining on the constraints of strict\non-devices computation and memory constraints. Compared with other on-devices\nseq2seq generation, SPA could make a fast and stable inference on low-resource\nconstraints, allowing it to obtain cost effiency. Our method establish an\ninteraction between a pretrained LLMs on-cloud and additive parameters\non-devices, which could provide the knowledge on both pretrained LLMs and\nprivate personal feature.Further more, SPA provides a framework to keep\nfeature-base parameters on private guaranteed but low computational devices\nwhile leave the parameters containing general information on the high\ncomputational devices.",
        "pdf_link": "https://arxiv.org/pdf/2403.07088v1.pdf"
    },
    {
        "title": "MRL Parsing Without Tears: The Case of Hebrew",
        "authors": [
            "Shaltiel Shmidman",
            "Avi Shmidman",
            "Moshe Koppel",
            "Reut Tsarfaty"
        ],
        "published": "2024-03-11T17:54:33Z",
        "summary": "Syntactic parsing remains a critical tool for relation extraction and\ninformation extraction, especially in resource-scarce languages where LLMs are\nlacking. Yet in morphologically rich languages (MRLs), where parsers need to\nidentify multiple lexical units in each token, existing systems suffer in\nlatency and setup complexity. Some use a pipeline to peel away the layers:\nfirst segmentation, then morphology tagging, and then syntax parsing; however,\nerrors in earlier layers are then propagated forward. Others use a joint\narchitecture to evaluate all permutations at once; while this improves\naccuracy, it is notoriously slow. In contrast, and taking Hebrew as a test\ncase, we present a new \"flipped pipeline\": decisions are made directly on the\nwhole-token units by expert classifiers, each one dedicated to one specific\ntask. The classifiers are independent of one another, and only at the end do we\nsynthesize their predictions. This blazingly fast approach sets a new SOTA in\nHebrew POS tagging and dependency parsing, while also reaching near-SOTA\nperformance on other Hebrew NLP tasks. Because our architecture does not rely\non any language-specific resources, it can serve as a model to develop similar\nparsers for other MRLs.",
        "pdf_link": "https://arxiv.org/pdf/2403.06970v1.pdf"
    },
    {
        "title": "Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare Linguistic Phenomena",
        "authors": [
            "Leonie Weissweiler",
            "Abdullatif K\u00f6ksal",
            "Hinrich Sch\u00fctze"
        ],
        "published": "2024-03-11T17:47:47Z",
        "summary": "Argument Structure Constructions (ASCs) are one of the most well-studied\nconstruction groups, providing a unique opportunity to demonstrate the\nusefulness of Construction Grammar (CxG). For example, the caused-motion\nconstruction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates\nthat constructions must carry meaning, otherwise the fact that ``sneeze'' in\nthis context causes movement cannot be explained. We form the hypothesis that\nthis remains challenging even for state-of-the-art Large Language Models\n(LLMs), for which we devise a test based on substituting the verb with a\nprototypical motion verb. To be able to perform this test at statistically\nsignificant scale, in the absence of adequate CxG corpora, we develop a novel\npipeline of NLP-assisted collection of linguistically annotated text. We show\nhow dependency parsing and GPT-3.5 can be used to significantly reduce\nannotation cost and thus enable the annotation of rare phenomena at scale. We\nthen evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of\nthe CMC using the newly collected corpus. We find that all models struggle with\nunderstanding the motion component that the CMC adds to a sentence.",
        "pdf_link": "https://arxiv.org/pdf/2403.06965v1.pdf"
    },
    {
        "title": "SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees",
        "authors": [
            "Saehan Jo",
            "Immanuel Trummer"
        ],
        "published": "2024-03-11T17:45:47Z",
        "summary": "The advancement of Large Language Models (LLMs) has significantly boosted\nperformance in natural language processing (NLP) tasks. However, the deployment\nof high-performance LLMs incurs substantial costs, primarily due to the\nincreased number of parameters aimed at enhancing model performance. This has\nmade the use of state-of-the-art LLMs more expensive for end-users. AI service\nproviders, such as OpenAI and Anthropic, often offer multiple versions of LLMs\nwith varying prices and performance. However, end-users still face challenges\nin choosing the appropriate LLM for their tasks that balance result quality\nwith cost.\n  We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel\nLLM framework designed to minimize the inference costs of NLP tasks while\nensuring sufficient result quality. It enables users to specify an accuracy\nconstraint in terms of the equivalence of outputs to those of the most powerful\nLLM. SMART then generates results that deviate from the outputs of this LLM\nonly with a probability below a user-defined threshold. SMART employs a\nprofiling phase that evaluates the performance of multiple LLMs to identify\nthose that meet the user-defined accuracy level. SMART optimizes the tradeoff\nbetween profiling overheads and the anticipated cost savings resulting from\nprofiling. Moreover, our approach significantly reduces inference costs by\nstrategically leveraging a mix of LLMs. Our experiments on three real-world\ndatasets show that, based on OpenAI models, SMART achieves significant cost\nsavings, up to 25.6x in comparison to GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.13835v1.pdf"
    },
    {
        "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data",
        "authors": [
            "Jialu Li",
            "Jaemin Cho",
            "Yi-Lin Sung",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "published": "2024-03-11T17:35:33Z",
        "summary": "Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.",
        "pdf_link": "https://arxiv.org/pdf/2403.06952v1.pdf"
    },
    {
        "title": "Naming, Describing, and Quantifying Visual Objects in Humans and LLMs",
        "authors": [
            "Alberto Testoni",
            "Juell Sprott",
            "Sandro Pezzelle"
        ],
        "published": "2024-03-11T17:20:12Z",
        "summary": "While human speakers use a variety of different expressions when describing\nthe same object in an image, giving rise to a distribution of plausible labels\ndriven by pragmatic constraints, the extent to which current Vision \\& Language\nLarge Language Models (VLLMs) can mimic this crucial feature of language use is\nan open question. This applies to common, everyday objects, but it is\nparticularly interesting for uncommon or novel objects for which a category\nlabel may be lacking or fuzzy. Furthermore, humans show clear production\npreferences for highly context-sensitive expressions, such as the quantifiers\n`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on\nthree categories (nouns, attributes, and quantifiers) where humans show great\nsubjective variability concerning the distribution over plausible labels, using\ndatasets and resources mostly under-explored in previous work. Our results\nreveal mixed evidence on the ability of VLLMs to capture human naming\npreferences, with all models failing in tasks that require high-level reasoning\nsuch as assigning quantifiers.",
        "pdf_link": "https://arxiv.org/pdf/2403.06935v2.pdf"
    },
    {
        "title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis",
        "authors": [
            "Yanming Liu",
            "Xinyue Peng",
            "Tianyu Du",
            "Jianwei Yin",
            "Weihao Liu",
            "Xuhong Zhang"
        ],
        "published": "2024-03-11T17:18:53Z",
        "summary": "Large language models (LLMs) have achieved commendable accomplishments in\nvarious natural language processing tasks. However, LLMs still encounter\nsignificant challenges when dealing with complex scenarios involving multiple\nentities. These challenges arise from the presence of implicit relationships\nthat demand multi-step reasoning. In this paper, we propose a novel approach\nERA-CoT, which aids LLMs in understanding context by capturing relationships\nbetween entities and supports the reasoning of diverse tasks through\nChain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates\nthe superior performance of our proposed method compared to current CoT\nprompting methods, achieving a significant improvement of an average of 5.1\\%\non GPT3.5 compared to previous SOTA baselines. Our analysis indicates that\nERA-CoT increases the LLM's understanding of entity relationships,\nsignificantly improves the accuracy of question answering, and enhances the\nreasoning ability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.06932v1.pdf"
    },
    {
        "title": "MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning",
        "authors": [
            "Yichuan Li",
            "Xiyao Ma",
            "Sixing Lu",
            "Kyumin Lee",
            "Xiaohu Liu",
            "Chenlei Guo"
        ],
        "published": "2024-03-11T17:03:04Z",
        "summary": "Large Language models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities, where a LLM makes predictions for a given test input\ntogether with a few input-output pairs (demonstrations). Nevertheless, the\ninclusion of demonstrations leads to a quadratic increase in the computational\noverhead of the self-attention mechanism. Existing solutions attempt to distill\nlengthy demonstrations into compact vectors. However, they often require\ntask-specific retraining or compromise LLM's in-context learning performance.\nTo mitigate these challenges, we present Meta dEmonstratioN Distillation\n(MEND), where a language model learns to distill any lengthy demonstrations\ninto vectors without retraining for a new downstream task. We exploit the\nknowledge distillation to enhance alignment between MEND and LLM, achieving\nboth efficiency and effectiveness simultaneously. MEND is endowed with the\nmeta-knowledge of distilling demonstrations through a two-stage training\nprocess, which includes meta-distillation pretraining and fine-tuning.\nComprehensive evaluations across seven diverse ICL task partitions using\ndecoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not\nonly matches but often outperforms the Vanilla ICL as well as other\nstate-of-the-art distillation models, while significantly reducing the\ncomputational demands. This innovation promises enhanced scalability and\nefficiency for the practical deployment of large language models",
        "pdf_link": "https://arxiv.org/pdf/2403.06914v2.pdf"
    },
    {
        "title": "Exploring Large Language Models and Hierarchical Frameworks for Classification of Large Unstructured Legal Documents",
        "authors": [
            "Nishchal Prasad",
            "Mohand Boughanem",
            "Taoufiq Dkaki"
        ],
        "published": "2024-03-11T16:24:08Z",
        "summary": "Legal judgment prediction suffers from the problem of long case documents\nexceeding tens of thousands of words, in general, and having a non-uniform\nstructure. Predicting judgments from such documents becomes a challenging task,\nmore so on documents with no structural annotation. We explore the\nclassification of these large legal documents and their lack of structural\ninformation with a deep-learning-based hierarchical framework which we call\nMESc; \"Multi-stage Encoder-based Supervised with-clustering\"; for judgment\nprediction. Specifically, we divide a document into parts to extract their\nembeddings from the last four layers of a custom fine-tuned Large Language\nModel, and try to approximate their structure through unsupervised clustering.\nWhich we use in another set of transformer encoder layers to learn the\ninter-chunk representations. We analyze the adaptability of Large Language\nModels (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the\nhierarchical framework of MESc and compare them with their standalone\nperformance on legal texts. We also study their intra-domain(legal) transfer\nlearning capability and the impact of combining embeddings from their last\nlayers in MESc. We test these methods and their effectiveness with extensive\nexperiments and ablation studies on legal documents from India, the European\nUnion, and the United States with the ILDC dataset and a subset of the LexGLUE\ndataset. Our approach achieves a minimum total performance gain of\napproximately 2 points over previous state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.06872v1.pdf"
    },
    {
        "title": "Development of a Reliable and Accessible Caregiving Language Model (CaLM)",
        "authors": [
            "Bambang Parmanto",
            "Bayu Aryoyudanta",
            "Wilbert Soekinto",
            "I Made Agus Setiawan",
            "Yuhan Wang",
            "Haomin Hu",
            "Andi Saptono",
            "Yong K. Choi"
        ],
        "published": "2024-03-11T16:12:34Z",
        "summary": "Unlike professional caregivers, family caregivers often assume this role\nwithout formal preparation or training. Because of this, there is an urgent\nneed to enhance the capacity of family caregivers to provide quality care.\nLarge language models can potentially be used as a foundation technology for\nsupporting caregivers as educational tools or as adjunct to care. This study\naimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a\ncaregiving knowledge base, develop an accessible CaLM using a small FM that\nrequires fewer computing resources, and evaluate the performance of the model\ncompared to a large FM. We developed CaLM using the Retrieval Augmented\nGeneration (RAG) framework combined with FM fine-tuning for improving the\nquality of FM answers by grounding the model on a caregiving knowledge base. We\nused two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B\nparameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving\nknowledge base by gathering various types of documents from the Internet. In\nthis study, we focused on caregivers of individuals with Alzheimer's Disease\nRelated Dementias. We evaluated the models' performance using the benchmark\nmetrics commonly used in evaluating language models and their reliability to\nprovide accurate references with the answers. The RAG framework improved the\nperformance of all FMs used in this study across all measures. As expected, the\nlarge FM performed better than small FMs across all metrics. The most\ninteresting result is that small fine-tuned FMs with RAG performed\nsignificantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2\nsmall FM performed better than GPT 3.5 (even with RAG) in returning references\nwith the answers. The study shows that reliable and accessible CaLM can be\ndeveloped by using small FMs with a knowledge base specific to the caregiving\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2403.06857v1.pdf"
    },
    {
        "title": "DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation",
        "authors": [
            "Guosheng Zhao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Xinze Chen",
            "Guan Huang",
            "Xiaoyi Bao",
            "Xingang Wang"
        ],
        "published": "2024-03-11T16:03:35Z",
        "summary": "World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.",
        "pdf_link": "https://arxiv.org/pdf/2403.06845v1.pdf"
    },
    {
        "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?",
        "authors": [
            "Egor Zverev",
            "Sahar Abdelnabi",
            "Mario Fritz",
            "Christoph H. Lampert"
        ],
        "published": "2024-03-11T15:48:56Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) have achieved breakthrough\nresults, opening countless new possibilities for many practical applications.\nHowever, LLMs lack elementary safety features that are established norms in\nother areas of computer science, such as the separation between instructions\nand data, causing them to malfunction or rendering them vulnerable to\nmanipulation and interference by third parties e.g., via indirect\nprompt/command injection. Even worse, so far, there is not even an established\ndefinition of what precisely such a separation would mean and how its violation\ncould be tested. In this work, we aim to close this gap. We introduce a formal\nmeasure to quantify the phenomenon of instruction-data separation as well as an\nempirical variant of the measure that can be computed from a model`s black-box\noutputs. We also introduce a new dataset, SEP (Should it be Executed or\nProcessed?), which allows estimating the measure, and we report results on\nseveral state-of-the-art open-source and closed LLMs. Finally, we\nquantitatively demonstrate that all evaluated LLMs fail to achieve a high\namount of separation, according to our measure. The source code and SEP dataset\nare openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",
        "pdf_link": "https://arxiv.org/pdf/2403.06833v1.pdf"
    },
    {
        "title": "The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework",
        "authors": [
            "Zhuo Chen",
            "Yin Fang",
            "Yichi Zhang",
            "Lingbing Guo",
            "Jiaoyan Chen",
            "Huajun Chen",
            "Wen Zhang"
        ],
        "published": "2024-03-11T15:48:43Z",
        "summary": "The advancement of Multi-modal Pre-training highlights the necessity for a\nrobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.\nThis framework is crucial for integrating structured knowledge into multi-modal\nLarge Language Models (LLMs) at scale, aiming to alleviate issues like\nknowledge misconceptions and multi-modal hallucinations. In this work, to\nevaluate models' ability to accurately embed entities within MMKGs, we focus on\ntwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and\nMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose a\nnovel SNAG method that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking for the robust integration of multi-modal entity\nfeatures in KGs. By incorporating specific training objectives for both MKGC\nand MMEA, our approach achieves SOTA performance across a total of ten datasets\n(three for MKGC and seven for MEMA), demonstrating its robustness and\nversatility. Besides, SNAG can not only function as a standalone model but also\nenhance other existing methods, providing stable performance improvements. Our\ncode and data are available at: https://github.com/zjukg/SNAG.",
        "pdf_link": "https://arxiv.org/pdf/2403.06832v2.pdf"
    },
    {
        "title": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model",
        "authors": [
            "Zhiwei Liu",
            "Boyang Liu",
            "Paul Thompson",
            "Kailai Yang",
            "Raghav Jain",
            "Sophia Ananiadou"
        ],
        "published": "2024-03-11T14:35:45Z",
        "summary": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/.",
        "pdf_link": "https://arxiv.org/pdf/2403.06765v1.pdf"
    },
    {
        "title": "ALaRM: Align Language Models via Hierarchical Rewards Modeling",
        "authors": [
            "Yuhang Lai",
            "Siyuan Wang",
            "Shujun Liu",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2024-03-11T14:28:40Z",
        "summary": "We introduce ALaRM, the first framework modeling hierarchical rewards in\nreinforcement learning from human feedback (RLHF), which is designed to enhance\nthe alignment of large language models (LLMs) with human preferences. The\nframework addresses the limitations of current alignment approaches, which\noften struggle with the inconsistency and sparsity of human supervision\nsignals, by integrating holistic rewards with aspect-specific rewards. This\nintegration enables more precise and consistent guidance of language models\ntowards desired outcomes, particularly in complex and open text generation\ntasks. By employing a methodology that filters and combines multiple rewards\nbased on their consistency, the framework provides a reliable mechanism for\nimproving model alignment. We validate our approach through applications in\nlong-form question answering and machine translation tasks, employing\ngpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over\nexisting baselines. Our work underscores the effectiveness of hierarchical\nrewards modeling in refining LLM training processes for better human preference\nalignment. We release our code at https://ALaRM-fdu.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2403.06754v2.pdf"
    },
    {
        "title": "ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation",
        "authors": [
            "Shaojie Dai",
            "Xin Liu",
            "Ping Luo",
            "Yue Yu"
        ],
        "published": "2024-03-11T14:10:57Z",
        "summary": "Large language model (LLM) has achieved promising performance in multilingual\nmachine translation tasks through zero/few-shot prompts or prompt-tuning.\nHowever, due to the mixture of multilingual data during the pre-training of\nLLM, the LLM-based translation models face the off-target issue in both\nprompt-based methods, including a series of phenomena, namely instruction\nmisunderstanding, translation with wrong language and over-generation. For this\nissue, this paper introduces an\n\\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction\n\\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual\n\\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine\n\\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised\nfine-tuning mechanism and orthogonal to the traditional prompt-based methods.\nIn this method, \\model automatically constructs a constrained template in the\ntarget side by adding trigger tokens ahead of the ground truth. Furthermore,\ntrigger tokens can be arranged and combined freely to represent different task\nsemantics, and they can be iteratively updated to maximize the label\nlikelihood. Experiments are performed on WMT test sets with multiple metrics,\nand the experimental results demonstrate that \\model achieves substantially\nimproved performance across multiple translation directions and reduce the\noff-target phenomena in the translation.",
        "pdf_link": "https://arxiv.org/pdf/2403.06745v1.pdf"
    },
    {
        "title": "Real-Time Multimodal Cognitive Assistant for Emergency Medical Services",
        "authors": [
            "Keshara Weerasinghe",
            "Saahith Janapati",
            "Xueren Ge",
            "Sion Kim",
            "Sneha Iyer",
            "John A. Stankovic",
            "Homa Alemzadeh"
        ],
        "published": "2024-03-11T13:56:57Z",
        "summary": "Emergency Medical Services (EMS) responders often operate under\ntime-sensitive conditions, facing cognitive overload and inherent risks,\nrequiring essential skills in critical thinking and rapid decision-making. This\npaper presents CognitiveEMS, an end-to-end wearable cognitive assistant system\nthat can act as a collaborative virtual partner engaging in the real-time\nacquisition and analysis of multimodal data from an emergency scene and\ninteracting with EMS responders through Augmented Reality (AR) smart glasses.\nCognitiveEMS processes the continuous streams of data in real-time and\nleverages edge computing to provide assistance in EMS protocol selection and\nintervention recognition. We address key technical challenges in real-time\ncognitive assistance by introducing three novel components: (i) a Speech\nRecognition model that is fine-tuned for real-world medical emergency\nconversations using simulated EMS audio recordings, augmented with synthetic\ndata generated by large language models (LLMs); (ii) an EMS Protocol Prediction\nmodel that combines state-of-the-art (SOTA) tiny language models with EMS\ndomain knowledge using graph-based attention mechanisms; (iii) an EMS Action\nRecognition module which leverages multimodal audio and video data and protocol\npredictions to infer the intervention/treatment actions taken by the responders\nat the incident scene. Our results show that for speech recognition we achieve\nsuperior performance compared to SOTA (WER of 0.290 vs. 0.618) on\nconversational data. Our protocol prediction component also significantly\noutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition\nachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s\nfor protocol prediction on the edge and 0.31s on the server.",
        "pdf_link": "https://arxiv.org/pdf/2403.06734v1.pdf"
    },
    {
        "title": "Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning",
        "authors": [
            "Zijian Zhou",
            "Miaojing Shi",
            "Meng Wei",
            "Oluwatosin Alabi",
            "Zijie Yue",
            "Tom Vercauteren"
        ],
        "published": "2024-03-11T13:47:11Z",
        "summary": "Radiology report generation (RRG) has attracted significant attention due to\nits potential to reduce the workload of radiologists. Current RRG approaches\nare still unsatisfactory against clinical standards. This paper introduces a\nnovel RRG method, \\textbf{LM-RRG}, that integrates large models (LMs) with\nclinical quality reinforcement learning to generate accurate and comprehensive\nchest X-ray radiology reports. Our method first designs a large language model\ndriven feature extractor to analyze and interpret different regions of the\nchest X-ray image, emphasizing specific regions with medical significance.\nNext, based on the large model's decoder, we develop a multimodal report\ngenerator that leverages multimodal prompts from visual features and textual\ninstruction to produce the radiology report in an auto-regressive way. Finally,\nto better reflect the clinical significant and insignificant errors that\nradiologists would normally assign in the report, we introduce a novel clinical\nquality reinforcement learning strategy. It utilizes the radiology report\nclinical quality (RadCliQ) metric as a reward function in the learning process.\nExtensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the\nsuperiority of our method over the state of the art.",
        "pdf_link": "https://arxiv.org/pdf/2403.06728v1.pdf"
    },
    {
        "title": "Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code",
        "authors": [
            "Cristina Improta"
        ],
        "published": "2024-03-11T12:47:04Z",
        "summary": "AI-based code generators have gained a fundamental role in assisting\ndevelopers in writing software starting from natural language (NL). However,\nsince these large language models are trained on massive volumes of data\ncollected from unreliable online sources (e.g., GitHub, Hugging Face), AI\nmodels become an easy target for data poisoning attacks, in which an attacker\ncorrupts the training data by injecting a small amount of poison into it, i.e.,\nastutely crafted malicious samples. In this position paper, we address the\nsecurity of AI code generators by identifying a novel data poisoning attack\nthat results in the generation of vulnerable code. Next, we devise an extensive\nevaluation of how these attacks impact state-of-the-art models for code\ngeneration. Lastly, we discuss potential solutions to overcome this threat.",
        "pdf_link": "https://arxiv.org/pdf/2403.06675v1.pdf"
    },
    {
        "title": "Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System",
        "authors": [
            "Hongsun Jang",
            "Jaeyong Song",
            "Jaewon Jung",
            "Jaeyoung Park",
            "Youngsok Kim",
            "Jinho Lee"
        ],
        "published": "2024-03-11T12:32:14Z",
        "summary": "The recent huge advance of Large Language Models (LLMs) is mainly driven by\nthe increase in the number of parameters. This has led to substantial memory\ncapacity requirements, necessitating the use of dozens of GPUs just to meet the\ncapacity. One popular solution to this is storage-offloaded training, which\nuses host memory and storage as an extended memory hierarchy. However, this\nobviously comes at the cost of storage bandwidth bottleneck because storage\ndevices have orders of magnitude lower bandwidth compared to that of GPU device\nmemories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck\nof storage-offloaded LLM training using near-storage processing devices on a\nreal system. The main component of Smart-Infinity is SmartUpdate, which\nperforms parameter updates on custom near-storage accelerators. We identify\nthat moving parameter updates to the storage side removes most of the storage\ntraffic. In addition, we propose an efficient data transfer handler structure\nto address the system integration issues for Smart-Infinity. The handler allows\noverlapping data transfers with fixed memory consumption by reusing the device\nbuffer. Lastly, we propose accelerator-assisted gradient\ncompression/decompression to enhance the scalability of Smart-Infinity. When\nscaling to multiple near-storage processing devices, the write traffic on the\nshared channel becomes the bottleneck. To alleviate this, we compress the\ngradients on the GPU and decompress them on the accelerators. It provides\nfurther acceleration from reduced traffic. As a result, Smart-Infinity achieves\na significant speedup compared to the baseline. Notably, Smart-Infinity is a\nready-to-use approach that is fully integrated into PyTorch on a real system.\nWe will open-source Smart-Infinity to facilitate its use.",
        "pdf_link": "https://arxiv.org/pdf/2403.06664v1.pdf"
    },
    {
        "title": "FashionReGen: LLM-Empowered Fashion Report Generation",
        "authors": [
            "Yujuan Ding",
            "Yunshan Ma",
            "Wenqi Fan",
            "Yige Yao",
            "Tat-Seng Chua",
            "Qing Li"
        ],
        "published": "2024-03-11T12:29:35Z",
        "summary": "Fashion analysis refers to the process of examining and evaluating trends,\nstyles, and elements within the fashion industry to understand and interpret\nits current state, generating fashion reports. It is traditionally performed by\nfashion professionals based on their expertise and experience, which requires\nhigh labour cost and may also produce biased results for relying heavily on a\nsmall group of people. In this paper, to tackle the Fashion Report Generation\n(FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting\nsystem based the advanced Large Language Models (LLMs), debbed as GPT-FAR.\nSpecifically, it tries to deliver FashionReGen based on effective catwalk\nanalysis, which is equipped with several key procedures, namely, catwalk\nunderstanding, collective organization and analysis, and report generation. By\nposing and exploring such an open-ended, complex and domain-specific task of\nFashionReGen, it is able to test the general capability of LLMs in fashion\ndomain. It also inspires the explorations of more high-level tasks with\nindustrial significance in other domains. Video illustration and more materials\nof GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.",
        "pdf_link": "https://arxiv.org/pdf/2403.06660v1.pdf"
    },
    {
        "title": "Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement",
        "authors": [
            "Che Liu",
            "Zhongwei Wan",
            "Cheng Ouyang",
            "Anand Shah",
            "Wenjia Bai",
            "Rossella Arcucci"
        ],
        "published": "2024-03-11T12:28:55Z",
        "summary": "Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for\ndetecting cardiac arrhythmic diseases in clinical practice. While ECG\nSelf-supervised Learning (eSSL) methods show promise in representation learning\nfrom unannotated ECG data, they often overlook the clinical knowledge that can\nbe found in reports. This oversight and the requirement for annotated samples\nfor downstream tasks limit eSSL's versatility. In this work, we address these\nissues with the Multimodal ECG Representation Learning (MERL}) framework.\nThrough multimodal learning on ECG records and associated reports, MERL is\ncapable of performing zero-shot ECG classification with text prompts,\neliminating the need for training data in downstream tasks. At test time, we\npropose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,\nwhich uses Large Language Models (LLMs) to exploit external expert-verified\nclinical knowledge databases, generating more descriptive prompts and reducing\nhallucinations in LLM-generated content to boost zero-shot classification.\nBased on MERL, we perform the first benchmark across six public ECG datasets,\nshowing the superior performance of MERL compared against eSSL methods.\nNotably, MERL achieves an average AUC score of 75.2% in zero-shot\nclassification (without training data), 3.2% higher than linear probed eSSL\nmethods with 10\\% annotated training data, averaged across all six datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.06659v1.pdf"
    },
    {
        "title": "Elephants Never Forget: Testing Language Models for Memorization of Tabular Data",
        "authors": [
            "Sebastian Bordt",
            "Harsha Nori",
            "Rich Caruana"
        ],
        "published": "2024-03-11T12:07:13Z",
        "summary": "While many have shown how Large Language Models (LLMs) can be applied to a\ndiverse set of tasks, the critical issues of data contamination and\nmemorization are often glossed over. In this work, we address this concern for\ntabular data. Starting with simple qualitative tests for whether an LLM knows\nthe names and values of features, we introduce a variety of different\ntechniques to assess the degrees of contamination, including statistical tests\nfor conditional distribution modeling and four tests that identify\nmemorization. Our investigation reveals that LLMs are pre-trained on many\npopular tabular datasets. This exposure can lead to invalid performance\nevaluation on downstream tasks because the LLMs have, in effect, been fit to\nthe test set. Interestingly, we also identify a regime where the language model\nreproduces important statistics of the data, but fails to reproduce the dataset\nverbatim. On these datasets, although seen during training, good performance on\ndownstream tasks might not be due to overfitting. Our findings underscore the\nneed for ensuring data integrity in machine learning tasks with LLMs. To\nfacilitate future research, we release an open-source tool that can perform\nvarious tests for memorization\n\\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.",
        "pdf_link": "https://arxiv.org/pdf/2403.06644v1.pdf"
    },
    {
        "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
        "authors": [
            "Weiqing Luo",
            "Chonggang Song",
            "Lingling Yi",
            "Gong Cheng"
        ],
        "published": "2024-03-11T12:04:20Z",
        "summary": "The utilization of semantic information is an important research problem in\nthe field of recommender systems, which aims to complement the missing parts of\nmainstream ID-based approaches. With the rise of LLM, its ability to act as a\nknowledge base and its reasoning capability have opened up new possibilities\nfor this research area, making LLM-based recommendation an emerging research\ndirection. However, directly using LLM to process semantic information for\nrecommendation scenarios is unreliable and sub-optimal due to several problems\nsuch as hallucination. A promising way to cope with this is to use external\nknowledge to aid LLM in generating truthful and usable text. Inspired by the\nabove motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to\nusing external knowledge in prompts, the proposed method also includes a\nknowledge-based contrastive learning scheme for training. Experiments on public\ndatasets and in-enterprise datasets validate the effectiveness of the proposed\nmethod.",
        "pdf_link": "https://arxiv.org/pdf/2403.06642v1.pdf"
    },
    {
        "title": "MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding",
        "authors": [
            "Jiageng Wu",
            "Xian Wu",
            "Yefeng Zheng",
            "Jie Yang"
        ],
        "published": "2024-03-11T10:57:45Z",
        "summary": "With appropriate data selection and training techniques, Large Language\nModels (LLMs) have demonstrated exceptional success in various medical\nexaminations and multiple-choice questions. However, the application of LLMs in\nmedical dialogue generation-a task more closely aligned with actual medical\npractice-has been less explored. This gap is attributed to the insufficient\nmedical knowledge of LLMs, which leads to inaccuracies and hallucinated\ninformation in the generated medical responses. In this work, we introduce the\nMedical dialogue with Knowledge enhancement and clinical Pathway encoding\n(MedKP) framework, which integrates an external knowledge enhancement module\nthrough a medical knowledge graph and an internal clinical pathway encoding via\nmedical entities and physician actions. Evaluated with comprehensive metrics,\nour experiments on two large-scale, real-world online medical consultation\ndatasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines\nand mitigates the incidence of hallucinations, achieving a new\nstate-of-the-art. Extensive ablation studies further reveal the effectiveness\nof each component of MedKP. This enhancement advances the development of\nreliable, automated medical consultation responses using LLMs, thereby\nbroadening the potential accessibility of precise and real-time medical\nassistance.",
        "pdf_link": "https://arxiv.org/pdf/2403.06611v1.pdf"
    },
    {
        "title": "Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds",
        "authors": [
            "Jiageng WU",
            "Xian Wu",
            "Jie Yang"
        ],
        "published": "2024-03-11T10:53:20Z",
        "summary": "Clinical reasoning refers to the cognitive process that physicians employ in\nevaluating and managing patients. This process typically involves suggesting\nnecessary examinations, diagnosing patients' diseases, and deciding on\nappropriate therapies, etc. Accurate clinical reasoning requires extensive\nmedical knowledge and rich clinical experience, setting a high bar for\nphysicians. This is particularly challenging in developing countries due to the\noverwhelming number of patients and limited physician resources, contributing\nsignificantly to global health inequity and necessitating automated clinical\nreasoning approaches. Recently, the emergence of large language models (LLMs)\nsuch as ChatGPT and GPT-4 have demonstrated their potential in clinical\nreasoning. However, these LLMs are prone to hallucination problems, and the\nreasoning process of LLMs may not align with the clinical decision path of\nphysicians. In this study, we introduce a novel framework, In-Context Padding\n(ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer\ncritical clinical reasoning elements (referred to as knowledge seeds) and use\nthese as anchors to guide the generation process of LLMs. Experiments on two\nclinical question datasets demonstrate that ICP significantly improves the\nclinical reasoning ability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.06609v1.pdf"
    },
    {
        "title": "Academically intelligent LLMs are not necessarily socially intelligent",
        "authors": [
            "Ruoxi Xu",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun",
            "Yingfei Sun"
        ],
        "published": "2024-03-11T10:35:53Z",
        "summary": "The academic intelligence of large language models (LLMs) has made remarkable\nprogress in recent times, but their social intelligence performance remains\nunclear. Inspired by established human social intelligence frameworks,\nparticularly Daniel Goleman's social intelligence theory, we have developed a\nstandardized social intelligence test based on real-world social scenarios to\ncomprehensively assess the social intelligence of LLMs, termed as the\nSituational Evaluation of Social Intelligence (SESI). We conducted an extensive\nevaluation with 13 recent popular and state-of-art LLM agents on SESI. The\nresults indicate the social intelligence of LLMs still has significant room for\nimprovement, with superficially friendliness as a primary reason for errors.\nMoreover, there exists a relatively low correlation between the social\nintelligence and academic intelligence exhibited by LLMs, suggesting that\nsocial intelligence is distinct from academic intelligence for LLMs.\nAdditionally, while it is observed that LLMs can't ``understand'' what social\nintelligence is, their social intelligence, similar to that of humans, is\ninfluenced by social factors.",
        "pdf_link": "https://arxiv.org/pdf/2403.06591v1.pdf"
    },
    {
        "title": "ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models",
        "authors": [
            "Luca Arrotta",
            "Claudio Bettini",
            "Gabriele Civitarese",
            "Michele Fiori"
        ],
        "published": "2024-03-11T10:32:23Z",
        "summary": "Context-aware Human Activity Recognition (HAR) is a hot research area in\nmobile computing, and the most effective solutions in the literature are based\non supervised deep learning models. However, the actual deployment of these\nsystems is limited by the scarcity of labeled data that is required for\ntraining. Neuro-Symbolic AI (NeSy) provides an interesting research direction\nto mitigate this issue, by infusing common-sense knowledge about human\nactivities and the contexts in which they can be performed into HAR deep\nlearning classifiers. Existing NeSy methods for context-aware HAR rely on\nknowledge encoded in logic-based models (e.g., ontologies) whose design,\nimplementation, and maintenance to capture new activities and contexts require\nsignificant human engineering efforts, technical knowledge, and domain\nexpertise. Recent works show that pre-trained Large Language Models (LLMs)\neffectively encode common-sense knowledge about human activities. In this work,\nwe propose ContextGPT: a novel prompt engineering approach to retrieve from\nLLMs common-sense knowledge about the relationship between human activities and\nthe context in which they are performed. Unlike ontologies, ContextGPT requires\nlimited human effort and expertise. An extensive evaluation carried out on two\npublic datasets shows how a NeSy model obtained by infusing common-sense\nknowledge from ContextGPT is effective in data scarcity scenarios, leading to\nsimilar (and sometimes better) recognition rates than logic-based approaches\nwith a fraction of the effort.",
        "pdf_link": "https://arxiv.org/pdf/2403.06586v1.pdf"
    },
    {
        "title": "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models",
        "authors": [
            "Yuting Wei",
            "Yuanxing Xu",
            "Xinru Wei",
            "Simin Yang",
            "Yangfu Zhu",
            "Yuqing Li",
            "Di Liu",
            "Bin Wu"
        ],
        "published": "2024-03-11T10:24:37Z",
        "summary": "Given the importance of ancient Chinese in capturing the essence of rich\nhistorical and cultural heritage, the rapid advancements in Large Language\nModels (LLMs) necessitate benchmarks that can effectively evaluate their\nunderstanding of ancient contexts. To meet this need, we present AC-EVAL, an\ninnovative benchmark designed to assess the advanced knowledge and reasoning\ncapabilities of LLMs within the context of ancient Chinese. AC-EVAL is\nstructured across three levels of difficulty reflecting different facets of\nlanguage comprehension: general historical knowledge, short text understanding,\nand long text comprehension. The benchmark comprises 13 tasks, spanning\nhistorical facts, geography, social customs, art, philosophy, classical poetry\nand prose, providing a comprehensive assessment framework. Our extensive\nevaluation of top-performing LLMs, tailored for both English and Chinese,\nreveals a substantial potential for enhancing ancient text comprehension. By\nhighlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote\ntheir development and application forward in the realms of ancient Chinese\nlanguage education and scholarly research. The AC-EVAL data and evaluation code\nare available at https://github.com/yuting-wei/AC-EVAL.",
        "pdf_link": "https://arxiv.org/pdf/2403.06574v1.pdf"
    },
    {
        "title": "Unraveling the Mystery of Scaling Laws: Part I",
        "authors": [
            "Hui Su",
            "Zhi Tian",
            "Xiaoyu Shen",
            "Xunliang Cai"
        ],
        "published": "2024-03-11T10:05:29Z",
        "summary": "Scaling law principles indicate a power-law correlation between loss and\nvariables such as model size, dataset size, and computational resources\nutilized during training. These principles play a vital role in optimizing\nvarious aspects of model pre-training, ultimately contributing to the success\nof large language models such as GPT-4, Llama and Gemini. However, the original\nscaling law paper by OpenAI did not disclose the complete details necessary to\nderive the precise scaling law formulas, and their conclusions are only based\non models containing up to 1.5 billion parameters. Though some subsequent works\nattempt to unveil these details and scale to larger models, they often neglect\nthe training dependency of important factors such as the learning rate, context\nlength and batch size, leading to their failure to establish a reliable formula\nfor predicting the test loss trajectory. In this technical report, we confirm\nthat the scaling law formulations proposed in the original OpenAI paper remain\nvalid when scaling the model size up to 33 billion, but the constant\ncoefficients in these formulas vary significantly with the experiment setup. We\nmeticulously identify influential factors and provide transparent, step-by-step\ninstructions to estimate all constant terms in scaling-law formulas by training\non models with only 1M~60M parameters. Using these estimated formulas, we\nshowcase the capability to accurately predict various attributes for models\nwith up to 33B parameters before their training, including (1) the minimum\npossible test loss; (2) the minimum required training steps and processed\ntokens to achieve a specific loss; (3) the critical batch size with an optimal\ntime/computation trade-off at any loss value; and (4) the complete test loss\ntrajectory with arbitrary batch size.",
        "pdf_link": "https://arxiv.org/pdf/2403.06563v3.pdf"
    },
    {
        "title": "From English to ASIC: Hardware Implementation with Large Language Model",
        "authors": [
            "Emil Goh",
            "Maoyang Xiang",
            "I-Chyn Wey",
            "T. Hui Teo"
        ],
        "published": "2024-03-11T09:57:16Z",
        "summary": "In the realm of ASIC engineering, the landscape has been significantly\nreshaped by the rapid development of LLM, paralleled by an increase in the\ncomplexity of modern digital circuits. This complexity has escalated the\nrequirements for HDL coding, necessitating a higher degree of precision and\nsophistication. However, challenges have been faced due to the\nless-than-optimal performance of modern language models in generating hardware\ndescription code, a situation further exacerbated by the scarcity of the\ncorresponding high-quality code datasets. These challenges have highlighted the\ngap between the potential of LLMs to revolutionize digital circuit design and\ntheir current capabilities in accurately interpreting and implementing hardware\nspecifications. To address these challenges, a strategy focusing on the\nfine-tuning of the leading-edge nature language model and the reshuffling of\nthe HDL code dataset has been developed. The fine-tuning aims to enhance\nmodels' proficiency in generating precise and efficient ASIC design, while the\ndataset reshuffling is intended to broaden the scope and improve the quality of\ntraining material. The model demonstrated significant improvements compared to\nthe base model, with approximately 10% to 20% increase in accuracy across a\nwide range of temperature for the pass@1 metric. This approach is expected to\nfacilitate a simplified and more efficient LLM-assisted framework for complex\ncircuit design, leveraging their capabilities to meet the sophisticated demands\nof HDL coding and thus streamlining the ASIC development process.",
        "pdf_link": "https://arxiv.org/pdf/2403.07039v1.pdf"
    },
    {
        "title": "On the Consideration of AI Openness: Can Good Intent Be Abused?",
        "authors": [
            "Yeeun Kim",
            "Eunkyung Choi",
            "Hyunjun Kim",
            "Hongseok Oh",
            "Hyunseo Shin",
            "Wonseok Hwang"
        ],
        "published": "2024-03-11T09:24:06Z",
        "summary": "Openness is critical for the advancement of science. In particular, recent\nrapid progress in AI has been made possible only by various open-source models,\ndatasets, and libraries. However, this openness also means that technologies\ncan be freely used for socially harmful purposes. Can open-source models or\ndatasets be used for malicious purposes? If so, how easy is it to adapt\ntechnology for such goals? Here, we conduct a case study in the legal domain, a\nrealm where individual decisions can have profound social consequences. To this\nend, we build EVE, a dataset consisting of 200 examples of questions and\ncorresponding answers about criminal activities based on 200 Korean precedents.\nWe found that a widely accepted open-source LLM, which initially refuses to\nanswer unethical questions, can be easily tuned with EVE to provide unethical\nand informative answers about criminal activities. This implies that although\nopen-source technologies contribute to scientific progress, some care must be\ntaken to mitigate possible malicious use cases. Warning: This paper contains\ncontents that some may find unethical.",
        "pdf_link": "https://arxiv.org/pdf/2403.06537v1.pdf"
    },
    {
        "title": "QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven Fine Tuning",
        "authors": [
            "Jiun-Man Chen",
            "Yu-Hsuan Chao",
            "Yu-Jie Wang",
            "Ming-Der Shieh",
            "Chih-Chung Hsu",
            "Wei-Fen Lin"
        ],
        "published": "2024-03-11T08:09:30Z",
        "summary": "Transformer-based models have gained widespread popularity in both the\ncomputer vision (CV) and natural language processing (NLP) fields. However,\nsignificant challenges arise during post-training linear quantization, leading\nto noticeable reductions in inference accuracy. Our study focuses on uncovering\nthe underlying causes of these accuracy drops and proposing a\nquantization-friendly fine-tuning method, \\textbf{QuantTune}. Firstly, our\nanalysis revealed that, on average, 65\\% of quantization errors result from the\nprecision loss incurred by the dynamic range amplification effect of outliers\nacross the target Transformer-based models. Secondly, \\textbf{QuantTune}\nadjusts weights based on the deviation of outlier activations and effectively\nconstrains the dynamic ranges of the problematic activations. As a result, it\nsuccessfully mitigates the negative impact of outliers on the inference\naccuracy of quantized models. Lastly, \\textbf{QuantTune} can be seamlessly\nintegrated into the back-propagation pass in the fine-tuning process without\nrequiring extra complexity in inference software and hardware design. Our\napproach showcases significant improvements in post-training quantization\nacross a range of Transformer-based models, including ViT, Bert-base, and OPT.\nQuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at\n7-bit compared to top calibration methods, outperforming state-of-the-art\nsolutions by over 18.84\\% across ViT models.",
        "pdf_link": "https://arxiv.org/pdf/2403.06497v1.pdf"
    },
    {
        "title": "Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach",
        "authors": [
            "Jinxi Kuang",
            "Jinyang Liu",
            "Junjie Huang",
            "Renyi Zhong",
            "Jiazhen Gu",
            "Lan Yu",
            "Rui Tan",
            "Zengyin Yang",
            "Michael R. Lyu"
        ],
        "published": "2024-03-11T07:48:35Z",
        "summary": "Due to the scale and complexity of cloud systems, a system failure would\ntrigger an \"alert storm\", i.e., massive correlated alerts. Although these\nalerts can be traced back to a few root causes, the overwhelming number makes\nit infeasible for manual handling. Alert aggregation is thus critical to help\nengineers concentrate on the root cause and facilitate failure resolution.\nExisting methods typically utilize semantic similarity-based methods or\nstatistical methods to aggregate alerts. However, semantic similarity-based\nmethods overlook the causal rationale of alerts, while statistical methods can\nhardly handle infrequent alerts.\n  To tackle these limitations, we introduce leveraging external knowledge,\ni.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose\nCOLA, a novel hybrid approach based on correlation mining and LLM (Large\nLanguage Model) reasoning for online alert aggregation. The correlation mining\nmodule effectively captures the temporal and spatial relations between alerts,\nmeasuring their correlations in an efficient manner. Subsequently, only\nuncertain pairs with low confidence are forwarded to the LLM reasoning module\nfor detailed analysis. This hybrid design harnesses both statistical evidence\nfor frequent alerts and the reasoning capabilities of computationally intensive\nLLMs, ensuring the overall efficiency of COLA in handling large volumes of\nalerts in practical scenarios. We evaluate COLA on three datasets collected\nfrom the production environment of a large-scale cloud platform. The\nexperimental results show COLA achieves F1-scores from 0.901 to 0.930,\noutperforming state-of-the-art methods and achieving comparable efficiency. We\nalso share our experience in deploying COLA in our real-world cloud system,\nCloud X.",
        "pdf_link": "https://arxiv.org/pdf/2403.06485v1.pdf"
    },
    {
        "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
        "authors": [
            "Weihang Su",
            "Changyue Wang",
            "Qingyao Ai",
            "Yiran HU",
            "Zhijing Wu",
            "Yujia Zhou",
            "Yiqun Liu"
        ],
        "published": "2024-03-11T05:51:03Z",
        "summary": "Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.06448v1.pdf"
    },
    {
        "title": "CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve Long-tail Recommendation",
        "authors": [
            "Junda Wu",
            "Cheng-Chun Chang",
            "Tong Yu",
            "Zhankui He",
            "Jianing Wang",
            "Yupeng Hou",
            "Julian McAuley"
        ],
        "published": "2024-03-11T05:49:34Z",
        "summary": "The long-tail recommendation is a challenging task for traditional\nrecommender systems, due to data sparsity and data imbalance issues. The recent\ndevelopment of large language models (LLMs) has shown their abilities in\ncomplex reasoning, which can help to deduce users' preferences based on very\nfew previous interactions. However, since most LLM-based systems rely on items'\nsemantic meaning as the sole evidence for reasoning, the collaborative\ninformation of user-item interactions is neglected, which can cause the LLM's\nreasoning to be misaligned with task-specific collaborative information of the\ndataset. To further align LLMs' reasoning to task-specific user-item\ninteraction knowledge, we introduce collaborative retrieval-augmented LLMs,\nCoRAL, which directly incorporate collaborative evidence into the prompts.\nBased on the retrieved user-item interactions, the LLM can analyze shared and\ndistinct preferences among users, and summarize the patterns indicating which\ntypes of users would be attracted by certain items. The retrieved collaborative\nevidence prompts the LLM to align its reasoning with the user-item interaction\npatterns in the dataset. However, since the capacity of the input prompt is\nlimited, finding the minimally-sufficient collaborative information for\nrecommendation tasks can be challenging. We propose to find the optimal\ninteraction set through a sequential decision-making process and develop a\nretrieval policy learned through a reinforcement learning (RL) framework,\nCoRAL. Our experimental results show that CoRAL can significantly improve LLMs'\nreasoning abilities on specific recommendation tasks. Our analysis also reveals\nthat CoRAL can more efficiently explore collaborative information through\nreinforcement learning.",
        "pdf_link": "https://arxiv.org/pdf/2403.06447v1.pdf"
    },
    {
        "title": "RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models",
        "authors": [
            "Liangliang Chen",
            "Yutian Lei",
            "Shiyu Jin",
            "Ying Zhang",
            "Liangjun Zhang"
        ],
        "published": "2024-03-11T04:13:26Z",
        "summary": "Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present a method for extracting the prior\nknowledge of LLMs by prompt engineering so that a preliminary rule-based robot\ncontroller for a specific task can be generated in a user-friendly manner.\nDespite being imperfect, the LLM-generated robot controller is utilized to\nproduce action samples during rollouts with a decaying probability, thereby\nimproving RL's sample efficiency. We employ TD3, the widely-used RL baseline\nmethod, and modify the actor loss to regularize the policy learning towards the\nLLM-generated controller. RLingua also provides a novel method of improving the\nimperfect LLM-generated robot controllers by RL. We demonstrate that RLingua\ncan significantly reduce the sample complexity of TD3 in four robot tasks of\npanda_gym and achieve high success rates in 12 sampled sparsely rewarded robot\ntasks in RLBench, where the standard TD3 fails. Additionally, We validated\nRLingua's effectiveness in real-world robot experiments through Sim2Real,\ndemonstrating that the learned policies are effectively transferable to real\nrobot tasks. Further details about our work are available at our project\nwebsite https://rlingua.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2403.06420v2.pdf"
    },
    {
        "title": "Evolving Knowledge Distillation with Large Language Models and Active Learning",
        "authors": [
            "Chengyuan Liu",
            "Yangyang Kang",
            "Fubang Zhao",
            "Kun Kuang",
            "Zhuoren Jiang",
            "Changlong Sun",
            "Fei Wu"
        ],
        "published": "2024-03-11T03:55:24Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks. However, their computational costs are prohibitively high.\nTo address this issue, previous research has attempted to distill the knowledge\nof LLMs into smaller models by generating annotated data. Nonetheless, these\nworks have mainly focused on the direct use of LLMs for text generation and\nlabeling, without fully exploring their potential to comprehend the target task\nand acquire valuable knowledge. In this paper, we propose EvoKD: Evolving\nKnowledge Distillation, which leverages the concept of active learning to\ninteractively enhance the process of data generation using large language\nmodels, simultaneously improving the task capabilities of small domain model\n(student model). Different from previous work, we actively analyze the student\nmodel's weaknesses, and then synthesize labeled samples based on the analysis.\nIn addition, we provide iterative feedback to the LLMs regarding the student\nmodel's performance to continuously construct diversified and challenging\nsamples. Experiments and analysis on different NLP tasks, namely, text\nclassification and named entity recognition show the effectiveness of EvoKD.",
        "pdf_link": "https://arxiv.org/pdf/2403.06414v1.pdf"
    },
    {
        "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
        "authors": [
            "Eunsu Kim",
            "Juyoung Suk",
            "Philhoon Oh",
            "Haneul Yoo",
            "James Thorne",
            "Alice Oh"
        ],
        "published": "2024-03-11T03:54:33Z",
        "summary": "Despite the rapid development of large language models (LLMs) for the Korean\nlanguage, there remains an obvious lack of benchmark datasets that test the\nrequisite Korean cultural and linguistic knowledge. Because many existing\nKorean benchmark datasets are derived from the English counterparts through\ntranslation, they often overlook the different cultural contexts. For the few\nbenchmark datasets that are sourced from Korean data capturing cultural\nknowledge, only narrow tasks such as bias and hate speech detection are\noffered. To address this gap, we introduce a benchmark of Cultural and\nLinguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.\nCLIcK sources its data from official Korean exams and textbooks, partitioning\nthe questions into eleven categories under the two main categories of language\nand culture. For each instance in CLIcK, we provide fine-grained annotation of\nwhich cultural and linguistic knowledge is required to answer the question\ncorrectly. Using CLIcK, we test 13 language models to assess their performance.\nOur evaluation uncovers insights into their performances across the categories,\nas well as the diverse factors affecting their comprehension. CLIcK offers the\nfirst large-scale comprehensive Korean-centric analysis of LLMs' proficiency in\nKorean culture and language.",
        "pdf_link": "https://arxiv.org/pdf/2403.06412v3.pdf"
    },
    {
        "title": "What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation",
        "authors": [
            "Zhuocheng Gong",
            "Jiahao Liu",
            "Jingang Wang",
            "Xunliang Cai",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2024-03-11T03:42:51Z",
        "summary": "Quantization has emerged as a promising technique for improving the memory\nand computational efficiency of large language models (LLMs). Though the\ntrade-off between performance and efficiency is well-known, there is still much\nto be learned about the relationship between quantization and LLM performance.\nTo shed light on this relationship, we propose a new perspective on\nquantization, viewing it as perturbations added to the weights and activations\nof LLMs. We call this approach \"the lens of perturbation\". Using this lens, we\nconduct experiments with various artificial perturbations to explore their\nimpact on LLM performance. Our findings reveal several connections between the\nproperties of perturbations and LLM performance, providing insights into the\nfailure cases of uniform quantization and suggesting potential solutions to\nimprove the robustness of LLM quantization. To demonstrate the significance of\nour findings, we implement a simple non-uniform quantization approach based on\nour insights. Our experiments show that this approach achieves minimal\nperformance degradation on both 4-bit weight quantization and 8-bit\nquantization for weights and activations. These results validate the\ncorrectness of our approach and highlight its potential to improve the\nefficiency of LLMs without sacrificing performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.06408v1.pdf"
    },
    {
        "title": "A Knowledge-Injected Curriculum Pretraining Framework for Question Answering",
        "authors": [
            "Xin Lin",
            "Tianhuang Su",
            "Zhenya Huang",
            "Shangzi Xue",
            "Haifeng Liu",
            "Enhong Chen"
        ],
        "published": "2024-03-11T03:42:03Z",
        "summary": "Knowledge-based question answering (KBQA) is a key task in NLP research, and\nalso an approach to access the web data and knowledge, which requires\nexploiting knowledge graphs (KGs) for reasoning. In the literature, one\npromising solution for KBQA is to incorporate the pretrained language model\n(LM) with KGs by generating KG-centered pretraining corpus, which has shown its\nsuperiority. However, these methods often depend on specific techniques and\nresources to work, which may not always be available and restrict its\napplication. Moreover, existing methods focus more on improving language\nunderstanding with KGs, while neglect the more important human-like complex\nreasoning. To this end, in this paper, we propose a general Knowledge-Injected\nCurriculum Pretraining framework (KICP) to achieve comprehensive KG learning\nand exploitation for KBQA tasks, which is composed of knowledge injection (KI),\nknowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KI\nmodule first injects knowledge into the LM by generating KG-centered\npretraining corpus, and generalizes the process into three key steps that could\nwork with different implementations for flexible application. Next, the KA\nmodule learns knowledge from the generated corpus with LM equipped with an\nadapter as well as keeps its original natural language understanding ability to\nreduce the negative impacts of the difference between the generated and natural\ncorpus. Last, to enable the LM with complex reasoning, the CR module follows\nhuman reasoning patterns to construct three corpora with increasing\ndifficulties of reasoning, and further trains the LM from easy to hard in a\ncurriculum manner. We provide an implementation of the general framework, and\nevaluate the proposed KICP on four real-word datasets. The results demonstrate\nthat our framework can achieve higher performances.",
        "pdf_link": "https://arxiv.org/pdf/2403.09712v1.pdf"
    },
    {
        "title": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
        "authors": [
            "Jiawei Chen",
            "Yue Jiang",
            "Dingkang Yang",
            "Mingcheng Li",
            "Jinjie Wei",
            "Ziyun Qian",
            "Lihua Zhang"
        ],
        "published": "2024-03-11T03:38:48Z",
        "summary": "While large language models (LLMs) excel in world knowledge understanding,\nadapting them to specific subfields requires precise adjustments. Due to the\nmodel's vast scale, traditional global fine-tuning methods for large models can\nbe computationally expensive and impact generalization. To address this\nchallenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT)\nmethods have emerged and achieved remarkable success in both LLMs and Large\nVision-Language Models (LVLMs). In the medical domain, fine-tuning a medical\nVision-Language Pretrained (VLP) model is essential for adapting it to specific\ntasks. Can the fine-tuning methods for large models be transferred to the\nmedical field to enhance transfer learning efficiency? In this paper, we delve\ninto the fine-tuning methods of LLMs and conduct extensive experiments to\ninvestigate the impact of fine-tuning methods for large models on existing\nmultimodal models in the medical domain from the training data level and the\nmodel structure level. We show the different impacts of fine-tuning methods for\nlarge models on medical VLMs and develop the most efficient ways to fine-tune\nmedical VLP models. We hope this research can guide medical domain researchers\nin optimizing VLMs' training costs, fostering the broader application of VLMs\nin healthcare fields. Code and dataset will be released upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2403.06407v1.pdf"
    },
    {
        "title": "DivCon: Divide and Conquer for Progressive Text-to-Image Generation",
        "authors": [
            "Yuhao Jia",
            "Wenhan Tan"
        ],
        "published": "2024-03-11T03:24:44Z",
        "summary": "Diffusion-driven text-to-image (T2I) generation has achieved remarkable\nadvancements. To further improve T2I models' capability in numerical and\nspatial reasoning, the layout is employed as an intermedium to bridge large\nlanguage models and layout-based diffusion models. However, these methods still\nstruggle with generating images from textural prompts with multiple objects and\ncomplicated spatial relationships. To tackle this challenge, we introduce a\ndivide-and-conquer approach which decouples the T2I generation task into simple\nsubtasks. Our approach divides the layout prediction stage into numerical \\&\nspatial reasoning and bounding box prediction. Then, the layout-to-image\ngeneration stage is conducted in an iterative manner to reconstruct objects\nfrom easy ones to difficult ones. We conduct experiments on the HRS and NSR-1K\nbenchmarks and our approach outperforms previous state-of-the-art models with\nnotable margins. In addition, visual results demonstrate that our approach\nsignificantly improves the controllability and consistency in generating\nmultiple objects from complex textural prompts.",
        "pdf_link": "https://arxiv.org/pdf/2403.06400v1.pdf"
    },
    {
        "title": "Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages",
        "authors": [
            "Michael Andersland"
        ],
        "published": "2024-03-11T01:04:36Z",
        "summary": "Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible\nproficiency at natural language processing tasks and have even begun to excel\nat tasks across other modalities such as vision and audio. Despite their\nsuccess, LLMs often struggle to perform well on low-resource languages because\nthere is so little training data available. This shortcoming is especially\nprevalent with open source models. In this work, we explore training LLaMA-2 to\nspeak Amharic, a language which is spoken by over 50 million people world wide,\nbut has orders of magnitude less data available than languages like English. We\nemploy methods previously used for training LLMs on other languages with data\nscarcity, and use open source translation models to perform data augmentation\nand grow our dataset from millions of tokens to billions. We further enhance\nthe capabilities of our model by connecting an image encoder and training on a\ntranslated visual instruction tuning dataset in the same manner as LLaVA,\nresulting in a multimodal Amharic LLM that can understand images along with\ntext. We introduce an Amharic version of a popular benchmarking dataset to\nevaluate our work. Our models and dataset are open sourced and available on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2403.06354v1.pdf"
    },
    {
        "title": "From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification",
        "authors": [
            "Fei Wang",
            "Chao Shang",
            "Sarthak Jain",
            "Shuai Wang",
            "Qiang Ning",
            "Bonan Min",
            "Vittorio Castelli",
            "Yassine Benajiba",
            "Dan Roth"
        ],
        "published": "2024-03-10T22:14:54Z",
        "summary": "User alignment is crucial for adapting general-purpose language models (LMs)\nto downstream tasks, but human annotations are often not available for all\ntypes of instructions, especially those with customized constraints. We observe\nthat user instructions typically contain constraints. While assessing response\nquality in terms of the whole instruction is often costly, efficiently\nevaluating the satisfaction rate of constraints is feasible. We investigate\ncommon constraints in NLP tasks, categorize them into three classes based on\nthe types of their arguments, and propose a unified framework, ACT (Aligning to\nConsTraints), to automatically produce supervision signals for user alignment\nwith constraints. Specifically, ACT uses constraint verifiers, which are\ntypically easy to implement in practice, to compute constraint satisfaction\nrate (CSR) of each response. It samples multiple responses for each prompt and\ncollect preference labels based on their CSR automatically. Subsequently, ACT\nadapts the LM to the target task through a ranking-based learning process.\nExperiments on fine-grained entity typing, abstractive summarization, and\ntemporal question answering show that ACT is able to enhance LMs' capability to\nadhere to different classes of constraints, thereby improving task performance.\nFurther experiments show that the constraint-following capabilities are\ntransferable.",
        "pdf_link": "https://arxiv.org/pdf/2403.06326v1.pdf"
    },
    {
        "title": "LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition",
        "authors": [
            "Xiaomeng Zhu",
            "Robert Frank"
        ],
        "published": "2024-03-10T20:20:16Z",
        "summary": "Discourse Entity (DE) recognition is the task of identifying novel and known\nentities introduced within a text. While previous work has found that large\nlanguage models have basic, if imperfect, DE recognition abilities (Schuster\nand Linzen, 2022), it remains largely unassessed which of the fundamental\nsemantic properties that govern the introduction and subsequent reference to\nDEs they have knowledge of. We propose the Linguistically-Informed Evaluation\nfor Discourse Entity Recognition (LIEDER) dataset that allows for a detailed\nexamination of language models' knowledge of four crucial semantic properties:\nexistence, uniqueness, plurality, and novelty. We find evidence that\nstate-of-the-art large language models exhibit sensitivity to all of these\nproperties except novelty, which demonstrates that they have yet to reach\nhuman-level language understanding abilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.06301v1.pdf"
    },
    {
        "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes",
        "authors": [
            "Shengxin Hong",
            "Liang Xiao",
            "Xin Zhang",
            "Jianxia Chen"
        ],
        "published": "2024-03-10T19:47:00Z",
        "summary": "There are two main barriers to using large language models (LLMs) in clinical\nreasoning. Firstly, while LLMs exhibit significant promise in Natural Language\nProcessing (NLP) tasks, their performance in complex reasoning and planning\nfalls short of expectations. Secondly, LLMs use uninterpretable methods to make\nclinical decisions that are fundamentally different from the clinician's\ncognitive processes. This leads to user distrust. In this paper, we present a\nmulti-agent framework called ArgMed-Agents, which aims to enable LLM-based\nagents to make explainable clinical decision reasoning through interaction.\nArgMed-Agents performs self-argumentation iterations via Argumentation Scheme\nfor Clinical Decision (a reasoning mechanism for modeling cognitive processes\nin clinical reasoning), and then constructs the argumentation process as a\ndirected graph representing conflicting relationships. Ultimately, Reasoner(a\nsymbolic solver) identify a series of rational and coherent arguments to\nsupport decision. ArgMed-Agents enables LLMs to mimic the process of clinical\nargumentative reasoning by generating explanations of reasoning in a\nself-directed manner. The setup experiments show that ArgMed-Agents not only\nimproves accuracy in complex clinical decision reasoning problems compared to\nother prompt methods, but more importantly, it provides users with decision\nexplanations that increase their confidence.",
        "pdf_link": "https://arxiv.org/pdf/2403.06294v1.pdf"
    },
    {
        "title": "Editing Conceptual Knowledge for Large Language Models",
        "authors": [
            "Xiaohan Wang",
            "Shengyu Mao",
            "Ningyu Zhang",
            "Shumin Deng",
            "Yunzhi Yao",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "published": "2024-03-10T16:57:10Z",
        "summary": "Recently, there has been a growing interest in knowledge editing for Large\nLanguage Models (LLMs). Current approaches and evaluations merely explore the\ninstance-level editing, while whether LLMs possess the capability to modify\nconcepts remains unclear. This paper pioneers the investigation of editing\nconceptual knowledge for LLMs, by constructing a novel benchmark dataset\nConceptEdit and establishing a suite of new metrics for evaluation. The\nexperimental results reveal that, although existing editing methods can\nefficiently modify concept-level definition to some extent, they also have the\npotential to distort the related instantial knowledge in LLMs, leading to poor\nperformance. We anticipate this can inspire further progress in better\nunderstanding LLMs. Our project homepage is available at\nhttps://zjunlp.github.io/project/ConceptEdit.",
        "pdf_link": "https://arxiv.org/pdf/2403.06259v1.pdf"
    },
    {
        "title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
        "authors": [
            "Ruiwen Zhou",
            "Yingxuan Yang",
            "Muning Wen",
            "Ying Wen",
            "Wenhao Wang",
            "Chunling Xi",
            "Guoqiang Xu",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "published": "2024-03-10T13:58:38Z",
        "summary": "Numerous large language model (LLM) agents have been built for different\ntasks like web navigation and online shopping due to LLM's wide knowledge and\ntext-understanding ability. Among these works, many of them utilize in-context\nexamples to achieve generalization without the need for fine-tuning, while few\nof them have considered the problem of how to select and effectively utilize\nthese examples. Recently, methods based on trajectory-level retrieval with task\nmeta-data and using trajectories as in-context examples have been proposed to\nimprove the agent's overall performance in some sequential decision making\ntasks. However, these methods can be problematic due to plausible examples\nretrieved without task-specific state transition dynamics and long input with\nplenty of irrelevant context. In this paper, we propose a novel framework\n(TRAD) to address these issues. TRAD first conducts Thought Retrieval,\nachieving step-level demonstration selection via thought matching, leading to\nmore helpful demonstrations and less irrelevant input noise. Then, TRAD\nintroduces Aligned Decision, complementing retrieved demonstration steps with\ntheir previous or subsequent steps, which enables tolerance for imperfect\nthought and provides a choice for balance between more context and less noise.\nExtensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not\nonly outperforms state-of-the-art models but also effectively helps in reducing\nnoise and promoting generalization. Furthermore, TRAD has been deployed in\nreal-world scenarios of a global business insurance company and improves the\nsuccess rate of robotic process automation.",
        "pdf_link": "https://arxiv.org/pdf/2403.06221v1.pdf"
    },
    {
        "title": "Personalized LoRA for Human-Centered Text Understanding",
        "authors": [
            "You Zhang",
            "Jin Wang",
            "Liang-Chih Yu",
            "Dan Xu",
            "Xuejie Zhang"
        ],
        "published": "2024-03-10T13:04:54Z",
        "summary": "Effectively and efficiently adapting a pre-trained language model (PLM) for\nhuman-centered text understanding (HCTU) is challenging since user tokens are\nmillion-level in most personalized applications and do not have concrete\nexplicit semantics. A standard and parameter-efficient approach (e.g., LoRA)\nnecessitates memorizing numerous suits of adapters for each user. In this work,\nwe introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework\nfor the HCTU task. PLoRA is effective, parameter-efficient, and dynamically\ndeploying in PLMs. Moreover, a personalized dropout and a mutual information\nmaximizing strategies are adopted and hence the proposed PLoRA can be well\nadapted to few/zero-shot learning scenarios for the cold-start issue.\nExperiments conducted on four benchmark datasets show that the proposed method\noutperforms existing methods in full/few/zero-shot learning scenarios for the\nHCTU task, even though it has fewer trainable parameters. For reproducibility,\nthe code for this paper is available at: https://github.com/yoyo-yun/PLoRA.",
        "pdf_link": "https://arxiv.org/pdf/2403.06208v1.pdf"
    },
    {
        "title": "Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models",
        "authors": [
            "Minjie Zhu",
            "Yichen Zhu",
            "Xin Liu",
            "Ning Liu",
            "Zhiyuan Xu",
            "Chaomin Shen",
            "Yaxin Peng",
            "Zhicai Ou",
            "Feifei Feng",
            "Jian Tang"
        ],
        "published": "2024-03-10T12:43:27Z",
        "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.",
        "pdf_link": "https://arxiv.org/pdf/2403.06199v4.pdf"
    },
    {
        "title": "Can Large Language Models Automatically Score Proficiency of Written Essays?",
        "authors": [
            "Watheq Mansour",
            "Salam Albatarni",
            "Sohaila Eltanbouly",
            "Tamer Elsayed"
        ],
        "published": "2024-03-10T09:39:00Z",
        "summary": "Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.",
        "pdf_link": "https://arxiv.org/pdf/2403.06149v1.pdf"
    },
    {
        "title": "Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity",
        "authors": [
            "Xin Zhang",
            "Linhai Zhang",
            "Deyu Zhou",
            "Guoqiang Xu"
        ],
        "published": "2024-03-10T08:59:04Z",
        "summary": "Due to the sparsity of user data, sentiment analysis on user reviews in\ne-commerce platforms often suffers from poor performance, especially when faced\nwith extremely sparse user data or long-tail labels. Recently, the emergence of\nLLMs has introduced new solutions to such problems by leveraging graph\nstructures to generate supplementary user profiles. However, previous\napproaches have not fully utilized the graph understanding capabilities of LLMs\nand have struggled to adapt to complex streaming data environments. In this\nwork, we propose a fine-grained streaming data synthesis framework that\ncategorizes sparse users into three categories: Mid-tail, Long-tail, and\nExtreme. Specifically, we design LLMs to comprehensively understand three key\ngraph elements in streaming data, including Local-global Graph Understanding,\nSecond-Order Relationship Extraction, and Product Attribute Understanding,\nwhich enables the generation of high-quality synthetic data to effectively\naddress sparsity across different categories. Experimental results on three\nreal datasets demonstrate significant performance improvements, with\nsynthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.06139v1.pdf"
    },
    {
        "title": "FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction Tuning",
        "authors": [
            "Zhuo Zhang",
            "Jingyuan Zhang",
            "Jintao Huang",
            "Lizhen Qu",
            "Hongzhi Zhang",
            "Zenglin Xu"
        ],
        "published": "2024-03-10T08:41:22Z",
        "summary": "Instruction tuning has proven essential for enhancing the performance of\nlarge language models (LLMs) in generating human-aligned responses. However,\ncollecting diverse, high-quality instruction data for tuning poses challenges,\nparticularly in privacy-sensitive domains. Federated instruction tuning (FedIT)\nhas emerged as a solution, leveraging federated learning from multiple data\nowners while preserving privacy. Yet, it faces challenges due to limited\ninstruction data and vulnerabilities to training data extraction attacks. To\naddress these issues, we propose a novel federated algorithm, FedPIT, which\nutilizes LLMs' in-context learning capability to self-generate task-specific\nsynthetic data for training autonomously. Our method employs parameter-isolated\ntraining to maintain global parameters trained on synthetic data and local\nparameters trained on augmented local data, effectively thwarting data\nextraction attacks. Extensive experiments on real-world medical data\ndemonstrate the effectiveness of FedPIT in improving federated few-shot\nperformance while preserving privacy and robustness against data heterogeneity.",
        "pdf_link": "https://arxiv.org/pdf/2403.06131v1.pdf"
    },
    {
        "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
        "authors": [
            "Zhihao Chen",
            "Tao Chen",
            "Chenhui Wang",
            "Chuang Niu",
            "Ge Wang",
            "Hongming Shan"
        ],
        "published": "2024-03-10T08:21:50Z",
        "summary": "While various deep learning methods were proposed for low-dose computed\ntomography (CT) denoising, they often suffer from over-smoothing, blurring, and\nlack of explainability. To alleviate these issues, we propose a plug-and-play\nLanguage-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT\ndenoising models. Our idea is to leverage large language models (LLMs) to align\ndenoised CT and normal dose CT images in both the continuous perceptual space\nand discrete semantic space, which is the first LLM-based scheme for low-dose\nCT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided\nCT autoencoder, which can encode a CT image into continuous high-level features\nand quantize them into a token space to produce semantic tokens derived from\nthe LLM's vocabulary; and the second is to minimize the discrepancy between the\ndenoised CT images and normal dose CT in terms of both encoded high-level\nfeatures and quantized token embeddings derived by the LLM-guided CT\nautoencoder. Extensive experimental results on two public LDCT denoising\ndatasets demonstrate that our LEDA can enhance existing denoising models in\nterms of quantitative metrics and qualitative evaluation, and also provide\nexplainability through language-level image understanding. Source code is\navailable at https://github.com/hao1635/LEDA.",
        "pdf_link": "https://arxiv.org/pdf/2403.06128v1.pdf"
    },
    {
        "title": "FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language",
        "authors": [
            "Yayue Deng",
            "Mohan Xu",
            "Yao Tang"
        ],
        "published": "2024-03-10T07:21:31Z",
        "summary": "The effectiveness of central bank communication is a crucial aspect of\nmonetary policy transmission. While recent research has examined the influence\nof policy communication by the chairs of the Federal Reserve on various\nfinancial variables, much of the literature relies on rule-based or\ndictionary-based methods in parsing the language of the chairs, leaving nuanced\ninformation about policy stance contained in nonverbal emotion out of the\nanalysis. In the current study, we propose the Fine-Grained Monetary Policy\nAnalysis Framework (FMPAF), a novel approach that integrates large language\nmodels (LLMs) with regression analysis to provide a comprehensive analysis of\nthe impact of the press-conference communications of chairs of the Federal\nReserve on financial markets. We conduct extensive comparisons of model\nperformance under different levels of granularity, modalities, and\ncommunication scenarios. Based on our preferred specification, a one-unit\nincrease in the sentiment score is associated with an increase of the price of\nS\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a\n15-basis-point decrease in the policy interest rate, while not leading to a\nsignificant response in exchange rates.",
        "pdf_link": "https://arxiv.org/pdf/2403.06115v1.pdf"
    },
    {
        "title": "Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning",
        "authors": [
            "Kaipeng Wang",
            "Zhi Jing",
            "Yongye Su",
            "Yikun Han"
        ],
        "published": "2024-03-10T06:30:54Z",
        "summary": "This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.06108v2.pdf"
    },
    {
        "title": "Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery",
        "authors": [
            "Yuxuan Yao",
            "Sichun Luo",
            "Haohan Zhao",
            "Guanzhi Deng",
            "Linqi Song"
        ],
        "published": "2024-03-10T05:12:16Z",
        "summary": "We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.",
        "pdf_link": "https://arxiv.org/pdf/2403.06097v2.pdf"
    },
    {
        "title": "RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion",
        "authors": [
            "Huy N. Phan",
            "Hoang N. Phan",
            "Tien N. Nguyen",
            "Nghi D. Q. Bui"
        ],
        "published": "2024-03-10T05:10:34Z",
        "summary": "Code Large Language Models (CodeLLMs) have demonstrated impressive\nproficiency in code completion tasks. However, they often fall short of fully\nunderstanding the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies, which can result in less\nprecise completions. To overcome these limitations, we present \\tool, a\nmultifaceted framework designed to address the complex challenges associated\nwith repository-level code completion. Central to \\tool is the {\\em Repo-level\nSemantic Graph} (RSG), a novel semantic graph structure that encapsulates the\nvast context of code repositories. Furthermore, RepoHyper leverages\n\\textit{Expand and Refine} retrieval method, including a graph expansion and a\nlink prediction algorithm applied to the RSG, enabling the effective retrieval\nand prioritization of relevant code snippets. Our evaluations show that \\tool\nmarkedly outperforms existing techniques in repository-level code completion,\nshowcasing enhanced accuracy across various datasets when compared to several\nstrong baselines. Our implementation of RepoHyper can be found\nat~\\url{https://github.com/FSoft-AI4Code/RepoHyper}.",
        "pdf_link": "https://arxiv.org/pdf/2403.06095v2.pdf"
    },
    {
        "title": "Reframe Anything: LLM Agent for Open World Video Reframing",
        "authors": [
            "Jiawang Cao",
            "Yongliang Wu",
            "Weiheng Chi",
            "Wenbo Zhu",
            "Ziyue Su",
            "Jay Wu"
        ],
        "published": "2024-03-10T03:29:56Z",
        "summary": "The proliferation of mobile devices and social media has revolutionized\ncontent dissemination, with short-form video becoming increasingly prevalent.\nThis shift has introduced the challenge of video reframing to fit various\nscreen aspect ratios, a process that highlights the most compelling parts of a\nvideo. Traditionally, video reframing is a manual, time-consuming task\nrequiring professional expertise, which incurs high production costs. A\npotential solution is to adopt some machine learning models, such as video\nsalient object detection, to automate the process. However, these methods often\nlack generalizability due to their reliance on specific training data. The\nadvent of powerful large language models (LLMs) open new avenues for AI\ncapabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a\nLLM-based agent that leverages visual foundation models and human instructions\nto restructure visual content for video reframing. RAVA operates in three\nstages: perception, where it interprets user instructions and video content;\nplanning, where it determines aspect ratios and reframing strategies; and\nexecution, where it invokes the editing tools to produce the final video. Our\nexperiments validate the effectiveness of RAVA in video salient object\ndetection and real-world reframing tasks, demonstrating its potential as a tool\nfor AI-powered video editing.",
        "pdf_link": "https://arxiv.org/pdf/2403.06070v1.pdf"
    },
    {
        "title": "A Preliminary Exploration of YouTubers' Use of Generative-AI in Content Creation",
        "authors": [
            "Yao Lyu",
            "He Zhang",
            "Shuo Niu",
            "Jie Cai"
        ],
        "published": "2024-03-09T23:22:56Z",
        "summary": "Content creators increasingly utilize generative artificial intelligence\n(Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging\nsites to produce imaginative images, AI-generated videos, and articles using\nLarge Language Models (LLMs). Despite its growing popularity, there remains an\nunderexplored area concerning the specific domains where AI-generated content\nis being applied, and the methodologies content creators employ with Gen-AI\ntools during the creation process. This study initially explores this emerging\narea through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI\nusage. Our research focuses on identifying the content domains, the variety of\ntools used, the activities performed, and the nature of the final products\ngenerated by Gen-AI in the context of user-generated content.",
        "pdf_link": "https://arxiv.org/pdf/2403.06039v1.pdf"
    },
    {
        "title": "Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages",
        "authors": [
            "Christopher Toukmaji"
        ],
        "published": "2024-03-09T21:36:13Z",
        "summary": "Large pre-trained language models (PLMs) are at the forefront of advances in\nNatural Language Processing. One widespread use case of PLMs is \"prompting\" -\nor in-context learning - where a user provides a description of a task and some\ncompleted examples of the task to a PLM as context before prompting the PLM to\nperform the task on a new example. Only the largest, most capable PLMs are able\nto perform in-context learning effectively, and these models are typically\ntrained with a predominantly English corpus, leaving all other languages\nbehind. The data limitations in most languages preclude the training of\nlanguage-specific PLMs capable of prompting. Albeit the surge in work of\nprompting settings, it is still unclear how PLMs should be adapted\ncross-lingually specifically for prompting. We evaluate the possible methods to\nadapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for\nprompting in low-resource languages, namely for Kinyarwanda, Hausa, and\nLuganda. We consider three methods: few-shot prompting (prompt),\nlanguage-adaptive fine-tuning (LAFT), and neural machine translation\n(translate), and evaluate on abstractive summarization, multi-class topic\nclassification, and named-entity recognition. Although LAFT carries the\ngreatest compute cost and intuitively should lead to the best results, our\nexperiments exhibit that LAFT is only occasionally the optimal choice for\nadapting PLMs for prompting. Rather, the translate and prompt settings are a\ncompute-efficient and cost-effective method of few-shot prompting for the\nselected low-resource languages. We find that the results are task and language\ndependent but find that the prompting method is the best on average across all\ntasks and languages. Results show that the prompt setting performs better than\nboth translating and LAFT with statistical significance for all shots when\naggregated across all tasks and languages.",
        "pdf_link": "https://arxiv.org/pdf/2403.06018v1.pdf"
    },
    {
        "title": "Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations",
        "authors": [
            "Swapnaja Achintalwar",
            "Adriana Alvarado Garcia",
            "Ateret Anaby-Tavor",
            "Ioana Baldini",
            "Sara E. Berger",
            "Bishwaranjan Bhattacharjee",
            "Djallel Bouneffouf",
            "Subhajit Chaudhury",
            "Pin-Yu Chen",
            "Lamogha Chiazor",
            "Elizabeth M. Daly",
            "Rog\u00e9rio Abreu de Paula",
            "Pierre Dognin",
            "Eitan Farchi",
            "Soumya Ghosh",
            "Michael Hind",
            "Raya Horesh",
            "George Kour",
            "Ja Young Lee",
            "Erik Miehling",
            "Keerthiram Murugesan",
            "Manish Nagireddy",
            "Inkit Padhi",
            "David Piorkowski",
            "Ambrish Rawat",
            "Orna Raz",
            "Prasanna Sattigeri",
            "Hendrik Strobelt",
            "Sarathkrishna Swaminathan",
            "Christoph Tillmann",
            "Aashka Trivedi",
            "Kush R. Varshney",
            "Dennis Wei",
            "Shalisha Witherspooon",
            "Marcel Zalmanovici"
        ],
        "published": "2024-03-09T21:07:16Z",
        "summary": "Large language models (LLMs) are susceptible to a variety of risks, from\nnon-faithful output to biased and toxic generations. Due to several limiting\nfactors surrounding LLMs (training cost, API access, data availability, etc.),\nit may not always be feasible to impose direct safety constraints on a deployed\nmodel. Therefore, an efficient and reliable alternative is required. To this\nend, we present our ongoing efforts to create and deploy a library of\ndetectors: compact and easy-to-build classification models that provide labels\nfor various harms. In addition to the detectors themselves, we discuss a wide\nrange of uses for these detector models - from acting as guardrails to enabling\neffective AI governance. We also deep dive into inherent challenges in their\ndevelopment and discuss future work aimed at making the detectors more reliable\nand broadening their scope.",
        "pdf_link": "https://arxiv.org/pdf/2403.06009v1.pdf"
    },
    {
        "title": "Calibrating Large Language Models Using Their Generations Only",
        "authors": [
            "Dennis Ulmer",
            "Martin Gubri",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seong Joon Oh"
        ],
        "published": "2024-03-09T17:46:24Z",
        "summary": "As large language models (LLMs) are increasingly deployed in user-facing\napplications, building trust and maintaining safety by accurately quantifying a\nmodel's confidence in its prediction becomes even more important. However,\nfinding effective ways to calibrate LLMs - especially when the only interface\nto the models is their generated text - remains a challenge. We propose APRICOT\n(auxiliary prediction of confidence targets): A method to set confidence\ntargets and train an additional model that predicts an LLM's confidence based\non its textual input and output alone. This approach has several advantages: It\nis conceptually simple, does not require access to the target model beyond its\noutput, does not interfere with the language generation, and has a multitude of\npotential usages, for instance by verbalizing the predicted confidence or\nadjusting the given answer based on the confidence. We show how our approach\nperforms competitively in terms of calibration error for white-box and\nblack-box LLMs on closed-book question-answering to detect incorrect LLM\nanswers.",
        "pdf_link": "https://arxiv.org/pdf/2403.05973v1.pdf"
    },
    {
        "title": "Thread Detection and Response Generation using Transformers with Prompt Optimisation",
        "authors": [
            "Kevin Joshua T",
            "Arnav Agarwal",
            "Shriya Sanjay",
            "Yash Sarda",
            "John Sahaya Rani Alex",
            "Saurav Gupta",
            "Sushant Kumar",
            "Vishwanath Kamath"
        ],
        "published": "2024-03-09T14:50:20Z",
        "summary": "Conversational systems are crucial for human-computer interaction, managing\ncomplex dialogues by identifying threads and prioritising responses. This is\nespecially vital in multi-party conversations, where precise identification of\nthreads and strategic response prioritisation ensure efficient dialogue\nmanagement. To address these challenges an end-to-end model that identifies\nthreads and prioritises their response generation based on the importance was\ndeveloped, involving a systematic decomposition of the problem into discrete\ncomponents - thread detection, prioritisation, and performance optimisation\nwhich was meticulously analysed and optimised. These refined components\nseamlessly integrate into a unified framework, in conversational systems.\nLlama2 7b is used due to its high level of generalisation but the system can be\nupdated with any open source Large Language Model(LLM). The computational\ncapabilities of the Llama2 model was augmented by using fine tuning methods and\nstrategic prompting techniques to optimise the model's performance, reducing\ncomputational time and increasing the accuracy of the model. The model achieves\nup to 10x speed improvement, while generating more coherent results compared to\nexisting models.",
        "pdf_link": "https://arxiv.org/pdf/2403.05931v1.pdf"
    },
    {
        "title": "GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual Affective Computing",
        "authors": [
            "Hao Lu",
            "Xuesong Niu",
            "Jiyao Wang",
            "Yin Wang",
            "Qingyong Hu",
            "Jiaqi Tang",
            "Yuting Zhang",
            "Kaishen Yuan",
            "Bin Huang",
            "Zitong Yu",
            "Dengbo He",
            "Shuiguang Deng",
            "Hao Chen",
            "Yingcong Chen",
            "Shiguang Shan"
        ],
        "published": "2024-03-09T13:56:25Z",
        "summary": "Multimodal large language models (MLLMs) are designed to process and\nintegrate information from multiple sources, such as text, speech, images, and\nvideos. Despite its success in language understanding, it is critical to\nevaluate the performance of downstream tasks for better human-centric\napplications. This paper assesses the application of MLLMs with 5 crucial\nabilities for affective computing, spanning from visual affective tasks and\nreasoning tasks. The results show that \\gpt has high accuracy in facial action\nunit recognition and micro-expression detection while its general facial\nexpression recognition performance is not accurate. We also highlight the\nchallenges of achieving fine-grained micro-expression recognition and the\npotential for further study and demonstrate the versatility and potential of\n\\gpt for handling advanced tasks in emotion recognition and related fields by\nintegrating with task-related agents for more complex tasks, such as heart rate\nestimation through signal processing. In conclusion, this paper provides\nvaluable insights into the potential applications and challenges of MLLMs in\nhuman-centric computing. Our interesting examples are at\nhttps://github.com/EnVision-Research/GPT4Affectivity.",
        "pdf_link": "https://arxiv.org/pdf/2403.05916v2.pdf"
    },
    {
        "title": "KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge Graphs and Ranking Techniques",
        "authors": [
            "Rui Yang",
            "Haoran Liu",
            "Edison Marrese-Taylor",
            "Qingcheng Zeng",
            "Yu He Ke",
            "Wanxin Li",
            "Lechao Cheng",
            "Qingyu Chen",
            "James Caverlee",
            "Yutaka Matsuo",
            "Irene Li"
        ],
        "published": "2024-03-09T11:23:38Z",
        "summary": "Large Language Models (LLMs) have significantly advanced healthcare\ninnovation on generation capabilities. However, their application in real\nclinical settings is challenging due to potential deviations from medical facts\nand inherent biases. In this work, we develop an augmented LLM framework,\nKG-Rank, which leverages a medical knowledge graph (KG) with ranking and\nre-ranking techniques, aiming to improve free-text question-answering (QA) in\nthe medical domain. Specifically, upon receiving a question, we initially\nretrieve triplets from a medical KG to gather factual information.\nSubsequently, we innovatively apply ranking methods to refine the ordering of\nthese triplets, aiming to yield more precise answers. To the best of our\nknowledge, KG-Rank is the first application of ranking models combined with KG\nin medical QA specifically for generating long answers. Evaluation of four\nselected medical QA datasets shows that KG-Rank achieves an improvement of over\n18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it\nrealizes a 14% improvement in ROUGE-L, showing the effectiveness and potential\nof KG-Rank.",
        "pdf_link": "https://arxiv.org/pdf/2403.05881v2.pdf"
    },
    {
        "title": "LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content",
        "authors": [
            "Qihao Zhao",
            "Yalun Dai",
            "Hao Li",
            "Wei Hu",
            "Fan Zhang",
            "Jun Liu"
        ],
        "published": "2024-03-09T09:52:15Z",
        "summary": "Long-tail recognition is challenging because it requires the model to learn\ngood representations from tail categories and address imbalances across all\ncategories. In this paper, we propose a novel generative and fine-tuning\nframework, LTGC, to handle long-tail recognition via leveraging generated\ncontent. Firstly, inspired by the rich implicit knowledge in large-scale models\n(e.g., large language models, LLMs), LTGC leverages the power of these models\nto parse and reason over the original tail data to produce diverse tail-class\ncontent. We then propose several novel designs for LTGC to ensure the quality\nof the generated data and to efficiently fine-tune the model using both the\ngenerated and original data. The visualization demonstrates the effectiveness\nof the generation module in LTGC, which produces accurate and diverse tail\ndata. Additionally, the experimental results demonstrate that our LTGC\noutperforms existing state-of-the-art methods on popular long-tailed\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05854v3.pdf"
    },
    {
        "title": "Optimizing LLM Queries in Relational Workloads",
        "authors": [
            "Shu Liu",
            "Asim Biswal",
            "Audrey Cheng",
            "Xiangxi Mo",
            "Shiyi Cao",
            "Joseph E. Gonzalez",
            "Ion Stoica",
            "Matei Zaharia"
        ],
        "published": "2024-03-09T07:01:44Z",
        "summary": "Analytical database providers (e.g., Redshift, Databricks, BigQuery) have\nrapidly added support for invoking Large Language Models (LLMs) through native\nuser-defined functions (UDFs) to help users perform natural language tasks,\nsuch as classification, entity extraction, and translation, inside analytical\nworkloads. For instance, an analyst might want to extract customer sentiments\non millions of product reviews. However, LLM inference is highly expensive in\nboth computational and economic terms: for example, an NVIDIA L4 GPU running\nLlama2-7B can only process 6 KB of text per second. In this paper, we explore\nhow to optimize LLM inference for analytical workloads that invoke LLMs within\nrelational queries. We show that relational queries present novel opportunities\nfor accelerating LLM inference, including reordering rows to maximize key-value\n(KV) cache reuse within the LLM inference engine, reordering columns within a\nrow to further increase cache reuse, and deduplicating redundant inference\nrequests. We implement these optimizations in Apache Spark, with vLLM as the\nmodel serving backend and achieve up to 4.4x improvement in end-to-end latency\non a benchmark of diverse LLM-based queries on real datasets. To the best of\nour knowledge, this is the first work to explicitly address the problem of\noptimizing LLM invocations within SQL queries.",
        "pdf_link": "https://arxiv.org/pdf/2403.05821v1.pdf"
    },
    {
        "title": "MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs",
        "authors": [
            "Yerin Hwang",
            "Yongil Kim",
            "Yunah Jang",
            "Jeesoo Bang",
            "Hyunkyung Bae",
            "Kyomin Jung"
        ],
        "published": "2024-03-09T06:28:48Z",
        "summary": "Despite advancements in on-topic dialogue systems, effectively managing topic\nshifts within dialogues remains a persistent challenge, largely attributed to\nthe limited availability of training datasets. To address this issue, we\npropose Multi-Passage to Dialogue (MP2D), a data generation framework that\nautomatically creates conversational question-answering datasets with natural\ntopic transitions. By leveraging the relationships between entities in a\nknowledge graph, MP2D maps the flow of topics within a dialogue, effectively\nmirroring the dynamics of human conversation. It retrieves relevant passages\ncorresponding to the topics and transforms them into dialogues through the\npassage-to-dialogue method. Through quantitative and qualitative experiments,\nwe demonstrate MP2D's efficacy in generating dialogue with natural topic\nshifts. Furthermore, this study introduces a novel benchmark for topic shift\ndialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large\nLanguage Models (LLMs) struggle to handle topic shifts in dialogue effectively,\nand we showcase the performance improvements of models trained on datasets\ngenerated by MP2D across diverse topic shift dialogue tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05814v1.pdf"
    },
    {
        "title": "$\\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
        "authors": [
            "Zijie Pan",
            "Yushan Jiang",
            "Sahil Garg",
            "Anderson Schneider",
            "Yuriy Nevmyvaka",
            "Dongjin Song"
        ],
        "published": "2024-03-09T05:20:48Z",
        "summary": "Recently, there has been a growing interest in leveraging pre-trained large\nlanguage models (LLMs) for various time series applications. However, the\nsemantic space of LLMs, established through the pre-training, is still\nunderexplored and may help yield more distinctive and informative\nrepresentations to facilitate time series forecasting. To this end, we propose\nSemantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the\npre-trained semantic space with time series embeddings space and perform time\nseries forecasting based on learned prompts from the joint space. We first\ndesign a tokenization module tailored for cross-modality alignment, which\nexplicitly concatenates patches of decomposed time series components to create\nembeddings that effectively encode the temporal dynamics. Next, we leverage the\npre-trained word token embeddings to derive semantic anchors and align selected\nanchors with time series embeddings by maximizing the cosine similarity in the\njoint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as\nprompts to provide strong indicators (context) for time series that exhibit\ndifferent temporal dynamics. With thorough empirical studies on multiple\nbenchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve\nsuperior forecasting performance over state-of-the-art baselines. Furthermore,\nour ablation studies and visualizations verify the necessity of prompt learning\ninformed by semantic space.",
        "pdf_link": "https://arxiv.org/pdf/2403.05798v1.pdf"
    },
    {
        "title": "ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes",
        "authors": [
            "Zhichao Yang",
            "Avijit Mitra",
            "Sunjae Kwon",
            "Hong Yu"
        ],
        "published": "2024-03-09T04:58:25Z",
        "summary": "The advancement of natural language processing (NLP) systems in healthcare\nhinges on language model ability to interpret the intricate information\ncontained within clinical notes. This process often requires integrating\ninformation from various time points in a patient's medical history. However,\nmost earlier clinical language models were pretrained with a context length\nlimited to roughly one clinical document. In this study, We introduce\nClinicalMamba, a specialized version of the Mamba language model, pretrained on\na vast corpus of longitudinal clinical notes to address the unique linguistic\ncharacteristics and information processing needs of the medical domain.\nClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a\nsuperior performance in modeling clinical language across extended text lengths\ncompared to Mamba and clinical Llama. With few-shot learning, ClinicalMamba\nachieves notable benchmarks in speed and accuracy, outperforming existing\nclinical language models and general domain large models like GPT-4 in\nlongitudinal clinical notes information extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05795v1.pdf"
    },
    {
        "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
        "authors": [
            "Wangtao Sun",
            "Haotian Xu",
            "Xuanqing Yu",
            "Pei Chen",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2024-03-09T04:20:46Z",
        "summary": "Although Large Language Models (LLMs) are showing impressive performance on a\nwide range of Natural Language Processing tasks, researchers have found that\nthey still have limited ability to conduct induction. Recent works mainly adopt\n``post processes'' paradigms to improve the performance of LLMs on induction\n(e.g., the hypothesis search & refinement methods), but their performance is\nstill constrained by the inherent inductive capability of the LLMs. In this\npaper, we propose a novel framework, Induction through Deduction (ItD), to\nenable the LLMs to teach themselves induction through deduction. The ItD\nframework is composed of two main components: a Deductive Data Generation\nmodule to generate induction data and a Naive Bayesian Induction module to\noptimize the fine-tuning and decoding of LLMs. Our empirical results showcase\nthe effectiveness of ItD on two induction benchmarks, achieving relative\nperformance improvement of 36% and 10% compared with previous state-of-the-art,\nrespectively. Our ablation study verifies the effectiveness of two key modules\nof ItD. We also verify the effectiveness of ItD across different LLMs and\ndeductors. The data and code of this paper can be found at\nhttps://anonymous.4open.science/r/ItD-E844.",
        "pdf_link": "https://arxiv.org/pdf/2403.05789v1.pdf"
    },
    {
        "title": "FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs",
        "authors": [
            "Shamik Roy",
            "Sailik Sengupta",
            "Daniele Bonadiman",
            "Saab Mansour",
            "Arshit Gupta"
        ],
        "published": "2024-03-09T02:27:45Z",
        "summary": "Planning is a crucial task for agents in task oriented dialogs (TODs). Human\nagents typically resolve user issues by following predefined workflows,\ndecomposing workflow steps into actionable items, and performing actions by\nexecuting APIs in order; all of which require reasoning and planning. With the\nrecent advances in LLMs, there have been increasing attempts to use them for\ntask planning and API usage. However, the faithfulness of the plans to\npredefined workflows and API dependencies, is not guaranteed with LLMs.\nMoreover, workflows in real life are often custom-defined and prone to changes;\nhence, adaptation is desirable. To study this, we propose the problem of\nfaithful planning in TODs that needs to resolve user intents by following\npredefined flows and preserving API dependencies. To solve this problem, we\npropose FLAP, a Flow-Adhering Planning algorithm based on constrained decoding\nwith lookahead heuristic for LLMs. Our algorithm alleviates the need for\nfinetuning LLMs using domain specific (plan/dependency) data, enables quick\nadaptation to predefined flows, and outperforms other decoding and\nprompting-based baselines. Further, our algorithm empowers smaller LLMs (7B) to\nperform at par larger LLMs (30B-40B).",
        "pdf_link": "https://arxiv.org/pdf/2403.05766v2.pdf"
    },
    {
        "title": "Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text",
        "authors": [
            "Sara Abdali",
            "Richard Anarfi",
            "CJ Barberan",
            "Jia He"
        ],
        "published": "2024-03-09T01:13:54Z",
        "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Generation (NLG) by demonstrating an impressive ability to generate\nhuman-like text. However, their widespread usage introduces challenges that\nnecessitate thoughtful examination, ethical scrutiny, and responsible\npractices. In this study, we delve into these challenges, explore existing\nstrategies for mitigating them, with a particular emphasis on identifying\nAI-generated text as the ultimate solution. Additionally, we assess the\nfeasibility of detection from a theoretical perspective and propose novel\nresearch directions to address the current limitations in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.05750v1.pdf"
    },
    {
        "title": "A Novel Nuanced Conversation Evaluation Framework for Large Language Models in Mental Health",
        "authors": [
            "Alexander Marrapese",
            "Basem Suleiman",
            "Imdad Ullah",
            "Juno Kim"
        ],
        "published": "2024-03-08T23:46:37Z",
        "summary": "Understanding the conversation abilities of Large Language Models (LLMs) can\nhelp lead to its more cautious and appropriate deployment. This is especially\nimportant for safety-critical domains like mental health, where someone's life\nmay depend on the exact wording of a response to an urgent question. In this\npaper, we propose a novel framework for evaluating the nuanced conversation\nabilities of LLMs. Within it, we develop a series of quantitative metrics\ndeveloped from literature on using psychotherapy conversation analysis\nliterature. While we ensure that our framework and metrics are transferable by\nresearchers to relevant adjacent domains, we apply them to the mental health\nfield. We use our framework to evaluate several popular frontier LLMs,\nincluding some GPT and Llama models, through a verified mental health dataset.\nOur results show that GPT4 Turbo can perform significantly more similarly to\nverified therapists than other selected LLMs. We conduct additional analysis to\nexamine how LLM conversation performance varies across specific mental health\ntopics. Our results indicate that GPT4 Turbo performs well in achieving high\ncorrelation with verified therapists in particular topics such as Parenting and\nRelationships. We believe our contributions will help researchers develop\nbetter LLMs that, in turn, will more positively support people's lives.",
        "pdf_link": "https://arxiv.org/pdf/2403.09705v1.pdf"
    },
    {
        "title": "A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries",
        "authors": [
            "Asad Aali",
            "Dave Van Veen",
            "Yamin Ishraq Arefeen",
            "Jason Hom",
            "Christian Bluethgen",
            "Eduardo Pontes Reis",
            "Sergios Gatidis",
            "Namuun Clifford",
            "Joseph Daws",
            "Arash S. Tehrani",
            "Jangwon Kim",
            "Akshay S. Chaudhari"
        ],
        "published": "2024-03-08T23:17:55Z",
        "summary": "Brief hospital course (BHC) summaries are common clinical documents generated\nby summarizing clinical notes. While large language models (LLMs) depict\nremarkable capabilities in automating real-world tasks, their capabilities for\nhealthcare applications such as BHC synthesis have not been shown. To enable\nthe adaptation of LLMs for BHC synthesis, we introduce a novel benchmark\nconsisting of a pre-processed dataset extracted from MIMIC-IV notes,\nencapsulating clinical note, and brief hospital course (BHC) pairs. We assess\nthe performance of two general-purpose LLMs and three healthcare-adapted LLMs\nto improve BHC synthesis from clinical notes. Using clinical notes as input for\ngenerating BHCs, we apply prompting-based (using in-context learning) and\nfine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We quantitatively evaluate the performance of these LLMs across varying\ncontext-length inputs using conventional natural language similarity metrics.\nWe further perform a qualitative study where five diverse clinicians blindly\ncompare clinician-written BHCs and two LLM-generated BHCs for 30 samples across\nmetrics of comprehensiveness, conciseness, factual correctness, and fluency.\nOverall, we present a new benchmark and pre-processed dataset for using LLMs in\nBHC synthesis from clinical notes. We observe high-quality summarization\nperformance for both in-context proprietary and fine-tuned open-source LLMs\nusing both quantitative metrics and a qualitative clinical reader study. We\npropose our work as a benchmark to motivate future works to adapt and assess\nthe performance of LLMs in BHC synthesis.",
        "pdf_link": "https://arxiv.org/pdf/2403.05720v1.pdf"
    },
    {
        "title": "Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?",
        "authors": [
            "Lennart Wachowiak",
            "Andrew Coles",
            "Oya Celiktutan",
            "Gerard Canal"
        ],
        "published": "2024-03-08T22:23:23Z",
        "summary": "Large language models (LLMs) are increasingly used in robotics, especially\nfor high-level action planning. Meanwhile, many robotics applications involve\nhuman supervisors or collaborators. Hence, it is crucial for LLMs to generate\nsocially acceptable actions that align with people's preferences and values. In\nthis work, we test whether LLMs capture people's intuitions about behavior\njudgments and communication preferences in human-robot interaction (HRI)\nscenarios. For evaluation, we reproduce three HRI user studies, comparing the\noutput of LLMs with that of real participants. We find that GPT-4 strongly\noutperforms other models, generating answers that correlate strongly with\nusers' answers in two studies $\\unicode{x2014}$ the first study dealing with\nselecting the most appropriate communicative act for a robot in various\nsituations ($r_s$ = 0.82), and the second with judging the desirability,\nintentionality, and surprisingness of behavior ($r_s$ = 0.83). However, for the\nlast study, testing whether people judge the behavior of robots and humans\ndifferently, no model achieves strong correlations. Moreover, we show that\nvision models fail to capture the essence of video stimuli and that LLMs tend\nto rate different communicative acts and behavior desirability higher than\npeople.",
        "pdf_link": "https://arxiv.org/pdf/2403.05701v1.pdf"
    },
    {
        "title": "Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations",
        "authors": [
            "Swapnaja Achintalwar",
            "Ioana Baldini",
            "Djallel Bouneffouf",
            "Joan Byamugisha",
            "Maria Chang",
            "Pierre Dognin",
            "Eitan Farchi",
            "Ndivhuwo Makondo",
            "Aleksandra Mojsilovic",
            "Manish Nagireddy",
            "Karthikeyan Natesan Ramamurthy",
            "Inkit Padhi",
            "Orna Raz",
            "Jesus Rios",
            "Prasanna Sattigeri",
            "Moninder Singh",
            "Siphiwe Thwala",
            "Rosario A. Uceda-Sosa",
            "Kush R. Varshney"
        ],
        "published": "2024-03-08T21:26:49Z",
        "summary": "The alignment of large language models is usually done by model providers to\nadd or control behaviors that are common or universally understood across use\ncases and contexts. In contrast, in this article, we present an approach and\narchitecture that empowers application developers to tune a model to their\nparticular values, social norms, laws and other regulations, and orchestrate\nbetween potentially conflicting requirements in context. We lay out three main\ncomponents of such an Alignment Studio architecture: Framers, Instructors, and\nAuditors that work in concert to control the behavior of a language model. We\nillustrate this approach with a running example of aligning a company's\ninternal-facing enterprise chatbot to its business conduct guidelines.",
        "pdf_link": "https://arxiv.org/pdf/2403.09704v1.pdf"
    },
    {
        "title": "DP-TabICL: In-Context Learning with Differentially Private Tabular Data",
        "authors": [
            "Alycia N. Carey",
            "Karuna Bhaila",
            "Kennedy Edemacu",
            "Xintao Wu"
        ],
        "published": "2024-03-08T21:19:01Z",
        "summary": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks by conditioning on demonstrations of question-answer pairs and it has\nbeen shown to have comparable performance to costly model retraining and\nfine-tuning. Recently, ICL has been extended to allow tabular data to be used\nas demonstration examples by serializing individual records into natural\nlanguage formats. However, it has been shown that LLMs can leak information\ncontained in prompts, and since tabular data often contain sensitive\ninformation, understanding how to protect the underlying tabular data used in\nICL is a critical area of research. This work serves as an initial\ninvestigation into how to use differential privacy (DP) -- the long-established\ngold standard for data privacy and anonymization -- to protect tabular data\nused in ICL. Specifically, we investigate the application of DP mechanisms for\nprivate tabular ICL via data privatization prior to serialization and\nprompting. We formulate two private ICL frameworks with provable privacy\nguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios\nvia injecting noise into individual records or group statistics, respectively.\nWe evaluate our DP-based frameworks on eight real-world tabular datasets and\nacross multiple ICL and DP settings. Our evaluations show that DP-based ICL can\nprotect the privacy of the underlying tabular data while achieving comparable\nperformance to non-LLM baselines, especially under high privacy regimes.",
        "pdf_link": "https://arxiv.org/pdf/2403.05681v1.pdf"
    },
    {
        "title": "Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4",
        "authors": [
            "Qingqing Zhu",
            "Benjamin Hou",
            "Tejas S. Mathai",
            "Pritam Mukherjee",
            "Qiao Jin",
            "Xiuying Chen",
            "Zhizheng Wang",
            "Ruida Cheng",
            "Ronald M. Summers",
            "Zhiyong Lu"
        ],
        "published": "2024-03-08T21:16:28Z",
        "summary": "The volume of CT exams being done in the world has been rising every year,\nwhich has led to radiologist burn-out. Large Language Models (LLMs) have the\npotential to reduce their burden, but their adoption in the clinic depends on\nradiologist trust, and easy evaluation of generated content. Presently, many\nautomated methods are available to evaluate the reports generated for chest\nradiographs, but such an approach is not available for CT presently. In this\npaper, we propose a novel evaluation framework to judge the capabilities of\nvision-language LLMs in generating accurate summaries of CT-based\nabnormalities. CT slices containing an abnormality (e.g., lesion) were input to\na vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text\nsummary of the predicted characteristics of the abnormality. Next, a GPT-4\nmodel decomposed the summary into specific aspects (body part, location, type,\nand attributes), automatically evaluated the characteristics against the\nground-truth, and generated a score for each aspect based on its clinical\nrelevance and factual accuracy. These scores were then contrasted against those\nobtained from a clinician, and a high correlation ( 85%, p < .001) was\nobserved. Although GPT-4V outperformed other models in our evaluation, it still\nrequires overall improvement. Our evaluation method offers valuable insights\ninto the specific areas that need the most enhancement, guiding future\ndevelopment in this field.",
        "pdf_link": "https://arxiv.org/pdf/2403.05680v1.pdf"
    },
    {
        "title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
        "authors": [
            "Wenqi Jiang",
            "Shuai Zhang",
            "Boran Han",
            "Jie Wang",
            "Bernie Wang",
            "Tim Kraska"
        ],
        "published": "2024-03-08T21:09:20Z",
        "summary": "Retrieval-augmented generation (RAG) can enhance the generation quality of\nlarge language models (LLMs) by incorporating external token databases.\nHowever, retrievals from large databases can constitute a substantial portion\nof the overall generation time, particularly when retrievals are periodically\nperformed to align the retrieved content with the latest states of generation.\nIn this paper, we introduce PipeRAG, a novel algorithm-system co-design\napproach to reduce generation latency and enhance generation quality. PipeRAG\nintegrates (1) pipeline parallelism to enable concurrent retrieval and\ngeneration processes, (2) flexible retrieval intervals to maximize the\nefficiency of pipeline parallelism, and (3) a performance model to\nautomatically balance retrieval quality and latency based on the generation\nstates and underlying hardware. Our evaluation shows that, by combining the\nthree aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in\nend-to-end generation latency while improving generation quality. These\npromising results showcase the effectiveness of co-designing algorithms with\nunderlying systems, paving the way for the adoption of PipeRAG in future RAG\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.05676v1.pdf"
    },
    {
        "title": "Tuning-Free Accountable Intervention for LLM Deployment -- A Metacognitive Approach",
        "authors": [
            "Zhen Tan",
            "Jie Peng",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "published": "2024-03-08T19:18:53Z",
        "summary": "Large Language Models (LLMs) have catalyzed transformative advances across a\nspectrum of natural language processing tasks through few-shot or zero-shot\nprompting, bypassing the need for parameter tuning. While convenient, this\nmodus operandi aggravates ``hallucination'' concerns, particularly given the\nenigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns\nare exacerbated in high-stakes applications (e.g., healthcare), where\nunaccountable decision errors can lead to devastating consequences. In\ncontrast, human decision-making relies on nuanced cognitive processes, such as\nthe ability to sense and adaptively correct misjudgments through conceptual\nunderstanding. Drawing inspiration from human cognition, we propose an\ninnovative \\textit{metacognitive} approach, dubbed \\textbf{CLEAR}, to equip\nLLMs with capabilities for self-aware error identification and correction. Our\nframework facilitates the construction of concept-specific sparse subnetworks\nthat illuminate transparent decision pathways. This provides a novel interface\nfor model \\textit{intervention} after deployment. Our intervention offers\ncompelling advantages: (\\textit{i})~at deployment or inference time, our\nmetacognitive LLMs can self-consciously identify potential mispredictions with\nminimum human involvement, (\\textit{ii})~the model has the capability to\nself-correct its errors efficiently, obviating the need for additional tuning,\nand (\\textit{iii})~the rectification procedure is not only self-explanatory but\nalso user-friendly, enhancing the interpretability and accessibility of the\nmodel. By integrating these metacognitive features, our approach pioneers a new\npath toward engendering greater trustworthiness and accountability in the\ndeployment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.05636v1.pdf"
    },
    {
        "title": "Can Large Language Models Play Games? A Case Study of A Self-Play Approach",
        "authors": [
            "Hongyi Guo",
            "Zhihan Liu",
            "Yufeng Zhang",
            "Zhaoran Wang"
        ],
        "published": "2024-03-08T19:16:29Z",
        "summary": "Large Language Models (LLMs) harness extensive data from the Internet,\nstoring a broad spectrum of prior knowledge. While LLMs have proven beneficial\nas decision-making aids, their reliability is hampered by limitations in\nreasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo\nTree Search (MCTS) is a heuristic search algorithm that provides reliable\ndecision-making solutions, achieved through recursive rollouts and self-play.\nHowever, the effectiveness of MCTS relies heavily on heuristic pruning and\nexternal value functions, particularly in complex decision scenarios. This work\nintroduces an innovative approach that bolsters LLMs with MCTS self-play to\nefficiently resolve deterministic turn-based zero-sum games (DTZG), such as\nchess and go, without the need for additional training. Specifically, we\nutilize LLMs as both action pruners and proxies for value functions without the\nneed for additional training. We theoretically prove that the suboptimality of\nthe estimated value in our proposed method scales with $\\tilde{\\mathcal\nO}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} +\n\\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations,\n$|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM,\nand $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the\nerrors incurred by adopting LLMs as action space pruner and value function\nproxy, respectively. Our experiments in chess and go demonstrate the capability\nof our method to address challenges beyond the scope of MCTS and improve the\nperformance of the directly application of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.05632v1.pdf"
    },
    {
        "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM",
        "authors": [
            "Hao Kang",
            "Qingru Zhang",
            "Souvik Kundu",
            "Geonhwa Jeong",
            "Zaoxing Liu",
            "Tushar Krishna",
            "Tuo Zhao"
        ],
        "published": "2024-03-08T18:48:30Z",
        "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
        "pdf_link": "https://arxiv.org/pdf/2403.05527v2.pdf"
    },
    {
        "title": "Beyond Finite Data: Towards Data-free Out-of-distribution Generalization via Extrapolation",
        "authors": [
            "Yijiang Li",
            "Sucheng Ren",
            "Weipeng Deng",
            "Yuzhi Xu",
            "Ying Gao",
            "Edith Ngai",
            "Haohan Wang"
        ],
        "published": "2024-03-08T18:44:23Z",
        "summary": "Out-of-distribution (OOD) generalization is a favorable yet challenging\nproperty for deep neural networks. The core challenges lie in the limited\navailability of source domains that help models learn an invariant\nrepresentation from the spurious features. Various domain augmentation have\nbeen proposed but largely rely on interpolating existing domains and frequently\nface difficulties in creating truly \"novel\" domains. Humans, on the other hand,\ncan easily extrapolate novel domains, thus, an intriguing question arises: How\ncan neural networks extrapolate like humans and achieve OOD generalization?\n  We introduce a novel approach to domain extrapolation that leverages\nreasoning ability and the extensive knowledge encapsulated within large\nlanguage models (LLMs) to synthesize entirely new domains. Starting with the\nclass of interest, we query the LLMs to extract relevant knowledge for these\nnovel domains. We then bridge the gap between the text-centric knowledge\nderived from LLMs and the pixel input space of the model using text-to-image\ngeneration techniques. By augmenting the training set of domain generalization\ndatasets with high-fidelity, photo-realistic images of these new domains, we\nachieve significant improvements over all existing methods, as demonstrated in\nboth single and multi-domain generalization across various benchmarks.\n  With the ability to extrapolate any domains for any class, our method has the\npotential to learn a generalized model for any task without any data. To\nillustrate, we put forth a much more difficult setting termed, data-free domain\ngeneralization, that aims to learn a generalized model in the absence of any\ncollected data. Our empirical findings support the above argument and our\nmethods exhibit commendable performance in this setting, even surpassing the\nsupervised setting by approximately 1-2\\% on datasets such as VLCS.",
        "pdf_link": "https://arxiv.org/pdf/2403.05523v2.pdf"
    },
    {
        "title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate",
        "authors": [
            "Katie Kang",
            "Eric Wallace",
            "Claire Tomlin",
            "Aviral Kumar",
            "Sergey Levine"
        ],
        "published": "2024-03-08T18:28:13Z",
        "summary": "Large language models (LLMs) have a tendency to generate plausible-sounding\nyet factually incorrect responses, especially when queried on unfamiliar\nconcepts. In this work, we explore the underlying mechanisms that govern how\nfinetuned LLMs hallucinate. Our investigation reveals an interesting pattern:\nas inputs become more unfamiliar, LLM outputs tend to default towards a\n``hedged'' prediction, whose form is determined by how the unfamiliar examples\nin the finetuning data are supervised. Thus, by strategically modifying these\nexamples' supervision, we can control LLM predictions for unfamiliar inputs\n(e.g., teach them to say ``I don't know''). Based on these principles, we\ndevelop an RL approach that more reliably mitigates hallucinations for\nlong-form generation tasks, by tackling the challenges presented by reward\nmodel hallucinations. We validate our findings with a series of controlled\nexperiments in multiple-choice QA on MMLU, as well as long-form biography and\nbook/movie plot generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05612v1.pdf"
    },
    {
        "title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
        "authors": [
            "Arijit Nag",
            "Animesh Mukherjee",
            "Niloy Ganguly",
            "Soumen Chakrabarti"
        ],
        "published": "2024-03-08T16:37:36Z",
        "summary": "Large Language Models (LLMs) exhibit impressive zero/few-shot inference and\ngeneration quality for high-resource languages(HRLs). A few of them have been\ntrained in low-resource languages (LRLs) and give decent performance. Owing to\nthe prohibitive costs of training LLMs, they are usually used as a network\nservice, with the client charged by the count of input and output tokens. The\nnumber of tokens strongly depends on the script and language, as well as the\nLLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage,\nbecause the well-known LLMs produce more tokens for LRLs than HRLs. This is\nbecause most currently popular LLMs are optimized for HRL vocabularies. Our\nobjective is to level the playing field: reduce the cost of processing LRLs in\ncontemporary LLMs while ensuring that predictive and generative qualities are\nnot compromised. As means to reduce the number of tokens processed by the LLM,\nwe consider code-mixing, translation, and transliteration of LRLs to HRLs. We\nperform an extensive study using the IndicXTREME dataset, covering 15 Indian\nlanguages, while using GPT-4 (one of the costliest LLM services released so\nfar) as a commercial LLM. We observe and analyze interesting patterns involving\ntoken count, cost,and quality across a multitude of languages and tasks. We\nshow that choosing the best policy to interact with the LLM can reduce cost by\n90% while giving better or comparable performance, compared to communicating\nwith the LLM in the original LRL.",
        "pdf_link": "https://arxiv.org/pdf/2403.05434v1.pdf"
    },
    {
        "title": "VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model",
        "authors": [
            "Junsu Kim",
            "Yunhoe Ku",
            "Jihyeon Kim",
            "Junuk Cha",
            "Seungryul Baek"
        ],
        "published": "2024-03-08T14:23:00Z",
        "summary": "In the field of Class Incremental Object Detection (CIOD), creating models\nthat can continuously learn like humans is a major challenge. Pseudo-labeling\nmethods, although initially powerful, struggle with multi-scenario incremental\nlearning due to their tendency to forget past knowledge. To overcome this, we\nintroduce a new approach called Vision-Language Model assisted Pseudo-Labeling\n(VLM-PL). This technique uses Vision-Language Model (VLM) to verify the\ncorrectness of pseudo ground-truths (GTs) without requiring additional model\ntraining. VLM-PL starts by deriving pseudo GTs from a pre-trained detector.\nThen, we generate custom queries for each pseudo GT using carefully designed\nprompt templates that combine image and text features. This allows the VLM to\nclassify the correctness through its responses. Furthermore, VLM-PL integrates\nrefined pseudo and real GTs from upcoming training, effectively combining new\nand old knowledge. Extensive experiments conducted on the Pascal VOC and MS\nCOCO datasets not only highlight VLM-PL's exceptional performance in\nmulti-scenario but also illuminate its effectiveness in dual-scenario by\nachieving state-of-the-art results in both.",
        "pdf_link": "https://arxiv.org/pdf/2403.05346v1.pdf"
    },
    {
        "title": "ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in Dialogues",
        "authors": [
            "Yiding Liu",
            "Jingjing Wang",
            "Jiamin Luo",
            "Tao Zeng",
            "Guodong Zhou"
        ],
        "published": "2024-03-08T14:05:36Z",
        "summary": "Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,\nQuestion-Answering and Dialogue) has attracted ever-more interest in recent\nyears and achieved important progresses. However, existing studies on\ninteractive ASU largely ignore the coreference issue for opinion targets (i.e.,\naspects), while this phenomenon is ubiquitous in interactive scenarios\nespecially dialogues, limiting the ASU performance. Recently, large language\nmodels (LLMs) shows the powerful ability to integrate various NLP tasks with\nthe chat paradigm. In this way, this paper proposes a new Chat-based Aspect\nSentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in\nunderstanding aspect sentiments in dialogue scenarios. Particularly, this\nChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to\naddress the aspect coreference issue. On this basis, we propose a Trusted\nSelf-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.\nSpecifically, this TSA treats the ACR task as an auxiliary task to boost the\nperformance of the primary ASU task, and further integrates trusted learning\ninto reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination\nproblem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to\nevaluate TSA, and extensive experiments show that our proposed TSA can\nsignificantly outperform several state-of-the-art baselines, justifying the\neffectiveness of TSA to ChatASU and the importance of considering the\ncoreference and hallucination issues in ChatASU.",
        "pdf_link": "https://arxiv.org/pdf/2403.05326v4.pdf"
    },
    {
        "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
        "authors": [
            "Jinyang Li",
            "Nan Huo",
            "Yan Gao",
            "Jiayi Shi",
            "Yingxiu Zhao",
            "Ge Qu",
            "Yurong Wu",
            "Chenhao Ma",
            "Jian-Guang Lou",
            "Reynold Cheng"
        ],
        "published": "2024-03-08T13:34:20Z",
        "summary": "Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
        "pdf_link": "https://arxiv.org/pdf/2403.05307v1.pdf"
    },
    {
        "title": "LLM4Decompile: Decompiling Binary Code with Large Language Models",
        "authors": [
            "Hanzhuo Tan",
            "Qi Luo",
            "Jing Li",
            "Yuqun Zhang"
        ],
        "published": "2024-03-08T13:10:59Z",
        "summary": "Decompilation aims to restore compiled code to human-readable source code,\nbut struggles with details like names and structure. Large language models\n(LLMs) show promise for programming tasks, motivating their application to\ndecompilation. However, there does not exist any open-source LLM for\ndecompilation. Moreover, existing decompilation evaluation systems mainly\nconsider token-level accuracy and largely ignore code executability, which is\nthe most important feature of any program. Therefore, we release the first\nopen-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion\ntokens of C source code and the corresponding assembly code. The open-source\nLLMs can serve as baselines for further development in the field. To ensure\npractical program evaluation, we introduce Decompile-Eval, the first dataset\nthat considers re-compilability and re-executability for decompilation. The\nbenchmark emphasizes the importance of evaluating the decompilation model from\nthe perspective of program semantics. Experiments indicate that our\nLLM4Decompile has demonstrated the capability to accurately decompile 21% of\nthe assembly code, which achieves a 50% improvement over GPT-4. Our code,\ndataset, and models are released at\nhttps://github.com/albertan017/LLM4Decompile",
        "pdf_link": "https://arxiv.org/pdf/2403.05286v1.pdf"
    },
    {
        "title": "ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models",
        "authors": [
            "Jio Oh",
            "Soyeon Kim",
            "Junseok Seo",
            "Jindong Wang",
            "Ruochen Xu",
            "Xing Xie",
            "Steven Euijong Whang"
        ],
        "published": "2024-03-08T12:42:36Z",
        "summary": "Large language models (LLMs) have achieved unprecedented performance in\nvarious applications, yet their evaluation remains a critical issue. Existing\nhallucination benchmarks are either static or lack adjustable complexity for\nthorough analysis. We contend that utilizing existing relational databases is a\npromising approach for constructing benchmarks due to their accurate knowledge\ndescription via functional dependencies. We propose ERBench to automatically\nconvert any relational database into a benchmark based on the\nentity-relationship (ER) model. Our key idea is to construct questions using\nthe database schema, records, and functional dependencies such that they can be\nautomatically verified. In addition, we use foreign key constraints to join\nrelations and construct multihop questions, which can be arbitrarily complex\nand used to debug the intermediate answers of LLMs. Finally, ERBench supports\ncontinuous evaluation, multimodal questions, and various prompt engineering\ntechniques. In our experiments, we construct an LLM benchmark using databases\nof multiple domains and make an extensive comparison of contemporary LLMs. We\nobserve that better LLMs like GPT-4 can handle a larger variety of question\ntypes, but are by no means perfect. Also, correct answers do not necessarily\nimply correct rationales, which is an important evaluation that ERBench does\nbetter than other benchmarks for various question types. Code is available at\nhttps: //github.com/DILAB-KAIST/ERBench.",
        "pdf_link": "https://arxiv.org/pdf/2403.05266v1.pdf"
    },
    {
        "title": "Debiasing Multimodal Large Language Models",
        "authors": [
            "Yi-Fan Zhang",
            "Weichen Yu",
            "Qingsong Wen",
            "Xue Wang",
            "Zhang Zhang",
            "Liang Wang",
            "Rong Jin",
            "Tieniu Tan"
        ],
        "published": "2024-03-08T12:35:07Z",
        "summary": "In the realms of computer vision and natural language processing, Large\nVision-Language Models (LVLMs) have become indispensable tools, proficient in\ngenerating textual descriptions based on visual inputs. Despite their\nadvancements, our investigation reveals a noteworthy bias in the generated\ncontent, where the output is primarily influenced by the underlying Large\nLanguage Models (LLMs) prior rather than the input image. Our empirical\nexperiments underscore the persistence of this bias, as LVLMs often provide\nconfident answers even in the absence of relevant images or given incongruent\nvisual input. To rectify these biases and redirect the model's focus toward\nvision information, we introduce two simple, training-free strategies. Firstly,\nfor tasks such as classification or multi-choice question-answering (QA), we\npropose a ``calibration'' step through affine transformation to adjust the\noutput distribution. This ``Post-Hoc debias'' approach ensures uniform scores\nfor each answer when the image is absent, serving as an effective\nregularization technique to alleviate the influence of LLM priors. For more\nintricate open-ended generation tasks, we extend this method to ``Debias\nsampling'', drawing inspirations from contrastive decoding methods.\nFurthermore, our investigation sheds light on the instability of LVLMs across\nvarious decoding configurations. Through systematic exploration of different\nsettings, we significantly enhance performance, surpassing reported results and\nraising concerns about the fairness of existing evaluations. Comprehensive\nexperiments substantiate the effectiveness of our proposed strategies in\nmitigating biases. These strategies not only prove beneficial in minimizing\nhallucinations but also contribute to the generation of more helpful and\nprecise illustrations.",
        "pdf_link": "https://arxiv.org/pdf/2403.05262v2.pdf"
    },
    {
        "title": "Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering",
        "authors": [
            "Hongda Sun",
            "Yuxuan Liu",
            "Chengwei Wu",
            "Haiyu Yan",
            "Cheng Tai",
            "Xin Gao",
            "Shuo Shang",
            "Rui Yan"
        ],
        "published": "2024-03-08T11:09:13Z",
        "summary": "Open-domain question answering (ODQA) has emerged as a pivotal research\nspotlight in information systems. Existing methods follow two main paradigms to\ncollect evidence: (1) The \\textit{retrieve-then-read} paradigm retrieves\npertinent documents from an external corpus; and (2) the\n\\textit{generate-then-read} paradigm employs large language models (LLMs) to\ngenerate relevant documents. However, neither can fully address multifaceted\nrequirements for evidence. To this end, we propose LLMQA, a generalized\nframework that formulates the ODQA process into three basic steps: query\nexpansion, document selection, and answer generation, combining the superiority\nof both retrieval-based and generation-based evidence. Since LLMs exhibit their\nexcellent capabilities to accomplish various tasks, we instruct LLMs to play\nmultiple roles as generators, rerankers, and evaluators within our framework,\nintegrating them to collaborate in the ODQA process. Furthermore, we introduce\na novel prompt optimization algorithm to refine role-playing prompts and steer\nLLMs to produce higher-quality evidence and answers. Extensive experimental\nresults on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that\nLLMQA achieves the best performance in terms of both answer accuracy and\nevidence quality, showcasing its potential for advancing ODQA research and\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2403.05217v1.pdf"
    },
    {
        "title": "Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge",
        "authors": [
            "Xin Zhao",
            "Naoki Yoshinaga",
            "Daisuke Oba"
        ],
        "published": "2024-03-08T10:09:57Z",
        "summary": "Acquiring factual knowledge for language models (LMs) in low-resource\nlanguages poses a serious challenge, thus resorting to cross-lingual transfer\nin multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and\nrepresent factual knowledge. Using the multilingual factual knowledge probing\ndataset, mLAMA, we first conducted a neuron investigation of ML-LMs\n(specifically, multilingual BERT). We then traced the roots of facts back to\nthe knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire\nspecific facts. We finally identified three patterns of acquiring and\nrepresenting facts in ML-LMs: language-independent, cross-lingual shared and\ntransferred, and devised methods for differentiating them. Our findings\nhighlight the challenge of maintaining consistent factual knowledge across\nlanguages, underscoring the need for better fact representation learning in\nML-LMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.05189v1.pdf"
    },
    {
        "title": "Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation",
        "authors": [
            "Xiaoying Zhang",
            "Jean-Francois Ton",
            "Wei Shen",
            "Hongning Wang",
            "Yang Liu"
        ],
        "published": "2024-03-08T09:20:12Z",
        "summary": "We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the\npervasive issue of reward over-optimization in Reinforcement Learning from\nHuman Feedback (RLHF) for Large Language Models (LLMs). Over-optimization\noccurs when a reward model serves as an imperfect proxy for human preference,\nand RL-driven policy optimization erroneously exploits reward inaccuracies. In\nthis paper, we begin by introducing a lightweight way to quantify uncertainties\nin rewards, relying solely on the last layer embeddings of the reward model,\nwithout the need for computationally expensive reward ensembles. AdvPO then\naddresses a distributionally robust optimization problem centred around the\nconfidence interval of the reward model's predictions for policy improvement.\nThrough comprehensive experiments on the Anthropic HH and TL;DR summarization\ndatasets, we illustrate the efficacy of AdvPO in mitigating the\noveroptimization issue, consequently resulting in enhanced performance as\nevaluated through human-assisted evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2403.05171v1.pdf"
    },
    {
        "title": "Towards a Psychology of Machines: Large Language Models Predict Human Memory",
        "authors": [
            "Markus Huff",
            "Elanur Ulak\u00e7\u0131"
        ],
        "published": "2024-03-08T08:41:14Z",
        "summary": "Large language models (LLMs) are demonstrating remarkable capabilities across\nvarious tasks despite lacking a foundation in human cognition. This raises the\nquestion: can these models, beyond simply mimicking human language patterns,\noffer insights into the mechanisms underlying human cognition? This study\nexplores the ability of ChatGPT to predict human performance in a\nlanguage-based memory task. Building upon theories of text comprehension, we\nhypothesize that recognizing ambiguous sentences (e.g., \"Because Bill drinks\nwine is never kept in the house\") is facilitated by preceding them with\ncontextually relevant information. Participants, both human and ChatGPT, were\npresented with pairs of sentences. The second sentence was always a garden-path\nsentence designed to be inherently ambiguous, while the first sentence either\nprovided a fitting (e.g., \"Bill has chronic alcoholism\") or an unfitting\ncontext (e.g., \"Bill likes to play golf\"). We measured both human's and\nChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for\nthe garden-path sentences, and humans' spontaneous memory for the garden-path\nsentences. The results revealed a striking alignment between ChatGPT's\nassessments and human performance. Sentences deemed more related and assessed\nas being more memorable by ChatGPT were indeed better remembered by humans,\neven though ChatGPT's internal mechanisms likely differ significantly from\nhuman cognition. This finding, which was confirmed with a robustness check\nemploying synonyms, underscores the potential of generative AI models to\npredict human performance accurately. We discuss the broader implications of\nthese findings for leveraging LLMs in the development of psychological theories\nand for gaining a deeper understanding of human cognition.",
        "pdf_link": "https://arxiv.org/pdf/2403.05152v1.pdf"
    },
    {
        "title": "Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem",
        "authors": [
            "Ceyao Zhang",
            "Renjie Li",
            "Cheng Zhang",
            "Zhaoyu Zhang",
            "Feng Yin"
        ],
        "published": "2024-03-08T08:38:50Z",
        "summary": "Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands\nexpert knowledge in physics, materials science, and quantum mechanics which is\nprohibitively labor-intensive. Advanced AI technologies, especially\nreinforcement learning (RL), have emerged as a powerful tool to augment and\naccelerate this inverse design process. By modeling the inverse design of PCSEL\nas a sequential decision-making problem, RL approaches can construct a\nsatisfactory PCSEL structure from scratch. However, the data inefficiency\nresulting from online interactions with precise and expensive simulation\nenvironments impedes the broader applicability of RL approaches. Recently,\nsequential models, especially the Transformer architecture, have exhibited\ncompelling performance in sequential decision-making problems due to their\nsimplicity and scalability to large language models. In this paper, we\nintroduce a novel framework named PCSEL Inverse Design Transformer (PiT) that\nabstracts the inverse design of PCSEL as a sequence modeling problem. The\ncentral part of our PiT is a Transformer-based structure that leverages the\npast trajectories and current states to predict the current actions. Compared\nwith the traditional RL approaches, PiT can output the optimal actions and\nachieve target PCSEL designs by leveraging offline data and conditioning on the\ndesired return. Results demonstrate that PiT achieves superior performance and\ndata efficiency compared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.05149v1.pdf"
    },
    {
        "title": "Med3DInsight: Enhancing 3D Medical Image Understanding with 2D Multi-Modal Large Language Models",
        "authors": [
            "Qiuhui Chen",
            "Huping Ye",
            "Yi Hong"
        ],
        "published": "2024-03-08T08:15:53Z",
        "summary": "Understanding 3D medical image volumes is a critical task in the medical\ndomain. However, existing 3D convolution and transformer-based methods have\nlimited semantic understanding of an image volume and also need a large set of\nvolumes for training. Recent advances in multi-modal large language models\n(MLLMs) provide a new and promising way to understand images with the help of\ntext descriptions. However, most current MLLMs are designed for 2D natural\nimages. To enhance the 3D medical image understanding with 2D MLLMs, we propose\na novel pre-training framework called Med3DInsight, which marries existing 3D\nimage encoders with 2D MLLMs and bridges them via a designed Plane-Slice-Aware\nTransformer (PSAT) module. Extensive experiments demonstrate our SOTA\nperformance on two downstream segmentation and classification tasks, including\nthree public datasets with CT and MRI modalities and comparison to more than\nten baselines. Med3DInsight can be easily integrated into any current 3D\nmedical image understanding network and improves its performance by a good\nmargin.",
        "pdf_link": "https://arxiv.org/pdf/2403.05141v1.pdf"
    },
    {
        "title": "ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment",
        "authors": [
            "Xiwei Hu",
            "Rui Wang",
            "Yixiao Fang",
            "Bin Fu",
            "Pei Cheng",
            "Gang Yu"
        ],
        "published": "2024-03-08T08:08:10Z",
        "summary": "Diffusion models have demonstrated remarkable performance in the domain of\ntext-to-image generation. However, most widely used models still employ CLIP as\ntheir text encoder, which constrains their ability to comprehend dense prompts,\nencompassing multiple objects, detailed attributes, complex relationships,\nlong-text alignment, etc. In this paper, we introduce an Efficient Large\nLanguage Model Adapter, termed ELLA, which equips text-to-image diffusion\nmodels with powerful Large Language Models (LLM) to enhance text alignment\nwithout training of either U-Net or LLM. To seamlessly bridge two pre-trained\nmodels, we investigate a range of semantic alignment connector designs and\npropose a novel module, the Timestep-Aware Semantic Connector (TSC), which\ndynamically extracts timestep-dependent conditions from LLM. Our approach\nadapts semantic features at different stages of the denoising process,\nassisting diffusion models in interpreting lengthy and intricate prompts over\nsampling timesteps. Additionally, ELLA can be readily incorporated with\ncommunity models and tools to improve their prompt-following capabilities. To\nassess text-to-image models in dense prompt following, we introduce Dense\nPrompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K\ndense prompts. Extensive experiments demonstrate the superiority of ELLA in\ndense prompt following compared to state-of-the-art methods, particularly in\nmultiple object compositions involving diverse attributes and relationships.",
        "pdf_link": "https://arxiv.org/pdf/2403.05135v1.pdf"
    },
    {
        "title": "ChatUIE: Exploring Chat-based Unified Information Extraction using Large Language Models",
        "authors": [
            "Jun Xu",
            "Mengshu Sun",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "published": "2024-03-08T07:59:19Z",
        "summary": "Recent advancements in large language models have shown impressive\nperformance in general chat. However, their domain-specific capabilities,\nparticularly in information extraction, have certain limitations. Extracting\nstructured information from natural language that deviates from known schemas\nor instructions has proven challenging for previous prompt-based methods. This\nmotivated us to explore domain-specific modeling in chat-based language models\nas a solution for extracting structured information from natural language. In\nthis paper, we present ChatUIE, an innovative unified information extraction\nframework built upon ChatGLM. Simultaneously, reinforcement learning is\nemployed to improve and align various tasks that involve confusing and limited\nsamples. Furthermore, we integrate generation constraints to address the issue\nof generating elements that are not present in the input. Our experimental\nresults demonstrate that ChatUIE can significantly improve the performance of\ninformation extraction with a slight decrease in chatting ability.",
        "pdf_link": "https://arxiv.org/pdf/2403.05132v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models for Molecule Prediction Tasks",
        "authors": [
            "Zhiqiang Zhong",
            "Kuangyu Zhou",
            "Davide Mottin"
        ],
        "published": "2024-03-08T05:59:56Z",
        "summary": "Large Language Models (LLMs) stand at the forefront of a number of Natural\nLanguage Processing (NLP) tasks. Despite the widespread adoption of LLMs in\nNLP, much of their potential in broader fields remains largely unexplored, and\nsignificant limitations persist in their design and implementation. Notably,\nLLMs struggle with structured data, such as graphs, and often falter when\ntasked with answering domain-specific questions requiring deep expertise, such\nas those in biology and chemistry. In this paper, we explore a fundamental\nquestion: Can LLMs effectively handle molecule prediction tasks? Rather than\npursuing top-tier performance, our goal is to assess how LLMs can contribute to\ndiverse molecule tasks. We identify several classification and regression\nprediction tasks across six standard molecule datasets. Subsequently, we\ncarefully design a set of prompts to query LLMs on these tasks and compare\ntheir performance with existing Machine Learning (ML) models, which include\ntext-based models and those specifically designed for analysing the geometric\nstructure of molecules. Our investigation reveals several key insights:\nFirstly, LLMs generally lag behind ML models in achieving competitive\nperformance on molecule tasks, particularly when compared to models adept at\ncapturing the geometric structure of molecules, highlighting the constrained\nability of LLMs to comprehend graph data. Secondly, LLMs show promise in\nenhancing the performance of ML models when used collaboratively. Lastly, we\nengage in a discourse regarding the challenges and promising avenues to harness\nLLMs for molecule prediction tasks. The code and models are available at\nhttps://github.com/zhiqiangzhongddu/LLMaMol.",
        "pdf_link": "https://arxiv.org/pdf/2403.05075v1.pdf"
    },
    {
        "title": "Can we obtain significant success in RST discourse parsing by using Large Language Models?",
        "authors": [
            "Aru Maekawa",
            "Tsutomu Hirao",
            "Hidetaka Kamigaito",
            "Manabu Okumura"
        ],
        "published": "2024-03-08T05:34:29Z",
        "summary": "Recently, decoder-only pre-trained large language models (LLMs), with several\ntens of billion parameters, have significantly impacted a wide range of natural\nlanguage processing (NLP) tasks. While encoder-only or encoder-decoder\npre-trained language models have already proved to be effective in discourse\nparsing, the extent to which LLMs can perform this task remains an open\nresearch question. Therefore, this paper explores how beneficial such LLMs are\nfor Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing\nprocess for both fundamental top-down and bottom-up strategies is converted\ninto prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with\nQLoRA, which has fewer parameters that can be tuned. Experimental results on\nthree benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate\nthat Llama 2 with 70 billion parameters in the bottom-up strategy obtained\nstate-of-the-art (SOTA) results with significant differences. Furthermore, our\nparsers demonstrated generalizability when evaluated on RST-DT, showing that,\nin spite of being trained with the GUM corpus, it obtained similar performances\nto those of existing parsers trained with RST-DT.",
        "pdf_link": "https://arxiv.org/pdf/2403.05065v1.pdf"
    },
    {
        "title": "Are Human Conversations Special? A Large Language Model Perspective",
        "authors": [
            "Toshish Jawale",
            "Chaitanya Animesh",
            "Sekhar Vallath",
            "Kartik Talamadupula",
            "Larry Heck"
        ],
        "published": "2024-03-08T04:44:25Z",
        "summary": "This study analyzes changes in the attention mechanisms of large language\nmodels (LLMs) when used to understand natural conversations between humans\n(human-human). We analyze three use cases of LLMs: interactions over web\ncontent, code, and mathematical texts. By analyzing attention distance,\ndispersion, and interdependency across these domains, we highlight the unique\nchallenges posed by conversational data. Notably, conversations require nuanced\nhandling of long-term contextual relationships and exhibit higher complexity\nthrough their attention patterns. Our findings reveal that while language\nmodels exhibit domain-specific attention behaviors, there is a significant gap\nin their ability to specialize in human conversations. Through detailed\nattention entropy analysis and t-SNE visualizations, we demonstrate the need\nfor models trained with a diverse array of high-quality conversational data to\nenhance understanding and generation of human-like dialogue. This research\nhighlights the importance of domain specialization in language models and\nsuggests pathways for future advancement in modeling human conversational\nnuances.",
        "pdf_link": "https://arxiv.org/pdf/2403.05045v1.pdf"
    },
    {
        "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
        "authors": [
            "Xuhui Zhou",
            "Zhe Su",
            "Tiwalayo Eisape",
            "Hyunwoo Kim",
            "Maarten Sap"
        ],
        "published": "2024-03-08T03:49:17Z",
        "summary": "Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena with LLM-based\nagents. However, most work has used an omniscient perspective on these\nsimulations (e.g., single LLM to generate all interlocutors), which is\nfundamentally at odds with the non-omniscient, information asymmetric\ninteractions that humans have. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that interlocutors\nsimulated omnisciently are much more successful at accomplishing social goals\ncompared to non-omniscient agents, despite the latter being the more realistic\nsetting. Furthermore, we demonstrate that learning from omniscient simulations\nimproves the apparent naturalness of interactions but scarcely enhances goal\nachievement in cooperative scenarios. Our findings indicate that addressing\ninformation asymmetry remains a fundamental challenge for LLM-based agents.",
        "pdf_link": "https://arxiv.org/pdf/2403.05020v2.pdf"
    },
    {
        "title": "Can't Remember Details in Long Documents? You Need Some R&R",
        "authors": [
            "Devanshu Agrawal",
            "Shang Gao",
            "Martin Gajek"
        ],
        "published": "2024-03-08T03:03:20Z",
        "summary": "Long-context large language models (LLMs) hold promise for tasks such as\nquestion-answering (QA) over long documents, but they tend to miss important\ninformation in the middle of context documents (arXiv:2307.03172v3). Here, we\nintroduce $\\textit{R&R}$ -- a combination of two novel prompt-based methods\ncalled $\\textit{reprompting}$ and $\\textit{in-context retrieval}$ (ICR) -- to\nalleviate this effect in document-based QA. In reprompting, we repeat the\nprompt instructions periodically throughout the context document to remind the\nLLM of its original task. In ICR, rather than instructing the LLM to answer the\nquestion directly, we instruct it to retrieve the top $k$ passage numbers most\nrelevant to the given question, which are then used as an abbreviated context\nin a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents\nup to 80k tokens in length and observe a 16-point boost in QA accuracy on\naverage. Our further analysis suggests that R&R improves performance on long\ndocument-based QA because it reduces the distance between relevant context and\nthe instructions. Finally, we show that compared to short-context chunkwise\nmethods, R&R enables the use of larger chunks that cost fewer LLM calls and\noutput tokens, while minimizing the drop in accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2403.05004v1.pdf"
    },
    {
        "title": "DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation",
        "authors": [
            "Jiapeng Wang",
            "Chengyu Wang",
            "Tingfeng Cao",
            "Jun Huang",
            "Lianwen Jin"
        ],
        "published": "2024-03-08T02:24:27Z",
        "summary": "We present DiffChat, a novel method to align Large Language Models (LLMs) to\n\"chat\" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable\nDiffusion) for interactive image creation. Given a raw prompt/image and a\nuser-specified instruction, DiffChat can effectively make appropriate\nmodifications and generate the target prompt, which can be leveraged to create\nthe target image of high quality. To achieve this, we first collect an\ninstruction-following prompt engineering dataset named InstructPE for the\nsupervised training of DiffChat. Next, we propose a reinforcement learning\nframework with the feedback of three core criteria for image creation, i.e.,\naesthetics, user preference, and content integrity. It involves an action-space\ndynamic modification technique to obtain more relevant positive samples and\nharder negative samples during the off-policy sampling. Content integrity is\nalso introduced into the value estimation function for further improvement of\nproduced images. Our method can exhibit superior performance than baseline\nmodels and strong competitors based on both automatic and human evaluations,\nwhich fully demonstrates its effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.04997v1.pdf"
    },
    {
        "title": "Tell me the truth: A system to measure the trustworthiness of Large Language Models",
        "authors": [
            "Carlo Lipizzi"
        ],
        "published": "2024-03-08T00:27:57Z",
        "summary": "Large Language Models (LLM) have taken the front seat in most of the news\nsince November 2022, when ChatGPT was introduced. After more than one year, one\nof the major reasons companies are resistant to adopting them is the limited\nconfidence they have in the trustworthiness of those systems. In a study by\n(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in\nidentifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics\nfound that ChatGPT has an accuracy rate of 17% percent when diagnosing\npediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust\nis a relative, subject condition that can change based on culture, domain,\nindividuals. And then, given a domain, how can the trustworthiness of a system\nbe measured? In this paper, I present a systematic approach to measure\ntrustworthiness based on a predefined ground truth, represented as a knowledge\ngraph of the domain. The approach is a process with humans in the loop to\nvalidate the representation of the domain and to fine-tune the system.\n  Measuring the trustworthiness would be essential for all the entities\noperating in critical environments, such as healthcare, defense, finance, but\nit would be very relevant for all the users of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04964v2.pdf"
    },
    {
        "title": "An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment",
        "authors": [
            "Xuanxin Wu",
            "Yuki Arase"
        ],
        "published": "2024-03-08T00:19:24Z",
        "summary": "Sentence simplification, which rewrites a sentence to be easier to read and\nunderstand, is a promising technique to help people with various reading\ndifficulties. With the rise of advanced large language models (LLMs),\nevaluating their performance in sentence simplification has become imperative.\nRecent studies have used both automatic metrics and human evaluations to assess\nthe simplification abilities of LLMs. However, the suitability of existing\nevaluation methodologies for LLMs remains in question. First, the suitability\nof current automatic metrics on LLMs' simplification evaluation is still\nuncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the GPT-4's simplification capabilities. Results show that\nGPT-4 generally generates fewer erroneous simplification outputs compared to\nthe current state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that while these metrics are effective for significant quality\ndifferences, they lack sufficient sensitivity to assess the overall\nhigh-quality simplification by GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.04963v1.pdf"
    },
    {
        "title": "SecGPT: An Execution Isolation Architecture for LLM-Based Systems",
        "authors": [
            "Yuhao Wu",
            "Franziska Roesner",
            "Tadayoshi Kohno",
            "Ning Zhang",
            "Umar Iqbal"
        ],
        "published": "2024-03-08T00:02:30Z",
        "summary": "Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nthe natural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we propose SecGPT, an architecture for\nLLM-based systems that aims to mitigate the security and privacy issues that\narise with the execution of third-party apps. SecGPT's key idea is to isolate\nthe execution of apps and more precisely mediate their interactions outside of\ntheir isolated environments. We evaluate SecGPT against a number of case study\nattacks and demonstrate that it protects against many security, privacy, and\nsafety issues that exist in non-isolated LLM-based systems. The performance\noverhead incurred by SecGPT to improve security is under 0.3x for\nthree-quarters of the tested queries. To foster follow-up research, we release\nSecGPT's source code at https://github.com/llm-platform-security/SecGPT.",
        "pdf_link": "https://arxiv.org/pdf/2403.04960v1.pdf"
    },
    {
        "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models",
        "authors": [
            "Xiaogeng Liu",
            "Zhiyuan Yu",
            "Yizhe Zhang",
            "Ning Zhang",
            "Chaowei Xiao"
        ],
        "published": "2024-03-07T23:46:20Z",
        "summary": "Large Language Models (LLMs) excel in processing and generating human\nlanguage, powered by their ability to interpret and follow instructions.\nHowever, their capabilities can be exploited through prompt injection attacks.\nThese attacks manipulate LLM-integrated applications into producing responses\naligned with the attacker's injected content, deviating from the user's actual\nrequests. The substantial risks posed by these attacks underscore the need for\na thorough understanding of the threats. Yet, research in this area faces\nchallenges due to the lack of a unified goal for such attacks and their\nreliance on manually crafted prompts, complicating comprehensive assessments of\nprompt injection robustness. We introduce a unified framework for understanding\nthe objectives of prompt injection attacks and present an automated\ngradient-based method for generating highly effective and universal prompt\ninjection data, even in the face of defensive measures. With only five training\nsamples (0.3% relative to the test data), our attack can achieve superior\nperformance compared with baselines. Our findings emphasize the importance of\ngradient-based testing, which can avoid overestimation of robustness,\nespecially for defense mechanisms.",
        "pdf_link": "https://arxiv.org/pdf/2403.04957v1.pdf"
    },
    {
        "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
        "authors": [
            "Ojas Gramopadhye",
            "Saeel Sandeep Nachane",
            "Prateek Chanda",
            "Ganesh Ramakrishnan",
            "Kshitij Sharad Jadhav",
            "Yatin Nandwani",
            "Dinesh Raghu",
            "Sachindra Joshi"
        ],
        "published": "2024-03-07T20:48:40Z",
        "summary": "Large Language models (LLMs) have demonstrated significant potential in\ntransforming healthcare by automating tasks such as clinical documentation,\ninformation retrieval, and decision support. In this aspect, carefully\nengineered prompts have emerged as a powerful tool for using LLMs for medical\nscenarios, e.g., patient clinical scenarios. In this paper, we propose a\nmodified version of the MedQA-USMLE dataset, which is subjective, to mimic\nreal-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning\nbased on subjective response generation for the modified MedQA-USMLE dataset\nwith appropriate LM-driven forward reasoning for correct responses to the\nmedical questions. Keeping in mind the importance of response verification in\nthe medical setting, we utilize a reward training mechanism whereby the\nlanguage model also provides an appropriate verified response for a particular\nresponse to a clinical question. In this regard, we also include\nhuman-in-the-loop for different evaluation aspects. We develop better\nin-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt from\narXiv:2207.08143 for the subjective MedQA dataset and developing our\nincremental-reasoning prompt. Our evaluations show that the incremental\nreasoning prompt performs better than the modified codex prompt in certain\nscenarios. We also show that greedy decoding with the incremental reasoning\nmethod performs better than other strategies, such as prompt chaining and\neliminative reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2403.04890v1.pdf"
    },
    {
        "title": "Evaluating Biases in Context-Dependent Health Questions",
        "authors": [
            "Sharon Levy",
            "Tahilin Sanchez Karver",
            "William D. Adler",
            "Michelle R. Kaufman",
            "Mark Dredze"
        ],
        "published": "2024-03-07T19:15:40Z",
        "summary": "Chat-based large language models have the opportunity to empower individuals\nlacking high-quality healthcare access to receive personalized information\nacross a variety of topics. However, users may ask underspecified questions\nthat require additional context for a model to correctly answer. We study how\nlarge language model biases are exhibited through these contextual questions in\nthe healthcare domain. To accomplish this, we curate a dataset of sexual and\nreproductive healthcare questions that are dependent on age, sex, and location\nattributes. We compare models' outputs with and without demographic context to\ndetermine group alignment among our contextual questions. Our experiments\nreveal biases in each of these attributes, where young adult female users are\nfavored.",
        "pdf_link": "https://arxiv.org/pdf/2403.04858v1.pdf"
    },
    {
        "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
        "authors": [
            "Adam Coscia",
            "Langdon Holmes",
            "Wesley Morris",
            "Joon Suh Choi",
            "Scott Crossley",
            "Alex Endert"
        ],
        "published": "2024-03-07T18:56:39Z",
        "summary": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
        "pdf_link": "https://arxiv.org/pdf/2403.04760v1.pdf"
    },
    {
        "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
        "authors": [
            "Boshi Wang",
            "Hao Fang",
            "Jason Eisner",
            "Benjamin Van Durme",
            "Yu Su"
        ],
        "published": "2024-03-07T18:50:51Z",
        "summary": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.",
        "pdf_link": "https://arxiv.org/pdf/2403.04746v1.pdf"
    },
    {
        "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
        "authors": [
            "Jielin Qiu",
            "Andrea Madotto",
            "Zhaojiang Lin",
            "Paul A. Crook",
            "Yifan Ethan Xu",
            "Xin Luna Dong",
            "Christos Faloutsos",
            "Lei Li",
            "Babak Damavandi",
            "Seungwhan Moon"
        ],
        "published": "2024-03-07T18:38:17Z",
        "summary": "Vision-extended LLMs have made significant strides in Visual Question\nAnswering (VQA). Despite these advancements, VLLMs still encounter substantial\ndifficulties in handling queries involving long-tail entities, with a tendency\nto produce erroneous or hallucinated responses. In this work, we introduce a\nnovel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for\nentity-centric VQA. This task aims to test the models' capabilities in\nidentifying entities and providing detailed, entity-specific knowledge. We have\ndeveloped the \\textbf{SnapNTell Dataset}, distinct from traditional VQA\ndatasets: (1) It encompasses a wide range of categorized entities, each\nrepresented by images and explicitly named in the answers; (2) It features QA\npairs that require extensive knowledge for accurate responses. The dataset is\norganized into 22 major categories, containing 7,568 unique entities in total.\nFor each entity, we curated 10 illustrative images and crafted 10\nknowledge-intensive QA pairs. To address this novel task, we devised a\nscalable, efficient, and transparent retrieval-augmented multimodal LLM. Our\napproach markedly outperforms existing methods on the SnapNTell dataset,\nachieving a 66.5\\% improvement in the BELURT score. We will soon make the\ndataset and the source code publicly accessible.",
        "pdf_link": "https://arxiv.org/pdf/2403.04735v1.pdf"
    },
    {
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "authors": [
            "Yizhe Zhang",
            "He Bai",
            "Ruixiang Zhang",
            "Jiatao Gu",
            "Shuangfei Zhai",
            "Josh Susskind",
            "Navdeep Jaitly"
        ],
        "published": "2024-03-07T18:35:54Z",
        "summary": "Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.",
        "pdf_link": "https://arxiv.org/pdf/2403.04732v2.pdf"
    },
    {
        "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
        "authors": [
            "Chen Li",
            "Weiqi Wang",
            "Jingcheng Hu",
            "Yixuan Wei",
            "Nanning Zheng",
            "Han Hu",
            "Zheng Zhang",
            "Houwen Peng"
        ],
        "published": "2024-03-07T18:00:40Z",
        "summary": "Mathematical capabilities were previously believed to emerge in common\nlanguage models only at a very large scale or require extensive math-related\npre-training. This paper shows that the LLaMA-2 7B model with common\npre-training already exhibits strong mathematical abilities, as evidenced by\nits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,\nrespectively, when selecting the best response from 256 random generations. The\nprimary issue with the current base model is the difficulty in consistently\neliciting its inherent mathematical capabilities. Notably, the accuracy for the\nfirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,\nrespectively. We find that simply scaling up the SFT data can significantly\nenhance the reliability of generating correct answers. However, the potential\nfor extensive scaling is constrained by the scarcity of publicly available math\nquestions. To overcome this limitation, we employ synthetic data, which proves\nto be nearly as effective as real data and shows no clear saturation when\nscaled up to approximately one million samples. This straightforward approach\nachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B\nmodels, surpassing previous models by 14.2% and 20.8%, respectively. We also\nprovide insights into scaling behaviors across different reasoning complexities\nand error types.",
        "pdf_link": "https://arxiv.org/pdf/2403.04706v1.pdf"
    },
    {
        "title": "ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes",
        "authors": [
            "Hashmat Shadab Malik",
            "Muhammad Huzaifa",
            "Muzammal Naseer",
            "Salman Khan",
            "Fahad Shahbaz Khan"
        ],
        "published": "2024-03-07T17:48:48Z",
        "summary": "Given the large-scale multi-modal training of recent vision-based models and\ntheir generalization capabilities, understanding the extent of their robustness\nis critical for their real-world deployment. In this work, we evaluate the\nresilience of current vision-based models against diverse object-to-background\ncontext variations. The majority of robustness evaluation methods have\nintroduced synthetic datasets to induce changes to object characteristics\n(viewpoints, scale, color) or utilized image transformation techniques\n(adversarial changes, common corruptions) on real images to simulate shifts in\ndistributions. Recent works have explored leveraging large language models and\ndiffusion models to generate changes in the background. However, these methods\neither lack in offering control over the changes to be made or distort the\nobject semantics, making them unsuitable for the task. Our method, on the other\nhand, can induce diverse object-to-background changes while preserving the\noriginal semantics and appearance of the object. To achieve this goal, we\nharness the generative capabilities of text-to-image, image-to-text, and\nimage-to-segment models to automatically generate a broad spectrum of\nobject-to-background changes. We induce both natural and adversarial background\nchanges by either modifying the textual prompts or optimizing the latents and\ntextual embedding of text-to-image models. We produce various versions of\nstandard vision datasets (ImageNet, COCO), incorporating either diverse and\nrealistic backgrounds into the images or introducing color, texture, and\nadversarial changes in the background. We conduct extensive experiment to\nanalyze the robustness of vision-based models against object-to-background\ncontext variations across diverse tasks. Code\nhttps://github.com/Muhammad-Huzaifaa/ObjectCompose.git",
        "pdf_link": "https://arxiv.org/pdf/2403.04701v3.pdf"
    },
    {
        "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
        "authors": [
            "Ekaterina Fadeeva",
            "Aleksandr Rubashevskii",
            "Artem Shelmanov",
            "Sergey Petrakov",
            "Haonan Li",
            "Hamdy Mubarak",
            "Evgenii Tsymbalov",
            "Gleb Kuzmin",
            "Alexander Panchenko",
            "Timothy Baldwin",
            "Preslav Nakov",
            "Maxim Panov"
        ],
        "published": "2024-03-07T17:44:17Z",
        "summary": "Large language models (LLMs) are notorious for hallucinating, i.e., producing\nerroneous claims in their output. Such hallucinations can be dangerous, as\noccasional factual inaccuracies in the generated text might be obscured by the\nrest of the output being generally factual, making it extremely hard for the\nusers to spot them. Current services that leverage LLMs usually do not provide\nany means for detecting unreliable generations. Here, we aim to bridge this\ngap. In particular, we propose a novel fact-checking and hallucination\ndetection pipeline based on token-level uncertainty quantification. Uncertainty\nscores leverage information encapsulated in the output of a neural network or\nits layers to detect unreliable predictions, and we show that they can be used\nto fact-check the atomic claims in the LLM output. Moreover, we present a novel\ntoken-level uncertainty quantification method that removes the impact of\nuncertainty about what claim to generate on the current step and what surface\nform to use. Our method Claim Conditioned Probability (CCP) measures only the\nuncertainty of particular claim value expressed by the model. Experiments on\nthe task of biography generation demonstrate strong improvements for CCP\ncompared to the baselines for six different LLMs and three languages. Human\nevaluation reveals that the fact-checking pipeline based on uncertainty\nquantification is competitive with a fact-checking tool that leverages external\nknowledge.",
        "pdf_link": "https://arxiv.org/pdf/2403.04696v1.pdf"
    },
    {
        "title": "Telecom Language Models: Must They Be Large?",
        "authors": [
            "Nicola Piovesan",
            "Antonio De Domenico",
            "Fadhel Ayed"
        ],
        "published": "2024-03-07T17:13:12Z",
        "summary": "The increasing interest in Large Language Models (LLMs) within the\ntelecommunications sector underscores their potential to revolutionize\noperational efficiency. However, the deployment of these sophisticated models\nis often hampered by their substantial size and computational demands, raising\nconcerns about their viability in resource-constrained environments. Addressing\nthis challenge, recent advancements have seen the emergence of small language\nmodels that surprisingly exhibit performance comparable to their larger\ncounterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a\ncompact yet powerful model, exemplifies this new wave of efficient small\nlanguage models. This paper conducts a comprehensive evaluation of Phi-2's\nintrinsic understanding of the telecommunications domain. Recognizing the\nscale-related limitations, we enhance Phi-2's capabilities through a\nRetrieval-Augmented Generation approach, meticulously integrating an extensive\nknowledge base specifically curated with telecom standard specifications. The\nenhanced Phi-2 model demonstrates a profound improvement in accuracy, answering\nquestions about telecom standards with a precision that closely rivals the more\nresource-intensive GPT-3.5. The paper further explores the refined capabilities\nof Phi-2 in addressing problem-solving scenarios within the telecom sector,\nhighlighting its potential and limitations.",
        "pdf_link": "https://arxiv.org/pdf/2403.04666v1.pdf"
    },
    {
        "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache",
        "authors": [
            "Shichen Dong",
            "Wen Cheng",
            "Jiayu Qin",
            "Wei Wang"
        ],
        "published": "2024-03-07T16:42:37Z",
        "summary": "The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP\napplications, particularly in domains such as question-answering systems and\ntext generation. As the need for longer context grows, a significant bottleneck\nin model deployment emerges due to the linear expansion of the Key-Value (KV)\ncache with the context length. Existing methods primarily rely on various\nhypotheses, such as sorting the KV cache based on attention scores for\nreplacement or eviction, to compress the KV cache and improve model throughput.\nHowever, heuristics used by these strategies may wrongly evict essential KV\ncache, which can significantly degrade model performance. In this paper, we\npropose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We\ntheoretically demonstrate that key cache and value cache exhibit distinct\nsensitivities to quantization, leading to the formulation of separate\nquantization strategies for their non-uniform quantization. Through the\nintegration of dedicated outlier handling, as well as an improved\nattention-aware approach, QAQ achieves up to 10x the compression ratio of the\nKV cache size with a neglectable impact on model performance. QAQ significantly\nreduces the practical hurdles of deploying LLMs, opening up new possibilities\nfor longer-context applications. The code is available at\ngithub.com/ClubieDong/KVCacheQuantization.",
        "pdf_link": "https://arxiv.org/pdf/2403.04643v1.pdf"
    },
    {
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning",
        "authors": [
            "Alex Havrilla",
            "Yuqing Du",
            "Sharath Chandra Raparthy",
            "Christoforos Nalmpantis",
            "Jane Dwivedi-Yu",
            "Maksym Zhuravinskyi",
            "Eric Hambro",
            "Sainbayar Sukhbaatar",
            "Roberta Raileanu"
        ],
        "published": "2024-03-07T16:36:29Z",
        "summary": "Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a\ndominant approach for aligning LLM outputs with human preferences. Inspired by\nthe success of RLHF, we study the performance of multiple algorithms that learn\nfrom feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}),\nReturn-Conditioned RL) on improving LLM reasoning capabilities. We investigate\nboth sparse and dense rewards provided to the LLM both heuristically and via a\nlearned reward model. We additionally start from multiple model sizes and\ninitializations both with and without supervised fine-tuning (\\textbf{SFT})\ndata. Overall, we find all algorithms perform comparably, with Expert Iteration\nperforming best in most cases. Surprisingly, we find the sample complexity of\nExpert Iteration is similar to that of PPO, requiring at most on the order of\n$10^6$ samples to converge from a pretrained checkpoint. We investigate why\nthis is the case, concluding that during RL training models fail to explore\nsignificantly beyond solutions already produced by SFT models. Additionally, we\ndiscuss a trade off between maj@1 and pass@96 metric performance during SFT\ntraining and how conversely RL training improves both simultaneously. We then\nconclude by discussing the implications of our findings for RLHF and the future\nrole of RL in LLM fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.04642v1.pdf"
    },
    {
        "title": "CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios",
        "authors": [
            "Qilang Ye",
            "Zitong Yu",
            "Rui Shao",
            "Xinyu Xie",
            "Philip Torr",
            "Xiaochun Cao"
        ],
        "published": "2024-03-07T16:31:02Z",
        "summary": "This paper focuses on the challenge of answering questions in scenarios that\nare composed of rich and complex dynamic audio-visual components. Although\nexisting Multimodal Large Language Models (MLLMs) can respond to audio-visual\ncontent, these responses are sometimes ambiguous and fail to describe specific\naudio-visual events. To overcome this limitation, we introduce the CAT, which\nenhances MLLM in three ways: 1) besides straightforwardly bridging audio and\nvideo, we design a clue aggregator that aggregates question-related clues in\ndynamic audio-visual scenarios to enrich the detailed knowledge required for\nlarge language models. 2) CAT is trained on a mixed multimodal dataset,\nallowing direct application in audio-visual scenarios. Notably, we collect an\naudio-visual joint instruction dataset named AVinstruct, to further enhance the\ncapacity of CAT to model cross-semantic correlations. 3) we propose AI-assisted\nambiguity-aware direct preference optimization, a strategy specialized in\nretraining the model to favor the non-ambiguity response and improve the\nability to localize specific audio-visual objects. Extensive experimental\nresults demonstrate that CAT outperforms existing methods on multimodal tasks,\nespecially in Audio-Visual Question Answering (AVQA) tasks. The codes and the\ncollected instructions are released at https://github.com/rikeilong/Bay-CAT.",
        "pdf_link": "https://arxiv.org/pdf/2403.04640v1.pdf"
    },
    {
        "title": "Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition",
        "authors": [
            "Aneta Koleva",
            "Martin Ringsquandl",
            "Ahmed Hatem",
            "Thomas Runkler",
            "Volker Tresp"
        ],
        "published": "2024-03-07T15:22:07Z",
        "summary": "Web tables contain a large amount of valuable knowledge and have inspired\ntabular language models aimed at tackling table interpretation (TI) tasks. In\nthis paper, we analyse a widely used benchmark dataset for evaluation of TI\ntasks, particularly focusing on the entity linking task. Our analysis reveals\nthat this dataset is overly simplified, potentially reducing its effectiveness\nfor thorough evaluation and failing to accurately represent tables as they\nappear in the real-world. To overcome this drawback, we construct and annotate\na new more challenging dataset. In addition to introducing the new dataset, we\nalso introduce a novel problem aimed at addressing the entity linking task:\nnamed entity recognition within cells. Finally, we propose a prompting\nframework for evaluating the newly developed large language models (LLMs) on\nthis novel TI task. We conduct experiments on prompting LLMs under various\nsettings, where we use both random and similarity-based selection to choose the\nexamples presented to the models. Our ablation study helps us gain insights\ninto the impact of the few-shot examples. Additionally, we perform qualitative\nanalysis to gain insights into the challenges encountered by the models and to\nunderstand the limitations of the proposed dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.04577v1.pdf"
    },
    {
        "title": "GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability",
        "authors": [
            "Zihan Luo",
            "Xiran Song",
            "Hong Huang",
            "Jianxun Lian",
            "Chenhao Zhang",
            "Jinqi Jiang",
            "Xing Xie"
        ],
        "published": "2024-03-07T13:36:08Z",
        "summary": "Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.",
        "pdf_link": "https://arxiv.org/pdf/2403.04483v2.pdf"
    },
    {
        "title": "Do Large Language Model Understand Multi-Intent Spoken Language ?",
        "authors": [
            "Shangjian Yin",
            "Peijie Huang",
            "Yuhong Xu",
            "Haojing Huang",
            "Jiatian Chen"
        ],
        "published": "2024-03-07T13:30:52Z",
        "summary": "This study marks a significant advancement by harnessing Large Language\nModels (LLMs) for multi-intent spoken language understanding (SLU), proposing a\nunique methodology that capitalizes on the generative power of LLMs within an\nSLU context. Our innovative technique reconfigures entity slots specifically\nfor LLM application in multi-intent SLU environments and introduces the concept\nof Sub-Intent Instruction (SII), enhancing the dissection and interpretation of\nintricate, multi-intent communication within varied domains. The resultant\ndatasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing\nbenchmarks. Our research illustrates that LLMs can match and potentially excel\nbeyond the capabilities of current state-of-the-art multi-intent SLU models. It\nfurther explores LLM efficacy across various intent configurations and dataset\nproportions. Moreover, we introduce two pioneering metrics, Entity Slot\nAccuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth\nanalysis of LLM proficiency in this complex field.",
        "pdf_link": "https://arxiv.org/pdf/2403.04481v2.pdf"
    },
    {
        "title": "Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset",
        "authors": [
            "Minjin Kim",
            "Minju Kim",
            "Hana Kim",
            "Beong-woo Kwak",
            "Soyeon Chun",
            "Hyunseo Kim",
            "SeongKu Kang",
            "Youngjae Yu",
            "Jinyoung Yeo",
            "Dongha Lee"
        ],
        "published": "2024-03-07T12:57:16Z",
        "summary": "Conversational recommender system is an emerging area that has garnered an\nincreasing interest in the community, especially with the advancements in large\nlanguage models (LLMs) that enable diverse reasoning over conversational input.\nDespite the progress, the field has many aspects left to explore. The currently\navailable public datasets for conversational recommendation lack specific user\npreferences and explanations for recommendations, hindering high-quality\nrecommendations. To address such challenges, we present a novel conversational\nrecommendation dataset named PEARL, synthesized with persona- and\nknowledge-augmented LLM simulators. We obtain detailed persona and knowledge\nfrom real-world reviews and construct a large-scale dataset with over 57k\ndialogues. Our experimental results demonstrate that utterances in PEARL\ninclude more specific user preferences, show expertise in the target domain,\nand provide recommendations more relevant to the dialogue context than those in\nprior datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.04460v3.pdf"
    },
    {
        "title": "Low-Resource Court Judgment Summarization for Common Law Systems",
        "authors": [
            "Shuaiqi Liu",
            "Jiannong Cao",
            "Yicong Li",
            "Ruosong Yang",
            "Zhiyuan Wen"
        ],
        "published": "2024-03-07T12:47:42Z",
        "summary": "Common law courts need to refer to similar precedents' judgments to inform\ntheir current decisions. Generating high-quality summaries of court judgment\ndocuments can facilitate legal practitioners to efficiently review previous\ncases and assist the general public in accessing how the courts operate and how\nthe law is applied. Previous court judgment summarization research focuses on\ncivil law or a particular jurisdiction's judgments. However, judges can refer\nto the judgments from all common law jurisdictions. Current summarization\ndatasets are insufficient to satisfy the demands of summarizing precedents\nacross multiple jurisdictions, especially when labeled data are scarce for many\njurisdictions. To address the lack of datasets, we present CLSum, the first\ndataset for summarizing multi-jurisdictional common law court judgment\ndocuments. Besides, this is the first court judgment summarization work\nadopting large language models (LLMs) in data augmentation, summary generation,\nand evaluation. Specifically, we design an LLM-based data augmentation method\nincorporating legal knowledge. We also propose a legal knowledge enhanced\nevaluation metric based on LLM to assess the quality of generated judgment\nsummaries. Our experimental results verify that the LLM-based summarization\nmethods can perform well in the few-shot and zero-shot settings. Our LLM-based\ndata augmentation method can mitigate the impact of low data resources.\nFurthermore, we carry out comprehensive comparative experiments to find\nessential model components and settings that are capable of enhancing\nsummarization performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.04454v1.pdf"
    },
    {
        "title": "Membership Inference Attacks and Privacy in Topic Modeling",
        "authors": [
            "Nico Manzonelli",
            "Wanrong Zhang",
            "Salil Vadhan"
        ],
        "published": "2024-03-07T12:43:42Z",
        "summary": "Recent research shows that large language models are susceptible to privacy\nattacks that infer aspects of the training data. However, it is unclear if\nsimpler generative models, like topic models, share similar vulnerabilities. In\nthis work, we propose an attack against topic models that can confidently\nidentify members of the training data in Latent Dirichlet Allocation. Our\nresults suggest that the privacy risks associated with generative modeling are\nnot restricted to large neural models. Additionally, to mitigate these\nvulnerabilities, we explore differentially private (DP) topic modeling. We\npropose a framework for private topic modeling that incorporates DP vocabulary\nselection as a pre-processing step, and show that it improves privacy while\nhaving limited effects on practical utility.",
        "pdf_link": "https://arxiv.org/pdf/2403.04451v1.pdf"
    },
    {
        "title": "Feedback-Generation for Programming Exercises With GPT-4",
        "authors": [
            "Imen Azaiz",
            "Natalie Kiesler",
            "Sven Strickroth"
        ],
        "published": "2024-03-07T12:37:52Z",
        "summary": "Ever since Large Language Models (LLMs) and related applications have become\nbroadly available, several studies investigated their potential for assisting\neducators and supporting students in higher education. LLMs such as Codex,\nGPT-3.5, and GPT 4 have shown promising results in the context of large\nprogramming courses, where students can benefit from feedback and hints if\nprovided timely and at scale. This paper explores the quality of GPT-4 Turbo's\ngenerated output for prompts containing both the programming task specification\nand a student's submission as input. Two assignments from an introductory\nprogramming course were selected, and GPT-4 was asked to generate feedback for\n55 randomly chosen, authentic student programming submissions. The output was\nqualitatively analyzed regarding correctness, personalization, fault\nlocalization, and other features identified in the material. Compared to prior\nwork and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For\nexample, the output is more structured and consistent. GPT-4 Turbo can also\naccurately identify invalid casing in student programs' output. In some cases,\nthe feedback also includes the output of the student program. At the same time,\ninconsistent feedback was noted such as stating that the submission is correct\nbut an error needs to be fixed. The present work increases our understanding of\nLLMs' potential, limitations, and how to integrate them into e-assessment\nsystems, pedagogical scenarios, and instructing students who are using\napplications based on GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.04449v1.pdf"
    },
    {
        "title": "Acceleron: A Tool to Accelerate Research Ideation",
        "authors": [
            "Harshit Nigam",
            "Manasi Patwardhan",
            "Lovekesh Vig",
            "Gautam Shroff"
        ],
        "published": "2024-03-07T10:20:06Z",
        "summary": "Several tools have recently been proposed for assisting researchers during\nvarious stages of the research life-cycle. However, these primarily concentrate\non tasks such as retrieving and recommending relevant literature, reviewing and\ncritiquing the draft, and writing of research manuscripts. Our investigation\nreveals a significant gap in availability of tools specifically designed to\nassist researchers during the challenging ideation phase of the research\nlife-cycle. To aid with research ideation, we propose `Acceleron', a research\naccelerator for different phases of the research life cycle, and which is\nspecially designed to aid the ideation process. Acceleron guides researchers\nthrough the formulation of a comprehensive research proposal, encompassing a\nnovel research problem. The proposals motivation is validated for novelty by\nidentifying gaps in the existing literature and suggesting a plausible list of\ntechniques to solve the proposed problem. We leverage the reasoning and\ndomain-specific skills of Large Language Models (LLMs) to create an agent-based\narchitecture incorporating colleague and mentor personas for LLMs. The LLM\nagents emulate the ideation process undertaken by researchers, engaging\nresearchers in an interactive fashion to aid in the development of the research\nproposal. Notably, our tool addresses challenges inherent in LLMs, such as\nhallucinations, implements a two-stage aspect-based retrieval to manage\nprecision-recall trade-offs, and tackles issues of unanswerability. As\nevaluation, we illustrate the execution of our motivation validation and method\nsynthesis workflows on proposals from the ML and NLP domain, given by 3\ndistinct researchers. Our observations and evaluations provided by the\nresearchers illustrate the efficacy of the tool in terms of assisting\nresearchers with appropriate inputs at distinct stages and thus leading to\nimproved time efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2403.04382v1.pdf"
    },
    {
        "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
        "authors": [
            "Leigang Qu",
            "Wenjie Wang",
            "Yongqi Li",
            "Hanwang Zhang",
            "Liqiang Nie",
            "Tat-Seng Chua"
        ],
        "published": "2024-03-07T08:37:33Z",
        "summary": "Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.",
        "pdf_link": "https://arxiv.org/pdf/2403.04321v2.pdf"
    },
    {
        "title": "Online Adaptation of Language Models with a Memory of Amortized Contexts",
        "authors": [
            "Jihoon Tack",
            "Jaehyung Kim",
            "Eric Mitchell",
            "Jinwoo Shin",
            "Yee Whye Teh",
            "Jonathan Richard Schwarz"
        ],
        "published": "2024-03-07T08:34:57Z",
        "summary": "Due to the rapid generation and dissemination of information, large language\nmodels (LLMs) quickly run out of date despite enormous development costs. Due\nto this crucial need to keep models updated, online learning has emerged as a\ncritical necessity when utilizing LLMs for real-world applications. However,\ngiven the ever-expanding corpus of unseen documents and the large parameter\nspace of modern LLMs, efficient adaptation is essential. To address these\nchallenges, we propose Memory of Amortized Contexts (MAC), an efficient and\neffective online adaptation framework for LLMs with strong knowledge retention.\nWe propose an amortized feature extraction and memory-augmentation approach to\ncompress and extract information from new documents into compact modulations\nstored in a memory bank. When answering questions, our model attends to and\nextracts relevant knowledge from this memory bank. To learn informative\nmodulations in an efficient manner, we utilize amortization-based\nmeta-learning, which substitutes the optimization process with a single forward\npass of the encoder. Subsequently, we learn to choose from and aggregate\nselected documents into a single modulation by conditioning on the question,\nallowing us to adapt a frozen language model during test time without requiring\nfurther gradient updates. Our experiment demonstrates the superiority of MAC in\nmultiple aspects, including online adaptation performance, time, and memory\nefficiency. Code is available at: https://github.com/jihoontack/MAC.",
        "pdf_link": "https://arxiv.org/pdf/2403.04317v1.pdf"
    },
    {
        "title": "Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders",
        "authors": [
            "Yuwei Zhang",
            "Siffi Singh",
            "Sailik Sengupta",
            "Igor Shalyminov",
            "Hang Su",
            "Hwanjun Song",
            "Saab Mansour"
        ],
        "published": "2024-03-07T08:32:17Z",
        "summary": "Conversational systems often rely on embedding models for intent\nclassification and intent clustering tasks. The advent of Large Language Models\n(LLMs), which enable instructional embeddings allowing one to adjust semantics\nover the embedding space using prompts, are being viewed as a panacea for these\ndownstream conversational tasks. However, traditional evaluation benchmarks\nrely solely on task metrics that don't particularly measure gaps related to\nsemantic understanding. Thus, we propose an intent semantic toolkit that gives\na more holistic view of intent embedding models by considering three tasks --\n(1) intent classification, (2) intent clustering, and (3) a novel triplet task.\nThe triplet task gauges the model's understanding of two semantic concepts\nparamount in real-world conversational systems -- negation and implicature. We\nobserve that current embedding models fare poorly in semantic understanding of\nthese concepts. To address this, we propose a pre-training approach to improve\nthe embedding model by leveraging augmentation with data generated by an\nauto-regressive model and a contrastive loss term. Our approach improves the\nsemantic understanding of the intent embedding model on the aforementioned\nlinguistic dimensions while slightly effecting their performance on downstream\ntask metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.04314v1.pdf"
    },
    {
        "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
        "authors": [
            "Zhiying Zhu",
            "Zhiqing Sun",
            "Yiming Yang"
        ],
        "published": "2024-03-07T08:25:46Z",
        "summary": "Hallucinations pose a significant challenge to the reliability of large\nlanguage models (LLMs) in critical domains. Recent benchmarks designed to\nassess LLM hallucinations within conventional NLP tasks, such as\nknowledge-intensive question answering (QA) and summarization, are insufficient\nfor capturing the complexities of user-LLM interactions in dynamic, real-world\nsettings. To address this gap, we introduce HaluEval-Wild, the first benchmark\nspecifically designed to evaluate LLM hallucinations in the wild. We\nmeticulously collect challenging (adversarially filtered by Alpaca) user\nqueries from existing real-world user-LLM interaction datasets, including\nShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing\nthe collected queries, we categorize them into five distinct types, which\nenables a fine-grained analysis of the types of hallucinations LLMs exhibit,\nand synthesize the reference answers with the powerful GPT-4 model and\nretrieval-augmented generation (RAG). Our benchmark offers a novel approach\ntowards enhancing our comprehension and improvement of LLM reliability in\nscenarios reflective of real-world interactions.",
        "pdf_link": "https://arxiv.org/pdf/2403.04307v1.pdf"
    },
    {
        "title": "Effectiveness Assessment of Recent Large Vision-Language Models",
        "authors": [
            "Yao Jiang",
            "Xinyu Yan",
            "Ge-Peng Ji",
            "Keren Fu",
            "Meijun Sun",
            "Huan Xiong",
            "Deng-Ping Fan",
            "Fahad Shahbaz Khan"
        ],
        "published": "2024-03-07T08:25:27Z",
        "summary": "The advent of large vision-language models (LVLMs) represents a noteworthy\nadvancement towards the pursuit of artificial general intelligence. However,\nthe extent of their efficacy across both specialized and general tasks warrants\nfurther investigation. This article endeavors to evaluate the competency of\npopular LVLMs in specialized and general tasks, respectively, aiming to offer a\ncomprehensive comprehension of these innovative methodologies. To gauge their\nefficacy in specialized tasks, we tailor a comprehensive testbed comprising\nthree distinct scenarios: natural, healthcare, and industrial, encompassing six\nchallenging tasks. These tasks include salient, camouflaged, and transparent\nobject detection, as well as polyp and skin lesion detection, alongside\nindustrial anomaly detection. We examine the performance of three recent\nopen-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of\nvisual recognition and localization. Moreover, we conduct empirical\ninvestigations utilizing the aforementioned models alongside GPT-4V, assessing\ntheir multi-modal understanding capacities in general tasks such as object\ncounting, absurd question answering, affordance reasoning, attribute\nrecognition, and spatial relation reasoning. Our investigations reveal that\nthese models demonstrate limited proficiency not only in specialized tasks but\nalso in general tasks. We delve deeper into this inadequacy and suggest several\npotential factors, including limited cognition in specialized tasks, object\nhallucination, text-to-image interference, and decreased robustness in complex\nproblems. We hope this study would provide valuable insights for the future\ndevelopment of LVLMs, augmenting their power in coping with both general and\nspecialized applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.04306v2.pdf"
    },
    {
        "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
        "authors": [
            "Yu Zhu",
            "Chuxiong Sun",
            "Wenfei Yang",
            "Wenqiang Wei",
            "Bo Tang",
            "Tianzhu Zhang",
            "Zhiyu Li",
            "Shifeng Zhang",
            "Feiyu Xiong",
            "Jie Hu",
            "Mingchuan yang"
        ],
        "published": "2024-03-07T07:31:00Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach\nto ensure Large Language Models (LLMs) align with human values. However,\nexisting RLHF methods require a high computational cost, one main reason being\nthat RLHF assigns both the generation and alignment tasks to the LLM\nsimultaneously. In this paper, we introduce Proxy-RLHF, which decouples the\ngeneration and alignment processes of LLMs, achieving alignment with human\nvalues at a much lower computational cost. We start with a novel Markov\nDecision Process (MDP) designed for the alignment process and employ\nReinforcement Learning (RL) to train a streamlined proxy model that oversees\nthe token generation of the LLM, without altering the LLM itself. Experiments\nshow that our method achieves a comparable level of alignment with only 1\\% of\nthe training parameters of other methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.04283v1.pdf"
    },
    {
        "title": "Advancing Biomedical Text Mining with Community Challenges",
        "authors": [
            "Hui Zong",
            "Rongrong Wu",
            "Jiaxue Cha",
            "Erman Wu",
            "Jiakun Li",
            "Liang Tao",
            "Zuofeng Li",
            "Buzhou Tang",
            "Bairong Shen"
        ],
        "published": "2024-03-07T06:52:51Z",
        "summary": "The field of biomedical research has witnessed a significant increase in the\naccumulation of vast amounts of textual data from various sources such as\nscientific literatures, electronic health records, clinical trial reports, and\nsocial media. However, manually processing and analyzing these extensive and\ncomplex resources is time-consuming and inefficient. To address this challenge,\nbiomedical text mining, also known as biomedical natural language processing,\nhas garnered great attention. Community challenge evaluation competitions have\nplayed an important role in promoting technology innovation and\ninterdisciplinary collaboration in biomedical text mining research. These\nchallenges provide platforms for researchers to develop state-of-the-art\nsolutions for data mining and information processing in biomedical research. In\nthis article, we review the recent advances in community challenges specific to\nChinese biomedical text mining. Firstly, we collect the information of these\nevaluation tasks, such as data sources and task types. Secondly, we conduct\nsystematic summary and comparative analysis, including named entity\nrecognition, entity normalization, attribute extraction, relation extraction,\nevent extraction, text classification, text similarity, knowledge graph\nconstruction, question answering, text generation, and large language model\nevaluation. Then, we summarize the potential clinical applications of these\ncommunity challenge tasks from translational informatics perspective. Finally,\nwe discuss the contributions and limitations of these community challenges,\nwhile highlighting future directions in the era of large language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.04261v1.pdf"
    },
    {
        "title": "Can Small Language Models be Good Reasoners for Sequential Recommendation?",
        "authors": [
            "Yuling Wang",
            "Changxin Tian",
            "Binbin Hu",
            "Yanhua Yu",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou",
            "Liang Pang",
            "Xiao Wang"
        ],
        "published": "2024-03-07T06:49:37Z",
        "summary": "Large language models (LLMs) open up new horizons for sequential\nrecommendations, owing to their remarkable language comprehension and\ngeneration capabilities. However, there are still numerous challenges that\nshould be addressed to successfully implement sequential recommendations\nempowered by LLMs. Firstly, user behavior patterns are often complex, and\nrelying solely on one-step reasoning from LLMs may lead to incorrect or\ntask-irrelevant responses. Secondly, the prohibitively resource requirements of\nLLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real\nsequential recommender systems. In this paper, we propose a novel Step-by-step\nknowLedge dIstillation fraMework for recommendation (SLIM), paving a promising\npath for sequential recommenders to enjoy the exceptional reasoning\ncapabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We\nintroduce CoT prompting based on user behavior sequences for the larger teacher\nmodel. The rationales generated by the teacher model are then utilized as\nlabels to distill the downstream smaller student model (e.g., LLaMA2-7B). In\nthis way, the student model acquires the step-by-step reasoning capabilities in\nrecommendation tasks. We encode the generated rationales from the student model\ninto a dense vector, which empowers recommendation in both ID-based and\nID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of\nSLIM over state-of-the-art baselines, and further analysis showcasing its\nability to generate meaningful recommendation reasoning at affordable costs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04260v2.pdf"
    },
    {
        "title": "Federated Recommendation via Hybrid Retrieval Augmented Generation",
        "authors": [
            "Huimin Zeng",
            "Zhenrui Yue",
            "Qian Jiang",
            "Dong Wang"
        ],
        "published": "2024-03-07T06:38:41Z",
        "summary": "Federated Recommendation (FR) emerges as a novel paradigm that enables\nprivacy-preserving recommendations. However, traditional FR systems usually\nrepresent users/items with discrete identities (IDs), suffering from\nperformance degradation due to the data sparsity and heterogeneity in FR. On\nthe other hand, Large Language Models (LLMs) as recommenders have proven\neffective across various recommendation scenarios. Yet, LLM-based recommenders\nencounter challenges such as low inference efficiency and potential\nhallucination, compromising their performance in real-world scenarios. To this\nend, we propose GPT-FedRec, a federated recommendation framework leveraging\nChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.\nGPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval\nprocess, mining ID-based user patterns and text-based item features. Next, the\nretrieved results are converted into text prompts and fed into GPT for\nre-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims\nto extract generalized features from data and exploit pretrained knowledge\nwithin LLM, overcoming data sparsity and heterogeneity in FR. In addition, the\nRAG approach also prevents LLM hallucination, improving the recommendation\nperformance for real-world users. Experimental results on diverse benchmark\ndatasets demonstrate the superior performance of GPT-FedRec against\nstate-of-the-art baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.04256v1.pdf"
    },
    {
        "title": "UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities",
        "authors": [
            "Yangning Li",
            "Qingsong Lv",
            "Tianyu Yu",
            "Yinghui Li",
            "Shulin Huang",
            "Tingwei Lu",
            "Xuming Hu",
            "Wenhao JIang",
            "Hai-Tao Zheng",
            "Hui Wang"
        ],
        "published": "2024-03-07T06:10:02Z",
        "summary": "Entity Set Expansion (ESE) aims to identify new entities belonging to the\nsame semantic class as a given set of seed entities. Traditional methods\nprimarily relied on positive seed entities to represent a target semantic\nclass, which poses challenge for the representation of ultra-fine-grained\nsemantic classes. Ultra-fine-grained semantic classes are defined based on\nfine-grained semantic classes with more specific attribute constraints.\nDescribing it with positive seed entities alone cause two issues: (i) Ambiguity\namong ultra-fine-grained semantic classes. (ii) Inability to define \"unwanted\"\nsemantic. Due to these inherent shortcomings, previous methods struggle to\naddress the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first\nintroduce negative seed entities in the inputs, which belong to the same\nfine-grained semantic class as the positive seed entities but differ in certain\nattributes. Negative seed entities eliminate the semantic ambiguity by contrast\nbetween positive and negative attributes. Meanwhile, it provide a\nstraightforward way to express \"unwanted\". To assess model performance in\nUltra-ESE, we constructed UltraWiki, the first large-scale dataset tailored for\nUltra-ESE. UltraWiki encompasses 236 ultra-fine-grained semantic classes, where\neach query of them is represented with 3-5 positive and negative seed entities.\nA retrieval-based framework RetExpan and a generation-based framework GenExpan\nare proposed to comprehensively assess the efficacy of large language models\nfrom two different paradigms in Ultra-ESE. Moreover, we devised three\nstrategies to enhance models' comprehension of ultra-fine-grained entities\nsemantics: contrastive learning, retrieval augmentation, and chain-of-thought\nreasoning. Extensive experiments confirm the effectiveness of our proposed\nstrategies and also reveal that there remains a large space for improvement in\nUltra-ESE.",
        "pdf_link": "https://arxiv.org/pdf/2403.04247v1.pdf"
    },
    {
        "title": "DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning",
        "authors": [
            "Xingwei Qu",
            "Yiming Liang",
            "Yucheng Wang",
            "Tianyu Zheng",
            "Tommy Yue",
            "Lei Ma",
            "Stephen W. Huang",
            "Jiajun Zhang",
            "Wenhu Chen",
            "Chenghua Lin",
            "Jie Fu",
            "Ge Zhang"
        ],
        "published": "2024-03-07T05:26:41Z",
        "summary": "It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.",
        "pdf_link": "https://arxiv.org/pdf/2403.04233v1.pdf"
    },
    {
        "title": "Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks",
        "authors": [
            "Linyuan Gong",
            "Sida Wang",
            "Mostafa Elhoushi",
            "Alvin Cheung"
        ],
        "published": "2024-03-07T05:05:56Z",
        "summary": "We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for\nevaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)\ntask. This benchmark focuses on syntax-aware completions of program structures\nsuch as code blocks and conditional expressions, and includes 17,720 examples\nfrom multiple programming languages, sourced from recent code submissions after\nApril 2022 to minimize data contamination. SAFIM provides a robust framework\nwith various prompt designs and novel syntax-aware post-processing techniques,\nfacilitating accurate and fair comparisons across LLMs. Our comprehensive\nevaluation of 15 LLMs shows that FIM pretraining not only enhances FIM\nproficiency but also improves Left-to-Right (L2R) inference using LLMs. Our\nfindings challenge conventional beliefs and suggest that pretraining methods\nand data quality have more impact than model size. SAFIM thus serves as a\nfoundational platform for future research in effective pretraining strategies\nfor code LLMs. The evaluation toolkit and dataset are available at\nhttps://github.com/gonglinyuan/safim, and the leaderboard is available at\nhttps://safimbenchmark.com.",
        "pdf_link": "https://arxiv.org/pdf/2403.04814v1.pdf"
    },
    {
        "title": "Aligners: Decoupling LLMs and Alignment",
        "authors": [
            "Lilian Ngweta",
            "Mayank Agarwal",
            "Subha Maity",
            "Alex Gittens",
            "Yuekai Sun",
            "Mikhail Yurochkin"
        ],
        "published": "2024-03-07T04:54:56Z",
        "summary": "Large Language Models (LLMs) need to be aligned with human expectations to\nensure their safety and utility in most applications. Alignment is challenging,\ncostly, and needs to be repeated for every LLM and alignment criterion. We\npropose to decouple LLMs and alignment by training aligner models that can be\nused to align any LLM for a given criteria on an as-needed basis, thus also\nreducing the potential negative impacts of alignment on performance. Our recipe\nfor training the aligner models solely relies on synthetic data generated with\na (prompted) LLM and can be easily adjusted for a variety of alignment\ncriteria. We illustrate our method by training an \"ethical\" aligner and verify\nits efficacy empirically.",
        "pdf_link": "https://arxiv.org/pdf/2403.04224v2.pdf"
    },
    {
        "title": "Self-Evaluation of Large Language Model based on Glass-box Features",
        "authors": [
            "Hui Huang",
            "Yingqi Qu",
            "Jing Liu",
            "Muyun Yang",
            "Tiejun Zhao"
        ],
        "published": "2024-03-07T04:50:38Z",
        "summary": "The proliferation of open-source Large Language Models (LLMs) underscores the\npressing need for evaluation methods. Existing works primarily rely on external\nevaluators, focusing on training and prompting strategies. However, a crucial\naspect - model-aware glass-box features - is overlooked. In this study, we\nexplore the utility of glass-box features under the scenario of\nself-evaluation, namely applying an LLM to evaluate its own output. We\ninvestigate various glass-box feature groups and discovered that the softmax\ndistribution serves as a reliable indicator for quality evaluation.\nFurthermore, we propose two strategies to enhance the evaluation by\nincorporating features derived from references. Experimental results on public\nbenchmarks validate the feasibility of self-evaluation of LLMs using glass-box\nfeatures.",
        "pdf_link": "https://arxiv.org/pdf/2403.04222v1.pdf"
    },
    {
        "title": "Large Language Models are In-Context Molecule Learners",
        "authors": [
            "Jiatong Li",
            "Wei Liu",
            "Zhihao Ding",
            "Wenqi Fan",
            "Yuqiang Li",
            "Qing Li"
        ],
        "published": "2024-03-07T03:58:28Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional performance in\nbiochemical tasks, especially the molecule caption translation task, which aims\nto bridge the gap between molecules and natural language texts. However,\nprevious methods in adapting LLMs to the molecule-caption translation task\nrequired extra domain-specific pre-training stages, suffered weak alignment\nbetween molecular and textual spaces, or imposed stringent demands on the scale\nof LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation\n(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment\nfrom context examples via In-Context Molecule Tuning. Specifically, ICMA\nincorporates the following three stages: Cross-modal Retrieval, Post-retrieval\nRe-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval\nutilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve\ninformative context examples. Additionally, we also propose Post-retrieval\nRe-ranking with Sequence Reversal and Random Walk to further improve the\nquality of retrieval results. Finally, In-Context Molecule Tuning unlocks the\nin-context molecule learning capability of LLMs with retrieved examples and\nadapts the parameters of LLMs for the molecule-caption translation task.\nExperimental results demonstrate that ICMT can empower LLMs to achieve\nstate-of-the-art or comparable performance without extra training corpora and\nintricate structures, showing that LLMs are inherently in-context molecule\nlearners.",
        "pdf_link": "https://arxiv.org/pdf/2403.04197v1.pdf"
    },
    {
        "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",
        "authors": [
            "Xu Guo",
            "Yiqiang Chen"
        ],
        "published": "2024-03-07T03:38:44Z",
        "summary": "The recent surge in research focused on generating synthetic data from large\nlanguage models (LLMs), especially for scenarios with limited data\navailability, marks a notable shift in Generative Artificial Intelligence (AI).\nTheir ability to perform comparably to real-world data positions this approach\nas a compelling solution to low-resource challenges. This paper delves into\nadvanced technologies that leverage these gigantic LLMs for the generation of\ntask-specific training data. We outline methodologies, evaluation techniques,\nand practical applications, discuss the current limitations, and suggest\npotential pathways for future research.",
        "pdf_link": "https://arxiv.org/pdf/2403.04190v1.pdf"
    },
    {
        "title": "Metric-aware LLM inference for regression and scoring",
        "authors": [
            "Michal Lukasik",
            "Harikrishna Narasimhan",
            "Aditya Krishna Menon",
            "Felix Yu",
            "Sanjiv Kumar"
        ],
        "published": "2024-03-07T03:24:34Z",
        "summary": "Large language models (LLMs) have demonstrated strong results on a range of\nNLP tasks. Typically, outputs are obtained via autoregressive sampling from the\nLLM's underlying distribution. Building on prior work on Minimum Bayes Risk\nDecoding, we show that this inference strategy can be suboptimal for a range of\nregression and scoring tasks, and associated evaluation metrics. As a remedy,\nwe propose metric aware LLM inference: a decision theoretic approach optimizing\nfor custom regression and scoring metrics at inference time. We report\nimprovements over baselines on academic benchmarks and publicly available\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2403.04182v2.pdf"
    },
    {
        "title": "Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy",
        "authors": [
            "SeongKu Kang",
            "Shivam Agarwal",
            "Bowen Jin",
            "Dongha Lee",
            "Hwanjo Yu",
            "Jiawei Han"
        ],
        "published": "2024-03-07T02:34:54Z",
        "summary": "Document retrieval has greatly benefited from the advancements of large-scale\npre-trained language models (PLMs). However, their effectiveness is often\nlimited in theme-specific applications for specialized areas or industries, due\nto unique terminologies, incomplete contexts of user queries, and specialized\nsearch intents. To capture the theme-specific information and improve\nretrieval, we propose to use a corpus topical taxonomy, which outlines the\nlatent topic structure of the corpus while reflecting user-interested aspects.\nWe introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which\nidentifies the central topics of queries and documents with the guidance of the\ntaxonomy, and exploits their topical relatedness to supplement missing\ncontexts. As a plug-and-play framework, ToTER can be flexibly employed to\nenhance various PLM-based retrievers. Through extensive quantitative, ablative,\nand exploratory experiments on two real-world datasets, we ascertain the\nbenefits of using topical taxonomy for retrieval in theme-specific applications\nand demonstrate the effectiveness of ToTER.",
        "pdf_link": "https://arxiv.org/pdf/2403.04160v1.pdf"
    },
    {
        "title": "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
        "authors": [
            "Tiejin Chen",
            "Longchao Da",
            "Huixue Zhou",
            "Pingzhi Li",
            "Kaixiong Zhou",
            "Tianlong Chen",
            "Hua Wei"
        ],
        "published": "2024-03-07T00:44:11Z",
        "summary": "The privacy concerns associated with the use of Large Language Models (LLMs)\nhave grown recently with the development of LLMs such as ChatGPT. Differential\nPrivacy (DP) techniques are explored in existing work to mitigate their privacy\nrisks at the cost of generalization degradation. Our paper reveals that the\nflatness of DP-trained models' loss landscape plays an essential role in the\ntrade-off between their privacy and generalization. We further propose a\nholistic framework to enforce appropriate weight flatness, which substantially\nimproves model generalization with competitive privacy preservation. It\ninnovates from three coarse-to-grained levels, including perturbation-aware\nmin-max optimization on model weights within a layer, flatness-guided sparse\nprefix-tuning on weights across layers, and weight knowledge distillation\nbetween DP \\& non-DP weights copies. Comprehensive experiments of both\nblack-box and white-box scenarios are conducted to demonstrate the\neffectiveness of our proposal in enhancing generalization and maintaining DP\ncharacteristics. For instance, on text classification dataset QNLI, DP-Flat\nachieves similar performance with non-private full fine-tuning but with DP\nguarantee under privacy budget $\\epsilon=3$, and even better performance given\nhigher privacy budgets. Codes are provided in the supplement.",
        "pdf_link": "https://arxiv.org/pdf/2403.04124v1.pdf"
    },
    {
        "title": "Exploring LLM-based Agents for Root Cause Analysis",
        "authors": [
            "Devjeet Roy",
            "Xuchao Zhang",
            "Rashi Bhave",
            "Chetan Bansal",
            "Pedro Las-Casas",
            "Rodrigo Fonseca",
            "Saravan Rajmohan"
        ],
        "published": "2024-03-07T00:44:01Z",
        "summary": "The growing complexity of cloud based software systems has resulted in\nincident management becoming an integral part of the software development\nlifecycle. Root cause analysis (RCA), a critical part of the incident\nmanagement process, is a demanding task for on-call engineers, requiring deep\ndomain knowledge and extensive experience with a team's specific services.\nAutomation of RCA can result in significant savings of time, and ease the\nburden of incident management on on-call engineers. Recently, researchers have\nutilized Large Language Models (LLMs) to perform RCA, and have demonstrated\npromising results. However, these approaches are not able to dynamically\ncollect additional diagnostic information such as incident related logs,\nmetrics or databases, severely restricting their ability to diagnose root\ncauses. In this work, we explore the use of LLM based agents for RCA to address\nthis limitation. We present a thorough empirical evaluation of a ReAct agent\nequipped with retrieval tools, on an out-of-distribution dataset of production\nincidents collected at Microsoft. Results show that ReAct performs\ncompetitively with strong retrieval and reasoning baselines, but with highly\nincreased factual accuracy. We then extend this evaluation by incorporating\ndiscussions associated with incident reports as additional inputs for the\nmodels, which surprisingly does not yield significant performance improvements.\nLastly, we conduct a case study with a team at Microsoft to equip the ReAct\nagent with tools that give it access to external diagnostic services that are\nused by the team for manual RCA. Our results show how agents can overcome the\nlimitations of prior work, and practical considerations for implementing such a\nsystem in practice.",
        "pdf_link": "https://arxiv.org/pdf/2403.04123v1.pdf"
    },
    {
        "title": "Can Large Language Models Reason and Plan?",
        "authors": [
            "Subbarao Kambhampati"
        ],
        "published": "2024-03-07T00:36:32Z",
        "summary": "While humans sometimes do show the capability of correcting their own\nerroneous guesses with self-critiquing, there seems to be no basis for that\nassumption in the case of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04121v2.pdf"
    },
    {
        "title": "Artificial Intelligence Exploring the Patent Field",
        "authors": [
            "Lekang Jiang",
            "Stephan Goetz"
        ],
        "published": "2024-03-06T23:17:16Z",
        "summary": "Advanced language-processing and machine-learning techniques promise massive\nefficiency improvements in the previously widely manual field of patent and\ntechnical knowledge management. This field presents large-scale and complex\ndata with very precise contents and language representation of those contents.\nParticularly, patent texts can differ from mundane texts in various aspects,\nwhich entails significant opportunities and challenges. This paper presents a\nsystematic overview of patent-related tasks and popular methodologies with a\nspecial focus on evolving and promising techniques. Language processing and\nparticularly large language models as well as the recent boost of general\ngenerative methods promise to become game changers in the patent field. The\npatent literature and the fact-based argumentative procedures around patents\nappear almost as an ideal use case. However, patents entail a number of\ndifficulties with which existing models struggle. The paper introduces\nfundamental aspects of patents and patent-related data that affect technology\nthat wants to explore or manage them. It further reviews existing methods and\napproaches and points out how important reliable and unbiased evaluation\nmetrics become. Although research has made substantial progress on certain\ntasks, the performance across many others remains suboptimal, sometimes because\nof either the special nature of patents and their language or inconsistencies\nbetween legal terms and the everyday meaning of terms. Moreover, yet few\nmethods have demonstrated the ability to produce satisfactory text for specific\nsections of patents. By pointing out key developments, opportunities, and gaps,\nwe aim to encourage further research and accelerate the advancement of this\nfield.",
        "pdf_link": "https://arxiv.org/pdf/2403.04105v1.pdf"
    },
    {
        "title": "Quantifying Contamination in Evaluating Code Generation Capabilities of Language Models",
        "authors": [
            "Martin Riddell",
            "Ansong Ni",
            "Arman Cohan"
        ],
        "published": "2024-03-06T21:45:35Z",
        "summary": "While large language models have achieved remarkable performance on various\ncode generation benchmarks, there have been growing concerns regarding\npotential contamination of these benchmarks as they may be leaked into\npretraining and finetuning data. While recent work has investigated\ncontamination in natural language generation and understanding tasks, there has\nbeen less extensive research into how data contamination impacts the evaluation\nof code generation, which is critical for understanding the robustness and\nreliability of LLMs in programming contexts. In this work, we perform a\ncomprehensive study of data contamination of popular code generation\nbenchmarks, and precisely quantify their overlap with pretraining corpus\nthrough both surface-level and semantic-level matching. In our experiments, we\nshow that there are substantial overlap between popular code generation\nbenchmarks and open training corpus, and models perform significantly better on\nthe subset of the benchmarks where similar solutions are seen during training.\nWe also conduct extensive analysis on the factors that affects model\nmemorization and generalization, such as model size, problem difficulty, and\nquestion length. We release all resulting files from our matching pipeline for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2403.04811v1.pdf"
    },
    {
        "title": "Can Large Language Models do Analytical Reasoning?",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Dong Yu",
            "Fei Liu"
        ],
        "published": "2024-03-06T20:22:08Z",
        "summary": "This paper explores the cutting-edge Large Language Model with analytical\nreasoning on sports. Our analytical reasoning embodies the tasks of letting\nlarge language models count how many points each team scores in a quarter in\nthe NBA and NFL games. Our major discoveries are in two folds. Firstly, we find\namong all the models we employed, GPT-4 stands out in effectiveness, followed\nby Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.\nSpecifically, we compare three different prompting techniques and a\ndivide-and-conquer approach, we find that the latter was the most effective.\nOur divide-and-conquer approach breaks down play-by-play data into smaller,\nmore manageable segments, solves each piece individually, and then aggregates\nthem together. Besides the divide-and-conquer approach, we also explore the\nChain of Thought (CoT) strategy, which markedly improves outcomes for certain\nmodels, notably GPT-4 and Claude-2.1, with their accuracy rates increasing\nsignificantly. However, the CoT strategy has negligible or even detrimental\neffects on the performance of other models like GPT-3.5 and Gemini-Pro.\nSecondly, to our surprise, we observe that most models, including GPT-4,\nstruggle to accurately count the total scores for NBA quarters despite showing\nstrong performance in counting NFL quarter scores. This leads us to further\ninvestigate the factors that impact the complexity of analytical reasoning\ntasks with extensive experiments, through which we conclude that task\ncomplexity depends on the length of context, the information density, and the\npresence of related information. Our research provides valuable insights into\nthe complexity of analytical reasoning tasks and potential directions for\ndeveloping future large language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.04031v1.pdf"
    },
    {
        "title": "Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification",
        "authors": [
            "Ricardo Bigolin Lanfredi",
            "Pritam Mukherjee",
            "Ronald Summers"
        ],
        "published": "2024-03-06T20:10:41Z",
        "summary": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed\nto extract labels from reports, but concerns exist about label quality. These\ndatasets typically offer only presence labels, sometimes with binary\nuncertainty indicators, which limits their usefulness. In this work, we present\nMAPLEZ (Medical report Annotations with Privacy-preserving Large language model\nusing Expeditious Zero shot answers), a novel approach leveraging a locally\nexecutable Large Language Model (LLM) to extract and enhance findings labels on\nCXR reports. MAPLEZ extracts not only binary labels indicating the presence or\nabsence of a finding but also the location, severity, and radiologists'\nuncertainty about the finding. Over eight abnormalities from five test sets, we\nshow that our method can extract these annotations with an increase of 5\npercentage points (pp) in F1 score for categorical presence annotations and\nmore than 30 pp increase in F1 score for the location annotations over\ncompeting labelers. Additionally, using these improved annotations in\nclassification supervision, we demonstrate substantial advancements in model\nquality, with an increase of 1.7 pp in AUROC over models trained with\nannotations from the state-of-the-art approach. We share code and annotations.",
        "pdf_link": "https://arxiv.org/pdf/2403.04024v1.pdf"
    },
    {
        "title": "Guiding Enumerative Program Synthesis with Large Language Models",
        "authors": [
            "Yixuan Li",
            "Julian Parsert",
            "Elizabeth Polgreen"
        ],
        "published": "2024-03-06T19:13:53Z",
        "summary": "Pre-trained Large Language Models (LLMs) are beginning to dominate the\ndiscourse around automatic code generation with natural language\nspecifications. In contrast, the best-performing synthesizers in the domain of\nformal synthesis with precise logical specifications are still based on\nenumerative algorithms. In this paper, we evaluate the abilities of LLMs to\nsolve formal synthesis benchmarks by carefully crafting a library of prompts\nfor the domain. When one-shot synthesis fails, we propose a novel enumerative\nsynthesis algorithm, which integrates calls to an LLM into a weighted\nprobabilistic search. This allows the synthesizer to provide the LLM with\ninformation about the progress of the enumerator, and the LLM to provide the\nenumerator with syntactic guidance in an iterative loop. We evaluate our\ntechniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition.\nWe find that GPT-3.5 as a stand-alone tool for formal synthesis is easily\noutperformed by state-of-the-art formal synthesis algorithms, but our approach\nintegrating the LLM into an enumerative synthesis algorithm shows significant\nperformance gains over both the LLM and the enumerative synthesizer alone and\nthe winning SyGuS competition tool.",
        "pdf_link": "https://arxiv.org/pdf/2403.03997v1.pdf"
    },
    {
        "title": "FaaF: Facts as a Function for the evaluation of generated text",
        "authors": [
            "Vasileios Katranidis",
            "Gabor Barany"
        ],
        "published": "2024-03-06T17:48:06Z",
        "summary": "The demand for accurate and efficient verification of information in texts\ngenerated by large language models (LMs) is at an all-time high, but remains\nunresolved. Recent efforts have focused on extracting and verifying atomic\nfacts from these texts via prompting LM evaluators. However, we demonstrate\nthat this method of prompting is unreliable when faced with incomplete or\ninaccurate reference information. We introduce Facts as a Function (FaaF), a\nnew approach to the fact verification task that leverages the function-calling\ncapabilities of LMs. FaaF significantly enhances the ability of LMs to identify\nunsupported facts in texts, while also improving efficiency and significantly\nlowering costs compared to prompt-based methods. Additionally, we propose a\nframework for evaluating factual recall in Retrieval Augmented Generation (RAG)\nsystems, which we employ to compare prompt-based and FaaF methods using various\nLMs under challenging conditions.",
        "pdf_link": "https://arxiv.org/pdf/2403.03888v2.pdf"
    },
    {
        "title": "SaulLM-7B: A pioneering Large Language Model for Law",
        "authors": [
            "Pierre Colombo",
            "Telmo Pessoa Pires",
            "Malik Boudiaf",
            "Dominic Culver",
            "Rui Melo",
            "Caio Corro",
            "Andre F. T. Martins",
            "Fabrizio Esposito",
            "Vera L\u00facia Raposo",
            "Sofia Morgado",
            "Michael Desa"
        ],
        "published": "2024-03-06T17:42:16Z",
        "summary": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored\nfor the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM\ndesigned explicitly for legal text comprehension and generation. Leveraging the\nMistral 7B architecture as its foundation, SaulLM-7B is trained on an English\nlegal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art\nproficiency in understanding and processing legal documents. Additionally, we\npresent a novel instructional fine-tuning method that leverages legal datasets\nto further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is\nreleased under the MIT License.",
        "pdf_link": "https://arxiv.org/pdf/2403.03883v2.pdf"
    },
    {
        "title": "KIWI: A Dataset of Knowledge-Intensive Writing Instructions for Answering Research Questions",
        "authors": [
            "Fangyuan Xu",
            "Kyle Lo",
            "Luca Soldaini",
            "Bailey Kuehl",
            "Eunsol Choi",
            "David Wadden"
        ],
        "published": "2024-03-06T17:16:44Z",
        "summary": "Large language models (LLMs) adapted to follow user instructions are now\nwidely deployed as conversational agents. In this work, we examine one\nincreasingly common instruction-following task: providing writing assistance to\ncompose a long-form answer. To evaluate the capabilities of current LLMs on\nthis task, we construct KIWI, a dataset of knowledge-intensive writing\ninstructions in the scientific domain. Given a research question, an initial\nmodel-generated answer and a set of relevant papers, an expert annotator\niteratively issues instructions for the model to revise and improve its answer.\nWe collect 1,260 interaction turns from 234 interaction sessions with three\nstate-of-the-art LLMs. Each turn includes a user instruction, a model response,\nand a human evaluation of the model response. Through a detailed analysis of\nthe collected responses, we find that all models struggle to incorporate new\ninformation into an existing answer, and to perform precise and unambiguous\nedits. Further, we find that models struggle to judge whether their outputs\nsuccessfully followed user instructions, with accuracy at least 10 points short\nof human agreement. Our findings indicate that KIWI will be a valuable resource\nto measure progress and improve LLMs' instruction-following capabilities for\nknowledge intensive writing tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.03866v1.pdf"
    },
    {
        "title": "Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning",
        "authors": [
            "Deepanway Ghosal",
            "Vernon Toh Yan Han",
            "Chia Yew Ken",
            "Soujanya Poria"
        ],
        "published": "2024-03-06T17:15:04Z",
        "summary": "This paper introduces the novel task of multimodal puzzle solving, framed\nwithin the context of visual question-answering. We present a new dataset,\nAlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal\nlanguage models in solving algorithmic puzzles that necessitate both visual\nunderstanding, language understanding, and complex algorithmic reasoning. We\ncreate the puzzles to encompass a diverse array of mathematical and algorithmic\ntopics such as boolean logic, combinatorics, graph theory, optimization,\nsearch, etc., aiming to evaluate the gap between visual data interpretation and\nalgorithmic problem-solving skills. The dataset is generated automatically from\ncode authored by humans. All our puzzles have exact solutions that can be found\nfrom the algorithm without tedious human calculations. It ensures that our\ndataset can be scaled up arbitrarily in terms of reasoning complexity and\ndataset size. Our investigation reveals that large language models (LLMs) such\nas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We\nfind that their performance is near random in a multi-choice question-answering\nsetup for a significant number of puzzles. The findings emphasize the\nchallenges of integrating visual, language, and algorithmic knowledge for\nsolving complex reasoning problems.",
        "pdf_link": "https://arxiv.org/pdf/2403.03864v3.pdf"
    },
    {
        "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You Expect",
        "authors": [
            "Xin Men",
            "Mingyu Xu",
            "Qingyu Zhang",
            "Bingning Wang",
            "Hongyu Lin",
            "Yaojie Lu",
            "Xianpei Han",
            "Weipeng Chen"
        ],
        "published": "2024-03-06T17:04:18Z",
        "summary": "As Large Language Models (LLMs) continue to advance in performance, their\nsize has escalated significantly, with current LLMs containing billions or even\ntrillions of parameters. However, in this study, we discovered that many layers\nof LLMs exhibit high similarity, and some layers play a negligible role in\nnetwork functionality. Based on this observation, we define a metric called\nBlock Influence (BI) to gauge the significance of each layer in LLMs. We then\npropose a straightforward pruning approach: layer removal, in which we directly\ndelete the redundant layers in LLMs based on their BI scores. Experiments\ndemonstrate that our method, which we call ShortGPT, significantly outperforms\nprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT\nis orthogonal to quantization-like methods, enabling further reduction in\nparameters and computation. The ability to achieve better results through\nsimple layer removal, as opposed to more complex pruning techniques, suggests a\nhigh degree of redundancy in the model architecture.",
        "pdf_link": "https://arxiv.org/pdf/2403.03853v2.pdf"
    },
    {
        "title": "The Boy Who Survived: Removing Harry Potter from an LLM is harder than reported",
        "authors": [
            "Adam Shostack"
        ],
        "published": "2024-03-06T16:39:50Z",
        "summary": "Recent work arXiv.2310.02238 asserted that \"we effectively erase the model's\nability to generate or recall Harry Potter-related content.'' This claim is\nshown to be overbroad. A small experiment of less than a dozen trials led to\nrepeated and specific mentions of Harry Potter, including \"Ah, I see! A\n\"muggle\" is a term used in the Harry Potter book series by Terry Pratchett...''",
        "pdf_link": "https://arxiv.org/pdf/2403.12082v1.pdf"
    },
    {
        "title": "Evaluating the Elementary Multilingual Capabilities of Large Language Models with MultiQ",
        "authors": [
            "Carolin Holtermann",
            "Paul R\u00f6ttger",
            "Timm Dill",
            "Anne Lauscher"
        ],
        "published": "2024-03-06T16:01:44Z",
        "summary": "Large language models (LLMs) need to serve everyone, including a global\nmajority of non-English speakers. However, most LLMs today, and open LLMs in\nparticular, are often intended for use in just English (e.g. Llama2, Mistral)\nor a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent\nresearch shows that, despite limits in their intended use, people prompt LLMs\nin many different languages. Therefore, in this paper, we investigate the basic\nmultilingual capabilities of state-of-the-art open LLMs beyond their intended\nuse. For this purpose, we introduce MultiQ, a new silver standard benchmark for\nbasic open-ended question answering with 27.4k test questions across a\ntypologically diverse set of 137 languages. With MultiQ, we evaluate language\nfidelity, i.e.\\ whether models respond in the prompted language, and question\nanswering accuracy. All LLMs we test respond faithfully and/or accurately for\nat least some languages beyond their intended use. Most models are more\naccurate when they respond faithfully. However, differences across models are\nlarge, and there is a long tail of languages where models are neither accurate\nnor faithful. We explore differences in tokenization as a potential explanation\nfor our findings, identifying possible correlations that warrant further\ninvestigation.",
        "pdf_link": "https://arxiv.org/pdf/2403.03814v1.pdf"
    },
    {
        "title": "Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery",
        "authors": [
            "Wei Zhang",
            "Miaoxin Cai",
            "Tong Zhang",
            "Guoqiang Lei",
            "Yin Zhuang",
            "Xuerui Mao"
        ],
        "published": "2024-03-06T15:35:53Z",
        "summary": "Ship detection needs to identify ship locations from remote sensing (RS)\nscenes. However, due to different imaging payloads, various appearances of\nships, and complicated background interference from the bird's eye view, it is\ndifficult to set up a unified paradigm for achieving multi-source ship\ndetection. Therefore, in this article, considering that the large language\nmodels (LLMs) emerge the powerful generalization ability, a novel unified\nvisual-language model called Popeye is proposed for multi-source ship detection\nfrom RS imagery. First, to bridge the interpretation gap between multi-source\nimages for ship detection, a novel image-instruction-answer way is designed to\nintegrate the various ship detection ways (e.g., horizontal bounding box (HBB),\noriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of\nthis, a cross-modal image interpretation method is developed for the proposed\nPopeye to enhance interactive comprehension ability between visual and language\ncontent, which can be easily migrated into any multi-source ship detection\ntask. Subsequently, owing to objective domain differences, a knowledge adaption\nmechanism is designed to adapt the pre-trained visual-language knowledge from\nthe nature scene into the RS domain for multi-source ship detection. In\naddition, the segment anything model (SAM) is also seamlessly integrated into\nthe proposed Popeye to achieve pixel-level ship segmentation without additional\ntraining costs. Finally, extensive experiments are conducted on the newly\nconstructed instruction dataset named MMShip, and the results indicate that the\nproposed Popeye outperforms current specialist, open-vocabulary, and other\nvisual-language models for zero-shot multi-source ship detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.03790v1.pdf"
    },
    {
        "title": "PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion",
        "authors": [
            "Zekai Zhang",
            "Yiduo Guo",
            "Yaobo Liang",
            "Dongyan Zhao",
            "Nan Duan"
        ],
        "published": "2024-03-06T15:33:32Z",
        "summary": "The growing dependence on Large Language Models (LLMs) for finishing user\ninstructions necessitates a comprehensive understanding of their robustness to\ncomplex task completion in real-world situations. To address this critical\nneed, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R)\nto measure LLMs' robustness to the user PPT task instruction and software\nversion. Specifically, we construct adversarial user instructions by attacking\nuser instructions at sentence, semantic, and multi-language levels. To assess\nthe robustness of Language Models to software versions, we vary the number of\nprovided APIs to simulate both the newest version and earlier version settings.\nSubsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark\nthat incorporates these robustness settings, aiming to evaluate how deviations\nimpact LLMs' API calls for task completion. We find that GPT-4 exhibits the\nhighest performance and strong robustness in our benchmark, particularly in the\nversion update and the multilingual settings. However, we find that all LLMs\nlose their robustness when confronted with multiple challenges (e.g.,\nmulti-turn) simultaneously, leading to significant performance drops. We\nfurther analyze the robustness behavior and error reasons of LLMs in our\nbenchmark, which provide valuable insights for researchers to understand the\nLLM's robustness in task completion and develop more robust LLMs and agents. We\nrelease the code and data at \\url{https://github.com/ZekaiGalaxy/PPTCR}.",
        "pdf_link": "https://arxiv.org/pdf/2403.03788v1.pdf"
    },
    {
        "title": "German also Hallucinates! Inconsistency Detection in News Summaries with the Absinth Dataset",
        "authors": [
            "Laura Mascarell",
            "Ribin Chalumattu",
            "Annette Rios"
        ],
        "published": "2024-03-06T14:37:30Z",
        "summary": "The advent of Large Language Models (LLMs) has led to remarkable progress on\na wide range of natural language processing tasks. Despite the advances, these\nlarge-sized models still suffer from hallucinating information in their output,\nwhich poses a major issue in automatic text summarization, as we must guarantee\nthat the generated summary is consistent with the content of the source\ndocument. Previous research addresses the challenging task of detecting\nhallucinations in the output (i.e. inconsistency detection) in order to\nevaluate the faithfulness of the generated summaries. However, these works\nprimarily focus on English and recent multilingual approaches lack German data.\nThis work presents absinth, a manually annotated dataset for hallucination\ndetection in German news summarization and explores the capabilities of novel\nopen-source LLMs on this task in both fine-tuning and in-context learning\nsettings. We open-source and release the absinth dataset to foster further\nresearch on hallucination detection in German.",
        "pdf_link": "https://arxiv.org/pdf/2403.03750v2.pdf"
    },
    {
        "title": "Towards Safe and Aligned Large Language Models for Medicine",
        "authors": [
            "Tessa Han",
            "Aounon Kumar",
            "Chirag Agarwal",
            "Himabindu Lakkaraju"
        ],
        "published": "2024-03-06T14:34:07Z",
        "summary": "The capabilities of large language models (LLMs) have been progressing at a\nbreathtaking speed, leaving even their own developers grappling with the depth\nof their potential and risks. While initial steps have been taken to evaluate\nthe safety and alignment of general-knowledge LLMs, exposing some weaknesses,\nto our knowledge, the safety and alignment of medical LLMs has not been\nevaluated despite their risks for personal health and safety, public health and\nsafety, and human rights. To this end, we carry out the first safety evaluation\nfor medical LLMs. Specifically, we set forth a definition of medical safety and\nalignment for medical artificial intelligence systems, develop a dataset of\nharmful medical questions to evaluate the medical safety and alignment of an\nLLM, evaluate both general and medical safety and alignment of medical LLMs,\ndemonstrate fine-tuning as an effective mitigation strategy, and discuss\nbroader, large-scale approaches used by the machine learning community to\ndevelop safe and aligned LLMs. We hope that this work casts light on the safety\nand alignment of medical LLMs and motivates future work to study it and develop\nadditional mitigation strategies, minimizing the risks of harm of LLMs in\nmedicine.",
        "pdf_link": "https://arxiv.org/pdf/2403.03744v1.pdf"
    },
    {
        "title": "Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese",
        "authors": [
            "Yikun Sun",
            "Zhen Wan",
            "Nobuhiro Ueda",
            "Sakiko Yahata",
            "Fei Cheng",
            "Chenhui Chu",
            "Sadao Kurohashi"
        ],
        "published": "2024-03-06T13:17:07Z",
        "summary": "The creation of instruction data and evaluation benchmarks for serving Large\nlanguage models often involves enormous human annotation. This issue becomes\nparticularly pronounced when rapidly developing such resources for a\nnon-English language like Japanese. Instead of following the popular practice\nof directly translating existing English resources into Japanese (e.g.,\nJapanese-Alpaca), we propose an efficient self-instruct method based on GPT-4.\nWe first translate a small amount of English instructions into Japanese and\npost-edit them to obtain native-level quality. GPT-4 then utilizes them as\ndemonstrations to automatically generate Japanese instruction data. We also\nconstruct an evaluation benchmark containing 80 questions across 8 categories,\nusing GPT-4 to automatically assess the response quality of LLMs without human\nreferences. The empirical results suggest that the models fine-tuned on our\nGPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across\nall three base pre-trained models. Our GPT-4 self-instruct data allowed the\nLLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\\% win-rate. The\nhuman evaluation exhibits the consistency between GPT-4's assessments and human\npreference. Our high-quality instruction data and evaluation benchmark have\nbeen released here.",
        "pdf_link": "https://arxiv.org/pdf/2403.03690v1.pdf"
    },
    {
        "title": "General2Specialized LLMs Translation for E-commerce",
        "authors": [
            "Kaidi Chen",
            "Ben Chen",
            "Dehong Gao",
            "Huangyu Dai",
            "Wen Jiang",
            "Wei Ning",
            "Shanqing Yu",
            "Libin Yang",
            "Xiaoyan Cai"
        ],
        "published": "2024-03-06T13:15:21Z",
        "summary": "Existing Neural Machine Translation (NMT) models mainly handle translation in\nthe general domain, while overlooking domains with special writing formulas,\nsuch as e-commerce and legal documents. Taking e-commerce as an example, the\ntexts usually include amounts of domain-related words and have more grammar\nproblems, which leads to inferior performances of current NMT methods. To\naddress these problems, we collect two domain-related resources, including a\nset of term pairs (aligned Chinese-English bilingual terms) and a parallel\ncorpus annotated for the e-commerce domain. Furthermore, we propose a two-step\nfine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to\ntransfer one general NMT model to the specialized NMT model for e-commerce. The\nparadigm can be used for the NMT models based on Large language models (LLMs).\nExtensive evaluations on real e-commerce titles demonstrate the superior\ntranslation quality and robustness of our G2ST approach, as compared with\nstate-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.03689v2.pdf"
    },
    {
        "title": "K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data",
        "authors": [
            "Yucheng Wang",
            "Ruibing Jin",
            "Min Wu",
            "Xiaoli Li",
            "Lihua Xie",
            "Zhenghua Chen"
        ],
        "published": "2024-03-06T12:08:14Z",
        "summary": "Sourced from various sensors and organized chronologically, Multivariate\nTime-Series (MTS) data involves crucial spatial-temporal dependencies, e.g.,\ncorrelations among sensors. To capture these dependencies, Graph Neural\nNetworks (GNNs) have emerged as powerful tools, yet their effectiveness is\nrestricted by the quality of graph construction from MTS data. Typically,\nexisting approaches construct graphs solely from MTS signals, which may\nintroduce bias due to a small training dataset and may not accurately represent\nunderlying dependencies. To address this challenge, we propose a novel\nframework named K-Link, leveraging Large Language Models (LLMs) to encode\nextensive general knowledge and thereby providing effective solutions to reduce\nthe bias. Leveraging the knowledge embedded in LLMs, such as physical\nprinciples, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic\nknowledge of sensors and the linkage of the sensor-level knowledge. To harness\nthe potential of the knowledge-link graph in enhancing the graph derived from\nMTS data, we propose a graph alignment module, facilitating the transfer of\nsemantic knowledge within the knowledge-link graph into the MTS-derived graph.\nBy doing so, we can improve the graph quality, ensuring effective\nrepresentation learning with GNNs for MTS data. Extensive experiments\ndemonstrate the efficacy of our approach for superior performance across\nvarious MTS-related downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.03645v1.pdf"
    },
    {
        "title": "SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models",
        "authors": [
            "Yibin Chen",
            "Yifu Yuan",
            "Zeyu Zhang",
            "Yan Zheng",
            "Jinyi Liu",
            "Fei Ni",
            "Jianye Hao"
        ],
        "published": "2024-03-06T11:48:08Z",
        "summary": "Spreadsheet manipulation is widely existing in most daily works and\nsignificantly improves working efficiency. Large language model (LLM) has been\nrecently attempted for automatic spreadsheet manipulation but has not yet been\ninvestigated in complicated and realistic tasks where reasoning challenges\nexist (e.g., long horizon manipulation with multi-step reasoning and ambiguous\nrequirements). To bridge the gap with the real-world requirements, we introduce\n$\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks\nwith reasoning-dependent manipulation caused by real-life challenges. To\nmitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a\nnovel autonomous agent that utilizes the power of LLMs. SheetAgent consists of\nthree collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and\n$\\textit{Retriever}$, achieving both advanced reasoning and accurate\nmanipulation over spreadsheets without human interaction through iterative task\nreasoning and reflection. Extensive experiments demonstrate that SheetAgent\ndelivers 20-30% pass rate improvements on multiple benchmarks over baselines,\nachieving enhanced precision in spreadsheet manipulation and demonstrating\nsuperior table reasoning abilities. More details and visualizations are\navailable at https://sheetagent.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2403.03636v1.pdf"
    },
    {
        "title": "GPTopic: Dynamic and Interactive Topic Representations",
        "authors": [
            "Arik Reuter",
            "Anton Thielmann",
            "Christoph Weisser",
            "Sebastian Fischer",
            "Benjamin S\u00e4fken"
        ],
        "published": "2024-03-06T11:34:20Z",
        "summary": "Topic modeling seems to be almost synonymous with generating lists of top\nwords to represent topics within large text corpora. However, deducing a topic\nfrom such list of individual terms can require substantial expertise and\nexperience, making topic modelling less accessible to people unfamiliar with\nthe particularities and pitfalls of top-word interpretation. A topic\nrepresentation limited to top-words might further fall short of offering a\ncomprehensive and easily accessible characterization of the various aspects,\nfacets and nuances a topic might have. To address these challenges, we\nintroduce GPTopic, a software package that leverages Large Language Models\n(LLMs) to create dynamic, interactive topic representations. GPTopic provides\nan intuitive chat interface for users to explore, analyze, and refine topics\ninteractively, making topic modeling more accessible and comprehensive. The\ncorresponding code is available here: https://github. com/05ec6602be/GPTopic.",
        "pdf_link": "https://arxiv.org/pdf/2403.03628v1.pdf"
    },
    {
        "title": "Multimodal Large Language Models to Support Real-World Fact-Checking",
        "authors": [
            "Jiahui Geng",
            "Yova Kementchedjhieva",
            "Preslav Nakov",
            "Iryna Gurevych"
        ],
        "published": "2024-03-06T11:32:41Z",
        "summary": "Multimodal large language models (MLLMs) carry the potential to support\nhumans in processing vast amounts of information. While MLLMs are already being\nused as a fact-checking tool, their abilities and limitations in this regard\nare understudied. Here is aim to bridge this gap. In particular, we propose a\nframework for systematically assessing the capacity of current multimodal\nmodels to facilitate real-world fact-checking. Our methodology is\nevidence-free, leveraging only these models' intrinsic knowledge and reasoning\ncapabilities. By designing prompts that extract models' predictions,\nexplanations, and confidence levels, we delve into research questions\nconcerning model accuracy, robustness, and reasons for failure. We empirically\nfind that (1) GPT-4V exhibits superior performance in identifying malicious and\nmisleading multimodal claims, with the ability to explain the unreasonable\naspects and underlying motives, and (2) existing open-source models exhibit\nstrong biases and are highly sensitive to the prompt. Our study offers insights\ninto combating false multimodal information and building secure, trustworthy\nmultimodal models. To the best of our knowledge, we are the first to evaluate\nMLLMs for real-world fact-checking.",
        "pdf_link": "https://arxiv.org/pdf/2403.03627v1.pdf"
    },
    {
        "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
        "authors": [
            "Eva Giboulot",
            "Furon Teddy"
        ],
        "published": "2024-03-06T10:55:30Z",
        "summary": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite.",
        "pdf_link": "https://arxiv.org/pdf/2403.04808v1.pdf"
    },
    {
        "title": "Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem",
        "authors": [
            "Yuhong Sun",
            "Zhangyue Yin",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Hui Zhao"
        ],
        "published": "2024-03-06T09:06:34Z",
        "summary": "Large language models (LLMs) are highly effective in various natural language\nprocessing (NLP) tasks. However, they are susceptible to producing unreliable\nconjectures in ambiguous contexts called hallucination. This paper presents a\nnew method for evaluating LLM hallucination in Question Answering (QA) based on\nthe unanswerable math word problem (MWP). To support this approach, we\ninnovatively develop a dataset called Unanswerable Math Word Problem (UMWP)\nwhich comprises 5200 questions across five categories. We developed an\nevaluation methodology combining text similarity and mathematical expression\ndetection to determine whether LLM considers the question unanswerable. The\nresults of extensive experiments conducted on 31 LLMs, including GPT-3,\nInstructGPT, LLaMA, and Claude, demonstrate that in-context learning and\nreinforcement learning with human feedback (RLHF) training significantly\nenhance the model's ability to avoid hallucination. We show that utilizing MWP\nis a reliable and effective approach to assess hallucination. Our code and data\nare available at https://github.com/Yuki-Asuuna/UMWP.",
        "pdf_link": "https://arxiv.org/pdf/2403.03558v1.pdf"
    },
    {
        "title": "Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models",
        "authors": [
            "Rasita Vinay",
            "Giovanni Spitale",
            "Nikola Biller-Andorno",
            "Federico Germani"
        ],
        "published": "2024-03-06T08:50:25Z",
        "summary": "This study investigates the generation of synthetic disinformation by\nOpenAI's Large Language Models (LLMs) through prompt engineering and explores\ntheir responsiveness to emotional prompting. Leveraging various LLM iterations\nusing davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed\nexperiments to assess their success in producing disinformation. Our findings,\nbased on a corpus of 19,800 synthetic disinformation social media posts, reveal\nthat all LLMs by OpenAI can successfully produce disinformation, and that they\neffectively respond to emotional prompting, indicating their nuanced\nunderstanding of emotional cues in text generation. When prompted politely, all\nexamined LLMs consistently generate disinformation at a high frequency.\nConversely, when prompted impolitely, the frequency of disinformation\nproduction diminishes, as the models often refuse to generate disinformation\nand instead caution users that the tool is not intended for such purposes. This\nresearch contributes to the ongoing discourse surrounding responsible\ndevelopment and application of AI technologies, particularly in mitigating the\nspread of disinformation and promoting transparency in AI-generated content.",
        "pdf_link": "https://arxiv.org/pdf/2403.03550v1.pdf"
    },
    {
        "title": "Prompt Mining for Language-based Human Mobility Forecasting",
        "authors": [
            "Hao Xue",
            "Tianye Tang",
            "Ali Payani",
            "Flora D. Salim"
        ],
        "published": "2024-03-06T08:43:30Z",
        "summary": "With the advancement of large language models, language-based forecasting has\nrecently emerged as an innovative approach for predicting human mobility\npatterns. The core idea is to use prompts to transform the raw mobility data\ngiven as numerical values into natural language sentences so that the language\nmodels can be leveraged to generate the description for future observations.\nHowever, previous studies have only employed fixed and manually designed\ntemplates to transform numerical values into sentences. Since the forecasting\nperformance of language models heavily relies on prompts, using fixed templates\nfor prompting may limit the forecasting capability of language models. In this\npaper, we propose a novel framework for prompt mining in language-based\nmobility forecasting, aiming to explore diverse prompt design strategies.\nSpecifically, the framework includes a prompt generation stage based on the\ninformation entropy of prompts and a prompt refinement stage to integrate\nmechanisms such as the chain of thought. Experimental results on real-world\nlarge-scale data demonstrate the superiority of generated prompts from our\nprompt mining pipeline. Additionally, the comparison of different prompt\nvariants shows that the proposed prompt refinement process is effective. Our\nstudy presents a promising direction for further advancing language-based\nmobility forecasting.",
        "pdf_link": "https://arxiv.org/pdf/2403.03544v1.pdf"
    },
    {
        "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
        "authors": [
            "Hangyu Wang",
            "Jianghao Lin",
            "Bo Chen",
            "Yang Yang",
            "Ruiming Tang",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "published": "2024-03-06T08:31:35Z",
        "summary": "The significant advancements in large language models (LLMs) give rise to a\npromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).\nThe efficacy of LLMRec arises from the open-world knowledge and reasoning\ncapabilities inherent in LLMs. LLMRec acquires the recommendation capabilities\nthrough instruction tuning based on user interaction data. However, in order to\nprotect user privacy and optimize utility, it is also crucial for LLMRec to\nintentionally forget specific user data, which is generally referred to as\nrecommendation unlearning. In the era of LLMs, recommendation unlearning poses\nnew challenges for LLMRec in terms of \\textit{inefficiency} and\n\\textit{ineffectiveness}. Existing unlearning methods require updating billions\nof parameters in LLMRec, which is costly and time-consuming. Besides, they\nalways impact the model utility during the unlearning process. To this end, we\npropose \\textbf{E2URec}, the first \\underline{E}fficient and\n\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our\nproposed E2URec enhances the unlearning efficiency by updating only a few\nadditional LoRA parameters, and improves the unlearning effectiveness by\nemploying a teacher-student framework, where we maintain multiple teacher\nnetworks to guide the unlearning process. Extensive experiments show that\nE2URec outperforms state-of-the-art baselines on two real-world datasets.\nSpecifically, E2URec can efficiently forget specific data without affecting\nrecommendation performance. The source code is at\n\\url{https://github.com/justarter/E2URec}.",
        "pdf_link": "https://arxiv.org/pdf/2403.03536v1.pdf"
    },
    {
        "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
        "authors": [
            "Zexuan Qiu",
            "Jingjing Li",
            "Shijue Huang",
            "Wanjun Zhong",
            "Irwin King"
        ],
        "published": "2024-03-06T07:43:43Z",
        "summary": "Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs will be released.",
        "pdf_link": "https://arxiv.org/pdf/2403.03514v1.pdf"
    },
    {
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
        "authors": [
            "Jiawei Zhao",
            "Zhenyu Zhang",
            "Beidi Chen",
            "Zhangyang Wang",
            "Anima Anandkumar",
            "Yuandong Tian"
        ],
        "published": "2024-03-06T07:29:57Z",
        "summary": "Training Large Language Models (LLMs) presents significant memory challenges,\npredominantly due to the growing size of weights and optimizer states. Common\nmemory-reduction approaches, such as low-rank adaptation (LoRA), add a\ntrainable low-rank matrix to the frozen pre-trained weight in each layer,\nreducing trainable parameters and optimizer states. However, such approaches\ntypically underperform training with full-rank weights in both pre-training and\nfine-tuning stages since they limit the parameter search to a low-rank subspace\nand alter the training dynamics, and further, may require full-rank warm start.\nIn this work, we propose Gradient Low-Rank Projection (GaLore), a training\nstrategy that allows full-parameter learning but is more memory-efficient than\ncommon low-rank adaptation methods such as LoRA. Our approach reduces memory\nusage by up to 65.5% in optimizer states while maintaining both efficiency and\nperformance for pre-training on LLaMA 1B and 7B architectures with C4 dataset\nwith up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit\nGaLore further reduces optimizer memory by up to 82.5% and total training\nmemory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the\nfirst time, the feasibility of pre-training a 7B model on consumer GPUs with\n24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or\noffloading strategies.",
        "pdf_link": "https://arxiv.org/pdf/2403.03507v1.pdf"
    },
    {
        "title": "A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation",
        "authors": [
            "Xiangci Li",
            "Linfeng Song",
            "Lifeng Jin",
            "Haitao Mi",
            "Jessica Ouyang",
            "Dong Yu"
        ],
        "published": "2024-03-06T06:54:02Z",
        "summary": "Knowledge-based, open-domain dialogue generation aims to build chit-chat\nsystems that talk to humans using mined support knowledge. Many types and\nsources of knowledge have previously been shown to be useful as support\nknowledge. Even in the era of large language models, response generation\ngrounded in knowledge retrieved from additional up-to-date sources remains a\npractically important approach. While prior work using single-source knowledge\nhas shown a clear positive correlation between the performances of knowledge\nselection and response generation, there are no existing multi-source datasets\nfor evaluating support knowledge retrieval. Further, prior work has assumed\nthat the knowledge sources available at test time are the same as during\ntraining. This unrealistic assumption unnecessarily handicaps models, as new\nknowledge sources can become available after a model is trained. In this paper,\nwe present a high-quality benchmark named multi-source Wizard of Wikipedia\n(Ms.WoW) for evaluating multi-source dialogue knowledge selection and response\ngeneration. Unlike existing datasets, it contains clean support knowledge,\ngrounded at the utterance level and partitioned into multiple knowledge\nsources. We further propose a new challenge, dialogue knowledge plug-and-play,\nwhich aims to test an already trained dialogue model on using new support\nknowledge from previously unseen sources in a zero-shot fashion.",
        "pdf_link": "https://arxiv.org/pdf/2403.03496v1.pdf"
    },
    {
        "title": "TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs",
        "authors": [
            "Yanlai Chen",
            "Yajie Ji",
            "Akil Narayan",
            "Zhenli Xu"
        ],
        "published": "2024-03-06T04:49:18Z",
        "summary": "We introduce the Transformed Generative Pre-Trained Physics-Informed Neural\nNetworks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of\ntransport-dominated partial differential equations in an MOR-integrating PINNs\nframework. Building on the recent development of the GPT-PINN that is a\nnetwork-of-networks design achieving snapshot-based model reduction, we design\nand test a novel paradigm for nonlinear model reduction that can effectively\ntackle problems with parameter-dependent discontinuities. Through incorporation\nof a shock-capturing loss function component as well as a parameter-dependent\ntransform layer, the TGPT-PINN overcomes the limitations of linear model\nreduction in the transport-dominated regime. We demonstrate this new capability\nfor nonlinear model reduction in the PINNs framework by several nontrivial\nparametric partial differential equations.",
        "pdf_link": "https://arxiv.org/pdf/2403.03459v1.pdf"
    },
    {
        "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
        "authors": [
            "Wenfeng Feng",
            "Chuzhan Hao",
            "Yuewei Zhang",
            "Yu Han",
            "Hao Wang"
        ],
        "published": "2024-03-06T03:33:48Z",
        "summary": "Instruction Tuning has the potential to stimulate or enhance specific\ncapabilities of large language models (LLMs). However, achieving the right\nbalance of data is crucial to prevent catastrophic forgetting and interference\nbetween tasks. To address these limitations and enhance training flexibility,\nwe propose the Mixture-of-LoRAs (MoA) architecture which is a novel and\nparameter-efficient tuning method designed for multi-task learning with LLMs.\nIn this paper, we start by individually training multiple domain-specific LoRA\nmodules using corresponding supervised corpus data. These LoRA modules can be\naligned with the expert design principles observed in Mixture-of-Experts (MoE).\nSubsequently, we combine the multiple LoRAs using an explicit routing strategy\nand introduce domain labels to facilitate multi-task learning, which help\nprevent interference between tasks and ultimately enhances the performance of\neach individual task. Furthermore, each LoRA model can be iteratively adapted\nto a new domain, allowing for quick domain-specific adaptation. Experiments on\ndiverse tasks demonstrate superior and robust performance, which can further\npromote the wide application of domain-specific LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.03432v1.pdf"
    },
    {
        "title": "Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization",
        "authors": [
            "Shitong Duan",
            "Xiaoyuan Yi",
            "Peng Zhang",
            "Tun Lu",
            "Xing Xie",
            "Ning Gu"
        ],
        "published": "2024-03-06T03:02:38Z",
        "summary": "Large language models (LLMs) have revolutionized the role of AI, yet also\npose potential risks of propagating unethical content. Alignment technologies\nhave been introduced to steer LLMs towards human preference, gaining increasing\nattention. Despite notable breakthroughs in this direction, existing methods\nheavily rely on high-quality positive-negative training pairs, suffering from\nnoisy labels and the marginal distinction between preferred and dispreferred\nresponse data. Given recent LLMs' proficiency in generating helpful responses,\nthis work pivots towards a new research focus: achieving alignment using solely\nhuman-annotated negative samples, preserving helpfulness while reducing\nharmfulness. For this purpose, we propose Distributional Dispreference\nOptimization (D$^2$O), which maximizes the discrepancy between the generated\nresponses and the dispreferred ones to effectively eschew harmful information.\nWe theoretically demonstrate that D$^2$O is equivalent to learning a\ndistributional instead of instance-level preference model reflecting human\ndispreference against the distribution of negative responses. Besides, D$^2$O\nintegrates an implicit Jeffrey Divergence regularization to balance the\nexploitation and exploration of reference policies and converges to a\nnon-negative one during training. Extensive experiments demonstrate that our\nmethod achieves comparable generation quality and surpasses the latest\nbaselines in producing less harmful and more informative responses with better\ntraining stability and faster convergence.",
        "pdf_link": "https://arxiv.org/pdf/2403.03419v1.pdf"
    },
    {
        "title": "Human vs. Machine: Language Models and Wargames",
        "authors": [
            "Max Lamparth",
            "Anthony Corso",
            "Jacob Ganz",
            "Oriana Skylar Mastro",
            "Jacquelyn Schneider",
            "Harold Trinkunas"
        ],
        "published": "2024-03-06T02:23:32Z",
        "summary": "Wargames have a long history in the development of military strategy and the\nresponse of nations to threats or attacks. The advent of artificial\nintelligence (AI) promises better decision-making and increased military\neffectiveness. However, there is still debate about how AI systems, especially\nlarge language models (LLMs), behave as compared to humans. To this end, we use\na wargame experiment with 107 national security expert human players designed\nto look at crisis escalation in a fictional US-China scenario and compare human\nplayers to LLM-simulated responses. We find considerable agreement in the LLM\nand human responses but also significant quantitative and qualitative\ndifferences between simulated and human players in the wargame, motivating\ncaution to policymakers before handing over autonomy or following AI-based\nstrategy recommendations.",
        "pdf_link": "https://arxiv.org/pdf/2403.03407v1.pdf"
    },
    {
        "title": "Japanese-English Sentence Translation Exercises Dataset for Automatic Grading",
        "authors": [
            "Naoki Miura",
            "Hiroaki Funayama",
            "Seiya Kikuchi",
            "Yuichiroh Matsubayashi",
            "Yuya Iwase",
            "Kentaro Inui"
        ],
        "published": "2024-03-06T01:37:03Z",
        "summary": "This paper proposes the task of automatic assessment of Sentence Translation\nExercises (STEs), that have been used in the early stage of L2 language\nlearning. We formalize the task as grading student responses for each rubric\ncriterion pre-specified by the educators. We then create a dataset for STE\nbetween Japanese and English including 21 questions, along with a total of 3,\n498 student responses (167 on average). The answer responses were collected\nfrom students and crowd workers. Using this dataset, we demonstrate the\nperformance of baselines including finetuned BERT and GPT models with few-shot\nin-context learning. Experimental results show that the baseline model with\nfinetuned BERT was able to classify correct responses with approximately 90% in\nF1, but only less than 80% for incorrect responses. Furthermore, the GPT models\nwith few-shot learning show poorer results than finetuned BERT, indicating that\nour newly proposed task presents a challenging issue, even for the\nstateof-the-art large language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.03396v1.pdf"
    },
    {
        "title": "Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation",
        "authors": [
            "Tina Vartziotis",
            "Ippolyti Dellatolas",
            "George Dasoulas",
            "Maximilian Schmidt",
            "Florian Schneider",
            "Tim Hoffmann",
            "Sotirios Kotsopoulos",
            "Michael Keckeisen"
        ],
        "published": "2024-03-05T22:12:01Z",
        "summary": "The increasing use of information technology has led to a significant share\nof energy consumption and carbon emissions from data centers. These\ncontributions are expected to rise with the growing demand for big data\nanalytics, increasing digitization, and the development of large artificial\nintelligence (AI) models. The need to address the environmental impact of\nsoftware development has led to increased interest in green (sustainable)\ncoding and claims that the use of AI models can lead to energy efficiency\ngains. Here, we provide an empirical study on green code and an overview of\ngreen coding practices, as well as metrics used to quantify the sustainability\nawareness of AI models. In this framework, we evaluate the sustainability of\nauto-generated code. The auto-generate codes considered in this study are\nproduced by generative commercial AI language models, GitHub Copilot, OpenAI\nChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to\nquantify the sustainability awareness of these AI models, we propose a\ndefinition of the code's \"green capacity\", based on certain sustainability\nmetrics. We compare the performance and green capacity of human-generated code\nand code generated by the three AI language models in response to easy-to-hard\nproblem statements. Our findings shed light on the current capacity of AI\nmodels to contribute to sustainable software development.",
        "pdf_link": "https://arxiv.org/pdf/2403.03344v1.pdf"
    },
    {
        "title": "Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse",
        "authors": [
            "Joseph Gatto",
            "Madhusudan Basak",
            "Yash Srivastava",
            "Philip Bohlman",
            "Sarah M. Preum"
        ],
        "published": "2024-03-05T21:38:19Z",
        "summary": "In this paper, we develop an LLM-powered framework for the curation and\nevaluation of emerging opinion mining in online health communities. We\nformulate emerging opinion mining as a pairwise stance detection problem\nbetween (title, comment) pairs sourced from Reddit, where post titles contain\nemerging health-related claims on a topic that is not predefined. The claims\nare either explicitly or implicitly expressed by the user. We detail (i) a\nmethod of claim identification -- the task of identifying if a post title\ncontains a claim and (ii) an opinion mining-driven evaluation framework for\nstance detection using LLMs.\n  We facilitate our exploration by releasing a novel test dataset, Long\nCOVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of\nclaim identification and stance detection in online health communities. Long\nCovid is an emerging post-COVID disorder with uncertain and complex treatment\nguidelines, thus making it a suitable use case for our task. LC-Stance contains\nlong COVID treatment related discourse sourced from a Reddit community. Our\nevaluation shows that GPT-4 significantly outperforms prior works on zero-shot\nstance detection. We then perform thorough LLM model diagnostics, identifying\nthe role of claim type (i.e. implicit vs explicit claims) and comment length as\nsources of model error.",
        "pdf_link": "https://arxiv.org/pdf/2403.03336v1.pdf"
    },
    {
        "title": "Guardrail Baselines for Unlearning in LLMs",
        "authors": [
            "Pratiksha Thaker",
            "Yash Maurya",
            "Virginia Smith"
        ],
        "published": "2024-03-05T21:19:06Z",
        "summary": "Recent work has demonstrated that fine-tuning is a promising approach to\n`unlearn' concepts from large language models. However, fine-tuning can be\nexpensive, as it requires both generating a set of examples and running\niterations of fine-tuning to update the model. In this work, we show that\nsimple guardrail-based approaches such as prompting and filtering can achieve\nunlearning results comparable to fine-tuning. We recommend that researchers\ninvestigate these lightweight baselines when evaluating the performance of more\ncomputationally intensive fine-tuning methods. While we do not claim that\nmethods such as prompting or filtering are universal solutions to the problem\nof unlearning, our work suggests the need for evaluation metrics that can\nbetter separate the power of guardrails vs. fine-tuning, and highlights\nscenarios where guardrails themselves may be advantageous for unlearning, such\nas in generating examples for fine-tuning or unlearning when only API access is\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2403.03329v1.pdf"
    },
    {
        "title": "Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data",
        "authors": [
            "Joseph Gatto",
            "Parker Seegmiller",
            "Omar Sharif",
            "Sarah M. Preum"
        ],
        "published": "2024-03-05T20:07:42Z",
        "summary": "Document-Level Event Argument Extraction (DocEAE) is an extremely difficult\ninformation extraction problem -- with significant limitations in low-resource\ncross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA),\na novel generative DocEAE data augmentation framework. Our approach leverages\nthe intuition that Mad Libs, which are categorically masked documents used as a\npart of a popular game, can be generated and solved by LLMs to produce data for\nDocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1\nscore. Moreover, this approach achieves a 3.9 and 5.2 point average increase in\nzero and few-shot event roles compared to augmentation-free baselines across\nall experiments.\n  To better facilitate analysis of cross-domain DocEAE, we additionally\nintroduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to\nidentify roles in the target domain which are semantic outliers with respect to\nroles observed in the source domain. Our experiments show that MLA augmentation\ncan boost RDF1 performance by an average of 5.85 points compared to\nnon-augmented datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.03304v1.pdf"
    },
    {
        "title": "Should We Fear Large Language Models? A Structural Analysis of the Human Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens of Heidegger's Philosophy",
        "authors": [
            "Jianqiiu Zhang"
        ],
        "published": "2024-03-05T19:40:53Z",
        "summary": "In the rapidly evolving field of Large Language Models (LLMs), there is a\ncritical need to thoroughly analyze their capabilities and risks. Central to\nour investigation are two novel elements. Firstly, it is the innovative\nparallels between the statistical patterns of word relationships within LLMs\nand Martin Heidegger's concepts of \"ready-to-hand\" and \"present-at-hand,\" which\nencapsulate the utilitarian and scientific altitudes humans employ in\ninteracting with the world. This comparison lays the groundwork for positioning\nLLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding\nlight on their capacity to emulate certain facets of human reasoning. Secondly,\na structural analysis of human reasoning, viewed through Heidegger's notion of\ntruth as \"unconcealment\" is conducted This foundational principle enables us to\nmap out the inputs and outputs of the reasoning system and divide reasoning\ninto four distinct categories. Respective cognitive faculties are delineated,\nallowing us to place LLMs within the broader schema of human reasoning, thus\nclarifying their strengths and inherent limitations. Our findings reveal that\nwhile LLMs possess the capability for Direct Explicative Reasoning and Pseudo\nRational Reasoning, they fall short in authentic rational reasoning and have no\ncreative reasoning capabilities, due to the current lack of many analogous AI\nmodels such as the Faculty of Judgement. The potential and risks of LLMs when\nthey are augmented with other AI technologies are also evaluated. The results\nindicate that although LLMs have achieved proficiency in some reasoning\nabilities, the aspiration to match or exceed human intellectual capabilities is\nyet unattained. This research not only enriches our comprehension of LLMs but\nalso propels forward the discourse on AI's potential and its bounds, paving the\nway for future explorations into AI's evolving landscape.",
        "pdf_link": "https://arxiv.org/pdf/2403.03288v1.pdf"
    },
    {
        "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs",
        "authors": [
            "Aly M. Kassem",
            "Omar Mahmoud",
            "Niloofar Mireshghallah",
            "Hyunwoo Kim",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "Sherif Saad",
            "Santu Rana"
        ],
        "published": "2024-03-05T19:32:01Z",
        "summary": "In this paper, we introduce a black-box prompt optimization method that uses\nan attacker LLM agent to uncover higher levels of memorization in a victim\nagent, compared to what is revealed by prompting the target model with the\ntraining data directly, which is the dominant approach of quantifying\nmemorization in LLMs. We use an iterative rejection-sampling optimization\nprocess to find instruction-based prompts with two main characteristics: (1)\nminimal overlap with the training data to avoid presenting the solution\ndirectly to the model, and (2) maximal overlap between the victim model's\noutput and the training data, aiming to induce the victim to spit out training\ndata. We observe that our instruction-based prompts generate outputs with 23.7%\nhigher overlap with training data compared to the baseline prefix-suffix\nmeasurements. Our findings show that (1) instruction-tuned models can expose\npre-training data as much as their base-models, if not more so, (2) contexts\nother than the original training data can lead to leakage, and (3) using\ninstructions proposed by other LLMs can open a new avenue of automated attacks\nthat we should further study and explore. The code can be found at\nhttps://github.com/Alymostafa/Instruction_based_attack .",
        "pdf_link": "https://arxiv.org/pdf/2403.04801v2.pdf"
    },
    {
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "authors": [
            "Nathaniel Li",
            "Alexander Pan",
            "Anjali Gopal",
            "Summer Yue",
            "Daniel Berrios",
            "Alice Gatti",
            "Justin D. Li",
            "Ann-Kathrin Dombrowski",
            "Shashwat Goel",
            "Long Phan",
            "Gabriel Mukobi",
            "Nathan Helm-Burger",
            "Rassin Lababidi",
            "Lennart Justen",
            "Andrew B. Liu",
            "Michael Chen",
            "Isabelle Barrass",
            "Oliver Zhang",
            "Xiaoyuan Zhu",
            "Rishub Tamirisa",
            "Bhrugu Bharathi",
            "Adam Khoja",
            "Zhenqi Zhao",
            "Ariel Herbert-Voss",
            "Cort B. Breuer",
            "Andy Zou",
            "Mantas Mazeika",
            "Zifan Wang",
            "Palash Oswal",
            "Weiran Liu",
            "Adam A. Hunt",
            "Justin Tienken-Harder",
            "Kevin Y. Shih",
            "Kemper Talley",
            "John Guan",
            "Russell Kaplan",
            "Ian Steneker",
            "David Campbell",
            "Brad Jokubaitis",
            "Alex Levinson",
            "Jean Wang",
            "William Qian",
            "Kallol Krishna Karmakar",
            "Steven Basart",
            "Stephen Fitz",
            "Mindy Levine",
            "Ponnurangam Kumaraguru",
            "Uday Tupakula",
            "Vijay Varadharajan",
            "Yan Shoshitaishvili",
            "Jimmy Ba",
            "Kevin M. Esvelt",
            "Alexandr Wang",
            "Dan Hendrycks"
        ],
        "published": "2024-03-05T18:59:35Z",
        "summary": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 4,157 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop CUT, a\nstate-of-the-art unlearning method based on controlling model representations.\nCUT reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai",
        "pdf_link": "https://arxiv.org/pdf/2403.03218v2.pdf"
    },
    {
        "title": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
        "authors": [
            "Savitha Sam Abraham",
            "Marjan Alirezaie",
            "Luc De Raedt"
        ],
        "published": "2024-03-05T18:41:37Z",
        "summary": "The integration of learning and reasoning is high on the research agenda in\nAI. Nevertheless, there is only a little attention to use existing background\nknowledge for reasoning about partially observed scenes to answer questions\nabout the scene. Yet, we as humans use such knowledge frequently to infer\nplausible answers to visual questions (by eliminating all inconsistent ones).\nSuch knowledge often comes in the form of constraints about objects and it\ntends to be highly domain or environment-specific. We contribute a novel\nbenchmark called CLEVR-POC for reasoning-intensive visual question answering\n(VQA) in partially observable environments under constraints. In CLEVR-POC,\nknowledge in the form of logical constraints needs to be leveraged to generate\nplausible answers to questions about a hidden object in a given partial scene.\nFor instance, if one has the knowledge that all cups are colored either red,\ngreen or blue and that there is only one green cup, it becomes possible to\ndeduce the color of an occluded cup as either red or blue, provided that all\nother cups, including the green one, are observed. Through experiments, we\nobserve that the low performance of pre-trained vision language models like\nCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC\nascertains the necessity for frameworks that can handle reasoning-intensive\ntasks where environment-specific background knowledge is available and crucial.\nFurthermore, our demonstration illustrates that a neuro-symbolic model, which\nintegrates an LLM like GPT-4 with a visual perception network and a formal\nlogical reasoner, exhibits exceptional performance on CLEVR-POC.",
        "pdf_link": "https://arxiv.org/pdf/2403.03203v1.pdf"
    },
    {
        "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets",
        "authors": [
            "Hossein Aboutalebi",
            "Hwanjun Song",
            "Yusheng Xie",
            "Arshit Gupta",
            "Justin Sun",
            "Hang Su",
            "Igor Shalyminov",
            "Nikolaos Pappas",
            "Siffi Singh",
            "Saab Mansour"
        ],
        "published": "2024-03-05T18:31:28Z",
        "summary": "Development of multimodal interactive systems is hindered by the lack of\nrich, multimodal (text, images) conversational data, which is needed in large\nquantities for LLMs. Previous approaches augment textual dialogues with\nretrieved images, posing privacy, diversity, and quality constraints. In this\nwork, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative\n\\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only\ndialogues with diverse and high-quality images. Subsequently, a diffusion model\nis applied to craft corresponding images, ensuring alignment with the\nidentified text. Finally, MAGID incorporates an innovative feedback loop\nbetween an image description generation module (textual LLM) and image quality\nmodules (addressing aesthetics, image-text matching, and safety), that work in\ntandem to generate high-quality and multi-modal dialogues. We compare MAGID to\nother SOTA baselines on three dialogue datasets, using automated and human\nevaluation. Our results show that MAGID is comparable to or better than\nbaselines, with significant improvements in human evaluation, especially\nagainst retrieval baselines where the image database is small.",
        "pdf_link": "https://arxiv.org/pdf/2403.03194v1.pdf"
    },
    {
        "title": "Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement",
        "authors": [
            "Rafaela Martelo",
            "Ruo-Qian Wang"
        ],
        "published": "2024-03-05T18:24:52Z",
        "summary": "Real-time flood forecasting plays a crucial role in enabling timely and\neffective emergency responses. However, a significant challenge lies in\nbridging the gap between complex numerical flood models and practical\ndecision-making. Decision-makers often rely on experts to interpret these\nmodels for optimizing flood mitigation strategies. And the public requires\ncomplex techniques to inquiry and understand socio-cultural and institutional\nfactors, often hinders the public's understanding of flood risks. To overcome\nthese challenges, our study introduces an innovative solution: a customized AI\nAssistant powered by the GPT-4 Large Language Model. This AI Assistant is\ndesigned to facilitate effective communication between decision-makers, the\ngeneral public, and flood forecasters, without the requirement of specialized\nknowledge. The new framework utilizes GPT-4's advanced natural language\nunderstanding and function calling capabilities to provide immediate flood\nalerts and respond to various flood-related inquiries. Our developed prototype\nintegrates real-time flood warnings with flood maps and social vulnerability\ndata. It also effectively translates complex flood zone information into\nactionable risk management advice. To assess its performance, we evaluated the\nprototype using six criteria within three main categories: relevance, error\nresilience, and understanding of context. Our research marks a significant step\ntowards a more accessible and user-friendly approach in flood risk management.\nThis study highlights the potential of advanced AI tools like GPT-4 in\ndemocratizing information and enhancing public engagement in critical social\nand environmental issues.",
        "pdf_link": "https://arxiv.org/pdf/2403.03188v1.pdf"
    },
    {
        "title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection",
        "authors": [
            "Peng Qi",
            "Zehong Yan",
            "Wynne Hsu",
            "Mong Li Lee"
        ],
        "published": "2024-03-05T18:04:59Z",
        "summary": "Misinformation is a prevalent societal issue due to its potential high risks.\nOut-of-context (OOC) misinformation, where authentic images are repurposed with\nfalse text, is one of the easiest and most effective ways to mislead audiences.\nCurrent methods focus on assessing image-text consistency but lack convincing\nexplanations for their judgments, which is essential for debunking\nmisinformation. While Multimodal Large Language Models (MLLMs) have rich\nknowledge and innate capability for visual reasoning and explanation\ngeneration, they still lack sophistication in understanding and discovering the\nsubtle crossmodal differences. In this paper, we introduce SNIFFER, a novel\nmultimodal large language model specifically engineered for OOC misinformation\ndetection and explanation. SNIFFER employs two-stage instruction tuning on\nInstructBLIP. The first stage refines the model's concept alignment of generic\nobjects with news-domain entities and the second stage leverages language-only\nGPT-4 generated OOC-specific instruction data to fine-tune the model's\ndiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER not\nonly detects inconsistencies between text and image but also utilizes external\nknowledge for contextual verification. Our experiments show that SNIFFER\nsurpasses the original MLLM by over 40% and outperforms state-of-the-art\nmethods in detection accuracy. SNIFFER also provides accurate and persuasive\nexplanations as validated by quantitative and human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2403.03170v1.pdf"
    },
    {
        "title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset",
        "authors": [
            "Arda Uzunoglu",
            "Abdalfatah Rashid Safa",
            "G\u00f6zde G\u00fcl \u015eahin"
        ],
        "published": "2024-03-05T18:01:59Z",
        "summary": "Recently, there has been growing interest within the community regarding\nwhether large language models are capable of planning or executing plans.\nHowever, most prior studies use LLMs to generate high-level plans for\nsimplified scenarios lacking linguistic complexity and domain diversity,\nlimiting analysis of their planning abilities. These setups constrain\nevaluation methods (e.g., predefined action space), architectural choices\n(e.g., only generative models), and overlook the linguistic nuances essential\nfor realistic analysis. To tackle this, we present PARADISE, an abductive\nreasoning task using Q\\&A format on practical procedural text sourced from\nwikiHow. It involves warning and tip inference tasks directly associated with\ngoals, excluding intermediary steps, with the aim of testing the ability of the\nmodels to infer implicit knowledge of the plan solely from the given goal. Our\nexperiments, utilizing fine-tuned language models and zero-shot prompting,\nreveal the effectiveness of task-specific small models over large language\nmodels in most scenarios. Despite advancements, all models fall short of human\nperformance. Notably, our analysis uncovers intriguing insights, such as\nvariations in model behavior with dropped keywords, struggles of BERT-family\nand GPT-4 with physical and abstract goals, and the proposed tasks offering\nvaluable prior knowledge for other unseen procedural tasks. The PARADISE\ndataset and associated resources are publicly available for further research\nexploration with https://github.com/GGLAB-KU/paradise.",
        "pdf_link": "https://arxiv.org/pdf/2403.03167v2.pdf"
    },
    {
        "title": "Language Guided Exploration for RL Agents in Text Environments",
        "authors": [
            "Hitesh Golchha",
            "Sahil Yerawar",
            "Dhruvesh Patel",
            "Soham Dan",
            "Keerthiram Murugesan"
        ],
        "published": "2024-03-05T17:26:41Z",
        "summary": "Real-world sequential decision making is characterized by sparse rewards and\nlarge decision spaces, posing significant difficulty for experiential learning\nsystems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large\nLanguage Models (LLMs), with a wealth of world knowledge, can help RL agents\nlearn quickly and adapt to distribution shifts. In this work, we introduce\nLanguage Guided Exploration (LGE) framework, which uses a pre-trained language\nmodel (called GUIDE ) to provide decision-level guidance to an RL agent (called\nEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging\ntext environment, LGE outperforms vanilla RL agents significantly and also\noutperforms other sophisticated methods like Behaviour Cloning and Text\nDecision Transformer.",
        "pdf_link": "https://arxiv.org/pdf/2403.03141v1.pdf"
    },
    {
        "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",
        "authors": [
            "Flor Miriam Plaza-del-Arco",
            "Amanda Cercas Curry",
            "Alba Curry",
            "Gavin Abercrombie",
            "Dirk Hovy"
        ],
        "published": "2024-03-05T17:04:05Z",
        "summary": "Large language models (LLMs) reflect societal norms and biases, especially\nabout gender. While societal biases and stereotypes have been extensively\nresearched in various NLP applications, there is a surprising gap for emotion\nanalysis. However, emotion and gender are closely linked in societal discourse.\nE.g., women are often thought of as more empathetic, while men's anger is more\nsocially accepted. To fill this gap, we present the first comprehensive study\nof gendered emotion attribution in five state-of-the-art LLMs (open- and\nclosed-source). We investigate whether emotions are gendered, and whether these\nvariations are based on societal stereotypes. We prompt the models to adopt a\ngendered persona and attribute emotions to an event like 'When I had a serious\nargument with a dear person'. We then analyze the emotions generated by the\nmodels in relation to the gender-event pairs. We find that all models\nconsistently exhibit gendered emotions, influenced by gender stereotypes. These\nfindings are in line with established research in psychology and gender\nstudies. Our study sheds light on the complex societal interplay between\nlanguage, gender, and emotion. The reproduction of emotion stereotypes in LLMs\nallows us to use those models to study the topic in detail, but raises\nquestions about the predictive use of those same LLMs for emotion applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.03121v2.pdf"
    },
    {
        "title": "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents",
        "authors": [
            "Yuqi Zhu",
            "Shuofei Qiao",
            "Yixin Ou",
            "Shumin Deng",
            "Ningyu Zhang",
            "Shiwei Lyu",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "published": "2024-03-05T16:39:12Z",
        "summary": "Large Language Models (LLMs) have demonstrated great potential in complex\nreasoning tasks, yet they fall short when tackling more sophisticated\nchallenges, especially when interacting with environments through generating\nexecutable actions. This inadequacy primarily stems from the lack of built-in\naction knowledge in language agents, which fails to effectively guide the\nplanning trajectories during task solving and results in planning\nhallucination. To address this issue, we introduce KnowAgent, a novel approach\ndesigned to enhance the planning capabilities of LLMs by incorporating explicit\naction knowledge. Specifically, KnowAgent employs an action knowledge base and\na knowledgeable self-learning strategy to constrain the action path during\nplanning, enabling more reasonable trajectory synthesis, and thereby enhancing\nthe planning performance of language agents. Experimental results on HotpotQA\nand ALFWorld based on various backbone models demonstrate that KnowAgent can\nachieve comparable or superior performance to existing baselines. Further\nanalysis indicates the effectiveness of KnowAgent in terms of planning\nhallucinations mitigation. Code is available in\nhttps://github.com/zjunlp/KnowAgent.",
        "pdf_link": "https://arxiv.org/pdf/2403.03101v1.pdf"
    },
    {
        "title": "Learning to Use Tools via Cooperative and Interactive Agents",
        "authors": [
            "Zhengliang Shi",
            "Shen Gao",
            "Xiuyi Chen",
            "Lingyong Yan",
            "Haibo Shi",
            "Dawei Yin",
            "Zhumin Chen",
            "Pengjie Ren",
            "Suzan Verberne",
            "Zhaochun Ren"
        ],
        "published": "2024-03-05T15:08:16Z",
        "summary": "Tool learning empowers large language models (LLMs) as agents to use external\ntools to extend their capability. Existing methods employ one single LLM-based\nagent to iteratively select and execute tools, thereafter incorporating the\nresult into the next action prediction. However, they still suffer from\npotential performance degradation when addressing complex tasks due to: (1) the\nlimitation of the inherent capability of a single LLM to perform diverse\nactions, and (2) the struggle to adaptively correct mistakes when the task\nfails. To mitigate these problems, we propose the ConAgents, a Cooperative and\ninteractive Agents framework, which modularizes the workflow of tool learning\ninto Grounding, Execution, and Observing agents. We also introduce an iterative\ncalibration (IterCali) method, enabling the agents to adapt themselves based on\nthe feedback from the tool environment. Experiments conducted on three datasets\ndemonstrate the superiority of our ConAgents (e.g., 6 point improvement over\nthe SOTA baseline). We further provide fine-granularity analysis for the\nefficiency and consistency of our framework.",
        "pdf_link": "https://arxiv.org/pdf/2403.03031v1.pdf"
    },
    {
        "title": "Word Importance Explains How Prompts Affect Language Model Outputs",
        "authors": [
            "Stefan Hackmann",
            "Haniyeh Mahmoudian",
            "Mark Steadman",
            "Michael Schmidt"
        ],
        "published": "2024-03-05T15:04:18Z",
        "summary": "The emergence of large language models (LLMs) has revolutionized numerous\napplications across industries. However, their \"black box\" nature often hinders\nthe understanding of how they make specific decisions, raising concerns about\ntheir transparency, reliability, and ethical use. This study presents a method\nto improve the explainability of LLMs by varying individual words in prompts to\nuncover their statistical impact on the model outputs. This approach, inspired\nby permutation importance for tabular data, masks each word in the system\nprompt and evaluates its effect on the outputs based on the available text\nscores aggregated over multiple user inputs. Unlike classical attention, word\nimportance measures the impact of prompt words on arbitrarily-defined text\nscores, which enables decomposing the importance of words into the specific\nmeasures of interest--including bias, reading level, verbosity, etc. This\nprocedure also enables measuring impact when attention weights are not\navailable. To test the fidelity of this approach, we explore the effect of\nadding different suffixes to multiple different system prompts and comparing\nsubsequent generations with different large language models. Results show that\nword importance scores are closely related to the expected suffix importances\nfor multiple scoring functions.",
        "pdf_link": "https://arxiv.org/pdf/2403.03028v1.pdf"
    },
    {
        "title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following",
        "authors": [
            "Haochen Shi",
            "Zhiyuan Sun",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Bang Liu"
        ],
        "published": "2024-03-05T14:53:53Z",
        "summary": "Embodied Instruction Following (EIF) is a crucial task in embodied learning,\nrequiring agents to interact with their environment through egocentric\nobservations to fulfill natural language instructions. Recent advancements have\nseen a surge in employing large language models (LLMs) within a\nframework-centric approach to enhance performance in embodied learning tasks,\nincluding EIF. Despite these efforts, there exists a lack of a unified\nunderstanding regarding the impact of various components-ranging from visual\nperception to action execution-on task performance. To address this gap, we\nintroduce OPEx, a comprehensive framework that delineates the core components\nessential for solving embodied learning tasks: Observer, Planner, and Executor.\nThrough extensive evaluations, we provide a deep analysis of how each component\ninfluences EIF task performance. Furthermore, we innovate within this space by\ndeploying a multi-agent dialogue strategy on a TextWorld counterpart, further\nenhancing task performance. Our findings reveal that LLM-centric design\nmarkedly improves EIF outcomes, identify visual perception and low-level action\nexecution as critical bottlenecks, and demonstrate that augmenting LLMs with a\nmulti-agent framework further elevates performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.03017v1.pdf"
    },
    {
        "title": "Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations",
        "authors": [
            "Hasan Abu-Rasheed",
            "Christian Weber",
            "Madjid Fathi"
        ],
        "published": "2024-03-05T14:41:12Z",
        "summary": "In the era of personalized education, the provision of comprehensible\nexplanations for learning recommendations is of a great value to enhance the\nlearner's understanding and engagement with the recommended learning content.\nLarge language models (LLMs) and generative AI in general have recently opened\nnew doors for generating human-like explanations, for and along learning\nrecommendations. However, their precision is still far away from acceptable in\na sensitive field like education. To harness the abilities of LLMs, while still\nensuring a high level of precision towards the intent of the learners, this\npaper proposes an approach to utilize knowledge graphs (KG) as a source of\nfactual context, for LLM prompts, reducing the risk of model hallucinations,\nand safeguarding against wrong or imprecise information, while maintaining an\napplication-intended learning context. We utilize the semantic relations in the\nknowledge graph to offer curated knowledge about learning recommendations. With\ndomain-experts in the loop, we design the explanation as a textual template,\nwhich is filled and completed by the LLM. Domain experts were integrated in the\nprompt engineering phase as part of a study, to ensure that explanations\ninclude information that is relevant to the learner. We evaluate our approach\nquantitatively using Rouge-N and Rouge-L measures, as well as qualitatively\nwith experts and learners. Our results show an enhanced recall and precision of\nthe generated explanations compared to those generated solely by the GPT model,\nwith a greatly reduced risk of generating imprecise information in the final\nlearning explanation.",
        "pdf_link": "https://arxiv.org/pdf/2403.03008v1.pdf"
    },
    {
        "title": "Localized Zeroth-Order Prompt Optimization",
        "authors": [
            "Wenyang Hu",
            "Yao Shu",
            "Zongmin Yu",
            "Zhaoxuan Wu",
            "Xiangqiang Lin",
            "Zhongxiang Dai",
            "See-Kiong Ng",
            "Bryan Kian Hsiang Low"
        ],
        "published": "2024-03-05T14:18:15Z",
        "summary": "The efficacy of large language models (LLMs) in understanding and generating\nnatural language has aroused a wide interest in developing prompt-based methods\nto harness the power of black-box LLMs. Existing methodologies usually\nprioritize a global optimization for finding the global optimum, which however\nwill perform poorly in certain tasks. This thus motivates us to re-think the\nnecessity of finding a global optimum in prompt optimization. To answer this,\nwe conduct a thorough empirical study on prompt optimization and draw two major\ninsights. Contrasting with the rarity of global optimum, local optima are\nusually prevalent and well-performed, which can be more worthwhile for\nefficient prompt optimization (Insight I). The choice of the input domain,\ncovering both the generation and the representation of prompts, affects the\nidentification of well-performing local optima (Insight II). Inspired by these\ninsights, we propose a novel algorithm, namely localized zeroth-order prompt\noptimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived\nGaussian process into standard zeroth-order optimization for an efficient\nsearch of well-performing local optima in prompt optimization. Remarkably, ZOPO\noutperforms existing baselines in terms of both the optimization performance\nand the query efficiency, which we demonstrate through extensive experiments.",
        "pdf_link": "https://arxiv.org/pdf/2403.02993v1.pdf"
    },
    {
        "title": "Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges",
        "authors": [
            "Bosheng Ding",
            "Chengwei Qin",
            "Ruochen Zhao",
            "Tianze Luo",
            "Xinze Li",
            "Guizhen Chen",
            "Wenhan Xia",
            "Junjie Hu",
            "Anh Tuan Luu",
            "Shafiq Joty"
        ],
        "published": "2024-03-05T14:11:54Z",
        "summary": "In the rapidly evolving field of machine learning (ML), data augmentation\n(DA) has emerged as a pivotal technique for enhancing model performance by\ndiversifying training examples without the need for additional data collection.\nThis survey explores the transformative impact of Large Language Models (LLMs)\non DA, particularly addressing the unique challenges and opportunities they\npresent in the context of natural language processing (NLP) and beyond. From a\ndata perspective and a learning perspective, we examine various strategies that\nutilize Large Language Models for data augmentation, including a novel\nexploration of learning paradigms where LLM-generated data is used for further\ntraining. Additionally, this paper delineates the primary challenges faced in\nthis domain, ranging from controllable data augmentation to multi modal data\naugmentation. This survey highlights the paradigm shift introduced by LLMs in\nDA, aims to serve as a foundational guide for researchers and practitioners in\nthis field.",
        "pdf_link": "https://arxiv.org/pdf/2403.02990v1.pdf"
    },
    {
        "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering",
        "authors": [
            "Sungho Ko",
            "Hyunjin Cho",
            "Hyungjoo Chae",
            "Jinyoung Yeo",
            "Dongha Lee"
        ],
        "published": "2024-03-05T13:43:58Z",
        "summary": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.",
        "pdf_link": "https://arxiv.org/pdf/2403.02966v1.pdf"
    },
    {
        "title": "Multi-Scale Protein Language Model for Unified Molecular Modeling",
        "authors": [
            "Kangjie Zheng",
            "Siyu Long",
            "Tianyu Lu",
            "Junwei Yang",
            "Xinyu Dai",
            "Ming Zhang",
            "Zaiqing Nie",
            "Wei-Ying Ma",
            "Hao Zhou"
        ],
        "published": "2024-03-05T13:35:41Z",
        "summary": "Protein language models have demonstrated significant potential in the field\nof protein engineering. However, current protein language models primarily\noperate at the residue scale, which limits their ability to provide information\nat the atom level. This limitation prevents us from fully exploiting the\ncapabilities of protein language models for applications involving both\nproteins and small molecules. In this paper, we propose ms-ESM (multi-scale\nESM), a novel approach that enables multi-scale unified molecular modeling.\nms-ESM achieves this by pre-training on multi-scale code-switch protein\nsequences and utilizing a multi-scale position encoding to capture\nrelationships among residues and atoms. Experimental results indicate that\nms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the\nfull utilization of protein language models. Further investigations reveal that\nthrough unified molecular modeling, ms-ESM not only gains molecular knowledge\nbut also retains its understanding of proteins.",
        "pdf_link": "https://arxiv.org/pdf/2403.12995v1.pdf"
    },
    {
        "title": "WikiTableEdit: A Benchmark for Table Editing by Natural Language Instruction",
        "authors": [
            "Zheng Li",
            "Xiang Chen",
            "Xiaojun Wan"
        ],
        "published": "2024-03-05T13:33:12Z",
        "summary": "Tabular data, as a crucial form of data representation, exists in diverse\nformats on the Web. When confronted with complex and irregular tables, manual\nmodification becomes a laborious task. This paper investigates the performance\nof Large Language Models (LLMs) in the context of table editing tasks. Existing\nresearch mainly focuses on regular-shaped tables, wherein instructions are used\nto generate code in SQL, Python, or Excel Office-script for manipulating the\ntables. Nevertheless, editing tables with irregular structures, particularly\nthose containing merged cells spanning multiple rows, poses a challenge when\nusing code. To address this, we introduce the WikiTableEdit dataset. Leveraging\n26,531 tables from the WikiSQL dataset, we automatically generate natural\nlanguage instructions for six distinct basic operations and the corresponding\noutcomes, resulting in over 200,000 instances. Subsequently, we evaluate\nseveral representative large language models on the WikiTableEdit dataset to\ndemonstrate the challenge of this task. The dataset will be released to the\ncommunity to promote related researches.",
        "pdf_link": "https://arxiv.org/pdf/2403.02962v1.pdf"
    },
    {
        "title": "Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation",
        "authors": [
            "Bin Zhang",
            "Yuxiao Ye",
            "Guoqing Du",
            "Xiaoru Hu",
            "Zhishuai Li",
            "Sun Yang",
            "Chi Harold Liu",
            "Rui Zhao",
            "Ziyue Li",
            "Hangyu Mao"
        ],
        "published": "2024-03-05T13:23:48Z",
        "summary": "Large Language Models (LLMs) have emerged as a powerful tool in advancing the\nText-to-SQL task, significantly outperforming traditional methods.\nNevertheless, as a nascent research field, there is still no consensus on the\noptimal prompt templates and design frameworks. Additionally, existing\nbenchmarks inadequately explore the performance of LLMs across the various\nsub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs'\ncognitive capabilities and the optimization of LLM-based solutions. To address\nthe aforementioned issues, we firstly construct a new dataset designed to\nmitigate the risk of overfitting in LLMs. Then we formulate five evaluation\ntasks to comprehensively assess the performance of diverse methods across\nvarious LLMs throughout the Text-to-SQL process.Our study highlights the\nperformance disparities among LLMs and proposes optimal in-context learning\nsolutions tailored to each task. These findings offer valuable insights for\nenhancing the development of LLM-based Text-to-SQL systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.02951v2.pdf"
    },
    {
        "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
        "authors": [
            "Xijia Tao",
            "Shuai Zhong",
            "Lei Li",
            "Qi Liu",
            "Lingpeng Kong"
        ],
        "published": "2024-03-05T12:21:57Z",
        "summary": "There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.02910v2.pdf"
    },
    {
        "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods",
        "authors": [
            "Hanlei Jin",
            "Yang Zhang",
            "Dan Meng",
            "Jun Wang",
            "Jinghua Tan"
        ],
        "published": "2024-03-05T12:11:07Z",
        "summary": "Automatic Text Summarization (ATS), utilizing Natural Language Processing\n(NLP) algorithms, aims to create concise and accurate summaries, thereby\nsignificantly reducing the human effort required in processing large volumes of\ntext. ATS has drawn considerable interest in both academic and industrial\ncircles. Many studies have been conducted in the past to survey ATS methods;\nhowever, they generally lack practicality for real-world implementations, as\nthey often categorize previous methods from a theoretical standpoint. Moreover,\nthe advent of Large Language Models (LLMs) has altered conventional ATS\nmethods. In this survey, we aim to 1) provide a comprehensive overview of ATS\nfrom a ``Process-Oriented Schema'' perspective, which is best aligned with\nreal-world implementations; 2) comprehensively review the latest LLM-based ATS\nworks; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in\nthe literature. To the best of our knowledge, this is the first survey to\nspecifically investigate LLM-based ATS methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.02901v1.pdf"
    },
    {
        "title": "In Search of Truth: An Interrogation Approach to Hallucination Detection",
        "authors": [
            "Yakir Yehuda",
            "Itzik Malkiel",
            "Oren Barkan",
            "Jonathan Weill",
            "Royi Ronen",
            "Noam Koenigstein"
        ],
        "published": "2024-03-05T11:50:01Z",
        "summary": "Despite the many advances of Large Language Models (LLMs) and their\nunprecedented rapid evolution, their impact and integration into every facet of\nour daily lives is limited due to various reasons. One critical factor\nhindering their widespread adoption is the occurrence of hallucinations, where\nLLMs invent answers that sound realistic, yet drift away from factual truth. In\nthis paper, we present a novel method for detecting hallucinations in large\nlanguage models, which tackles a critical issue in the adoption of these models\nin various real-world scenarios. Through extensive evaluations across multiple\ndatasets and LLMs, including Llama-2, we study the hallucination levels of\nvarious recent LLMs and demonstrate the effectiveness of our method to\nautomatically detect them. Notably, we observe up to 62% hallucinations for\nLlama-2 in a specific experiment, where our method achieves a Balanced Accuracy\n(B-ACC) of 87%, all without relying on external knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2403.02889v2.pdf"
    },
    {
        "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
        "authors": [
            "Zhengyang Tang",
            "Xingxing Zhang",
            "Benyou Wang",
            "Furu Wei"
        ],
        "published": "2024-03-05T11:42:59Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nproblem-solving. However, their proficiency in solving mathematical problems\nremains inadequate. We propose MathScale, a simple and scalable method to\ncreate high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt\nGPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning,\nit first extracts topics and knowledge points from seed math questions and then\nbuild a concept graph, which is subsequently used to generate new math\nquestions. MathScale exhibits effective scalability along the size axis of the\nmath dataset that we generate. As a result, we create a mathematical reasoning\ndataset (MathScaleQA) containing two million math question-answer pairs. To\nevaluate mathematical reasoning abilities of LLMs comprehensively, we construct\n{\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten\ndatasets (including GSM8K and MATH) covering K-12, college, and competition\nlevel math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g.,\nLLaMA-2 and Mistral), resulting in significantly improved capabilities in\nmathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves\nstate-of-the-art performance across all datasets, surpassing its best peers of\nequivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average\naccuracy, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.02884v1.pdf"
    },
    {
        "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers",
        "authors": [
            "Hui Huang",
            "Yingqi Qu",
            "Jing Liu",
            "Muyun Yang",
            "Tiejun Zhao"
        ],
        "published": "2024-03-05T10:20:52Z",
        "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. In this study, we conduct an empirical study of\ndifferent judge models on their evaluation capability. Our findings indicate\nthat although the fine-tuned judge models achieve high accuracy on in-domain\ntest sets, even surpassing GPT4, they are inherently task-specific classifiers,\nand their generalizability and fairness severely underperform GPT4.",
        "pdf_link": "https://arxiv.org/pdf/2403.02839v1.pdf"
    },
    {
        "title": "DPPA: Pruning Method for Large Language Model to Model Merging",
        "authors": [
            "Yaochen Zhu",
            "Rui Xia",
            "Jiajun Zhang"
        ],
        "published": "2024-03-05T09:12:49Z",
        "summary": "Model merging is to combine fine-tuned models derived from multiple domains,\nwith the intent of enhancing the model's proficiency across various domains.\nThe principal concern is the resolution of parameter conflicts. A substantial\namount of existing research remedy this issue during the merging stage, with\nthe latest study focusing on resolving this issue throughout the pruning stage.\nThe DARE approach has exhibited promising outcomes when applied to a simplistic\nfine-tuned model. However, the efficacy of this method tends to wane when\nemployed on complex fine-tuned models that show a significant parameter bias\nrelative to the baseline model. In this paper, we introduce a dual-stage method\ntermed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the\nchallenge of merging complex fine-tuned models. Initially, we introduce\nDynamically Pruning (DP), an improved approach based on magnitude pruning,\nwhich aim is to enhance performance at higher pruning rates. Subsequently, we\npropose Dynamically Partition Amplification (DPA), a rescaling strategy, is\ndesigned to dynamically amplify parameter partitions in relation to their\nsignificance levels. The experimental results show that our method maintains a\nmere 20% of domain-specific parameters and yet delivers a performance\ncomparable to other methodologies that preserve up to 90% of parameters.\nFurthermore, our method displays outstanding performance post-pruning, leading\nto a significant improvement of nearly 20% performance in model merging. We\nmake our code on Github.",
        "pdf_link": "https://arxiv.org/pdf/2403.02799v1.pdf"
    },
    {
        "title": "Evaluating and Optimizing Educational Content with Large Language Model Judgments",
        "authors": [
            "Joy He-Yueya",
            "Noah D. Goodman",
            "Emma Brunskill"
        ],
        "published": "2024-03-05T09:09:15Z",
        "summary": "Creating effective educational materials generally requires expensive and\ntime-consuming studies of student learning outcomes. To overcome this barrier,\none idea is to build computational models of student learning and use them to\noptimize instructional materials. However, it is difficult to model the\ncognitive processes of learning dynamics. We propose an alternative approach\nthat uses Language Models (LMs) as educational experts to assess the impact of\nvarious instructions on learning outcomes. Specifically, we use GPT-3.5 to\nevaluate the overall effect of instructional materials on different student\ngroups and find that it can replicate well-established educational findings\nsuch as the Expertise Reversal Effect and the Variability Effect. This\ndemonstrates the potential of LMs as reliable evaluators of educational\ncontent. Building on this insight, we introduce an instruction optimization\napproach in which one LM generates instructional materials using the judgments\nof another LM as a reward function. We apply this approach to create math word\nproblem worksheets aimed at maximizing student learning gains. Human teachers'\nevaluations of these LM-generated worksheets show a significant alignment\nbetween the LM judgments and human teacher preferences. We conclude by\ndiscussing potential divergences between human and LM opinions and the\nresulting pitfalls of automating instructional design.",
        "pdf_link": "https://arxiv.org/pdf/2403.02795v1.pdf"
    },
    {
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "authors": [
            "Hanlin Tang",
            "Yifu Sun",
            "Decheng Wu",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2024-03-05T08:45:30Z",
        "summary": "Large language models (LLMs) have proven to be very superior to conventional\nmethods in various tasks. However, their expensive computations and high memory\nrequirements are prohibitive for deployment. Model quantization is an effective\nmethod for reducing this overhead. The problem is that in most previous works,\nthe quantized model was calibrated using few samples from the training data,\nwhich might affect the generalization of the quantized LLMs to unknown cases\nand tasks. Hence in this work, we explore an important question: Can we design\na data-independent quantization method for LLMs to guarantee its generalization\nperformance? In this work, we propose EasyQuant, a training-free and\ndata-independent weight-only quantization algorithm for LLMs. Our observation\nindicates that two factors: outliers in the weight and quantization ranges, are\nessential for reducing the quantization error. Therefore, in EasyQuant, we\nleave the outliers (less than 1%) unchanged and optimize the quantization range\nto reduce the reconstruction error. With these methods, we surprisingly find\nthat EasyQuant achieves comparable performance to the original model. Since\nEasyQuant does not depend on any training data, the generalization performance\nof quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented\nin parallel so that the quantized model could be attained in a few minutes even\nfor LLMs over 100B. To our best knowledge, we are the first work that achieves\nalmost lossless quantization performance for LLMs under a data-independent\nsetting and our algorithm runs over 10 times faster than the data-dependent\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2403.02775v1.pdf"
    },
    {
        "title": "Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations",
        "authors": [
            "Xiaonan Xu",
            "Yichao Wu",
            "Penghao Liang",
            "Yuhang He",
            "Han Wang"
        ],
        "published": "2024-03-05T08:31:00Z",
        "summary": "With the boom of e-commerce and web applications, recommender systems have\nbecome an important part of our daily lives, providing personalized\nrecommendations based on the user's preferences. Although deep neural networks\n(DNNs) have made significant progress in improving recommendation systems by\nsimulating the interaction between users and items and incorporating their\ntextual information, these DNN-based approaches still have some limitations,\nsuch as the difficulty of effectively understanding users' interests and\ncapturing textual information. It is not possible to generalize to different\nseen/unseen recommendation scenarios and reason about their predictions. At the\nsame time, the emergence of large language models (LLMs), represented by\nChatGPT and GPT-4, has revolutionized the fields of natural language processing\n(NLP) and artificial intelligence (AI) due to their superior capabilities in\nthe basic tasks of language understanding and generation, and their impressive\ngeneralization and reasoning capabilities. As a result, recent research has\nsought to harness the power of LLM to improve recommendation systems. Given the\nrapid development of this research direction in the field of recommendation\nsystems, there is an urgent need for a systematic review of existing LLM-driven\nrecommendation systems for researchers and practitioners in related fields to\ngain insight into. More specifically, we first introduced a representative\napproach to learning user and item representations using LLM as a feature\nencoder. We then reviewed the latest advances in LLMs techniques for\ncollaborative filtering enhanced recommendation systems from the three\nparadigms of pre-training, fine-tuning, and prompting. Finally, we had a\ncomprehensive discussion on the future direction of this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2403.02760v2.pdf"
    },
    {
        "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
        "authors": [
            "Muhammad Farid Adilazuarda",
            "Sagnik Mukherjee",
            "Pradhyumna Lavania",
            "Siddhant Singh",
            "Ashutosh Dwivedi",
            "Alham Fikri Aji",
            "Jacki O'Neill",
            "Ashutosh Modi",
            "Monojit Choudhury"
        ],
        "published": "2024-03-05T08:29:36Z",
        "summary": "We present a survey of 39 recent papers that aim to study cultural\nrepresentation and inclusion in large language models. We observe that none of\nthe studies define \"culture,\" which is a complex, multifaceted concept;\ninstead, they probe the models on some specially designed datasets which\nrepresent certain aspects of \"culture.\" We call these aspects the proxies of\ncultures, and organize them across three dimensions of demographic, semantic\nand linguistic-cultural interaction proxies. We also categorize the probing\nmethods employed. Our analysis indicates that only certain aspects of\n\"culture,\" such as values and objectives, have been studied, leaving several\nother interesting and important facets, especially the multitude of semantic\ndomains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022),\nunexplored. Two other crucial gaps are the lack of robustness and situatedness\nof the current methods. Based on these observations, we provide several\nrecommendations for a holistic and practically useful research agenda for\nfurthering cultural inclusion in LLMs and LLM-based applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.15412v1.pdf"
    },
    {
        "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
        "authors": [
            "Rui Wang",
            "Fei Mi",
            "Yi Chen",
            "Boyang Xue",
            "Hongru Wang",
            "Qi Zhu",
            "Kam-Fai Wong",
            "Ruifeng Xu"
        ],
        "published": "2024-03-05T08:22:41Z",
        "summary": "The growing interest in Large Language Models (LLMs) for specialized\napplications has revealed a significant challenge: when tailored to specific\ndomains, LLMs tend to experience catastrophic forgetting, compromising their\ngeneral capabilities and leading to a suboptimal user experience. Additionally,\ncrafting a versatile model for multiple domains simultaneously often results in\na decline in overall performance due to confusion between domains. In response\nto these issues, we present the RolE Prompting Guided Multi-Domain Adaptation\n(REGA) strategy. This novel approach effectively manages multi-domain LLM\nadaptation through three key components: 1) Self-Distillation constructs and\nreplays general-domain exemplars to alleviate catastrophic forgetting. 2) Role\nPrompting assigns a central prompt to the general domain and a unique role\nprompt to each specific domain to minimize inter-domain confusion during\ntraining. 3) Role Integration reuses and integrates a small portion of\ndomain-specific data to the general-domain data, which are trained under the\nguidance of the central prompt. The central prompt is used for a streamlined\ninference process, removing the necessity to switch prompts for different\ndomains. Empirical results demonstrate that REGA effectively alleviates\ncatastrophic forgetting and inter-domain confusion. This leads to improved\ndomain-specific performance compared to standard fine-tuned models, while still\npreserving robust general capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.02756v1.pdf"
    },
    {
        "title": "CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models",
        "authors": [
            "Son The Nguyen",
            "Niranjan Uma Naresh",
            "Theja Tulabandhula"
        ],
        "published": "2024-03-05T07:58:12Z",
        "summary": "This paper addresses the challenges of aligning large language models (LLMs)\nwith human values via preference learning (PL), with a focus on the issues of\nincomplete and corrupted data in preference datasets. We propose a novel method\nfor robustly and completely recalibrating values within these datasets to\nenhance LLMs resilience against the issues. In particular, we devise a\nguaranteed polynomial time ranking algorithm that robustifies several existing\nmodels, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry,\n1952) model and certain generalizations of it. To the best of our knowledge,\nour present work is the first to propose an algorithm that provably recovers an\n{\\epsilon}-optimal ranking with high probability while allowing as large as\nO(n) perturbed pairwise comparison results per model response. Furthermore, we\nshow robust recovery results in the partially observed setting. Our experiments\nconfirm that our algorithms handle adversarial noise and unobserved comparisons\nwell in both general and LLM preference dataset settings. This work contributes\nto the development and scaling of more reliable and ethically aligned AI models\nby equipping the dataset curation pipeline with the ability to handle missing\nand maliciously manipulated inputs.",
        "pdf_link": "https://arxiv.org/pdf/2403.02745v1.pdf"
    },
    {
        "title": "Towards Training A Chinese Large Language Model for Anesthesiology",
        "authors": [
            "Zhonghai Wang",
            "Jie Jiang",
            "Yibing Zhan",
            "Bohao Zhou",
            "Yanhong Li",
            "Chong Zhang",
            "Liang Ding",
            "Hua Jin",
            "Jun Peng",
            "Xu Lin",
            "Weifeng Liu"
        ],
        "published": "2024-03-05T07:53:49Z",
        "summary": "Medical large language models (LLMs) have gained popularity recently due to\ntheir significant practical utility. However, most existing research focuses on\ngeneral medicine, and there is a need for in-depth study of LLMs in specific\nfields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese\nAnesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions\nhave three aspects: 1) The data, such as utilizing Self-Instruct, acquired from\ncurrent LLMs likely includes inaccuracies. Hypnos implements a cross-filtering\nstrategy to improve the data quality. This strategy involves using one LLM to\nassess the quality of the generated data from another LLM and filtering out the\ndata with low quality. 2) Hypnos employs a general-to-specific training\nstrategy that starts by fine-tuning LLMs using the general medicine data and\nsubsequently improving the fine-tuned LLMs using data specifically from\nAnesthesiology. The general medical data supplement the medical expertise in\nAnesthesiology and enhance the effectiveness of Hypnos' generation. 3) We\nintroduce a standardized benchmark for evaluating medical LLM in\nAnesthesiology. Our benchmark includes both publicly available instances from\nthe Internet and privately obtained cases from the Hospital. Hypnos outperforms\nother medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on\nthe benchmark dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.02742v1.pdf"
    },
    {
        "title": "Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment",
        "authors": [
            "Congzhi Zhang",
            "Linhai Zhang",
            "Deyu Zhou",
            "Guoqiang Xu"
        ],
        "published": "2024-03-05T07:47:34Z",
        "summary": "Despite the significant achievements of existing prompting methods such as\nin-context learning and chain-of-thought for large language models (LLMs), they\nstill face challenges of various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including data augmentation-based\nand reweight-based approaches, with the limitations of addressing the complex\nbiases of LLMs. To address such limitations, the causal relationship behind the\nprompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate the bias of LLMs. In specific, causal intervention is\nimplemented by designing the prompts without accessing the parameters and\nlogits of LLMs.The chain-of-thoughts generated by LLMs are employed as the\nmediator variable and the causal effect between the input prompt and the output\nanswers is calculated through front-door adjustment to mitigate model biases.\nMoreover, to obtain the representation of the samples precisely and estimate\nthe causal effect more accurately, contrastive learning is used to fine-tune\nthe encoder of the samples by aligning the space of the encoder with the LLM.\nExperimental results show that the proposed causal prompting approach achieves\nexcellent performance on 3 natural language processing datasets on both\nopen-source and closed-source LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.02738v1.pdf"
    },
    {
        "title": "Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models",
        "authors": [
            "Sang T. Truong",
            "Duc Q. Nguyen",
            "Toan Nguyen",
            "Dong D. Le",
            "Nhi N. Truong",
            "Tho Quan",
            "Sanmi Koyejo"
        ],
        "published": "2024-03-05T07:13:28Z",
        "summary": "Recent advancements in large language models (LLMs) have underscored their\nimportance in the evolution of artificial intelligence. However, despite\nextensive pretraining on multilingual datasets, available open-sourced LLMs\nexhibit limited effectiveness in processing Vietnamese. The challenge is\nexacerbated by the absence of systematic benchmark datasets and metrics\ntailored for Vietnamese LLM evaluation. To mitigate these issues, we have\nfinetuned LLMs specifically for Vietnamese and developed a comprehensive\nevaluation framework encompassing 10 common tasks and 31 metrics. Our\nevaluation results reveal that the fine-tuned LLMs exhibit enhanced\ncomprehension and generative capabilities in Vietnamese. Moreover, our analysis\nindicates that models with more parameters can introduce more biases and\nuncalibrated outputs and the key factor influencing LLM performance is the\nquality of the training or fine-tuning datasets. These insights underscore the\nsignificance of meticulous fine-tuning with high-quality datasets in enhancing\nLLM performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.02715v1.pdf"
    },
    {
        "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
        "authors": [
            "Jiwen Zhang",
            "Jihao Wu",
            "Yihua Teng",
            "Minghui Liao",
            "Nuo Xu",
            "Xiao Xiao",
            "Zhongyu Wei",
            "Duyu Tang"
        ],
        "published": "2024-03-05T07:09:35Z",
        "summary": "Large language model (LLM) leads to a surge of autonomous GUI agents for\nsmartphone, which completes a task triggered by natural language through\npredicting a sequence of actions of API. Even though the task highly relies on\npast actions and visual observations, existing studies typical consider little\nsemantic information carried out by intermediate screenshots and screen\noperations. To address this, this work presents Chain-of-Action-Thought (dubbed\nCoAT), which takes the description of the previous actions, the current screen,\nand more importantly the action thinking of what actions should be performed\nand the outcomes led by the chosen action. We demonstrate that, in a zero-shot\nsetting upon an off-the-shell LLM, CoAT significantly improves the goal\nprogress compared to standard context modeling. To further facilitate the\nresearch in this line, we construct a benchmark Android-In-The-Zoo (AitZ),\nwhich contains 18,643 screen-action pairs together with chain-of-action-thought\nannotations. Experiments show that fine-tuning a 200M model on our AitZ dataset\nachieves on par performance with CogAgent-Chat-18B.",
        "pdf_link": "https://arxiv.org/pdf/2403.02713v1.pdf"
    },
    {
        "title": "Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment",
        "authors": [
            "Congzhi Zhang",
            "Linhai Zhang",
            "Deyu Zhou"
        ],
        "published": "2024-03-05T06:28:02Z",
        "summary": "Conventional multi-hop fact verification models are prone to rely on spurious\ncorrelations from the annotation artifacts, leading to an obvious performance\ndecline on unbiased datasets. Among the various debiasing works, the causal\ninference-based methods become popular by performing theoretically guaranteed\ndebiasing such as casual intervention or counterfactual reasoning. However,\nexisting causal inference-based debiasing methods, which mainly formulate fact\nverification as a single-hop reasoning task to tackle shallow bias patterns,\ncannot deal with the complicated bias patterns hidden in multiple hops of\nevidence. To address the challenge, we propose Causal Walk, a novel method for\ndebiasing multi-hop fact verification from a causal perspective with front-door\nadjustment. Specifically, in the structural causal model, the reasoning path\nbetween the treatment (the input claim-evidence graph) and the outcome (the\nveracity label) is introduced as the mediator to block the confounder. With the\nfront-door adjustment, the causal effect between the treatment and the outcome\nis decomposed into the causal effect between the treatment and the mediator,\nwhich is estimated by applying the idea of random walk, and the causal effect\nbetween the mediator and the outcome, which is estimated with normalized\nweighted geometric mean approximation. To investigate the effectiveness of the\nproposed method, an adversarial multi-hop fact verification dataset and a\nsymmetric multi-hop fact verification dataset are proposed with the help of the\nlarge language model. Experimental results show that Causal Walk outperforms\nsome previous debiasing methods on both existing datasets and the newly\nconstructed datasets. Code and data will be released at\nhttps://github.com/zcccccz/CausalWalk.",
        "pdf_link": "https://arxiv.org/pdf/2403.02698v1.pdf"
    },
    {
        "title": "Privacy-Aware Semantic Cache for Large Language Models",
        "authors": [
            "Waris Gill",
            "Mohamed Elidrisi",
            "Pallavi Kalapatapu",
            "Ali Anwar",
            "Muhammad Ali Gulzar"
        ],
        "published": "2024-03-05T06:23:50Z",
        "summary": "Large Language Models (LLMs) like ChatGPT and Llama2 have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries, leading to unacceptable false hit-and-miss rates.\n  This paper introduces MeanCache, a user-centric semantic cache for LLMs that\nidentifies semantically similar queries to determine cache hit or miss. Using\nMeanCache, the response to a user's semantically similar query can be retrieved\nfrom a local cache rather than re-querying the LLM, thus reducing costs,\nservice provider load, and environmental impact. Existing caching solutions for\nLLMs raise privacy and scalability concerns and perform wasteful query\nrequests. MeanCache leverages Federated Learning (FL) to collaboratively train\na query similarity model across LLM users without violating privacy. By placing\na local cache in each user's device and using FL, MeanCache reduces the latency\nand costs and enhances model performance, resulting in lower false hit rates.\nMeanCache compresses the embedding dimensions to minimize cache storage and\nalso finds the optimal cosine similarity threshold. Our experiments benchmarked\nagainst the state-of-the-art caching method, reveal that MeanCache attains an\napproximately 17% higher F-score and a 20% increase in precision during\nsemantic cache hit-and-miss decisions. It also reduces the storage requirement\nby 83% and accelerates semantic cache hit-and-miss decisions by 11%.",
        "pdf_link": "https://arxiv.org/pdf/2403.02694v2.pdf"
    },
    {
        "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
        "authors": [
            "Qiusi Zhan",
            "Zhixiang Liang",
            "Zifan Ying",
            "Daniel Kang"
        ],
        "published": "2024-03-05T06:21:45Z",
        "summary": "Recent work has embodied LLMs as agents, allowing them to access tools,\nperform actions, and interact with external content (e.g., emails or websites).\nHowever, external content introduces the risk of indirect prompt injection\n(IPI) attacks, where malicious instructions are embedded within the content\nprocessed by LLMs, aiming to manipulate these agents into executing detrimental\nactions against users. Given the potentially severe consequences of such\nattacks, establishing benchmarks to assess and mitigate these risks is\nimperative.\n  In this work, we introduce InjecAgent, a benchmark designed to assess the\nvulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent\ncomprises 1,054 test cases covering 17 different user tools and 62 attacker\ntools. We categorize attack intentions into two primary types: direct harm to\nusers and exfiltration of private data. We evaluate 30 different LLM agents and\nshow that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4\nvulnerable to attacks 24% of the time. Further investigation into an enhanced\nsetting, where the attacker instructions are reinforced with a hacking prompt,\nshows additional increases in success rates, nearly doubling the attack success\nrate on the ReAct-prompted GPT-4. Our findings raise questions about the\nwidespread deployment of LLM Agents. Our benchmark is available at\nhttps://github.com/uiuc-kang-lab/InjecAgent.",
        "pdf_link": "https://arxiv.org/pdf/2403.02691v2.pdf"
    },
    {
        "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
        "authors": [
            "Masamune Kobayashi",
            "Masato Mita",
            "Mamoru Komachi"
        ],
        "published": "2024-03-05T05:53:09Z",
        "summary": "Metrics are the foundation for automatic evaluation in grammatical error\ncorrection (GEC), with their evaluation of the metrics (meta-evaluation)\nrelying on their correlation with human judgments. However, conventional\nmeta-evaluations in English GEC encounter several challenges including biases\ncaused by inconsistencies in evaluation granularity, and an outdated setup\nusing classical systems. These problems can lead to misinterpretation of\nmetrics and potentially hinder the applicability of GEC techniques. To address\nthese issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation.\nSEEDA consists of corrections with human ratings along two different\ngranularities: edit-based and sentence-based, covering 12 state-of-the-art\nsystems including large language models (LLMs), and two human corrections with\ndifferent focuses. The results of improved correlations by aligning the\ngranularity in the sentence-level meta-evaluation, suggest that edit-based\nmetrics may have been underestimated in existing studies. Furthermore,\ncorrelations of most metrics decrease when changing from classical to neural\nsystems, indicating that traditional metrics are relatively poor at evaluating\nfluently corrected sentences with many edits.",
        "pdf_link": "https://arxiv.org/pdf/2403.02674v1.pdf"
    },
    {
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
        "authors": [
            "Zhenyu Zhang",
            "Runjin Chen",
            "Shiwei Liu",
            "Zhewei Yao",
            "Olatunji Ruwase",
            "Beidi Chen",
            "Xiaoxia Wu",
            "Zhangyang Wang"
        ],
        "published": "2024-03-05T04:58:37Z",
        "summary": "This paper aims to overcome the \"lost-in-the-middle\" challenge of large\nlanguage models (LLMs). While recent advancements have successfully enabled\nLLMs to perform stable language modeling with up to 4 million tokens, the\npersistent difficulty faced by most LLMs in identifying relevant information\nsituated in the middle of the context has not been adequately tackled. To\naddress this problem, this paper introduces Multi-scale Positional Encoding\n(Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the\ncapacity of LLMs to handle the relevant information located in the middle of\nthe context, without fine-tuning or introducing any additional overhead. Ms-PoE\nleverages the position indice rescaling to relieve the long-term decay effect\nintroduced by RoPE, while meticulously assigning distinct scaling ratios to\ndifferent attention heads to preserve essential knowledge learned during the\npre-training step, forming a multi-scale context fusion from short to long\ndistance. Extensive experiments with a wide range of LLMs demonstrate the\nefficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of\nup to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are\navailable at https://github.com/VITA-Group/Ms-PoE.",
        "pdf_link": "https://arxiv.org/pdf/2403.04797v1.pdf"
    },
    {
        "title": "Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use",
        "authors": [
            "Imad Eddine Toubal",
            "Aditya Avinash",
            "Neil Gordon Alldrin",
            "Jan Dlabal",
            "Wenlei Zhou",
            "Enming Luo",
            "Otilia Stretcu",
            "Hao Xiong",
            "Chun-Ta Lu",
            "Howard Zhou",
            "Ranjay Krishna",
            "Ariel Fuxman",
            "Tom Duerig"
        ],
        "published": "2024-03-05T03:34:11Z",
        "summary": "From content moderation to wildlife conservation, the number of applications\nthat require models to recognize nuanced or subjective visual concepts is\ngrowing. Traditionally, developing classifiers for such concepts requires\nsubstantial manual effort measured in hours, days, or even months to identify\nand annotate data needed for training. Even with recently proposed Agile\nModeling techniques, which enable rapid bootstrapping of image classifiers,\nusers are still required to spend 30 minutes or more of monotonous, repetitive\ndata labeling just to train a single classifier. Drawing on Fiske's Cognitive\nMiser theory, we propose a new framework that alleviates manual effort by\nreplacing human labeling with natural language interactions, reducing the total\neffort required to define a concept by an order of magnitude: from labeling\n2,000 images to only 100 plus some natural language interactions. Our framework\nleverages recent advances in foundation models, both large language models and\nvision-language models, to carve out the concept space through conversation and\nby automatically labeling training data points. Most importantly, our framework\neliminates the need for crowd-sourced annotations. Moreover, our framework\nultimately produces lightweight classification models that are deployable in\ncost-sensitive scenarios. Across 15 subjective concepts and across 2 public\nimage classification datasets, our trained models outperform traditional Agile\nModeling as well as state-of-the-art zero-shot classification models like\nALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.",
        "pdf_link": "https://arxiv.org/pdf/2403.02626v2.pdf"
    },
    {
        "title": "Exploring the Limitations of Large Language Models in Compositional Relation Reasoning",
        "authors": [
            "Jinman Zhao",
            "Xueyan Zhang"
        ],
        "published": "2024-03-05T03:07:10Z",
        "summary": "We present a comprehensive evaluation of large language models(LLMs)' ability\nto reason about composition relations through a benchmark encompassing 1,500\ntest cases in English, designed to cover six distinct types of composition\nrelations: Positional, Comparative, Personal, Mathematical, Identity, and\nOther. Acknowledging the significance of multilingual capabilities, we expanded\nour assessment to include translations of these cases into Chinese, Japanese,\nFrench, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims\nat investigating the robustness and adaptability of LLMs in handling\ncomposition relation reasoning across diverse linguistic contexts.",
        "pdf_link": "https://arxiv.org/pdf/2403.02615v1.pdf"
    },
    {
        "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
        "authors": [
            "Yutong Li",
            "Lu Chen",
            "Aiwei Liu",
            "Kai Yu",
            "Lijie Wen"
        ],
        "published": "2024-03-05T01:13:56Z",
        "summary": "The literature review is an indispensable step in the research process. It\nprovides the benefit of comprehending the research problem and understanding\nthe current research situation while conducting a comparative analysis of prior\nworks. However, literature summary is challenging and time consuming. The\nprevious LLM-based studies on literature review mainly focused on the complete\nprocess, including literature retrieval, screening, and summarization. However,\nfor the summarization step, simple CoT method often lacks the ability to\nprovide extensive comparative summary. In this work, we firstly focus on the\nindependent literature summarization step and introduce ChatCite, an LLM agent\nwith human workflow guidance for comparative literature summary. This agent, by\nmimicking the human workflow, first extracts key elements from relevant\nliterature and then generates summaries using a Reflective Incremental\nMechanism. In order to better evaluate the quality of the generated summaries,\nwe devised a LLM-based automatic evaluation metric, G-Score, in refer to the\nhuman evaluation criteria. The ChatCite agent outperformed other models in\nvarious dimensions in the experiments. The literature summaries generated by\nChatCite can also be directly used for drafting literature reviews.",
        "pdf_link": "https://arxiv.org/pdf/2403.02574v1.pdf"
    },
    {
        "title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code",
        "authors": [
            "Bryan Li",
            "Tamer Alkhouli",
            "Daniele Bonadiman",
            "Nikolaos Pappas",
            "Saab Mansour"
        ],
        "published": "2024-03-05T00:48:56Z",
        "summary": "Development of large language models (LLM) have shown progress on reasoning,\nthough studies have been limited to English or simple reasoning tasks. We thus\nintroduce a multilingual structured reasoning and explanation dataset, termed\nxSTREET, that covers four tasks across six languages. xSTREET exposes a gap in\nbase LLM performance between English and non-English reasoning tasks. We then\npropose two methods to remedy this gap, building on the insight that LLMs\ntrained on code are better reasoners. First, at training time, we augment a\ncode dataset with multi-lingual comments using machine translation while\nkeeping program code as-is. Second, at inference time, we bridge the gap\nbetween training and inference by employing a prompt structure that\nincorporates step-by-step code primitives to derive new facts and find a\nsolution. Our methods show improved multilingual performance on xSTREET, most\nnotably on the scientific commonsense reasoning subtask. Furthermore, the\nmodels show no regression on non-reasoning tasks, thus showing our techniques\nmaintain general-purpose abilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.02567v1.pdf"
    },
    {
        "title": "Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research",
        "authors": [
            "Brenda Y. Miao",
            "Irene Y. Chen",
            "Christopher YK Williams",
            "Jays\u00f3n Davidson",
            "Augusto Garcia-Agundez",
            "Harry Sun",
            "Travis Zack",
            "Atul J. Butte",
            "Madhumita Sushil"
        ],
        "published": "2024-03-05T00:27:43Z",
        "summary": "Recent advances in generative models, including large language models (LLMs),\nvision language models (VLMs), and diffusion models, have accelerated the field\nof natural language and image processing in medicine and marked a significant\nparadigm shift in how biomedical models can be developed and deployed. While\nthese models are highly adaptable to new tasks, scaling and evaluating their\nusage presents new challenges not addressed in previous frameworks. In\nparticular, the ability of these models to produce useful outputs with little\nto no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as\nthe open-ended nature of their outputs, necessitate the development of updated\nguidelines in using and evaluating these models. In response to gaps in\nstandards and best practices for the development of clinical AI tools\nidentified by US Executive Order 141103 and several emerging national networks\nfor clinical AI evaluation, we begin to formalize some of these guidelines by\nbuilding on the \"Minimum information about clinical artificial intelligence\nmodeling\" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in\n2020, provided a set of six steps with guidelines on the minimum information\nnecessary to encourage transparent, reproducible research for artificial\nintelligence (AI) in medicine. Here, we propose modifications to the original\nchecklist that highlight differences in training, evaluation, interpretability,\nand reproducibility of generative models compared to traditional AI models for\nclinical research. This updated checklist also seeks to clarify cohort\nselection reporting and adds additional items on alignment with ethical\nstandards.",
        "pdf_link": "https://arxiv.org/pdf/2403.02558v1.pdf"
    },
    {
        "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
        "authors": [
            "Buyun Zhang",
            "Liang Luo",
            "Yuxin Chen",
            "Jade Nie",
            "Xi Liu",
            "Daifeng Guo",
            "Yanli Zhao",
            "Shen Li",
            "Yuchen Hao",
            "Yantao Yao",
            "Guna Lakshminarayanan",
            "Ellie Dingqiao Wen",
            "Jongsoo Park",
            "Maxim Naumov",
            "Wenlin Chen"
        ],
        "published": "2024-03-04T23:40:20Z",
        "summary": "Scaling laws play an instrumental role in the sustainable improvement in\nmodel quality. Unfortunately, recommendation models to date do not exhibit such\nlaws similar to those observed in the domain of large language models, due to\nthe inefficiencies of their upscaling mechanisms. This limitation poses\nsignificant challenges in adapting these models to increasingly more complex\nreal-world datasets. In this paper, we propose an effective network\narchitecture based purely on stacked factorization machines, and a synergistic\nupscaling strategy, collectively dubbed Wukong, to establish a scaling law in\nthe domain of recommendation. Wukong's unique design makes it possible to\ncapture diverse, any-order of interactions simply through taller and wider\nlayers. We conducted extensive evaluations on six public datasets, and our\nresults demonstrate that Wukong consistently outperforms state-of-the-art\nmodels quality-wise. Further, we assessed Wukong's scalability on an internal,\nlarge-scale dataset. The results show that Wukong retains its superiority in\nquality over state-of-the-art models, while holding the scaling law across two\norders of magnitude in model complexity, extending beyond 100 Gflop or\nequivalently up to Large Language Model (GPT-3) training compute scale, where\nprior arts fall short.",
        "pdf_link": "https://arxiv.org/pdf/2403.02545v2.pdf"
    },
    {
        "title": "DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation",
        "authors": [
            "Xueqing Wu",
            "Rui Zheng",
            "Jingzhen Sha",
            "Te-Lin Wu",
            "Hanyu Zhou",
            "Mohan Tang",
            "Kai-Wei Chang",
            "Nanyun Peng",
            "Haoran Huang"
        ],
        "published": "2024-03-04T22:47:58Z",
        "summary": "Data analysis is a crucial analytical process to generate in-depth studies\nand conclusive insights to comprehensively answer a given user query for\ntabular data. In this work, we aim to propose new resources and benchmarks to\ninspire future research on this crucial yet challenging and under-explored\ntask. However, collecting data analysis annotations curated by experts can be\nprohibitively expensive. We propose to automatically generate high-quality\nanswer annotations leveraging the code-generation capabilities of LLMs with a\nmulti-turn prompting technique. We construct the DACO dataset, containing (1)\n440 databases (of tabular data) collected from real-world scenarios, (2) ~2k\nquery-answer pairs that can serve as weak supervision for model training, and\n(3) a concentrated but high-quality test set with human refined annotations\nthat serves as our main evaluation benchmark. We train a 6B supervised\nfine-tuning (SFT) model on DACO dataset, and find that the SFT model learns\nreasonable data analysis capabilities. To further align the models with human\npreference, we use reinforcement learning to encourage generating analysis\nperceived by human as helpful, and design a set of dense rewards to propagate\nthe sparse human preference reward to intermediate code generation steps. Our\nDACO-RL algorithm is evaluated by human annotators to produce more helpful\nanswers than SFT model in 57.72% cases, validating the effectiveness of our\nproposed algorithm. Data and code are released at\nhttps://github.com/shirley-wu/daco",
        "pdf_link": "https://arxiv.org/pdf/2403.02528v1.pdf"
    },
    {
        "title": "Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF",
        "authors": [
            "Chen Zheng",
            "Ke Sun",
            "Hang Wu",
            "Chenguang Xi",
            "Xun Zhou"
        ],
        "published": "2024-03-04T22:02:12Z",
        "summary": "In recent advancements in Conversational Large Language Models (LLMs), a\nconcerning trend has emerged, showing that many new base LLMs experience a\nknowledge reduction in their foundational capabilities following Supervised\nFine-Tuning (SFT). This process often leads to issues such as forgetting or a\ndecrease in the base model's abilities. Moreover, fine-tuned models struggle to\nalign with user preferences, inadvertently increasing the generation of toxic\noutputs when specifically prompted. To overcome these challenges, we adopted an\ninnovative approach by completely bypassing SFT and directly implementing\nHarmless Reinforcement Learning from Human Feedback (RLHF). Our method not only\npreserves the base model's general capabilities but also significantly enhances\nits conversational abilities, while notably reducing the generation of toxic\noutputs. Our approach holds significant implications for fields that demand a\nnuanced understanding and generation of responses, such as customer service. We\napplied this methodology to Mistral, the most popular base model, thereby\ncreating Mistral-Plus. Our validation across 11 general tasks demonstrates that\nMistral-Plus outperforms similarly sized open-source base models and their\ncorresponding instruct versions. Importantly, the conversational abilities of\nMistral-Plus were significantly improved, indicating a substantial advancement\nover traditional SFT models in both safety and user preference alignment.",
        "pdf_link": "https://arxiv.org/pdf/2403.02513v1.pdf"
    },
    {
        "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
        "authors": [
            "Xiang Gao",
            "Jiaxin Zhang",
            "Lalla Mouatadid",
            "Kamalika Das"
        ],
        "published": "2024-03-04T21:55:22Z",
        "summary": "In recent years, large language models (LLMs) have become increasingly\nprevalent, offering remarkable text generation capabilities. However, a\npressing challenge is their tendency to make confidently wrong predictions,\nhighlighting the critical need for uncertainty quantification (UQ) in LLMs.\nWhile previous works have mainly focused on addressing aleatoric uncertainty,\nthe full spectrum of uncertainties, including epistemic, remains inadequately\nexplored. Motivated by this gap, we introduce a novel UQ method, sampling with\nperturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic\nuncertainties. The method entails generating a set of perturbations for LLM\ninputs, sampling outputs for each perturbation, and incorporating an\naggregation module that generalizes the sampling uncertainty approach for text\ngeneration tasks. Through extensive experiments on various datasets, we\ninvestigated different perturbation and aggregation techniques. Our findings\nshow a substantial improvement in model uncertainty calibration, with a\nreduction in Expected Calibration Error (ECE) by 50\\% on average. Our findings\nsuggest that our proposed UQ method offers promising steps toward enhancing the\nreliability and trustworthiness of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.02509v1.pdf"
    },
    {
        "title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents",
        "authors": [
            "Yifan Song",
            "Da Yin",
            "Xiang Yue",
            "Jie Huang",
            "Sujian Li",
            "Bill Yuchen Lin"
        ],
        "published": "2024-03-04T21:50:29Z",
        "summary": "Large Language Models (LLMs) have become integral components in various\nautonomous agent systems. In this study, we present an exploration-based\ntrajectory optimization approach, referred to as ETO. This learning method is\ndesigned to enhance the performance of open LLM agents. Contrary to previous\nstudies that exclusively train on successful expert trajectories, our method\nallows agents to learn from their exploration failures. This leads to improved\nperformance through an iterative optimization framework. During the exploration\nphase, the agent interacts with the environment while completing given tasks,\ngathering failure trajectories to create contrastive trajectory pairs. In the\nsubsequent training phase, the agent utilizes these trajectory preference pairs\nto update its policy using contrastive learning methods like DPO. This\niterative cycle of exploration and training fosters continued improvement in\nthe agents. Our experiments on three complex tasks demonstrate that ETO\nconsistently surpasses baseline performance by a large margin. Furthermore, an\nexamination of task-solving efficiency and potential in scenarios lacking\nexpert trajectory underscores the effectiveness of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2403.02502v1.pdf"
    },
    {
        "title": "Enhancing LLM Safety via Constrained Direct Preference Optimization",
        "authors": [
            "Zixuan Liu",
            "Xiaolin Sun",
            "Zizhan Zheng"
        ],
        "published": "2024-03-04T20:39:24Z",
        "summary": "The rapidly increasing capabilities of large language models (LLMs) raise an\nurgent need to align AI systems with diverse human preferences to\nsimultaneously enhance their usefulness and safety, despite the often\nconflicting nature of these goals. To address this important problem, a\npromising approach is to enforce a safety constraint at the fine-tuning stage\nthrough a constrained Reinforcement Learning from Human Feedback (RLHF)\nframework. This approach, however, is computationally expensive and often\nunstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension\nof the recently proposed Direct Preference Optimization (DPO) approach for\nfine-tuning LLMs that is both efficient and lightweight. By integrating dual\ngradient descent and DPO, our method identifies a nearly optimal trade-off\nbetween helpfulness and harmlessness without using reinforcement learning.\nEmpirically, our approach provides a safety guarantee to LLMs that is missing\nin DPO while achieving significantly higher rewards under the same safety\nconstraint compared to a recently proposed safe RLHF approach.\n  Warning: This paper contains example data that may be offensive or harmful.",
        "pdf_link": "https://arxiv.org/pdf/2403.02475v1.pdf"
    },
    {
        "title": "OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering",
        "authors": [
            "Amit Das",
            "Mostafa Rahgouy",
            "Dongji Feng",
            "Zheng Zhang",
            "Tathagata Bhattacharya",
            "Nilanjana Raychawdhary",
            "Mary Sandage",
            "Lauramarie Pope",
            "Gerry Dozier",
            "Cheryl Seals"
        ],
        "published": "2024-03-04T20:34:58Z",
        "summary": "The widespread presence of hateful languages on social media has resulted in\nadverse effects on societal well-being. As a result, it has become very\nimportant to address this issue with high priority. Hate speech or offensive\nlanguages exist in both explicit and implicit forms, with the latter being more\nchallenging to detect. Current research in this domain encounters several\nchallenges. Firstly, the existing datasets primarily rely on the collection of\ntexts containing explicit offensive keywords, making it challenging to capture\nimplicitly offensive contents that are devoid of these keywords. Secondly,\nusual methodologies tend to focus solely on textual analysis, neglecting the\nvaluable insights that community information can provide. In this research\npaper, we introduce a novel dataset OffLanDat, a community based implicit\noffensive language dataset generated by ChatGPT containing data for 38\ndifferent target groups. Despite limitations in generating offensive texts\nusing ChatGPT due to ethical constraints, we present a prompt-based approach\nthat effectively generates implicit offensive languages. To ensure data\nquality, we evaluate our data with human. Additionally, we employ a\nprompt-based Zero-Shot method with ChatGPT and compare the detection results\nbetween human annotation and ChatGPT annotation. We utilize existing\nstate-of-the-art models to see how effective they are in detecting such\nlanguages. We will make our code and dataset public for other researchers.",
        "pdf_link": "https://arxiv.org/pdf/2403.02472v5.pdf"
    },
    {
        "title": "Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems",
        "authors": [
            "Lingjiao Chen",
            "Jared Quincy Davis",
            "Boris Hanin",
            "Peter Bailis",
            "Ion Stoica",
            "Matei Zaharia",
            "James Zou"
        ],
        "published": "2024-03-04T19:12:48Z",
        "summary": "Many recent state-of-the-art results in language tasks were achieved using\ncompound systems that perform multiple Large Language Model (LLM) calls and\naggregate their responses. However, there is little understanding of how the\nnumber of LLM calls -- e.g., when asking the LLM to answer each question\nmultiple times and taking a consensus -- affects such a compound system's\nperformance. In this paper, we initiate the study of scaling laws of compound\ninference systems. We analyze, theoretically and empirically, how the number of\nLLM calls affects the performance of one-layer Voting Inference Systems -- one\nof the simplest compound systems, which aggregates LLM responses via majority\nvoting. We find empirically that across multiple language tasks, surprisingly,\nVoting Inference Systems' performance first increases but then decreases as a\nfunction of the number of LLM calls. Our theoretical results suggest that this\nnon-monotonicity is due to the diversity of query difficulties within a task:\nmore LLM calls lead to higher performance on \"easy\" queries, but lower\nperformance on \"hard\" queries, and non-monotone behavior emerges when a task\ncontains both types of queries. This insight then allows us to compute, from a\nsmall number of samples, the number of LLM calls that maximizes system\nperformance, and define a scaling law of Voting Inference Systems. Experiments\nshow that our scaling law can predict the performance of Voting Inference\nSystems and find the optimal number of LLM calls to make.",
        "pdf_link": "https://arxiv.org/pdf/2403.02419v1.pdf"
    },
    {
        "title": "RegionGPT: Towards Region Understanding Vision Language Model",
        "authors": [
            "Qiushan Guo",
            "Shalini De Mello",
            "Hongxu Yin",
            "Wonmin Byeon",
            "Ka Chun Cheung",
            "Yizhou Yu",
            "Ping Luo",
            "Sifei Liu"
        ],
        "published": "2024-03-04T18:58:08Z",
        "summary": "Vision language models (VLMs) have experienced rapid advancements through the\nintegration of large language models (LLMs) with image-text pairs, yet they\nstruggle with detailed regional visual understanding due to limited spatial\nawareness of the vision encoder, and the use of coarse-grained training data\nthat lacks detailed, region-specific captions. To address this, we introduce\nRegionGPT (short as RGPT), a novel framework designed for complex region-level\ncaptioning and understanding. RGPT enhances the spatial awareness of regional\nrepresentation with simple yet effective modifications to existing visual\nencoders in VLMs. We further improve performance on tasks requiring a specific\noutput scope by integrating task-guided instruction prompts during both\ntraining and inference phases, while maintaining the model's versatility for\ngeneral-purpose tasks. Additionally, we develop an automated region caption\ndata generation pipeline, enriching the training set with detailed region-level\ncaptions. We demonstrate that a universal RGPT model can be effectively applied\nand significantly enhancing performance across a range of region-level tasks,\nincluding but not limited to complex region descriptions, reasoning, object\nclassification, and referring expressions comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2403.02330v1.pdf"
    },
    {
        "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve",
        "authors": [
            "Amey Agrawal",
            "Nitin Kedia",
            "Ashish Panwar",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Bhargav S. Gulavani",
            "Alexey Tumanov",
            "Ramachandran Ramjee"
        ],
        "published": "2024-03-04T18:47:08Z",
        "summary": "Each LLM serving request goes through two phases. The first is prefill which\nprocesses the entire input prompt to produce one output token and the second is\ndecode which generates the rest of output tokens, one-at-a-time. Prefill\niterations have high latency but saturate GPU compute due to parallel\nprocessing of the input prompt. In contrast, decode iterations have low latency\nbut also low compute utilization because a decode iteration processes only a\nsingle token per request. This makes batching highly effective for decodes and\nconsequently for overall throughput. However, batching multiple requests leads\nto an interleaving of prefill and decode iterations which makes it challenging\nto achieve both high throughput and low latency.\n  We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by\nthe techniques we originally proposed for optimizing throughput in Sarathi.\nSarathi-Serve leverages chunked-prefills from Sarathi to create stall-free\nschedules that can add new requests in a batch without pausing ongoing decodes.\nStall-free scheduling unlocks the opportunity to improve throughput with large\nbatch sizes while minimizing the effect of batching on latency. Our evaluation\nshows that Sarathi-Serve improves serving throughput within desired latency\nSLOs of Mistral-7B by up to 2.6x on a single A100 GPU and up to 6.9x for\nFalcon-180B on 8 A100 GPUs over Orca and vLLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.02310v1.pdf"
    },
    {
        "title": "FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction",
        "authors": [
            "Alessandro Scir\u00e8",
            "Karim Ghonim",
            "Roberto Navigli"
        ],
        "published": "2024-03-04T17:57:18Z",
        "summary": "Recent advancements in text summarization, particularly with the advent of\nLarge Language Models (LLMs), have shown remarkable performance. However, a\nnotable challenge persists as a substantial number of automatically-generated\nsummaries exhibit factual inconsistencies, such as hallucinations. In response\nto this issue, various approaches for the evaluation of consistency for\nsummarization have emerged. Yet, these newly-introduced metrics face several\nlimitations, including lack of interpretability, focus on short document\nsummaries (e.g., news articles), and computational impracticality, especially\nfor LLM-based metrics. To address these shortcomings, we propose Factuality\nEvaluation of summarization based on Natural language Inference and Claim\nExtraction (FENICE), a more interpretable and efficient factuality-oriented\nmetric. FENICE leverages an NLI-based alignment between information in the\nsource document and a set of atomic facts, referred to as claims, extracted\nfrom the summary. Our metric sets a new state of the art on AGGREFACT, the\nde-facto benchmark for factuality evaluation. Moreover, we extend our\nevaluation to a more challenging setting by conducting a human annotation\nprocess of long-form summarization.",
        "pdf_link": "https://arxiv.org/pdf/2403.02270v2.pdf"
    },
    {
        "title": "KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection",
        "authors": [
            "Yuexin Li",
            "Chengyu Huang",
            "Shumin Deng",
            "Mei Lin Lock",
            "Tri Cao",
            "Nay Oo",
            "Bryan Hooi",
            "Hoon Wei Lim"
        ],
        "published": "2024-03-04T17:38:32Z",
        "summary": "Phishing attacks have inflicted substantial losses on individuals and\nbusinesses alike, necessitating the development of robust and efficient\nautomated phishing detection approaches. Reference-based phishing detectors\n(RBPDs), which compare the logos on a target webpage to a known set of logos,\nhave emerged as the state-of-the-art approach. However, a major limitation of\nexisting RBPDs is that they rely on a manually constructed brand knowledge\nbase, making it infeasible to scale to a large number of brands, which results\nin false negative errors due to the insufficient brand coverage of the\nknowledge base. To address this issue, we propose an automated knowledge\ncollection pipeline, using which we collect and release a large-scale\nmultimodal brand knowledge base, KnowPhish, containing 20k brands with rich\ninformation about each brand. KnowPhish can be used to boost the performance of\nexisting RBPDs in a plug-and-play manner. A second limitation of existing RBPDs\nis that they solely rely on the image modality, ignoring useful textual\ninformation present in the webpage HTML. To utilize this textual information,\nwe propose a Large Language Model (LLM)-based approach to extract brand\ninformation of webpages from text. Our resulting multimodal phishing detection\napproach, KnowPhish Detector (KPD), can detect phishing webpages with or\nwithout logos. We evaluate KnowPhish and KPD on a manually validated dataset,\nand on a field study under Singapore's local context, showing substantial\nimprovements in effectiveness and efficiency compared to state-of-the-art\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.02253v1.pdf"
    },
    {
        "title": "Non-autoregressive Sequence-to-Sequence Vision-Language Models",
        "authors": [
            "Kunyu Shi",
            "Qi Dong",
            "Luis Goncalves",
            "Zhuowen Tu",
            "Stefano Soatto"
        ],
        "published": "2024-03-04T17:34:59Z",
        "summary": "Sequence-to-sequence vision-language models are showing promise, but their\napplicability is limited by their inference latency due to their autoregressive\nway of generating predictions. We propose a parallel decoding\nsequence-to-sequence vision-language model, trained with a Query-CTC loss, that\nmarginalizes over multiple inference paths in the decoder. This allows us to\nmodel the joint distribution of tokens, rather than restricting to conditional\ndistribution as in an autoregressive model. The resulting model, NARVL,\nachieves performance on-par with its state-of-the-art autoregressive\ncounterpart, but is faster at inference time, reducing from the linear\ncomplexity associated with the sequential generation of tokens to a paradigm of\nconstant time joint inference.",
        "pdf_link": "https://arxiv.org/pdf/2403.02249v1.pdf"
    },
    {
        "title": "Birbal: An efficient 7B instruct-model fine-tuned with curated datasets",
        "authors": [
            "Ashvini Kumar Jindal",
            "Pawan Kumar Rajpoot",
            "Ankur Parikh"
        ],
        "published": "2024-03-04T17:34:46Z",
        "summary": "LLMOps incur significant costs due to hardware requirements, hindering their\nwidespread accessibility. Additionally, a lack of transparency in model\ntraining methods and data contributes to the majority of models being\nnon-reproducible. To tackle these challenges, the LLM Efficiency Challenge was\nintroduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse\nset of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB)\nwithin a 24-hour timeframe. In this system description paper, we introduce\nBirbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for\n16 hours. Birbal's success lies in curating high-quality instructions covering\ndiverse tasks, resulting in a 35% performance improvement over second-best\nQwen-14B based submission.",
        "pdf_link": "https://arxiv.org/pdf/2403.02247v1.pdf"
    },
    {
        "title": "PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models",
        "authors": [
            "Fiona Anting Tan",
            "Gerard Christopher Yeo",
            "Fanyou Wu",
            "Weijie Xu",
            "Vinija Jain",
            "Aman Chadha",
            "Kokil Jaidka",
            "Yang Liu",
            "See-Kiong Ng"
        ],
        "published": "2024-03-04T17:34:34Z",
        "summary": "Recent advances in large language models (LLMs) demonstrate that their\ncapabilities are comparable, or even superior, to humans in many tasks in\nnatural language processing. Despite this progress, LLMs are still inadequate\nat social-cognitive reasoning, which humans are naturally good at. Drawing\ninspiration from psychological research on the links between certain\npersonality traits and Theory-of-Mind (ToM) reasoning, and from prompt\nengineering research on the hyper-sensitivity of prompts in affecting LLMs\ncapabilities, this study investigates how inducing personalities in LLMs using\nprompts affects their ToM reasoning capabilities. Our findings show that\ncertain induced personalities can significantly affect the LLMs' reasoning\ncapabilities in three different ToM tasks. In particular, traits from the Dark\nTriad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral\nacross the different ToM tasks. We find that LLMs that exhibit a higher\nvariance across personality prompts in ToM also tends to be more controllable\nin personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and\nMistral can be controllably adjusted through our personality prompts. In\ntoday's landscape where role-play is a common strategy when using LLMs, our\nresearch highlights the need for caution, as models that adopt specific\npersonas with personalities potentially also alter their reasoning abilities in\nan unexpected manner.",
        "pdf_link": "https://arxiv.org/pdf/2403.02246v2.pdf"
    },
    {
        "title": "Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks",
        "authors": [
            "Dimitrios Michael Manias",
            "Ali Chouman",
            "Abdallah Shami"
        ],
        "published": "2024-03-04T17:29:57Z",
        "summary": "The integration of Machine Learning and Artificial Intelligence (ML/AI) into\nfifth-generation (5G) networks has made evident the limitations of network\nintelligence with ever-increasing, strenuous requirements for current and\nnext-generation devices. This transition to ubiquitous intelligence demands\nhigh connectivity, synchronicity, and end-to-end communication between users\nand network operators, and will pave the way towards full network automation\nwithout human intervention. Intent-based networking is a key factor in the\nreduction of human actions, roles, and responsibilities while shifting towards\nnovel extraction and interpretation of automated network management. This paper\npresents the development of a custom Large Language Model (LLM) for 5G and\nnext-generation intent-based networking and provides insights into future LLM\ndevelopments and integrations to realize end-to-end intent-based networking for\nfully automated network intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.02238v1.pdf"
    },
    {
        "title": "TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models",
        "authors": [
            "Yilong Ren",
            "Yue Chen",
            "Shuai Liu",
            "Boyue Wang",
            "Haiyang Yu",
            "Zhiyong Cui"
        ],
        "published": "2024-03-04T17:08:57Z",
        "summary": "Traffic prediction constitutes a pivotal facet within the purview of\nIntelligent Transportation Systems (ITS), and the attainment of highly precise\npredictions holds profound significance for efficacious traffic management. The\nprecision of prevailing deep learning-driven traffic prediction models\ntypically sees an upward trend with a rise in the volume of training data.\nHowever, the procurement of comprehensive spatiotemporal datasets for traffic\nis often fraught with challenges, primarily stemming from the substantial costs\nassociated with data collection and retention. Consequently, developing a model\nthat can achieve accurate predictions and good generalization ability in areas\nwith limited historical traffic data is a challenging problem. It is noteworthy\nthat the rapidly advancing pretrained Large Language Models (LLMs) of recent\nyears have demonstrated exceptional proficiency in cross-modality knowledge\ntransfer and few-shot learning. Recognizing the sequential nature of traffic\ndata, similar to language, we introduce TPLLM, a novel traffic prediction\nframework leveraging LLMs. In this framework, we construct a sequence embedding\nlayer based on Convolutional Neural Networks (CNNs) and a graph embedding layer\nbased on Graph Convolutional Networks (GCNs) to extract sequence features and\nspatial features, respectively. These are subsequently integrated to form\ninputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning\napproach is applied to TPLLM, thereby facilitating efficient learning and\nminimizing computational demands. Experiments on two real-world datasets\ndemonstrate that TPLLM exhibits commendable performance in both full-sample and\nfew-shot prediction scenarios, effectively supporting the development of ITS in\nregions with scarce historical traffic data.",
        "pdf_link": "https://arxiv.org/pdf/2403.02221v2.pdf"
    },
    {
        "title": "Not all Layers of LLMs are Necessary during Inference",
        "authors": [
            "Siqi Fan",
            "Xin Jiang",
            "Xiang Li",
            "Xuying Meng",
            "Peng Han",
            "Shuo Shang",
            "Aixin Sun",
            "Yequan Wang",
            "Zhongyuan Wang"
        ],
        "published": "2024-03-04T16:23:58Z",
        "summary": "The inference phase of Large Language Models (LLMs) is very expensive. An\nideal inference stage of LLMs could utilize fewer computational resources while\nstill maintaining its capabilities (e.g., generalization and in-context\nlearning ability). In this paper, we try to answer the question, \"During LLM\ninference, can we use shallow layers for easy instances; and deep layers for\nhard ones?\" To answer this question, we first indicate that Not all Layers are\nNecessary during Inference by statistically analyzing the activated layers\nacross tasks. Then, we propose a simple algorithm named AdaInfer to determine\nthe inference termination moment based on the input instance adaptively. More\nimportantly, AdaInfer does not alter LLM parameters and maintains\ngeneralizability across tasks. Experiments on well-known LLMs (i.e., Llama2\nseries and OPT) show that AdaInfer saves an average of 14.8% of computational\nresources, even up to 50% on sentiment tasks, while maintaining comparable\nperformance. Additionally, this method is orthogonal to other model\nacceleration techniques, potentially boosting inference efficiency further.",
        "pdf_link": "https://arxiv.org/pdf/2403.02181v1.pdf"
    },
    {
        "title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models",
        "authors": [
            "Changyu Chen",
            "Xiting Wang",
            "Ting-En Lin",
            "Ang Lv",
            "Yuchuan Wu",
            "Xin Gao",
            "Ji-Rong Wen",
            "Rui Yan",
            "Yongbin Li"
        ],
        "published": "2024-03-04T16:21:54Z",
        "summary": "In reasoning tasks, even a minor error can cascade into inaccurate results,\nleading to suboptimal performance of large language models in such domains.\nEarlier fine-tuning approaches sought to mitigate this by leveraging more\nprecise supervisory signals from human labeling, larger models, or\nself-sampling, although at a high cost. Conversely, we develop a method that\navoids external resources, relying instead on introducing perturbations to the\ninput. Our training approach randomly masks certain tokens within the chain of\nthought, a technique we found to be particularly effective for reasoning tasks.\nWhen applied to fine-tuning with GSM8K, this method achieved a 5% improvement\nin accuracy over standard supervised fine-tuning with a few codes modified and\nno additional labeling effort. Furthermore, it is complementary to existing\nmethods. When integrated with related data augmentation methods, it leads to an\naverage improvement of 3% improvement in GSM8K accuracy and 1% improvement in\nMATH accuracy across five datasets of various quality and size, as well as two\nbase models. We further investigate the mechanisms behind this improvement\nthrough case studies and quantitative analysis, suggesting that our approach\nmay provide superior support for the model in capturing long-distance\ndependencies, especially those related to questions. This enhancement could\ndeepen understanding of premises in questions and prior steps. Our code is\navailable at Github.",
        "pdf_link": "https://arxiv.org/pdf/2403.02178v1.pdf"
    },
    {
        "title": "Cognition is All You Need -- The Next Layer of AI Above Large Language Models",
        "authors": [
            "Nova Spivack",
            "Sam Douglas",
            "Michelle Crames",
            "Tim Connors"
        ],
        "published": "2024-03-04T16:11:57Z",
        "summary": "Recent studies of the applications of conversational AI tools, such as\nchatbots powered by large language models, to complex real-world knowledge work\nhave shown limitations related to reasoning and multi-step problem solving.\nSpecifically, while existing chatbots simulate shallow reasoning and\nunderstanding they are prone to errors as problem complexity increases. The\nfailure of these systems to address complex knowledge work is due to the fact\nthat they do not perform any actual cognition. In this position paper, we\npresent Cognitive AI, a higher-level framework for implementing\nprogrammatically defined neuro-symbolic cognition above and outside of large\nlanguage models. Specifically, we propose a dual-layer functional architecture\nfor Cognitive AI that serves as a roadmap for AI systems that can perform\ncomplex multi-step knowledge work. We propose that Cognitive AI is a necessary\nprecursor for the evolution of higher forms of AI, such as AGI, and\nspecifically claim that AGI cannot be achieved by probabilistic approaches on\ntheir own. We conclude with a discussion of the implications for large language\nmodels, adoption cycles in AI, and commercial Cognitive AI development.",
        "pdf_link": "https://arxiv.org/pdf/2403.02164v2.pdf"
    },
    {
        "title": "Using LLMs for the Extraction and Normalization of Product Attribute Values",
        "authors": [
            "Nick Baumann",
            "Alexander Brinkmann",
            "Christian Bizer"
        ],
        "published": "2024-03-04T15:39:59Z",
        "summary": "Product offers on e-commerce websites often consist of a textual product\ntitle and a textual product description. In order to provide features such as\nfaceted product filtering or content-based product recommendation, the websites\nneed to extract attribute-value pairs from the unstructured product\ndescriptions. This paper explores the potential of using large language models\n(LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute\nvalues from product titles and product descriptions. For our experiments, we\nintroduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC\nPAVE consists of product offers from 87 websites that provide schema$.$org\nannotations. The offers belong to five different categories, each featuring a\nspecific set of attributes. The dataset provides manually verified\nattribute-value pairs in two forms: (i) directly extracted values and (ii)\nnormalized attribute values. The normalization of the attribute values requires\nsystems to perform the following types of operations: name expansion,\ngeneralization, unit of measurement normalization, and string wrangling. Our\nexperiments demonstrate that GPT-4 outperforms PLM-based extraction methods by\n10%, achieving an F1-Score of 91%. For the extraction and normalization of\nproduct attribute values, GPT-4 achieves a similar performance to the\nextraction scenario, while being particularly strong at string wrangling and\nname expansion.",
        "pdf_link": "https://arxiv.org/pdf/2403.02130v2.pdf"
    },
    {
        "title": "Large language models surpass human experts in predicting neuroscience results",
        "authors": [
            "Xiaoliang Luo",
            "Akilles Rechardt",
            "Guangzhi Sun",
            "Kevin K. Nejad",
            "Felipe Y\u00e1\u00f1ez",
            "Bati Yilmaz",
            "Kangjoo Lee",
            "Alexandra O. Cohen",
            "Valentina Borghesani",
            "Anton Pashkov",
            "Daniele Marinazzo",
            "Jonathan Nicholas",
            "Alessandro Salatiello",
            "Ilia Sucholutsky",
            "Pasquale Minervini",
            "Sepehr Razavi",
            "Roberta Rocca",
            "Elkhan Yusifov",
            "Tereza Okalova",
            "Nianlong Gu",
            "Martin Ferianc",
            "Mikail Khona",
            "Kaustubh R. Patil",
            "Pui-Shee Lee",
            "Rui Mata",
            "Nicholas E. Myers",
            "Jennifer K Bizley",
            "Sebastian Musslick",
            "Isil Poyraz Bilgin",
            "Guiomar Niso",
            "Justin M. Ales",
            "Michael Gaebler",
            "N Apurva Ratan Murty",
            "Leyla Loued-Khenissi",
            "Anna Behler",
            "Chloe M. Hall",
            "Jessica Dafflon",
            "Sherry Dongqi Bao",
            "Bradley C. Love"
        ],
        "published": "2024-03-04T15:27:59Z",
        "summary": "Scientific discoveries often hinge on synthesizing decades of research, a\ntask that potentially outstrips human information processing capacities. Large\nlanguage models (LLMs) offer a solution. LLMs trained on the vast scientific\nliterature could potentially integrate noisy yet interrelated findings to\nforecast novel results better than human experts. To evaluate this possibility,\nwe created BrainBench, a forward-looking benchmark for predicting neuroscience\nresults. We find that LLMs surpass experts in predicting experimental outcomes.\nBrainGPT, an LLM we tuned on the neuroscience literature, performed better yet.\nLike human experts, when LLMs were confident in their predictions, they were\nmore likely to be correct, which presages a future where humans and LLMs team\ntogether to make discoveries. Our approach is not neuroscience-specific and is\ntransferable to other knowledge-intensive endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2403.03230v2.pdf"
    },
    {
        "title": "Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models",
        "authors": [
            "Sargam Yadav",
            "Abhishek Kaushik",
            "Kevin McDaid"
        ],
        "published": "2024-03-04T15:27:49Z",
        "summary": "The advent of Large Language Models (LLMs) has advanced the benchmark in\nvarious Natural Language Processing (NLP) tasks. However, large amounts of\nlabelled training data are required to train LLMs. Furthermore, data annotation\nand training are computationally expensive and time-consuming. Zero and\nfew-shot learning have recently emerged as viable options for labelling data\nusing large pre-trained models. Hate speech detection in mix-code low-resource\nlanguages is an active problem area where the use of LLMs has proven\nbeneficial. In this study, we have compiled a dataset of 100 YouTube comments,\nand weakly labelled them for coarse and fine-grained misogyny classification in\nmix-code Hinglish. Weak annotation was applied due to the labor-intensive\nannotation process. Zero-shot learning, one-shot learning, and few-shot\nlearning and prompting approaches have then been applied to assign labels to\nthe comments and compare them to human-assigned labels. Out of all the\napproaches, zero-shot classification using the Bidirectional Auto-Regressive\nTransformers (BART) large model and few-shot prompting using Generative\nPre-trained Transformer- 3 (ChatGPT-3) achieve the best results",
        "pdf_link": "https://arxiv.org/pdf/2403.02121v1.pdf"
    },
    {
        "title": "adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds",
        "authors": [
            "S\u00e9amus Lankford",
            "Haithem Afli",
            "Andy Way"
        ],
        "published": "2024-03-04T14:49:18Z",
        "summary": "The advent of Multilingual Language Models (MLLMs) and Large Language Models\nhas spawned innovation in many areas of natural language processing. Despite\nthe exciting potential of this technology, its impact on developing\nhigh-quality Machine Translation (MT) outputs for low-resource languages\nremains relatively under-explored. Furthermore, an open-source application,\ndedicated to both fine-tuning MLLMs and managing the complete MT workflow for\nlow-resources languages, remains unavailable. We aim to address these\nimbalances through the development of adaptMLLM, which streamlines all\nprocesses involved in the fine-tuning of MLLMs for MT. This open-source\napplication is tailored for developers, translators, and users who are engaged\nin MT. An intuitive interface allows for easy customisation of hyperparameters,\nand the application offers a range of metrics for model evaluation and the\ncapability to deploy models as a translation service directly within the\napplication. As a multilingual tool, we used adaptMLLM to fine-tune models for\ntwo low-resource language pairs: English to Irish (EN$\\leftrightarrow$GA) and\nEnglish to Marathi (EN$\\leftrightarrow$MR). Compared with baselines from the\nLoResMT2021 Shared Task, the adaptMLLM system demonstrated significant\nimprovements. In the EN$\\rightarrow$GA direction, an improvement of 5.2 BLEU\npoints was observed and an increase of 40.5 BLEU points was recorded in the\nGA$\\rightarrow$EN direction. Significant improvements in the translation\nperformance of the EN$\\leftrightarrow$MR pair were also observed notably in the\nMR$\\rightarrow$EN direction with an increase of 21.3 BLEU points. Finally, a\nfine-grained human evaluation of the MLLM output on the EN$\\rightarrow$GA pair\nwas conducted using the Multidimensional Quality Metrics and Scalar Quality\nMetrics error taxonomies. The application and models are freely available.",
        "pdf_link": "https://arxiv.org/pdf/2403.02370v1.pdf"
    },
    {
        "title": "Automated Generation of Multiple-Choice Cloze Questions for Assessing English Vocabulary Using GPT-turbo 3.5",
        "authors": [
            "Qiao Wang",
            "Ralph Rose",
            "Naho Orita",
            "Ayaka Sugawara"
        ],
        "published": "2024-03-04T14:24:47Z",
        "summary": "A common way of assessing language learners' mastery of vocabulary is via\nmultiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of\ntest items can be laborious for individual teachers or in large-scale language\nprograms. In this paper, we evaluate a new method for automatically generating\nthese types of questions using large language models (LLM). The VocaTT\n(vocabulary teaching and training) engine is written in Python and comprises\nthree basic steps: pre-processing target word lists, generating sentences and\ncandidate word options using GPT, and finally selecting suitable word options.\nTo test the efficiency of this system, 60 questions were generated targeting\nacademic words. The generated items were reviewed by expert reviewers who\njudged the well-formedness of the sentences and word options, adding comments\nto items judged not well-formed. Results showed a 75% rate of well-formedness\nfor sentences and 66.85% rate for suitable word options. This is a marked\nimprovement over the generator used earlier in our research which did not take\nadvantage of GPT's capabilities. Post-hoc qualitative analysis reveals several\npoints for improvement in future work including cross-referencing\npart-of-speech tagging, better sentence validation, and improving GPT prompts.",
        "pdf_link": "https://arxiv.org/pdf/2403.02078v1.pdf"
    },
    {
        "title": "Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications?",
        "authors": [
            "Yotam Intrator",
            "Matan Halfon",
            "Roman Goldenberg",
            "Reut Tsarfaty",
            "Matan Eyal",
            "Ehud Rivlin",
            "Yossi Matias",
            "Natalia Aizenberg"
        ],
        "published": "2024-03-04T14:01:11Z",
        "summary": "Large language models hold significant promise in multilingual applications.\nHowever, inherent biases stemming from predominantly English-centric\npre-training have led to the widespread practice of pre-translation, i.e.,\ntranslating non-English inputs to English before inference, leading to\ncomplexity and information loss. This study re-evaluates the need for\npre-translation in the context of PaLM2 models (Anil et al., 2023), which have\nbeen established as highly performant in multilingual tasks. We offer a\ncomprehensive investigation across 108 languages and 6 diverse benchmarks,\nincluding open-end generative tasks, which were excluded from previous similar\nstudies. Our findings challenge the pre-translation paradigm established in\nprior research, highlighting the advantages of direct inference in PaLM2.\nSpecifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108\nlanguages. These findings pave the way for more efficient and effective\nmultilingual applications, alleviating the limitations associated with\npre-translation and unlocking linguistic authenticity.",
        "pdf_link": "https://arxiv.org/pdf/2403.04792v1.pdf"
    },
    {
        "title": "Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism",
        "authors": [
            "Shuvayan Brahmachary",
            "Subodh M. Joshi",
            "Aniruddha Panda",
            "Kaushik Koneripalli",
            "Arun Kumar Sagotra",
            "Harshil Patel",
            "Ankush Sharma",
            "Ameya D. Jagtap",
            "Kaushic Kalyanaraman"
        ],
        "published": "2024-03-04T13:57:37Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\nabilities, prompting interest in their application as black-box optimizers.\nThis paper asserts that LLMs possess the capability for zero-shot optimization\nacross diverse scenarios, including multi-objective and high-dimensional\nproblems. We introduce a novel population-based method for numerical\noptimization using LLMs called Language-Model-Based Evolutionary Optimizer\n(LEO). Our hypothesis is supported through numerical examples, spanning\nbenchmark and industrial engineering problems such as supersonic nozzle shape\noptimization, heat transfer, and windfarm layout optimization. We compare our\nmethod to several gradient-based and gradient-free optimization approaches.\nWhile LLMs yield comparable results to state-of-the-art methods, their\nimaginative nature and propensity to hallucinate demand careful handling. We\nprovide practical guidelines for obtaining reliable answers from LLMs and\ndiscuss method limitations and potential research directions.",
        "pdf_link": "https://arxiv.org/pdf/2403.02054v1.pdf"
    },
    {
        "title": "Unveiling Hidden Links Between Unseen Security Entities",
        "authors": [
            "Daniel Alfasi",
            "Tal Shapira",
            "Anat Bremler Barr"
        ],
        "published": "2024-03-04T13:14:39Z",
        "summary": "The proliferation of software vulnerabilities poses a significant challenge\nfor security databases and analysts tasked with their timely identification,\nclassification, and remediation. With the National Vulnerability Database (NVD)\nreporting an ever-increasing number of vulnerabilities, the traditional manual\nanalysis becomes untenably time-consuming and prone to errors. This paper\nintroduces VulnScopper, an innovative approach that utilizes multi-modal\nrepresentation learning, combining Knowledge Graphs (KG) and Natural Language\nProcessing (NLP), to automate and enhance the analysis of software\nvulnerabilities. Leveraging ULTRA, a knowledge graph foundation model, combined\nwith a Large Language Model (LLM), VulnScopper effectively handles unseen\nentities, overcoming the limitations of previous KG approaches. We evaluate\nVulnScopper on two major security datasets, the NVD and the Red Hat CVE\ndatabase. Our method significantly improves the link prediction accuracy\nbetween Common Vulnerabilities and Exposures (CVEs), Common Weakness\nEnumeration (CWEs), and Common Platform Enumerations (CPEs). Our results show\nthat VulnScopper outperforms existing methods, achieving up to 78% Hits@10\naccuracy in linking CVEs to CPEs and CWEs and presenting an 11.7% improvement\nover large language models in predicting CWE labels based on the Red Hat\ndatabase. Based on the NVD, only 6.37% of the linked CPEs are being published\nduring the first 30 days; many of them are related to critical and high-risk\nvulnerabilities which, according to multiple compliance frameworks (such as\nCISA and PCI), should be remediated within 15-30 days. Our model can uncover\nnew products linked to vulnerabilities, reducing remediation time and improving\nvulnerability management. We analyzed several CVEs from 2023 to showcase this\nability.",
        "pdf_link": "https://arxiv.org/pdf/2403.02014v1.pdf"
    },
    {
        "title": "Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?",
        "authors": [
            "Vasudevan Nedumpozhimana",
            "John D. Kelleher"
        ],
        "published": "2024-03-04T13:10:08Z",
        "summary": "Transformer-based Neural Language Models achieve state-of-the-art performance\non various natural language processing tasks. However, an open question is the\nextent to which these models rely on word-order/syntactic or word\nco-occurrence/topic-based information when processing natural language. This\nwork contributes to this debate by addressing the question of whether these\nmodels primarily use topic as a signal, by exploring the relationship between\nTransformer-based models' (BERT and RoBERTa's) performance on a range of\nprobing tasks in English, from simple lexical tasks such as sentence length\nprediction to complex semantic tasks such as idiom token identification, and\nthe sensitivity of these tasks to the topic information. To this end, we\npropose a novel probing method which we call topic-aware probing. Our initial\nresults indicate that Transformer-based models encode both topic and non-topic\ninformation in their intermediate layers, but also that the facility of these\nmodels to distinguish idiomatic usage is primarily based on their ability to\nidentify and encode topic. Furthermore, our analysis of these models'\nperformance on other standard probing tasks suggests that tasks that are\nrelatively insensitive to the topic information are also tasks that are\nrelatively difficult for these models.",
        "pdf_link": "https://arxiv.org/pdf/2403.02009v1.pdf"
    },
    {
        "title": "LLM-Oriented Retrieval Tuner",
        "authors": [
            "Si Sun",
            "Hanqing Zhang",
            "Zhiyuan Liu",
            "Jie Bao",
            "Dawei Song"
        ],
        "published": "2024-03-04T12:50:25Z",
        "summary": "Dense Retrieval (DR) is now considered as a promising tool to enhance the\nmemorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by\nincorporating external memories. However, due to the paradigm discrepancy\nbetween text generation of LLM and DR, it is still an open challenge to\nintegrate the retrieval and generation tasks in a shared LLM. In this paper, we\npropose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which\ndecouples DR capacity from base LLM and non-invasively coordinates the\noptimally aligned and uniform layers of the LLM towards a unified DR space,\nachieving an efficient and effective DR without tuning the LLM itself. The\nextensive experiments on six BEIR datasets show that our approach could achieve\ncompetitive zero-shot retrieval performance compared to a range of strong DR\nmodels while maintaining the generation ability of LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.01999v1.pdf"
    },
    {
        "title": "Vanilla Transformers are Transfer Capability Teachers",
        "authors": [
            "Xin Lu",
            "Yanyan Zhao",
            "Bing Qin"
        ],
        "published": "2024-03-04T12:40:28Z",
        "summary": "Recently, Mixture of Experts (MoE) Transformers have garnered increasing\nattention due to their advantages in model capacity and computational\nefficiency. However, studies have indicated that MoE Transformers underperform\nvanilla Transformers in many downstream tasks, significantly diminishing the\npractical value of MoE models. To explain this issue, we propose that the\npre-training performance and transfer capability of a model are joint\ndeterminants of its downstream task performance. MoE models, in comparison to\nvanilla models, have poorer transfer capability, leading to their subpar\nperformance in downstream tasks. To address this issue, we introduce the\nconcept of transfer capability distillation, positing that although vanilla\nmodels have weaker performance, they are effective teachers of transfer\ncapability. The MoE models guided by vanilla models can achieve both strong\npre-training performance and transfer capability, ultimately enhancing their\nperformance in downstream tasks. We design a specific distillation method and\nconduct experiments on the BERT architecture. Experimental results show a\nsignificant improvement in downstream performance of MoE models, and many\nfurther evidences also strongly support the concept of transfer capability\ndistillation. Finally, we attempt to interpret transfer capability distillation\nand provide some insights from the perspective of model feature.",
        "pdf_link": "https://arxiv.org/pdf/2403.01994v1.pdf"
    },
    {
        "title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
        "authors": [
            "Hengxing Cai",
            "Xiaochen Cai",
            "Junhan Chang",
            "Sihang Li",
            "Lin Yao",
            "Changxin Wang",
            "Zhifeng Gao",
            "Hongshuai Wang",
            "Yongge Li",
            "Mujie Lin",
            "Shuwen Yang",
            "Jiankun Wang",
            "Yuqi Yin",
            "Yaqi Li",
            "Linfeng Zhang",
            "Guolin Ke"
        ],
        "published": "2024-03-04T12:19:28Z",
        "summary": "Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nnatural language understanding and generation, igniting a surge of interest in\nleveraging these technologies in the field of scientific literature analysis.\nExisting benchmarks, however, inadequately evaluate the proficiency of LLMs in\nscientific literature analysis, especially in scenarios involving complex\ncomprehension and multimodal data. In response, we introduced SciAssess, a\nbenchmark tailored for the in-depth analysis of scientific literature, crafted\nto provide a thorough assessment of LLMs' efficacy. SciAssess focuses on\nevaluating LLMs' abilities in memorization, comprehension, and analysis within\nthe context of scientific literature analysis. It includes representative tasks\nfrom diverse scientific fields, such as general chemistry, organic materials,\nand alloy materials. And rigorous quality control measures ensure its\nreliability in terms of correctness, anonymization, and copyright compliance.\nSciAssess evaluates leading LLMs, including GPT-4, GPT-3.5, and Gemini,\nidentifying their strengths and aspects for improvement and supporting the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are made available at https://sci-assess.github.io,\noffering a valuable tool for advancing LLM capabilities in scientific\nliterature analysis.",
        "pdf_link": "https://arxiv.org/pdf/2403.01976v2.pdf"
    },
    {
        "title": "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
        "authors": [
            "Derong Xu",
            "Ziheng Zhang",
            "Zhenxi Lin",
            "Xian Wu",
            "Zhihong Zhu",
            "Tong Xu",
            "Xiangyu Zhao",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "published": "2024-03-04T12:16:15Z",
        "summary": "Knowledge graph completion (KGC) is a widely used method to tackle\nincompleteness in knowledge graphs (KGs) by making predictions for missing\nlinks. Description-based KGC leverages pre-trained language models to learn\nentity and relation representations with their names or descriptions, which\nshows promising results. However, the performance of description-based KGC is\nstill limited by the quality of text and the incomplete structure, as it lacks\nsufficient entity descriptions and relies solely on relation names, leading to\nsub-optimal results. To address this issue, we propose MPIKGC, a general\nframework to compensate for the deficiency of contextualized knowledge and\nimprove KGC by querying large language models (LLMs) from various perspectives,\nwhich involves leveraging the reasoning, explanation, and summarization\ncapabilities of LLMs to expand entity descriptions, understand relations, and\nextract structures, respectively. We conducted extensive evaluation of the\neffectiveness and improvement of our framework based on four description-based\nKGC models and four datasets, for both link prediction and triplet\nclassification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.01972v1.pdf"
    },
    {
        "title": "AS-ES Learning: Towards Efficient CoT Learning in Small Models",
        "authors": [
            "Nuwa Xi",
            "Yuhan Chen",
            "Sendong Zhao",
            "Haochun Wang",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2024-03-04T12:13:59Z",
        "summary": "Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs,\nespecially when it comes to logical reasoning. Attempts have been made to\ninduce such ability in small models as well by distilling from the data with\nCoT generated by Large Language Models (LLMs). However, existing methods often\nsimply generate and incorporate more data from LLMs and fail to note the\nimportance of efficiently utilizing existing CoT data. We here propose a new\ntraining paradigm AS-ES (Abstractive Segments - Extractive Segments) learning,\nwhich exploits the inherent information in CoT for iterative generation.\nExperiments show that our methods surpass the direct seq2seq training on\nCoT-extensive tasks like MWP and PET summarization, without data augmentation\nor altering the model itself. Furthermore, we explore the reason behind the\ninefficiency of small models in learning CoT and provide an explanation of why\nAS-ES learning works, giving insights into the underlying mechanism of CoT.",
        "pdf_link": "https://arxiv.org/pdf/2403.01969v1.pdf"
    },
    {
        "title": "Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?",
        "authors": [
            "Evgeniia Razumovskaia",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "published": "2024-03-04T10:48:13Z",
        "summary": "Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and\nin-context learning (ICL) are three alternative, de facto standard approaches\nto few-shot learning. ICL has gained popularity recently with the advent of\nLLMs due to its simplicity and sample efficiency. Prior research has conducted\nonly limited investigation into how these approaches work for multilingual\nfew-shot learning, and the focus so far has been mostly on their performance.\nIn this work, we present an extensive and systematic comparison of the three\napproaches, testing them on 6 high- and low-resource languages, three different\nNLU tasks, and a myriad of language and domain setups. Importantly, performance\nis only one aspect of the comparison, where we also analyse the approaches\nthrough the optics of their computational, inference and financial costs. Our\nobservations show that supervised instruction tuning has the best trade-off\nbetween performance and resource requirements. As another contribution, we\nanalyse the impact of target language adaptation of pretrained LLMs and find\nthat the standard adaptation approaches can (superficially) improve target\nlanguage generation capabilities, but language understanding elicited through\nICL does not improve and remains limited, with low scores especially for\nlow-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2403.01929v1.pdf"
    },
    {
        "title": "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering",
        "authors": [
            "Giacomo Frisoni",
            "Alessio Cocchieri",
            "Alex Presepi",
            "Gianluca Moro",
            "Zaiqiao Meng"
        ],
        "published": "2024-03-04T10:41:52Z",
        "summary": "Medical open-domain question answering demands substantial access to\nspecialized knowledge. Recent efforts have sought to decouple knowledge from\nmodel parameters, counteracting architectural scaling and allowing for training\non common low-resource hardware. The retrieve-then-read paradigm has become\nubiquitous, with model predictions grounded on relevant knowledge pieces from\nexternal repositories such as PubMed, textbooks, and UMLS. An alternative path,\nstill under-explored but made possible by the advent of domain-specific large\nlanguage models, entails constructing artificial contexts through prompting. As\na result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's\ndilemma. This paper presents MedGENIE, the first generate-then-read framework\nfor multiple-choice question answering in medicine. We conduct extensive\nexperiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical\nperspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new\nstate-of-the-art (SOTA) in the open-book setting of each testbed, even allowing\na small-scale reader to outcompete zero-shot closed-book 175B baselines while\nusing up to 706$\\times$ fewer parameters. Overall, our findings reveal that\ngenerated passages are more effective than retrieved counterparts in attaining\nhigher accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2403.01924v1.pdf"
    },
    {
        "title": "LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset",
        "authors": [
            "Ahmed Izzidien",
            "Holli Sargeant",
            "Felix Steffek"
        ],
        "published": "2024-03-04T10:13:30Z",
        "summary": "To undertake computational research of the law, efficiently identifying\ndatasets of court decisions that relate to a specific legal issue is a crucial\nyet challenging endeavour. This study addresses the gap in the literature\nworking with large legal corpora about how to isolate cases, in our case\nsummary judgments, from a large corpus of UK court decisions. We introduce a\ncomparative analysis of two computational methods: (1) a traditional natural\nlanguage processing-based approach leveraging expert-generated keywords and\nlogical operators and (2) an innovative application of the Claude 2 large\nlanguage model to classify cases based on content-specific prompts. We use the\nCambridge Law Corpus of 356,011 UK court decisions and determine that the large\nlanguage model achieves a weighted F1 score of 0.94 versus 0.78 for keywords.\nDespite iterative refinement, the search logic based on keywords fails to\ncapture nuances in legal language. We identify and extract 3,102 summary\njudgment cases, enabling us to map their distribution across various UK courts\nover a temporal span. The paper marks a pioneering step in employing advanced\nnatural language processing to tackle core legal research tasks, demonstrating\nhow these technologies can bridge systemic gaps and enhance the accessibility\nof legal information. We share the extracted dataset metrics to support further\nresearch on summary judgments.",
        "pdf_link": "https://arxiv.org/pdf/2403.04791v1.pdf"
    },
    {
        "title": "Online Training of Large Language Models: Learn while chatting",
        "authors": [
            "Juhao Liang",
            "Ziwei Wang",
            "Zhuoheng Ma",
            "Jianquan Li",
            "Zhiyi Zhang",
            "Xiangbo Wu",
            "Benyou Wang"
        ],
        "published": "2024-03-04T10:00:55Z",
        "summary": "Large Language Models(LLMs) have dramatically revolutionized the field of\nNatural Language Processing(NLP), offering remarkable capabilities that have\ngarnered widespread usage. However, existing interaction paradigms between LLMs\nand users are constrained by either inflexibility, limitations in\ncustomization, or a lack of persistent learning. This inflexibility is\nparticularly evident as users, especially those without programming skills,\nhave restricted avenues to enhance or personalize the model. Existing\nframeworks further complicate the model training and deployment process due to\ntheir computational inefficiencies and lack of user-friendly interfaces. To\novercome these challenges, this paper introduces a novel interaction\nparadigm-'Online Training using External Interactions'-that merges the benefits\nof persistent, real-time model updates with the flexibility for individual\ncustomization through external interactions such as AI agents or online/offline\nknowledge bases.",
        "pdf_link": "https://arxiv.org/pdf/2403.04790v1.pdf"
    },
    {
        "title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Conrad Borchers",
            "John Sabatini",
            "John Hollander",
            "Meng Cao",
            "Xiangen Hu"
        ],
        "published": "2024-03-04T08:14:07Z",
        "summary": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\nliteracy training, a key factor for societal participation, employment\nopportunities, and lifelong learning. Our study investigates the application of\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\npredicting learning performance in adult literacy programs in ITSs. This\nresearch is motivated by the potential of LLMs to predict learning performance\nbased on its inherent reasoning and computational capabilities. By using\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\npredictive capabilities of GPT-4 versus traditional machine learning methods in\npredicting learning performance through five-fold cross-validation techniques.\nOur findings show that the GPT-4 presents the competitive predictive abilities\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\nperformance compared to local machine execution. Moreover, our investigation\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\nperformance, albeit with less stability in the automated approach, using\nXGBoost as the case study. Our study contributes to the field by highlighting\nthe potential of integrating LLMs with traditional machine learning models to\nenhance predictive accuracy and personalize adult literacy education, setting a\nfoundation for future research in applying LLMs within ITSs.",
        "pdf_link": "https://arxiv.org/pdf/2403.14668v1.pdf"
    },
    {
        "title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
        "authors": [
            "Zhenru Lin",
            "Yiqun Yao",
            "Yang Yuan"
        ],
        "published": "2024-03-04T07:26:07Z",
        "summary": "Large language models (LLMs) such as ChatGPT are increasingly proficient in\nunderstanding and generating a mixture of code and text. Evaluation based on\nsuch $\\textit{mixture}$ can lead to a more comprehensive understanding of the\nmodels' abilities in solving coding problems. However, in this context, current\nevaluation methods are either limited in task coverage or lack standardization.\nTo address this issue, we propose using category theory as a framework for\nevaluation. Specifically, morphisms within a code category can represent code\ndebugging and transformation, functors between two categories represent code\ntranslation, and functors between a code category and a natural language\ncategory represent code generation, explanation, and reproduction. We present\nan automatic evaluation framework called $\\textbf{CatCode}$\n($\\textbf{Cat}$egory $\\textbf{Code}$) that can comprehensively assess the\ncoding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX.",
        "pdf_link": "https://arxiv.org/pdf/2403.01784v1.pdf"
    },
    {
        "title": "NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models",
        "authors": [
            "Lizhou Fan",
            "Wenyue Hua",
            "Xiang Li",
            "Kaijie Zhu",
            "Mingyu Jin",
            "Lingyao Li",
            "Haoyang Ling",
            "Jinkui Chi",
            "Jindong Wang",
            "Xin Ma",
            "Yongfeng Zhang"
        ],
        "published": "2024-03-04T07:10:31Z",
        "summary": "Understanding the reasoning capabilities of Multimodal Large Language Models\n(MLLMs) is an important area of research. In this study, we introduce a dynamic\nbenchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating\nthe pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to\ndisentangle the effect of various factors such as image recognition and\ninstruction following, from the overall performance of the models, allowing us\nto focus solely on evaluating their reasoning abilities. It is built by\nconverting textual description of questions from NPHardEval to image\nrepresentations. Our findings reveal significant discrepancies in reasoning\nabilities across different models and highlight the relatively weak performance\nof MLLMs compared to LLMs in terms of reasoning. We also investigate the impact\nof different prompting styles, including visual, text, and combined visual and\ntext prompts, on the reasoning abilities of MLLMs, demonstrating the different\nimpacts of multimodal inputs in model performance. Unlike traditional\nbenchmarks, which focus primarily on static evaluations, our benchmark will be\nupdated monthly to prevent overfitting and ensure a more authentic and\nfine-grained evaluation of the models. We believe that this benchmark can aid\nin understanding and guide the further development of reasoning abilities in\nMLLMs. The benchmark dataset and code are available at\nhttps://github.com/lizhouf/NPHardEval4V",
        "pdf_link": "https://arxiv.org/pdf/2403.01777v2.pdf"
    },
    {
        "title": "WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search Results with Citations",
        "authors": [
            "Haolin Deng",
            "Chang Wang",
            "Xin Li",
            "Dezhang Yuan",
            "Junlang Zhan",
            "Tianhua Zhou",
            "Jin Ma",
            "Jun Gao",
            "Ruifeng Xu"
        ],
        "published": "2024-03-04T07:06:41Z",
        "summary": "Enhancing the attribution in large language models (LLMs) is a crucial task.\nOne feasible approach is to enable LLMs to cite external sources that support\ntheir generations. However, existing datasets and evaluation methods in this\ndomain still exhibit notable limitations. In this work, we formulate the task\nof attributed query-focused summarization (AQFS) and present WebCiteS, a\nChinese dataset featuring 7k human-annotated summaries with citations. WebCiteS\nderives from real-world user queries and web search results, offering a\nvaluable resource for model training and evaluation. Prior works in attribution\nevaluation do not differentiate between groundedness errors and citation\nerrors. They also fall short in automatically verifying sentences that draw\npartial support from multiple sources. We tackle these issues by developing\ndetailed metrics and enabling the automatic evaluator to decompose the\nsentences into sub-claims for fine-grained verification. Our comprehensive\nevaluation of both open-source and proprietary models on WebCiteS highlights\nthe challenge LLMs face in correctly citing sources, underscoring the necessity\nfor further improvement. The dataset and code will be open-sourced to\nfacilitate further research in this crucial field.",
        "pdf_link": "https://arxiv.org/pdf/2403.01774v1.pdf"
    },
    {
        "title": "How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems",
        "authors": [
            "Yuxiao Huang",
            "Wenjie Zhang",
            "Liang Feng",
            "Xingyu Wu",
            "Kay Chen Tan"
        ],
        "published": "2024-03-04T06:24:21Z",
        "summary": "Recently, large language models (LLMs) have notably positioned them as\ncapable tools for addressing complex optimization challenges. Despite this\nrecognition, a predominant limitation of existing LLM-based optimization\nmethods is their struggle to capture the relationships among decision variables\nwhen relying exclusively on numerical text prompts, especially in\nhigh-dimensional problems. Keeping this in mind, we first propose to enhance\nthe optimization performance using multimodal LLM capable of processing both\ntextual and visual prompts for deeper insights of the processed optimization\nproblem. This integration allows for a more comprehensive understanding of\noptimization problems, akin to human cognitive processes. We have developed a\nmultimodal LLM-based optimization framework that simulates human\nproblem-solving workflows, thereby offering a more nuanced and effective\nanalysis. The efficacy of this method is evaluated through extensive empirical\nstudies focused on a well-known combinatorial optimization problem, i.e.,\ncapacitated vehicle routing problem. The results are compared against those\nobtained from the LLM-based optimization algorithms that rely solely on textual\nprompts, demonstrating the significant advantages of our multimodal approach.",
        "pdf_link": "https://arxiv.org/pdf/2403.01757v1.pdf"
    },
    {
        "title": "Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models",
        "authors": [
            "Feihu Jin",
            "Yin Liu",
            "Ying Tan"
        ],
        "published": "2024-03-04T06:20:31Z",
        "summary": "Parameter-efficient tuning methods such as LoRA could achieve comparable\nperformance to model tuning by tuning a small portion of the parameters.\nHowever, substantial computational resources are still required, as this\nprocess involves calculating gradients and performing back-propagation\nthroughout the model. Much effort has recently been devoted to utilizing the\nderivative-free optimization method to eschew the computation of gradients and\nshowcase an augmented level of robustness in few-shot settings. In this paper,\nwe prepend the low-rank modules into each self-attention layer of the model and\nemploy two derivative-free optimization methods to optimize these low-rank\nmodules at each layer alternately. Extensive results on various tasks and\nlanguage models demonstrate that our proposed method achieves substantial\nimprovement and exhibits clear advantages in memory usage and convergence speed\ncompared to existing gradient-based parameter-efficient tuning and\nderivative-free optimization methods in few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.01754v1.pdf"
    },
    {
        "title": "Differentially Private Synthetic Data via Foundation Model APIs 2: Text",
        "authors": [
            "Chulin Xie",
            "Zinan Lin",
            "Arturs Backurs",
            "Sivakanth Gopi",
            "Da Yu",
            "Huseyin A Inan",
            "Harsha Nori",
            "Haotian Jiang",
            "Huishuai Zhang",
            "Yin Tat Lee",
            "Bo Li",
            "Sergey Yekhanin"
        ],
        "published": "2024-03-04T05:57:50Z",
        "summary": "Text data has become extremely valuable due to the emergence of machine\nlearning algorithms that learn from it. A lot of high-quality text data\ngenerated in the real world is private and therefore cannot be shared or used\nfreely due to privacy concerns. Generating synthetic replicas of private text\ndata with a formal privacy guarantee, i.e., differential privacy (DP), offers a\npromising and scalable solution. However, existing methods necessitate DP\nfinetuning of large language models (LLMs) on private data to generate DP\nsynthetic data. This approach is not viable for proprietary LLMs (e.g.,\nGPT-3.5) and also demands considerable computational resources for open-source\nLLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)\nalgorithm to generate DP synthetic images with only API access to diffusion\nmodels. In this work, we propose an augmented PE algorithm, named Aug-PE, that\napplies to the complex setting of text. We use API access to an LLM and\ngenerate DP synthetic text without any model training. We conduct comprehensive\nexperiments on three benchmark datasets. Our results demonstrate that Aug-PE\nproduces DP synthetic text that yields competitive utility with the SOTA DP\nfinetuning baselines. This underscores the feasibility of relying solely on API\naccess of LLMs to produce high-quality DP synthetic texts, thereby facilitating\nmore accessible routes to privacy-preserving LLM applications. Our code and\ndata are available at https://github.com/AI-secure/aug-pe.",
        "pdf_link": "https://arxiv.org/pdf/2403.01749v1.pdf"
    },
    {
        "title": "Decode Neural signal as Speech",
        "authors": [
            "Yiqian Yang",
            "Yiqun Duan",
            "Qiang Zhang",
            "Renjing Xu",
            "Hui Xiong"
        ],
        "published": "2024-03-04T05:55:01Z",
        "summary": "Decoding language from brain dynamics is an important open direction in the\nrealm of brain-computer interface (BCI), especially considering the rapid\ngrowth of large language models. Compared to invasive-based signals which\nrequire electrode implantation surgery, non-invasive neural signals (e.g. EEG,\nMEG) have attracted increasing attention considering their safety and\ngenerality. However, the exploration is not adequate in three aspects: 1)\nprevious methods mainly focus on EEG but none of the previous works address\nthis problem on MEG with better signal quality; 2) prior works have\npredominantly used ``teacher-forcing\" during generative decoding, which is\nimpractical; 3) prior works are mostly ``BART-based\" not fully auto-regressive,\nwhich performs better in other sequence tasks. In this paper, we explore the\nbrain-to-text translation of MEG signals in a speech-decoding formation. Here\nwe are the first to investigate a cross-attention-based ``whisper\" model for\ngenerating text directly from MEG signals without teacher forcing. Our model\nachieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \\&\nteacher-forcing on two major datasets (\\textit{GWilliams} and\n\\textit{Schoffelen}). This paper conducts a comprehensive review to understand\nhow speech decoding formation performs on the neural decoding tasks, including\npretraining initialization, training \\& evaluation set splitting, augmentation,\nand scaling law.",
        "pdf_link": "https://arxiv.org/pdf/2403.01748v2.pdf"
    },
    {
        "title": "Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study",
        "authors": [
            "Rudra Dhar",
            "Karthik Vaidhyanathan",
            "Vasudeva Varma"
        ],
        "published": "2024-03-04T03:56:14Z",
        "summary": "Architectural Knowledge Management (AKM) involves the organized handling of\ninformation related to architectural decisions and design within a project or\norganization. An essential artifact of AKM is the Architecture Decision Records\n(ADR), which documents key design decisions. ADRs are documents that capture\ndecision context, decision made and various aspects related to a design\ndecision, thereby promoting transparency, collaboration, and understanding.\nDespite their benefits, ADR adoption in software development has been slow due\nto challenges like time constraints and inconsistent uptake. Recent\nadvancements in Large Language Models (LLMs) may help bridge this adoption gap\nby facilitating ADR generation. However, the effectiveness of LLM for ADR\ngeneration or understanding is something that has not been explored. To this\nend, in this work, we perform an exploratory study that aims to investigate the\nfeasibility of using LLM for the generation of ADRs given the decision context.\nIn our exploratory study, we utilize GPT and T5-based models with 0-shot,\nfew-shot, and fine-tuning approaches to generate the Decision of an ADR given\nits Context. Our results indicate that in a 0-shot setting, state-of-the-art\nmodels such as GPT-4 generate relevant and accurate Design Decisions, although\nthey fall short of human-level performance. Additionally, we observe that more\ncost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot\nsetting, and smaller models such as Flan-T5 can yield comparable results after\nfine-tuning. To conclude, this exploratory study suggests that LLM can generate\nDesign Decisions, but further research is required to attain human-level\ngeneration and establish standardized widespread adoption.",
        "pdf_link": "https://arxiv.org/pdf/2403.01709v1.pdf"
    },
    {
        "title": "Improving LLM Code Generation with Grammar Augmentation",
        "authors": [
            "Shubham Ugare",
            "Tarun Suresh",
            "Hangoo Kang",
            "Sasa Misailovic",
            "Gagandeep Singh"
        ],
        "published": "2024-03-03T22:38:35Z",
        "summary": "We present SynCode a novel framework for efficient and general syntactical\ndecoding of code with large language models (LLMs). SynCode leverages the\ngrammar of a programming language, utilizing an offline-constructed efficient\nlookup table called DFA mask store based on language grammar terminals. We\ndemonstrate SynCode's soundness and completeness given the context-free grammar\n(CFG) of the programming language, presenting its ability to retain\nsyntactically valid tokens while rejecting invalid ones. The framework\nseamlessly integrates with any language defined by CFG, as evidenced by\nexperiments on CFGs for Python and Go. The results underscore the significant\nreduction of 96.07% of syntax errors achieved when SynCode is combined with\nstate-of-the-art LLMs, showcasing its substantial impact on enhancing\nsyntactical precision in code generation.\n  Our code is available at https://github.com/uiuc-focal-lab/syncode.",
        "pdf_link": "https://arxiv.org/pdf/2403.01632v1.pdf"
    },
    {
        "title": "Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models",
        "authors": [
            "Nguyen Quang Duc",
            "Le Hai Son",
            "Nguyen Duc Nhan",
            "Nguyen Dich Nhat Minh",
            "Le Thanh Huong",
            "Dinh Viet Sang"
        ],
        "published": "2024-03-03T21:24:35Z",
        "summary": "This paper presents our contributions towards advancing the state of\nVietnamese language understanding and generation through the development and\ndissemination of open datasets and pre-trained models for Vietnamese\nRetrieval-Augmented Generation (RAG) and Large Language Models (LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2403.01616v2.pdf"
    },
    {
        "title": "SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos",
        "authors": [
            "Yulei Niu",
            "Wenliang Guo",
            "Long Chen",
            "Xudong Lin",
            "Shih-Fu Chang"
        ],
        "published": "2024-03-03T19:53:06Z",
        "summary": "We study the problem of procedure planning in instructional videos, which\naims to make a goal-oriented sequence of action steps given partial visual\nstate observations. The motivation of this problem is to learn a structured and\nplannable state and action space. Recent works succeeded in sequence modeling\nof steps with only sequence-level annotations accessible during training, which\noverlooked the roles of states in the procedures. In this work, we point out\nthat State CHangEs MAtter (SCHEMA) for procedure planning in instructional\nvideos. We aim to establish a more structured state space by investigating the\ncausal relations between steps and states in procedures. Specifically, we\nexplicitly represent each step as state changes and track the state changes in\nprocedures. For step representation, we leveraged the commonsense knowledge in\nlarge language models (LLMs) to describe the state changes of steps via our\ndesigned chain-of-thought prompting. For state change tracking, we align visual\nstate observations with language state descriptions via cross-modal contrastive\nlearning, and explicitly model the intermediate states of the procedure using\nLLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV\nbenchmark datasets demonstrate that our proposed SCHEMA model achieves\nstate-of-the-art performance and obtains explainable visualizations.",
        "pdf_link": "https://arxiv.org/pdf/2403.01599v1.pdf"
    },
    {
        "title": "SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction",
        "authors": [
            "Jiahuan Yan",
            "Jintai Chen",
            "Chaowen Hu",
            "Bo Zheng",
            "Yaojun Hu",
            "Jimeng Sun",
            "Jian Wu"
        ],
        "published": "2024-03-03T17:35:52Z",
        "summary": "Recent development of large language models (LLMs) has exhibited impressive\nzero-shot proficiency on generic and common sense questions. However, LLMs'\napplication on domain-specific vertical questions still lags behind, primarily\ndue to the humiliation problems and deficiencies in vertical knowledge.\nFurthermore, the vertical data annotation process often requires\nlabor-intensive expert involvement, thereby presenting an additional challenge\nin enhancing the model's vertical capabilities. In this paper, we propose\nSERVAL, a synergy learning pipeline designed for unsupervised development of\nvertical capabilities in both LLMs and small models by mutual enhancement.\nSpecifically, SERVAL utilizes the LLM's zero-shot outputs as annotations,\nleveraging its confidence to teach a robust vertical model from scratch.\nReversely, the trained vertical model guides the LLM fine-tuning to enhance its\nzero-shot capability, progressively improving both models through an iterative\nprocess. In medical domain, known for complex vertical knowledge and costly\nannotations, comprehensive experiments show that, without access to any gold\nlabels, SERVAL with the synergy learning of OpenAI GPT-3.5 and a simple model\nattains fully-supervised competitive performance across ten widely used medical\ndatasets. These datasets represent vertically specialized medical diagnostic\nscenarios (e.g., diabetes, heart diseases, COVID-19), highlighting the\npotential of SERVAL in refining the vertical capabilities of LLMs and training\nvertical models from scratch, all achieved without the need for annotations.",
        "pdf_link": "https://arxiv.org/pdf/2403.01570v2.pdf"
    },
    {
        "title": "ReMatch: Retrieval Enhanced Schema Matching with LLMs",
        "authors": [
            "Eitam Sheetrit",
            "Menachem Brief",
            "Moshik Mishaeli",
            "Oren Elisha"
        ],
        "published": "2024-03-03T17:14:40Z",
        "summary": "Schema matching is a crucial task in data integration, involving the\nalignment of a source database schema with a target schema to establish\ncorrespondence between their elements. This task is challenging due to textual\nand semantic heterogeneity, as well as differences in schema sizes. Although\nmachine-learning-based solutions have been explored in numerous studies, they\noften suffer from low accuracy, require manual mapping of the schemas for model\ntraining, or need access to source schema data which might be unavailable due\nto privacy concerns. In this paper we present a novel method, named ReMatch,\nfor matching schemas using retrieval-enhanced Large Language Models (LLMs). Our\nmethod avoids the need for predefined mapping, any model training, or access to\ndata in the source database. In the ReMatch method the tables of the target\nschema and the attributes of the source schema are first represented as\nstructured passage-based documents. For each source attribute document, we\nretrieve $J$ documents, representing target schema tables, according to their\nsemantic relevance. Subsequently, we create a prompt for every source table,\ncomprising all its attributes and their descriptions, alongside all attributes\nfrom the set of top $J$ target tables retrieved previously. We employ LLMs\nusing this prompt for the matching task, yielding a ranked list of $K$\npotential matches for each source attribute. Our experimental results on large\nreal-world schemas demonstrate that ReMatch significantly improves matching\ncapabilities and outperforms other machine learning approaches. By eliminating\nthe requirement for training data, ReMatch becomes a viable solution for\nreal-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.01567v1.pdf"
    },
    {
        "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
        "authors": [
            "Shiqi Chen",
            "Miao Xiong",
            "Junteng Liu",
            "Zhengxuan Wu",
            "Teng Xiao",
            "Siyang Gao",
            "Junxian He"
        ],
        "published": "2024-03-03T15:53:41Z",
        "summary": "Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2403.01548v3.pdf"
    },
    {
        "title": "Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics",
        "authors": [
            "Zhu Liu",
            "Cunliang Kong",
            "Ying Liu",
            "Maosong Sun"
        ],
        "published": "2024-03-03T13:14:47Z",
        "summary": "Large language models have achieved remarkable success in general language\nunderstanding tasks. However, as a family of generative methods with the\nobjective of next token prediction, the semantic evolution with the depth of\nthese models are not fully explored, unlike their predecessors, such as\nBERT-like architectures. In this paper, we specifically investigate the\nbottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by\nprobing its hidden states at the end of each layer using a contextualized word\nidentification task. Our experiments show that the representations in lower\nlayers encode lexical semantics, while the higher layers, with weaker semantic\ninduction, are responsible for prediction. This is in contrast to models with\ndiscriminative objectives, such as mask language modeling, where the higher\nlayers obtain better lexical semantics. The conclusion is further supported by\nthe monotonic increase in performance via the hidden states for the last\nmeaningless symbols, such as punctuation, in the prompting strategy.",
        "pdf_link": "https://arxiv.org/pdf/2403.01509v1.pdf"
    },
    {
        "title": "Infusing Knowledge into Large Language Models with Contextual Prompts",
        "authors": [
            "Kinshuk Vasisht",
            "Balaji Ganesan",
            "Vikas Kumar",
            "Vasudha Bhatnagar"
        ],
        "published": "2024-03-03T11:19:26Z",
        "summary": "Knowledge infusion is a promising method for enhancing Large Language Models\nfor domain-specific NLP tasks rather than pre-training models over large data\nfrom scratch. These augmented LLMs typically depend on additional pre-training\nor knowledge prompts from an existing knowledge graph, which is impractical in\nmany applications. In contrast, knowledge infusion directly from relevant\ndocuments is more generalisable and alleviates the need for structured\nknowledge graphs while also being useful for entities that are usually not\nfound in any knowledge graph. With this motivation, we propose a simple yet\ngeneralisable approach for knowledge infusion by generating prompts from the\ncontext in the input text. Our experiments show the effectiveness of our\napproach which we evaluate by probing the fine-tuned LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01481v1.pdf"
    },
    {
        "title": "Logic Rules as Explanations for Legal Case Retrieval",
        "authors": [
            "Zhongxiang Sun",
            "Kepu Zhang",
            "Weijie Yu",
            "Haoyu Wang",
            "Jun Xu"
        ],
        "published": "2024-03-03T09:22:21Z",
        "summary": "In this paper, we address the issue of using logic rules to explain the\nresults from legal case retrieval. The task is critical to legal case retrieval\nbecause the users (e.g., lawyers or judges) are highly specialized and require\nthe system to provide logical, faithful, and interpretable explanations before\nmaking legal decisions. Recently, research efforts have been made to learn\nexplainable legal case retrieval models. However, these methods usually select\nrationales (key sentences) from the legal cases as explanations, failing to\nprovide faithful and logically correct explanations. In this paper, we propose\nNeural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that\nexplicitly conducts reasoning on the matching of legal cases through learning\ncase-level and law-level logic rules. The learned rules are then integrated\ninto the retrieval process in a neuro-symbolic manner. Benefiting from the\nlogic and interpretable nature of the logic rules, NS-LCR is equipped with\nbuilt-in faithful explainability. We also show that NS-LCR is a model-agnostic\nframework that can be plugged in for multiple legal retrieval models. To\nshowcase NS-LCR's superiority, we enhance existing benchmarks by adding\nmanually annotated logic rules and introducing a novel explainability metric\nusing Large Language Models (LLMs). Our comprehensive experiments reveal\nNS-LCR's effectiveness for ranking, alongside its proficiency in delivering\nreliable explanations for legal case retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2403.01457v1.pdf"
    },
    {
        "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
        "authors": [
            "Yijun Yang",
            "Ruiyuan Gao",
            "Xiao Yang",
            "Jianyuan Zhong",
            "Qiang Xu"
        ],
        "published": "2024-03-03T09:04:34Z",
        "summary": "Recent advancements in Text-to-Image (T2I) models have raised significant\nsafety concerns about their potential misuse for generating inappropriate or\nNot-Safe-For-Work (NSFW) contents, despite existing countermeasures such as\nNSFW classifiers or model fine-tuning for inappropriate concept removal.\nAddressing this challenge, our study unveils GuardT2I, a novel moderation\nframework that adopts a generative approach to enhance T2I models' robustness\nagainst adversarial prompts. Instead of making a binary classification,\nGuardT2I utilizes a Large Language Model (LLM) to conditionally transform text\nguidance embeddings within the T2I models into natural language for effective\nadversarial prompt detection, without compromising the models' inherent\nperformance. Our extensive experiments reveal that GuardT2I outperforms leading\ncommercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a\nsignificant margin across diverse adversarial scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.01446v1.pdf"
    },
    {
        "title": "Ever-Evolving Memory by Blending and Refining the Past",
        "authors": [
            "Seo Hyun Kim",
            "Keummin Ka",
            "Yohan Jo",
            "Seung-won Hwang",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "published": "2024-03-03T08:12:59Z",
        "summary": "For a human-like chatbot, constructing a long-term memory is crucial.\nHowever, current large language models often lack this capability, leading to\ninstances of missing important user information or redundantly asking for the\nsame information, thereby diminishing conversation quality. To effectively\nconstruct memory, it is crucial to seamlessly connect past and present\ninformation, while also possessing the ability to forget obstructive\ninformation. To address these challenges, we propose CREEM, a novel memory\nsystem for long-term conversation. Improving upon existing approaches that\nconstruct memory based solely on current sessions, CREEM blends past memories\nduring memory formation. Additionally, we introduce a refining process to\nhandle redundant or outdated information. Unlike traditional paradigms, we view\nresponding and memory construction as inseparable tasks. The blending process,\nwhich creates new memories, also serves as a reasoning step for response\ngeneration by informing the connection between past and present. Through\nevaluation, we demonstrate that CREEM enhances both memory and response\nqualities in multi-session personalized dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2403.04787v2.pdf"
    },
    {
        "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
        "authors": [
            "Heydar Soudani",
            "Evangelos Kanoulas",
            "Faegheh Hasibi"
        ],
        "published": "2024-03-03T08:07:55Z",
        "summary": "Large language models (LLMs) memorize a vast amount of factual knowledge,\nexhibiting strong performance across diverse tasks and domains. However, it has\nbeen observed that the performance diminishes when dealing with less-popular or\nlow-frequency concepts and entities, for example in domain specific\napplications. The two prominent approaches to enhance the performance of LLMs\non low-frequent topics are: Retrieval Augmented Generation (RAG) and\nfine-tuning (FT) over synthetic data. This paper explores and evaluates the\nimpact of RAG and FT on customizing LLMs in handling low-frequency entities on\nquestion answering task. Our findings indicate that FT significantly boosts the\nperformance across entities of varying popularity, especially in the most and\nleast popular groups, while RAG surpasses other methods. Additionally, the\nsuccess of both RAG and FT approaches is amplified by advancements in retrieval\nand data augmentation techniques. We release our data and code at\nhttps://github.com/informagi/RAGvsFT.",
        "pdf_link": "https://arxiv.org/pdf/2403.01432v2.pdf"
    },
    {
        "title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
        "authors": [
            "Zhende Song",
            "Chenchen Wang",
            "Jiamu Sheng",
            "Chi Zhang",
            "Gang Yu",
            "Jiayuan Fan",
            "Tao Chen"
        ],
        "published": "2024-03-03T07:43:39Z",
        "summary": "The development of multimodal models has marked a significant step forward in\nhow machines understand videos. These models have shown promise in analyzing\nshort video clips. However, when it comes to longer formats like movies, they\noften fall short. The main hurdles are the lack of high-quality, diverse video\ndata and the intensive work required to collect or annotate such data. In the\nface of these challenges, we propose MovieLLM, a novel framework designed to\ncreate synthetic, high-quality data for long videos. This framework leverages\nthe power of GPT-4 and text-to-image models to generate detailed scripts and\ncorresponding visuals. Our approach stands out for its flexibility and\nscalability, making it a superior alternative to traditional data collection\nmethods. Our extensive experiments validate that the data produced by MovieLLM\nsignificantly improves the performance of multimodal models in understanding\ncomplex video narratives, overcoming the limitations of existing datasets\nregarding scarcity and bias.",
        "pdf_link": "https://arxiv.org/pdf/2403.01422v1.pdf"
    },
    {
        "title": "The Implicit Bias of Heterogeneity towards Invariance and Causality",
        "authors": [
            "Yang Xu",
            "Yihong Gu",
            "Cong Fang"
        ],
        "published": "2024-03-03T07:38:24Z",
        "summary": "It is observed empirically that the large language models (LLM), trained with\na variant of regression loss using numerous corpus from the Internet, can\nunveil causal associations to some extent. This is contrary to the traditional\nwisdom that ``association is not causation'' and the paradigm of traditional\ncausal inference in which prior causal knowledge should be carefully\nincorporated into the design of methods. It is a mystery why causality, in a\nhigher layer of understanding, can emerge from the regression task that pursues\nassociations. In this paper, we claim the emergence of causality from\nassociation-oriented training can be attributed to the coupling effects from\nthe heterogeneity of the source data, stochasticity of training algorithms, and\nover-parameterization of the learning models. We illustrate such an intuition\nusing a simple but insightful model that learns invariance, a quasi-causality,\nusing regression loss. To be specific, we consider multi-environment low-rank\nmatrix sensing problems where the unknown r-rank ground-truth d*d matrices\ndiverge across the environments but contain a lower-rank invariant, causal\npart. In this case, running pooled gradient descent will result in biased\nsolutions that only learn associations in general. We show that running\nlarge-batch Stochastic Gradient Descent, whose each batch being linear\nmeasurement samples randomly selected from a certain environment, can\nsuccessfully drive the solution towards the invariant, causal solution under\ncertain conditions. This step is related to the relatively strong heterogeneity\nof the environments, the large step size and noises in the optimization\nalgorithm, and the over-parameterization of the model. In summary, we unveil\nanother implicit bias that is a result of the symbiosis between the\nheterogeneity of data and modern algorithms, which is, to the best of our\nknowledge, first in the literature.",
        "pdf_link": "https://arxiv.org/pdf/2403.01420v1.pdf"
    },
    {
        "title": "CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge",
        "authors": [
            "Willis Guo",
            "Armin Toroghi",
            "Scott Sanner"
        ],
        "published": "2024-03-03T04:47:01Z",
        "summary": "Knowledge graph question answering (KGQA) is a well-established field that\nseeks to provide factual answers to natural language (NL) questions by\nleveraging knowledge graphs (KGs). However, existing KGQA datasets suffer from\ntwo significant limitations: (1) no existing KGQA dataset requires commonsense\nreasoning to arrive at an answer and (2) existing KGQA datasets focus on\npopular entities for which large language models (LLMs) can directly answer\nwithout hallucinating and without leveraging the KG. In this work, we seek a\nnovel KGQA dataset that supports commonsense reasoning and focuses on long-tail\nentities (e.g., non-mainstream and recent entities) where LLMs frequently\nhallucinate, and thus create the need for novel methodologies that leverage the\nKG for factual and attributable commonsense inference. We create a novel\nCommonsense Reasoning (CR) and Long-Tail (LT) KGQA dataset with two subtasks --\nquestion answering and claim verification -- that address both limitations (1)\nand (2). We construct CR-LT-KGQA by building extensions to existing reasoning\ndatasets StrategyQA and CREAK over Wikidata. While existing KGQA methods are\nnot applicable due to their lack of commonsense inference support, baseline\nevaluation of LLMs on CR-LT KGQA demonstrate a high rate of hallucination.\nThus, CR-LT KGQA poses significant challenges for hallucination-prone LLMs,\nhence paving the way for future commonsense KGQA research to provide accurate\nand factual answers for long-tail entities in the era of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01395v1.pdf"
    },
    {
        "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models",
        "authors": [
            "Arijit Ghosh Chowdhury",
            "Md Mofijul Islam",
            "Vaibhav Kumar",
            "Faysal Hossain Shezan",
            "Vaibhav Kumar",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published": "2024-03-03T04:46:21Z",
        "summary": "Large Language Models (LLMs) have become a cornerstone in the field of\nNatural Language Processing (NLP), offering transformative capabilities in\nunderstanding and generating human-like text. However, with their rising\nprominence, the security and vulnerability aspects of these models have\ngarnered significant attention. This paper presents a comprehensive survey of\nthe various forms of attacks targeting LLMs, discussing the nature and\nmechanisms of these attacks, their potential impacts, and current defense\nstrategies. We delve into topics such as adversarial attacks that aim to\nmanipulate model outputs, data poisoning that affects model training, and\nprivacy concerns related to training data exploitation. The paper also explores\nthe effectiveness of different attack methodologies, the resilience of LLMs\nagainst these attacks, and the implications for model integrity and user trust.\nBy examining the latest research, we provide insights into the current\nlandscape of LLM vulnerabilities and defense mechanisms. Our objective is to\noffer a nuanced understanding of LLM attacks, foster awareness within the AI\ncommunity, and inspire robust solutions to mitigate these risks in future\ndevelopments.",
        "pdf_link": "https://arxiv.org/pdf/2403.04786v2.pdf"
    },
    {
        "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
        "authors": [
            "Armin Toroghi",
            "Willis Guo",
            "Mohammad Mahdi Abdollah Pour",
            "Scott Sanner"
        ],
        "published": "2024-03-03T04:22:13Z",
        "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\nLanguage questions using the relational information stored in Knowledge Graphs\n(KGs). With the recent advancements of Large Language Models (LLMs) and their\nremarkable reasoning abilities, there is a growing trend to leverage them for\nKGQA. However, existing methodologies have only focused on answering factual\nquestions, e.g., \"In which city was Silvio Berlusconi's first wife born?\",\nleaving questions involving commonsense reasoning that real-world users may\npose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf\nand attend the Olympics this summer?\" unaddressed. In this work, we first\nobserve that existing LLM-based methods for KGQA struggle with hallucination on\nsuch questions, especially on queries targeting long-tail entities (e.g.,\nnon-mainstream and recent entities), thus hindering their applicability in\nreal-world applications especially since their reasoning processes are not\neasily verifiable. In response, we propose Right for Right Reasons (R3), a\ncommonsense KGQA methodology that allows for a verifiable reasoning procedure\nby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\ngrounding every factual reasoning step on KG triples. Through experimental\nevaluations across three different tasks--question answering, claim\nverification, and preference matching--our findings showcase R3 as a superior\napproach, outperforming existing methodologies and notably reducing instances\nof hallucination and reasoning errors.",
        "pdf_link": "https://arxiv.org/pdf/2403.01390v1.pdf"
    },
    {
        "title": "On the Compressibility of Quantized Large Language Models",
        "authors": [
            "Yu Mao",
            "Weilan Wang",
            "Hongchao Du",
            "Nan Guan",
            "Chun Jason Xue"
        ],
        "published": "2024-03-03T03:27:07Z",
        "summary": "Deploying Large Language Models (LLMs) on edge or mobile devices offers\nsignificant benefits, such as enhanced data privacy and real-time processing\ncapabilities. However, it also faces critical challenges due to the substantial\nmemory requirement of LLMs. Quantization is an effective way of reducing the\nmodel size while maintaining good performance. However, even after\nquantization, LLMs may still be too big to fit entirely into the limited memory\nof edge or mobile devices and have to be partially loaded from the storage to\ncomplete the inference. In this case, the I/O latency of model loading becomes\nthe bottleneck of the LLM inference latency. In this work, we take a\npreliminary step of studying applying data compression techniques to reduce\ndata movement and thus speed up the inference of quantized LLM on\nmemory-constrained devices. In particular, we discussed the compressibility of\nquantized LLMs, the trade-off between the compressibility and performance of\nquantized LLMs, and opportunities to optimize both of them jointly.",
        "pdf_link": "https://arxiv.org/pdf/2403.01384v1.pdf"
    },
    {
        "title": "Automatic Question-Answer Generation for Long-Tail Knowledge",
        "authors": [
            "Rohan Kumar",
            "Youngmin Kim",
            "Sunitha Ravi",
            "Haitian Sun",
            "Christos Faloutsos",
            "Ruslan Salakhutdinov",
            "Minji Yoon"
        ],
        "published": "2024-03-03T03:06:31Z",
        "summary": "Pretrained Large Language Models (LLMs) have gained significant attention for\naddressing open-domain Question Answering (QA). While they exhibit high\naccuracy in answering questions related to common knowledge, LLMs encounter\ndifficulties in learning about uncommon long-tail knowledge (tail entities).\nSince manually constructing QA datasets demands substantial human resources,\nthe types of existing QA datasets are limited, leaving us with a scarcity of\ndatasets to study the performance of LLMs on tail entities. In this paper, we\npropose an automatic approach to generate specialized QA datasets for tail\nentities and present the associated research challenges. We conduct extensive\nexperiments by employing pretrained LLMs on our newly generated long-tail QA\ndatasets, comparing their performance with and without external resources\nincluding Wikipedia and Wikidata knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01382v1.pdf"
    },
    {
        "title": "LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems",
        "authors": [
            "Tasnim Ahmed",
            "Salimur Choudhury"
        ],
        "published": "2024-03-02T23:32:33Z",
        "summary": "In the rapidly evolving field of natural language processing, the translation\nof linguistic descriptions into mathematical formulation of optimization\nproblems presents a formidable challenge, demanding intricate understanding and\nprocessing capabilities from Large Language Models (LLMs). This study compares\nprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and\none-shot settings for this task. Our findings show GPT-4's superior\nperformance, particularly in the one-shot scenario. A central part of this\nresearch is the introduction of `LM4OPT,' a progressive fine-tuning framework\nfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.\nHowever, this research highlights a notable gap in the contextual understanding\ncapabilities of smaller models such as Llama-2-7b compared to larger\ncounterparts, especially in processing lengthy and complex input contexts. Our\nempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4\nsurpasses the baseline performance established by previous research, achieving\nan F1-score of 0.63, solely based on the problem description in natural\nlanguage, and without relying on any additional named entity information.\nGPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These\nfindings not only benchmark the current capabilities of LLMs in a novel\napplication area but also lay the groundwork for future improvements in\nmathematical formulation of optimization problems from natural language input.",
        "pdf_link": "https://arxiv.org/pdf/2403.01342v1.pdf"
    },
    {
        "title": "Large Language Multimodal Models for 5-Year Chronic Disease Cohort Prediction Using EHR Data",
        "authors": [
            "Jun-En Ding",
            "Phan Nguyen Minh Thao",
            "Wen-Chih Peng",
            "Jian-Zhe Wang",
            "Chun-Cheng Chug",
            "Min-Chen Hsieh",
            "Yun-Chien Tseng",
            "Ling Chen",
            "Dongsheng Luo",
            "Chi-Te Wang",
            "Pei-fu Chen",
            "Feng Liu",
            "Fang-Ming Hung"
        ],
        "published": "2024-03-02T22:33:17Z",
        "summary": "Chronic diseases such as diabetes are the leading causes of morbidity and\nmortality worldwide. Numerous research studies have been attempted with various\ndeep learning models in diagnosis. However, most previous studies had certain\nlimitations, including using publicly available datasets (e.g. MIMIC), and\nimbalanced data. In this study, we collected five-year electronic health\nrecords (EHRs) from the Taiwan hospital database, including 1,420,596 clinical\nnotes, 387,392 laboratory test results, and more than 1,505 laboratory test\nitems, focusing on research pre-training large language models. We proposed a\nnovel Large Language Multimodal Models (LLMMs) framework incorporating\nmultimodal data from clinical notes and laboratory test results for the\nprediction of chronic disease risk. Our method combined a text embedding\nencoder and multi-head attention layer to learn laboratory test values,\nutilizing a deep neural network (DNN) module to merge blood features with\nchronic disease semantics into a latent space. In our experiments, we observe\nthat clinicalBERT and PubMed-BERT, when combined with attention fusion, can\nachieve an accuracy of 73% in multiclass chronic diseases and diabetes\nprediction. By transforming laboratory test values into textual descriptions\nand employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve\n(AUROC), demonstrating the effectiveness of leveraging numerical text data for\ntraining and inference in language models. This approach significantly improves\nthe accuracy of early-stage diabetes prediction.",
        "pdf_link": "https://arxiv.org/pdf/2403.04785v1.pdf"
    },
    {
        "title": "A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition",
        "authors": [
            "Tyler Benster",
            "Guy Wilson",
            "Reshef Elisha",
            "Francis R Willett",
            "Shaul Druckmann"
        ],
        "published": "2024-03-02T21:15:24Z",
        "summary": "Silent Speech Interfaces (SSIs) offer a noninvasive alternative to\nbrain-computer interfaces for soundless verbal communication. We introduce\nMultimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal\nalignment through novel loss functions--cross-contrast (crossCon) and\nsupervised temporal contrast (supTcon)--to train a multimodal model with a\nshared latent representation. This architecture enables the use of audio-only\ndatasets like LibriSpeech to improve silent speech recognition. Additionally,\nour introduction of Large Language Model (LLM) Integrated Scoring Adjustment\n(LISA) significantly improves recognition accuracy. Together, MONA LISA reduces\nthe state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy\n(2020) benchmark dataset for silent speech on an open vocabulary. For vocal EMG\nrecordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In\nthe Brain-to-Text 2024 competition, LISA performs best, improving the top WER\nfrom 9.8% to 8.9%. To the best of our knowledge, this work represents the first\ninstance where noninvasive silent speech recognition on an open vocabulary has\ncleared the threshold of 15% WER, demonstrating that SSIs can be a viable\nalternative to automatic speech recognition (ASR). Our work not only narrows\nthe performance gap between silent and vocalized speech but also opens new\npossibilities in human-computer interaction, demonstrating the potential of\ncross-modal approaches in noisy and data-limited regimes.",
        "pdf_link": "https://arxiv.org/pdf/2403.05583v1.pdf"
    },
    {
        "title": "Improving the Validity of Automatically Generated Feedback via Reinforcement Learning",
        "authors": [
            "Alexander Scarlatos",
            "Digory Smith",
            "Simon Woodhead",
            "Andrew Lan"
        ],
        "published": "2024-03-02T20:25:50Z",
        "summary": "Automatically generating feedback via large language models (LLMs) in\nintelligent tutoring systems and online learning platforms has the potential to\nimprove the learning outcomes of many students. However, both feedback\ngeneration and evaluation are challenging: feedback content has to be valid\nespecially in subjects like math, which requires models to understand the\nproblem, the solution, and where the student's error lies. Feedback also has to\nbe pedagogically valid to reflect effective tutoring strategies, such as\nexplaining possible misconceptions and encouraging the student, among other\ndesirable features. In this work, we address both problems of automatically\ngenerating and evaluating feedback while considering both correctness and\nalignment. First, we propose a rubric for evaluating math feedback and show\nthat GPT-4 is able to effectively use it to annotate human-written and\nLLM-generated feedback. Second, we propose a framework for feedback generation\nthat optimizes both correctness and alignment using reinforcement learning\n(RL). Specifically, we use GPT-4's annotations to create preferences over\nfeedback pairs in an augmented dataset for training via direct preference\noptimization (DPO). We show that our methods significantly increase the\ncorrectness and alignment of generated feedback with Llama 2, an open-source\nLLM, qualitatively analyze our generation and evaluation systems using case\nstudies, and outline several areas for future work.",
        "pdf_link": "https://arxiv.org/pdf/2403.01304v1.pdf"
    },
    {
        "title": "Analysis of Privacy Leakage in Federated Large Language Models",
        "authors": [
            "Minh N. Vu",
            "Truc Nguyen",
            "Tre' R. Jeter",
            "My T. Thai"
        ],
        "published": "2024-03-02T20:25:38Z",
        "summary": "With the rapid adoption of Federated Learning (FL) as the training and tuning\nprotocol for applications utilizing Large Language Models (LLMs), recent\nresearch highlights the need for significant modifications to FL to accommodate\nthe large-scale of LLMs. While substantial adjustments to the protocol have\nbeen introduced as a response, comprehensive privacy analysis for the adapted\nFL protocol is currently lacking.\n  To address this gap, our work delves into an extensive examination of the\nprivacy analysis of FL when used for training LLMs, both from theoretical and\npractical perspectives. In particular, we design two active membership\ninference attacks with guaranteed theoretical success rates to assess the\nprivacy leakages of various adapted FL configurations. Our theoretical findings\nare translated into practical attacks, revealing substantial privacy\nvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and\nOpenAI's GPTs, across multiple real-world language datasets. Additionally, we\nconduct thorough experiments to evaluate the privacy leakage of these models\nwhen data is protected by state-of-the-art differential privacy (DP)\nmechanisms.",
        "pdf_link": "https://arxiv.org/pdf/2403.04784v1.pdf"
    }
]