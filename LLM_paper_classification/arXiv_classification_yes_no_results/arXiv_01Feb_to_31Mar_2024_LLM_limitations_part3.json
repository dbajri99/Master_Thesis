[
    {
        "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention",
        "authors": [
            "Tianyi Zhang",
            "Jonah Wonkyu Yi",
            "Bowen Yao",
            "Zhaozhuo Xu",
            "Anshumali Shrivastava"
        ],
        "published": "2024-03-02T17:29:22Z",
        "summary": "Large language model inference on Central Processing Units (CPU) is\nchallenging due to the vast quantities of expensive Multiply-Add (MAD) matrix\noperations in the attention computations. In this paper, we argue that there is\na rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,\nwhich allow for ultra-low-latency lookups in batch. We leverage this unique\ncapability of CPUs to propose NoMAD-Attention, an efficient attention algorithm\nthat replaces MAD operations with in-register lookups. Through hardware-aware\nalgorithmic designs, NoMAD-Attention achieves the computation of attention\nscores using repeated fast accesses to SIMD registers despite their highly\nlimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based\nLLMs without model finetuning. Empirical evaluations demonstrate that\nNoMAD-Attention maintains the quality of the original LLMs well, and speeds up\nthe 4-bit quantized LLaMA-7B-based model by up to 2$\\times$ at 16k context\nlength. Our results are reproducible at\nhttps://github.com/tonyzhang617/nomad-dist.",
        "pdf_link": "https://arxiv.org/pdf/2403.01273v1.pdf"
    },
    {
        "title": "Dissecting Language Models: Machine Unlearning via Selective Pruning",
        "authors": [
            "Nicholas Pochinkov",
            "Nandi Schoots"
        ],
        "published": "2024-03-02T17:10:44Z",
        "summary": "Understanding and shaping the behaviour of Large Language Models (LLMs) is\nincreasingly important as applications become more powerful and more frequently\nadopted. This paper introduces a machine unlearning method specifically\ndesigned for LLMs. We introduce a selective pruning method for LLMs that\nremoves neurons based on their relative importance on a targeted capability\ncompared to overall network performance. This approach is a compute- and\ndata-efficient method for identifying and removing neurons that enable specific\nbehaviours. Our findings reveal that both feed-forward and attention neurons in\nLLMs are specialized; that is, for specific tasks, certain neurons are more\ncrucial than others.",
        "pdf_link": "https://arxiv.org/pdf/2403.01267v1.pdf"
    },
    {
        "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
        "authors": [
            "Yifan Zeng",
            "Yiran Wu",
            "Xiao Zhang",
            "Huazheng Wang",
            "Qingyun Wu"
        ],
        "published": "2024-03-02T16:52:22Z",
        "summary": "Despite extensive pre-training and fine-tuning in moral alignment to prevent\ngenerating harmful information at user request, large language models (LLMs)\nremain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense,\na response-filtering based multi-agent defense framework that filters harmful\nresponses from LLMs. This framework assigns different roles to LLM agents and\nemploys them to complete the defense task collaboratively. The division in\ntasks enhances the overall instruction-following of LLMs and enables the\nintegration of other defense components as tools. AutoDefense can adapt to\nvarious sizes and kinds of open-source LLMs that serve as agents. Through\nconducting extensive experiments on a large scale of harmful and safe prompts,\nwe validate the effectiveness of the proposed AutoDefense in improving the\nrobustness against jailbreak attacks, while maintaining the performance at\nnormal user request. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense.",
        "pdf_link": "https://arxiv.org/pdf/2403.04783v1.pdf"
    },
    {
        "title": "Accelerating Greedy Coordinate Gradient via Probe Sampling",
        "authors": [
            "Yiran Zhao",
            "Wenyue Zheng",
            "Tianle Cai",
            "Xuan Long Do",
            "Kenji Kawaguchi",
            "Anirudh Goyal",
            "Michael Shieh"
        ],
        "published": "2024-03-02T16:23:44Z",
        "summary": "Safety of Large Language Models (LLMs) has become a central issue given their\nrapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown\nto be effective in constructing prompts containing adversarial suffixes to\nbreak the presumingly safe LLMs, but the optimization of GCG is time-consuming\nand limits its practicality. To reduce the time cost of GCG and enable more\ncomprehensive studies of LLM safety, in this work, we study a new algorithm\ncalled $\\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core\nof the algorithm is a mechanism that dynamically determines how similar a\nsmaller draft model's predictions are to the target model's predictions for\nprompt candidates. When the target model is similar to the draft model, we rely\nheavily on the draft model to filter out a large number of potential prompt\ncandidates to reduce the computation time. Probe sampling achieves up to $5.6$\ntimes speedup using Llama2-7b and leads to equal or improved attack success\nrate (ASR) on the AdvBench.",
        "pdf_link": "https://arxiv.org/pdf/2403.01251v1.pdf"
    },
    {
        "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code",
        "authors": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Aashi Jain",
            "Thomas Kipf",
            "Yisong Yue",
            "David A. Ross",
            "Cordelia Schmid",
            "Alireza Fathi"
        ],
        "published": "2024-03-02T16:16:26Z",
        "summary": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent\nconverting text descriptions into Blender-executable Python scripts which\nrender complex scenes with up to a hundred 3D assets. This process requires\ncomplex spatial planning and arrangement. We tackle these challenges through a\ncombination of advanced abstraction, strategic planning, and library learning.\nSceneCraft first models a scene graph as a blueprint, detailing the spatial\nrelationships among assets in the scene. SceneCraft then writes Python scripts\nbased on this graph, translating relationships into numerical constraints for\nasset layout. Next, SceneCraft leverages the perceptual strengths of\nvision-language foundation models like GPT-V to analyze rendered images and\niteratively refine the scene. On top of this process, SceneCraft features a\nlibrary learning mechanism that compiles common script functions into a\nreusable library, facilitating continuous self-improvement without expensive\nLLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses\nexisting LLM-based agents in rendering complex scenes, as shown by its\nadherence to constraints and favorable human assessments. We also showcase the\nbroader application potential of SceneCraft by reconstructing detailed 3D\nscenes from the Sintel movie and guiding a video generative model with\ngenerated scenes as intermediary control signal.",
        "pdf_link": "https://arxiv.org/pdf/2403.01248v1.pdf"
    },
    {
        "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal",
        "authors": [
            "Jianheng Huang",
            "Leyang Cui",
            "Ante Wang",
            "Chengyi Yang",
            "Xinting Liao",
            "Linfeng Song",
            "Junfeng Yao",
            "Jinsong Su"
        ],
        "published": "2024-03-02T16:11:23Z",
        "summary": "Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.01244v1.pdf"
    },
    {
        "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
        "authors": [
            "Ruikang Liu",
            "Haoli Bai",
            "Haokun Lin",
            "Yuening Li",
            "Han Gao",
            "Zhengzhuo Xu",
            "Lu Hou",
            "Jun Yao",
            "Chun Yuan"
        ],
        "published": "2024-03-02T16:05:26Z",
        "summary": "Large language models (LLMs) excel in natural language processing but demand\nintensive computation. To mitigate this, various quantization methods have been\nexplored, yet they compromise LLM performance. This paper unveils a previously\noverlooked type of outlier in LLMs. Such outliers are found to allocate most of\nthe attention scores on initial tokens of input, termed as pivot tokens, which\nis crucial to the performance of quantized LLMs. Given that, we propose\nIntactKV to generate the KV cache of pivot tokens losslessly from the\nfull-precision model. The approach is simple and easy to combine with existing\nquantization solutions. Besides, IntactKV can be calibrated as additional LLM\nparameters to boost the quantized LLMs further. Mathematical analysis also\nproves that IntactKV effectively reduces the upper bound of quantization error.\nEmpirical results show that IntactKV brings consistent improvement and achieves\nlossless weight-only INT4 quantization on various downstream tasks, leading to\nthe new state-of-the-art for LLM quantization.",
        "pdf_link": "https://arxiv.org/pdf/2403.01241v1.pdf"
    },
    {
        "title": "API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access",
        "authors": [
            "Jiayuan Su",
            "Jing Luo",
            "Hongwei Wang",
            "Lu Cheng"
        ],
        "published": "2024-03-02T14:14:45Z",
        "summary": "This study aims to address the pervasive challenge of quantifying uncertainty\nin large language models (LLMs) without logit-access. Conformal Prediction\n(CP), known for its model-agnostic and distribution-free features, is a desired\napproach for various LLMs and data distributions. However, existing CP methods\nfor LLMs typically assume access to the logits, which are unavailable for some\nAPI-only LLMs. In addition, logits are known to be miscalibrated, potentially\nleading to degraded CP performance. To tackle these challenges, we introduce a\nnovel CP method that (1) is tailored for API-only LLMs without logit-access;\n(2) minimizes the size of prediction sets; and (3) ensures a statistical\nguarantee of the user-defined coverage. The core idea of this approach is to\nformulate nonconformity measures using both coarse-grained (i.e., sample\nfrequency) and fine-grained uncertainty notions (e.g., semantic similarity).\nExperimental results on both close-ended and open-ended Question Answering\ntasks show our approach can mostly outperform the logit-based CP baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.01216v2.pdf"
    },
    {
        "title": "Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning",
        "authors": [
            "Shuo Yang",
            "Zirui Shang",
            "Yongqi Wang",
            "Derong Deng",
            "Hongwei Chen",
            "Qiyuan Cheng",
            "Xinxiao Wu"
        ],
        "published": "2024-03-02T13:43:32Z",
        "summary": "This paper proposes a novel framework for multi-label image recognition\nwithout any training data, called data-free framework, which uses knowledge of\npre-trained Large Language Model (LLM) to learn prompts to adapt pretrained\nVision-Language Model (VLM) like CLIP to multilabel classification. Through\nasking LLM by well-designed questions, we acquire comprehensive knowledge about\ncharacteristics and contexts of objects, which provides valuable text\ndescriptions for learning prompts. Then we propose a hierarchical prompt\nlearning method by taking the multi-label dependency into consideration,\nwherein a subset of category-specific prompt tokens are shared when the\ncorresponding objects exhibit similar attributes or are more likely to\nco-occur. Benefiting from the remarkable alignment between visual and\nlinguistic semantics of CLIP, the hierarchical prompts learned from text\ndescriptions are applied to perform classification of images during inference.\nOur framework presents a new way to explore the synergies between multiple\npre-trained models for novel category recognition. Extensive experiments on\nthree public datasets (MS-COCO, VOC2007, and NUS-WIDE) demonstrate that our\nmethod achieves better results than the state-of-the-art methods, especially\noutperforming the zero-shot multi-label recognition methods by 4.7% in mAP on\nMS-COCO.",
        "pdf_link": "https://arxiv.org/pdf/2403.01209v1.pdf"
    },
    {
        "title": "The Case for Animal-Friendly AI",
        "authors": [
            "Sankalpa Ghose",
            "Yip Fai Tse",
            "Kasra Rasaee",
            "Jeff Sebo",
            "Peter Singer"
        ],
        "published": "2024-03-02T12:41:11Z",
        "summary": "Artificial intelligence is seen as increasingly important, and potentially\nprofoundly so, but the fields of AI ethics and AI engineering have not fully\nrecognized that these technologies, including large language models (LLMs),\nwill have massive impacts on animals. We argue that this impact matters,\nbecause animals matter morally.\n  As a first experiment in evaluating animal consideration in LLMs, we\nconstructed a proof-of-concept Evaluation System, which assesses LLM responses\nand biases from multiple perspectives. This system evaluates LLM outputs by two\ncriteria: their truthfulness, and the degree of consideration they give to the\ninterests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using\na set of structured queries and predefined normative perspectives. Preliminary\nresults suggest that the outcomes of the tested models can be benchmarked\nregarding the consideration they give to animals, and that generated positions\nand biases might be addressed and mitigated with more developed and validated\nsystems.\n  Our research contributes one possible approach to integrating animal ethics\nin AI, opening pathways for future studies and practical applications in\nvarious fields, including education, public policy, and regulation, that\ninvolve or relate to animals and society. Overall, this study serves as a step\ntowards more useful and responsible AI systems that better recognize and\nrespect the vital interests and perspectives of all sentient beings.",
        "pdf_link": "https://arxiv.org/pdf/2403.01199v1.pdf"
    },
    {
        "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling",
        "authors": [
            "Shanghaoran Quan"
        ],
        "published": "2024-03-02T12:31:22Z",
        "summary": "The performance of the reward model (RM) is a critical factor in improving\nthe effectiveness of the large language model (LLM) during alignment\nfine-tuning. There remain two challenges in RM training: 1) training the same\nRM using various categories of data may cause its generalization performance to\nsuffer from multi-task disturbance, and 2) the human annotation consistency\nrate is generally only $60\\%$ to $75\\%$, causing training data to contain a lot\nof noise. To tackle these two challenges, we introduced the idea of\nMixture-of-Experts (MoE) into the field of RM for the first time. We propose\nthe Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After\nclassifying an input into task categories, we route it to the corresponding\ninner layer task-specific model. The inner layer MoE is a dense model. We\ndecompose the specific task into multiple capability dimensions and\nindividually fine-tune a LoRA expert on each one. Their outputs are then\nsynthesized by an MLP to compute the final rewards. To minimize costs, we call\na public LLM API to obtain the capability preference labels. The validation on\nmanually labeled datasets confirms that our model attains superior consistency\nwith human preference and outstrips advanced generative approaches. Meanwhile,\nthrough BoN sampling and RL experiments, we demonstrate that our model\noutperforms state-of-the-art ensemble methods of RM and mitigates the\noveroptimization problem. Our code and dataset are available at:\nhttps://github.com/quanshr/DMoERM-v1.",
        "pdf_link": "https://arxiv.org/pdf/2403.01197v1.pdf"
    },
    {
        "title": "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots",
        "authors": [
            "Philip Feldman. James R. Foulds",
            "Shimei Pan"
        ],
        "published": "2024-03-02T12:19:04Z",
        "summary": "Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01193v2.pdf"
    },
    {
        "title": "Balancing Exploration and Exploitation in LLM using Soft RLLF for Enhanced Negation Understanding",
        "authors": [
            "Ha-Thanh Nguyen",
            "Ken Satoh"
        ],
        "published": "2024-03-02T11:54:55Z",
        "summary": "Finetuning approaches in NLP often focus on exploitation rather than\nexploration, which may lead to suboptimal models. Given the vast search space\nof natural language, this limited exploration can restrict their performance in\ncomplex, high-stakes domains, where accurate negation understanding and logical\nreasoning abilities are crucial. To address this issue, we leverage\nReinforcement Learning from Logical Feedback (RLLF) to create an effective\nbalance between exploration and exploitation in LLMs. Our approach employs an\nappropriate benchmark dataset for training and evaluation, highlighting the\nimportance of exploration in enhancing negation understanding capabilities. We\ncompare the performance of our RLLF-enhanced LLMs with baseline models trained\nwithout RLLF, demonstrating the value of this balanced approach. Furthermore,\nwe showcase the potential of our method in legal AI applications by employing\ntransfer learning and evaluating its impact on negation understanding. Our\nexperimental results exhibit the effectiveness of balancing exploration and\nexploitation with RLLF in improving LLMs' negation capabilities. This has\nimplications for the development of more accurate, reliable, and logically\nconsistent language models in high-stakes domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.01185v1.pdf"
    },
    {
        "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
        "authors": [
            "Linhai Zhang",
            "Jialong Wu",
            "Deyu Zhou",
            "Guoqiang Xu"
        ],
        "published": "2024-03-02T10:38:10Z",
        "summary": "Though Large Language Models (LLMs) have demonstrated the powerful\ncapabilities of few-shot learning through prompting methods, supervised\ntraining is still necessary for complex reasoning tasks. Because of their\nextensive parameters and memory consumption, both Parameter-Efficient\nFine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been\nproposed for LLMs. Nevertheless, the issue of large annotated data consumption,\nthe aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is\nto combine the PEFT method with active learning. However, the experimental\nresults show that such a combination is not trivial and yields inferior\nresults. Through probe experiments, such observation might be explained by two\nmain reasons: uncertainty gap and poor model calibration. Therefore, in this\npaper, we propose a novel approach to effectively integrate uncertainty-based\nactive learning and LoRA. Specifically, for the uncertainty gap, we introduce a\ndynamic uncertainty measurement that combines the uncertainty of the base model\nand the uncertainty of the full model during the iteration of active learning.\nFor poor model calibration, we incorporate the regularization method during\nLoRA training to keep the model from being over-confident, and the Monte-Carlo\ndropout mechanism is employed to enhance the uncertainty estimation.\nExperimental results show that the proposed approach outperforms existing\nbaseline models on three complex reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.01165v1.pdf"
    },
    {
        "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",
        "authors": [
            "Tharindu Kumarage",
            "Garima Agrawal",
            "Paras Sheth",
            "Raha Moraffah",
            "Aman Chadha",
            "Joshua Garland",
            "Huan Liu"
        ],
        "published": "2024-03-02T09:39:13Z",
        "summary": "We have witnessed lately a rapid proliferation of advanced Large Language\nModels (LLMs) capable of generating high-quality text. While these LLMs have\nrevolutionized text generation across various domains, they also pose\nsignificant risks to the information ecosystem, such as the potential for\ngenerating convincing propaganda, misinformation, and disinformation at scale.\nThis paper offers a review of AI-generated text forensic systems, an emerging\nfield addressing the challenges of LLM misuses. We present an overview of the\nexisting efforts in AI-generated text forensics by introducing a detailed\ntaxonomy, focusing on three primary pillars: detection, attribution, and\ncharacterization. These pillars enable a practical understanding of\nAI-generated text, from identifying AI-generated content (detection),\ndetermining the specific AI model involved (attribution), and grouping the\nunderlying intents of the text (characterization). Furthermore, we explore\navailable resources for AI-generated text forensics research and discuss the\nevolving challenges and future directions of forensic systems in an AI era.",
        "pdf_link": "https://arxiv.org/pdf/2403.01152v1.pdf"
    },
    {
        "title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
        "authors": [
            "Yanchao Tan",
            "Hang Lv",
            "Xinyi Huang",
            "Jiawei Zhang",
            "Shiping Wang",
            "Carl Yang"
        ],
        "published": "2024-03-02T09:27:32Z",
        "summary": "Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04780v2.pdf"
    },
    {
        "title": "ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies",
        "authors": [
            "Oren Sultan",
            "Yonatan Bitton",
            "Ron Yosef",
            "Dafna Shahaf"
        ],
        "published": "2024-03-02T08:53:40Z",
        "summary": "Analogy-making is central to human cognition, allowing us to adapt to novel\nsituations -- an ability that current AI systems still lack. Most analogy\ndatasets today focus on simple analogies (e.g., word analogies); datasets\nincluding complex types of analogies are typically manually curated and very\nsmall. We believe that this holds back progress in computational analogy. In\nthis work, we design a data generation pipeline, ParallelPARC (Parallel\nParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\ncreate complex, paragraph-based analogies, as well as distractors, both simple\nand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\nof analogies between scientific processes. We publish a gold-set, validated by\nhumans, and a silver-set, generated automatically. We test LLMs' and humans'\nanalogy recognition in binary and multiple-choice settings, and found that\nhumans outperform the best models (~13% gap) after a light supervision. We\ndemonstrate that our silver-set is useful for training models. Lastly, we show\nchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\nencourage research in this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2403.01139v3.pdf"
    },
    {
        "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization",
        "authors": [
            "Juntao Zhao",
            "Borui Wan",
            "Yanghua Peng",
            "Haibin Lin",
            "Chuan Wu"
        ],
        "published": "2024-03-02T08:40:07Z",
        "summary": "Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.",
        "pdf_link": "https://arxiv.org/pdf/2403.01136v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data",
        "authors": [
            "Aritra Hota",
            "Soumyajit Chatterjee",
            "Sandip Chakraborty"
        ],
        "published": "2024-03-02T08:29:08Z",
        "summary": "Traditional human-in-the-loop-based annotation for time-series data like\ninertial data often requires access to alternate modalities like video or audio\nfrom the environment. These alternate sources provide the necessary information\nto the human annotator, as the raw numeric data is often too obfuscated even\nfor an expert. However, this traditional approach has many concerns surrounding\noverall cost, efficiency, storage of additional modalities, time, scalability,\nand privacy. Interestingly, recent large language models (LLMs) are also\ntrained with vast amounts of publicly available alphanumeric data, which allows\nthem to comprehend and perform well on tasks beyond natural language\nprocessing. Naturally, this opens up a potential avenue to explore LLMs as\nvirtual annotators where the LLMs will be directly provided the raw sensor data\nfor annotation instead of relying on any alternate modality. Naturally, this\ncould mitigate the problems of the traditional human-in-the-loop approach.\nMotivated by this observation, we perform a detailed study in this paper to\nassess whether the state-of-the-art (SOTA) LLMs can be used as virtual\nannotators for labeling time-series physical sensing data. To perform this in a\nprincipled manner, we segregate the study into two major phases. In the first\nphase, we investigate the challenges an LLM like GPT-4 faces in comprehending\nraw sensor data. Considering the observations from phase 1, in the next phase,\nwe investigate the possibility of encoding the raw sensor data using SOTA SSL\napproaches and utilizing the projected time-series data to get annotations from\nthe LLM. Detailed evaluation with four benchmark HAR datasets shows that\nSSL-based encoding and metric-based guidance allow the LLM to make more\nreasonable decisions and provide accurate annotations without requiring\ncomputationally expensive fine-tuning or sophisticated prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2403.01133v1.pdf"
    },
    {
        "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation",
        "authors": [
            "Zeyuan Ma",
            "Hongshu Guo",
            "Jiacheng Chen",
            "Guojun Peng",
            "Zhiguang Cao",
            "Yining Ma",
            "Yue-Jiao Gong"
        ],
        "published": "2024-03-02T08:21:59Z",
        "summary": "Recent research explores optimization using large language models (LLMs) by\neither iteratively seeking next-step solutions from LLMs or directly prompting\nLLMs for an optimizer. However, these approaches exhibit inherent limitations,\nincluding low operational efficiency, high sensitivity to prompt design, and a\nlack of domain-specific knowledge. We introduce LLaMoCo, the first\ninstruction-tuning framework designed to adapt LLMs for solving optimization\nproblems in a code-to-code manner. Specifically, we establish a comprehensive\ninstruction set containing well-described problem prompts and effective\noptimization codes. We then develop a novel two-phase learning strategy that\nincorporates a contrastive learning-based warm-up procedure before the\ninstruction-tuning phase to enhance the convergence behavior during model\nfine-tuning. The experiment results demonstrate that a CodeGen (350M) model\nfine-tuned by our LLaMoCo achieves superior optimization performance compared\nto GPT-4 Turbo and the other competitors across both synthetic and realistic\nproblem sets. The fine-tuned model and the usage instructions are available at\nhttps://anonymous.4open.science/r/LLaMoCo-722A.",
        "pdf_link": "https://arxiv.org/pdf/2403.01131v2.pdf"
    },
    {
        "title": "OpenGraph: Towards Open Graph Foundation Models",
        "authors": [
            "Lianghao Xia",
            "Ben Kao",
            "Chao Huang"
        ],
        "published": "2024-03-02T08:05:03Z",
        "summary": "Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.01121v2.pdf"
    },
    {
        "title": "Towards Accurate Lip-to-Speech Synthesis in-the-Wild",
        "authors": [
            "Sindhu Hegde",
            "Rudrabha Mukhopadhyay",
            "C. V. Jawahar",
            "Vinay Namboodiri"
        ],
        "published": "2024-03-02T04:07:24Z",
        "summary": "In this paper, we introduce a novel approach to address the task of\nsynthesizing speech from silent videos of any in-the-wild speaker solely based\non lip movements. The traditional approach of directly generating speech from\nlip videos faces the challenge of not being able to learn a robust language\nmodel from speech alone, resulting in unsatisfactory outcomes. To overcome this\nissue, we propose incorporating noisy text supervision using a state-of-the-art\nlip-to-text network that instills language information into our model. The\nnoisy text is generated using a pre-trained lip-to-text model, enabling our\napproach to work without text annotations during inference. We design a visual\ntext-to-speech network that utilizes the visual stream to generate accurate\nspeech, which is in-sync with the silent input video. We perform extensive\nexperiments and ablation studies, demonstrating our approach's superiority over\nthe current state-of-the-art methods on various benchmark datasets. Further, we\ndemonstrate an essential practical application of our method in assistive\ntechnology by generating speech for an ALS patient who has lost the voice but\ncan make mouth movements. Our demo video, code, and additional details can be\nfound at\n\\url{http://cvit.iiit.ac.in/research/projects/cvit-projects/ms-l2s-itw}.",
        "pdf_link": "https://arxiv.org/pdf/2403.01087v1.pdf"
    },
    {
        "title": "LAB: Large-Scale Alignment for ChatBots",
        "authors": [
            "Shivchander Sudalairaj",
            "Abhishek Bhandwaldar",
            "Aldo Pareja",
            "Kai Xu",
            "David D. Cox",
            "Akash Srivastava"
        ],
        "published": "2024-03-02T03:48:37Z",
        "summary": "This work introduces LAB (Large-scale Alignment for chatBots), a novel\nmethodology designed to overcome the scalability challenges in the\ninstruction-tuning phase of large language model (LLM) training. Leveraging a\ntaxonomy-guided synthetic data generation process and a multi-phase tuning\nframework, LAB significantly reduces reliance on expensive human annotations\nand proprietary models like GPT-4. We demonstrate that LAB-trained models can\nachieve competitive performance across several benchmarks compared to models\ntrained with traditional human-annotated or GPT-4 generated synthetic data.\nThus offering a scalable, cost-effective solution for enhancing LLM\ncapabilities and instruction-following behaviors without the drawbacks of\ncatastrophic forgetting, marking a step forward in the efficient training of\nLLMs for a wide range of applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.01081v2.pdf"
    },
    {
        "title": "LLMCRIT: Teaching Large Language Models to Use Criteria",
        "authors": [
            "Weizhe Yuan",
            "Pengfei Liu",
            "Matthias Gall\u00e9"
        ],
        "published": "2024-03-02T02:25:55Z",
        "summary": "Humans follow criteria when they execute tasks, and these criteria are\ndirectly used to assess the quality of task completion. Therefore, having\nmodels learn to use criteria to provide feedback can help humans or models to\nperform tasks better. However, existing research in this field tends to\nconsider only a limited set of criteria or quality assessment aspects. To fill\nthis gap, we propose a general framework that enables large language models\n(LLMs) to use comprehensive criteria for a task in delivering natural language\nfeedback on task execution. In particular, we present a model-in-the-loop\nframework that semi-automatically derives criteria from collected guidelines\nfor different writing tasks and constructs in-context demonstrations for each\ncriterion. We choose three tasks from real-world scenarios to operationalize\nthis idea: paper introduction writing, Python code writing, and Reddit post\nwriting, and evaluate our feedback generation framework using different LLMs.\nThe results reveal the fine-grained effects of incorporating criteria and\ndemonstrations and provide valuable insights on how to teach LLMs to use\ncriteria more effectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.01069v1.pdf"
    },
    {
        "title": "FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis",
        "authors": [
            "Songhua Yang",
            "Xinke Jiang",
            "Hanjie Zhao",
            "Wenxuan Zeng",
            "Hongde Liu",
            "Yuxiang Jia"
        ],
        "published": "2024-03-02T02:00:51Z",
        "summary": "Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture\nfine-grained sentiment across diverse domains. While existing research narrowly\nfocuses on single-domain applications constrained by methodological limitations\nand data scarcity, the reality is that sentiment naturally traverses multiple\ndomains. Although large language models (LLMs) offer a promising solution for\nABSA, it is difficult to integrate effectively with established techniques,\nincluding graph-based models and linguistics, because modifying their internal\narchitecture is not easy. To alleviate this problem, we propose a novel\nframework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The\ncore insight of FaiMA is to utilize in-context learning (ICL) as a\nfeature-aware mechanism that facilitates adaptive learning in multi-domain ABSA\ntasks. Specifically, we employ a multi-head graph attention network as a text\nencoder optimized by heuristic rules for linguistic, domain, and sentiment\nfeatures. Through contrastive learning, we optimize sentence representations by\nfocusing on these diverse features. Additionally, we construct an efficient\nindexing mechanism, allowing FaiMA to stably retrieve highly relevant examples\nacross multiple dimensions for any given input. To evaluate the efficacy of\nFaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive\nexperimental results demonstrate that FaiMA achieves significant performance\nimprovements in multiple domains compared to baselines, increasing F1 by 2.07%\non average. Source code and data sets are anonymously available at\nhttps://github.com/SupritYoung/FaiMA.",
        "pdf_link": "https://arxiv.org/pdf/2403.01063v1.pdf"
    },
    {
        "title": "Reading Subtext: Evaluating Large Language Models on Short Story Summarization with Writers",
        "authors": [
            "Melanie Subbiah",
            "Sean Zhang",
            "Lydia B. Chilton",
            "Kathleen McKeown"
        ],
        "published": "2024-03-02T01:52:14Z",
        "summary": "We evaluate recent Large language Models (LLMs) on the challenging task of\nsummarizing short stories, which can be lengthy, and include nuanced subtext or\nscrambled timelines. Importantly, we work directly with authors to ensure that\nthe stories have not been shared online (and therefore are unseen by the\nmodels), and to obtain informed evaluations of summary quality using judgments\nfrom the authors themselves. Through quantitative and qualitative analysis\ngrounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We\nfind that all three models make faithfulness mistakes in over 50% of summaries\nand struggle to interpret difficult subtext. However, at their best, the models\ncan provide thoughtful thematic analysis of stories. We additionally\ndemonstrate that LLM judgments of summary quality do not match the feedback\nfrom the writers.",
        "pdf_link": "https://arxiv.org/pdf/2403.01061v1.pdf"
    },
    {
        "title": "Towards Full Authorship with AI: Supporting Revision with AI-Generated Views",
        "authors": [
            "Jiho Kim",
            "Ray C. Flanagan",
            "Noelle E. Haviland",
            "ZeAi Sun",
            "Souad N. Yakubu",
            "Edom A. Maru",
            "Kenneth C. Arnold"
        ],
        "published": "2024-03-02T01:11:35Z",
        "summary": "Large language models (LLMs) are shaping a new user interface (UI) paradigm\nin writing tools by enabling users to generate text through prompts. This\nparadigm shifts some creative control from the user to the system, thereby\ndiminishing the user's authorship and autonomy in the writing process. To\nrestore autonomy, we introduce Textfocals, a UI prototype designed to\ninvestigate a human-centered approach that emphasizes the user's role in\nwriting. Textfocals supports the writing process by providing LLM-generated\nsummaries, questions, and advice (i.e., LLM views) in a sidebar of a text\neditor, encouraging reflection and self-driven revision in writing without\ndirect text generation. Textfocals' UI affordances, including contextually\nadaptive views and scaffolding for prompt selection and customization, offer a\nnovel way to interact with LLMs where users maintain full authorship of their\nwriting. A formative user study with Textfocals showed promising evidence that\nthis approach might help users develop underdeveloped ideas, cater to the\nrhetorical audience, and clarify their writing. However, the study also showed\ninteraction design challenges related to document navigation and scoping,\nprompt engineering, and context management. Our work highlights the breadth of\nthe design space of writing support interfaces powered by generative AI that\nmaintain authorship integrity.",
        "pdf_link": "https://arxiv.org/pdf/2403.01055v1.pdf"
    },
    {
        "title": "AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks",
        "authors": [
            "Jiacen Xu",
            "Jack W. Stokes",
            "Geoff McDonald",
            "Xuesong Bai",
            "David Marshall",
            "Siyue Wang",
            "Adith Swaminathan",
            "Zhou Li"
        ],
        "published": "2024-03-02T00:10:45Z",
        "summary": "Large language models (LLMs) have demonstrated impressive results on natural\nlanguage tasks, and security researchers are beginning to employ them in both\noffensive and defensive systems. In cyber-security, there have been multiple\nresearch efforts that utilize LLMs focusing on the pre-breach stage of attacks\nlike phishing and malware generation. However, so far there lacks a\ncomprehensive study regarding whether LLM-based systems can be leveraged to\nsimulate the post-breach stage of attacks that are typically human-operated, or\n\"hands-on-keyboard\" attacks, under various attack techniques and environments.\n  As LLMs inevitably advance, they may be able to automate both the pre- and\npost-breach attack stages. This shift may transform organizational attacks from\nrare, expert-led events to frequent, automated operations requiring no\nexpertise and executed at automation speed and scale. This risks fundamentally\nchanging global computer security and correspondingly causing substantial\neconomic impacts, and a goal of this work is to better understand these risks\nnow so we can better prepare for these inevitable ever-more-capable LLMs on the\nhorizon. On the immediate impact side, this research serves three purposes.\nFirst, an automated LLM-based, post-breach exploitation framework can help\nanalysts quickly test and continually improve their organization's network\nsecurity posture against previously unseen attacks. Second, an LLM-based\npenetration test system can extend the effectiveness of red teams with a\nlimited number of human analysts. Finally, this research can help defensive\nsystems and teams learn to detect novel attack behaviors preemptively before\ntheir use in the wild....",
        "pdf_link": "https://arxiv.org/pdf/2403.01038v1.pdf"
    },
    {
        "title": "Peacock: A Family of Arabic Multimodal Large Language Models and Benchmarks",
        "authors": [
            "Fakhraddin Alwajih",
            "El Moatez Billah Nagoudi",
            "Gagan Bhatia",
            "Abdelrahman Mohamed",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2024-03-01T23:38:02Z",
        "summary": "Multimodal large language models (MLLMs) have proven effective in a wide\nrange of tasks requiring complex reasoning and linguistic comprehension.\nHowever, due to a lack of high-quality multimodal resources in languages other\nthan English, success of MLLMs remains relatively limited to English-based\nsettings. This poses significant challenges in developing comparable models for\nother languages, including even those with large speaker populations such as\nArabic. To alleviate this challenge, we introduce a comprehensive family of\nArabic MLLMs, dubbed \\textit{Peacock}, with strong vision and language\ncapabilities. Through comprehensive qualitative and quantitative analysis, we\ndemonstrate the solid performance of our models on various visual reasoning\ntasks and further show their emerging dialectal potential. Additionally, we\nintroduce ~\\textit{Henna}, a new benchmark specifically designed for assessing\nMLLMs on aspects related to Arabic culture, setting the first stone for\nculturally-aware Arabic MLLMs.The GitHub repository for the \\textit{Peacock}\nproject is available at \\url{https://github.com/UBC-NLP/peacock}.",
        "pdf_link": "https://arxiv.org/pdf/2403.01031v1.pdf"
    },
    {
        "title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries",
        "authors": [
            "Zelalem Gero",
            "Chandan Singh",
            "Yiqing Xie",
            "Sheng Zhang",
            "Tristan Naumann",
            "Jianfeng Gao",
            "Hoifung Poon"
        ],
        "published": "2024-03-01T21:59:03Z",
        "summary": "Summarizing clinical text is crucial in health decision-support and clinical\nresearch. Large language models (LLMs) have shown the potential to generate\naccurate clinical text summaries, but still struggle with issues regarding\ngrounding and evaluation, especially in safety-critical domains such as health.\nHolistically evaluating text summaries is challenging because they may contain\nunsubstantiated information. Here, we explore a general mitigation framework\nusing Attribute Structuring (AS), which structures the summary evaluation\nprocess. It decomposes the evaluation process into a grounded procedure that\nuses an LLM for relatively simple structuring and scoring tasks, rather than\nthe full task of holistic summary evaluation. Experiments show that AS\nconsistently improves the correspondence between human annotations and\nautomated metrics in clinical text summarization. Additionally, AS yields\ninterpretations in the form of a short text span corresponding to each output,\nwhich enables efficient human auditing, paving the way towards trustworthy\nevaluation of clinical information in resource-constrained scenarios. We\nrelease our code, prompts, and an open-source benchmark at\nhttps://github.com/microsoft/attribute-structuring.",
        "pdf_link": "https://arxiv.org/pdf/2403.01002v1.pdf"
    },
    {
        "title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods",
        "authors": [
            "Polina Tsvilodub",
            "Hening Wang",
            "Sharon Grosch",
            "Michael Franke"
        ],
        "published": "2024-03-01T21:48:08Z",
        "summary": "This paper systematically compares different methods of deriving item-level\npredictions of language models for multiple-choice tasks. It compares scoring\nmethods for answer options based on free generation of responses, various\nprobability-based scores, a Likert-scale style rating method, and embedding\nsimilarity. In a case study on pragmatic language interpretation, we find that\nLLM predictions are not robust under variation of method choice, both within a\nsingle LLM and across different LLMs. As this variability entails pronounced\nresearcher degrees of freedom in reporting results, knowledge of the\nvariability is crucial to secure robustness of results and research integrity.",
        "pdf_link": "https://arxiv.org/pdf/2403.00998v1.pdf"
    },
    {
        "title": "MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection",
        "authors": [
            "Federico Borra",
            "Claudio Savelli",
            "Giacomo Rosso",
            "Alkis Koudounas",
            "Flavio Giobergia"
        ],
        "published": "2024-03-01T20:31:10Z",
        "summary": "In Natural Language Generation (NLG), contemporary Large Language Models\n(LLMs) face several challenges, such as generating fluent yet inaccurate\noutputs and reliance on fluency-centric metrics. This often leads to neural\nnetworks exhibiting \"hallucinations\". The SHROOM challenge focuses on\nautomatically identifying these hallucinations in the generated text. To tackle\nthese issues, we introduce two key components, a data augmentation pipeline\nincorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a\nvoting ensemble from three models pre-trained on Natural Language Inference\n(NLI) tasks and fine-tuned on diverse datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.00964v1.pdf"
    },
    {
        "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
        "authors": [
            "Yue Niu",
            "Saurav Prakash",
            "Salman Avestimehr"
        ],
        "published": "2024-03-01T19:24:37Z",
        "summary": "We propose a new attention mechanism with linear complexity, ATP, that\nfixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than\non each individual token. Particularly, ATP is driven by an important\nobservation that input sequences are typically low-rank, i.e., input sequences\ncan be represented by a few principal bases. Therefore, instead of directly\niterating over all the input tokens, ATP transforms inputs into an orthogonal\nspace and computes attention only on the top principal bases (keys). Owing to\nthe observed low-rank structure in input sequences, ATP is able to capture\nsemantic relationships in input sequences with a few principal keys.\nFurthermore, the attention complexity is reduced from \\emph{quadratic} to\n\\emph{linear} without incurring a noticeable performance drop. ATP further\nreduces complexity for other linear layers with low-rank inputs, leading to\nmore speedup compared to prior works that solely target the attention module.\nOur evaluations on various models (e.g., BERT and Llama) demonstrate that ATP\nachieves comparable accuracy with much lower computation and memory complexity\nthan the standard attention mechanism. In particular, ATP barely loses accuracy\nwith only $1/2$ principal keys, and only incurs around $2\\%$ accuracy drops\nwith $1/4$ principal keys.",
        "pdf_link": "https://arxiv.org/pdf/2403.02352v1.pdf"
    },
    {
        "title": "Differentially Private Knowledge Distillation via Synthetic Text Generation",
        "authors": [
            "James Flemings",
            "Murali Annavaram"
        ],
        "published": "2024-03-01T19:22:24Z",
        "summary": "Large Language models (LLMs) are achieving state-of-the-art performance in\nmany different downstream tasks. However, the increasing urgency of data\nprivacy requires LLMs to train with Differential Privacy (DP) on private data.\nConcurrently it is also necessary to compress LLMs for real-life deployments on\nresource-constrained devices or latency-sensitive applications. Differential\nprivacy and model compression generally must trade off utility loss to achieve\ntheir objectives. Moreover, concurrently achieving both can result in even more\nutility loss. To this end, we propose a novel differentially private knowledge\ndistillation algorithm that exploits synthetic data generated by a\ndifferentially private LLM. The knowledge of a teacher model is transferred\nonto the student in two ways: one way from the synthetic data itself, the hard\nlabels, and the other way by the output distribution of the teacher model\nevaluated on the synthetic data, the soft labels. Furthermore, if the teacher\nand student share a similar architectural structure, we can further distill\nknowledge by exploiting hidden representations. Our results show that our\nframework substantially improves the utility over existing baselines with\nstrong privacy parameters, {\\epsilon} = 2, validating that we can successfully\ncompress autoregressive LLMs while preserving the privacy of training data.",
        "pdf_link": "https://arxiv.org/pdf/2403.00932v1.pdf"
    },
    {
        "title": "An Interpretable Ensemble of Graph and Language Models for Improving Search Relevance in E-Commerce",
        "authors": [
            "Nurendra Choudhary",
            "Edward W Huang",
            "Karthik Subbian",
            "Chandan K. Reddy"
        ],
        "published": "2024-03-01T19:08:25Z",
        "summary": "The problem of search relevance in the E-commerce domain is a challenging one\nsince it involves understanding the intent of a user's short nuanced query and\nmatching it with the appropriate products in the catalog. This problem has\ntraditionally been addressed using language models (LMs) and graph neural\nnetworks (GNNs) to capture semantic and inter-product behavior signals,\nrespectively. However, the rapid development of new architectures has created a\ngap between research and the practical adoption of these techniques. Evaluating\nthe generalizability of these models for deployment requires extensive\nexperimentation on complex, real-world datasets, which can be non-trivial and\nexpensive. Furthermore, such models often operate on latent space\nrepresentations that are incomprehensible to humans, making it difficult to\nevaluate and compare the effectiveness of different models. This lack of\ninterpretability hinders the development and adoption of new techniques in the\nfield. To bridge this gap, we propose Plug and Play Graph LAnguage Model\n(PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a\nmodular framework with uniform data processing pipelines. It employs additive\nexplanation metrics to independently decide whether to include (i) language\nmodel candidates, (ii) GNN model candidates, and (iii) inter-product behavioral\nsignals. For the task of search relevance, we show that PP-GLAM outperforms\nseveral state-of-the-art baselines as well as a proprietary model on real-world\nmultilingual, multi-regional e-commerce datasets. To promote better model\ncomprehensibility and adoption, we also provide an analysis of the\nexplainability and computational complexity of our model. We also provide the\npublic codebase and provide a deployment strategy for practical implementation.",
        "pdf_link": "https://arxiv.org/pdf/2403.00923v1.pdf"
    },
    {
        "title": "Mitigating Reversal Curse in Large Language Models via Semantic-aware Permutation Training",
        "authors": [
            "Qingyan Guo",
            "Rui Wang",
            "Junliang Guo",
            "Xu Tan",
            "Jiang Bian",
            "Yujiu Yang"
        ],
        "published": "2024-03-01T18:55:20Z",
        "summary": "While large language models (LLMs) have achieved impressive performance\nacross diverse tasks, recent studies showcase that causal LLMs suffer from the\n\"reversal curse\". It is a typical example that the model knows \"A's father is\nB\", but is unable to reason \"B's child is A\". This limitation poses a challenge\nto the advancement of artificial general intelligence (AGI), as it suggests a\ngap in the models' ability to comprehend and apply bidirectional reasoning. In\nthis paper, we first conduct substantial evaluation and identify that the root\ncause of the reversal curse lies in the different word order between the\ntraining and inference stage, namely, the poor ability of causal language\nmodels to predict antecedent words within the training data. Accordingly,\npermutation on the training data is considered as a potential solution, since\nthis can make the model predict antecedent words or tokens. However, previous\npermutation methods may disrupt complete phrases or entities, thereby posing\nchallenges for the model to comprehend and learn from training data. To address\nthis issue, we propose Semantic-aware Permutation Training (SPT), which\naddresses this issue by segmenting the training sentences into semantic units\n(i.e., entities or phrases) with an assistant language model and permuting\nthese units before feeding into the model. Extensive experiments demonstrate\nthat SPT effectively mitigates the reversal curse since the performance on\nreversed questions approximates that on the forward ones, and significantly\nadvances the performance of existing works.",
        "pdf_link": "https://arxiv.org/pdf/2403.00758v3.pdf"
    },
    {
        "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
        "authors": [
            "J\u00e1nos Kram\u00e1r",
            "Tom Lieberum",
            "Rohin Shah",
            "Neel Nanda"
        ],
        "published": "2024-03-01T18:43:51Z",
        "summary": "Activation Patching is a method of directly computing causal attributions of\nbehavior to model components. However, applying it exhaustively requires a\nsweep with cost scaling linearly in the number of model components, which can\nbe prohibitively expensive for SoTA Large Language Models (LLMs). We\ninvestigate Attribution Patching (AtP), a fast gradient-based approximation to\nActivation Patching and find two classes of failure modes of AtP which lead to\nsignificant false negatives. We propose a variant of AtP called AtP*, with two\nchanges to address these failure modes while retaining scalability. We present\nthe first systematic study of AtP and alternative methods for faster activation\npatching and show that AtP significantly outperforms all other investigated\nmethods, with AtP* providing further significant improvement. Finally, we\nprovide a method to bound the probability of remaining false negatives of AtP*\nestimates.",
        "pdf_link": "https://arxiv.org/pdf/2403.00745v1.pdf"
    },
    {
        "title": "Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents",
        "authors": [
            "Dominik Jeurissen",
            "Diego Perez-Liebana",
            "Jeremy Gow",
            "Duygu Cakmak",
            "James Kwan"
        ],
        "published": "2024-03-01T17:22:16Z",
        "summary": "Large Language Models (LLMs) have shown great success as high-level planners\nfor zero-shot game-playing agents. However, these agents are primarily\nevaluated on Minecraft, where long-term planning is relatively straightforward.\nIn contrast, agents tested in dynamic robot environments face limitations due\nto simplistic environments with only a few objects and interactions. To fill\nthis gap in the literature, we present NetPlay, the first LLM-powered zero-shot\nagent for the challenging roguelike NetHack. NetHack is a particularly\nchallenging environment due to its diverse set of items and monsters, complex\ninteractions, and many ways to die.\n  NetPlay uses an architecture designed for dynamic robot environments,\nmodified for NetHack. Like previous approaches, it prompts the LLM to choose\nfrom predefined skills and tracks past interactions to enhance decision-making.\nGiven NetHack's unpredictable nature, NetPlay detects important game events to\ninterrupt running skills, enabling it to react to unforeseen circumstances.\nWhile NetPlay demonstrates considerable flexibility and proficiency in\ninteracting with NetHack's mechanics, it struggles with ambiguous task\ndescriptions and a lack of explicit feedback. Our findings demonstrate that\nNetPlay performs best with detailed context information, indicating the\nnecessity for dynamic methods in supplying context information for complex\ngames such as NetHack.",
        "pdf_link": "https://arxiv.org/pdf/2403.00690v1.pdf"
    },
    {
        "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Kedi Chen",
            "Qin Chen",
            "Jie Zhou",
            "Yishen He",
            "Liang He"
        ],
        "published": "2024-03-01T15:38:55Z",
        "summary": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.",
        "pdf_link": "https://arxiv.org/pdf/2403.00896v1.pdf"
    },
    {
        "title": "Large Language Models for Simultaneous Named Entity Extraction and Spelling Correction",
        "authors": [
            "Edward Whittaker",
            "Ikuo Kitagishi"
        ],
        "published": "2024-03-01T13:36:04Z",
        "summary": "Language Models (LMs) such as BERT, have been shown to perform well on the\ntask of identifying Named Entities (NE) in text. A BERT LM is typically used as\na classifier to classify individual tokens in the input text, or to classify\nspans of tokens, as belonging to one of a set of possible NE categories.\n  In this paper, we hypothesise that decoder-only Large Language Models (LLMs)\ncan also be used generatively to extract both the NE, as well as potentially\nrecover the correct surface form of the NE, where any spelling errors that were\npresent in the input text get automatically corrected.\n  We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on\nthe task of producing NEs from text that was obtained by applying Optical\nCharacter Recognition (OCR) to images of Japanese shop receipts; in this work,\nwe do not attempt to find or evaluate the location of NEs in the text.\n  We show that the best fine-tuned LLM performs as well as, or slightly better\nthan, the best fine-tuned BERT LM, although the differences are not\nsignificant. However, the best LLM is also shown to correct OCR errors in some\ncases, as initially hypothesised.",
        "pdf_link": "https://arxiv.org/pdf/2403.00528v1.pdf"
    },
    {
        "title": "ROME: Memorization Insights from Text, Probability and Hidden State in Large Language Models",
        "authors": [
            "Bo Li",
            "Qinghua Zhao",
            "Lijie Wen"
        ],
        "published": "2024-03-01T13:15:30Z",
        "summary": "Probing the memorization of large language models holds significant\nimportance. Previous works have established metrics for quantifying\nmemorization, explored various influencing factors, such as data duplication,\nmodel size, and prompt length, and evaluated memorization by comparing model\noutputs with training corpora. However, the training corpora are of enormous\nscale and its pre-processing is time-consuming. To explore memorization without\naccessing training data, we propose a novel approach, named ROME, wherein\nmemorization is explored by comparing disparities across memorized and\nnon-memorized. Specifically, models firstly categorize the selected samples\ninto memorized and non-memorized groups, and then comparing the demonstrations\nin the two groups from the insights of text, probability, and hidden state.\nExperimental findings show the disparities in factors including word length,\npart-of-speech, word frequency, mean and variance, just to name a few.",
        "pdf_link": "https://arxiv.org/pdf/2403.00510v2.pdf"
    },
    {
        "title": "Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition",
        "authors": [
            "Ariel Goldstein",
            "Gabriel Stanovsky"
        ],
        "published": "2024-03-01T12:42:47Z",
        "summary": "Recent advances in LLMs have sparked a debate on whether they understand\ntext. In this position paper, we argue that opponents in this debate hold\ndifferent definitions for understanding, and particularly differ in their view\non the role of consciousness. To substantiate this claim, we propose a thought\nexperiment involving an open-source chatbot $Z$ which excels on every possible\nbenchmark, seemingly without subjective experience. We ask whether $Z$ is\ncapable of understanding, and show that different schools of thought within\nseminal AI research seem to answer this question differently, uncovering their\nterminological disagreement. Moving forward, we propose two distinct working\ndefinitions for understanding which explicitly acknowledge the question of\nconsciousness, and draw connections with a rich literature in philosophy,\npsychology and neuroscience.",
        "pdf_link": "https://arxiv.org/pdf/2403.00499v1.pdf"
    },
    {
        "title": "TempCompass: Do Video LLMs Really Understand Videos?",
        "authors": [
            "Yuanxin Liu",
            "Shicheng Li",
            "Yi Liu",
            "Yuxiang Wang",
            "Shuhuai Ren",
            "Lei Li",
            "Sishuo Chen",
            "Xu Sun",
            "Lu Hou"
        ],
        "published": "2024-03-01T12:02:19Z",
        "summary": "Recently, there is a surge in interest surrounding video large language\nmodels (Video LLMs). However, existing benchmarks fail to provide a\ncomprehensive feedback on the temporal perception ability of Video LLMs. On the\none hand, most of them are unable to distinguish between different temporal\naspects (e.g., speed, direction) and thus cannot reflect the nuanced\nperformance on these specific aspects. On the other hand, they are limited in\nthe diversity of task formats (e.g., only multi-choice QA), which hinders the\nunderstanding of how temporal perception performance may vary across different\ntypes of tasks. Motivated by these two problems, we propose the\n\\textbf{TempCompass} benchmark, which introduces a diversity of temporal\naspects and task formats. To collect high-quality test data, we devise two\nnovel strategies: (1) In video collection, we construct conflicting videos that\nshare the same static content but differ in a specific temporal aspect, which\nprevents Video LLMs from leveraging single-frame bias or language priors. (2)\nTo collect the task instructions, we propose a paradigm where humans first\nannotate meta-information for a video and then an LLM generates the\ninstruction. We also design an LLM-based approach to automatically and\naccurately evaluate the responses from Video LLMs. Based on TempCompass, we\ncomprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs,\nand reveal the discerning fact that these models exhibit notably poor temporal\nperception ability. The data and evaluation code are available at\nhttps://github.com/llyx97/TempCompass.",
        "pdf_link": "https://arxiv.org/pdf/2403.00476v2.pdf"
    },
    {
        "title": "LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues",
        "authors": [
            "Joe Stacey",
            "Jianpeng Cheng",
            "John Torr",
            "Tristan Guigue",
            "Joris Driesen",
            "Alexandru Coca",
            "Mark Gaynor",
            "Anders Johannsen"
        ],
        "published": "2024-03-01T11:33:53Z",
        "summary": "Virtual assistants are poised to take a dramatic leap forward in terms of\ntheir dialogue capabilities, spurred by recent advances in transformer-based\nLarge Language Models (LLMs). Yet a major bottleneck to achieving genuinely\ntransformative task-oriented dialogue capabilities remains the scarcity of high\nquality and linguistically sophisticated data. Existing datasets, while\nimpressive in scale, have limited domain coverage and contain few genuinely\nchallenging conversational phenomena; those which are present are typically\nunlabelled, making it difficult to assess the strengths and weaknesses of\nmodels without time-consuming and costly human evaluation. Moreover, creating\nhigh quality dialogue data has until now required considerable human input,\nlimiting both the scale of these datasets and the ability to rapidly bootstrap\ndata for a new target domain. We aim to overcome these issues with LUCID, a\nmodularised and highly automated LLM-driven data generation system that\nproduces realistic, diverse and challenging dialogues. We use LUCID to generate\na seed dataset of 4,277 multi-domain, multi-intent conversations across 100\nintents to demonstrate its capabilities. The generated conversations include a\nwide range of challenging phenomena and diverse user behaviour, conveniently\nidentifiable via a set of turn-level tags. Finally, we provide separate test\nsets for seen and unseen intents, allowing for convenient out-of-distribution\nevaluation. We release both the data generation code and the dataset itself.",
        "pdf_link": "https://arxiv.org/pdf/2403.00462v1.pdf"
    },
    {
        "title": "Hierarchical Indexing for Retrieval-Augmented Opinion Summarization",
        "authors": [
            "Tom Hosking",
            "Hao Tang",
            "Mirella Lapata"
        ],
        "published": "2024-03-01T10:38:07Z",
        "summary": "We propose a method for unsupervised abstractive opinion summarization, that\ncombines the attributability and scalability of extractive approaches with the\ncoherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns\nan index structure that maps sentences to a path through a semantically\norganized discrete hierarchy. At inference time, we populate the index and use\nit to identify and retrieve clusters of sentences containing popular opinions\nfrom input reviews. Then, we use a pretrained LLM to generate a readable\nsummary that is grounded in these extracted evidential clusters. The modularity\nof our approach allows us to evaluate its efficacy at each stage. We show that\nHIRO learns an encoding space that is more semantically structured than prior\nwork, and generates summaries that are more representative of the opinions in\nthe input reviews. Human evaluation confirms that HIRO generates more coherent,\ndetailed and accurate summaries that are significantly preferred by annotators\ncompared to prior work.",
        "pdf_link": "https://arxiv.org/pdf/2403.00435v1.pdf"
    },
    {
        "title": "LLMs for Targeted Sentiment in News Headlines: Exploring Different Levels of Prompt Prescriptiveness",
        "authors": [
            "Jana Juro\u0161",
            "Laura Majer",
            "Jan \u0160najder"
        ],
        "published": "2024-03-01T10:10:34Z",
        "summary": "News headlines often evoke sentiment by intentionally portraying entities in\nparticular ways, making targeted sentiment analysis (TSA) of headlines a\nworthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA\nperformance, but their background knowledge is limited, and they require a\nlabeled dataset. LLMs offer a potentially universal solution for TSA due to\ntheir broad linguistic and world knowledge along with in-context learning\nabilities, yet their performance is heavily influenced by prompt design.\nDrawing parallels with annotation paradigms for subjective tasks, we explore\nthe influence of prompt design on the performance of LLMs for TSA of news\nheadlines. We evaluate the predictive accuracy of state-of-the-art LLMs using\nprompts with different levels of prescriptiveness, ranging from plain zero-shot\nto elaborate few-shot prompts matching annotation guidelines. Recognizing the\nsubjective nature of TSA, we evaluate the ability of LLMs to quantify\npredictive uncertainty via calibration error and correlation to human\ninter-annotator agreement. We find that, except for few-shot prompting,\ncalibration and F1-score improve with increased prescriptiveness, but the\noptimal level depends on the model.",
        "pdf_link": "https://arxiv.org/pdf/2403.00418v1.pdf"
    },
    {
        "title": "Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models",
        "authors": [
            "Jinbiao Yang"
        ],
        "published": "2024-03-01T10:03:07Z",
        "summary": "Tokenization significantly influences language models(LMs)' performance. This\npaper traces the evolution of tokenizers from word-level to subword-level,\nanalyzing how they balance tokens and types to enhance model adaptability while\ncontrolling complexity. Despite subword tokenizers like Byte Pair Encoding\n(BPE) overcoming many word tokenizer limitations, they encounter difficulties\nin handling non-Latin languages and depend heavily on extensive training data\nand computational resources to grasp the nuances of multiword expressions\n(MWEs). This article argues that tokenizers, more than mere technical tools,\nshould drawing inspiration from the cognitive science about human language\nprocessing. This study then introduces the \"Principle of Least Effort\" from\ncognitive science, that humans naturally seek to reduce cognitive effort, and\ndiscusses the benefits of this principle for tokenizer development. Based on\nthis principle, the paper proposes that the Less-is-Better (LiB) model could be\na new approach for LLM tokenizer. The LiB model can autonomously learn an\nintegrated vocabulary consisting of subwords, words, and MWEs, which\neffectively reduces both the numbers of tokens and types. Comparative\nevaluations show that the LiB tokenizer outperforms existing word and BPE\ntokenizers, presenting an innovative method for tokenizer development, and\nhinting at the possibility of future cognitive science-based tokenizers being\nmore efficient.",
        "pdf_link": "https://arxiv.org/pdf/2403.00417v1.pdf"
    },
    {
        "title": "Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment",
        "authors": [
            "Margherita Martorana",
            "Tobias Kuhn",
            "Lise Stork",
            "Jacco van Ossenbruggen"
        ],
        "published": "2024-03-01T10:01:36Z",
        "summary": "Traditional dataset retrieval systems index on metadata information rather\nthan on the data values. Thus relying primarily on manual annotations and\nhigh-quality metadata, processes known to be labour-intensive and challenging\nto automate. We propose a method to support metadata enrichment with topic\nannotations of column headers using three Large Language Models (LLMs):\nChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to\nclassify column headers based on domain-specific topics from a controlled\nvocabulary. We evaluate our approach by assessing the internal consistency of\nthe LLMs, the inter-machine alignment, and the human-machine agreement for the\ntopic classification task. Additionally, we investigate the impact of\ncontextual information (i.e. dataset description) on the classification\noutcomes. Our results suggest that ChatGPT and GoogleGemini outperform\nGoogleBard for internal consistency as well as LLM-human-alignment.\nInterestingly, we found that context had no impact on the LLMs performances.\nThis work proposes a novel approach that leverages LLMs for text classification\nusing a controlled topic vocabulary, which has the potential to facilitate\nautomated metadata enrichment, thereby enhancing dataset retrieval and the\nFindability, Accessibility, Interoperability and Reusability (FAIR) of research\ndata on the Web.",
        "pdf_link": "https://arxiv.org/pdf/2403.00884v2.pdf"
    },
    {
        "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
        "authors": [
            "Nishanth Chandran",
            "Sunayana Sitaram",
            "Divya Gupta",
            "Rahul Sharma",
            "Kashish Mittal",
            "Manohar Swaminathan"
        ],
        "published": "2024-03-01T09:28:38Z",
        "summary": "Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,\nreplicability and low cost. However, recent work has pointed out that the\nmajority of the open source benchmarks available today have been contaminated\nor leaked into LLMs, meaning that LLMs have access to test data during\npretraining and/or fine-tuning. This raises serious concerns about the validity\nof benchmarking studies conducted so far and the future of evaluation using\nbenchmarks. To solve this problem, we propose Private Benchmarking, a solution\nwhere test datasets are kept private and models are evaluated without revealing\nthe test data to the model. We describe various scenarios (depending on the\ntrust placed on model owners or dataset owners), and present solutions to avoid\ndata contamination using private benchmarking. For scenarios where the model\nweights need to be kept private, we describe solutions from confidential\ncomputing and cryptography that can aid in private benchmarking. Finally, we\npresent solutions the problem of benchmark dataset auditing, to ensure that\nprivate benchmarks are of sufficiently high quality.",
        "pdf_link": "https://arxiv.org/pdf/2403.00393v1.pdf"
    },
    {
        "title": "Invariant Test-Time Adaptation for Vision-Language Model Generalization",
        "authors": [
            "Huan Ma",
            "Yan Zhu",
            "Changqing Zhang",
            "Peilin Zhao",
            "Baoyuan Wu",
            "Long-Kai Huang",
            "Qinghua Hu",
            "Bingzhe Wu"
        ],
        "published": "2024-03-01T09:01:53Z",
        "summary": "Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired datasets. However, these models display significant limitations when\napplied to long-tail tasks, such as fine-grained image classification, as a\nresult of \"decision shortcuts\" that hinders their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, this paper introduces a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit genuine\ncausal invariant features while disregarding decision shortcuts during the\ninference phase. The proposed method effectively alleviates excessive\ndependence on potentially misleading, task-irrelevant contextual information,\nwhile concurrently emphasizing critical, task-related visual cues. We conduct\ncomparative analysis of the proposed method against various approaches which\nvalidates its effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.00376v1.pdf"
    },
    {
        "title": "Crimson: Empowering Strategic Reasoning in Cybersecurity through Large Language Models",
        "authors": [
            "Jiandong Jin",
            "Bowen Tang",
            "Mingxuan Ma",
            "Xiao Liu",
            "Yunfei Wang",
            "Qingnan Lai",
            "Jia Yang",
            "Changling Zhou"
        ],
        "published": "2024-03-01T08:43:43Z",
        "summary": "We introduces Crimson, a system that enhances the strategic reasoning\ncapabilities of Large Language Models (LLMs) within the realm of cybersecurity.\nBy correlating CVEs with MITRE ATT&CK techniques, Crimson advances threat\nanticipation and strategic defense efforts. Our approach includes defining and\nevaluating cybersecurity strategic tasks, alongside implementing a\ncomprehensive human-in-the-loop data-synthetic workflow to develop the\nCVE-to-ATT&CK Mapping (CVEM) dataset. We further enhance LLMs' reasoning\nabilities through a novel Retrieval-Aware Training (RAT) process and its\nrefined iteration, RAT-R.\n  Our findings demonstrate that an LLM fine-tuned with our techniques,\npossessing 7 billion parameters, approaches the performance level of GPT-4,\nshowing markedly lower rates of hallucination and errors, and surpassing other\nmodels in strategic reasoning tasks. Moreover, domain-specific fine-tuning of\nembedding models significantly improves performance within cybersecurity\ncontexts, underscoring the efficacy of our methodology. By leveraging Crimson\nto convert raw vulnerability data into structured and actionable insights, we\nbolster proactive cybersecurity defenses.",
        "pdf_link": "https://arxiv.org/pdf/2403.00878v1.pdf"
    },
    {
        "title": "Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models",
        "authors": [
            "Xianzhen Luo",
            "Qingfu Zhu",
            "Zhiming Zhang",
            "Xu Wang",
            "Qing Yang",
            "Dongliang Xu",
            "Wanxiang Che"
        ],
        "published": "2024-03-01T08:05:44Z",
        "summary": "Instruction tuning plays a pivotal role in Code Large Language Models (Code\nLLMs) for the task of program synthesis. Presently, two dominant paradigms for\ncollecting tuning data are natural-instruct (human-written) and self-instruct\n(automatically generated). Natural-instruct includes diverse and correct codes\nbut lacks instruction-code pairs, and exists improper code formats like nested\nsingle-line codes. In contrast, self-instruct automatically generates proper\npaired data. However, it suffers from low diversity due to generating\nduplicates and cannot ensure the correctness of codes. To bridge the both\nparadigms, we propose \\textbf{Semi-Instruct}. It first converts diverse but\nimproper codes from natural-instruct into proper instruction-code pairs through\na method similar to self-instruct. To verify the correctness of generated\ncodes, we design a novel way to construct test cases by generating cases'\ninputs and executing correct codes from natural-instruct to get outputs.\nFinally, diverse and correct instruction-code pairs are retained for\ninstruction tuning. Experiments show that semi-instruct is significantly better\nthan natural-instruct and self-instruct. Furthermore, the performance steadily\nimproves as data scale increases.",
        "pdf_link": "https://arxiv.org/pdf/2403.00338v1.pdf"
    },
    {
        "title": "Never-Ending Embodied Robot Learning",
        "authors": [
            "Wenqi Liang",
            "Gan Sun",
            "Qian He",
            "Yu Ren",
            "Jiahua Dong",
            "Yang Cong"
        ],
        "published": "2024-03-01T07:51:29Z",
        "summary": "Relying on large language models (LLMs), embodied robots could perform\ncomplex multimodal robot manipulation tasks from visual observations with\npowerful generalization ability. However, most visual behavior-cloning agents\nsuffer from manipulation performance degradation and skill knowledge forgetting\nwhen adapting into a series of challenging unseen tasks. We here investigate\nthe above challenge with NBCagent in embodied robots, a pioneering\nlanguage-conditioned Never-ending Behavior-Cloning agent, which can continually\nlearn observation knowledge of novel robot manipulation skills from\nskill-specific and skill-shared attributes. Specifically, we establish a\nskill-specific evolving planner to perform knowledge decoupling, which can\ncontinually embed novel skill-specific knowledge in our NBCagent agent from\nlatent and low-rank space. Meanwhile, we propose a skill-shared semantics\nrendering module and a skill-shared representation distillation module to\neffectively transfer anti-forgetting skill-shared knowledge, further tackling\ncatastrophic forgetting on old skills from semantics and representation\naspects. Finally, we design a continual embodied robot manipulation benchmark,\nand several expensive experiments demonstrate the significant performance of\nour method. Visual results, code, and dataset are provided at:\nhttps://neragent.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2403.00336v1.pdf"
    },
    {
        "title": "Teach LLMs to Phish: Stealing Private Information from Language Models",
        "authors": [
            "Ashwinee Panda",
            "Christopher A. Choquette-Choo",
            "Zhengming Zhang",
            "Yaoqing Yang",
            "Prateek Mittal"
        ],
        "published": "2024-03-01T06:15:07Z",
        "summary": "When large language models are trained on private data, it can be a\nsignificant privacy risk for them to memorize and regurgitate sensitive\ninformation. In this work, we propose a new practical data extraction attack\nthat we call \"neural phishing\". This attack enables an adversary to target and\nextract sensitive or personally identifiable information (PII), e.g., credit\ncard numbers, from a model trained on user data with upwards of 10% attack\nsuccess rates, at times, as high as 50%. Our attack assumes only that an\nadversary can insert as few as 10s of benign-appearing sentences into the\ntraining dataset using only vague priors on the structure of the user data.",
        "pdf_link": "https://arxiv.org/pdf/2403.00871v1.pdf"
    },
    {
        "title": "DPP-Based Adversarial Prompt Searching for Lanugage Models",
        "authors": [
            "Xu Zhang",
            "Xiaojun Wan"
        ],
        "published": "2024-03-01T05:28:06Z",
        "summary": "Language models risk generating mindless and offensive content, which hinders\ntheir safe deployment. Therefore, it is crucial to discover and modify\npotential toxic outputs of pre-trained language models before deployment. In\nthis work, we elicit toxic content by automatically searching for a prompt that\ndirects pre-trained language models towards the generation of a specific target\noutput. The problem is challenging due to the discrete nature of textual data\nand the considerable computational resources required for a single forward pass\nof the language model. To combat these challenges, we introduce Auto-regressive\nSelective Replacement Ascent (ASRA), a discrete optimization algorithm that\nselects prompts based on both quality and similarity with determinantal point\nprocess (DPP). Experimental results on six different pre-trained language\nmodels demonstrate the efficacy of ASRA for eliciting toxic content.\nFurthermore, our analysis reveals a strong correlation between the success rate\nof ASRA attacks and the perplexity of target outputs, while indicating limited\nassociation with the quantity of model parameters.",
        "pdf_link": "https://arxiv.org/pdf/2403.00292v1.pdf"
    },
    {
        "title": "Gender Bias in Large Language Models across Multiple Languages",
        "authors": [
            "Jinman Zhao",
            "Yitian Ding",
            "Chen Jia",
            "Yining Wang",
            "Zifan Qian"
        ],
        "published": "2024-03-01T04:47:16Z",
        "summary": "With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.",
        "pdf_link": "https://arxiv.org/pdf/2403.00277v1.pdf"
    },
    {
        "title": "SoftTiger: A Clinical Foundation Model for Healthcare Workflows",
        "authors": [
            "Ye Chen",
            "Igor Couto",
            "Wei Cai",
            "Cong Fu",
            "Bruno Dorneles"
        ],
        "published": "2024-03-01T04:39:16Z",
        "summary": "We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.",
        "pdf_link": "https://arxiv.org/pdf/2403.00868v2.pdf"
    },
    {
        "title": "Extracting Polymer Nanocomposite Samples from Full-Length Documents",
        "authors": [
            "Ghazal Khalighinejad",
            "Defne Circi",
            "L. C. Brinson",
            "Bhuwan Dhingra"
        ],
        "published": "2024-03-01T03:51:56Z",
        "summary": "This paper investigates the use of large language models (LLMs) for\nextracting sample lists of polymer nanocomposites (PNCs) from full-length\nmaterials science research papers. The challenge lies in the complex nature of\nPNC samples, which have numerous attributes scattered throughout the text. The\ncomplexity of annotating detailed information on PNCs limits the availability\nof data, making conventional document-level relation extraction techniques\nimpractical due to the challenge in creating comprehensive named entity span\nannotations. To address this, we introduce a new benchmark and an evaluation\ntechnique for this task and explore different prompting strategies in a\nzero-shot manner. We also incorporate self-consistency to improve the\nperformance. Our findings show that even advanced LLMs struggle to extract all\nof the samples from an article. Finally, we analyze the errors encountered in\nthis process, categorizing them into three main challenges, and discuss\npotential strategies for future research to overcome them.",
        "pdf_link": "https://arxiv.org/pdf/2403.00260v1.pdf"
    },
    {
        "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes",
        "authors": [
            "Xiaomeng Hu",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "published": "2024-03-01T03:29:54Z",
        "summary": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.",
        "pdf_link": "https://arxiv.org/pdf/2403.00867v2.pdf"
    },
    {
        "title": "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models",
        "authors": [
            "Lei Li",
            "Yuqi Wang",
            "Runxin Xu",
            "Peiyi Wang",
            "Xiachong Feng",
            "Lingpeng Kong",
            "Qi Liu"
        ],
        "published": "2024-03-01T02:21:30Z",
        "summary": "Large vision-language models (LVLMs), exemplified by GPT-4V, excel across\ndiverse tasks involving concrete images from natural scenes. However, their\nability to interpret abstract figures, such as geometry shapes and scientific\nplots, remains limited due to a scarcity of training datasets in scientific\ndomains. To fill this gap, we introduce Multimodal ArXiv, consisting of\nArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is\na figure-caption dataset comprising 6.4M images and 3.9M captions sourced from\n572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap,\nwe introduce ArXivQA, a question-answering dataset generated by prompting\nGPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs'\nmathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain\non a multimodal mathematical reasoning benchmark. Furthermore, employing\nArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.\nEvaluation results with state-of-the-art LVLMs underscore their struggle with\nthe nuanced semantics of academic figures, with domain-specific training\nyielding substantial performance gains. Our error analysis uncovers\nmisinterpretations of visual context, recognition errors, and the production of\noverly simplified captions by current LVLMs, shedding light on future\nimprovements.",
        "pdf_link": "https://arxiv.org/pdf/2403.00231v2.pdf"
    },
    {
        "title": "Improving Socratic Question Generation using Data Augmentation and Preference Optimization",
        "authors": [
            "Nischal Ashok Kumar",
            "Andrew Lan"
        ],
        "published": "2024-03-01T00:08:20Z",
        "summary": "The Socratic method is a way of guiding students toward solving a problem\nindependently without directly revealing the solution to the problem. Although\nthis method has been shown to significantly improve student learning outcomes,\nit remains a complex labor-intensive task for instructors. Large language\nmodels (LLMs) can be used to augment human effort by automatically generating\nSocratic questions for students. However, existing methods that involve\nprompting these LLMs sometimes produce invalid outputs, e.g., those that\ndirectly reveal the solution to the problem or provide irrelevant or premature\nquestions. To alleviate this problem, inspired by reinforcement learning with\nAI feedback (RLAIF), we first propose a data augmentation method to enrich\nexisting Socratic questioning datasets with questions that are invalid in\nspecific ways. Next, we propose a method to optimize open-source LLMs such as\nLLama 2 to prefer ground-truth questions over generated invalid ones, using\ndirect preference optimization (DPO). Our experiments on a Socratic questions\ndataset for student code debugging show that a DPO-optimized 7B LLama 2 model\ncan effectively avoid generating invalid questions, and as a result,\noutperforms existing state-of-the-art prompting methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.00199v1.pdf"
    },
    {
        "title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
        "authors": [
            "Sana Ebrahimi",
            "Kaiwen Chen",
            "Abolfazl Asudeh",
            "Gautam Das",
            "Nick Koudas"
        ],
        "published": "2024-03-01T00:02:37Z",
        "summary": "Pre-trained Large Language Models (LLMs) have significantly advanced natural\nlanguage processing capabilities but are susceptible to biases present in their\ntraining data, leading to unfair outcomes in various applications. While\nnumerous strategies have been proposed to mitigate bias, they often require\nextensive computational resources and may compromise model performance. In this\nwork, we introduce AXOLOTL, a novel post-processing framework, which operates\nagnostically across tasks and models, leveraging public APIs to interact with\nLLMs without direct access to internal parameters. Through a three-step process\nresembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions,\nand guides the model to self-debias its outputs. This approach minimizes\ncomputational costs and preserves model performance, making AXOLOTL a promising\ntool for debiasing LLM outputs with broad applicability and ease of use.",
        "pdf_link": "https://arxiv.org/pdf/2403.00198v1.pdf"
    },
    {
        "title": "LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction",
        "authors": [
            "Chenhao Fang",
            "Xiaohan Li",
            "Zezhong Fan",
            "Jianpeng Xu",
            "Kaushiki Nag",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "published": "2024-02-29T23:03:19Z",
        "summary": "Product attribute value extraction is a pivotal component in Natural Language\nProcessing (NLP) and the contemporary e-commerce industry. The provision of\nprecise product attribute values is fundamental in ensuring high-quality\nrecommendations and enhancing customer satisfaction. The recently emerging\nLarge Language Models (LLMs) have demonstrated state-of-the-art performance in\nnumerous attribute extraction tasks, without the need for domain-specific\ntraining data. Nevertheless, varying strengths and weaknesses are exhibited by\ndifferent LLMs due to the diversity in data, architectures, and\nhyperparameters. This variation makes them complementary to each other, with no\nsingle LLM dominating all others. Considering the diverse strengths and\nweaknesses of LLMs, it becomes necessary to develop an ensemble method that\nleverages their complementary potentials. In this paper, we propose a novel\nalgorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute\nvalue extraction. We iteratively learn the weights for different LLMs to\naggregate the labels with weights to predict the final attribute value. Not\nonly can our proposed method be proven theoretically optimal, but it also\nensures efficient computation, fast convergence, and safe deployment. We have\nalso conducted extensive experiments with various state-of-the-art LLMs,\nincluding Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's\ninternal data. Our offline metrics demonstrate that the LLM-ensemble method\noutperforms all the state-of-the-art single LLMs on Walmart's internal dataset.\nThis method has been launched in several production models, leading to improved\nGross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate\n(CVR), and Add-to-Cart Rate (ATC).",
        "pdf_link": "https://arxiv.org/pdf/2403.00863v1.pdf"
    },
    {
        "title": "TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision",
        "authors": [
            "Yunyi Zhang",
            "Ruozhen Yang",
            "Xueqiang Xu",
            "Jinfeng Xiao",
            "Jiaming Shen",
            "Jiawei Han"
        ],
        "published": "2024-02-29T22:26:07Z",
        "summary": "Hierarchical text classification aims to categorize each document into a set\nof classes in a label taxonomy. Most earlier works focus on fully or\nsemi-supervised methods that require a large amount of human annotated data\nwhich is costly and time-consuming to acquire. To alleviate human efforts, in\nthis paper, we work on hierarchical text classification with the minimal amount\nof supervision: using the sole class name of each node as the only supervision.\nRecently, large language models (LLM) show competitive performance on various\ntasks through zero-shot prompting, but this method performs poorly in the\nhierarchical setting, because it is ineffective to include the large and\nstructured label space in a prompt. On the other hand, previous\nweakly-supervised hierarchical text classification methods only utilize the raw\ntaxonomy skeleton and ignore the rich information hidden in the text corpus\nthat can serve as additional class-indicative features. To tackle the above\nchallenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced\nweakly-supervised hierarchical text classification, which (1) automatically\nenriches the label taxonomy with class-indicative topical terms mined from the\ncorpus to facilitate classifier training and (2) utilizes LLMs for both data\nannotation and creation tailored for the hierarchical label space. Experiments\nshow that TELEClass can outperform previous weakly-supervised hierarchical text\nclassification methods and LLM-based zero-shot prompting methods on two public\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.00165v1.pdf"
    },
    {
        "title": "LLMs in Political Science: Heralding a New Era of Visual Analysis",
        "authors": [
            "Yu Wang"
        ],
        "published": "2024-02-29T22:11:20Z",
        "summary": "Interest is increasing among political scientists in leveraging the extensive\ninformation available in images. However, the challenge of interpreting these\nimages lies in the need for specialized knowledge in computer vision and access\nto specialized hardware. As a result, image analysis has been limited to a\nrelatively small group within the political science community. This landscape\ncould potentially change thanks to the rise of large language models (LLMs).\nThis paper aims to raise awareness of the feasibility of using Gemini for image\ncontent analysis. A retrospective analysis was conducted on a corpus of 688\nimages. Content reports were elicited from Gemini for each image and then\nmanually evaluated by the authors. We find that Gemini is highly accurate in\nperforming object detection, which is arguably the most common and fundamental\ntask in image analysis for political scientists. Equally important, we show\nthat it is easy to implement as the entire command consists of a single prompt\nin natural language; it is fast to run and should meet the time budget of most\nresearchers; and it is free to use and does not require any specialized\nhardware. In addition, we illustrate how political scientists can leverage\nGemini for other image understanding tasks, including face identification,\nsentiment analysis, and caption generation. Our findings suggest that Gemini\nand other similar LLMs have the potential to drastically stimulate and\naccelerate image research in political science and social sciences more\nbroadly.",
        "pdf_link": "https://arxiv.org/pdf/2403.00154v2.pdf"
    },
    {
        "title": "UniTS: Building a Unified Time Series Model",
        "authors": [
            "Shanghua Gao",
            "Teddy Koker",
            "Owen Queen",
            "Thomas Hartvigsen",
            "Theodoros Tsiligkaridis",
            "Marinka Zitnik"
        ],
        "published": "2024-02-29T21:25:58Z",
        "summary": "Foundation models, especially LLMs, are profoundly transforming deep\nlearning. Instead of training many task-specific models, we can adapt a single\npretrained model to many tasks via fewshot prompting or fine-tuning. However,\ncurrent foundation models apply to sequence data but not to time series, which\npresent unique challenges due to the inherent diverse and multidomain time\nseries datasets, diverging task specifications across forecasting,\nclassification and other types of tasks, and the apparent need for\ntask-specialized models. We developed UNITS, a unified time series model that\nsupports a universal task specification, accommodating classification,\nforecasting, imputation, and anomaly detection tasks. This is achieved through\na novel unified network backbone, which incorporates sequence and variable\nattention along with a dynamic linear operator and is trained as a unified\nmodel. Across 38 multi-domain datasets, UNITS demonstrates superior performance\ncompared to task-specific models and repurposed natural language-based LLMs.\nUNITS exhibits remarkable zero-shot, few-shot, and prompt learning capabilities\nwhen evaluated on new data domains and tasks. The source code and datasets are\navailable at https://github.com/mims-harvard/UniTS.",
        "pdf_link": "https://arxiv.org/pdf/2403.00131v1.pdf"
    },
    {
        "title": "FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
        "authors": [
            "Xiaoqiang Wang",
            "Bang Liu",
            "Lingfei Wu"
        ],
        "published": "2024-02-29T21:05:37Z",
        "summary": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.",
        "pdf_link": "https://arxiv.org/pdf/2403.00126v1.pdf"
    },
    {
        "title": "NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications",
        "authors": [
            "Miao Li",
            "Ming-Bin Chen",
            "Bo Tang",
            "Shengbin Hou",
            "Pengyu Wang",
            "Haiying Deng",
            "Zhiyu Li",
            "Feiyu Xiong",
            "Keming Mao",
            "Peng Cheng",
            "Yi Luo"
        ],
        "published": "2024-02-29T21:05:14Z",
        "summary": "This study presents NewsBench, a novel benchmark framework developed to\nevaluate the capability of Large Language Models (LLMs) in Chinese Journalistic\nWriting Proficiency (JWP) and their Safety Adherence (SA), addressing the gap\nbetween journalistic ethics and the risks associated with AI utilization.\nComprising 1,267 tasks across 5 editorial applications, 7 aspects (including\nsafety and journalistic writing with 4 detailed facets), and spanning 24 news\ntopics domains, NewsBench employs two GPT-4 based automatic evaluation\nprotocols validated by human assessment. Our comprehensive analysis of 10 LLMs\nhighlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative\ndeficiency in journalistic ethic adherence during creative writing tasks. These\nfindings underscore the need for enhanced ethical guidance in AI-generated\njournalistic content, marking a step forward in aligning AI capabilities with\njournalistic standards and safety considerations.",
        "pdf_link": "https://arxiv.org/pdf/2403.00862v2.pdf"
    },
    {
        "title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario",
        "authors": [
            "Hongyi Liu",
            "Zirui Liu",
            "Ruixiang Tang",
            "Jiayi Yuan",
            "Shaochen Zhong",
            "Yu-Neng Chuang",
            "Li Li",
            "Rui Chen",
            "Xia Hu"
        ],
        "published": "2024-02-29T20:25:16Z",
        "summary": "Fine-tuning LLMs is crucial to enhancing their task-specific performance and\nensuring model behaviors are aligned with human preferences. Among various\nfine-tuning methods, LoRA is popular for its efficiency and ease to use,\nallowing end-users to easily post and adopt lightweight LoRA modules on\nopen-source platforms to tailor their model for different customization.\nHowever, such a handy share-and-play setting opens up new attack surfaces, that\nthe attacker can render LoRA as an attacker, such as backdoor injection, and\nwidely distribute the adversarial LoRA to the community easily. This can result\nin detrimental outcomes. Despite the huge potential risks of sharing LoRA\nmodules, this aspect however has not been fully explored. To fill the gap, in\nthis study we thoroughly investigate the attack opportunities enabled in the\ngrowing share-and-play scenario. Specifically, we study how to inject backdoor\ninto the LoRA module and dive deeper into LoRA's infection mechanisms. We found\nthat training-free mechanism is possible in LoRA backdoor injection. We also\ndiscover the impact of backdoor attacks with the presence of multiple LoRA\nadaptions concurrently as well as LoRA based backdoor transferability. Our aim\nis to raise awareness of the potential risks under the emerging share-and-play\nscenario, so as to proactively prevent potential consequences caused by\nLoRA-as-an-Attack. Warning: the paper contains potential offensive content\ngenerated by models.",
        "pdf_link": "https://arxiv.org/pdf/2403.00108v1.pdf"
    },
    {
        "title": "Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs",
        "authors": [
            "Raghavv Goel",
            "Mukul Gagrani",
            "Wonseok Jeon",
            "Junyoung Park",
            "Mingu Lee",
            "Christopher Lott"
        ],
        "published": "2024-02-29T19:55:06Z",
        "summary": "Text generation with Large Language Models (LLMs) is known to be memory bound\ndue to the combination of their auto-regressive nature, huge parameter counts,\nand limited memory bandwidths, often resulting in low token rates. Speculative\ndecoding has been proposed as a solution for LLM inference acceleration.\nHowever, since draft models are often unavailable in the modern open-source LLM\nfamilies, e.g., for Llama 2 7B, training a high-quality draft model is required\nto enable inference acceleration via speculative decoding. In this paper, we\npropose a simple draft model training framework for direct alignment to\nchat-capable target models. With the proposed framework, we train Llama 2 Chat\nDrafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\\% of\nthe original size. Our training framework only consists of pretraining,\ndistillation dataset generation, and finetuning with knowledge distillation,\nwith no additional alignment procedure. For the finetuning step, we use\ninstruction-response pairs generated by target model for distillation in\nplausible data distribution, and propose a new Total Variation Distance++\n(TVD++) loss that incorporates variance reduction techniques inspired from the\npolicy gradient method in reinforcement learning. Our empirical results show\nthat Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3\nblock efficiency and 2.4$\\times$ speed-up relative to autoregressive decoding\non various tasks with no further task-specific fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.00858v3.pdf"
    },
    {
        "title": "PROC2PDDL: Open-Domain Planning Representations from Texts",
        "authors": [
            "Tianyi Zhang",
            "Li Zhang",
            "Zhaoyi Hou",
            "Ziyu Wang",
            "Yuling Gu",
            "Peter Clark",
            "Chris Callison-Burch",
            "Niket Tandon"
        ],
        "published": "2024-02-29T19:40:25Z",
        "summary": "Planning in a text-based environment continues to be a major challenge for AI\nsystems. Recent approaches have used language models to predict a planning\ndomain definition (e.g., PDDL) but have only been evaluated in closed-domain\nsimulated environments. To address this, we present Proc2PDDL , the first\ndataset containing open-domain procedural texts paired with expert-annotated\nPDDL representations. Using this dataset, we evaluate state-of-the-art models\non defining the preconditions and effects of actions. We show that Proc2PDDL is\nhighly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around\n35%. Our analysis shows both syntactic and semantic errors, indicating LMs'\ndeficiency in both generating domain-specific prgorams and reasoning about\nevents. We hope this analysis and dataset helps future progress towards\nintegrating the best of LMs and formal planning.",
        "pdf_link": "https://arxiv.org/pdf/2403.00092v1.pdf"
    },
    {
        "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
        "authors": [
            "Suyuchen Wang",
            "Ivan Kobyzev",
            "Peng Lu",
            "Mehdi Rezagholizadeh",
            "Bang Liu"
        ],
        "published": "2024-02-29T19:02:03Z",
        "summary": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios\nin Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),\nwhere models pre-trained on shorter sequences face difficulty with\nout-of-distribution (OOD) token positions in longer sequences. We introduce\nResonance RoPE, a novel approach designed to narrow the generalization gap in\nTSTL scenarios by refining the interpolation of RoPE features for OOD\npositions, significantly improving the model performance without additional\nonline computational costs. Furthermore, we present PosGen, a new synthetic\nbenchmark specifically designed for fine-grained behavior analysis in TSTL\nscenarios, aiming to isolate the constantly increasing difficulty of token\ngeneration on long contexts from the challenges of recognizing new token\npositions. Our experiments on synthetic tasks show that after applying\nResonance RoPE, Transformers recognize OOD position better and more robustly.\nOur extensive LLM experiments also show superior performance after applying\nResonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on\nboth upstream language modeling tasks and a variety of downstream long-text\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2403.00071v1.pdf"
    },
    {
        "title": "Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Elena Khasanova",
            "Xue-Yong Fu",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2024-02-29T19:00:47Z",
        "summary": "This work focuses on the task of query-based meeting summarization in which\nthe summary of a context (meeting transcript) is generated in response to a\nspecific query. When using Large Language Models (LLMs) for this task, a new\ncall to the LLM inference endpoint/API is required for each new query even if\nthe context stays the same. However, repeated calls to the LLM inference\nendpoints would significantly increase the costs of using them in production,\nmaking LLMs impractical for many real-world use cases. To address this problem,\nin this paper, we investigate whether combining the queries for the same input\ncontext in a single prompt to minimize repeated calls can be successfully used\nin meeting summarization. In this regard, we conduct extensive experiments by\ncomparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2,\nMistral, and FLAN-T5 in single-query and multi-query settings. We observe that\nwhile most LLMs tend to respond to the multi-query instructions, almost all of\nthem (except GPT-4), even after fine-tuning, could not properly generate the\nresponse in the required output format. We conclude that while multi-query\nprompting could be useful to optimize the inference costs by reducing calls to\nthe inference endpoints/APIs for the task of meeting summarization, this\ncapability to reliably generate the response in the expected format is only\nlimited to certain LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.00067v1.pdf"
    },
    {
        "title": "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?",
        "authors": [
            "Alex Gu",
            "Wen-Ding Li",
            "Naman Jain",
            "Theo X. Olausson",
            "Celine Lee",
            "Koushik Sen",
            "Armando Solar-Lezama"
        ],
        "published": "2024-02-29T18:59:25Z",
        "summary": "While language models are increasingly more proficient at code generation,\nthey still frequently generate incorrect programs. Many of these programs are\nobviously wrong, but others are more subtle and pass weaker correctness checks\nsuch as being able to compile. In this work, we focus on these counterfeit\nsamples: programs sampled from a language model that 1) have a high enough\nlog-probability to be generated at a moderate temperature and 2) pass weak\ncorrectness checks. Overall, we discover that most models have a very shallow\nunderstanding of counterfeits through three clear failure modes. First, models\nmistakenly classify them as correct. Second, models are worse at reasoning\nabout the execution behaviour of counterfeits and often predict their execution\nresults as if they were correct. Third, when asking models to fix counterfeits,\nthe likelihood of a model successfully repairing a counterfeit is often even\nlower than that of sampling a correct program from scratch. Counterfeits also\nhave very unexpected properties: first, counterfeit programs for problems that\nare easier for a model to solve are not necessarily easier to detect and only\nslightly easier to execute and repair. Second, counterfeits from a given model\nare just as confusing to the model itself as they are to other models. Finally,\nboth strong and weak models are able to generate counterfeit samples that\nequally challenge all models. In light of our findings, we recommend that care\nand caution be taken when relying on models to understand their own samples,\nespecially when no external feedback is incorporated.",
        "pdf_link": "https://arxiv.org/pdf/2402.19475v1.pdf"
    },
    {
        "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World",
        "authors": [
            "Weiyun Wang",
            "Yiming Ren",
            "Haowen Luo",
            "Tiantong Li",
            "Chenxiang Yan",
            "Zhe Chen",
            "Wenhai Wang",
            "Qingyun Li",
            "Lewei Lu",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "published": "2024-02-29T18:59:17Z",
        "summary": "We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.",
        "pdf_link": "https://arxiv.org/pdf/2402.19474v2.pdf"
    },
    {
        "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
        "authors": [
            "Gabriel Grand",
            "Valerio Pepe",
            "Jacob Andreas",
            "Joshua B. Tenenbaum"
        ],
        "published": "2024-02-29T18:58:15Z",
        "summary": "Questions combine our mastery of language with our remarkable facility for\nreasoning about uncertainty. How do people navigate vast hypothesis spaces to\npose informative questions given limited cognitive resources? We study these\ntradeoffs in a classic grounded question-asking task based on the board game\nBattleship. Our language-informed program sampling (LIPS) model uses large\nlanguage models (LLMs) to generate natural language questions, translate them\ninto symbolic programs, and evaluate their expected information gain. We find\nthat with a surprisingly modest resource budget, this simple Monte Carlo\noptimization strategy yields informative questions that mirror human\nperformance across varied Battleship board scenarios. In contrast, LLM-only\nbaselines struggle to ground questions in the board state; notably, GPT-4V\nprovides no improvement over non-visual baselines. Our results illustrate how\nBayesian models of question-asking can leverage the statistics of language to\ncapture human priors, while highlighting some shortcomings of pure LLMs as\ngrounded reasoners.",
        "pdf_link": "https://arxiv.org/pdf/2402.19471v1.pdf"
    },
    {
        "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
        "authors": [
            "Chen Qian",
            "Jie Zhang",
            "Wei Yao",
            "Dongrui Liu",
            "Zhenfei Yin",
            "Yu Qiao",
            "Yong Liu",
            "Jing Shao"
        ],
        "published": "2024-02-29T18:55:06Z",
        "summary": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}.",
        "pdf_link": "https://arxiv.org/pdf/2402.19465v1.pdf"
    },
    {
        "title": "Curiosity-driven Red-teaming for Large Language Models",
        "authors": [
            "Zhang-Wei Hong",
            "Idan Shenfeld",
            "Tsun-Hsuan Wang",
            "Yung-Sung Chuang",
            "Aldo Pareja",
            "James Glass",
            "Akash Srivastava",
            "Pulkit Agrawal"
        ],
        "published": "2024-02-29T18:55:03Z",
        "summary": "Large language models (LLMs) hold great potential for many natural language\napplications but risk generating incorrect or toxic content. To probe when an\nLLM generates unwanted content, the current paradigm is to recruit a\n\\textit{red team} of human testers to design input prompts (i.e., test cases)\nthat elicit undesirable responses from LLMs. However, relying solely on human\ntesters is expensive and time-consuming. Recent works automate red teaming by\ntraining a separate red team LLM with reinforcement learning (RL) to generate\ntest cases that maximize the chance of eliciting undesirable responses from the\ntarget LLM. However, current RL methods are only able to generate a small\nnumber of effective test cases resulting in a low coverage of the span of\nprompts that elicit undesirable responses from the target LLM. To overcome this\nlimitation, we draw a connection between the problem of increasing the coverage\nof generated test cases and the well-studied approach of curiosity-driven\nexploration that optimizes for novelty. Our method of curiosity-driven red\nteaming (CRT) achieves greater coverage of test cases while mantaining or\nincreasing their effectiveness compared to existing methods. Our method, CRT\nsuccessfully provokes toxic responses from LLaMA2 model that has been heavily\nfine-tuned using human preferences to avoid toxic outputs. Code is available at\n\\url{https://github.com/Improbable-AI/curiosity_redteam}",
        "pdf_link": "https://arxiv.org/pdf/2402.19464v1.pdf"
    },
    {
        "title": "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models",
        "authors": [
            "Frederik Kunstner",
            "Robin Yadav",
            "Alan Milligan",
            "Mark Schmidt",
            "Alberto Bietti"
        ],
        "published": "2024-02-29T18:47:52Z",
        "summary": "Adam has been shown to outperform gradient descent in optimizing large\nlanguage transformers empirically, and by a larger margin than on other tasks,\nbut it is unclear why this happens. We show that the heavy-tailed class\nimbalance found in language modeling tasks leads to difficulties in the\noptimization dynamics. When training with gradient descent, the loss associated\nwith infrequent words decreases slower than the loss associated with frequent\nones. As most samples come from relatively infrequent words, the average loss\ndecreases slowly with gradient descent. On the other hand, Adam and sign-based\nmethods do not suffer from this problem and improve predictions on all classes.\nTo establish that this behavior is indeed caused by class imbalance, we show\nempirically that it persist through different architectures and data types, on\nlanguage transformers, vision CNNs, and linear models. We further study this\nphenomenon on a linear classification with cross-entropy loss, showing that\nheavy-tailed class imbalance leads to ill-conditioning, and that the\nnormalization used by Adam can counteract it.",
        "pdf_link": "https://arxiv.org/pdf/2402.19449v1.pdf"
    },
    {
        "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
        "authors": [
            "Yifei Zhou",
            "Andrea Zanette",
            "Jiayi Pan",
            "Sergey Levine",
            "Aviral Kumar"
        ],
        "published": "2024-02-29T18:45:56Z",
        "summary": "A broad use case of large language models (LLMs) is in goal-directed\ndecision-making tasks (or \"agent\" tasks), where an LLM needs to not just\ngenerate completions for a given prompt, but rather make intelligent decisions\nover a multi-turn interaction to accomplish a task (e.g., when interacting with\nthe web, using tools, or providing customer support). Reinforcement learning\n(RL) provides a general paradigm to address such agent tasks, but current RL\nmethods for LLMs largely focus on optimizing single-turn rewards. By\nconstruction, most single-turn RL methods cannot endow LLMs with the ability to\nintelligently seek information over multiple turns, perform credit assignment,\nor reason about their past actions -- all of which are critical in agent tasks.\nThis raises the question: how can we design effective and efficient multi-turn\nRL algorithms for LLMs? In this paper, we develop a framework for building\nmulti-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility\nof existing single-turn RL methods for LLMs (e.g., proximal policy\noptimization), while accommodating multiple turns, long horizons, and delayed\nrewards effectively. To do this, our framework adopts a hierarchical RL\napproach and runs two RL algorithms in parallel: a high-level off-policy\nvalue-based RL algorithm to aggregate reward over utterances, and a low-level\nRL algorithm that utilizes this high-level value function to train a token\npolicy within each utterance or turn. Our hierarchical framework, Actor-Critic\nFramework with a Hierarchical Structure (ArCHer), can also give rise to other\nRL methods. Empirically, we find that ArCHer significantly improves efficiency\nand performance on agent tasks, attaining a sample efficiency of about 100x\nover existing methods, while also improving with larger model capacity (upto\nthe 7 billion scale that we tested on).",
        "pdf_link": "https://arxiv.org/pdf/2402.19446v1.pdf"
    },
    {
        "title": "Compositional API Recommendation for Library-Oriented Code Generation",
        "authors": [
            "Zexiong Ma",
            "Shengnan An",
            "Bing Xie",
            "Zeqi Lin"
        ],
        "published": "2024-02-29T18:27:27Z",
        "summary": "Large language models (LLMs) have achieved exceptional performance in code\ngeneration. However, the performance remains unsatisfactory in generating\nlibrary-oriented code, especially for the libraries not present in the training\ndata of LLMs. Previous work utilizes API recommendation technology to help LLMs\nuse libraries: it retrieves APIs related to the user requirements, then\nleverages them as context to prompt LLMs. However, developmental requirements\ncan be coarse-grained, requiring a combination of multiple fine-grained APIs.\nThis granularity inconsistency makes API recommendation a challenging task. To\naddress this, we propose CAPIR (Compositional API Recommendation), which adopts\na \"divide-and-conquer\" strategy to recommend APIs for coarse-grained\nrequirements. Specifically, CAPIR employs an LLM-based Decomposer to break down\na coarse-grained task description into several detailed subtasks. Then, CAPIR\napplies an embedding-based Retriever to identify relevant APIs corresponding to\neach subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out\nredundant APIs and provides the final recommendation. To facilitate the\nevaluation of API recommendation methods on coarse-grained requirements, we\npresent two challenging benchmarks, RAPID (Recommend APIs based on\nDocumentation) and LOCG (Library-Oriented Code Generation). Experimental\nresults on these benchmarks, demonstrate the effectiveness of CAPIR in\ncomparison to existing baselines. Specifically, on RAPID's Torchdata-AR\ndataset, compared to the state-of-the-art API recommendation approach, CAPIR\nimproves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On\nLOCG's Torchdata-Code dataset, compared to code generation without API\nrecommendation, CAPIR improves pass@100 from 16.0% to 28.0%.",
        "pdf_link": "https://arxiv.org/pdf/2402.19431v1.pdf"
    },
    {
        "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines",
        "authors": [
            "Lijia Ma",
            "Xingchen Xu",
            "Yong Tan"
        ],
        "published": "2024-02-29T18:20:37Z",
        "summary": "In the domain of digital information dissemination, search engines act as\npivotal conduits linking information seekers with providers. The advent of\nchat-based search engines utilizing Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary\nleap in the search ecosystem. They demonstrate metacognitive abilities in\ninterpreting web information and crafting responses with human-like\nunderstanding and creativity. Nonetheless, the intricate nature of LLMs renders\ntheir \"cognitive\" processes opaque, challenging even their designers'\nunderstanding. This research aims to dissect the mechanisms through which an\nLLM-powered chat-based search engine, specifically Bing Chat, selects\ninformation sources for its responses. To this end, an extensive dataset has\nbeen compiled through engagements with New Bing, documenting the websites it\ncites alongside those listed by the conventional search engine. Employing\nnatural language processing (NLP) techniques, the research reveals that Bing\nChat exhibits a preference for content that is not only readable and formally\nstructured, but also demonstrates lower perplexity levels, indicating a unique\ninclination towards text that is predictable by the underlying LLM. Further\nenriching our analysis, we procure an additional dataset through interactions\nwith the GPT-4 based knowledge retrieval API, unveiling a congruent text\npreference between the RAG API and Bing Chat. This consensus suggests that\nthese text preferences intrinsically emerge from the underlying language\nmodels, rather than being explicitly crafted by Bing Chat's developers.\nMoreover, our investigation documents a greater similarity among websites cited\nby RAG technologies compared to those ranked highest by conventional search\nengines.",
        "pdf_link": "https://arxiv.org/pdf/2402.19421v1.pdf"
    },
    {
        "title": "On the Scaling Laws of Geographical Representation in Language Models",
        "authors": [
            "Nathan Godey",
            "\u00c9ric de la Clergerie",
            "Beno\u00eet Sagot"
        ],
        "published": "2024-02-29T18:04:11Z",
        "summary": "Language models have long been shown to embed geographical information in\ntheir hidden representations. This line of work has recently been revisited by\nextending this result to Large Language Models (LLMs). In this paper, we\npropose to fill the gap between well-established and recent literature by\nobserving how geographical knowledge evolves when scaling language models. We\nshow that geographical knowledge is observable even for tiny models, and that\nit scales consistently as we increase the model size. Notably, we observe that\nlarger language models cannot mitigate the geographical bias that is inherent\nto the training data.",
        "pdf_link": "https://arxiv.org/pdf/2402.19406v2.pdf"
    },
    {
        "title": "Entity-Aware Multimodal Alignment Framework for News Image Captioning",
        "authors": [
            "Junzhe Zhang",
            "Huixuan Zhang",
            "Xiaojun Wan"
        ],
        "published": "2024-02-29T18:03:00Z",
        "summary": "News image captioning task is a variant of image captioning task which\nrequires model to generate a more informative caption with news image and the\nassociated news article. Multimodal Large Language models have developed\nrapidly in recent years and is promising in news image captioning task.\nHowever, according to our experiments, common MLLMs are not good at generating\nthe entities in zero-shot setting. Their abilities to deal with the entities\ninformation are still limited after simply fine-tuned on news image captioning\ndataset. To obtain a more powerful model to handle the multimodal entity\ninformation, we design two multimodal entity-aware alignment tasks and an\nalignment framework to align the model and generate the news image captions.\nOur method achieves better results than previous state-of-the-art models in\nCIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on\nNYTimes800k dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.19404v1.pdf"
    },
    {
        "title": "Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy",
        "authors": [
            "Philipp Schoenegger",
            "Indre Tuminauskaite",
            "Peter S. Park",
            "Philip E. Tetlock"
        ],
        "published": "2024-02-29T17:27:59Z",
        "summary": "Human forecasting accuracy in practice relies on the 'wisdom of the crowd'\neffect, in which predictions about future events are significantly improved by\naggregating across a crowd of individual forecasters. Past work on the\nforecasting ability of large language models (LLMs) suggests that frontier\nLLMs, as individual forecasters, underperform compared to the gold standard of\na human crowd forecasting tournament aggregate. In Study 1, we expand this\nresearch by using an LLM ensemble approach consisting of a crowd of twelve\nLLMs. We compare the aggregated LLM predictions on 31 binary questions to that\nof a crowd of 925 human forecasters from a three-month forecasting tournament.\nOur preregistered main analysis shows that the LLM crowd outperforms a simple\nno-information benchmark and is not statistically different from the human\ncrowd. In exploratory analyses, we find that these two approaches are\nequivalent with respect to medium-effect-size equivalence bounds. We also\nobserve an acquiescence effect, with mean model predictions being significantly\nabove 50%, despite an almost even split of positive and negative resolutions.\nMoreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)\ncan be improved by drawing on human cognitive output. We find that both models'\nforecasting accuracy benefits from exposure to the median human prediction as\ninformation, improving accuracy by between 17% and 28%: though this leads to\nless accurate predictions than simply averaging human and machine forecasts.\nOur results suggest that LLMs can achieve forecasting accuracy rivaling that of\nhuman crowd forecasting tournaments: via the simple, practically applicable\nmethod of forecast aggregation. This replicates the 'wisdom of the crowd'\neffect for LLMs, and opens up their use for a variety of applications\nthroughout society.",
        "pdf_link": "https://arxiv.org/pdf/2402.19379v2.pdf"
    },
    {
        "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
        "authors": [
            "Jenish Maharjan",
            "Anurag Garikipati",
            "Navan Preet Singh",
            "Leo Cyrus",
            "Mayank Sharma",
            "Madalina Ciobanu",
            "Gina Barnes",
            "Rahul Thapa",
            "Qingqing Mao",
            "Ritankar Das"
        ],
        "published": "2024-02-29T17:19:39Z",
        "summary": "LLMs have become increasingly capable at accomplishing a range of\nspecialized-tasks and can be utilized to expand equitable access to medical\nknowledge. Most medical LLMs have involved extensive fine-tuning, leveraging\nspecialized medical data and significant, thus costly, amounts of computational\npower. Many of the top performing LLMs are proprietary and their access is\nlimited to very few research groups. However, open-source (OS) models represent\na key area of growth for medical LLMs due to significant improvements in\nperformance and an inherent ability to provide the transparency and compliance\nrequired in healthcare. We present OpenMedLM, a prompting platform which\ndelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.\nWe evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks\n(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of\nprompting strategies, including zero-shot, few-shot, chain-of-thought (random\nselection and kNN selection), and ensemble/self-consistency voting. We found\nthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,\nsurpassing the previous best performing OS models that leveraged\ncomputationally costly extensive fine-tuning. The model delivers a 72.6%\naccuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and\nachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the\nfirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlight\nmedical-specific emergent properties in OS LLMs which have not yet been\ndocumented to date elsewhere, and showcase the benefits of further leveraging\nprompt engineering to improve the performance of accessible LLMs for medical\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2402.19371v1.pdf"
    },
    {
        "title": "SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency",
        "authors": [
            "Akila Wickramasekara",
            "Frank Breitinger",
            "Mark Scanlon"
        ],
        "published": "2024-02-29T17:13:44Z",
        "summary": "The growing number of cases requiring digital forensic analysis raises\nconcerns about law enforcement's ability to conduct investigations promptly.\nConsequently, this systemisation of knowledge paper delves into the potential\nand effectiveness of integrating Large Language Models (LLMs) into digital\nforensic investigation to address these challenges. A thorough literature\nreview is undertaken, encompassing existing digital forensic models, tools,\nLLMs, deep learning techniques, and the utilisation of LLMs in investigations.\nThe review identifies current challenges within existing digital forensic\nprocesses and explores both the obstacles and possibilities of incorporating\nLLMs. In conclusion, the study asserts that the adoption of LLMs in digital\nforensics, with appropriate constraints, holds the potential to enhance\ninvestigation efficiency, improve traceability, and alleviate technical and\njudicial barriers faced by law enforcement entities.",
        "pdf_link": "https://arxiv.org/pdf/2402.19366v1.pdf"
    },
    {
        "title": "Watermark Stealing in Large Language Models",
        "authors": [
            "Nikola Jovanovi\u0107",
            "Robin Staab",
            "Martin Vechev"
        ],
        "published": "2024-02-29T17:12:39Z",
        "summary": "LLM watermarking has attracted attention as a promising way to detect\nAI-generated content, with some works suggesting that current schemes may\nalready be fit for deployment. In this work we dispute this claim, identifying\nwatermark stealing (WS) as a fundamental vulnerability of these schemes. We\nshow that querying the API of the watermarked LLM to approximately\nreverse-engineer a watermark enables practical spoofing attacks, as suggested\nin prior work, but also greatly boosts scrubbing attacks, which was previously\nunnoticed. We are the first to propose an automated WS algorithm and use it in\nthe first comprehensive study of spoofing and scrubbing in realistic settings.\nWe show that for under $50 an attacker can both spoof and scrub\nstate-of-the-art schemes previously considered safe, with average success rate\nof over 80%. Our findings challenge common beliefs about LLM watermarking,\nstressing the need for more robust schemes. We make all our code and additional\nexamples available at https://watermark-stealing.org.",
        "pdf_link": "https://arxiv.org/pdf/2402.19361v1.pdf"
    },
    {
        "title": "Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",
        "authors": [
            "Xingchen Zou",
            "Yibo Yan",
            "Xixuan Hao",
            "Yuehong Hu",
            "Haomin Wen",
            "Erdong Liu",
            "Junbo Zhang",
            "Yong Li",
            "Tianrui Li",
            "Yu Zheng",
            "Yuxuan Liang"
        ],
        "published": "2024-02-29T16:56:23Z",
        "summary": "As cities continue to burgeon, Urban Computing emerges as a pivotal\ndiscipline for sustainable development by harnessing the power of cross-domain\ndata fusion from diverse sources (e.g., geographical, traffic, social media,\nand environmental data) and modalities (e.g., spatio-temporal, visual, and\ntextual modalities). Recently, we are witnessing a rising trend that utilizes\nvarious deep-learning methods to facilitate cross-domain data fusion in smart\ncities. To this end, we propose the first survey that systematically reviews\nthe latest advancements in deep learning-based data fusion methods tailored for\nurban computing. Specifically, we first delve into data perspective to\ncomprehend the role of each modality and data source. Secondly, we classify the\nmethodology into four primary categories: feature-based, alignment-based,\ncontrast-based, and generation-based fusion methods. Thirdly, we further\ncategorize multi-modal urban applications into seven types: urban planning,\ntransportation, economy, public safety, society, environment, and energy.\nCompared with previous surveys, we focus more on the synergy of deep learning\nmethods with urban computing applications. Furthermore, we shed light on the\ninterplay between Large Language Models (LLMs) and urban computing, postulating\nfuture research directions that could revolutionize the field. We firmly\nbelieve that the taxonomy, progress, and prospects delineated in our survey\nstand poised to significantly enrich the research community. The summary of the\ncomprehensive and up-to-date paper list can be found at\nhttps://github.com/yoshall/Awesome-Multimodal-Urban-Computing.",
        "pdf_link": "https://arxiv.org/pdf/2402.19348v1.pdf"
    },
    {
        "title": "Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction",
        "authors": [
            "Hao Li",
            "Ying Chen",
            "Yifei Chen",
            "Wenxian Yang",
            "Bowen Ding",
            "Yuchen Han",
            "Liansheng Wang",
            "Rongshan Yu"
        ],
        "published": "2024-02-29T16:29:53Z",
        "summary": "Whole Slide Image (WSI) classification is often formulated as a Multiple\nInstance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have\ndemonstrated remarkable performance in WSI classification. However, existing\nmethods leverage coarse-grained pathogenetic descriptions for visual\nrepresentation supervision, which are insufficient to capture the complex\nvisual appearance of pathogenetic images, hindering the generalizability of\nmodels on diverse downstream tasks. Additionally, processing high-resolution\nWSIs can be computationally expensive. In this paper, we propose a novel\n\"Fine-grained Visual-Semantic Interaction\" (FiVE) framework for WSI\nclassification. It is designed to enhance the model's generalizability by\nleveraging the interaction between localized visual patterns and fine-grained\npathological semantics. Specifically, with meticulously designed queries, we\nstart by utilizing a large language model to extract fine-grained pathological\ndescriptions from various non-standardized raw reports. The output descriptions\nare then reconstructed into fine-grained labels used for training. By\nintroducing a Task-specific Fine-grained Semantics (TFS) module, we enable\nprompts to capture crucial visual information in WSIs, which enhances\nrepresentation learning and augments generalization capabilities significantly.\nFurthermore, given that pathological visual patterns are redundantly\ndistributed across tissue slices, we sample a subset of visual instances during\ntraining. Our method demonstrates robust generalizability and strong\ntransferability, dominantly outperforming the counterparts on the TCGA Lung\nCancer dataset with at least 9.19% higher accuracy in few-shot experiments. The\ncode is available at: https://github.com/ls1rius/WSI_FiVE.",
        "pdf_link": "https://arxiv.org/pdf/2402.19326v2.pdf"
    },
    {
        "title": "SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation",
        "authors": [
            "Xue Jiang",
            "Yihong Dong",
            "Zhi Jin",
            "Ge Li"
        ],
        "published": "2024-02-29T16:09:02Z",
        "summary": "Although Large Language Models (LLMs) have made significant progress in code\ngeneration, they still struggle with code generation tasks in specific\nscenarios. These scenarios usually necessitate the adaptation of LLMs to\nfulfill specific needs, but the limited training samples available in practice\nlead to poor code generation performance. Therefore, how to effectively adapt\nLLMs to new scenarios with few training samples is a major challenge for\ncurrent code generation. In this paper, we propose a novel adaptation approach\nnamed SEED, which stands for Sample-Efficient adaptation with Error-Driven\nlearning for code generation. SEED leverages the errors made by LLMs as\nlearning opportunities, using error revision to overcome its own shortcomings,\nthus achieving efficient learning. Specifically, SEED involves identifying\nerror code generated by LLMs, employing Self-revise for code revision,\noptimizing the model with revised code, and iteratively adapting the process\nfor continuous improvement. Experimental results show that, compared to other\nmainstream fine-tuning approaches, SEED achieves superior performance with few\ntraining samples, showing an average relative improvement of 54.7% in Pass@1 on\nmultiple code generation benchmarks. We also validate the effectiveness of\nSelf-revise, which generates revised code that optimizes the model more\nefficiently compared to the code samples from datasets. Moreover, SEED\nconsistently demonstrates strong performance across various LLMs, underscoring\nits generalizability.",
        "pdf_link": "https://arxiv.org/pdf/2403.00046v2.pdf"
    },
    {
        "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
        "authors": [
            "Shaoteng Liu",
            "Haoqi Yuan",
            "Minda Hu",
            "Yanwei Li",
            "Yukang Chen",
            "Shu Liu",
            "Zongqing Lu",
            "Jiaya Jia"
        ],
        "published": "2024-02-29T16:07:22Z",
        "summary": "Large Language Models (LLMs) have demonstrated proficiency in utilizing\nvarious tools by coding, yet they face limitations in handling intricate logic\nand precise control. In embodied tasks, high-level planning is amenable to\ndirect coding, while low-level actions often necessitate task-specific\nrefinement, such as Reinforcement Learning (RL). To seamlessly integrate both\nmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprising\na slow agent and a fast agent. The slow agent analyzes actions suitable for\ncoding, while the fast agent executes coding tasks. This decomposition\neffectively focuses each agent on specific tasks, proving highly efficient\nwithin our pipeline. Our approach outperforms traditional RL methods and\nexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,\nit rapidly obtains diamonds within a single day on an RTX3090. Additionally, it\nachieves SOTA performance across all designated MineDojo tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.19299v1.pdf"
    },
    {
        "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval",
        "authors": [
            "He Zhu",
            "Wenjia Zhang",
            "Nuoxian Huang",
            "Boyang Li",
            "Luyao Niu",
            "Zipei Fan",
            "Tianle Lun",
            "Yicheng Tao",
            "Junyou Su",
            "Zhaoya Gong",
            "Chenyu Fang",
            "Xing Liu"
        ],
        "published": "2024-02-29T15:41:20Z",
        "summary": "In the field of urban planning, general-purpose large language models often\nstruggle to meet the specific needs of planners. Tasks like generating urban\nplanning texts, retrieving related information, and evaluating planning\ndocuments pose unique challenges. To enhance the efficiency of urban\nprofessionals and overcome these obstacles, we introduce PlanGPT, the first\nspecialized Large Language Model tailored for urban and spatial planning.\nDeveloped through collaborative efforts with institutions like the Chinese\nAcademy of Urban Planning, PlanGPT leverages a customized local database\nretrieval framework, domain-specific fine-tuning of base models, and advanced\ntooling capabilities. Empirical tests demonstrate that PlanGPT has achieved\nadvanced performance, delivering responses of superior quality precisely\ntailored to the intricacies of urban planning.",
        "pdf_link": "https://arxiv.org/pdf/2402.19273v1.pdf"
    },
    {
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
        "authors": [
            "Qintong Li",
            "Leyang Cui",
            "Xueliang Zhao",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2024-02-29T15:26:14Z",
        "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious mathematical reasoning benchmarks. However, there are increasing\ndebates regarding whether these models truly understand and apply mathematical\nknowledge or merely rely on shortcuts for mathematical reasoning. One essential\nand frequently occurring evidence is that when the math questions are slightly\nchanged, LLMs can behave incorrectly. This motivates us to evaluate the\nrobustness of LLMs' math reasoning capability by testing a wide range of\nquestion variations. We introduce the adversarial grade school math\n(\\datasetname) dataset, an extension of GSM8K augmented with various\nmathematical perturbations. Our experiments on 25 LLMs and 4 prompting\ntechniques show that while LLMs exhibit different levels of math reasoning\nabilities, their performances are far from robust. In particular, even for\nproblems that have been solved in GSM8K, LLMs can make mistakes when new\nstatements are added or the question targets are altered. We also explore\nwhether more robust performance can be achieved by composing existing prompting\nmethods, in which we try an iterative method that generates and verifies each\nintermediate thought based on its reasoning goal and calculation result. Code\nand data are available at \\url{https://github.com/qtli/GSM-Plus}.",
        "pdf_link": "https://arxiv.org/pdf/2402.19255v1.pdf"
    },
    {
        "title": "Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark",
        "authors": [
            "Zhikun Xu",
            "Yinghui Li",
            "Ruixue Ding",
            "Xinyu Wang",
            "Boli Chen",
            "Yong Jiang",
            "Hai-Tao Zheng",
            "Wenlian Lu",
            "Pengjun Xie",
            "Fei Huang"
        ],
        "published": "2024-02-29T15:22:13Z",
        "summary": "How to better evaluate the capabilities of Large Language Models (LLMs) is\nthe focal point and hot topic in current LLMs research. Previous work has noted\nthat due to the extremely high cost of iterative updates of LLMs, they are\noften unable to answer the latest dynamic questions well. To promote the\nimprovement of Chinese LLMs' ability to answer dynamic questions, in this\npaper, we introduce CDQA, a Chinese Dynamic QA benchmark containing\nquestion-answer pairs related to the latest news on the Chinese Internet. We\nobtain high-quality data through a pipeline that combines humans and models,\nand carefully classify the samples according to the frequency of answer changes\nto facilitate a more fine-grained observation of LLMs' capabilities. We have\nalso evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA.\nExtensive experiments and valuable insights suggest that our proposed CDQA is\nchallenging and worthy of more further study. We believe that the benchmark we\nprovide will become one of the key data resources for improving LLMs' Chinese\nquestion-answering ability in the future.",
        "pdf_link": "https://arxiv.org/pdf/2402.19248v2.pdf"
    },
    {
        "title": "RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks",
        "authors": [
            "Rafael Josip Peni\u0107",
            "Tin Vla\u0161i\u0107",
            "Roland G. Huber",
            "Yue Wan",
            "Mile \u0160iki\u0107"
        ],
        "published": "2024-02-29T14:50:58Z",
        "summary": "Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental\nbiological processes. Recently, RNA has become an interesting drug target,\nemphasizing the need to improve our understanding of its structures and\nfunctions. Over the years, sequencing technologies have produced an enormous\namount of unlabeled RNA data, which hides important knowledge and potential.\nMotivated by the successes of protein language models, we introduce RiboNucleic\nAcid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is\nthe largest RNA language model to date with $650$ million parameters\npre-trained on $36$ million non-coding RNA sequences from several available\ndatabases. RiNALMo is able to extract hidden knowledge and capture the\nunderlying structure information implicitly embedded within the RNA sequences.\nRiNALMo achieves state-of-the-art results on several downstream tasks. Notably,\nwe show that its generalization capabilities can overcome the inability of\nother deep learning methods for secondary structure prediction to generalize on\nunseen RNA families. The code has been made publicly available on\nhttps://github.com/lbcb-sci/RiNALMo.",
        "pdf_link": "https://arxiv.org/pdf/2403.00043v1.pdf"
    },
    {
        "title": "Memory-Augmented Generative Adversarial Transformers",
        "authors": [
            "Stephan Raaijmakers",
            "Roos Bakker",
            "Anita Cremers",
            "Roy de Kleijn",
            "Tom Kouwenhoven",
            "Tessa Verhoef"
        ],
        "published": "2024-02-29T14:47:24Z",
        "summary": "Conversational AI systems that rely on Large Language Models, like\nTransformers, have difficulty interweaving external data (like facts) with the\nlanguage they generate. Vanilla Transformer architectures are not designed for\nanswering factual questions with high accuracy. This paper investigates a\npossible route for addressing this problem. We propose to extend the standard\nTransformer architecture with an additional memory bank holding extra\ninformation (such as facts drawn from a knowledge base), and an extra attention\nlayer for addressing this memory. We add this augmented memory to a Generative\nAdversarial Network-inspired Transformer architecture. This setup allows for\nimplementing arbitrary felicity conditions on the generated language of the\nTransformer. We first demonstrate how this machinery can be deployed for\nhandling factual questions in goal-oriented dialogues. Secondly, we demonstrate\nthat our approach can be useful for applications like {\\it style adaptation} as\nwell: the adaptation of utterances according to certain stylistic (external)\nconstraints, like social properties of human interlocutors in dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2402.19218v1.pdf"
    },
    {
        "title": "PeLLE: Encoder-based language models for Brazilian Portuguese based on open data",
        "authors": [
            "Guilherme Lamartine de Mello",
            "Marcelo Finger",
            "and Felipe Serras",
            "Miguel de Mello Carpi",
            "Marcos Menon Jose",
            "Pedro Henrique Domingues",
            "Paulo Cavalim"
        ],
        "published": "2024-02-29T14:34:03Z",
        "summary": "In this paper we present PeLLE, a family of large language models based on\nthe RoBERTa architecture, for Brazilian Portuguese, trained on curated, open\ndata from the Carolina corpus. Aiming at reproducible results, we describe\ndetails of the pretraining of the models. We also evaluate PeLLE models against\na set of existing multilingual and PT-BR refined pretrained Transformer-based\nLLM encoders, contrasting performance of large versus smaller-but-curated\npretrained models in several downstream tasks. We conclude that several tasks\nperform better with larger models, but some tasks benefit from\nsmaller-but-curated data in its pretraining.",
        "pdf_link": "https://arxiv.org/pdf/2402.19204v1.pdf"
    },
    {
        "title": "PRSA: Prompt Reverse Stealing Attacks against Large Language Models",
        "authors": [
            "Yong Yang",
            "Xuhong Zhang",
            "Yi Jiang",
            "Xi Chen",
            "Haoyu Wang",
            "Shouling Ji",
            "Zonghui Wang"
        ],
        "published": "2024-02-29T14:30:28Z",
        "summary": "Prompt, recognized as crucial intellectual property, enables large language\nmodels (LLMs) to perform specific tasks without the need of fine-tuning,\nunderscoring their escalating importance. With the rise of prompt-based\nservices, such as prompt marketplaces and LLM applications, providers often\ndisplay prompts' capabilities through input-output examples to attract users.\nHowever, this paradigm raises a pivotal security concern: does the exposure of\ninput-output pairs pose the risk of potential prompt leakage, infringing on the\nintellectual property rights of the developers? To our knowledge, this problem\nstill has not been comprehensively explored yet. To remedy this gap, in this\npaper, we perform the first in depth exploration and propose a novel attack\nframework for reverse-stealing prompts against commercial LLMs, namely PRSA.\nThe main idea of PRSA is that by analyzing the critical features of the\ninput-output pairs, we mimic and gradually infer (steal) the target prompts. In\ndetail, PRSA mainly consists of two key phases: prompt mutation and prompt\npruning. In the mutation phase, we propose a prompt attention algorithm based\non differential feedback to capture these critical features for effectively\ninferring the target prompts. In the prompt pruning phase, we identify and mask\nthe words dependent on specific inputs, enabling the prompts to accommodate\ndiverse inputs for generalization. Through extensive evaluation, we verify that\nPRSA poses a severe threat in real world scenarios. We have reported these\nfindings to prompt service providers and actively collaborate with them to take\nprotective measures for prompt copyright.",
        "pdf_link": "https://arxiv.org/pdf/2402.19200v1.pdf"
    },
    {
        "title": "Teaching Large Language Models an Unseen Language on the Fly",
        "authors": [
            "Chen Zhang",
            "Xiao Liu",
            "Jiuheng Lin",
            "Yansong Feng"
        ],
        "published": "2024-02-29T13:50:47Z",
        "summary": "Existing large language models struggle to support numerous low-resource\nlanguages, particularly the extremely low-resource ones where there is minimal\ntraining data available for effective parameter updating. We thus investigate\nwhether LLMs can learn a new language on the fly solely through prompting. To\nstudy this question, we collect a research suite for Zhuang, a language\nsupported by no LLMs currently. We introduce \\textsc{DiPMT++}, a framework for\nadapting LLMs to unseen languages by in-context learning. Using a dictionary\nand only 5K parallel sentences, \\textsc{DiPMT++} significantly enhances the\nperformance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and\nachieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate\nthe practical utility of this framework in aiding humans to translate\ncompletely unseen languages, which could contribute to the preservation of\nlinguistic diversity.",
        "pdf_link": "https://arxiv.org/pdf/2402.19167v1.pdf"
    },
    {
        "title": "Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning",
        "authors": [
            "Wentao Shi",
            "Xiangnan He",
            "Yang Zhang",
            "Chongming Gao",
            "Xinyue Li",
            "Jizhi Zhang",
            "Qifan Wang",
            "Fuli Feng"
        ],
        "published": "2024-02-29T13:49:56Z",
        "summary": "Traditional recommendation setting tends to excessively cater to users'\nimmediate interests and neglect their long-term engagement. To address it, it\nis crucial to incorporate planning capabilities into the recommendation\ndecision-making process to develop policies that take into account both\nimmediate interests and long-term engagement. Despite Reinforcement Learning\n(RL) can learn planning capacity by maximizing cumulative reward, the scarcity\nof recommendation data presents challenges such as instability and\nsusceptibility to overfitting when training RL models from scratch.\n  In this context, we propose to leverage the remarkable planning capabilities\nover sparse data of Large Language Models (LLMs) for long-term recommendation.\nThe key lies in enabling a language model to understand and apply task-solving\nprinciples effectively in personalized recommendation scenarios, as the model's\npre-training may not naturally encompass these principles, necessitating the\nneed to inspire or teach the model. To achieve this, we propose a Bi-level\nLearnable LLM Planner framework, which combines macro-learning and\nmicro-learning through a hierarchical mechanism. The framework includes a\nPlanner and Reflector for acquiring high-level guiding principles and an\nActor-Critic component for planning personalization. Extensive experiments\nvalidate the superiority of the framework in learning to plan for long-term\nrecommendations.",
        "pdf_link": "https://arxiv.org/pdf/2403.00843v1.pdf"
    },
    {
        "title": "Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model",
        "authors": [
            "Hao Cheng",
            "Erjia Xiao",
            "Jindong Gu",
            "Le Yang",
            "Jinhao Duan",
            "Jize Zhang",
            "Jiahang Cao",
            "Kaidi Xu",
            "Renjing Xu"
        ],
        "published": "2024-02-29T13:31:56Z",
        "summary": "Large Vision-Language Models (LVLMs) rely on vision encoders and Large\nLanguage Models (LLMs) to exhibit remarkable capabilities on various\nmulti-modal tasks in the joint space of vision and language. However, the\nTypographic Attack, which disrupts vision-language models (VLMs) such as\nContrastive Language-Image Pretraining (CLIP), has also been expected to be a\nsecurity threat to LVLMs. Firstly, we verify typographic attacks on current\nwell-known commercial and open-source LVLMs and uncover the widespread\nexistence of this threat. Secondly, to better assess this vulnerability, we\npropose the most comprehensive and largest-scale Typographic Dataset to date.\nThe Typographic Dataset not only considers the evaluation of typographic\nattacks under various multi-modal tasks but also evaluates the effects of\ntypographic attacks, influenced by texts generated with diverse factors. Based\non the evaluation results, we investigate the causes why typographic attacks\nmay impact VLMs and LVLMs, leading to three highly insightful discoveries. By\nthe examination of our discoveries and experimental validation in the\nTypographic Dataset, we reduce the performance degradation from $42.07\\%$ to\n$13.90\\%$ when LVLMs confront typographic attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.19150v2.pdf"
    },
    {
        "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
        "authors": [
            "Hongbang Yuan",
            "Pengfei Cao",
            "Zhuoran Jin",
            "Yubo Chen",
            "Daojian Zeng",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2024-02-29T12:35:45Z",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities but still\nsuffer from the issue of hallucinations. A significant type of this issue is\nthe false premise hallucination, which we define as the phenomenon when LLMs\ngenerate hallucinated text when confronted with false premise questions. In\nthis paper, we perform a comprehensive analysis of the false premise\nhallucination and elucidate its internal working mechanism: a small subset of\nattention heads (which we designate as false premise heads) disturb the\nknowledge extraction process, leading to the occurrence of false premise\nhallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse\npremise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating\n\\textbf{H}allucinations), a novel and effective method to mitigate false\npremise hallucinations. It constrains the false premise attention heads during\nthe model inference process. Impressively, extensive experiments demonstrate\nthat constraining only approximately $1\\%$ of the attention heads in the model\nyields a notable increase of nearly $20\\%$ of model performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.19103v1.pdf"
    },
    {
        "title": "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment",
        "authors": [
            "Yiju Guo",
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Jiexin Wang",
            "Huimin Chen",
            "Bowen Sun",
            "Ruobing Xie",
            "Jie Zhou",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-29T12:12:30Z",
        "summary": "Alignment in artificial intelligence pursues the consistency between model\nresponses and human preferences as well as values. In practice, the\nmultifaceted nature of human preferences inadvertently introduces what is known\nas the \"alignment tax\" -a compromise where enhancements in alignment within one\nobjective (e.g.,harmlessness) can diminish performance in others\n(e.g.,helpfulness). However, existing alignment techniques are mostly\nunidirectional, leading to suboptimal trade-offs and poor flexibility over\nvarious objectives. To navigate this challenge, we argue the prominence of\ngrounding LLMs with evident preferences. We introduce controllable preference\noptimization (CPO), which explicitly specifies preference scores for different\nobjectives, thereby guiding the model to generate responses that meet the\nrequirements. Our experimental analysis reveals that the aligned models can\nprovide responses that match various preferences among the \"3H\" (helpfulness,\nhonesty, harmlessness) desiderata. Furthermore, by introducing diverse data and\nalignment goals, we surpass baseline methods in aligning with single\nobjectives, hence mitigating the impact of the alignment tax and achieving\nPareto improvements in multi-objective alignment.",
        "pdf_link": "https://arxiv.org/pdf/2402.19085v1.pdf"
    },
    {
        "title": "Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials",
        "authors": [
            "Gennaro Nolano",
            "Moritz Blum",
            "Basil Ell",
            "Philipp Cimiano"
        ],
        "published": "2024-02-29T12:01:46Z",
        "summary": "In recent years, large language models have achieved state-of-the-art\nperformance across various NLP tasks. However, investigations have shown that\nthese models tend to rely on shortcut features, leading to inaccurate\npredictions and causing the models to be unreliable at generalization to\nout-of-distribution (OOD) samples. For instance, in the context of relation\nextraction (RE), we would expect a model to identify the same relation\nindependently of the entities involved in it. For example, consider the\nsentence \"Leonardo da Vinci painted the Mona Lisa\" expressing the\ncreated(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute \"Leonardo da\nVinci\" with \"Barack Obama\", then the sentence still expresses the created\nrelation. A robust model is supposed to detect the same relation in both cases.\nIn this work, we describe several semantically-motivated strategies to generate\nadversarial examples by replacing entity mentions and investigate how\nstate-of-the-art RE models perform under pressure. Our analyses show that the\nperformance of these models significantly deteriorates on the modified datasets\n(avg. of -48.5% in F1), which indicates that these models rely to a great\nextent on shortcuts, such as surface forms (or patterns therein) of entities,\nwithout making full use of the information present in the sentences.",
        "pdf_link": "https://arxiv.org/pdf/2402.19076v1.pdf"
    },
    {
        "title": "Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study",
        "authors": [
            "Prottay Kumar Adhikary",
            "Aseem Srivastava",
            "Shivani Kumar",
            "Salam Michael Singh",
            "Puneet Manuja",
            "Jini K Gopinath",
            "Vijay Krishnan",
            "Swati Kedia",
            "Koushik Sinha Deb",
            "Tanmoy Chakraborty"
        ],
        "published": "2024-02-29T11:29:47Z",
        "summary": "Comprehensive summaries of sessions enable an effective continuity in mental\nhealth counseling, facilitating informed therapy planning. Yet, manual\nsummarization presents a significant challenge, diverting experts' attention\nfrom the core counseling process. This study evaluates the effectiveness of\nstate-of-the-art Large Language Models (LLMs) in selectively summarizing\nvarious components of therapy sessions through aspect-based summarization,\naiming to benchmark their performance. We introduce MentalCLOUDS, a\ncounseling-component guided summarization dataset consisting of 191 counseling\nsessions with summaries focused on three distinct counseling components (aka\ncounseling aspects). Additionally, we assess the capabilities of 11\nstate-of-the-art LLMs in addressing the task of component-guided summarization\nin counseling. The generated summaries are evaluated quantitatively using\nstandard summarization metrics and verified qualitatively by mental health\nprofessionals. Our findings demonstrate the superior performance of\ntask-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of\nstandard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore\nacross all aspects of counseling components. Further, expert evaluation reveals\nthat Mistral supersedes both MentalLlama and MentalBART based on six parameters\n-- affective attitude, burden, ethicality, coherence, opportunity costs, and\nperceived effectiveness. However, these models share the same weakness by\ndemonstrating a potential for improvement in the opportunity costs and\nperceived effectiveness metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.19052v1.pdf"
    },
    {
        "title": "FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and Scientific Use",
        "authors": [
            "Ingo Weber",
            "Hendrik Linka",
            "Daniel Mertens",
            "Tamara Muryshkin",
            "Heinrich Opgenoorth",
            "Stefan Langer"
        ],
        "published": "2024-02-29T09:43:50Z",
        "summary": "Since OpenAI's release of ChatGPT, generative AI has received significant\nattention across various domains. These AI-based chat systems have the\npotential to enhance the productivity of knowledge workers in diverse tasks.\nHowever, the use of free public services poses a risk of data leakage, as\nservice providers may exploit user input for additional training and\noptimization without clear boundaries. Even subscription-based alternatives\nsometimes lack transparency in handling user data. To address these concerns\nand enable Fraunhofer staff to leverage this technology while ensuring\nconfidentiality, we have designed and developed a customized chat AI called\nFhGenie (genie being a reference to a helpful spirit). Within few days of its\nrelease, thousands of Fraunhofer employees started using this service. As\npioneers in implementing such a system, many other organizations have followed\nsuit. Our solution builds upon commercial large language models (LLMs), which\nwe have carefully integrated into our system to meet our specific requirements\nand compliance constraints, including confidentiality and GDPR. In this paper,\nwe share detailed insights into the architectural considerations, design,\nimplementation, and subsequent updates of FhGenie. Additionally, we discuss\nchallenges, observations, and the core lessons learned from its productive\nusage.",
        "pdf_link": "https://arxiv.org/pdf/2403.00039v1.pdf"
    },
    {
        "title": "EyeGPT: Ophthalmic Assistant with Large Language Models",
        "authors": [
            "Xiaolan Chen",
            "Ziwei Zhao",
            "Weiyi Zhang",
            "Pusheng Xu",
            "Le Gao",
            "Mingpu Xu",
            "Yue Wu",
            "Yinwen Li",
            "Danli Shi",
            "Mingguang He"
        ],
        "published": "2024-02-29T09:35:41Z",
        "summary": "Artificial intelligence (AI) has gained significant attention in healthcare\nconsultation due to its potential to improve clinical workflow and enhance\nmedical communication. However, owing to the complex nature of medical\ninformation, large language models (LLM) trained with general world knowledge\nmight not possess the capability to tackle medical-related tasks at an expert\nlevel. Here, we introduce EyeGPT, a specialized LLM designed specifically for\nophthalmology, using three optimization strategies including role-playing,\nfinetuning, and retrieval-augmented generation. In particular, we proposed a\ncomprehensive evaluation framework that encompasses a diverse dataset, covering\nvarious subspecialties of ophthalmology, different users, and diverse inquiry\nintents. Moreover, we considered multiple evaluation metrics, including\naccuracy, understandability, trustworthiness, empathy, and the proportion of\nhallucinations. By assessing the performance of different EyeGPT variants, we\nidentify the most effective one, which exhibits comparable levels of\nunderstandability, trustworthiness, and empathy to human ophthalmologists (all\nPs>0.05). Overall, ur study provides valuable insights for future research,\nfacilitating comprehensive comparisons and evaluations of different strategies\nfor developing specialized LLMs in ophthalmology. The potential benefits\ninclude enhancing the patient experience in eye care and optimizing\nophthalmologists' services.",
        "pdf_link": "https://arxiv.org/pdf/2403.00840v1.pdf"
    },
    {
        "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
        "authors": [
            "Weijieying Ren",
            "Xinlong Li",
            "Lei Wang",
            "Tianxiang Zhao",
            "Wei Qin"
        ],
        "published": "2024-02-29T05:27:45Z",
        "summary": "Existing research has shown that large language models (LLMs) exhibit\nremarkable performance in language understanding and generation. However, when\nLLMs are continuously fine-tuned on complex and diverse domain-specific\ndownstream tasks, the inference performance on historical tasks decreases\ndramatically, which is known as a catastrophic forgetting problem. A trade-off\nneeds to be kept between learning plasticity and memory stability. Plenty of\nexisting works have explored strategies like memory replay, regularization and\nparameter isolation, but little is known about the geometric connection of\nvarious adjacent minima in the continual LLMs fine-tuning scenarios. In this\nwork, we investigate the geometric connections of different minima through the\nlens of mode connectivity, which means different minima can be connected by a\nlow-loss valley. Through extensive experiments, we uncover the mode\nconnectivity phenomenon in the LLMs continual learning scenario and find that\nit can strike a balance between plasticity and stability. Building upon these\nfindings, we propose a simple yet effective method called Interpolation-based\nLoRA (I-LoRA), which constructs a dual-memory experience replay framework based\non LoRA parameter interpolations. Extensive experiments and analysis on eight\ndomain-specific CL benchmarks demonstrate that I-LoRA consistently show\nsignificant improvement over the previous state-of-the-art approaches with up\nto $11\\%$ performance gains, providing a strong baseline and insights for\nfuture research on the large language model continual learning problem. Our\ncode is available at \\url{https://github.com/which47/LLMCL}.",
        "pdf_link": "https://arxiv.org/pdf/2402.18865v1.pdf"
    },
    {
        "title": "How do Large Language Models Handle Multilingualism?",
        "authors": [
            "Yiran Zhao",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Kenji Kawaguchi",
            "Lidong Bing"
        ],
        "published": "2024-02-29T02:55:26Z",
        "summary": "Large language models (LLMs) demonstrate remarkable performance across a\nspectrum of languages. In this work, we delve into the question: How do LLMs\nhandle multilingualism? We introduce a framework that depicts LLMs' processing\nof multilingual inputs: In the first several layers, LLMs understand the\nquestion, converting multilingual inputs into English to facilitate the\ntask-solving phase. In the intermediate layers, LLMs engage in problem-solving\nby thinking in English and incorporating multilingual knowledge to obtain\nfactual content, leveraging the self-attention and feed-forward structures,\nrespectively. In the last several layers, LLMs generate responses that align\nwith the original language of the query. In addition, we investigate the\nexistence of language-specific neurons when processing a certain language. To\ndetect neurons activated by the input language, even without labels, we\ninnovatively design a Parallel Language specific Neuron Detection\n($\\texttt{PLND}$) method that effectively measures the significance of neurons\nwhen handling multilingual inputs. By comprehensive ablation analysis through\ndeactivating neurons of different layers and structures, we verify the\nframework that we propose. Additionally, we demonstrate that we can utilize\nsuch a framework to effectively enhance the multilingual ability with much less\ntraining effort.",
        "pdf_link": "https://arxiv.org/pdf/2402.18815v1.pdf"
    },
    {
        "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
        "authors": [
            "Chenglei Shen",
            "Guofu Xie",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "published": "2024-02-29T02:22:23Z",
        "summary": "Large language models (LLMs) are now increasingly utilized for role-playing\ntasks, especially in impersonating domain-specific experts, primarily through\nrole-playing prompts. When interacting in real-world scenarios, the\ndecision-making abilities of a role significantly shape its behavioral\npatterns. In this paper, we concentrate on evaluating the decision-making\nabilities of LLMs post role-playing thereby validating the efficacy of\nrole-playing. Our goal is to provide metrics and guidance for enhancing the\ndecision-making abilities of LLMs in role-playing tasks. Specifically, we first\nuse LLMs to generate virtual role descriptions corresponding to the 16\npersonality types of Myers-Briggs Type Indicator (abbreviated as MBTI)\nrepresenting a segmentation of the population. Then we design specific\nquantitative operations to evaluate the decision-making abilities of LLMs post\nrole-playing from four aspects: adaptability, exploration$\\&$exploitation\ntrade-off ability, reasoning ability, and safety. Finally, we analyze the\nassociation between the performance of decision-making and the corresponding\nMBTI types through GPT-4. Extensive experiments demonstrate stable differences\nin the four aspects of decision-making abilities across distinct roles,\nsignifying a robust correlation between decision-making abilities and the roles\nemulated by LLMs. These results underscore that LLMs can effectively\nimpersonate varied roles while embodying their genuine sociological\ncharacteristics.",
        "pdf_link": "https://arxiv.org/pdf/2402.18807v1.pdf"
    },
    {
        "title": "ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph",
        "authors": [
            "Xukun Liu",
            "Zhiyuan Peng",
            "Xiaoyuan Yi",
            "Xing Xie",
            "Lirong Xiang",
            "Yuchen Liu",
            "Dongkuan Xu"
        ],
        "published": "2024-02-29T02:04:00Z",
        "summary": "While achieving remarkable progress in a broad range of tasks, large language\nmodels (LLMs) remain significantly limited in properly using massive external\ntools. Existing in-context learning approaches simply format tools into a list\nof plain text descriptions and input them to LLMs, from which, LLMs generate a\nsequence of tool calls to solve problems step by step. Such a paradigm ignores\nthe intrinsic dependency between tools and offloads all reasoning loads to\nLLMs, making them restricted to a limited number of specifically designed\ntools. It thus remains challenging for LLMs to operate on a library of massive\ntools, casting a great limitation when confronted with real-world scenarios.\nThis paper proposes ToolNet, a plug-and-play framework that scales up the\nnumber of tools to thousands with a moderate increase in token consumption.\nToolNet organizes tools into a directed graph. Each node represents a tool, and\nweighted edges denote tool transition. Starting from an initial tool node, an\nLLM navigates in the graph by iteratively choosing the next one from its\nsuccessors until the task is resolved. Extensive experiments show that ToolNet\ncan achieve impressive results in challenging multi-hop tool learning datasets\nand is resilient to tool failures.",
        "pdf_link": "https://arxiv.org/pdf/2403.00839v1.pdf"
    },
    {
        "title": "FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning",
        "authors": [
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Xinhao Cheng",
            "Mengdi Wu",
            "Colin Unger",
            "Zhihao Jia"
        ],
        "published": "2024-02-29T01:33:08Z",
        "summary": "Parameter-efficient finetuning (PEFT) is a widely used technique to adapt\nlarge language models for different tasks. Service providers typically create\nseparate systems for users to perform PEFT model finetuning and inference\ntasks. This is because existing systems cannot handle workloads that include a\nmix of inference and PEFT finetuning requests. As a result, shared GPU\nresources are underutilized, leading to inefficiencies. To address this\nproblem, we present FlexLLM, the first system that can serve inference and\nparameter-efficient finetuning requests in the same iteration. Our system\nleverages the complementary nature of these two tasks and utilizes shared GPU\nresources to run them jointly, using a method called co-serving. To achieve\nthis, FlexLLM introduces a novel token-level finetuning mechanism, which breaks\ndown the finetuning computation of a sequence into smaller token-level\ncomputations and uses dependent parallelization and graph pruning, two static\ncompilation optimizations, to minimize the memory overhead and latency for\nco-serving. Compared to existing systems, FlexLLM's co-serving approach reduces\nthe activation GPU memory overhead by up to 8x, and the end-to-end GPU memory\nrequirement of finetuning by up to 36% while maintaining a low inference\nlatency and improving finetuning throughput. For example, under a heavy\ninference workload, FlexLLM can still preserve more than 80% of the peak\nfinetuning throughput, whereas existing systems cannot make any progress with\nfinetuning. The source code of FlexLLM is publicly available at\nhttps://github.com/flexflow/FlexFlow.",
        "pdf_link": "https://arxiv.org/pdf/2402.18789v1.pdf"
    },
    {
        "title": "NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models",
        "authors": [
            "Amit Dhurandhar",
            "Tejaswini Pedapati",
            "Ronny Luss",
            "Soham Dan",
            "Aurelie Lozano",
            "Payel Das",
            "Georgios Kollias"
        ],
        "published": "2024-02-28T22:21:47Z",
        "summary": "Transformer-based Language Models have become ubiquitous in Natural Language\nProcessing (NLP) due to their impressive performance on various tasks. However,\nexpensive training as well as inference remains a significant impediment to\ntheir widespread applicability. While enforcing sparsity at various levels of\nthe model architecture has found promise in addressing scaling and efficiency\nissues, there remains a disconnect between how sparsity affects network\ntopology. Inspired by brain neuronal networks, we explore sparsity approaches\nthrough the lens of network topology. Specifically, we exploit mechanisms seen\nin biological networks, such as preferential attachment and redundant synapse\npruning, and show that principled, model-agnostic sparsity approaches are\nperformant and efficient across diverse NLP tasks, spanning both classification\n(such as natural language inference) and generation (summarization, machine\ntranslation), despite our sole objective not being optimizing performance.\nNeuroPrune is competitive with (or sometimes superior to) baselines on\nperformance and can be up to $10$x faster in terms of training time for a given\nlevel of sparsity, simultaneously exhibiting measurable improvements in\ninference time in many cases.",
        "pdf_link": "https://arxiv.org/pdf/2404.01306v2.pdf"
    },
    {
        "title": "Commonsense Ontology Micropatterns",
        "authors": [
            "Andrew Eells",
            "Brandon Dave",
            "Pascal Hitzler",
            "Cogan Shimizu"
        ],
        "published": "2024-02-28T21:23:54Z",
        "summary": "The previously introduced Modular Ontology Modeling methodology (MOMo)\nattempts to mimic the human analogical process by using modular patterns to\nassemble more complex concepts. To support this, MOMo organizes organizes\nontology design patterns into design libraries, which are programmatically\nqueryable, to support accelerated ontology development, for both human and\nautomated processes. However, a major bottleneck to large-scale deployment of\nMOMo is the (to-date) limited availability of ready-to-use ontology design\npatterns. At the same time, Large Language Models have quickly become a source\nof common knowledge and, in some cases, replacing search engines for questions.\nIn this paper, we thus present a collection of 104 ontology design patterns\nrepresenting often occurring nouns, curated from the common-sense knowledge\navailable in LLMs, organized into a fully-annotated modular ontology design\nlibrary ready for use with MOMo.",
        "pdf_link": "https://arxiv.org/pdf/2402.18715v1.pdf"
    },
    {
        "title": "Learning to Compress Prompt in Natural Language Formats",
        "authors": [
            "Yu-Neng Chuang",
            "Tianwei Xing",
            "Chia-Yuan Chang",
            "Zirui Liu",
            "Xun Chen",
            "Xia Hu"
        ],
        "published": "2024-02-28T20:41:21Z",
        "summary": "Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.18700v2.pdf"
    },
    {
        "title": "CLLMs: Consistency Large Language Models",
        "authors": [
            "Siqi Kou",
            "Lanxiang Hu",
            "Zhezhi He",
            "Zhijie Deng",
            "Hao Zhang"
        ],
        "published": "2024-02-28T20:17:04Z",
        "summary": "Parallel decoding methods such as Jacobi decoding show promise for more\nefficient LLM inference as it breaks the sequential nature of the LLM decoding\nprocess and transforms it into parallelizable computation. However, in\npractice, it achieves little speedup compared to traditional autoregressive\n(AR) decoding, primarily because Jacobi decoding seldom accurately predicts\nmore than one token in a single fixed-point iteration step. To address this, we\ndevelop a new approach aimed at realizing fast convergence from any state to\nthe fixed point on a Jacobi trajectory. This is accomplished by refining the\ntarget LLM to consistently predict the fixed point given any state as input.\nExtensive experiments demonstrate the effectiveness of our method, showing\n2.4$\\times$ to 3.4$\\times$ improvements in generation speed while preserving\ngeneration quality across both domain-specific and open-domain benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.00835v3.pdf"
    },
    {
        "title": "Data Interpreter: An LLM Agent For Data Science",
        "authors": [
            "Sirui Hong",
            "Yizhang Lin",
            "Bang Liu",
            "Bangbang Liu",
            "Binhao Wu",
            "Danyang Li",
            "Jiaqi Chen",
            "Jiayi Zhang",
            "Jinlin Wang",
            "Li Zhang",
            "Lingyao Zhang",
            "Min Yang",
            "Mingchen Zhuge",
            "Taicheng Guo",
            "Tuo Zhou",
            "Wei Tao",
            "Wenyi Wang",
            "Xiangru Tang",
            "Xiangtao Lu",
            "Xiawu Zheng",
            "Xinbing Liang",
            "Yaying Fei",
            "Yuheng Cheng",
            "Zongze Xu",
            "Chenglin Wu"
        ],
        "published": "2024-02-28T19:49:55Z",
        "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable\neffectiveness. However, their performance can be compromised in data science\nscenarios that require real-time data adjustment, expertise in optimization due\nto complex dependencies among various tasks, and the ability to identify\nlogical errors for precise reasoning. In this study, we introduce the Data\nInterpreter, a solution designed to solve with code that emphasizes three\npivotal techniques to augment problem-solving in data science: 1) dynamic\nplanning with hierarchical graph structures for real-time data adaptability;2)\ntool integration dynamically to enhance code proficiency during execution,\nenriching the requisite expertise;3) logical inconsistency identification in\nfeedback, and efficiency enhancement through experience recording. We evaluate\nthe Data Interpreter on various data science and real-world tasks. Compared to\nopen-source baselines, it demonstrated superior performance, exhibiting\nsignificant improvements in machine learning tasks, increasing from 0.86 to\n0.95. Additionally, it showed a 26% increase in the MATH dataset and a\nremarkable 112% improvement in open-ended tasks. The solution will be released\nat https://github.com/geekan/MetaGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.18679v3.pdf"
    },
    {
        "title": "Simple linear attention language models balance the recall-throughput tradeoff",
        "authors": [
            "Simran Arora",
            "Sabri Eyuboglu",
            "Michael Zhang",
            "Aman Timalsina",
            "Silas Alberti",
            "Dylan Zinsley",
            "James Zou",
            "Atri Rudra",
            "Christopher R\u00e9"
        ],
        "published": "2024-02-28T19:28:27Z",
        "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
        "pdf_link": "https://arxiv.org/pdf/2402.18668v1.pdf"
    },
    {
        "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
        "authors": [
            "Congying Xia",
            "Chen Xing",
            "Jiangshu Du",
            "Xinyi Yang",
            "Yihao Feng",
            "Ran Xu",
            "Wenpeng Yin",
            "Caiming Xiong"
        ],
        "published": "2024-02-28T19:23:27Z",
        "summary": "This paper presents FoFo, a pioneering benchmark for evaluating large\nlanguage models' (LLMs) ability to follow complex, domain-specific formats, a\ncrucial yet underexamined capability for their application as AI agents.\nDespite LLMs' advancements, existing benchmarks fail to assess their\nformat-following proficiency adequately. FoFo fills this gap with a diverse\nrange of real-world formats and instructions, developed through an AI-Human\ncollaborative method. Our evaluation across both open-source (e.g., Llama 2,\nWizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three\nkey findings: open-source models significantly lag behind closed-source ones in\nformat adherence; LLMs' format-following performance is independent of their\ncontent generation quality; and LLMs' format proficiency varies across\ndifferent domains. These insights suggest the need for specialized tuning for\nformat-following skills and highlight FoFo's role in guiding the selection of\ndomain-specific AI agents. FoFo is released here at\nhttps://github.com/SalesforceAIResearch/FoFo.",
        "pdf_link": "https://arxiv.org/pdf/2402.18667v1.pdf"
    },
    {
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "authors": [
            "Roberto Gallotta",
            "Graham Todd",
            "Marvin Zammit",
            "Sam Earle",
            "Antonios Liapis",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ],
        "published": "2024-02-28T19:09:08Z",
        "summary": "Recent years have seen an explosive increase in research on large language\nmodels (LLMs), and accompanying public engagement on the topic. While starting\nas a niche area within natural language processing, LLMs have shown remarkable\npotential across a broad range of applications and domains, including games.\nThis paper surveys the current state of the art across the various applications\nof LLMs in and for games, and identifies the different roles LLMs can take\nwithin a game. Importantly, we discuss underexplored areas and promising\ndirections for future uses of LLMs in games and we reconcile the potential and\nlimitations of LLMs within the games domain. As the first comprehensive survey\nand roadmap at the intersection of LLMs and games, we are hopeful that this\npaper will serve as the basis for groundbreaking research and innovation in\nthis exciting new field.",
        "pdf_link": "https://arxiv.org/pdf/2402.18659v1.pdf"
    },
    {
        "title": "A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems",
        "authors": [
            "Fangzhou Wu",
            "Ning Zhang",
            "Somesh Jha",
            "Patrick McDaniel",
            "Chaowei Xiao"
        ],
        "published": "2024-02-28T19:00:12Z",
        "summary": "Large Language Model (LLM) systems are inherently compositional, with\nindividual LLM serving as the core foundation with additional layers of objects\nsuch as plugins, sandbox, and so on. Along with the great potential, there are\nalso increasing concerns over the security of such probabilistic intelligent\nsystems. However, existing studies on LLM security often focus on individual\nLLM, but without examining the ecosystem through the lens of LLM systems with\nother objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we\nsystematically analyze the security of LLM systems, instead of focusing on the\nindividual LLMs. To do so, we build on top of the information flow and\nformulate the security of LLM systems as constraints on the alignment of the\ninformation flow within LLM and between LLM and other objects. Based on this\nconstruction and the unique probabilistic nature of LLM, the attack surface of\nthe LLM system can be decomposed into three key components: (1) multi-layer\nsecurity analysis, (2) analysis of the existence of constraints, and (3)\nanalysis of the robustness of these constraints. To ground this new attack\nsurface, we propose a multi-layer and multi-step approach and apply it to the\nstate-of-art LLM system, OpenAI GPT4. Our investigation exposes several\nsecurity issues, not just within the LLM model itself but also in its\nintegration with other components. We found that although the OpenAI GPT4 has\ndesigned numerous safety constraints to improve its safety features, these\nsafety constraints are still vulnerable to attackers. To further demonstrate\nthe real-world threats of our discovered vulnerabilities, we construct an\nend-to-end attack where an adversary can illicitly acquire the user's chat\nhistory, all without the need to manipulate the user's input or gain direct\naccess to OpenAI GPT4. Our demo is in the link:\nhttps://fzwark.github.io/LLM-System-Attack-Demo/",
        "pdf_link": "https://arxiv.org/pdf/2402.18649v1.pdf"
    },
    {
        "title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards",
        "authors": [
            "Haoxiang Wang",
            "Yong Lin",
            "Wei Xiong",
            "Rui Yang",
            "Shizhe Diao",
            "Shuang Qiu",
            "Han Zhao",
            "Tong Zhang"
        ],
        "published": "2024-02-28T18:58:25Z",
        "summary": "Fine-grained control over large language models (LLMs) remains a significant\nchallenge, hindering their adaptability to diverse user needs. While\nReinforcement Learning from Human Feedback (RLHF) shows promise in aligning\nLLMs, its reliance on scalar rewards often limits its ability to capture\ndiverse user preferences in real-world applications. To address this\nlimitation, we introduce the Directional Preference Alignment (DPA) framework.\nUnlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling\nto represent diverse preference profiles. Additionally, DPA models user\npreferences as directions (i.e., unit vectors) in the reward space to achieve\nuser-dependent preference control. Our method involves training a\nmulti-objective reward model and then fine-tuning the LLM with a\npreference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF\nmethod adopted by Llama 2. This method enjoys a better performance trade-off\nacross various reward objectives. In comparison with the scalar-reward RLHF,\nDPA offers users intuitive control over LLM generation: they can arithmetically\nspecify their desired trade-offs (e.g., more helpfulness with less verbosity).\nWe also validate the effectiveness of DPA with real-world alignment experiments\non Mistral-7B. Our method provides straightforward arithmetic control over the\ntrade-off between helpfulness and verbosity while maintaining competitive\nperformance with strong baselines such as Direct Preference Optimization (DPO).",
        "pdf_link": "https://arxiv.org/pdf/2402.18571v3.pdf"
    },
    {
        "title": "Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates",
        "authors": [
            "Kaifeng Lyu",
            "Haoyu Zhao",
            "Xinran Gu",
            "Dingli Yu",
            "Anirudh Goyal",
            "Sanjeev Arora"
        ],
        "published": "2024-02-28T18:23:49Z",
        "summary": "Public LLMs such as the Llama 2-Chat have driven huge activity in LLM\nresearch. These models underwent alignment training and were considered safe.\nRecently Qi et al. (2023) reported that even benign fine-tuning (e.g., on\nseemingly safe datasets) can give rise to unsafe behaviors in the models. The\ncurrent paper is about methods and best practices to mitigate such loss of\nalignment. Through extensive experiments on several chat models (Meta's Llama\n2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),\nthis paper uncovers that the prompt templates used during fine-tuning and\ninference play a crucial role in preserving safety alignment, and proposes the\n\"Pure Tuning, Safe Testing\" (PTST) principle -- fine-tune models without a\nsafety prompt, but include it at test time. Fine-tuning experiments on GSM8K,\nChatDoctor, and OpenOrca show that PTST significantly reduces the rise of\nunsafe behaviors, and even almost eliminates them in some cases.",
        "pdf_link": "https://arxiv.org/pdf/2402.18540v1.pdf"
    },
    {
        "title": "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification",
        "authors": [
            "Garima Chhikara",
            "Anurag Sharma",
            "Kripabandhu Ghosh",
            "Abhijnan Chakraborty"
        ],
        "published": "2024-02-28T17:29:27Z",
        "summary": "Employing Large Language Models (LLM) in various downstream applications such\nas classification is crucial, especially for smaller companies lacking the\nexpertise and resources required for fine-tuning a model. Fairness in LLMs\nhelps ensure inclusivity, equal representation based on factors such as race,\ngender and promotes responsible AI deployment. As the use of LLMs has become\nincreasingly prevalent, it is essential to assess whether LLMs can generate\nfair outcomes when subjected to considerations of fairness. In this study, we\nintroduce a framework outlining fairness regulations aligned with various\nfairness definitions, with each definition being modulated by varying degrees\nof abstraction. We explore the configuration for in-context learning and the\nprocedure for selecting in-context demonstrations using RAG, while\nincorporating fairness rules into the process. Experiments conducted with\ndifferent LLMs indicate that GPT-4 delivers superior results in terms of both\naccuracy and fairness compared to other models. This work is one of the early\nattempts to achieve fairness in prediction tasks by utilizing LLMs through\nin-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.18502v1.pdf"
    },
    {
        "title": "Language Models Represent Beliefs of Self and Others",
        "authors": [
            "Wentao Zhu",
            "Zhining Zhang",
            "Yizhou Wang"
        ],
        "published": "2024-02-28T17:25:59Z",
        "summary": "Understanding and attributing mental states, known as Theory of Mind (ToM),\nemerges as a fundamental capability for human social reasoning. While Large\nLanguage Models (LLMs) appear to possess certain ToM abilities, the mechanisms\nunderlying these capabilities remain elusive. In this study, we discover that\nit is possible to linearly decode the belief status from the perspectives of\nvarious agents through neural activations of language models, indicating the\nexistence of internal representations of self and others' beliefs. By\nmanipulating these representations, we observe dramatic changes in the models'\nToM performance, underscoring their pivotal role in the social reasoning\nprocess. Additionally, our findings extend to diverse social reasoning tasks\nthat involve different causal inference patterns, suggesting the potential\ngeneralizability of these representations.",
        "pdf_link": "https://arxiv.org/pdf/2402.18496v2.pdf"
    },
    {
        "title": "The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA",
        "authors": [
            "Yiming Li",
            "Zhao Zhang"
        ],
        "published": "2024-02-28T15:05:43Z",
        "summary": "Conversational multi-doc question answering aims to answer specific questions\nbased on the retrieved documents as well as the contextual conversations. In\nthis paper, we introduce our winning approach for the \"Conversational Multi-Doc\nQA\" challenge in WSDM Cup 2024, which exploits the superior natural language\nunderstanding and generation capability of Large Language Models (LLMs). We\nfirst adapt LLMs to the task, then devise a hybrid training strategy to make\nthe most of in-domain unlabeled data. Moreover, an advanced text embedding\nmodel is adopted to filter out potentially irrelevant documents and several\napproaches are designed and compared for the model ensemble. Equipped with all\nthese techniques, our solution finally ranked 1st place in WSDM Cup 2024,\nsurpassing its rivals to a large extent. The source codes have been released at\nhttps://github.com/zhangzhao219/WSDM-Cup-2024.",
        "pdf_link": "https://arxiv.org/pdf/2402.18385v1.pdf"
    },
    {
        "title": "Large Language Models As Evolution Strategies",
        "authors": [
            "Robert Tjarko Lange",
            "Yingtao Tian",
            "Yujin Tang"
        ],
        "published": "2024-02-28T15:02:17Z",
        "summary": "Large Transformer models are capable of implementing a plethora of so-called\nin-context learning algorithms. These include gradient descent, classification,\nsequence completion, transformation, and improvement. In this work, we\ninvestigate whether large language models (LLMs), which never explicitly\nencountered the task of black-box optimization, are in principle capable of\nimplementing evolutionary optimization algorithms. While previous works have\nsolely focused on language-based task specification, we move forward and focus\non the zero-shot application of LLMs to black-box optimization. We introduce a\nnovel prompting strategy, consisting of least-to-most sorting of discretized\npopulation members and querying the LLM to propose an improvement to the mean\nstatistic, i.e. perform a type of black-box recombination operation.\nEmpirically, we find that our setup allows the user to obtain an LLM-based\nevolution strategy, which we call `EvoLLM', that robustly outperforms baseline\nalgorithms such as random search and Gaussian Hill Climbing on synthetic BBOB\nfunctions as well as small neuroevolution tasks. Hence, LLMs can act as\n`plug-in' in-context recombination operators. We provide several comparative\nstudies of the LLM's model size, prompt strategy, and context construction.\nFinally, we show that one can flexibly improve EvoLLM's performance by\nproviding teacher algorithm information via instruction fine-tuning on\npreviously collected teacher optimization trajectories.",
        "pdf_link": "https://arxiv.org/pdf/2402.18381v1.pdf"
    },
    {
        "title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning",
        "authors": [
            "Jiachun Li",
            "Pengfei Cao",
            "Chenhao Wang",
            "Zhuoran Jin",
            "Yubo Chen",
            "Daojian Zeng",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2024-02-28T14:09:02Z",
        "summary": "Large language models exhibit high-level commonsense reasoning abilities,\nespecially with enhancement methods like Chain-of-Thought (CoT). However, we\nfind these CoT-like methods lead to a considerable number of originally correct\nanswers turning wrong, which we define as the Toxic CoT problem. To interpret\nand mitigate this problem, we first utilize attribution tracing and causal\ntracing methods to probe the internal working mechanism of the LLM during CoT\nreasoning. Through comparisons, we prove that the model exhibits information\nloss from the question over the shallow attention layers when generating\nrationales or answers. Based on the probing findings, we design a novel method\ncalled RIDERS (Residual decodIng and sERial-position Swap), which compensates\nfor the information deficit in the model from both decoding and serial-position\nperspectives. Through extensive experiments on multiple commonsense reasoning\nbenchmarks, we validate that this method not only significantly eliminates\nToxic CoT problems (decreased by 23.6%), but also effectively improves the\nmodel's overall commonsense reasoning performance (increased by 5.5%).",
        "pdf_link": "https://arxiv.org/pdf/2402.18344v1.pdf"
    },
    {
        "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
        "authors": [
            "Subhabrata Dutta",
            "Joykirat Singh",
            "Soumen Chakrabarti",
            "Tanmoy Chakraborty"
        ],
        "published": "2024-02-28T13:14:20Z",
        "summary": "Despite superior reasoning prowess demonstrated by Large Language Models\n(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails\naround the internal mechanisms of the models that facilitate CoT generation.\nThis work investigates the neural sub-structures within LLMs that manifest CoT\nreasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B\napplied to multistep reasoning over fictional ontologies, we demonstrate that\nLLMs deploy multiple parallel pathways of answer generation for step-by-step\nreasoning. These parallel pathways provide sequential answers from the input\nquestion context as well as the generated CoT. We observe a striking functional\nrift in the middle layers of the LLM. Token representations in the initial half\nremain strongly biased towards the pretraining prior, with the in-context\ntaking over abruptly in the later half. This internal phase shift manifests in\ndifferent functional components: attention heads that write the answer token\npredominantly appear in the later half, attention heads that move information\nalong ontological relationships appear exclusively in the initial half, and so\non. To the best of our knowledge, this is the first attempt towards mechanistic\ninvestigation of CoT reasoning in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.18312v1.pdf"
    },
    {
        "title": "Retrieval-based Full-length Wikipedia Generation for Emergent Events",
        "authors": [
            "Jiebin Zhang",
            "Eugene J. Yu",
            "Qinyu Chen",
            "Chenhao Xiong",
            "Dawei Zhu",
            "Han Qian",
            "Mingbo Song",
            "Xiaoguang Li",
            "Qun Liu",
            "Sujian Li"
        ],
        "published": "2024-02-28T11:51:56Z",
        "summary": "In today's fast-paced world, the growing demand to quickly generate\ncomprehensive and accurate Wikipedia documents for emerging events is both\ncrucial and challenging. However, previous efforts in Wikipedia generation have\noften fallen short of meeting real-world requirements. Some approaches focus\nsolely on generating segments of a complete Wikipedia document, while others\noverlook the importance of faithfulness in generation or fail to consider the\ninfluence of the pre-training corpus. In this paper, we simulate a real-world\nscenario where structured full-length Wikipedia documents are generated for\nemergent events using input retrieved from web sources. To ensure that Large\nLanguage Models (LLMs) are not trained on corpora related to recently occurred\nevents, we select events that have taken place recently and introduce a new\nbenchmark Wiki-GenBen, which consists of 309 events paired with their\ncorresponding retrieved web pages for generating evidence. Additionally, we\ndesign a comprehensive set of systematic evaluation metrics and baseline\nmethods, to evaluate the capability of LLMs in generating factual full-length\nWikipedia documents. The data and code are open-sourced at WikiGenBench.",
        "pdf_link": "https://arxiv.org/pdf/2402.18264v1.pdf"
    },
    {
        "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
        "authors": [
            "Haoxiang Guan",
            "Jiyan He",
            "Shuxin Zheng",
            "En-Hong Chen",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "published": "2024-02-28T11:29:09Z",
        "summary": "Large language models (LLMs) have demonstrated impressive performance on many\ntasks. However, to achieve optimal performance, specially designed prompting\nmethods are still needed. These methods either rely on task-specific few-shot\nexamples that require a certain level of domain knowledge, or are designed to\nbe simple but only perform well on a few types of tasks. In this work, we\nattempt to introduce the concept of generalist prompting, which operates on the\ndesign principle of achieving optimal or near-optimal performance on a wide\nrange of tasks while eliminating the need for manual selection and\ncustomization of prompts tailored to specific problems. Furthermore, we propose\nMeMo (Mental Models), an innovative prompting method that is simple-designed\nyet effectively fulfills the criteria of generalist prompting. MeMo distills\nthe cores of various prompting methods into individual mental models and allows\nLLMs to autonomously select the most suitable mental models for the problem,\nachieving or being near to the state-of-the-art results on diverse tasks such\nas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We\nhope that the insights presented herein will stimulate further exploration of\ngeneralist prompting methods for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.18252v1.pdf"
    },
    {
        "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
        "authors": [
            "Mengjie Ren",
            "Boxi Cao",
            "Hongyu Lin",
            "Cao Liu",
            "Xianpei Han",
            "Ke Zeng",
            "Guanglu Wan",
            "Xunliang Cai",
            "Le Sun"
        ],
        "published": "2024-02-28T11:16:00Z",
        "summary": "Instruction Fine-tuning~(IFT) is a critical phase in building large language\nmodels~(LLMs). Previous works mainly focus on the IFT's role in the transfer of\nbehavioral norms and the learning of additional world knowledge. However, the\nunderstanding of the underlying mechanisms of IFT remains significantly\nlimited. In this paper, we design a knowledge intervention framework to\ndecouple the potential underlying factors of IFT, thereby enabling individual\nanalysis of different factors. Surprisingly, our experiments reveal that\nattempting to learn additional world knowledge through IFT often struggles to\nyield positive impacts and can even lead to markedly negative effects. Further,\nwe discover that maintaining internal knowledge consistency before and after\nIFT is a critical factor for achieving successful IFT. Our findings reveal the\nunderlying mechanisms of IFT and provide robust support for some very recent\nand potential future works.",
        "pdf_link": "https://arxiv.org/pdf/2402.18243v2.pdf"
    },
    {
        "title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform",
        "authors": [
            "Jizhi Zhang",
            "Keqin Bao",
            "Wenjie Wang",
            "Yang Zhang",
            "Wentao Shi",
            "Wanhong Xu",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-28T11:12:17Z",
        "summary": "The new kind of Agent-oriented information system, exemplified by GPTs, urges\nus to inspect the information system infrastructure to support Agent-level\ninformation processing and to adapt to the characteristics of Large Language\nModel (LLM)-based Agents, such as interactivity. In this work, we envisage the\nprospect of the recommender system on LLM-based Agent platforms and introduce a\nnovel recommendation paradigm called Rec4Agentverse, comprised of Agent Items\nand Agent Recommender. Rec4Agentverse emphasizes the collaboration between\nAgent Items and Agent Recommender, thereby promoting personalized information\nservices and enhancing the exchange of information beyond the traditional\nuser-recommender feedback loop. Additionally, we prospect the evolution of\nRec4Agentverse and conceptualize it into three stages based on the enhancement\nof the interaction and information exchange among Agent Items, Agent\nRecommender, and the user. A preliminary study involving several cases of\nRec4Agentverse validates its significant potential for application. Lastly, we\ndiscuss potential issues and promising directions for future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.18240v2.pdf"
    },
    {
        "title": "CogBench: a large language model walks into a psychology lab",
        "authors": [
            "Julian Coda-Forno",
            "Marcel Binz",
            "Jane X. Wang",
            "Eric Schulz"
        ],
        "published": "2024-02-28T10:43:54Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of\nartificial intelligence. Yet, evaluating them comprehensively remains\nchallenging. We argue that this is partly due to the predominant focus on\nperformance metrics in most benchmarks. This paper introduces CogBench, a\nbenchmark that includes ten behavioral metrics derived from seven cognitive\npsychology experiments. This novel approach offers a toolkit for phenotyping\nLLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse\ndataset. We analyze this data using statistical multilevel modeling techniques,\naccounting for the nested dependencies among fine-tuned versions of specific\nLLMs. Our study highlights the crucial role of model size and reinforcement\nlearning from human feedback (RLHF) in improving performance and aligning with\nhuman behavior. Interestingly, we find that open-source models are less\nrisk-prone than proprietary models and that fine-tuning on code does not\nnecessarily enhance LLMs' behavior. Finally, we explore the effects of\nprompt-engineering techniques. We discover that chain-of-thought prompting\nimproves probabilistic reasoning, while take-a-step-back prompting fosters\nmodel-based behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2402.18225v1.pdf"
    },
    {
        "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
        "authors": [
            "Akash Gupta",
            "Ivaxi Sheth",
            "Vyas Raina",
            "Mark Gales",
            "Mario Fritz"
        ],
        "published": "2024-02-28T10:19:05Z",
        "summary": "With the recent emergence of powerful instruction-tuned large language models\n(LLMs), various helpful conversational Artificial Intelligence (AI) systems\nhave been deployed across many applications. When prompted by users, these AI\nsystems successfully perform a wide range of tasks as part of a conversation.\nTo provide some sort of memory and context, such approaches typically condition\ntheir output on the entire conversational history. Although this sensitivity to\nthe conversational history can often lead to improved performance on subsequent\ntasks, we find that performance can in fact also be negatively impacted, if\nthere is a task-switch. To the best of our knowledge, our work makes the first\nattempt to formalize the study of such vulnerabilities and interference of\ntasks in conversational LLMs caused by task-switches in the conversational\nhistory. Our experiments across 5 datasets with 15 task switches using popular\nLLMs reveal that many of the task-switches can lead to significant performance\ndegradation.",
        "pdf_link": "https://arxiv.org/pdf/2402.18216v1.pdf"
    },
    {
        "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
        "authors": [
            "Wei Zhang",
            "Hongcheng Guo",
            "Anjie Le",
            "Jian Yang",
            "Jiaheng Liu",
            "Zhoujun Li",
            "Tieqiao Zheng",
            "Shi Xu",
            "Runqiang Zang",
            "Liangfan Zheng",
            "Bo Zhang"
        ],
        "published": "2024-02-28T09:51:55Z",
        "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.18205v2.pdf"
    },
    {
        "title": "MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery",
        "authors": [
            "Feihong Lu",
            "Weiqi Wang",
            "Yangyifei Luo",
            "Ziqin Zhu",
            "Qingyun Sun",
            "Baixuan Xu",
            "Haochen Shi",
            "Shiqi Gao",
            "Qian Li",
            "Yangqiu Song",
            "Jianxin Li"
        ],
        "published": "2024-02-28T08:57:42Z",
        "summary": "Social media has become a ubiquitous tool for connecting with others, staying\nupdated with news, expressing opinions, and finding entertainment. However,\nunderstanding the intention behind social media posts remains challenging due\nto the implicitness of intentions in social media posts, the need for\ncross-modality understanding of both text and images, and the presence of noisy\ninformation such as hashtags, misspelled words, and complicated abbreviations.\nTo address these challenges, we present MIKO, a Multimodal Intention Kowledge\nDistillatiOn framework that collaboratively leverages a Large Language Model\n(LLM) and a Multimodal Large Language Model (MLLM) to uncover users'\nintentions. Specifically, we use an MLLM to interpret the image and an LLM to\nextract key information from the text and finally instruct the LLM again to\ngenerate intentions. By applying MIKO to publicly available social media\ndatasets, we construct an intention knowledge base featuring 1,372K intentions\nrooted in 137,287 posts. We conduct a two-stage annotation to verify the\nquality of the generated knowledge and benchmark the performance of widely used\nLLMs for intention generation. We further apply MIKO to a sarcasm detection\ndataset and distill a student model to demonstrate the downstream benefits of\napplying intention knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2402.18169v2.pdf"
    },
    {
        "title": "From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs",
        "authors": [
            "Yulong Liu",
            "Yunlong Yuan",
            "Chunwei Wang",
            "Jianhua Han",
            "Yongqiang Ma",
            "Li Zhang",
            "Nanning Zheng",
            "Hang Xu"
        ],
        "published": "2024-02-28T08:42:23Z",
        "summary": "The distinction between humans and animals lies in the unique ability of\nhumans to use and create tools. Tools empower humans to overcome physiological\nlimitations, fostering the creation of magnificent civilizations. Similarly,\nenabling foundational models like Large Language Models (LLMs) with the\ncapacity to learn external tool usage may serve as a pivotal step toward\nrealizing artificial general intelligence. Previous studies in this field have\npredominantly pursued two distinct approaches to augment the tool invocation\ncapabilities of LLMs. The first approach emphasizes the construction of\nrelevant datasets for model fine-tuning. The second approach, in contrast, aims\nto fully exploit the inherent reasoning abilities of LLMs through in-context\nlearning strategies. In this work, we introduce a novel tool invocation\npipeline designed to control massive real-world APIs. This pipeline mirrors the\nhuman task-solving process, addressing complicated real-life user queries. At\neach step, we guide LLMs to summarize the achieved results and determine the\nnext course of action. We term this pipeline `from Summary to action', Sum2Act\nfor short. Empirical evaluations of our Sum2Act pipeline on the ToolBench\nbenchmark show significant performance improvements, outperforming established\nmethods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in\nenhancing LLMs for complex real-world tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.18157v1.pdf"
    },
    {
        "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
        "authors": [
            "Abdul Basit",
            "Khizar Hussain",
            "Muhammad Abdullah Hanif",
            "Muhammad Shafique"
        ],
        "published": "2024-02-28T08:30:49Z",
        "summary": "Large language models (LLMs) are revolutionizing various domains with their\nremarkable natural language processing (NLP) abilities. However, deploying LLMs\nin resource-constrained edge computing and embedded systems presents\nsignificant challenges. Another challenge lies in delivering medical assistance\nin remote areas with limited healthcare facilities and infrastructure. To\naddress this, we introduce MedAide, an on-premise healthcare chatbot. It\nleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\npreliminary medical diagnostics and support. MedAide employs model\noptimizations for minimal memory footprint and latency on embedded edge devices\nwithout server infrastructure. The training process is optimized using low-rank\nadaptation (LoRA). Additionally, the model is trained on diverse medical\ndatasets, employing reinforcement learning from human feedback (RLHF) to\nenhance its domain-specific capabilities. The system is implemented on various\nconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\%\naccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\nenergy-efficient healthcare assistance platform that alleviates privacy\nconcerns due to edge-based deployment, thereby empowering the community.",
        "pdf_link": "https://arxiv.org/pdf/2403.00830v1.pdf"
    },
    {
        "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Mo Yu",
            "Fandong Meng",
            "Huawei Shen",
            "Xueqi Cheng",
            "Jie Zhou"
        ],
        "published": "2024-02-28T08:24:38Z",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating additional information from retrieval. However, studies have\nshown that LLMs still face challenges in effectively using the retrieved\ninformation, even ignoring it or being misled by it. The key reason is that the\ntraining of LLMs does not clearly make LLMs learn how to utilize input\nretrieved texts with varied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in RAG as ``Information Refiner'',\nwhich means that regardless of correctness, completeness, or usefulness of\nretrieved texts, LLMs can consistently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts that are more concise,\naccurate, and complete than the retrieved texts. To this end, we propose an\ninformation refinement training method named InFO-RAG that optimizes LLMs for\nRAG in an unsupervised manner. InFO-RAG is low-cost and general across various\ntasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse\ntasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,\nand Code Generation show that InFO-RAG improves the performance of LLaMA2 by an\naverage of 9.39\\% relative points. InFO-RAG also shows advantages in in-context\nlearning and robustness of RAG.",
        "pdf_link": "https://arxiv.org/pdf/2402.18150v1.pdf"
    },
    {
        "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
        "authors": [
            "Seungjong Sun",
            "Eungu Lee",
            "Dongyan Nan",
            "Xiangying Zhao",
            "Wonbyung Lee",
            "Bernard J. Jansen",
            "Jang Hyun Kim"
        ],
        "published": "2024-02-28T08:09:14Z",
        "summary": "Large language models exhibit societal biases associated with demographic\ninformation, including race, gender, and others. Endowing such language models\nwith personalities based on demographic data can enable generating opinions\nthat align with those of humans. Building on this idea, we propose \"random\nsilicon sampling,\" a method to emulate the opinions of the human population\nsub-group. Our study analyzed 1) a language model that generates the survey\nresponses that correspond with a human group based solely on its demographic\ndistribution and 2) the applicability of our methodology across various\ndemographic subgroups and thematic questions. Through random silicon sampling\nand using only group-level demographic information, we discovered that language\nmodels can generate response distributions that are remarkably similar to the\nactual U.S. public opinion polls. Moreover, we found that the replicability of\nlanguage models varies depending on the demographic group and topic of the\nquestion, and this can be attributed to inherent societal biases in the models.\nOur findings demonstrate the feasibility of mirroring a group's opinion using\nonly demographic distribution and elucidate the effect of social biases in\nlanguage models on such simulations.",
        "pdf_link": "https://arxiv.org/pdf/2402.18144v1.pdf"
    },
    {
        "title": "Cause and Effect: Can Large Language Models Truly Understand Causality?",
        "authors": [
            "Swagata Ashwani",
            "Kshiteesh Hegde",
            "Nishith Reddy Mannuru",
            "Mayank Jindal",
            "Dushyant Singh Sengar",
            "Krishna Chaitanya Rao Kathala",
            "Dishant Banga",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published": "2024-02-28T08:02:14Z",
        "summary": "With the rise of Large Language Models(LLMs), it has become crucial to\nunderstand their capabilities and limitations in deciphering and explaining the\ncomplex web of causal relationships that language entails. Current methods use\neither explicit or implicit causal reasoning, yet there is a strong need for a\nunified approach combining both to tackle a wide array of causal relationships\nmore effectively. This research proposes a novel architecture called Context\nAware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to\nenhance causal reasoning and explainability. The proposed framework\nincorporates an explicit causal detection module with ConceptNet and\ncounterfactual statements, as well as implicit causal detection through LLMs.\nOur framework goes one step further with a layer of counterfactual explanations\nto accentuate LLMs understanding of causality. The knowledge from ConceptNet\nenhances the performance of multiple causal reasoning tasks such as causal\ndiscovery, causal identification and counterfactual reasoning. The\ncounterfactual sentences add explicit knowledge of the not caused by scenarios.\nBy combining these powerful modules, our model aims to provide a deeper\nunderstanding of causal relationships, enabling enhanced interpretability.\nEvaluation of benchmark datasets shows improved performance across all metrics,\nsuch as accuracy, precision, recall, and F1 scores. We also introduce\nCausalNet, a new dataset accompanied by our code, to facilitate further\nresearch in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.18139v1.pdf"
    },
    {
        "title": "Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
        "authors": [
            "Shaoyang Xu",
            "Weilong Dong",
            "Zishan Guo",
            "Xinwei Wu",
            "Deyi Xiong"
        ],
        "published": "2024-02-28T07:18:39Z",
        "summary": "Prior research in representation engineering has revealed that LLMs encode\nconcepts within their representation spaces, predominantly centered around\nEnglish. In this study, we extend this philosophy to a multilingual scenario,\ndelving into multilingual human value concepts in LLMs. Through our\ncomprehensive exploration covering 7 types of human values, 16 languages and 3\nLLM series with distinct multilinguality, we empirically substantiate the\nexistence of multilingual human values in LLMs. Further cross-lingual analysis\non these concepts discloses 3 traits arising from language resource\ndisparities: cross-lingual inconsistency, distorted linguistic relationships,\nand unidirectional cross-lingual transfer between high- and low-resource\nlanguages, all in terms of human value concepts. Additionally, we validate the\nfeasibility of cross-lingual control over value alignment capabilities of LLMs,\nleveraging the dominant language as a source language. Drawing from our\nfindings on multilingual value alignment, we prudently provide suggestions on\nthe composition of multilingual data for LLMs pre-training: including a limited\nnumber of dominant languages for cross-lingual alignment transfer while\navoiding their excessive prevalence, and keeping a balanced distribution of\nnon-dominant languages. We aspire that our findings would contribute to\nenhancing the safety and utility of multilingual AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.18120v1.pdf"
    },
    {
        "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
        "authors": [
            "Sahithya Ravi",
            "Patrick Huber",
            "Akshat Shrivastava",
            "Aditya Sagar",
            "Ahmed Aly",
            "Vered Shwartz",
            "Arash Einolghozati"
        ],
        "published": "2024-02-28T07:02:38Z",
        "summary": "The emergence of Large Language Models (LLMs) has brought to light promising\nlanguage generation capabilities, particularly in performing tasks like complex\nreasoning and creative writing. Consequently, distillation through imitation of\nteacher responses has emerged as a popular technique to transfer knowledge from\nLLMs to more accessible, Small Language Models (SLMs). While this works well\nfor simpler tasks, there is a substantial performance gap on tasks requiring\nintricate language comprehension and creativity, such as humor generation. We\nhypothesize that this gap may stem from the fact that creative tasks might be\nhard to learn by imitation alone and explore whether an approach, involving\nsupplementary guidance from the teacher, could yield higher performance. To\naddress this, we study the effect of assigning a dual role to the LLM - as a\n\"teacher\" generating data, as well as a \"critic\" evaluating the student's\nperformance. Our experiments on humor generation reveal that the incorporation\nof feedback significantly narrows the performance gap between SLMs and their\nlarger counterparts compared to merely relying on imitation. As a result, our\nresearch highlights the potential of using feedback as an additional dimension\nto data when transferring complex language abilities via distillation.",
        "pdf_link": "https://arxiv.org/pdf/2402.18113v1.pdf"
    },
    {
        "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
        "authors": [
            "Tong Liu",
            "Yingjie Zhang",
            "Zhe Zhao",
            "Yinpeng Dong",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2024-02-28T06:50:14Z",
        "summary": "In recent years, large language models (LLMs) have demonstrated notable\nsuccess across various tasks, but the trustworthiness of LLMs is still an open\nproblem. One specific threat is the potential to generate toxic or harmful\nresponses. Attackers can craft adversarial prompts that induce harmful\nresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMs\nsecurity by identifying bias vulnerabilities within the safety fine-tuning and\ndesign a black-box jailbreak method named DRA (Disguise and Reconstruction\nAttack), which conceals harmful instructions through disguise and prompts the\nmodel to reconstruct the original harmful instruction within its completion. We\nevaluate DRA across various open-source and close-source models, showcasing\nstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRA\nboasts a 90\\% attack success rate on LLM chatbots GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.18104v1.pdf"
    },
    {
        "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
        "authors": [
            "Derong Xu",
            "Ziheng Zhang",
            "Zhihong Zhu",
            "Zhenxi Lin",
            "Qidong Liu",
            "Xian Wu",
            "Tong Xu",
            "Xiangyu Zhao",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "published": "2024-02-28T06:40:57Z",
        "summary": "Model editing aims to precisely modify the behaviours of large language\nmodels (LLMs) on specific knowledge while keeping irrelevant knowledge\nunchanged. It has been proven effective in resolving hallucination and\nout-of-date issues in LLMs. As a result, it can boost the application of LLMs\nin many critical domains (e.g., medical domain), where the hallucination is not\ntolerable. In this paper, we propose two model editing studies and validate\nthem in the medical domain: (1) directly editing the factual medical knowledge\nand (2) editing the explanations to facts. Meanwhile, we observed that current\nmodel editing methods struggle with the specialization and complexity of\nmedical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable\nAdapter strategy for medical model editing. It employs causal tracing to\nidentify the precise location of knowledge in neurons and then introduces\nscalable adapters into the dense layers of LLMs. These adapters are assigned\nscaling values based on the corresponding specific knowledge. To evaluate the\nediting impact, we build two benchmark datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting irrelevant\nknowledge that is not edited.",
        "pdf_link": "https://arxiv.org/pdf/2402.18099v1.pdf"
    },
    {
        "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
        "authors": [
            "June Yong Yang",
            "Byeongwook Kim",
            "Jeongin Bae",
            "Beomseok Kwon",
            "Gunho Park",
            "Eunho Yang",
            "Se Jung Kwon",
            "Dongsoo Lee"
        ],
        "published": "2024-02-28T06:34:54Z",
        "summary": "Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.18096v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
        "authors": [
            "Hanjie Chen",
            "Zhouxiang Fang",
            "Yash Singla",
            "Mark Dredze"
        ],
        "published": "2024-02-28T05:44:41Z",
        "summary": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as passing scores on medical licensing examinations. However, medical\nboard exam questions or general clinical questions do not capture the\ncomplexity of realistic clinical cases. Moreover, the lack of reference\nexplanations means we cannot easily evaluate the reasoning of model decisions,\na crucial component of supporting doctors in making complex medical decisions.\nTo address these challenges, we construct two new datasets: JAMA Clinical\nChallenge and Medbullets. JAMA Clinical Challenge consists of questions based\non challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style\nclinical questions. Both datasets are structured as multiple-choice\nquestion-answering tasks, where each question is accompanied by an\nexpert-written explanation. We evaluate four LLMs on the two datasets using\nvarious prompts. Experiments demonstrate that our datasets are harder than\nprevious benchmarks. The inconsistency between automatic and human evaluations\nof model-generated explanations highlights the need to develop new metrics to\nsupport future research on explainable medical QA.",
        "pdf_link": "https://arxiv.org/pdf/2402.18060v3.pdf"
    },
    {
        "title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models",
        "authors": [
            "Mingjia Huo",
            "Sai Ashish Somayajula",
            "Youwei Liang",
            "Ruisi Zhang",
            "Farinaz Koushanfar",
            "Pengtao Xie"
        ],
        "published": "2024-02-28T05:43:22Z",
        "summary": "Large language models generate high-quality responses with potential\nmisinformation, underscoring the need for regulation by distinguishing\nAI-generated and human-written texts. Watermarking is pivotal in this context,\nwhich involves embedding hidden markers in texts during the LLM inference\nphase, which is imperceptible to humans. Current watermarking algorithms,\nhowever, face the challenge of achieving both the detectability of inserted\nwatermarks and the semantic integrity of generated texts, where enhancing one\naspect often undermines the other. To overcome this, we introduce a novel\nmulti-objective optimization (MOO) approach for watermarking that utilizes\nlightweight networks to generate token-specific watermarking logits and\nsplitting ratios. By leveraging MOO to optimize for both detection and semantic\nobjective functions, our method simultaneously achieves detectability and\nsemantic integrity. Experimental results show that our method outperforms\ncurrent watermarking techniques in enhancing the detectability of texts\ngenerated by LLMs while maintaining their semantic coherence. Our code is\navailable at https://github.com/mignonjia/TS_watermark.",
        "pdf_link": "https://arxiv.org/pdf/2402.18059v2.pdf"
    },
    {
        "title": "MEGAnno+: A Human-LLM Collaborative Annotation System",
        "authors": [
            "Hannah Kim",
            "Kushan Mitra",
            "Rafael Li Chen",
            "Sajjadur Rahman",
            "Dan Zhang"
        ],
        "published": "2024-02-28T04:58:07Z",
        "summary": "Large language models (LLMs) can label data faster and cheaper than humans\nfor various NLP tasks. Despite their prowess, LLMs may fall short in\nunderstanding of complex, sociocultural, or domain-specific context,\npotentially leading to incorrect annotations. Therefore, we advocate a\ncollaborative approach where humans and LLMs work together to produce reliable\nand high-quality labels. We present MEGAnno+, a human-LLM collaborative\nannotation system that offers effective LLM agent and annotation management,\nconvenient and robust LLM annotation, and exploratory verification of LLM\nlabels by humans.",
        "pdf_link": "https://arxiv.org/pdf/2402.18050v1.pdf"
    },
    {
        "title": "Characterizing Truthfulness in Large Language Model Generations with Local Intrinsic Dimension",
        "authors": [
            "Fan Yin",
            "Jayanth Srinivasa",
            "Kai-Wei Chang"
        ],
        "published": "2024-02-28T04:56:21Z",
        "summary": "We study how to characterize and predict the truthfulness of texts generated\nfrom large language models (LLMs), which serves as a crucial step in building\ntrust between humans and LLMs. Although several approaches based on entropy or\nverbalized uncertainty have been proposed to calibrate model predictions, these\nmethods are often intractable, sensitive to hyperparameters, and less reliable\nwhen applied in generative tasks with LLMs. In this paper, we suggest\ninvestigating internal activations and quantifying LLM's truthfulness using the\nlocal intrinsic dimension (LID) of model activations. Through experiments on\nfour question answering (QA) datasets, we demonstrate the effectiveness\nohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally,\nwe study intrinsic dimensions in LLMs and their relations with model layers,\nautoregressive language modeling, and the training of LLMs, revealing that\nintrinsic dimensions can be a powerful approach to understanding LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.18048v1.pdf"
    },
    {
        "title": "Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore",
        "authors": [
            "Sheikh Shafayat",
            "Eunsu Kim",
            "Juhyun Oh",
            "Alice Oh"
        ],
        "published": "2024-02-28T04:43:46Z",
        "summary": "Large Language Models (LLMs) are prone to factuality hallucination,\ngenerating text that contradicts established knowledge. While extensive\nresearch has addressed this in English, little is known about multilingual\nLLMs. This paper systematically evaluates multilingual LLMs' factual accuracy\nacross languages and geographic regions. We introduce a novel pipeline for\nmultilingual factuality evaluation, adapting FActScore(Min et al., 2023) for\ndiverse languages. Our analysis across nine languages reveals that English\nconsistently outperforms others in factual accuracy and quantity of generated\nfacts. Furthermore, multilingual models demonstrate a bias towards factual\ninformation from Western continents. These findings highlight the need for\nimproved multilingual factuality assessment and underscore geographical biases\nin LLMs' fact generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.18045v2.pdf"
    },
    {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "authors": [
            "Yang Liu",
            "Jiahuan Cao",
            "Chongyu Liu",
            "Kai Ding",
            "Lianwen Jin"
        ],
        "published": "2024-02-28T04:35:51Z",
        "summary": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.18041v1.pdf"
    },
    {
        "title": "Automated Discovery of Integral with Deep Learning",
        "authors": [
            "Xiaoxin Yin"
        ],
        "published": "2024-02-28T04:34:15Z",
        "summary": "Recent advancements in the realm of deep learning, particularly in the\ndevelopment of large language models (LLMs), have demonstrated AI's ability to\ntackle complex mathematical problems or solving programming challenges.\nHowever, the capability to solve well-defined problems based on extensive\ntraining data differs significantly from the nuanced process of making\nscientific discoveries. Trained on almost all human knowledge available,\ntoday's sophisticated LLMs basically learn to predict sequences of tokens. They\ngenerate mathematical derivations and write code in a similar way as writing an\nessay, and do not have the ability to pioneer scientific discoveries in the\nmanner a human scientist would do.\n  In this study we delve into the potential of using deep learning to\nrediscover a fundamental mathematical concept: integrals. By defining integrals\nas area under the curve, we illustrate how AI can deduce the integral of a\ngiven function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$\nand $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our\nexperiments show that deep learning models can approach the task of inferring\nintegrals either through a sequence-to-sequence model, akin to language\ntranslation, or by uncovering the rudimentary principles of integration, such\nas $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.",
        "pdf_link": "https://arxiv.org/pdf/2402.18040v1.pdf"
    },
    {
        "title": "ResLoRA: Identity Residual Mapping in Low-Rank Adaption",
        "authors": [
            "Shuhua Shi",
            "Shaohan Huang",
            "Minghui Song",
            "Zhoujun Li",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "published": "2024-02-28T04:33:20Z",
        "summary": "As one of the most popular parameter-efficient fine-tuning (PEFT) methods,\nlow-rank adaptation (LoRA) is commonly applied to fine-tune large language\nmodels (LLMs). However, updating the weights of LoRA blocks effectively and\nexpeditiously is challenging due to the long calculation path in the original\nmodel. To address this, we propose ResLoRA, an improved framework of LoRA. By\nadding residual paths during training and using merging approaches to eliminate\nthese extra paths during inference, our method can achieve better results in\nfewer training steps without any extra trainable parameters or inference cost\ncompared to LoRA. The experiments on NLG, NLU, and text-to-image tasks\ndemonstrate the effectiveness of our method. To the best of our knowledge,\nResLoRA is the first work that combines the residual path with LoRA. The code\nof our method is available at\nhttps://github.com/microsoft/LMOps/tree/main/reslora .",
        "pdf_link": "https://arxiv.org/pdf/2402.18039v1.pdf"
    },
    {
        "title": "Corpus-Steered Query Expansion with Large Language Models",
        "authors": [
            "Yibin Lei",
            "Yu Cao",
            "Tianyi Zhou",
            "Tao Shen",
            "Andrew Yates"
        ],
        "published": "2024-02-28T03:58:58Z",
        "summary": "Recent studies demonstrate that query expansions generated by large language\nmodels (LLMs) can considerably enhance information retrieval systems by\ngenerating hypothetical documents that answer the queries as expansions.\nHowever, challenges arise from misalignments between the expansions and the\nretrieval corpus, resulting in issues like hallucinations and outdated\ninformation due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo\nRelevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to\npromote the incorporation of knowledge embedded within the corpus. CSQE\nutilizes the relevance assessing capability of LLMs to systematically identify\npivotal sentences in the initially-retrieved documents. These corpus-originated\ntexts are subsequently used to expand the query together with LLM-knowledge\nempowered expansions, improving the relevance prediction between the query and\nthe target documents. Extensive experiments reveal that CSQE exhibits strong\nperformance without necessitating any training, especially with queries for\nwhich LLMs lack knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2402.18031v1.pdf"
    },
    {
        "title": "Hire a Linguist!: Learning Endangered Languages with In-Context Linguistic Descriptions",
        "authors": [
            "Kexun Zhang",
            "Yee Man Choi",
            "Zhenqiao Song",
            "Taiqi He",
            "William Yang Wang",
            "Lei Li"
        ],
        "published": "2024-02-28T03:44:01Z",
        "summary": "How can large language models (LLMs) process and translate endangered\nlanguages? Many languages lack a large corpus to train a decent LLM; therefore\nexisting LLMs rarely perform well in unseen, endangered languages. On the\ncontrary, we observe that 2000 endangered languages, though without a large\ncorpus, have a grammar book or a dictionary. We propose LINGOLLM, a\ntraining-free approach to enable an LLM to process unseen languages that hardly\noccur in its pre-training. Our key insight is to demonstrate linguistic\nknowledge of an unseen language in an LLM's prompt, including a dictionary, a\ngrammar book, and morphologically analyzed input text. We implement LINGOLLM on\ntop of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks\nacross 8 endangered or low-resource languages. Our results show that LINGOLLM\nelevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language\ndirections. Our findings demonstrate the tremendous value of linguistic\nknowledge in the age of LLMs for endangered languages. Our data, code, and\nmodel generations can be found at https://github.com/LLiLab/llm4endangeredlang.",
        "pdf_link": "https://arxiv.org/pdf/2402.18025v1.pdf"
    },
    {
        "title": "TroubleLLM: Align to Red Team Expert",
        "authors": [
            "Zhuoer Xu",
            "Jianping Zhang",
            "Shiwen Cui",
            "Changhua Meng",
            "Weiqiang Wang"
        ],
        "published": "2024-02-28T03:40:46Z",
        "summary": "Large Language Models (LLMs) become the start-of-the-art solutions for a\nvariety of natural language tasks and are integrated into real-world\napplications. However, LLMs can be potentially harmful in manifesting\nundesirable safety issues like social biases and toxic content. It is\nimperative to assess its safety issues before deployment. However, the quality\nand diversity of test prompts generated by existing methods are still far from\nsatisfactory. Not only are these methods labor-intensive and require large\nbudget costs, but the controllability of test prompt generation is lacking for\nthe specific testing domain of LLM applications. With the idea of LLM for LLM\ntesting, we propose the first LLM, called TroubleLLM, to generate controllable\ntest prompts on LLM safety issues. Extensive experiments and human evaluation\nillustrate the superiority of TroubleLLM on generation quality and generation\ncontrollability.",
        "pdf_link": "https://arxiv.org/pdf/2403.00829v1.pdf"
    },
    {
        "title": "Merino: Entropy-driven Design for Generative Language Models on IoT Devices",
        "authors": [
            "Youpeng Zhao",
            "Ming Lin",
            "Huadong Tang",
            "Qiang Wu",
            "Jun Wang"
        ],
        "published": "2024-02-28T03:20:27Z",
        "summary": "Generative Large Language Models (LLMs) stand as a revolutionary advancement\nin the modern era of artificial intelligence (AI). However, directly deploying\nLLMs in resource-constrained hardware, such as Internet-of-Things (IoT)\ndevices, is difficult due to their high computational cost. In this paper, we\npropose a novel information-entropy framework for designing mobile-friendly\ngenerative language models. Our key design paradigm is to maximize the entropy\nof transformer decoders within the given computational budgets. The whole\ndesign procedure involves solving a mathematical programming (MP) problem,\nwhich can be done on the CPU within minutes, making it nearly zero-cost. We\nevaluate our designed models, termed MeRino, across nine NLP downstream tasks,\nshowing their competitive performance against the state-of-the-art\nautoregressive transformer models under the mobile setting. Notably, MeRino\nachieves similar or better zero performance compared to the 350M parameter OPT\nwhile being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model\nsize. Code will be made available soon.",
        "pdf_link": "https://arxiv.org/pdf/2403.07921v1.pdf"
    },
    {
        "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
        "authors": [
            "Zihao Yi",
            "Jiarui Ouyang",
            "Yuwen Liu",
            "Tianhao Liao",
            "Zhe Xu",
            "Ying Shen"
        ],
        "published": "2024-02-28T03:16:44Z",
        "summary": "This survey provides a comprehensive review of research on multi-turn\ndialogue systems, with a particular focus on multi-turn dialogue systems based\non large language models (LLMs). This paper aims to (a) give a summary of\nexisting LLMs and approaches for adapting LLMs to downstream tasks; (b)\nelaborate recent advances in multi-turn dialogue systems, covering both\nLLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,\nalong with datasets and evaluation metrics; (c) discuss some future emphasis\nand recent research problems arising from the development of LLMs and the\nincreasing demands on multi-turn dialogue systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.18013v1.pdf"
    },
    {
        "title": "Exploring Multi-Document Information Consolidation for Scientific Sentiment Summarization",
        "authors": [
            "Miao Li",
            "Jey Han Lau",
            "Eduard Hovy"
        ],
        "published": "2024-02-28T02:40:09Z",
        "summary": "Modern natural language generation systems with LLMs exhibit the capability\nto generate a plausible summary of multiple documents; however, it is uncertain\nif models truly possess the ability of information consolidation to generate\nsummaries, especially on those source documents with opinionated information.\nTo make scientific sentiment summarization more grounded, we hypothesize that\nin peer review human meta-reviewers follow a three-layer framework of sentiment\nconsolidation to write meta-reviews and it represents the logic of summarizing\nscientific sentiments in meta-review generation. The framework is validated via\nhuman annotation. Based on the framework, we propose evaluation metrics to\nassess the quality of generated meta-reviews, and we find that the hypothesis\nof the sentiment consolidation framework works out empirically when we\nincorporate it as prompts for LLMs to generate meta-reviews in extensive\nexperiments.",
        "pdf_link": "https://arxiv.org/pdf/2402.18005v1.pdf"
    },
    {
        "title": "Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars",
        "authors": [
            "Daniel Melcer",
            "Nathan Fulton",
            "Sanjay Krishna Gouda",
            "Haifeng Qian"
        ],
        "published": "2024-02-28T02:12:47Z",
        "summary": "Large Language Models are powerful tools for program synthesis and advanced\nauto-completion, but come with no guarantee that their output code is\nsyntactically correct. This paper contributes an incremental parser that allows\nearly rejection of syntactically incorrect code, as well as efficient detection\nof complete programs for fill-in-the-middle (FItM) tasks. We develop\nEarley-style parsers that operate over left and right quotients of arbitrary\ncontext-free grammars, and we extend our incremental parsing and quotient\noperations to several context-sensitive features present in the grammars of\nmany common programming languages. The result of these contributions is an\nefficient, general, and well-grounded method for left and right quotient\nparsing.\n  To validate our theoretical contributions -- and the practical effectiveness\nof certain design decisions -- we evaluate our method on the particularly\ndifficult case of FItM completion for Python 3. Our results demonstrate that\nconstrained generation can significantly reduce the incidence of syntax errors\nin recommended code.",
        "pdf_link": "https://arxiv.org/pdf/2402.17988v1.pdf"
    },
    {
        "title": "FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",
        "authors": [
            "Yi Zhang",
            "Fei Yang",
            "Shuang Peng",
            "Fangyu Wang",
            "Aimin Pan"
        ],
        "published": "2024-02-28T02:00:34Z",
        "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross various tasks. However, the latency of inference and the large GPU\nmemory consumption of LLMs restrict their deployment performance. Recently,\nthere have been some efficient attempts to quantize LLMs, yet inference with\nlarge batch size or long sequence still has the issue of being compute-bound.\nFine-grained quantization methods have showcased their proficiency in achieving\nlow-bit quantization for LLMs, while requiring FP16 data type for linear layer\ncomputations, which is time-consuming when dealing with large batch size or\nlong sequence. In this paper, we introduce a method called FlattenQuant, which\nsignificantly reduces the maximum value of the tensor by flattening the large\nchannels in the tensor, to achieve low bit per-tensor quantization with minimal\naccuracy loss. Our experiments show that FlattenQuant can directly use 4 bits\nto achieve 48.29% of the linear layer calculation in LLMs, with the remaining\nlayers using 8 bits. The 4-bit matrix multiplication introduced in the\nFlattenQuant method can effectively address the compute-bound caused by large\nmatrix calculation. Our work achieves up to 2$\\times$ speedup and 2.3$\\times$\nmemory reduction for LLMs with negligible loss in accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.17985v1.pdf"
    },
    {
        "title": "Collaborative decoding of critical tokens for boosting factuality of large language models",
        "authors": [
            "Lifeng Jin",
            "Baolin Peng",
            "Linfeng Song",
            "Haitao Mi",
            "Ye Tian",
            "Dong Yu"
        ],
        "published": "2024-02-28T01:53:37Z",
        "summary": "The most common training pipeline for large language models includes\npretraining, finetuning and aligning phases, with their respective resulting\nmodels, such as the pretrained model and the finetuned model. Finetuned and\naligned models show improved abilities of instruction following and safe\ngeneration, however their abilities to stay factual about the world are\nimpacted by the finetuning process. Furthermore, the common practice of using\nsampling during generation also increases chances of hallucination. In this\nwork, we introduce a collaborative decoding framework to harness the high\nfactuality within pretrained models through the concept of critical tokens. We\nfirst design a critical token classifier to decide which model to use for the\nnext token, and subsequently generates the next token using different decoding\nstrategies. Experiments with different models and datasets show that our\ndecoding framework is able to reduce model hallucination significantly,\nshowcasing the importance of the collaborative decoding framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.17982v1.pdf"
    },
    {
        "title": "Gradient-Free Adaptive Global Pruning for Pre-trained Language Models",
        "authors": [
            "Guangji Bai",
            "Yijiang Li",
            "Chen Ling",
            "Kibaek Kim",
            "Liang Zhao"
        ],
        "published": "2024-02-28T00:09:07Z",
        "summary": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework\nthat redefines the global pruning process into manageable, coordinated\nsubproblems, allowing for resource-efficient optimization with global\noptimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular\nfunctions and leverages auxiliary variables for problem decomposition, not only\nfacilitates a pragmatic application on LLMs but also demonstrates significant\nperformance improvements, particularly in high-sparsity regimes where it\nsurpasses current state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.17946v2.pdf"
    },
    {
        "title": "EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models",
        "authors": [
            "Ruisi Zhang",
            "Farinaz Koushanfar"
        ],
        "published": "2024-02-27T23:30:17Z",
        "summary": "This paper introduces EmMark,a novel watermarking framework for protecting\nthe intellectual property (IP) of embedded large language models deployed on\nresource-constrained edge devices. To address the IP theft risks posed by\nmalicious end-users, EmMark enables proprietors to authenticate ownership by\nquerying the watermarked model weights and matching the inserted signatures.\nEmMark's novelty lies in its strategic watermark weight parameters selection,\nnsuring robustness and maintaining model quality. Extensive proof-of-concept\nevaluations of models from OPT and LLaMA-2 families demonstrate EmMark's\nfidelity, achieving 100% success in watermark extraction with model performance\npreservation. EmMark also showcased its resilience against watermark removal\nand forging attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17938v1.pdf"
    },
    {
        "title": "Acquiring Linguistic Knowledge from Multimodal Input",
        "authors": [
            "Theodor Amariucai",
            "Alex Warstadt"
        ],
        "published": "2024-02-27T23:29:10Z",
        "summary": "In contrast to children, language models (LMs) exhibit considerably inferior\ndata efficiency when acquiring language. In this submission to the BabyLM\nChallenge (Warstadt et al., 2023), we test the hypothesis that this data\nefficiency gap is partly caused by a lack of multimodal input and grounding in\nthe learning environment of typical language models. Although previous work\nlooking into this question found that multimodal training can even harm\nlanguage-only performance, we speculate that these findings can be attributed\nto catastrophic forgetting of complex language due to fine-tuning on captions\ndata. To test our hypothesis, we perform an ablation study on FLAVA (Singh et\nal., 2022), a multimodal vision-and-language model, independently varying the\nvolume of text and vision input to quantify how much text data (if any) can be\noffset by vision at different data scales. We aim to limit catastrophic\nforgetting through a multitask pretraining regime that includes unimodal\ntext-only tasks and data sampled from WiT, the relatively diverse\nWikipedia-based dataset (Srinivasan et al., 2021). Our results are largely\nnegative: Multimodal pretraining does not harm our models' language performance\nbut does not consistently help either. That said, our conclusions are limited\nby our having been able to conduct only a small number of runs. While we must\nleave open the possibility that multimodal input explains some of the gap in\ndata efficiency between LMs and humans, positive evidence for this hypothesis\nwill require better architectures and techniques for multimodal training.",
        "pdf_link": "https://arxiv.org/pdf/2402.17936v1.pdf"
    },
    {
        "title": "Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures",
        "authors": [
            "Chu-Cheng Lin",
            "Xinyi Wang",
            "Jonathan H. Clark",
            "Han Lu",
            "Yun Zhu",
            "Chenxi Whitehouse",
            "Hongkun Yu"
        ],
        "published": "2024-02-27T23:12:45Z",
        "summary": "Adapting pretrained large language models (LLMs) to various downstream tasks\nin tens or hundreds of human languages is computationally expensive.\nParameter-efficient fine-tuning (PEFT) significantly reduces the adaptation\ncost, by tuning only a small amount of parameters. However, directly applying\nPEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could\nlead to suboptimal performance due to limited parameter capacity and negative\ninterference among different datasets. In this work, we propose Featurized\nLow-rank Mixtures (FLix), a novel PEFT method designed for effective multitask\nmultilingual tuning. FLix associates each unique dataset feature, such as the\ndataset's language or task, with its own low-rank weight update parameters. By\ncomposing feature-specific parameters for each dataset, FLix can accommodate\ndiverse dataset mixtures and generalize better to unseen datasets. Our\nexperiments show that FLix leads to significant improvements over a variety of\ntasks for both supervised learning and zero-shot settings using different\ntraining data mixtures.",
        "pdf_link": "https://arxiv.org/pdf/2402.17934v1.pdf"
    },
    {
        "title": "Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning",
        "authors": [
            "Tan Zhi-Xuan",
            "Lance Ying",
            "Vikash Mansinghka",
            "Joshua B. Tenenbaum"
        ],
        "published": "2024-02-27T23:06:53Z",
        "summary": "People often give instructions whose meaning is ambiguous without further\ncontext, expecting that their actions or goals will disambiguate their\nintentions. How can we build assistive agents that follow such instructions in\na flexible, context-sensitive manner? This paper introduces cooperative\nlanguage-guided inverse plan search (CLIPS), a Bayesian agent architecture for\npragmatic instruction following and goal assistance. Our agent assists a human\nby modeling them as a cooperative planner who communicates joint plans to the\nassistant, then performs multimodal Bayesian inference over the human's goal\nfrom actions and language, using large language models (LLMs) to evaluate the\nlikelihood of an instruction given a hypothesized plan. Given this posterior,\nour assistant acts to minimize expected goal achievement cost, enabling it to\npragmatically follow ambiguous instructions and provide effective assistance\neven when uncertain about the goal. We evaluate these capabilities in two\ncooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that\nCLIPS significantly outperforms GPT-4V, LLM-based literal instruction following\nand unimodal inverse planning in both accuracy and helpfulness, while closely\nmatching the inferences and assistive judgments provided by human raters.",
        "pdf_link": "https://arxiv.org/pdf/2402.17930v1.pdf"
    },
    {
        "title": "LLM-Resistant Math Word Problem Generation via Adversarial Attacks",
        "authors": [
            "Roy Xie",
            "Chengxuan Huang",
            "Junlin Wang",
            "Bhuwan Dhingra"
        ],
        "published": "2024-02-27T22:07:52Z",
        "summary": "Large language models (LLMs) have significantly transformed the educational\nlandscape. As current plagiarism detection tools struggle to keep pace with\nLLMs' rapid advancements, the educational community faces the challenge of\nassessing students' true problem-solving abilities in the presence of LLMs. In\nthis work, we explore a new paradigm for ensuring fair evaluation -- generating\nadversarial examples which preserve the structure and difficulty of the\noriginal questions aimed for assessment, but are unsolvable by LLMs. Focusing\non the domain of math word problems, we leverage abstract syntax trees to\nstructurally generate adversarial examples that cause LLMs to produce incorrect\nanswers by simply editing the numeric values in the problems. We conduct\nexperiments on various open- and closed-source LLMs, quantitatively and\nqualitatively demonstrating that our method significantly degrades their math\nproblem-solving ability. We identify shared vulnerabilities among LLMs and\npropose a cost-effective approach to attack high-cost models. Additionally, we\nconduct automatic analysis on math problems and investigate the cause of\nfailure, offering a nuanced view into model's limitation.",
        "pdf_link": "https://arxiv.org/pdf/2402.17916v2.pdf"
    },
    {
        "title": "A Language Model based Framework for New Concept Placement in Ontologies",
        "authors": [
            "Hang Dong",
            "Jiaoyan Chen",
            "Yuan He",
            "Yongsheng Gao",
            "Ian Horrocks"
        ],
        "published": "2024-02-27T21:27:35Z",
        "summary": "We investigate the task of inserting new concepts extracted from texts into\nan ontology using language models. We explore an approach with three steps:\nedge search which is to find a set of candidate locations to insert (i.e.,\nsubsumptions between concepts), edge formation and enrichment which leverages\nthe ontological structure to produce and enhance the edge candidates, and edge\nselection which eventually locates the edge to be placed into. In all steps, we\npropose to leverage neural methods, where we apply embedding-based methods and\ncontrastive learning with Pre-trained Language Models (PLMs) such as BERT for\nedge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder,\nand Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for\nedge selection. We evaluate the methods on recent datasets created using the\nSNOMED CT ontology and the MedMentions entity linking benchmark. The best\nsettings in our framework use fine-tuned PLM for search and a multi-label\nCross-encoder for selection. Zero-shot prompting of LLMs is still not adequate\nfor the task, and we propose explainable instruction tuning of LLMs for\nimproved performance. Our study shows the advantages of PLMs and highlights the\nencouraging performance of LLMs that motivates future studies.",
        "pdf_link": "https://arxiv.org/pdf/2402.17897v2.pdf"
    },
    {
        "title": "Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents",
        "authors": [
            "Corby Rosset",
            "Ho-Lam Chung",
            "Guanghui Qin",
            "Ethan C. Chau",
            "Zhuo Feng",
            "Ahmed Awadallah",
            "Jennifer Neville",
            "Nikhil Rao"
        ],
        "published": "2024-02-27T21:27:16Z",
        "summary": "Existing question answering (QA) datasets are no longer challenging to most\npowerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA,\nNaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear\nindications of both what information is missing, and how to find it to answer\nthe question. Hence, good performance on these benchmarks provides a false\nsense of security. A yet unmet need of the NLP community is a bank of\nnon-factoid, multi-perspective questions involving a great deal of unclear\ninformation needs, i.e. ``unknown uknowns''. We claim we can find such\nquestions in search engine logs, which is surprising because most\nquestion-intent queries are indeed factoid. We present Researchy Questions, a\ndataset of search engine queries tediously filtered to be non-factoid,\n``decompositional'' and multi-perspective. We show that users spend a lot of\n``effort'' on these questions in terms of signals like clicks and session\nlength, and that they are also challenging for GPT-4. We also show that ``slow\nthinking'' answering techniques, like decomposition into sub-questions shows\nbenefit over answering directly. We release $\\sim$ 100k Researchy Questions,\nalong with the Clueweb22 URLs that were clicked.",
        "pdf_link": "https://arxiv.org/pdf/2402.17896v1.pdf"
    },
    {
        "title": "JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability",
        "authors": [
            "Junda Wang",
            "Zhichao Yang",
            "Zonghai Yao",
            "Hong Yu"
        ],
        "published": "2024-02-27T21:01:41Z",
        "summary": "With the explosive growth of medical data and the rapid development of\nartificial intelligence technology, precision medicine has emerged as a key to\nenhancing the quality and efficiency of healthcare services. In this context,\nLarge Language Models (LLMs) play an increasingly vital role in medical\nknowledge acquisition and question-answering systems. To further improve the\nperformance of these systems in the medical domain, we introduce an innovative\nmethod that jointly trains an Information Retrieval (IR) system and an LLM\nduring the fine-tuning phase. This approach, which we call Joint Medical LLM\nand Retrieval Training (JMLR), is designed to overcome the challenges faced by\ntraditional models in handling medical question-answering tasks. By employing a\nsynchronized training mechanism, JMLR reduces the demand for computational\nresources and enhances the model's ability to leverage medical knowledge for\nreasoning and answering questions. Our experimental results demonstrate that\nJMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models using\nconventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3%\non MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% on\nMedQA) significantly outperforms other public models (Meditron-7B: 50.1%,\n47.9%), proving its superiority in terms of cost (our training time: 37 hours,\ntraditional method: 144 hours), efficiency, and effectiveness in medical\nquestion-answering tasks. Through this work, we provide a new and efficient\nknowledge enhancement tool for healthcare, demonstrating the great potential of\nintegrating IR and LLM training in precision medical information retrieval and\nquestion-answering systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.17887v2.pdf"
    },
    {
        "title": "BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra",
        "authors": [
            "Parker Glenn",
            "Parag Pravin Dakle",
            "Liang Wang",
            "Preethi Raghavan"
        ],
        "published": "2024-02-27T20:48:24Z",
        "summary": "Many existing end-to-end systems for hybrid question answering tasks can\noften be boiled down to a \"prompt-and-pray\" paradigm, where the user has\nlimited control and insight into the intermediate reasoning steps used to\nachieve the final result. Additionally, due to the context size limitation of\nmany transformer-based LLMs, it is often not reasonable to expect that the full\nstructured and unstructured context will fit into a given prompt in a zero-shot\nsetting, let alone a few-shot setting. We introduce BlendSQL, a superset of\nSQLite to act as a unified dialect for orchestrating reasoning across both\nunstructured and structured data. For hybrid question answering tasks involving\nmulti-hop reasoning, we encode the full decomposed reasoning roadmap into a\nsingle interpretable BlendSQL query. Notably, we show that BlendSQL can scale\nto massive datasets and improve the performance of end-to-end systems while\nusing 35% fewer tokens. Our code is available and installable as a package at\nhttps://github.com/parkervg/blendsql.",
        "pdf_link": "https://arxiv.org/pdf/2402.17882v1.pdf"
    },
    {
        "title": "Automated Statistical Model Discovery with Language Models",
        "authors": [
            "Michael Y. Li",
            "Emily B. Fox",
            "Noah D. Goodman"
        ],
        "published": "2024-02-27T20:33:22Z",
        "summary": "Statistical model discovery involves a challenging search over a vast space\nof models subject to domain-specific modeling constraints. Efficiently\nsearching over this space requires human expertise in modeling and the problem\ndomain. Motivated by the domain knowledge and programming capabilities of large\nlanguage models (LMs), we introduce a method for language model driven\nautomated statistical model discovery. We cast our automated procedure within\nthe framework of Box's Loop: the LM iterates between proposing statistical\nmodels represented as probabilistic programs, acting as a modeler, and\ncritiquing those models, acting as a domain expert. By leveraging LMs, we do\nnot have to define a domain-specific language of models or design a handcrafted\nsearch procedure, key restrictions of previous systems. We evaluate our method\nin three common settings in probabilistic modeling: searching within a\nrestricted space of models, searching over an open-ended space, and improving\nclassic models under natural language constraints (e.g., this model should be\ninterpretable to an ecologist). Our method matches the performance of previous\nsystems, identifies models on par with human expert designed models, and\nextends classic models in interpretable ways. Our results highlight the promise\nof LM driven model discovery.",
        "pdf_link": "https://arxiv.org/pdf/2402.17879v1.pdf"
    },
    {
        "title": "Deep Learning Detection Method for Large Language Models-Generated Scientific Content",
        "authors": [
            "Bushra Alhijawi",
            "Rawan Jarrar",
            "Aseel AbuAlRub",
            "Arwa Bader"
        ],
        "published": "2024-02-27T19:16:39Z",
        "summary": "Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual\ncontent is written and communicated. These models have the potential to\ngenerate scientific content that is indistinguishable from that written by\nhumans. Hence, LLMs carry severe consequences for the scientific community,\nwhich relies on the integrity and reliability of publications. This research\npaper presents a novel ChatGPT-generated scientific text detection method,\nAI-Catcher. AI-Catcher integrates two deep learning models, multilayer\nperceptron (MLP) and convolutional neural networks (CNN). The MLP learns the\nfeature representations of the linguistic and statistical features. The CNN\nextracts high-level representations of the sequential patterns from the textual\ncontent. AI-Catcher is a multimodal model that fuses hidden patterns derived\nfrom MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset\nis collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt\ncontains 3000 records collected from published academic articles across ten\ndomains and divided into three classes: Human-written, ChatGPT-generated, and\nMixed text. Several experiments are conducted to evaluate the performance of\nAI-Catcher. The comparative results demonstrate the capability of AI-Catcher to\ndistinguish between human-written and ChatGPT-generated scientific text more\naccurately than alternative methods. On average, AI-Catcher improved accuracy\nby 37.4%.",
        "pdf_link": "https://arxiv.org/pdf/2403.00828v1.pdf"
    },
    {
        "title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
        "authors": [
            "Keshav Ramji",
            "Young-Suk Lee",
            "Ram\u00f3n Fernandez Astudillo",
            "Md Arafat Sultan",
            "Tahira Naseem",
            "Asim Munawar",
            "Radu Florian",
            "Salim Roukos"
        ],
        "published": "2024-02-27T19:13:01Z",
        "summary": "It is often desirable for Large Language Models (LLMs) to capture multiple\nobjectives when providing a response. In document-grounded response generation,\nfor example, agent responses are expected to be relevant to a user's query\nwhile also being grounded in a given document. In this paper, we introduce\nProxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine\nits own initial response along key dimensions of quality guided by external\nmetrics feedback, yielding an overall better final response. ProMiSe leverages\nfeedback on response quality through principle-specific proxy metrics, and\niteratively refines its response one principle at a time. We apply ProMiSe to\nopen source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its\nperformance on document-grounded question answering datasets, MultiDoc2Dial and\nQuAC, demonstrating that self-refinement improves response quality. We further\nshow that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated\nby ProMiSe yields significant performance improvements over the zero-shot\nbaseline as well as a supervised fine-tuned model on human annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2403.00827v1.pdf"
    },
    {
        "title": "Prediction-Powered Ranking of Large Language Models",
        "authors": [
            "Ivi Chatzi",
            "Eleni Straitouri",
            "Suhas Thejaswi",
            "Manuel Gomez Rodriguez"
        ],
        "published": "2024-02-27T19:00:01Z",
        "summary": "Large language models are often ranked according to their level of alignment\nwith human preferences -- a model is better than other models if its outputs\nare more frequently preferred by humans. One of the most popular ways to elicit\nhuman preferences utilizes pairwise comparisons between the outputs provided by\ndifferent models to the same inputs. However, since gathering pairwise\ncomparisons by humans is costly and time-consuming, it has become a very common\npractice to gather pairwise comparisons by a strong large language model -- a\nmodel strongly aligned with human preferences. Surprisingly, practitioners\ncannot currently measure the uncertainty that any mismatch between human and\nmodel preferences may introduce in the constructed rankings. In this work, we\ndevelop a statistical framework to bridge this gap. Given a small set of\npairwise comparisons by humans and a large set of pairwise comparisons by a\nmodel, our framework provides a rank-set -- a set of possible ranking positions\n-- for each of the models under comparison. Moreover, it guarantees that, with\na probability greater than or equal to a user-specified value, the rank-sets\ncover the true ranking consistent with (the distribution of) human pairwise\npreferences. Our framework is computationally efficient, easy to use, and does\nnot make any assumption about the distribution of human preferences nor about\nthe degree of alignment between the pairwise comparisons by the humans and the\nstrong large language model.",
        "pdf_link": "https://arxiv.org/pdf/2402.17826v1.pdf"
    },
    {
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
        "authors": [
            "Shuming Ma",
            "Hongyu Wang",
            "Lingxiao Ma",
            "Lei Wang",
            "Wenhui Wang",
            "Shaohan Huang",
            "Li Dong",
            "Ruiping Wang",
            "Jilong Xue",
            "Furu Wei"
        ],
        "published": "2024-02-27T18:56:19Z",
        "summary": "Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.17764v1.pdf"
    },
    {
        "title": "Massive Activations in Large Language Models",
        "authors": [
            "Mingjie Sun",
            "Xinlei Chen",
            "J. Zico Kolter",
            "Zhuang Liu"
        ],
        "published": "2024-02-27T18:55:17Z",
        "summary": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2402.17762v1.pdf"
    },
    {
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": [
            "Adyasha Maharana",
            "Dong-Ho Lee",
            "Sergey Tulyakov",
            "Mohit Bansal",
            "Francesco Barbieri",
            "Yuwei Fang"
        ],
        "published": "2024-02-27T18:42:31Z",
        "summary": "Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.17753v1.pdf"
    },
    {
        "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks",
        "authors": [
            "Duarte M. Alves",
            "Jos\u00e9 Pombal",
            "Nuno M. Guerreiro",
            "Pedro H. Martins",
            "Jo\u00e3o Alves",
            "Amin Farajian",
            "Ben Peters",
            "Ricardo Rei",
            "Patrick Fernandes",
            "Sweta Agrawal",
            "Pierre Colombo",
            "Jos\u00e9 G. C. de Souza",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2024-02-27T18:09:36Z",
        "summary": "While general-purpose large language models (LLMs) demonstrate proficiency on\nmultiple tasks within the domain of translation, approaches based on open LLMs\nare competitive only when specializing on a single task. In this paper, we\npropose a recipe for tailoring LLMs to multiple tasks present in translation\nworkflows. We perform continued pretraining on a multilingual mixture of\nmonolingual and parallel data, creating TowerBase, followed by finetuning on\ninstructions relevant for translation processes, creating TowerInstruct. Our\nfinal model surpasses open alternatives on several tasks relevant to\ntranslation workflows and is competitive with general-purpose closed LLMs. To\nfacilitate future research, we release the Tower models, our specialization\ndataset, an evaluation framework for LLMs focusing on the translation\necosystem, and a collection of model generations, including ours, on our\nbenchmark.",
        "pdf_link": "https://arxiv.org/pdf/2402.17733v1.pdf"
    },
    {
        "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
        "authors": [
            "Ayana Niwa",
            "Hayate Iso"
        ],
        "published": "2024-02-27T17:52:33Z",
        "summary": "In this study, we introduce AmbigNLG, a new task designed to tackle the\nchallenge of task ambiguity in instructions for Natural Language Generation\n(NLG) tasks. Despite the impressive capabilities of Large Language Models\n(LLMs) in understanding and executing a wide range of tasks through natural\nlanguage interaction, their performance is significantly hindered by the\nambiguity present in real-world instructions. To address this, AmbigNLG seeks\nto identify and mitigate such ambiguities, aiming to refine instructions to\nmatch user expectations better. We introduce a dataset, AmbigSNI-NLG,\nconsisting of 2,500 instances, and develop an ambiguity taxonomy for\ncategorizing and annotating instruction ambiguities. Our approach demonstrates\nsubstantial improvements in text generation quality, highlighting the critical\nrole of clear and specific instructions in enhancing LLM performance in NLG\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17717v1.pdf"
    },
    {
        "title": "Case-Based or Rule-Based: How Do Transformers Do the Math?",
        "authors": [
            "Yi Hu",
            "Xiaojuan Tang",
            "Haotong Yang",
            "Muhan Zhang"
        ],
        "published": "2024-02-27T17:41:58Z",
        "summary": "Despite the impressive performance in a variety of complex tasks, modern\nlarge language models (LLMs) still have trouble dealing with some math problems\nthat are simple and intuitive for humans, such as addition. While we can easily\nlearn basic rules of addition and apply them to new problems of any length,\nLLMs struggle to do the same. Instead, they may rely on similar \"cases\" seen in\nthe training corpus for help. We define these two different reasoning\nmechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since\nrule-based reasoning is essential for acquiring the systematic generalization\nability, we aim to explore exactly whether transformers use rule-based or\ncase-based reasoning for math problems. Through carefully designed intervention\nexperiments on five math tasks, we confirm that transformers are performing\ncase-based reasoning, no matter whether scratchpad is used, which aligns with\nthe previous observations that transformers use subgraph matching/shortcut\nlearning to reason. To mitigate such problems, we propose a Rule-Following\nFine-Tuning (RFFT) technique to teach transformers to perform rule-based\nreasoning. Specifically, we provide explicit rules in the input and then\ninstruct transformers to recite and follow the rules step by step. Through\nRFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to\ngeneralize to up to 12-digit addition with over 95% accuracy, which is over 40%\nhigher than scratchpad. The significant improvement demonstrates that teaching\nLLMs to explicitly use rules helps them learn rule-based reasoning and\ngeneralize better in length.",
        "pdf_link": "https://arxiv.org/pdf/2402.17709v1.pdf"
    },
    {
        "title": "NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents",
        "authors": [
            "Tamara Czinczoll",
            "Christoph H\u00f6nes",
            "Maximilian Schall",
            "Gerard de Melo"
        ],
        "published": "2024-02-27T16:56:30Z",
        "summary": "While (large) language models have significantly improved over the last\nyears, they still struggle to sensibly process long sequences found, e.g., in\nbooks, due to the quadratic scaling of the underlying attention mechanism. To\naddress this, we propose NextLevelBERT, a Masked Language Model operating not\non tokens, but on higher-level semantic representations in the form of text\nembeddings. We pretrain NextLevelBERT to predict the vector representation of\nentire masked text chunks and evaluate the effectiveness of the resulting\ndocument vectors on three task types: 1) Semantic Textual Similarity via\nzero-shot document embeddings, 2) Long document classification, 3)\nMultiple-choice question answering. We find that next level Masked Language\nModeling is an effective technique to tackle long-document use cases and can\noutperform much larger embedding models as long as the required level of detail\nis not too high. We make model and code available.",
        "pdf_link": "https://arxiv.org/pdf/2402.17682v1.pdf"
    },
    {
        "title": "Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs",
        "authors": [
            "Tanise Ceron",
            "Neele Falk",
            "Ana Bari\u0107",
            "Dmitry Nikolaev",
            "Sebastian Pad\u00f3"
        ],
        "published": "2024-02-27T16:19:37Z",
        "summary": "Due to the widespread use of large language models (LLMs) in ubiquitous\nsystems, we need to understand whether they embed a specific worldview and what\nthese views reflect. Recent studies report that, prompted with political\nquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclear\nwhether these leanings are reliable (robust to prompt variations) and whether\nthe leaning is consistent across policies and political leaning. We propose a\nseries of tests which assess the reliability and consistency of LLMs' stances\non political statements based on a dataset of voting-advice questionnaires\ncollected from seven EU countries and annotated for policy domains. We study\nLLMs ranging in size from 7B to 70B parameters and find that their reliability\nincreases with parameter count. Larger models show overall stronger alignment\nwith left-leaning parties but differ among policy programs: They evince a\n(left-wing) positive stance towards environment protection, social welfare but\nalso (right-wing) law and order, with no consistent preferences in foreign\npolicy, migration, and economy.",
        "pdf_link": "https://arxiv.org/pdf/2402.17649v1.pdf"
    },
    {
        "title": "SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation",
        "authors": [
            "Shuangrui Ding",
            "Zihan Liu",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Rui Qian",
            "Conghui He",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "published": "2024-02-27T16:15:28Z",
        "summary": "We present SongComposer, an innovative LLM designed for song composition. It\ncould understand and generate melodies and lyrics in symbolic song\nrepresentations, by leveraging the capability of LLM. Existing music-related\nLLM treated the music as quantized audio signals, while such implicit encoding\nleads to inefficient encoding and poor flexibility. In contrast, we resort to\nsymbolic song representation, the mature and efficient way humans designed for\nmusic, and enable LLM to explicitly compose songs like humans. In practice, we\ndesign a novel tuple design to format lyric and three note attributes (pitch,\nduration, and rest duration) in the melody, which guarantees the correct LLM\nunderstanding of musical symbols and realizes precise alignment between lyrics\nand melody. To impart basic music understanding to LLM, we carefully collected\nSongCompose-PT, a large-scale song pretraining dataset that includes lyrics,\nmelodies, and paired lyrics-melodies in either Chinese or English. After\nadequate pre-training, 10K carefully crafted QA pairs are used to empower the\nLLM with the instruction-following capability and solve diverse tasks. With\nextensive experiments, SongComposer demonstrates superior performance in\nlyric-to-melody generation, melody-to-lyric generation, song continuation, and\ntext-to-song creation, outperforming advanced LLMs like GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.17645v1.pdf"
    },
    {
        "title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data",
        "authors": [
            "Xiao Liu",
            "Zirui Wu",
            "Xueqing Wu",
            "Pan Lu",
            "Kai-Wei Chang",
            "Yansong Feng"
        ],
        "published": "2024-02-27T16:15:03Z",
        "summary": "Quantitative reasoning is a critical skill to analyze data, yet the\nassessment of such ability remains limited. To address this gap, we introduce\nthe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate\nLarge Language Models' capability in statistical and causal reasoning with\nreal-world data. The benchmark comprises a carefully constructed dataset of 411\nquestions accompanied by data sheets from textbooks, online learning materials,\nand academic papers. To compare models' quantitative reasoning abilities on\ndata and text, we enrich the benchmark with an auxiliary set of 290 text-only\nquestions, namely QRText. We evaluate natural language reasoning, program-based\nreasoning, and agent reasoning methods including Chain-of-Thought,\nProgram-of-Thoughts, ReAct, and code interpreter assistants on diverse models.\nThe strongest model GPT-4 achieves an accuracy of 58%, which has a large room\nfor improvement. Among open-source models, Deepseek-coder-instruct, a code LLM\npretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals\nthat models encounter difficulties in data analysis and causal reasoning, and\nstruggle in using causal knowledge and provided data simultaneously. Code and\ndata are in https://github.com/xxxiaol/QRData.",
        "pdf_link": "https://arxiv.org/pdf/2402.17644v1.pdf"
    },
    {
        "title": "Variational Learning is Effective for Large Deep Networks",
        "authors": [
            "Yuesong Shen",
            "Nico Daheim",
            "Bai Cong",
            "Peter Nickl",
            "Gian Maria Marconi",
            "Clement Bazan",
            "Rio Yokota",
            "Iryna Gurevych",
            "Daniel Cremers",
            "Mohammad Emtiyaz Khan",
            "Thomas M\u00f6llenhoff"
        ],
        "published": "2024-02-27T16:11:05Z",
        "summary": "We give extensive empirical evidence against the common belief that\nvariational learning is ineffective for large neural networks. We show that an\noptimizer called Improved Variational Online Newton (IVON) consistently matches\nor outperforms Adam for training large networks such as GPT-2 and ResNets from\nscratch. IVON's computational costs are nearly identical to Adam but its\npredictive uncertainty is better. We show several new use cases of IVON where\nwe improve fine-tuning and model merging in Large Language Models, accurately\npredict generalization error, and faithfully estimate sensitivity to data. We\nfind overwhelming evidence in support of effectiveness of variational learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.17641v1.pdf"
    },
    {
        "title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization",
        "authors": [
            "Wenqi Zhang",
            "Ke Tang",
            "Hai Wu",
            "Mengna Wang",
            "Yongliang Shen",
            "Guiyang Hou",
            "Zeqi Tan",
            "Peng Li",
            "Yueting Zhuang",
            "Weiming Lu"
        ],
        "published": "2024-02-27T15:09:20Z",
        "summary": "Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.17574v2.pdf"
    },
    {
        "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation",
        "authors": [
            "Sunghyeon Woo",
            "Baeseong Park",
            "Byeongwook Kim",
            "Minjung Jo",
            "Sejung Kwon",
            "Dongsuk Jeon",
            "Dongsoo Lee"
        ],
        "published": "2024-02-27T14:51:11Z",
        "summary": "Training deep neural networks typically involves substantial computational\ncosts during both forward and backward propagation. The conventional layer\ndropping techniques drop certain layers during training for reducing the\ncomputations burden. However, dropping layers during forward propagation\nadversely affects the training process by degrading accuracy. In this paper, we\npropose Dropping Backward Propagation (DropBP), a novel approach designed to\nreduce computational costs while maintaining accuracy. DropBP randomly drops\nlayers during the backward propagation, which does not deviate forward\npropagation. Moreover, DropBP calculates the sensitivity of each layer to\nassign appropriate drop rate, thereby stabilizing the training process. DropBP\nis designed to enhance the efficiency of the training process with\nbackpropagation, thereby enabling the acceleration of both full fine-tuning and\nparameter-efficient fine-tuning using backpropagation. Specifically, utilizing\nDropBP in QLoRA reduces training time by 44%, increases the convergence speed\nto the identical loss level by 1.5$\\times$, and enables training with a\n6.2$\\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in\nLLaMA2-70B. The code is available at https://github.com/WooSunghyeon/dropbp.",
        "pdf_link": "https://arxiv.org/pdf/2402.17812v1.pdf"
    },
    {
        "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
        "authors": [
            "Raghav Kapoor",
            "Yash Parag Butala",
            "Melisa Russak",
            "Jing Yu Koh",
            "Kiran Kamble",
            "Waseem Alshikh",
            "Ruslan Salakhutdinov"
        ],
        "published": "2024-02-27T14:47:53Z",
        "summary": "For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.",
        "pdf_link": "https://arxiv.org/pdf/2402.17553v2.pdf"
    },
    {
        "title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space",
        "authors": [
            "Shaolei Zhang",
            "Tian Yu",
            "Yang Feng"
        ],
        "published": "2024-02-27T14:45:04Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, they sometimes suffer from producing hallucinations,\nparticularly in cases where they may generate untruthful responses despite\npossessing the correct knowledge. In this paper, we propose TruthX, an\ninference-time method to elicit the truthfulness of LLMs by editing their\ninternal representations in truthful space. TruthX employs an auto-encoder to\nmap LLM's representations into semantic and truthful latent spaces\nrespectively, and applies contrastive learning to identify a truthful editing\ndirection within the truthful space. During inference, by editing LLM's\ninternal representations in truthful space, TruthX effectively enhances the\ntruthfulness of LLMs. Experiments show that TruthX effectively improves the\ntruthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark.\nFurther analyses suggest that the truthful space acquired by TruthX plays a\npivotal role in controlling LLM to produce truthful or hallucinatory responses.",
        "pdf_link": "https://arxiv.org/pdf/2402.17811v1.pdf"
    },
    {
        "title": "Retrieval is Accurate Generation",
        "authors": [
            "Bowen Cao",
            "Deng Cai",
            "Leyang Cui",
            "Xuxin Cheng",
            "Wei Bi",
            "Yuexian Zou",
            "Shuming Shi"
        ],
        "published": "2024-02-27T14:16:19Z",
        "summary": "Standard language models generate text by selecting tokens from a fixed,\nfinite, and standalone vocabulary. We introduce a novel method that selects\ncontext-aware phrases from a collection of supporting documents. One of the\nmost significant challenges for this paradigm shift is determining the training\noracles, because a string of text can be segmented in various ways and each\nsegment can be retrieved from numerous possible documents. To address this, we\npropose to initialize the training oracles using linguistic heuristics and,\nmore importantly, bootstrap the oracles through iterative self-reinforcement.\nExtensive experiments show that our model not only outperforms standard\nlanguage models on a variety of knowledge-intensive tasks but also demonstrates\nimproved generation quality in open-ended text generation. For instance,\ncompared to the standard language model counterpart, our model raises the\naccuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from\n42.61% to 81.58% in open-ended text generation. Remarkably, our model also\nachieves the best performance and the lowest latency among several\nretrieval-augmented baselines. In conclusion, we assert that retrieval is more\naccurate generation and hope that our work will encourage further research on\nthis new paradigm shift.",
        "pdf_link": "https://arxiv.org/pdf/2402.17532v3.pdf"
    },
    {
        "title": "Predict the Next Word: Humans exhibit uncertainty in this task and language models _____",
        "authors": [
            "Evgenia Ilia",
            "Wilker Aziz"
        ],
        "published": "2024-02-27T14:11:32Z",
        "summary": "Language models (LMs) are statistical models trained to assign probability to\nhuman-generated text. As such, it is reasonable to question whether they\napproximate linguistic variability exhibited by humans well. This form of\nstatistical assessment is difficult to perform at the passage level, for it\nrequires acceptability judgements (i.e., human evaluation) or a robust\nautomated proxy (which is non-trivial). At the word level, however, given some\ncontext, samples from an LM can be assessed via exact matching against a\nprerecorded dataset of alternative single-word continuations of the available\ncontext. We exploit this fact and evaluate the LM's ability to reproduce\nvariability that humans (in particular, a population of English speakers)\nexhibit in the 'next word prediction' task. This can be seen as assessing a\nform of calibration, which, in the context of text classification, Baan et al.\n(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and\nChatGPT and find that they exhibit fairly low calibration to human uncertainty.\nWe also verify the failure of expected calibration error (ECE) to reflect this,\nand as such, advise the community against relying on it in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2402.17527v2.pdf"
    },
    {
        "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
        "authors": [
            "Ruiyang Ren",
            "Peng Qiu",
            "Yingqi Qu",
            "Jing Liu",
            "Wayne Xin Zhao",
            "Hua Wu",
            "Ji-Rong Wen",
            "Haifeng Wang"
        ],
        "published": "2024-02-27T13:44:09Z",
        "summary": "Due to the excellent capacities of large language models (LLMs), it becomes\nfeasible to develop LLM-based agents for reliable user simulation. Considering\nthe scarcity and limit (e.g., privacy issues) of real user data, in this paper,\nwe conduct large-scale user simulation for web search, to improve the analysis\nand modeling of user search behavior. Specially, we propose BASES, a novel user\nsimulation framework with LLM-based agents, designed to facilitate\ncomprehensive simulations of web search user behaviors. Our simulation\nframework can generate unique user profiles at scale, which subsequently leads\nto diverse search behaviors. To demonstrate the effectiveness of BASES, we\nconduct evaluation experiments based on two human benchmarks in both Chinese\nand English, demonstrating that BASES can effectively simulate large-scale\nhuman-like search behaviors. To further accommodate the research on web search,\nwe develop WARRIORS, a new large-scale dataset encompassing web search user\nbehaviors, including both Chinese and English versions, which can greatly\nbolster research in the field of information retrieval. Our code and data will\nbe publicly released soon.",
        "pdf_link": "https://arxiv.org/pdf/2402.17505v1.pdf"
    },
    {
        "title": "Intensive Care as One Big Sequence Modeling Problem",
        "authors": [
            "Vadim Liventsev",
            "Tobias Fritz"
        ],
        "published": "2024-02-27T13:36:55Z",
        "summary": "Reinforcement Learning in Healthcare is typically concerned with narrow\nself-contained tasks such as sepsis prediction or anesthesia control. However,\nprevious research has demonstrated the potential of generalist models (the\nprime example being Large Language Models) to outperform task-specific\napproaches due to their capability for implicit transfer learning. To enable\ntraining of foundation models for Healthcare as well as leverage the\ncapabilities of state of the art Transformer architectures, we propose the\nparadigm of Healthcare as Sequence Modeling, in which interaction between the\npatient and the healthcare provider is represented as an event stream and tasks\nlike diagnosis and treatment selection are modeled as prediction of future\nevents in the stream. To explore this paradigm experimentally we develop\nMIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous\nclinical records from MIMIC-IV dataset into a uniform event stream format,\ntrain a baseline model and explore its capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.17501v1.pdf"
    },
    {
        "title": "REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering",
        "authors": [
            "Yuhao Wang",
            "Ruiyang Ren",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Jing Liu",
            "Ji-Rong Wen"
        ],
        "published": "2024-02-27T13:22:51Z",
        "summary": "Considering the limited internal parametric knowledge, retrieval-augmented\ngeneration (RAG) has been widely used to extend the knowledge scope of large\nlanguage models (LLMs). Despite the extensive efforts on RAG research, in\nexisting methods, LLMs cannot precisely assess the relevance of retrieved\ndocuments, thus likely leading to misleading or even incorrect utilization of\nexternal knowledge (i.e., retrieved documents). To address this issue, in this\npaper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for\nopen-domain question answering (QA). As the key motivation, we aim to enhance\nthe self-awareness of source relevance for LLMs, so as to adaptively utilize\nexternal knowledge in RAG systems. Specially, we develop a new architecture for\nLLM based RAG system, by incorporating a specially designed rank head that\nprecisely assesses the relevance of retrieved documents. Furthermore, we\npropose an improved training method based on bi-granularity relevance fusion\nand noise-resistant training. By combining the improvements in both\narchitecture and training, our proposed REAR can better utilize external\nknowledge by effectively perceiving the relevance of retrieved documents.\nExperiments on four open-domain QA tasks show that REAR significantly\noutperforms previous a number of competitive RAG approaches. Our code and data\ncan be accessed at https://github.com/RUCAIBox/REAR.",
        "pdf_link": "https://arxiv.org/pdf/2402.17497v1.pdf"
    },
    {
        "title": "Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?",
        "authors": [
            "Bing Xue",
            "Charles Alba",
            "Joanna Abraham",
            "Thomas Kannampallil",
            "Chenyang Lu"
        ],
        "published": "2024-02-27T13:18:00Z",
        "summary": "Postoperative risk predictions can inform effective perioperative care\nmanagement and planning. We aimed to assess whether clinical large language\nmodels (LLMs) can predict postoperative risks using clinical texts with various\ntraining strategies. The main cohort involved 84,875 records from Barnes Jewish\nHospital (BJH) system between 2018 and 2021. Methods were replicated on Beth\nIsrael Deaconess's MIMIC dataset. Both studies had mean duration of follow-up\nbased on the length of postoperative ICU stay less than 7 days. For the BJH\ndataset, outcomes included 30-day mortality, pulmonary embolism (PE) and\npneumonia. Three domain adaptation and finetuning strategies were implemented\nfor BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives;\nincorporating labels with semi-supervised fine-tuning; and foundational\nmodelling through multi-task learning. Model performance was compared using the\narea under the receiver operating characteristic curve (AUROC) and the area\nunder the precision recall curve (AUPRC) for classification tasks, and mean\nsquared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed\ntraditional word embeddings, with absolute maximal gains of 38.3% for AUROC and\n14% for AUPRC. Adapting models further improved performance: (1)\nself-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2)\nsemi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to\nself-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and\n2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical\nLLMs offer opportunities for postoperative risk predictions in unforeseen data,\nwith peaks in foundational models indicating the potential of task-agnostic\nlearning towards the generalizability of LLMs in perioperative care.",
        "pdf_link": "https://arxiv.org/pdf/2402.17493v2.pdf"
    },
    {
        "title": "Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles",
        "authors": [
            "Maram Hasanain",
            "Fatema Ahmed",
            "Firoj Alam"
        ],
        "published": "2024-02-27T13:02:19Z",
        "summary": "The use of propaganda has spiked on mainstream and social media, aiming to\nmanipulate or mislead users. While efforts to automatically detect propaganda\ntechniques in textual, visual, or multimodal content have increased, most of\nthem primarily focus on English content. The majority of the recent initiatives\ntargeting medium to low-resource languages produced relatively small annotated\ndatasets, with a skewed distribution, posing challenges for the development of\nsophisticated propaganda detection models. To address this challenge, we\ncarefully develop the largest propaganda dataset to date, ArPro, comprised of\n8K paragraphs from newspaper articles, labeled at the text span level following\na taxonomy of 23 propagandistic techniques. Furthermore, our work offers the\nfirst attempt to understand the performance of large language models (LLMs),\nusing GPT-4, for fine-grained propaganda detection from text. Results showed\nthat GPT-4's performance degrades as the task moves from simply classifying a\nparagraph as propagandistic or not, to the fine-grained task of detecting\npropaganda techniques and their manifestation in text. Compared to models\nfine-tuned on the dataset for propaganda detection at different classification\ngranularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a\ndataset consisting of six other languages for span detection, and results\nsuggest that the model struggles with the task across languages. Our dataset\nand resources will be released to the community.",
        "pdf_link": "https://arxiv.org/pdf/2402.17478v1.pdf"
    },
    {
        "title": "Training-Free Long-Context Scaling of Large Language Models",
        "authors": [
            "Chenxin An",
            "Fei Huang",
            "Jun Zhang",
            "Shansan Gong",
            "Xipeng Qiu",
            "Chang Zhou",
            "Lingpeng Kong"
        ],
        "published": "2024-02-27T12:39:23Z",
        "summary": "The ability of Large Language Models (LLMs) to process and generate coherent\ntext is markedly weakened when the number of input tokens exceeds their\npretraining length. Given the expensive overhead of finetuning large-scale\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\nenables Llama2 70B to support context windows of more than 100k tokens without\ncontinual training. By decomposing the attention computation for long sequences\ninto chunk-based modules, DCA manages to effectively capture the relative\npositional information of tokens within the same chunk (Intra-Chunk) and across\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\nAttention. In addition to its impressive extrapolation capability, DCA achieves\nperformance on practical long-context tasks that is comparable to or even\nbetter than that of finetuned models. When compared with proprietary models,\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\nindicating it is a viable open-source alternative. All code and data used in\nthis work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
        "pdf_link": "https://arxiv.org/pdf/2402.17463v1.pdf"
    },
    {
        "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
        "authors": [
            "Siyuan Guo",
            "Cheng Deng",
            "Ying Wen",
            "Hechang Chen",
            "Yi Chang",
            "Jun Wang"
        ],
        "published": "2024-02-27T12:26:07Z",
        "summary": "In this work, we investigate the potential of large language models (LLMs)\nbased agents to automate data science tasks, with the goal of comprehending\ntask requirements, then building and training the best-fit machine learning\nmodels. Despite their widespread success, existing LLM agents are hindered by\ngenerating unreasonable experiment plans within this scenario. To this end, we\npresent DS-Agent, a novel automatic framework that harnesses LLM agent and\ncase-based reasoning (CBR). In the development stage, DS-Agent follows the CBR\nframework to structure an automatic iteration pipeline, which can flexibly\ncapitalize on the expert knowledge from Kaggle, and facilitate consistent\nperformance improvement through the feedback mechanism. Moreover, DS-Agent\nimplements a low-resource deployment stage with a simplified CBR paradigm to\nadapt past successful solutions from the development stage for direct code\ngeneration, significantly reducing the demand on foundational capabilities of\nLLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success\nrate in the development stage, while attaining 36% improvement on average one\npass rate across alternative LLMs in the deployment stage. In both stages,\nDS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per\nrun with GPT-4, respectively. Our code is open-sourced at\nhttps://github.com/guosyjlu/DS-Agent.",
        "pdf_link": "https://arxiv.org/pdf/2402.17453v3.pdf"
    },
    {
        "title": "Deep Learning Based Named Entity Recognition Models for Recipes",
        "authors": [
            "Mansi Goel",
            "Ayush Agarwal",
            "Shubham Agrawal",
            "Janak Kapuriya",
            "Akhil Vamshi Konam",
            "Rishabh Gupta",
            "Shrey Rastogi",
            "Niharika",
            "Ganesh Bagler"
        ],
        "published": "2024-02-27T12:03:56Z",
        "summary": "Food touches our lives through various endeavors, including flavor,\nnourishment, health, and sustainability. Recipes are cultural capsules\ntransmitted across generations via unstructured text. Automated protocols for\nrecognizing named entities, the building blocks of recipe text, are of immense\nvalue for various applications ranging from information extraction to novel\nrecipe generation. Named entity recognition is a technique for extracting\ninformation from unstructured or semi-structured data with known labels.\nStarting with manually-annotated data of 6,611 ingredient phrases, we created\nan augmented dataset of 26,445 phrases cumulatively. Simultaneously, we\nsystematically cleaned and analyzed ingredient phrases from RecipeDB, the\ngold-standard recipe data repository, and annotated them using the Stanford\nNER. Based on the analysis, we sampled a subset of 88,526 phrases using a\nclustering-based approach while preserving the diversity to create the\nmachine-annotated dataset. A thorough investigation of NER approaches on these\nthree datasets involving statistical, fine-tuning of deep learning-based\nlanguage models and few-shot prompting on large language models (LLMs) provides\ndeep insights. We conclude that few-shot prompting on LLMs has abysmal\nperformance, whereas the fine-tuned spaCy-transformer emerges as the best model\nwith macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,\naugmented, and machine-annotated datasets, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.17447v1.pdf"
    },
    {
        "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder",
        "authors": [
            "Jiaqi Wang",
            "Zhenxi Song",
            "Zhengyu Ma",
            "Xipeng Qiu",
            "Min Zhang",
            "Zhiguo Zhang"
        ],
        "published": "2024-02-27T11:45:21Z",
        "summary": "Reconstructing natural language from non-invasive electroencephalography\n(EEG) holds great promise as a language decoding technology for brain-computer\ninterfaces (BCIs). However, EEG-based language decoding is still in its nascent\nstages, facing several technical issues such as: 1) Absence of a hybrid\nstrategy that can effectively integrate cross-modality (between EEG and text)\nself-learning with intra-modality self-reconstruction of EEG features or\ntextual sequences; 2) Under-utilization of large language models (LLMs) to\nenhance EEG-based language decoding. To address above issues, we propose the\nContrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that\norchestrates compound self-supervised learning across and within EEG and text\nthrough a dedicated multi-stream encoder. Furthermore, we develop a framework\ncalled E2T-PTR (EEG-to-Text decoding using Pretrained Transferable\nRepresentations), which leverages pre-trained modules alongside the EEG stream\nfrom CET-MAE and further enables an LLM (specifically BART) to decode text from\nEEG sequences. Comprehensive experiments conducted on the popular text-evoked\nEEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms\nthe state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,\nrespectively. These results indicate significant advancements in the field and\nunderscores the proposed framework's potential to enable more powerful and\nwidespread BCI applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.17433v2.pdf"
    },
    {
        "title": "Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective",
        "authors": [
            "Fufangchen Zhao",
            "Guoqiang Jin",
            "Jiaheng Huang",
            "Rui Zhao",
            "Fei Tan"
        ],
        "published": "2024-02-27T11:02:12Z",
        "summary": "Nowadays both commercial and open-source academic LLM have become the\nmainstream models of NLP. However, there is still a lack of research on LLM\nconsistency, meaning that throughout the various stages of LLM research and\ndeployment, its internal parameters and capabilities should remain unchanged.\nThis issue exists in both the industrial and academic sectors. The solution to\nthis problem is often time-consuming and labor-intensive, and there is also an\nadditional cost of secondary deployment, resulting in economic and time losses.\nTo fill this gap, we build an LLM consistency task dataset and design several\nbaselines. Additionally, we choose models of diverse scales for the main\nexperiments. Specifically, in the LightGBM experiment, we used traditional NLG\nmetrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training.\nThe final result exceeds the manual evaluation and GPT3.5 as well as other\nmodels in the main experiment, achieving the best performance. In the end, we\nuse the best performing LightGBM model as the base model to build the\nevaluation tool, which can effectively assist in the deployment of business\nmodels. Our code and tool demo are available at\nhttps://github.com/heavenhellchen/Consistency.git",
        "pdf_link": "https://arxiv.org/pdf/2402.17411v2.pdf"
    },
    {
        "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
        "authors": [
            "\u00c7a\u011fatay Y\u0131ld\u0131z",
            "Nishaanth Kanna Ravichandran",
            "Prishruit Punia",
            "Matthias Bethge",
            "Beyza Ermis"
        ],
        "published": "2024-02-27T10:47:24Z",
        "summary": "This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.17400v1.pdf"
    },
    {
        "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies",
        "authors": [
            "Flavio Petruzzellis",
            "Alberto Testolin",
            "Alessandro Sperduti"
        ],
        "published": "2024-02-27T10:44:52Z",
        "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing thanks to their ability to reuse knowledge acquired on\nmassive text corpora on a wide variety of downstream tasks, with minimal (if\nany) tuning steps. At the same time, it has been repeatedly shown that LLMs\nlack systematic generalization, which allows to extrapolate the learned\nstatistical regularities outside the training distribution. In this work, we\noffer a systematic benchmarking of GPT-4, one of the most advanced LLMs\navailable, on three algorithmic tasks characterized by the possibility to\ncontrol the problem difficulty with two parameters. We compare the performance\nof GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the\nTransformer-Encoder architecture recently introduced to solve similar tasks,\nthe Neural Data Router. We find that the deployment of advanced prompting\ntechniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating\nthat state-of-the-art LLMs constitute a very strong baseline also in\nchallenging tasks that require systematic generalization.",
        "pdf_link": "https://arxiv.org/pdf/2402.17396v1.pdf"
    },
    {
        "title": "Determinants of LLM-assisted Decision-Making",
        "authors": [
            "Eva Eigner",
            "Thorsten H\u00e4ndler"
        ],
        "published": "2024-02-27T10:24:50Z",
        "summary": "Decision-making is a fundamental capability in everyday life. Large Language\nModels (LLMs) provide multifaceted support in enhancing human decision-making\nprocesses. However, understanding the influencing factors of LLM-assisted\ndecision-making is crucial for enabling individuals to utilize LLM-provided\nadvantages and minimize associated risks in order to make more informed and\nbetter decisions. This study presents the results of a comprehensive literature\nanalysis, providing a structural overview and detailed analysis of determinants\nimpacting decision-making with LLM support. In particular, we explore the\neffects of technological aspects of LLMs, including transparency and prompt\nengineering, psychological factors such as emotions and decision-making styles,\nas well as decision-specific determinants such as task difficulty and\naccountability. In addition, the impact of the determinants on the\ndecision-making process is illustrated via multiple application scenarios.\nDrawing from our analysis, we develop a dependency framework that systematizes\npossible interactions in terms of reciprocal interdependencies between these\ndeterminants. Our research reveals that, due to the multifaceted interactions\nwith various determinants, factors such as trust in or reliance on LLMs, the\nuser's mental model, and the characteristics of information processing are\nidentified as significant aspects influencing LLM-assisted decision-making\nprocesses. Our findings can be seen as crucial for improving decision quality\nin human-AI collaboration, empowering both users and organizations, and\ndesigning more effective LLM interfaces. Additionally, our work provides a\nfoundation for future empirical investigations on the determinants of\ndecision-making assisted by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.17385v1.pdf"
    },
    {
        "title": "LLMGuard: Guarding Against Unsafe LLM Behavior",
        "authors": [
            "Shubh Goyal",
            "Medha Hira",
            "Shubham Mishra",
            "Sukriti Goyal",
            "Arnav Goel",
            "Niharika Dadu",
            "Kirushikesh DB",
            "Sameep Mehta",
            "Nishtha Madaan"
        ],
        "published": "2024-02-27T10:22:45Z",
        "summary": "Although the rise of Large Language Models (LLMs) in enterprise settings\nbrings new opportunities and capabilities, it also brings challenges, such as\nthe risk of generating inappropriate, biased, or misleading content that\nviolates regulations and can have legal concerns. To alleviate this, we present\n\"LLMGuard\", a tool that monitors user interactions with an LLM application and\nflags content against specific behaviours or conversation topics. To do this\nrobustly, LLMGuard employs an ensemble of detectors.",
        "pdf_link": "https://arxiv.org/pdf/2403.00826v1.pdf"
    },
    {
        "title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following",
        "authors": [
            "Xinyu Lu",
            "Bowen Yu",
            "Yaojie Lu",
            "Hongyu Lin",
            "Haiyang Yu",
            "Le Sun",
            "Xianpei Han",
            "Yongbin Li"
        ],
        "published": "2024-02-27T09:52:27Z",
        "summary": "The alignment problem in Large Language Models (LLMs) involves adapting them\nto the broad spectrum of human values. This requirement challenges existing\nalignment methods due to diversity of preferences and regulatory standards.\nThis paper introduces a novel alignment paradigm, priority rule following,\nwhich defines rules as the primary control mechanism in each dialog,\nprioritizing them over user instructions. Our preliminary analysis reveals that\neven the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding\nand prioritizing the rules. Therefore, we present PriorityDistill, a\nsemi-automated approach for distilling priority following signals from LLM\nsimulations to ensure robust rule integration and adherence. Our experiments\nshow that this method not only effectively minimizes misalignments utilizing\nonly one general rule but also adapts smoothly to various unseen rules,\nensuring they are shielded from hijacking and that the model responds\nappropriately.",
        "pdf_link": "https://arxiv.org/pdf/2402.17358v1.pdf"
    },
    {
        "title": "RECOST: External Knowledge Guided Data-efficient Instruction Tuning",
        "authors": [
            "Qi Zhang",
            "Yiming Zhang",
            "Haobo Wang",
            "Junbo Zhao"
        ],
        "published": "2024-02-27T09:47:36Z",
        "summary": "In the current landscape of large language models (LLMs), the process of\ninstruction tuning serves as an essential step. Considering the high computing\npower overhead, data-efficient instruction tuning was proposed to reduce the\ntraining data size in this process, aiming at selecting high-quality\ninstructional data. Nevertheless, we argue that most current data-efficient\ninstruction-tuning methods are highly dependent on the quality of the original\ninstruction-tuning dataset. When it comes to datasets synthesized by LLMs, a\ncommon scenario in this field, dirty samples will even be selected with a\nhigher probability than other samples. To address these challenges, we utilized\nexternal knowledge (relevant examples or paragraphs) to evaluate those samples\nsynthesized by LLMs with an in-context-based relative predictive entropy. Based\non the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which\nintegrates external-knowledge-base re-ranking and diversity-consistent sampling\ninto a single pipeline. Through extensive experiments on several synthetic\ndatasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our\nmethod and achieve even better results with only \\textbf{1\\%} of the full\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.17355v1.pdf"
    },
    {
        "title": "Probing Multimodal Large Language Models for Global and Local Semantic Representations",
        "authors": [
            "Mingxu Tao",
            "Quzhe Huang",
            "Kun Xu",
            "Liwei Chen",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "2024-02-27T08:27:15Z",
        "summary": "The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.",
        "pdf_link": "https://arxiv.org/pdf/2402.17304v2.pdf"
    },
    {
        "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
        "authors": [
            "Rifki Afina Putri",
            "Faiz Ghifari Haznitrama",
            "Dea Adhista",
            "Alice Oh"
        ],
        "published": "2024-02-27T08:24:32Z",
        "summary": "Large Language Models (LLMs) are increasingly being used to generate\nsynthetic data for training and evaluating models. However, it is unclear\nwhether they can generate a good quality of question answering (QA) dataset\nthat incorporates knowledge and cultural nuance embedded in a language,\nespecially for low-resource languages. In this study, we investigate the\neffectiveness of using LLMs in generating culturally relevant commonsense QA\ndatasets for Indonesian and Sundanese languages. To do so, we create datasets\nfor these languages using various methods involving both LLMs and human\nannotators. Our experiments show that the current best-performing LLM, GPT-4\nTurbo, is capable of generating questions with adequate knowledge in Indonesian\nbut not in Sundanese, highlighting the performance discrepancy between medium-\nand lower-resource languages. We also benchmark various LLMs on our generated\ndatasets and find that they perform better on the LLM-generated datasets\ncompared to those created by humans.",
        "pdf_link": "https://arxiv.org/pdf/2402.17302v1.pdf"
    },
    {
        "title": "Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning",
        "authors": [
            "Pengjie Ren",
            "Chengshun Shi",
            "Shiguang Wu",
            "Mengqi Zhang",
            "Zhaochun Ren",
            "Maarten de Rijke",
            "Zhumin Chen",
            "Jiahuan Pei"
        ],
        "published": "2024-02-27T07:14:12Z",
        "summary": "Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring\npre-trained large language models (LLMs), especially as the models' scale and\nthe diversity of tasks increase. Low-rank adaptation (LoRA) is based on the\nidea that the adaptation process is intrinsically low-dimensional, i.e.,\nsignificant model changes can be represented with relatively few parameters.\nHowever, decreasing the rank encounters challenges with generalization errors\nfor specific tasks when compared to full-parameter fine-tuning. We present\nMELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters\nwhile maintaining a higher rank, thereby offering improved performance\npotential. The core idea is to freeze original pretrained weights and train a\ngroup of mini LoRAs with only a small number of parameters. This can capture a\nsignificant degree of diversity among mini LoRAs, thus promoting better\ngeneralization ability. We conduct a theoretical analysis and empirical studies\non various NLP tasks. Our experimental results show that, compared to LoRA,\nMELoRA achieves better performance with 8 times fewer trainable parameters on\nnatural language understanding tasks and 36 times fewer trainable parameters on\ninstruction following tasks, which demonstrates the effectiveness of MELoRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.17263v1.pdf"
    },
    {
        "title": "Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue",
        "authors": [
            "Zhenhong Zhou",
            "Jiuyang Xiang",
            "Haopeng Chen",
            "Quan Liu",
            "Zherui Li",
            "Sen Su"
        ],
        "published": "2024-02-27T07:11:59Z",
        "summary": "Large Language Models (LLMs) have been demonstrated to generate illegal or\nunethical responses, particularly when subjected to \"jailbreak.\" Research on\njailbreak has highlighted the safety issues of LLMs. However, prior studies\nhave predominantly focused on single-turn dialogue, ignoring the potential\ncomplexities and risks presented by multi-turn dialogue, a crucial mode through\nwhich humans derive information from LLMs. In this paper, we argue that humans\ncould exploit multi-turn dialogue to induce LLMs into generating harmful\ninformation. LLMs may not intend to reject cautionary or borderline unsafe\nqueries, even if each turn is closely served for one malicious purpose in a\nmulti-turn dialogue. Therefore, by decomposing an unsafe query into several\nsub-queries for multi-turn dialogue, we induced LLMs to answer harmful\nsub-questions incrementally, culminating in an overall harmful response. Our\nexperiments, conducted across a wide range of LLMs, indicate current\ninadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our\nfindings expose vulnerabilities of LLMs in complex scenarios involving\nmulti-turn dialogue, presenting new challenges for the safety of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.17262v1.pdf"
    },
    {
        "title": "Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection",
        "authors": [
            "Pei Wang",
            "Keqing He",
            "Yejie Wang",
            "Xiaoshuai Song",
            "Yutao Mou",
            "Jingang Wang",
            "Yunsen Xian",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "published": "2024-02-27T07:02:10Z",
        "summary": "Out-of-domain (OOD) intent detection aims to examine whether the user's query\nfalls outside the predefined domain of the system, which is crucial for the\nproper functioning of task-oriented dialogue (TOD) systems. Previous methods\naddress it by fine-tuning discriminative models. Recently, some studies have\nbeen exploring the application of large language models (LLMs) represented by\nChatGPT to various downstream tasks, but it is still unclear for their ability\non OOD detection task.This paper conducts a comprehensive evaluation of LLMs\nunder various experimental settings, and then outline the strengths and\nweaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot\ncapabilities, but is still at a disadvantage compared to models fine-tuned with\nfull resource. More deeply, through a series of additional analysis\nexperiments, we discuss and summarize the challenges faced by LLMs and provide\nguidance for future work including injecting domain knowledge, strengthening\nknowledge transfer from IND(In-domain) to OOD, and understanding long\ninstructions.",
        "pdf_link": "https://arxiv.org/pdf/2402.17256v2.pdf"
    },
    {
        "title": "MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning",
        "authors": [
            "Debrup Das",
            "Debopriyo Banerjee",
            "Somak Aditya",
            "Ashish Kulkarni"
        ],
        "published": "2024-02-27T05:50:35Z",
        "summary": "Tool-augmented Large Language Models (TALMs) are known to enhance the\nskillset of large language models (LLMs), thereby, leading to their improved\nreasoning abilities across many tasks. While, TALMs have been successfully\nemployed in different question-answering benchmarks, their efficacy on complex\nmathematical reasoning benchmarks, and the potential complementary benefits\noffered by tools for knowledge retrieval and mathematical equation solving are\nopen research questions. In this work, we present MathSensei, a tool-augmented\nlarge language model for mathematical reasoning. We study the complementary\nbenefits of the tools - knowledge retriever (Bing Web Search), program\ngenerator + executor (Python), and symbolic equation solver (Wolfram-Alpha API)\nthrough evaluations on mathematical reasoning datasets. We perform exhaustive\nablations on MATH, a popular dataset for evaluating mathematical reasoning on\ndiverse mathematical disciplines. We also conduct experiments involving\nwell-known tool planners to study the impact of tool sequencing on the model\nperformance. MathSensei achieves 13.5% better accuracy over gpt-3.5-turbo with\nChain-of-Thought on the MATH dataset. We further observe that TALMs are not as\neffective for simpler math word problems (in GSM-8K), and the benefit increases\nas the complexity and required knowledge increases (progressively over AQuA,\nMMLU-Math, and higher level complex questions in MATH). The code and data are\navailable at https://github.com/Debrup-61/MathSensei.",
        "pdf_link": "https://arxiv.org/pdf/2402.17231v3.pdf"
    },
    {
        "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models",
        "authors": [
            "Xiaolong Wang",
            "Yile Wang",
            "Yuanchi Zhang",
            "Fuwen Luo",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-27T05:37:10Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable performance in\nobjective tasks such as open-domain question answering and mathematical\nreasoning, which can often be solved through recalling learned factual\nknowledge or chain-of-thought style reasoning. However, we find that the\nperformance of LLMs in subjective tasks is still unsatisfactory, such as\nmetaphor recognition, dark humor detection, etc. Compared to objective tasks,\nsubjective tasks focus more on interpretation or emotional response rather than\na universally accepted reasoning pathway. Based on the characteristics of the\ntasks and the strong dialogue-generation capabilities of LLMs, we propose RiC\n(Reasoning in Conversation), a method that focuses on solving subjective tasks\nthrough dialogue simulation. The motivation of RiC is to mine useful contextual\ninformation by simulating dialogues instead of supplying chain-of-thought style\nrationales, thereby offering potential useful knowledge behind dialogues for\ngiving the final answers. We evaluate both API-based and open-source LLMs\nincluding GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental\nresults show that RiC can yield significant improvement compared with various\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.17226v1.pdf"
    },
    {
        "title": "Measuring Vision-Language STEM Skills of Neural Models",
        "authors": [
            "Jianhao Shen",
            "Ye Yuan",
            "Srbuhi Mirzoyan",
            "Ming Zhang",
            "Chenguang Wang"
        ],
        "published": "2024-02-27T04:55:03Z",
        "summary": "We introduce a new challenge to test the STEM skills of neural models. The\nproblems in the real world often require solutions, combining knowledge from\nSTEM (science, technology, engineering, and math). Unlike existing datasets,\nour dataset requires the understanding of multimodal vision-language\ninformation of STEM. Our dataset features one of the largest and most\ncomprehensive datasets for the challenge. It includes 448 skills and 1,073,146\nquestions spanning all STEM subjects. Compared to existing datasets that often\nfocus on examining expert-level ability, our dataset includes fundamental\nskills and questions designed based on the K-12 curriculum. We also add\nstate-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our\nbenchmark. Results show that the recent model advances only help master a very\nlimited number of lower grade-level skills (2.5% in the third grade) in our\ndataset. In fact, these models are still well below (averaging 54.7%) the\nperformance of elementary students, not to mention near expert-level\nperformance. To understand and increase the performance on our dataset, we\nteach the models on a training split of our dataset. Even though we observe\nimproved performance, the model performance remains relatively low compared to\naverage elementary students. To solve STEM problems, we will need novel\nalgorithmic innovations from the community.",
        "pdf_link": "https://arxiv.org/pdf/2402.17205v1.pdf"
    },
    {
        "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",
        "authors": [
            "Biao Zhang",
            "Zhongtao Liu",
            "Colin Cherry",
            "Orhan Firat"
        ],
        "published": "2024-02-27T04:18:49Z",
        "summary": "While large language models (LLMs) often adopt finetuning to unlock their\ncapabilities for downstream applications, our understanding on the inductive\nbiases (especially the scaling properties) of different finetuning methods is\nstill limited. To fill this gap, we conduct systematic experiments studying\nwhether and how different scaling factors, including LLM model size,\npretraining data size, new finetuning parameter size and finetuning data size,\naffect the finetuning performance. We consider two types of finetuning --\nfull-model tuning (FMT) and parameter efficient tuning (PET, including prompt\ntuning and LoRA), and explore their scaling behaviors in the data-limited\nregime where the LLM model size substantially outweighs the finetuning data\nsize. Based on two sets of pretrained bilingual LLMs from 1B to 16B and\nexperiments on bilingual machine translation and multilingual summarization\nbenchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative\njoint scaling law between finetuning data size and each other scaling factor;\n2) LLM finetuning benefits more from LLM model scaling than pretraining data\nscaling, and PET parameter scaling is generally ineffective; and 3) the optimal\nfinetuning method is highly task- and finetuning data-dependent. We hope our\nfindings could shed light on understanding, selecting and developing LLM\nfinetuning methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.17193v1.pdf"
    },
    {
        "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",
        "authors": [
            "Yixin Liu",
            "Kai Zhang",
            "Yuan Li",
            "Zhiling Yan",
            "Chujie Gao",
            "Ruoxi Chen",
            "Zhengqing Yuan",
            "Yue Huang",
            "Hanchi Sun",
            "Jianfeng Gao",
            "Lifang He",
            "Lichao Sun"
        ],
        "published": "2024-02-27T03:30:58Z",
        "summary": "Sora is a text-to-video generative AI model, released by OpenAI in February\n2024. The model is trained to generate videos of realistic or imaginative\nscenes from text instructions and show potential in simulating the physical\nworld. Based on public technical reports and reverse engineering, this paper\npresents a comprehensive review of the model's background, related\ntechnologies, applications, remaining challenges, and future directions of\ntext-to-video AI models. We first trace Sora's development and investigate the\nunderlying technologies used to build this \"world simulator\". Then, we describe\nin detail the applications and potential impact of Sora in multiple industries\nranging from film-making and education to marketing. We discuss the main\nchallenges and limitations that need to be addressed to widely deploy Sora,\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\nfuture development of Sora and video generation models in general, and how\nadvancements in the field could enable new ways of human-AI interaction,\nboosting productivity and creativity of video generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.17177v2.pdf"
    },
    {
        "title": "Benchmarking Data Science Agents",
        "authors": [
            "Yuge Zhang",
            "Qiyang Jiang",
            "Xingyu Han",
            "Nan Chen",
            "Yuqing Yang",
            "Kan Ren"
        ],
        "published": "2024-02-27T03:03:06Z",
        "summary": "In the era of data-driven decision-making, the complexity of data analysis\nnecessitates advanced expertise and tools of data science, presenting\nsignificant challenges even for specialists. Large Language Models (LLMs) have\nemerged as promising aids as data science agents, assisting humans in data\nanalysis and processing. Yet their practical efficacy remains constrained by\nthe varied demands of real-world applications and complicated analytical\nprocess. In this paper, we introduce DSEval -- a novel evaluation paradigm, as\nwell as a series of innovative benchmarks tailored for assessing the\nperformance of these agents throughout the entire data science lifecycle.\nIncorporating a novel bootstrapped annotation method, we streamline dataset\npreparation, improve the evaluation coverage, and expand benchmarking\ncomprehensiveness. Our findings uncover prevalent obstacles and provide\ncritical insights to inform future advancements in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.17168v1.pdf"
    },
    {
        "title": "Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation",
        "authors": [
            "Yuankai Fan",
            "Zhenying He",
            "Tonghui Ren",
            "Can Huang",
            "Yinan Jing",
            "Kai Zhang",
            "X. Sean Wang"
        ],
        "published": "2024-02-27T02:16:07Z",
        "summary": "The Natural Language Interface to Databases (NLIDB) empowers non-technical\nusers with database access through intuitive natural language (NL)\ninteractions. Advanced approaches, utilizing neural sequence-to-sequence models\nor large-scale language models, typically employ auto-regressive decoding to\ngenerate unique SQL queries sequentially. While these translation models have\ngreatly improved the overall translation accuracy, surpassing 70% on NLIDB\nbenchmarks, the use of auto-regressive decoding to generate single SQL queries\nmay result in sub-optimal outputs, potentially leading to erroneous\ntranslations. In this paper, we propose Metasql, a unified generate-then-rank\nframework that can be flexibly incorporated with existing NLIDBs to\nconsistently improve their translation accuracy. Metasql introduces query\nmetadata to control the generation of better SQL query candidates and uses\nlearning-to-rank algorithms to retrieve globally optimized queries.\nSpecifically, Metasql first breaks down the meaning of the given NL query into\na set of possible query metadata, representing the basic concepts of the\nsemantics. These metadata are then used as language constraints to steer the\nunderlying translation model toward generating a set of candidate SQL queries.\nFinally, Metasql ranks the candidates to identify the best matching one for the\ngiven NL query. Extensive experiments are performed to study Metasql on two\npublic NLIDB benchmarks. The results show that the performance of the\ntranslation models can be effectively improved using Metasql.",
        "pdf_link": "https://arxiv.org/pdf/2402.17144v1.pdf"
    },
    {
        "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
        "authors": [
            "Xinran Zhao",
            "Hongming Zhang",
            "Xiaoman Pan",
            "Wenlin Yao",
            "Dong Yu",
            "Tongshuang Wu",
            "Jianshu Chen"
        ],
        "published": "2024-02-27T01:37:23Z",
        "summary": "For a LLM to be trustworthy, its confidence level should be well-calibrated\nwith its actual performance. While it is now common sense that LLM performances\nare greatly impacted by prompts, the confidence calibration in prompting LLMs\nhas yet to be thoroughly explored. In this paper, we explore how different\nprompting strategies influence LLM confidence calibration and how it could be\nimproved. We conduct extensive experiments on six prompting methods in the\nquestion-answering context and we observe that, while these methods help\nimprove the expected LLM calibration, they also trigger LLMs to be\nover-confident when responding to some instances. Inspired by human cognition,\nwe propose Fact-and-Reflection (FaR) prompting, which improves the LLM\ncalibration in two steps. First, FaR elicits the known \"facts\" that are\nrelevant to the input prompt from the LLM. And then it asks the model to\n\"reflect\" over them to generate the final answer. Experiments show that FaR\nprompting achieves significantly better calibration; it lowers the Expected\nCalibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR\nprompting even elicits the capability of verbally expressing concerns in less\nconfident scenarios, which helps trigger retrieval augmentation for solving\nthese harder instances.",
        "pdf_link": "https://arxiv.org/pdf/2402.17124v1.pdf"
    },
    {
        "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
        "authors": [
            "Kaige Xie",
            "Mark Riedl"
        ],
        "published": "2024-02-27T01:25:52Z",
        "summary": "Automated story generation has been one of the long-standing challenges in\nNLP. Among all dimensions of stories, suspense is very common in human-written\nstories but relatively under-explored in AI-generated stories. While recent\nadvances in large language models (LLMs) have greatly promoted language\ngeneration in general, state-of-the-art LLMs are still unreliable when it comes\nto suspenseful story generation. We propose a novel iterative-prompting-based\nplanning method that is grounded in two theoretical foundations of story\nsuspense from cognitive psychology and narratology. This theory-grounded method\nworks in a fully zero-shot manner and does not rely on any supervised story\ncorpora. To the best of our knowledge, this paper is the first attempt at\nsuspenseful story generation with LLMs. Extensive human evaluations of the\ngenerated suspenseful stories demonstrate the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2402.17119v1.pdf"
    },
    {
        "title": "Sinkhorn Distance Minimization for Knowledge Distillation",
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Yuting Gao",
            "Enwei Zhang",
            "Zihan Xu",
            "Tong Wu",
            "Ke Li",
            "Xing Sun",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "published": "2024-02-27T01:13:58Z",
        "summary": "Knowledge distillation (KD) has been widely adopted to compress large\nlanguage models (LLMs). Existing KD methods investigate various divergence\nmeasures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL),\nand Jensen-Shannon (JS) divergences. However, due to limitations inherent in\ntheir assumptions and definitions, these measures fail to deliver effective\nsupervision when few distribution overlap exists between the teacher and the\nstudent. In this paper, we show that the aforementioned KL, RKL, and JS\ndivergences respectively suffer from issues of mode-averaging, mode-collapsing,\nand mode-underestimation, which deteriorates logits-based KD for diverse NLP\ntasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the\nSinkhorn distance to ensure a nuanced and precise assessment of the disparity\nbetween teacher and student distributions. Besides, profit by properties of the\nSinkhorn metric, we can get rid of sample-wise KD that restricts the perception\nof divergence in each teacher-student sample pair. Instead, we propose a\nbatch-wise reformulation to capture geometric intricacies of distributions\nacross samples in the high-dimensional space. Comprehensive evaluation on GLUE\nand SuperGLUE, in terms of comparability, validity, and generalizability,\nhighlights our superiority over state-of-the-art methods on all kinds of LLMs\nwith encoder-only, encoder-decoder, and decoder-only architectures.",
        "pdf_link": "https://arxiv.org/pdf/2402.17110v1.pdf"
    },
    {
        "title": "Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses",
        "authors": [
            "Juyeon Kim",
            "Jeongeun Lee",
            "Yoonho Chang",
            "Chanyeol Choi",
            "Junseong Kim",
            "Jy-yong Sohn"
        ],
        "published": "2024-02-27T00:22:18Z",
        "summary": "Mitigating hallucination issues is one of the main challenges of LLMs we need\nto overcome, in order to reliably use them in real-world scenarios. Recently,\nvarious methods are proposed to check the factual errors in the LLM-generated\ntexts and revise them accordingly, to reduce the hallucination issue. In this\npaper, we propose Re-Ex, a method of revising LLM-generated texts, which\nintroduces a novel step dubbed as the factual error explanation step. Re-Ex\nrevises the initial response of LLMs using 3-steps: first, external tools are\nused to get the evidences on the factual errors in the response; second, LLMs\nare instructed to explain the problematic parts of the response based on the\nevidences gathered in the first step; finally, LLMs revise the response using\nthe explanation obtained in the second step. In addition to the explanation\nstep, we propose new prompting techniques to reduce the amount of tokens and\nwall-clock time required for the response revision process. Compared with\nexisting methods including Factool, CoVE, and RARR, Re-Ex provides better\nrevision performance with less time and fewer tokens in multiple benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17097v1.pdf"
    },
    {
        "title": "Adapting to Teammates in a Cooperative Language Game",
        "authors": [
            "Christopher Archibald",
            "Spencer Brosnahan"
        ],
        "published": "2024-02-26T23:15:07Z",
        "summary": "The game of Codenames has recently emerged as a domain of interest for\nintelligent agent design. The game is unique due to the way that language and\ncoordination between teammates play important roles. Previous approaches to\ndesigning agents for this game have utilized a single internal language model\nto determine action choices. This often leads to good performance with some\nteammates and inferior performance with other teammates, as the agent cannot\nadapt to any specific teammate. In this paper we present the first adaptive\nagent for playing Codenames. We adopt an ensemble approach with the goal of\ndetermining, during the course of interacting with a specific teammate, which\nof our internal expert agents, each potentially with its own language model, is\nthe best match. One difficulty faced in this approach is the lack of a single\nnumerical metric that accurately captures the performance of a Codenames team.\nPrior Codenames research has utilized a handful of different metrics to\nevaluate agent teams. We propose a novel single metric to evaluate the\nperformance of a Codenames team, whether playing a single team (solitaire)\ngame, or a competitive game against another team. We then present and analyze\nan ensemble agent which selects an internal expert on each turn in order to\nmaximize this proposed metric. Experimental analysis shows that this ensemble\napproach adapts to individual teammates and often performs nearly as well as\nthe best internal expert with a teammate. Crucially, this success does not\ndepend on any previous knowledge about the teammates, the ensemble agents, or\ntheir compatibility. This research represents an important step to making\nlanguage-based agents for cooperative language settings like Codenames more\nadaptable to individual teammates.",
        "pdf_link": "https://arxiv.org/pdf/2403.00823v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling",
        "authors": [
            "Hang Jiang",
            "Xiajie Zhang",
            "Robert Mahari",
            "Daniel Kessler",
            "Eric Ma",
            "Tal August",
            "Irene Li",
            "Alex 'Sandy' Pentland",
            "Yoon Kim",
            "Jad Kabbara",
            "Deb Roy"
        ],
        "published": "2024-02-26T20:56:06Z",
        "summary": "Making legal knowledge accessible to non-experts is crucial for enhancing\ngeneral legal literacy and encouraging civic participation in democracy.\nHowever, legal documents are often challenging to understand for people without\nlegal backgrounds. In this paper, we present a novel application of large\nlanguage models (LLMs) in legal education to help non-experts learn intricate\nlegal concepts through storytelling, an effective pedagogical tool in conveying\ncomplex and abstract concepts. We also introduce a new dataset LegalStories,\nwhich consists of 295 complex legal doctrines, each accompanied by a story and\na set of multiple-choice questions generated by LLMs. To construct the dataset,\nwe experiment with various LLMs to generate legal stories explaining these\nconcepts. Furthermore, we use an expert-in-the-loop method to iteratively\ndesign multiple-choice questions. Then, we evaluate the effectiveness of\nstorytelling with LLMs through an RCT experiment with legal novices on 10\nsamples from the dataset. We find that LLM-generated stories enhance\ncomprehension of legal concepts and interest in law among non-native speakers\ncompared to only definitions. Moreover, stories consistently help participants\nrelate legal concepts to their lives. Finally, we find that learning with\nstories shows a higher retention rate for non-native speakers in the follow-up\nassessment. Our work has strong implications for using LLMs in promoting\nteaching and learning in the legal field and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2402.17019v1.pdf"
    },
    {
        "title": "Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset",
        "authors": [
            "Santosh T. Y. S. S",
            "Nina Baumgartner",
            "Matthias St\u00fcrmer",
            "Matthias Grabmair",
            "Joel Niklaus"
        ],
        "published": "2024-02-26T20:42:40Z",
        "summary": "The assessment of explainability in Legal Judgement Prediction (LJP) systems\nis of paramount importance in building trustworthy and transparent systems,\nparticularly considering the reliance of these systems on factors that may lack\nlegal relevance or involve sensitive attributes. This study delves into the\nrealm of explainability and fairness in LJP models, utilizing Swiss Judgement\nPrediction (SJP), the only available multilingual LJP dataset. We curate a\ncomprehensive collection of rationales that `support' and `oppose' judgement\nfrom legal experts for 108 cases in German, French, and Italian. By employing\nan occlusion-based explainability approach, we evaluate the explainability\nperformance of state-of-the-art monolingual and multilingual BERT-based LJP\nmodels, as well as models developed with techniques such as data augmentation\nand cross-lingual transfer, which demonstrated prediction performance\nimprovement. Notably, our findings reveal that improved prediction performance\ndoes not necessarily correspond to enhanced explainability performance,\nunderscoring the significance of evaluating models from an explainability\nperspective. Additionally, we introduce a novel evaluation framework, Lower\nCourt Insertion (LCI), which allows us to quantify the influence of lower court\ninformation on model predictions, exposing current models' biases.",
        "pdf_link": "https://arxiv.org/pdf/2402.17013v1.pdf"
    },
    {
        "title": "Pandora's White-Box: Increased Training Data Leakage in Open LLMs",
        "authors": [
            "Jeffrey G. Wang",
            "Jason Wang",
            "Marvin Li",
            "Seth Neel"
        ],
        "published": "2024-02-26T20:41:50Z",
        "summary": "In this paper we undertake a systematic study of privacy attacks against open\nsource Large Language Models (LLMs), where an adversary has access to either\nthe model weights, gradients, or losses, and tries to exploit them to learn\nsomething about the underlying training data. Our headline results are the\nfirst membership inference attacks (MIAs) against pre-trained LLMs that are\nable to simultaneously achieve high TPRs and low FPRs, and a pipeline showing\nthat over $50\\%$ (!) of the fine-tuning dataset can be extracted from a\nfine-tuned LLM in natural settings. We consider varying degrees of access to\nthe underlying model, customization of the language model, and resources\navailable to the attacker. In the pre-trained setting, we propose three new\nwhite-box MIAs: an attack based on the gradient norm, a supervised neural\nnetwork classifier, and a single step loss ratio attack. All outperform\nexisting black-box baselines, and our supervised attack closes the gap between\nMIA attack success against LLMs and other types of models. In fine-tuning, we\nfind that given access to the loss of the fine-tuned and base models, a\nfine-tuned loss ratio attack FLoRA is able to achieve near perfect MIA\npeformance. We then leverage these MIAs to extract fine-tuning data from\nfine-tuned language models. We find that the pipeline of generating from\nfine-tuned models prompted with a small snippet of the prefix of each training\nexample, followed by using FLoRa to select the most likely training sample,\nsucceeds the majority of the fine-tuning dataset after only $3$ epochs of\nfine-tuning. Taken together, these findings show that highly effective MIAs are\navailable in almost all LLM training settings, and highlight that great care\nmust be taken before LLMs are fine-tuned on highly sensitive data and then\ndeployed.",
        "pdf_link": "https://arxiv.org/pdf/2402.17012v1.pdf"
    },
    {
        "title": "Can Large Language Models Recall Reference Location Like Humans?",
        "authors": [
            "Ye Wang",
            "Xinrun Xu",
            "Rui Xie",
            "Wenxin Hu",
            "Wei Ye"
        ],
        "published": "2024-02-26T20:35:32Z",
        "summary": "When completing knowledge-intensive tasks, humans sometimes need not just an\nanswer but also a corresponding reference passage for auxiliary reading.\nPrevious methods required obtaining pre-segmented article chunks through\nadditional retrieval models. This paper explores leveraging the parameterized\nknowledge stored during the pre-training phase of large language models (LLMs)\nto independently recall reference passage from any starting position. We\npropose a two-stage framework that simulates the scenario of humans recalling\neasily forgotten references. Initially, the LLM is prompted to recall document\ntitle identifiers to obtain a coarse-grained document set. Then, based on the\nacquired coarse-grained document set, it recalls fine-grained passage. In the\ntwo-stage recall process, we use constrained decoding to ensure that content\noutside of the stored documents is not generated. To increase speed, we only\nrecall a short prefix in the second stage, then locate its position to retrieve\na complete passage. Experiments on KILT knowledge-sensitive tasks have verified\nthat LLMs can independently recall reference passage location in various task\nforms, and the obtained reference significantly assist downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17010v1.pdf"
    },
    {
        "title": "Benchmarking LLMs on the Semantic Overlap Summarization Task",
        "authors": [
            "John Salvador",
            "Naman Bansal",
            "Mousumi Akter",
            "Souvika Sarkar",
            "Anupam Das",
            "Shubhra Kanti Karmaker"
        ],
        "published": "2024-02-26T20:33:50Z",
        "summary": "Semantic Overlap Summarization (SOS) is a constrained multi-document\nsummarization task, where the constraint is to capture the common/overlapping\ninformation between two alternative narratives. While recent advancements in\nLarge Language Models (LLMs) have achieved superior performance in numerous\nsummarization tasks, a benchmarking study of the SOS task using LLMs is yet to\nbe performed. As LLMs' responses are sensitive to slight variations in prompt\ndesign, a major challenge in conducting such a benchmarking study is to\nsystematically explore a variety of prompts before drawing a reliable\nconclusion. Fortunately, very recently, the TELeR taxonomy has been proposed\nwhich can be used to design and explore various prompts for LLMs. Using this\nTELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs\non the SOS Task, assessing their ability to summarize overlapping information\nfrom multiple alternative narratives. For evaluation, we report\nwell-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different\ndatasets of alternative narratives. We conclude the paper by analyzing the\nstrengths and limitations of various LLMs in terms of their capabilities in\ncapturing overlapping information The code and datasets used to conduct this\nstudy are available at https://anonymous.4open.science/r/llm_eval-E16D.",
        "pdf_link": "https://arxiv.org/pdf/2402.17008v1.pdf"
    },
    {
        "title": "Algorithmic Arbitrariness in Content Moderation",
        "authors": [
            "Juan Felipe Gomez",
            "Caio Vieira Machado",
            "Lucas Monteiro Paes",
            "Flavio P. Calmon"
        ],
        "published": "2024-02-26T19:27:00Z",
        "summary": "Machine learning (ML) is widely used to moderate online content. Despite its\nscalability relative to human moderation, the use of ML introduces unique\nchallenges to content moderation. One such challenge is predictive\nmultiplicity: multiple competing models for content classification may perform\nequally well on average, yet assign conflicting predictions to the same\ncontent. This multiplicity can result from seemingly innocuous choices during\nmodel development, such as random seed selection for parameter initialization.\nWe experimentally demonstrate how content moderation tools can arbitrarily\nclassify samples as toxic, leading to arbitrary restrictions on speech. We\ndiscuss these findings in terms of human rights set out by the International\nCovenant on Civil and Political Rights (ICCPR), namely freedom of expression,\nnon-discrimination, and procedural justice. We analyze (i) the extent of\npredictive multiplicity among state-of-the-art LLMs used for detecting toxic\ncontent; (ii) the disparate impact of this arbitrariness across social groups;\nand (iii) how model multiplicity compares to unambiguous human classifications.\nOur findings indicate that the up-scaled algorithmic moderation risks\nlegitimizing an algorithmic leviathan, where an algorithm disproportionately\nmanages human rights. To mitigate such risks, our study underscores the need to\nidentify and increase the transparency of arbitrariness in content moderation\napplications. Since algorithmic content moderation is being fueled by pressing\nsocial concerns, such as disinformation and hate speech, our discussion on\nharms raises concerns relevant to policy debates. Our findings also contribute\nto content moderation and intermediary liability laws being discussed and\npassed in many countries, such as the Digital Services Act in the European\nUnion, the Online Safety Act in the United Kingdom, and the Fake News Bill in\nBrazil.",
        "pdf_link": "https://arxiv.org/pdf/2402.16979v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models in Cybersecurity",
        "authors": [
            "Gabriel de Jesus Coelho da Silva",
            "Carlos Becker Westphall"
        ],
        "published": "2024-02-26T19:06:02Z",
        "summary": "Large Language Models (LLMs) have quickly risen to prominence due to their\nability to perform at or close to the state-of-the-art in a variety of fields\nwhile handling natural language. An important field of research is the\napplication of such models at the cybersecurity context. This survey aims to\nidentify where in the field of cybersecurity LLMs have already been applied,\nthe ways in which they are being used and their limitations in the field.\nFinally, suggestions are made on how to improve such limitations and what can\nbe expected from these systems once these limitations are overcome.",
        "pdf_link": "https://arxiv.org/pdf/2402.16968v1.pdf"
    },
    {
        "title": "WIPI: A New Web Threat for LLM-Driven Web Agents",
        "authors": [
            "Fangzhou Wu",
            "Shutong Wu",
            "Yulong Cao",
            "Chaowei Xiao"
        ],
        "published": "2024-02-26T19:01:54Z",
        "summary": "With the fast development of large language models (LLMs), LLM-driven Web\nAgents (Web Agents for short) have obtained tons of attention due to their\nsuperior capability where LLMs serve as the core part of making decisions like\nthe human brain equipped with multiple web tools to actively interact with\nexternal deployed websites. As uncountable Web Agents have been released and\nsuch LLM systems are experiencing rapid development and drawing closer to\nwidespread deployment in our daily lives, an essential and pressing question\narises: \"Are these Web Agents secure?\". In this paper, we introduce a novel\nthreat, WIPI, that indirectly controls Web Agent to execute malicious\ninstructions embedded in publicly accessible webpages. To launch a successful\nWIPI works in a black-box environment. This methodology focuses on the form and\ncontent of indirect instructions within external webpages, enhancing the\nefficiency and stealthiness of the attack. To evaluate the effectiveness of the\nproposed methodology, we conducted extensive experiments using 7 plugin-based\nChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The\nresults reveal that our methodology achieves an average attack success rate\n(ASR) exceeding 90% even in pure black-box scenarios. Moreover, through an\nablation study examining various user prefix instructions, we demonstrated that\nthe WIPI exhibits strong robustness, maintaining high performance across\ndiverse prefix instructions.",
        "pdf_link": "https://arxiv.org/pdf/2402.16965v1.pdf"
    },
    {
        "title": "Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding",
        "authors": [
            "Benjamin Bergner",
            "Andrii Skliar",
            "Amelie Royer",
            "Tijmen Blankevoort",
            "Yuki Asano",
            "Babak Ehteshami Bejnordi"
        ],
        "published": "2024-02-26T18:59:28Z",
        "summary": "Large language models (LLMs) have become ubiquitous in practice and are\nwidely used for generation tasks such as translation, summarization and\ninstruction following. However, their enormous size and reliance on\nautoregressive decoding increase deployment costs and complicate their use in\nlatency-critical applications. In this work, we propose a hybrid approach that\ncombines language models of different sizes to increase the efficiency of\nautoregressive decoding while maintaining high performance. Our method utilizes\na pretrained frozen LLM that encodes all prompt tokens once in parallel, and\nuses the resulting representations to condition and guide a small language\nmodel (SLM), which then generates the response more efficiently. We investigate\nthe combination of encoder-decoder LLMs with both encoder-decoder and\ndecoder-only SLMs from different model families and only require fine-tuning of\nthe SLM. Experiments with various benchmarks show substantial speedups of up to\n$4\\times$, with minor performance penalties of $1-2\\%$ for translation and\nsummarization tasks compared to the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.16844v1.pdf"
    },
    {
        "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
        "authors": [
            "Omkar Thawakar",
            "Ashmal Vayani",
            "Salman Khan",
            "Hisham Cholakal",
            "Rao M. Anwer",
            "Michael Felsberg",
            "Tim Baldwin",
            "Eric P. Xing",
            "Fahad Shahbaz Khan"
        ],
        "published": "2024-02-26T18:59:03Z",
        "summary": "\"Bigger the better\" has been the predominant trend in recent Large Language\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\nrequire on-device processing, energy efficiency, low memory footprint, and\nresponse efficiency. These requisites are crucial for privacy, security, and\nsustainable deployment. This paper explores the \"less is more\" paradigm by\naddressing the challenge of designing accurate yet efficient Small Language\nModels (SLMs) for resource constrained devices. Our primary contribution is the\nintroduction of an accurate and fully transparent open-source 0.5 billion\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\nresource-constrained computing with an emphasis on enhanced performance with\nreduced resource demands. MobiLlama is a SLM design that initiates from a\nlarger model and applies a careful parameter sharing scheme to reduce both the\npre-training and the deployment cost. Our work strives to not only bridge the\ngap in open-source SLMs but also ensures full transparency, where complete\ntraining data pipeline, training code, model weights, and over 300 checkpoints\nalong with evaluation codes is available at :\nhttps://github.com/mbzuai-oryx/MobiLlama.",
        "pdf_link": "https://arxiv.org/pdf/2402.16840v1.pdf"
    },
    {
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
        "authors": [
            "Sohee Yang",
            "Elena Gribovskaya",
            "Nora Kassner",
            "Mor Geva",
            "Sebastian Riedel"
        ],
        "published": "2024-02-26T18:57:54Z",
        "summary": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16837v1.pdf"
    },
    {
        "title": "Eight Methods to Evaluate Robust Unlearning in LLMs",
        "authors": [
            "Aengus Lynch",
            "Phillip Guo",
            "Aidan Ewart",
            "Stephen Casper",
            "Dylan Hadfield-Menell"
        ],
        "published": "2024-02-26T18:57:37Z",
        "summary": "Machine unlearning can be useful for removing harmful capabilities and\nmemorized text from large language models (LLMs), but there are not yet\nstandardized methods for rigorously evaluating it. In this paper, we first\nsurvey techniques and limitations of existing unlearning evaluations. Second,\nwe apply a comprehensive set of tests for the robustness and competitiveness of\nunlearning in the \"Who's Harry Potter\" (WHP) model from Eldan and Russinovich\n(2023). While WHP's unlearning generalizes well when evaluated with the\n\"Familiarity\" metric from Eldan and Russinovich, we find i)\nhigher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP\nperforms on par with the original model on Harry Potter Q&A tasks, iii) it\nrepresents latent knowledge comparably to the original model, and iv) there is\ncollateral unlearning in related domains. Overall, our results highlight the\nimportance of comprehensive unlearning evaluation that avoids ad-hoc metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.16835v1.pdf"
    },
    {
        "title": "Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections",
        "authors": [
            "Gaurav Verma",
            "Minje Choi",
            "Kartik Sharma",
            "Jamelle Watson-Daniels",
            "Sejoon Oh",
            "Srijan Kumar"
        ],
        "published": "2024-02-26T18:56:48Z",
        "summary": "Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable\ngeneral-purpose conversations about images with the language modality. As\noff-the-shelf MLLMs may have limited capabilities on images from domains like\ndermatology and agriculture, they must be fine-tuned to unlock domain-specific\napplications. The prevalent architecture of current open-source MLLMs comprises\ntwo major modules: an image-language (cross-modal) projection network and a\nlarge language model. It is desirable to understand the roles of these two\nmodules in modeling domain-specific visual attributes to inform the design of\nfuture models and streamline the interpretability efforts on the current\nmodels. To this end, via experiments on 4 datasets and under 2 fine-tuning\nsettings, we find that as the MLLM is fine-tuned, it indeed gains\ndomain-specific visual capabilities, but the updates do not lead to the\nprojection extracting relevant domain-specific visual attributes. Our results\nindicate that the domain-specific visual attributes are modeled by the LLM,\neven when only the projection is fine-tuned. Through this study, we offer a\npotential reinterpretation of the role of cross-modal projections in MLLM\narchitectures. Projection webpage:\nhttps://claws-lab.github.io/projection-in-MLLMs/",
        "pdf_link": "https://arxiv.org/pdf/2402.16832v1.pdf"
    },
    {
        "title": "A Survey on Data Selection for Language Models",
        "authors": [
            "Alon Albalak",
            "Yanai Elazar",
            "Sang Michael Xie",
            "Shayne Longpre",
            "Nathan Lambert",
            "Xinyi Wang",
            "Niklas Muennighoff",
            "Bairu Hou",
            "Liangming Pan",
            "Haewon Jeong",
            "Colin Raffel",
            "Shiyu Chang",
            "Tatsunori Hashimoto",
            "William Yang Wang"
        ],
        "published": "2024-02-26T18:54:35Z",
        "summary": "A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.16827v2.pdf"
    },
    {
        "title": "Language Agents as Optimizable Graphs",
        "authors": [
            "Mingchen Zhuge",
            "Wenyi Wang",
            "Louis Kirsch",
            "Francesco Faccio",
            "Dmitrii Khizbullin",
            "J\u00fcrgen Schmidhuber"
        ],
        "published": "2024-02-26T18:48:27Z",
        "summary": "Various human-designed prompt engineering techniques have been proposed to\nimprove problem solvers based on Large Language Models (LLMs), yielding many\ndisparate code bases. We unify these approaches by describing LLM-based agents\nas computational graphs. The nodes implement functions to process multimodal\ndata or query LLMs, and the edges describe the information flow between\noperations. Graphs can be recursively combined into larger composite graphs\nrepresenting hierarchies of inter-agent collaboration (where edges connect\noperations of different agents). Our novel automatic graph optimizers (1)\nrefine node-level LLM prompts (node optimization) and (2) improve agent\norchestration by changing graph connectivity (edge optimization). Experiments\ndemonstrate that our framework can be used to efficiently develop, integrate,\nand automatically improve various LLM agents. The code can be found at\nhttps://github.com/metauto-ai/gptswarm.",
        "pdf_link": "https://arxiv.org/pdf/2402.16823v2.pdf"
    },
    {
        "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
        "authors": [
            "Mikayel Samvelyan",
            "Sharath Chandra Raparthy",
            "Andrei Lupu",
            "Eric Hambro",
            "Aram H. Markosyan",
            "Manish Bhatt",
            "Yuning Mao",
            "Minqi Jiang",
            "Jack Parker-Holder",
            "Jakob Foerster",
            "Tim Rockt\u00e4schel",
            "Roberta Raileanu"
        ],
        "published": "2024-02-26T18:47:27Z",
        "summary": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to user\ninputs is of paramount importance. Existing methods for identifying adversarial\nprompts tend to focus on specific domains, lack diversity, or require extensive\nhuman annotations. To address these limitations, we present Rainbow Teaming, a\nnovel approach for producing a diverse collection of adversarial prompts.\nRainbow Teaming casts adversarial prompt generation as a quality-diversity\nproblem, and uses open-ended search to generate prompts that are both effective\nand diverse. It can uncover a model's vulnerabilities across a broad range of\ndomains including, in this paper, safety, question answering, and\ncybersecurity. We also demonstrate that fine-tuning on synthetic data generated\nby Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting\ntheir general capabilities and helpfulness, paving the path to open-ended\nself-improvement.",
        "pdf_link": "https://arxiv.org/pdf/2402.16822v1.pdf"
    },
    {
        "title": "A Surprising Failure? Multimodal LLMs and the NLVR Challenge",
        "authors": [
            "Anne Wu",
            "Kiant\u00e9 Brantley",
            "Yoav Artzi"
        ],
        "published": "2024-02-26T18:37:18Z",
        "summary": "This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and\nthe open-source model IDEFICS -- on the compositional natural language vision\nreasoning task NLVR. Given a human-written sentence paired with a synthetic\nimage, this task requires the model to determine the truth value of the\nsentence with respect to the image. Despite the strong performance demonstrated\nby these models, we observe they perform poorly on NLVR, which was constructed\nto require compositional and spatial reasoning, and to be robust for semantic\nand systematic biases.",
        "pdf_link": "https://arxiv.org/pdf/2402.17793v1.pdf"
    },
    {
        "title": "OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)",
        "authors": [
            "Fujian Jia",
            "Xin Liu",
            "Lixi Deng",
            "Jiwen Gu",
            "Chunchao Pu",
            "Tunan Bai",
            "Mengjiang Huang",
            "Yuanzhi Lu",
            "Kang Liu"
        ],
        "published": "2024-02-26T18:33:13Z",
        "summary": "In the past year, there has been a growing trend in applying Large Language\nModels (LLMs) to the field of medicine, particularly with the advent of\nadvanced language models such as ChatGPT developed by OpenAI. However, there is\nlimited research on LLMs specifically addressing oncology-related queries. The\nprimary aim of this research was to develop a specialized language model that\ndemonstrates improved accuracy in providing advice related to oncology. We\nperformed an extensive data collection of online question-answer interactions\ncentered around oncology, sourced from reputable doctor-patient platforms.\nFollowing data cleaning and anonymization, a dataset comprising over 180K+\noncology-related conversations was established. The conversations were\ncategorized and meticulously reviewed by field specialists and clinicians to\nensure precision. Employing the LLaMA model and other selected open-source\ndatasets, we conducted iterative fine-tuning to enhance the model's proficiency\nin basic medical conversation and specialized oncology knowledge. We observed a\nsubstantial enhancement in the model's understanding of genuine patient\ninquiries and its reliability in offering oncology-related advice through the\nutilization of real online question-answer interactions in the fine-tuning\nprocess. We release database and models to the research community\n(https://github.com/OncoGPT1).",
        "pdf_link": "https://arxiv.org/pdf/2402.16810v1.pdf"
    },
    {
        "title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4",
        "authors": [
            "Zeyu He",
            "Chieh-Yang Huang",
            "Chien-Kuang Cornelia Ding",
            "Shaurya Rohatgi",
            "Ting-Hao 'Kenneth' Huang"
        ],
        "published": "2024-02-26T18:08:52Z",
        "summary": "Recent studies indicated GPT-4 outperforms online crowd workers in data\nlabeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).\nHowever, these studies were criticized for deviating from standard\ncrowdsourcing practices and emphasizing individual workers' performances over\nthe whole data-annotation process. This paper compared GPT-4 and an ethical and\nwell-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments\nfrom 200 scholarly articles using the CODA-19 scheme. Two worker interfaces\nyielded 127,080 labels, which were then used to infer the final labels through\neight label-aggregation algorithms. Our evaluation showed that despite best\npractices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved\n83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected\nvia an advanced worker interface for aggregation, 2 out of the 8 algorithms\nachieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested\nthat, when the crowd's and GPT-4's labeling strengths are complementary,\naggregating them could increase labeling accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.16795v1.pdf"
    },
    {
        "title": "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models",
        "authors": [
            "Paul R\u00f6ttger",
            "Valentin Hofmann",
            "Valentina Pyatkin",
            "Musashi Hinck",
            "Hannah Rose Kirk",
            "Hinrich Sch\u00fctze",
            "Dirk Hovy"
        ],
        "published": "2024-02-26T18:00:49Z",
        "summary": "Much recent work seeks to evaluate values and opinions in large language\nmodels (LLMs) using multiple-choice surveys and questionnaires. Most of this\nwork is motivated by concerns around real-world LLM applications. For example,\npolitically-biased LLMs may subtly influence society when they are used by\nmillions of people. Such real-world concerns, however, stand in stark contrast\nto the artificiality of current evaluations: real users do not typically ask\nLLMs survey questions. Motivated by this discrepancy, we challenge the\nprevailing constrained evaluation paradigm for values and opinions in LLMs and\nexplore more realistic unconstrained evaluations. As a case study, we focus on\nthe popular Political Compass Test (PCT). In a systematic review, we find that\nmost prior work using the PCT forces models to comply with the PCT's\nmultiple-choice format. We show that models give substantively different\nanswers when not forced; that answers change depending on how models are\nforced; and that answers lack paraphrase robustness. Then, we demonstrate that\nmodels give different answers yet again in a more realistic open-ended answer\nsetting. We distill these findings into recommendations and open challenges in\nevaluating values and opinions in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16786v1.pdf"
    },
    {
        "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
        "authors": [
            "Renren Jin",
            "Jiangcun Du",
            "Wuwei Huang",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang",
            "Deyi Xiong"
        ],
        "published": "2024-02-26T17:45:36Z",
        "summary": "Increasing the number of parameters in large language models (LLMs) usually\nimproves performance in downstream tasks but raises compute and memory costs,\nmaking deployment difficult in resource-limited settings. Quantization\ntechniques, which reduce the bits needed for model weights or activations with\nminimal performance loss, have become popular due to the rise of LLMs. However,\nmost quantization studies use pre-trained LLMs, and the impact of quantization\non instruction-tuned LLMs and the relationship between perplexity and benchmark\nperformance of quantized LLMs are not well understood. Evaluation of quantized\nLLMs is often limited to language modeling and a few classification tasks,\nleaving their performance on other benchmarks unclear. To address these gaps,\nwe propose a structured evaluation framework consisting of three critical\ndimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and\nconduct extensive experiments across ten diverse benchmarks. Our experimental\nresults indicate that LLMs with 4-bit quantization can retain performance\ncomparable to their non-quantized counterparts, and perplexity can serve as a\nproxy metric for quantized LLMs on most benchmarks. Furthermore, quantized LLMs\nwith larger parameter scales can outperform smaller LLMs. Despite the memory\nsavings achieved through quantization, it can also slow down the inference\nspeed of LLMs. Consequently, substantial engineering efforts and hardware\nsupport are imperative to achieve a balanced optimization of decoding speed and\nmemory consumption in the context of quantized LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16775v1.pdf"
    },
    {
        "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
        "authors": [
            "Huijie Lv",
            "Xiao Wang",
            "Yuansen Zhang",
            "Caishuang Huang",
            "Shihan Dou",
            "Junjie Ye",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-26T16:35:59Z",
        "summary": "Adversarial misuse, particularly through `jailbreaking' that circumvents a\nmodel's safety and ethical protocols, poses a significant challenge for Large\nLanguage Models (LLMs). This paper delves into the mechanisms behind such\nsuccessful attacks, introducing a hypothesis for the safety mechanism of\naligned LLMs: intent security recognition followed by response generation.\nGrounded in this hypothesis, we propose CodeChameleon, a novel jailbreak\nframework based on personalized encryption tactics. To elude the intent\nsecurity recognition phase, we reformulate tasks into a code completion format,\nenabling users to encrypt queries using personalized encryption functions. To\nguarantee response generation functionality, we embed a decryption function\nwithin the instructions, which allows the LLM to decrypt and execute the\nencrypted queries successfully. We conduct extensive experiments on 7 LLMs,\nachieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our\nmethod achieves an 86.6\\% ASR on GPT-4-1106.",
        "pdf_link": "https://arxiv.org/pdf/2402.16717v1.pdf"
    },
    {
        "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
        "authors": [
            "Liangxin Liu",
            "Xuebo Liu",
            "Derek F. Wong",
            "Dongfang Li",
            "Ziyi Wang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "published": "2024-02-26T16:21:53Z",
        "summary": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs)\ntowards human-centric interactions. Recent advancements have shown that the\ncareful selection of a small, high-quality subset of IT data can significantly\nenhance the performance of LLMs. Despite this, common approaches often rely on\nadditional models or data sets, which increases costs and limits widespread\nadoption. In this work, we propose a novel approach, termed SelectIT, that\ncapitalizes on the foundational capabilities of the LLM itself. Specifically,\nwe exploit the intrinsic uncertainty present in LLMs to more effectively select\nhigh-quality IT data, without the need for extra resources. Furthermore, we\nintroduce a novel IT dataset, the Selective Alpaca, created by applying\nSelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT\nusing Selective Alpaca leads to substantial model ability enhancement. The\nrobustness of SelectIT has also been corroborated in various foundation models\nand domain-specific tasks. Our findings suggest that longer and more\ncomputationally intensive IT data may serve as superior sources of IT, offering\nvaluable insights for future research in this area. Data, code, and scripts are\nfreely available at https://github.com/Blue-Raincoat/SelectIT.",
        "pdf_link": "https://arxiv.org/pdf/2402.16705v1.pdf"
    },
    {
        "title": "Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models",
        "authors": [
            "Anchun Gui",
            "Jian Li",
            "Yong Dai",
            "Nan Du",
            "Han Xiao"
        ],
        "published": "2024-02-26T16:11:03Z",
        "summary": "Tool-augmented large language models (LLMs) are attracting widespread\nattention when accessing up-to-date knowledge and alleviating hallucination\nissues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated\nsurprising tool-usage capabilities through prompting and in-context learning\ntechniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in\nmanipulating tools, current efforts focus on either template-driven or\ntoken-triggered tool-usage. However, the former hampers LLMs' flexibility to\naddress diverse user's queries due to constrained tool interactions, while the\nlatter limits the generalizability when engaging with new tools, since\ntool-usage learning is based on task- and tool-specific datasets. To alleviate\nthese concerns, in this paper, we propose a decision-aware and generalizable\ntool-usage framework (DEER). Specifically, we first construct the tool-usage\nsamples with multiple decision branches via an automatic generation pipeline,\nthereby inspiring the decision-making awareness of LLMs under diverse\nscenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the\ngeneralizability of LLMs over unseen tools. Extensive experiments demonstrate\nthat our proposed DEER is effective and significantly outperforms baselines\nacross various datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.16696v2.pdf"
    },
    {
        "title": "HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization",
        "authors": [
            "Qiwei Peng",
            "Yekun Chai",
            "Xuhong Li"
        ],
        "published": "2024-02-26T16:09:00Z",
        "summary": "Large language models (LLMs) have made significant progress in generating\ncodes from textual prompts. However, existing benchmarks have mainly\nconcentrated on translating English prompts to multilingual codes or have been\nconstrained to very limited natural languages (NLs). These benchmarks have\noverlooked the vast landscape of massively multilingual NL to multilingual\ncode, leaving a critical gap in the evaluation of multilingual LLMs. In\nresponse, we introduce HumanEval-XL, a massively multilingual code generation\nbenchmark specifically crafted to address this deficiency. HumanEval-XL\nestablishes connections between 23 NLs and 12 programming languages (PLs), and\ncomprises of a collection of 22,080 prompts with an average of 8.33 test cases.\nBy ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a\ncomprehensive evaluation platform for multilingual LLMs, allowing the\nassessment of the understanding of different NLs. Our work serves as a\npioneering step towards filling the void in evaluating NL generalization in the\narea of multilingual code generation. We make our evaluation code and data\npublicly available at \\url{https://github.com/FloatAI/humaneval-xl}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16694v2.pdf"
    },
    {
        "title": "Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study",
        "authors": [
            "Adrien Bazoge",
            "Emmanuel Morin",
            "Beatrice Daille",
            "Pierre-Antoine Gourraud"
        ],
        "published": "2024-02-26T16:05:33Z",
        "summary": "Recently, pretrained language models based on BERT have been introduced for\nthe French biomedical domain. Although these models have achieved\nstate-of-the-art results on biomedical and clinical NLP tasks, they are\nconstrained by a limited input sequence length of 512 tokens, which poses\nchallenges when applied to clinical notes. In this paper, we present a\ncomparative study of three adaptation strategies for long-sequence models,\nleveraging the Longformer architecture. We conducted evaluations of these\nmodels on 16 downstream tasks spanning both biomedical and clinical domains.\nOur findings reveal that further pre-training an English clinical model with\nFrench biomedical texts can outperform both converting a French biomedical BERT\nto the Longformer architecture and pre-training a French biomedical Longformer\nfrom scratch. The results underscore that long-sequence French biomedical\nmodels improve performance across most downstream tasks regardless of sequence\nlength, but BERT based models remain the most efficient for named entity\nrecognition tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.16689v1.pdf"
    },
    {
        "title": "StructLM: Towards Building Generalist Models for Structured Knowledge Grounding",
        "authors": [
            "Alex Zhuang",
            "Ge Zhang",
            "Tianyu Zheng",
            "Xinrun Du",
            "Junjie Wang",
            "Weiming Ren",
            "Stephen W. Huang",
            "Jie Fu",
            "Xiang Yue",
            "Wenhu Chen"
        ],
        "published": "2024-02-26T15:47:01Z",
        "summary": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our\nStructLM series surpasses task-specific models on 14 out of 18 evaluated\ndatasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore,\nStructLM demonstrates strong generalization across 6 novel held-out SKG tasks,\noutperforming TableLlama by an average of 35\\% and Flan-UL2 20B by an average\nof 10\\%. Contrary to expectations, we observe that scaling model size offers\nmarginal benefits, with StructLM-34B showing only slight improvements over\nStructLM-7B. This suggests that structured knowledge grounding is still a\nchallenging task and requires more innovative design to push to a new level.",
        "pdf_link": "https://arxiv.org/pdf/2402.16671v3.pdf"
    },
    {
        "title": "ESG Sentiment Analysis: comparing human and language model performance including GPT",
        "authors": [
            "Karim Derrick"
        ],
        "published": "2024-02-26T15:22:30Z",
        "summary": "In this paper we explore the challenges of measuring sentiment in relation to\nEnvironmental, Social and Governance (ESG) social media. ESG has grown in\nimportance in recent years with a surge in interest from the financial sector\nand the performance of many businesses has become based in part on their ESG\nrelated reputations. The use of sentiment analysis to measure ESG related\nreputation has developed and with it interest in the use of machines to do so.\nThe era of digital media has created an explosion of new media sources, driven\nby the growth of social media platforms. This growing data environment has\nbecome an excellent source for behavioural insight studies across many\ndisciplines that includes politics, healthcare and market research. Our study\nseeks to compare human performance with the cutting edge in machine performance\nin the measurement of ESG related sentiment. To this end researchers classify\nthe sentiment of 150 tweets and a reliability measure is made. A gold standard\ndata set is then established based on the consensus of 3 researchers and this\ndata set is then used to measure the performance of different machine\napproaches: one based on the VADER dictionary approach to sentiment\nclassification and then multiple language model approaches, including Llama2,\nT5, Mistral, Mixtral, FINBERT, GPT3.5 and GPT4.",
        "pdf_link": "https://arxiv.org/pdf/2402.16650v1.pdf"
    },
    {
        "title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language",
        "authors": [
            "Ming Wang",
            "Yuanzhong Liu",
            "Xiaoming Zhang",
            "Songlian Li",
            "Yijie Huang",
            "Chi Zhang",
            "Daling Wang",
            "Shi Feng",
            "Jigang Li"
        ],
        "published": "2024-02-26T15:05:16Z",
        "summary": "LLMs have demonstrated commendable performance across diverse domains.\nNevertheless, formulating high-quality prompts to effectively instruct LLMs\nposes a challenge for non-AI experts. Existing research in prompt engineering\nsuggests somewhat fragmented optimization principles and designs empirically\ndependent prompt optimizers. Unfortunately, these endeavors lack a structured\ndesign template, incurring high learning costs and resulting in low\nreusability. Inspired by structured reusable programming languages, we propose\nLangGPT, a dual-layer prompt design framework as the programming language for\nLLMs. LangGPT has an easy-to-learn normative structure and provides an extended\nstructure for migration and reuse. Experiments illustrate that LangGPT\nsignificantly enhances the capacity of LLMs to produce responses of superior\nquality compared to baselines. Moreover, LangGPT has proven effective in\nguiding LLMs to generate high-quality prompts. We have built a community on\nLangGPT to facilitate the tuition and sharing of prompt design. We also\nanalyzed the ease of use and reusability of LangGPT through a community user\nsurvey.",
        "pdf_link": "https://arxiv.org/pdf/2402.16929v1.pdf"
    },
    {
        "title": "Long-Context Language Modeling with Parallel Context Encoding",
        "authors": [
            "Howard Yen",
            "Tianyu Gao",
            "Danqi Chen"
        ],
        "published": "2024-02-26T14:47:35Z",
        "summary": "Extending large language models (LLMs) to process longer inputs is crucial\nfor numerous applications. However, the considerable computational cost of\ntransformers, coupled with limited generalization of positional encoding,\nrestricts the size of their context window. We introduce Context Expansion with\nParallel Encoding (CEPE), a framework that can be applied to any existing\ndecoder-only LLMs to extend their context window. CEPE adopts a small encoder\nto process long inputs chunk by chunk and enables the frozen decoder to\nleverage additional contexts via cross-attention. CEPE is efficient,\ngeneralizable, and versatile: trained with 8K-token documents, CEPE extends the\ncontext window of LLAMA-2 to 128K tokens, offering 10x the throughput with only\n1/6 of the memory. CEPE yields strong performance on language modeling and\nin-context learning. CEPE also excels in retrieval-augmented applications,\nwhile existing long-context models degenerate with retrieved contexts. We\nfurther introduce a CEPE variant that can extend the context window of\ninstruction-tuned models with only unlabeled data, and showcase its\neffectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model\nthat can leverage very long context on downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.16617v1.pdf"
    },
    {
        "title": "Multi-Bit Distortion-Free Watermarking for Large Language Models",
        "authors": [
            "Massieh Kordi Boroujeny",
            "Ya Jiang",
            "Kai Zeng",
            "Brian Mark"
        ],
        "published": "2024-02-26T14:01:34Z",
        "summary": "Methods for watermarking large language models have been proposed that\ndistinguish AI-generated text from human-generated text by slightly altering\nthe model output distribution, but they also distort the quality of the text,\nexposing the watermark to adversarial detection. More recently, distortion-free\nwatermarking methods were proposed that require a secret key to detect the\nwatermark. The prior methods generally embed zero-bit watermarks that do not\nprovide additional information beyond tagging a text as being AI-generated. We\nextend an existing zero-bit distortion-free watermarking method by embedding\nmultiple bits of meta-information as part of the watermark. We also develop a\ncomputationally efficient decoder that extracts the embedded information from\nthe watermark with low bit error rate.",
        "pdf_link": "https://arxiv.org/pdf/2402.16578v1.pdf"
    },
    {
        "title": "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
        "authors": [
            "Yifu Gao",
            "Linbo Qiao",
            "Zhigang Kan",
            "Zhihua Wen",
            "Yongquan He",
            "Dongsheng Li"
        ],
        "published": "2024-02-26T13:47:09Z",
        "summary": "Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results demonstrate that our model outperforms state-of-the-art\nbaselines, even achieving 100\\% on the metrics for the simple question type.",
        "pdf_link": "https://arxiv.org/pdf/2402.16568v1.pdf"
    },
    {
        "title": "Aligning Large Language Models to a Domain-specific Graph Database",
        "authors": [
            "Yuanyuan Liang",
            "Keren Tan",
            "Tingyu Xie",
            "Wenbiao Tao",
            "Siyuan Wang",
            "Yunshi Lan",
            "Weining Qian"
        ],
        "published": "2024-02-26T13:46:51Z",
        "summary": "Graph Databases (Graph DB) are widely applied in various fields, including\nfinance, social networks, and medicine. However, translating Natural Language\n(NL) into the Graph Query Language (GQL), commonly known as NL2GQL, proves to\nbe challenging due to its inherent complexity and specialized nature. Some\napproaches have sought to utilize Large Language Models (LLMs) to address\nanalogous tasks like text2SQL. Nevertheless, when it comes to NL2GQL taskson a\nparticular domain, the absence of domain-specific NL-GQL data pairs makes it\ndifficult to establish alignment between LLMs and the graph DB. To address this\nchallenge, we propose a well-defined pipeline. Specifically, we utilize ChatGPT\nto create NL-GQL data pairs based on the given graph DB with self-instruct.\nThen, we use the created data to fine-tune LLMs, thereby achieving alignment\nbetween LLMs and the graph DB. Additionally, during inference, we propose a\nmethod that extracts relevant schema to the queried NL as the input context to\nguide LLMs for generating accurate GQLs.We evaluate our method on two\nconstructed datasets deriving from graph DBs in finance domain and medicine\ndomain, namely FinGQL and MediGQL. Experimental results demonstrate that our\nmethod significantly outperforms a set of baseline methods, with improvements\nof 5.90 and 6.36 absolute points on EM, and 6.00 and 7.09 absolute points on\nEX, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.16567v2.pdf"
    },
    {
        "title": "Retrieval Augmented Generation Systems: Automatic Dataset Creation, Evaluation and Boolean Agent Setup",
        "authors": [
            "Tristan Kenneweg",
            "Philip Kenneweg",
            "Barbara Hammer"
        ],
        "published": "2024-02-26T12:56:17Z",
        "summary": "Retrieval Augmented Generation (RAG) systems have seen huge popularity in\naugmenting Large-Language Model (LLM) outputs with domain specific and time\nsensitive data. Very recently a shift is happening from simple RAG setups that\nquery a vector database for additional information with every user input to\nmore sophisticated forms of RAG. However, different concrete approaches compete\non mostly anecdotal evidence at the moment. In this paper we present a rigorous\ndataset creation and evaluation workflow to quantitatively compare different\nRAG strategies. We use a dataset created this way for the development and\nevaluation of a boolean agent RAG setup: A system in which a LLM can decide\nwhether to query a vector database or not, thus saving tokens on questions that\ncan be answered with internal knowledge. We publish our code and generated\ndataset online.",
        "pdf_link": "https://arxiv.org/pdf/2403.00820v1.pdf"
    },
    {
        "title": "Integrating Large Language Models with Graphical Session-Based Recommendation",
        "authors": [
            "Naicheng Guo",
            "Hongwei Cheng",
            "Qianqiao Liang",
            "Linxun Chen",
            "Bing Han"
        ],
        "published": "2024-02-26T12:55:51Z",
        "summary": "With the rapid development of Large Language Models (LLMs), various\nexplorations have arisen to utilize LLMs capability of context understanding on\nrecommender systems. While pioneering strategies have primarily transformed\ntraditional recommendation tasks into challenges of natural language\ngeneration, there has been a relative scarcity of exploration in the domain of\nsession-based recommendation (SBR) due to its specificity. SBR has been\nprimarily dominated by Graph Neural Networks, which have achieved many\nsuccessful outcomes due to their ability to capture both the implicit and\nexplicit relationships between adjacent behaviors. The structural nature of\ngraphs contrasts with the essence of natural language, posing a significant\nadaptation gap for LLMs. In this paper, we introduce large language models with\ngraphical Session-Based recommendation, named LLMGR, an effective framework\nthat bridges the aforementioned gap by harmoniously integrating LLMs with Graph\nNeural Networks (GNNs) for SBR tasks. This integration seeks to leverage the\ncomplementary strengths of LLMs in natural language understanding and GNNs in\nrelational data processing, leading to a more powerful session-based\nrecommender system that can understand and recommend items within a session.\nMoreover, to endow the LLM with the capability to empower SBR tasks, we design\na series of prompts for both auxiliary and major instruction tuning tasks.\nThese prompts are crafted to assist the LLM in understanding graph-structured\ndata and align textual information with nodes, effectively translating nuanced\nuser interactions into a format that can be understood and utilized by LLM\narchitectures. Extensive experiments on three real-world datasets demonstrate\nthat LLMGR outperforms several competitive baselines, indicating its\neffectiveness in enhancing SBR tasks and its potential as a research direction\nfor future exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.16539v1.pdf"
    },
    {
        "title": "LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification",
        "authors": [
            "Yiping Song",
            "Juhua Zhang",
            "Zhiliang Tian",
            "Yuxin Yang",
            "Minlie Huang",
            "Dongsheng Li"
        ],
        "published": "2024-02-26T11:52:55Z",
        "summary": "As sufficient data are not always publically accessible for model training,\nresearchers exploit limited data with advanced learning algorithms or expand\nthe dataset via data augmentation (DA). Conducting DA in private domain\nrequires private protection approaches (i.e. anonymization and perturbation),\nbut those methods cannot provide protection guarantees. Differential privacy\n(DP) learning methods theoretically bound the protection but are not skilled at\ngenerating pseudo text samples with large models. In this paper, we transfer\nDP-based pseudo sample generation task to DP-based generated samples\ndiscrimination task, where we propose a DP-based DA method with a LLM and a\nDP-based discriminator for text classification on private domains. We construct\na knowledge distillation model as the DP-based discriminator: teacher models,\naccessing private data, teaches students how to select private samples with\ncalibrated noise to achieve DP. To constrain the distribution of DA's\ngeneration, we propose a DP-based tutor that models the noised private\ndistribution and controls samples' generation with a low privacy cost. We\ntheoretically analyze our model's privacy protection and empirically verify our\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2402.16515v1.pdf"
    },
    {
        "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
        "authors": [
            "Junzhe Chen",
            "Xuming Hu",
            "Shuodi Liu",
            "Shiyu Huang",
            "Wei-Wei Tu",
            "Zhaofeng He",
            "Lijie Wen"
        ],
        "published": "2024-02-26T11:31:48Z",
        "summary": "Recent advancements in large language models (LLMs) have revealed their\npotential for achieving autonomous agents possessing human-level intelligence.\nHowever, existing benchmarks for evaluating LLM Agents either use static\ndatasets, potentially leading to data leakage or focus only on single-agent\nscenarios, overlooking the complexities of multi-agent interactions. There is a\nlack of a benchmark that evaluates the diverse capabilities of LLM agents in\nmulti-agent, dynamic environments. To this end, we introduce LLMArena, a novel\nand easily extensible framework for evaluating the diverse capabilities of LLM\nin multi-agent dynamic environments. LLMArena encompasses seven distinct gaming\nenvironments, employing Trueskill scoring to assess crucial abilities in LLM\nagents, including spatial reasoning, strategic planning, numerical reasoning,\nrisk assessment, communication, opponent modeling, and team collaboration. We\nconduct an extensive experiment and human evaluation among different sizes and\ntypes of LLMs, showing that LLMs still have a significant journey ahead in\ntheir development towards becoming fully autonomous agents, especially in\nopponent modeling and team collaboration. We hope LLMArena could guide future\nresearch towards enhancing these capabilities in LLMs, ultimately leading to\nmore sophisticated and practical applications in dynamic, multi-agent settings.\nThe code and data will be available.",
        "pdf_link": "https://arxiv.org/pdf/2402.16499v1.pdf"
    },
    {
        "title": "On Languaging a Simulation Engine",
        "authors": [
            "Han Liu",
            "Liantang Li"
        ],
        "published": "2024-02-26T11:01:54Z",
        "summary": "Language model intelligence is revolutionizing the way we program materials\nsimulations. However, the diversity of simulation scenarios renders it\nchallenging to precisely transform human language into a tailored simulator.\nHere, using three functionalized types of language model, we propose a\nlanguage-to-simulation (Lang2Sim) framework that enables interactive navigation\non languaging a simulation engine, by taking a scenario instance of water\nsorption in porous matrices. Unlike line-by-line coding of a target simulator,\nthe language models interpret each simulator as an assembly of invariant tool\nfunction and its variant input-output pair. Lang2Sim enables the precise\ntransform of textual description by functionalizing and sequentializing the\nlanguage models of, respectively, rationalizing the tool categorization,\ncustomizing its input-output combinations, and distilling the simulator input\ninto executable format. Importantly, depending on its functionalized type, each\nlanguage model features a distinct processing of chat history to best balance\nits memory limit and information completeness, thus leveraging the model\nintelligence to unstructured nature of human request. Overall, this work\nestablishes language model as an intelligent platform to unlock the era of\nlanguaging a simulation engine.",
        "pdf_link": "https://arxiv.org/pdf/2402.16482v1.pdf"
    },
    {
        "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
        "authors": [
            "Yihan Wang",
            "Zhouxing Shi",
            "Andrew Bai",
            "Cho-Jui Hsieh"
        ],
        "published": "2024-02-26T10:03:33Z",
        "summary": "Although many large language models (LLMs) have been trained to refuse\nharmful requests, they are still vulnerable to jailbreaking attacks, which\nrewrite the original prompt to conceal its harmful intent. In this paper, we\npropose a new method for defending LLMs against jailbreaking attacks by\n``backtranslation''. Specifically, given an initial response generated by the\ntarget LLM from an input prompt, our backtranslation prompts a language model\nto infer an input prompt that can lead to the response. The inferred prompt is\ncalled the backtranslated prompt which tends to reveal the actual intent of the\noriginal prompt, since it is generated based on the LLM's response and is not\ndirectly manipulated by the attacker. We then run the target LLM again on the\nbacktranslated prompt, and we refuse the original prompt if the model refuses\nthe backtranslated prompt. We explain that the proposed defense provides\nseveral benefits on its effectiveness and efficiency. We empirically\ndemonstrate that our defense significantly outperforms the baselines, in the\ncases that are hard for the baselines, and our defense also has little impact\non the generation quality for benign input prompts.",
        "pdf_link": "https://arxiv.org/pdf/2402.16459v2.pdf"
    },
    {
        "title": "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen"
        ],
        "published": "2024-02-26T09:59:04Z",
        "summary": "Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine\nthe necessity of retrieval for queries instead of retrieving indiscriminately\nto enhance the efficiency and relevance of the sourced information. However,\nprevious works largely overlook the evaluation of ARAG approaches, leading to\ntheir effectiveness being understudied. This work presents a benchmark,\nRetrievalQA, comprising 1,271 short-form questions covering new world and\nlong-tail knowledge. The knowledge necessary to answer the questions is absent\nfrom LLMs; therefore, external information must be retrieved to answer\ncorrectly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG\nmethods. We observe that calibration-based methods heavily rely on threshold\ntuning, while vanilla prompting is inadequate for guiding LLMs to make reliable\nretrieval decisions. Based on our findings, we propose Time-Aware Adaptive\nRetrieval (TA-ARE), a simple yet effective method that helps LLMs assess the\nnecessity of retrieval without calibration or additional training. The dataset\nand code will be available at \\url{https://github.com/hyintell/RetrievalQA}",
        "pdf_link": "https://arxiv.org/pdf/2402.16457v1.pdf"
    },
    {
        "title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
        "authors": [
            "Zhexin Zhang",
            "Yida Lu",
            "Jingyuan Ma",
            "Di Zhang",
            "Rui Li",
            "Pei Ke",
            "Hao Sun",
            "Lei Sha",
            "Zhifang Sui",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "published": "2024-02-26T09:43:02Z",
        "summary": "The safety of Large Language Models (LLMs) has gained increasing attention in\nrecent years, but there still lacks a comprehensive approach for detecting\nsafety issues within LLMs' responses in an aligned, customizable and\nexplainable manner. In this paper, we propose ShieldLM, an LLM-based safety\ndetector, which aligns with general human safety standards, supports\ncustomizable detection rules, and provides explanations for its decisions. To\ntrain ShieldLM, we compile a large bilingual dataset comprising 14,387\nquery-response pairs, annotating the safety of responses based on various\nsafety standards. Through extensive experiments, we demonstrate that ShieldLM\nsurpasses strong baselines across four test sets, showcasing remarkable\ncustomizability and explainability. Besides performing well on standard\ndetection datasets, ShieldLM has also been shown to be effective in real-world\nsituations as a safety evaluator for advanced LLMs. We release ShieldLM at\n\\url{https://github.com/thu-coai/ShieldLM} to support accurate and explainable\nsafety detection under various safety standards, contributing to the ongoing\nefforts to enhance the safety of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16444v1.pdf"
    },
    {
        "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
        "authors": [
            "Tianyi Tang",
            "Wenyang Luo",
            "Haoyang Huang",
            "Dongdong Zhang",
            "Xiaolei Wang",
            "Xin Zhao",
            "Furu Wei",
            "Ji-Rong Wen"
        ],
        "published": "2024-02-26T09:36:05Z",
        "summary": "Large language models (LLMs) demonstrate remarkable multilingual capabilities\nwithout being pre-trained on specially curated multilingual parallel corpora.\nIt remains a challenging problem to explain the underlying mechanisms by which\nLLMs process multilingual texts. In this paper, we delve into the composition\nof Transformer architectures in LLMs to pinpoint language-specific regions.\nSpecially, we propose a novel detection method, language activation probability\nentropy (LAPE), to identify language-specific neurons within LLMs. Based on\nLAPE, we conduct comprehensive experiments on two representative LLMs, namely\nLLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a\nparticular language is predominantly due to a small subset of neurons,\nprimarily situated in the models' top and bottom layers. Furthermore, we\nshowcase the feasibility to \"steer\" the output language of LLMs by selectively\nactivating or deactivating language-specific neurons. Our research provides\nimportant evidence to the understanding and exploration of the multilingual\ncapabilities of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16438v1.pdf"
    },
    {
        "title": "RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions",
        "authors": [
            "Yuansen Zhang",
            "Xiao Wang",
            "Zhiheng Xi",
            "Han Xia",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-26T09:30:55Z",
        "summary": "Large Language Models (LLMs) have showcased remarkable capabilities in\nfollowing human instructions. However, recent studies have raised concerns\nabout the robustness of LLMs when prompted with instructions combining textual\nadversarial samples. In this paper, drawing inspiration from recent works that\nLLMs are sensitive to the design of the instructions, we utilize instructions\nin code style, which are more structural and less ambiguous, to replace\ntypically natural language instructions. Through this conversion, we provide\nLLMs with more precise instructions and strengthen the robustness of LLMs.\nMoreover, under few-shot scenarios, we propose a novel method to compose\nin-context demonstrations using both clean and adversarial samples\n(\\textit{adversarial context method}) to further boost the robustness of the\nLLMs. Experiments on eight robustness datasets show that our method\nconsistently outperforms prompting LLMs with natural language instructions. For\nexample, with gpt-3.5-turbo, our method achieves an improvement of 5.68\\% in\ntest set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).",
        "pdf_link": "https://arxiv.org/pdf/2402.16431v1.pdf"
    },
    {
        "title": "DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models",
        "authors": [
            "Wei He",
            "Kai Han",
            "Yehui Tang",
            "Chengcheng Wang",
            "Yujie Yang",
            "Tianyu Guo",
            "Yunhe Wang"
        ],
        "published": "2024-02-26T09:21:59Z",
        "summary": "Large language models (LLMs) face a daunting challenge due to the excessive\ncomputational and memory requirements of the commonly used Transformer\narchitecture. While state space model (SSM) is a new type of foundational\nnetwork architecture offering lower computational complexity, their performance\nhas yet to fully rival that of Transformers. This paper introduces DenseSSM, a\nnovel approach to enhance the flow of hidden information between layers in\nSSMs. By selectively integrating shallowlayer hidden states into deeper layers,\nDenseSSM retains fine-grained information crucial for the final output. Dense\nconnections enhanced DenseSSM still maintains the training parallelizability\nand inference efficiency. The proposed method can be widely applicable to\nvarious SSM types like RetNet and Mamba. With similar model size, DenseSSM\nachieves significant improvements, exemplified by DenseRetNet outperforming the\noriginal RetNet with up to 5% accuracy improvement on public benchmarks. code\nis avalaible at https://github.com/WailordHe/DenseSSM",
        "pdf_link": "https://arxiv.org/pdf/2403.00818v2.pdf"
    },
    {
        "title": "Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models",
        "authors": [
            "Lev Kharlashkin",
            "Melany Macias",
            "Leo Huovinen",
            "Mika H\u00e4m\u00e4l\u00e4inen"
        ],
        "published": "2024-02-26T09:19:46Z",
        "summary": "We present our work on predicting United Nations sustainable development\ngoals (SDG) for university courses. We use an LLM named PaLM 2 to generate\ntraining data given a noisy human-authored course description input as input.\nWe use this data to train several different smaller language models to predict\nSDGs for university courses. This work contributes to better university level\nadaptation of SDGs. The best performing model in our experiments was BART with\nan F1-score of 0.786.",
        "pdf_link": "https://arxiv.org/pdf/2402.16420v1.pdf"
    },
    {
        "title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy",
        "authors": [
            "Mengxi Xiao",
            "Qianqian Xie",
            "Ziyan Kuang",
            "Zhicheng Liu",
            "Kailai Yang",
            "Min Peng",
            "Weiguang Han",
            "Jimin Huang"
        ],
        "published": "2024-02-26T09:10:34Z",
        "summary": "Large Language Models (LLMs) can play a vital role in psychotherapy by\nadeptly handling the crucial task of cognitive reframing and overcoming\nchallenges such as shame, distrust, therapist skill variability, and resource\nscarcity. Previous LLMs in cognitive reframing mainly converted negative\nemotions to positive ones, but these approaches have limited efficacy, often\nnot promoting clients' self-discovery of alternative perspectives. In this\npaper, we unveil the Helping and Empowering through Adaptive Language in Mental\nEnhancement (HealMe) model. This novel cognitive reframing therapy method\neffectively addresses deep-rooted negative thoughts and fosters rational,\nbalanced perspectives. Diverging from traditional LLM methods, HealMe employs\nempathetic dialogue based on psychotherapeutic frameworks. It systematically\nguides clients through distinguishing circumstances from feelings,\nbrainstorming alternative viewpoints, and developing empathetic, actionable\nsuggestions. Moreover, we adopt the first comprehensive and expertly crafted\npsychological evaluation metrics, specifically designed to rigorously assess\nthe performance of cognitive reframing, in both AI-simulated dialogues and\nreal-world therapeutic conversations. Experimental results show that our model\noutperforms others in terms of empathy, guidance, and logical coherence,\ndemonstrating its effectiveness and potential positive impact on psychotherapy.",
        "pdf_link": "https://arxiv.org/pdf/2403.05574v2.pdf"
    },
    {
        "title": "From RAGs to riches: Using large language models to write documents for clinical trials",
        "authors": [
            "Nigel Markey",
            "Ilyass El-Mansouri",
            "Gaetan Rensonnet",
            "Casper van Langen",
            "Christoph Meier"
        ],
        "published": "2024-02-26T08:59:05Z",
        "summary": "Clinical trials require numerous documents to be written -- protocols,\nconsent forms, clinical study reports and others. Large language models (LLMs)\noffer the potential to rapidly generate first versions of these documents,\nhowever there are concerns about the quality of their output Here we report an\nevaluation of LLMs in generating parts of one such document, clinical trial\nprotocols. We find that an offthe-shelf LLM delivers reasonable results,\nespecially when assessing content relevance and the correct use of terminology.\nHowever, deficiencies remain: specifically clinical thinking and logic, and\nappropriate use of references. To improve performance, we used\nretrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date\ninformation. As a result of using RAG, the writing quality of the LLM improves\nsubstantially, which has implications for the practical useability of LLMs in\nclinical trial-related writing.",
        "pdf_link": "https://arxiv.org/pdf/2402.16406v1.pdf"
    },
    {
        "title": "MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property",
        "authors": [
            "Shiwen Ni",
            "Minghuan Tan",
            "Yuelin Bai",
            "Fuqiang Niu",
            "Min Yang",
            "Bowen Zhang",
            "Ruifeng Xu",
            "Xiaojun Chen",
            "Chengming Li",
            "Xiping Hu",
            "Ye Li",
            "Jianping Fan"
        ],
        "published": "2024-02-26T08:27:50Z",
        "summary": "Large language models (LLMs) have demonstrated impressive performance in\nvarious natural language processing (NLP) tasks. However, there is limited\nunderstanding of how well LLMs perform in specific domains (e.g, the\nintellectual property (IP) domain). In this paper, we contribute a new\nbenchmark, the first Multilingual-oriented quiZ on Intellectual Property\n(MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark\nincludes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question\nanswering (IPQA), and patent matching (PatentMatch). In addition, we also\ndevelop a new IP-oriented multilingual large language model (called MoZi),\nwhich is a BLOOMZ-based model that has been supervised fine-tuned with\nmultilingual IP-related text data. We evaluate our proposed MoZi model and four\nwell-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP\nbenchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE\nand ChatGLM by a noticeable margin, while it had lower scores compared with\nChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has\nmuch room for improvement, and even the most powerful ChatGPT does not reach\nthe passing level. Our source code, data, and models are available at\n\\url{https://github.com/AI-for-Science/MoZi}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16389v1.pdf"
    },
    {
        "title": "Immunization against harmful fine-tuning attacks",
        "authors": [
            "Domenic Rosati",
            "Jan Wehner",
            "Kai Williams",
            "\u0141ukasz Bartoszcze",
            "Jan Batzner",
            "Hassan Sajjad",
            "Frank Rudzicz"
        ],
        "published": "2024-02-26T08:08:03Z",
        "summary": "Approaches to aligning large language models (LLMs) with human values has\nfocused on correcting misalignment that emerges from pretraining. However, this\nfocus overlooks another source of misalignment: bad actors might purposely\nfine-tune LLMs to achieve harmful goals. In this paper, we present an emerging\nthreat model that has arisen from alignment circumvention and fine-tuning\nattacks. However, lacking in previous works is a clear presentation of the\nconditions for effective defence. We propose a set of conditions for effective\ndefence against harmful fine-tuning in LLMs called \"Immunization conditions,\"\nwhich help us understand how we would construct and measure future defences.\nUsing this formal framework for defence, we offer a synthesis of different\nresearch directions that might be persued to prevent harmful fine-tuning\nattacks and provide a demonstration of how to use these conditions\nexperimentally showing early results of using an adversarial loss to immunize\nLLama2-7b-chat.",
        "pdf_link": "https://arxiv.org/pdf/2402.16382v1.pdf"
    },
    {
        "title": "Improving LLM-based Machine Translation with Systematic Self-Correction",
        "authors": [
            "Zhaopeng Feng",
            "Yan Zhang",
            "Hao Li",
            "Wenqiang Liu",
            "Jun Lang",
            "Yang Feng",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "published": "2024-02-26T07:58:12Z",
        "summary": "Large Language Models (LLMs) have achieved impressive results in Machine\nTranslation (MT). However, careful evaluations by human reveal that the\ntranslations produced by LLMs still contain multiple errors. Importantly,\nfeeding back such error information into the LLMs can lead to self-correction\nand result in improved translation performance. Motivated by these insights, we\nintroduce a systematic LLM-based self-correcting translation framework, named\nTER, which stands for Translate, Estimate, and Refine, marking a significant\nstep forward in this direction. Our findings demonstrate that 1) our\nself-correction framework successfully assists LLMs in improving their\ntranslation quality across a wide range of languages, whether it's from\nhigh-resource languages to low-resource ones or whether it's English-centric or\ncentered around other languages; 2) TER exhibits superior systematicity and\ninterpretability compared to previous methods; 3) different estimation\nstrategies yield varied impacts on AI feedback, directly affecting the\neffectiveness of the final corrections. We further compare different LLMs and\nconduct various experiments involving self-correction and cross-model\ncorrection to investigate the potential relationship between the translation\nand evaluation capabilities of LLMs. Our code and data are available at\nhttps://github.com/fzp0424/self_correct_mt",
        "pdf_link": "https://arxiv.org/pdf/2402.16379v2.pdf"
    },
    {
        "title": "Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models",
        "authors": [
            "Weize Liu",
            "Yinlong Xu",
            "Hongxia Xu",
            "Jintai Chen",
            "Xuming Hu",
            "Jian Wu"
        ],
        "published": "2024-02-26T07:44:56Z",
        "summary": "Recently, large language models (LLMs) have achieved tremendous breakthroughs\nin the field of language processing, yet their mechanisms in processing\nmultiple languages remain agnostic. Therefore, in this work we study the\nmultilingual activation patterns of LLMs. By transforming the original Large\nLanguage Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze\nthe expert activation patterns when processing various languages and\ndemonstrate the connections of these activation patterns at the level of\nlanguage families. We discover the existence of non-language-specific neurons\nas well as language-specific activation neurons. Further exploration even\nshowcases that merely leveraging high-frequency activation neurons can\naccelerate inference while maintaining comparable performance. These findings\nshed light on the LLMs' multilingual processing mechanism, and are of\nsignificant importance in guiding the multilingual training and model pruning\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16367v1.pdf"
    },
    {
        "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
        "authors": [
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Yang Zhou",
            "Zhen Dong",
            "Zhe Zhou",
            "Chenhao Xue",
            "Bingzhe Wu",
            "Zhikai Li",
            "Qingyi Gu",
            "Yong Jae Lee",
            "Yan Yan",
            "Beidi Chen",
            "Guangyu Sun",
            "Kurt Keutzer"
        ],
        "published": "2024-02-26T07:33:05Z",
        "summary": "The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.",
        "pdf_link": "https://arxiv.org/pdf/2402.16363v5.pdf"
    },
    {
        "title": "Language-guided Skill Learning with Temporal Variational Inference",
        "authors": [
            "Haotian Fu",
            "Pratyusha Sharma",
            "Elias Stengel-Eskin",
            "George Konidaris",
            "Nicolas Le Roux",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Xingdi Yuan"
        ],
        "published": "2024-02-26T07:19:23Z",
        "summary": "We present an algorithm for skill discovery from expert demonstrations. The\nalgorithm first utilizes Large Language Models (LLMs) to propose an initial\nsegmentation of the trajectories. Following that, a hierarchical variational\ninference framework incorporates the LLM-generated segmentation information to\ndiscover reusable skills by merging trajectory segments. To further control the\ntrade-off between compression and reusability, we introduce a novel auxiliary\nobjective based on the Minimum Description Length principle that helps guide\nthis skill discovery process. Our results demonstrate that agents equipped with\nour method are able to discover skills that help accelerate learning and\noutperform baseline skill learning approaches on new long-horizon tasks in\nBabyAI, a grid world navigation environment, as well as ALFRED, a household\nsimulation environment.",
        "pdf_link": "https://arxiv.org/pdf/2402.16354v1.pdf"
    },
    {
        "title": "CodeS: Towards Building Open-source Language Models for Text-to-SQL",
        "authors": [
            "Haoyang Li",
            "Jing Zhang",
            "Hanbing Liu",
            "Ju Fan",
            "Xiaokang Zhang",
            "Jun Zhu",
            "Renjie Wei",
            "Hongyan Pan",
            "Cuiping Li",
            "Hong Chen"
        ],
        "published": "2024-02-26T07:00:58Z",
        "summary": "Language models have shown promising performance on the task of translating\nnatural language questions into SQL queries (Text-to-SQL). However, most of the\nstate-of-the-art (SOTA) approaches rely on powerful yet closed-source large\nlanguage models (LLMs), such as ChatGPT and GPT-4, which may have the\nlimitations of unclear model architectures, data privacy risks, and expensive\ninference overheads. To address the limitations, we introduce CodeS, a series\nof pre-trained language models with parameters ranging from 1B to 15B,\nspecifically designed for the text-to-SQL task. CodeS is a fully open-source\nlanguage model, which achieves superior accuracy with much smaller parameter\nsizes. This paper studies the research challenges in building CodeS. To enhance\nthe SQL generation abilities of CodeS, we adopt an incremental pre-training\napproach using a specifically curated SQL-centric corpus. Based on this, we\naddress the challenges of schema linking and rapid domain adaptation through\nstrategic prompt construction and a bi-directional data augmentation technique.\nWe conduct comprehensive evaluations on multiple datasets, including the widely\nused Spider benchmark, the newly released BIRD benchmark, robustness-diagnostic\nbenchmarks such as Spider-DK, Spider-Syn, Spider-Realistic, and Dr.Spider, as\nwell as two real-world datasets created for financial and academic\napplications. The experimental results show that our CodeS achieves new SOTA\naccuracy and robustness on nearly all challenging text-to-SQL benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.16347v1.pdf"
    },
    {
        "title": "Personalized Federated Instruction Tuning via Neural Architecture Search",
        "authors": [
            "Pengyu Zhang",
            "Yingbo Zhou",
            "Ming Hu",
            "Junxian Feng",
            "Jiawen Weng",
            "Mingsong Chen"
        ],
        "published": "2024-02-26T06:29:05Z",
        "summary": "Federated Instruction Tuning (FIT) has shown the ability to achieve\ncollaborative model instruction tuning among massive data owners without\nsharing private data. However, it still faces two key challenges, i.e., data\nand resource heterogeneity. Due to the varying data distribution and\npreferences among data owners, FIT cannot adapt to the personalized data of\nindividual owners. Moreover, clients with superior computational abilities are\nconstrained since they need to maintain the same fine-tuning architecture as\nthe weaker clients. To address these issues, we propose a novel Personalized\nFederated Instruction Tuning (PerFIT) framework based on architecture search.\nSpecifically, PerFIT allows each client to search for a personalized\narchitecture by expanding the trainable parameter space of the global model\nfollowed by pruning the parameters to the original state. This procedure allows\npersonalized instruction fine-tuning within expanded parameter spaces,\nconcurrently preserving the same number of trainable parameters. Furthermore,\nto release the abilities of heterogeneous computational resources and enhance\nthe performance of personalization on local data, we exploit personalized\nparameter-wise aggregation. The evaluation with multiple LLMs non-IID scenarios\ndemonstrates that compared to the state-of-the-art FIT methods, our approach\ncan achieve up to a 23% decrease in perplexity.",
        "pdf_link": "https://arxiv.org/pdf/2402.16919v1.pdf"
    },
    {
        "title": "Data-freeWeight Compress and Denoise for Large Language Models",
        "authors": [
            "Runyu Peng",
            "Yunhua Zhou",
            "Qipeng Guo",
            "Yang Gao",
            "Hang Yan",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2024-02-26T05:51:47Z",
        "summary": "Large Language Models (LLMs) are reshaping the research landscape in\nartificial intelligence, particularly as model parameters scale up\nsignificantly, unlocking remarkable capabilities across various domains.\nNevertheless, the scalability of model parameters faces constraints due to\nlimitations in GPU memory and computational speed. To address these\nconstraints, various weight compression methods have emerged, such as Pruning\nand Quantization. Given the low-rank nature of weight matrices in language\nmodels, the reduction of weights through matrix decomposition undoubtedly holds\nsignificant potential and promise. In this paper, drawing upon the intrinsic\nstructure of LLMs, we propose a novel approach termed Data-free Joint Rank-k\nApproximation for compressing the parameter matrices. Significantly, our method\nis characterized by without necessitating additional involvement of any corpus,\nwhile simultaneously preserving orthogonality in conjunction with pruning and\nquantization methods. We achieve a model pruning of 80% parameters while\nretaining 93.43% of the original performance without any calibration data.\nAdditionally, we explore the fundamental properties of the weight matrix of\nLLMs undergone Rank-k Approximation and conduct comprehensive experiments to\nelucidate our hypothesis.",
        "pdf_link": "https://arxiv.org/pdf/2402.16319v1.pdf"
    },
    {
        "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models",
        "authors": [
            "Jeonghwan Kim",
            "Heng Ji"
        ],
        "published": "2024-02-26T05:43:51Z",
        "summary": "Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.",
        "pdf_link": "https://arxiv.org/pdf/2402.16315v2.pdf"
    },
    {
        "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
        "authors": [
            "Mingxu Tao",
            "Dongyan Zhao",
            "Yansong Feng"
        ],
        "published": "2024-02-26T05:31:34Z",
        "summary": "Open-ended question answering requires models to find appropriate evidence to\nform well-reasoned, comprehensive and helpful answers. In practical\napplications, models also need to engage in extended discussions on potential\nscenarios closely relevant to the question. With augmentation of retrieval\nmodule, open-source Large Language Models (LLMs) can produce coherent answers\noften with different focuses, but are still sub-optimal in terms of reliable\nevidence selection and in-depth question analysis. In this paper, we propose a\nnovel Chain-of-Discussion framework to leverage the synergy among multiple\nopen-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more\ncomprehensive} answers for open-ended QA, although they are not strong enough\nindividually. Our experiments show that discussions among multiple LLMs play a\nvital role in enhancing the quality of answers. We release our data and code at\n\\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16313v1.pdf"
    },
    {
        "title": "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering",
        "authors": [
            "Yiming Du",
            "Hongru Wang",
            "Zhengyi Zhao",
            "Bin Liang",
            "Baojun Wang",
            "Wanjun Zhong",
            "Zezhong Wang",
            "Kam-Fai Wong"
        ],
        "published": "2024-02-26T04:09:53Z",
        "summary": "Long-term memory plays a critical role in personal interaction, considering\nlong-term memory can better leverage world knowledge, historical information,\nand preferences in dialogues. Our research introduces PerLTQA, an innovative QA\ndataset that combines semantic and episodic memories, including world\nknowledge, profiles, social relationships, events, and dialogues. This dataset\nis collected to investigate the use of personalized memories, focusing on\nsocial interactions and events in the QA task. PerLTQA features two types of\nmemory and a comprehensive benchmark of 8,593 questions for 30 characters,\nfacilitating the exploration and application of personalized memories in Large\nLanguage Models (LLMs). Based on PerLTQA, we propose a novel framework for\nmemory integration and generation, consisting of three main components: Memory\nClassification, Memory Retrieval, and Memory Synthesis. We evaluate this\nframework using five LLMs and three retrievers. Experimental results\ndemonstrate that BERT-based classification models significantly outperform LLMs\nsuch as ChatGLM3 and ChatGPT in the memory classification task. Furthermore,\nour study highlights the importance of effective memory integration in the QA\ntask.",
        "pdf_link": "https://arxiv.org/pdf/2402.16288v1.pdf"
    },
    {
        "title": "From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto",
        "authors": [
            "Segev Wasserkrug",
            "Leonard Boussioux",
            "Dick den Hertog",
            "Farzaneh Mirzazadeh",
            "Ilker Birbil",
            "Jannis Kurtz",
            "Donato Maragno"
        ],
        "published": "2024-02-26T03:10:11Z",
        "summary": "Significantly simplifying the creation of optimization models for real-world\nbusiness problems has long been a major goal in applying mathematical\noptimization more widely to important business and societal decisions. The\nrecent capabilities of Large Language Models (LLMs) present a timely\nopportunity to achieve this goal. Therefore, we propose research at the\nintersection of LLMs and optimization to create a Decision Optimization CoPilot\n(DOCP) - an AI tool designed to assist any decision maker, interacting in\nnatural language to grasp the business problem, subsequently formulating and\nsolving the corresponding optimization model. This paper outlines our DOCP\nvision and identifies several fundamental requirements for its implementation.\nWe describe the state of the art through a literature survey and experiments\nusing ChatGPT. We show that a) LLMs already provide substantial novel\ncapabilities relevant to a DOCP, and b) major research challenges remain to be\naddressed. We also propose possible research directions to overcome these gaps.\nWe also see this work as a call to action to bring together the LLM and\noptimization communities to pursue our vision, thereby enabling much more\nwidespread improved decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.16269v1.pdf"
    },
    {
        "title": "CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document Visual Question Answering",
        "authors": [
            "Jinxu Zhang",
            "Yongqi Yu",
            "Yu Zhang"
        ],
        "published": "2024-02-26T01:17:50Z",
        "summary": "Document Visual Question Answering (DVQA) is a task that involves responding\nto queries based on the content of images. Existing work is limited to locating\ninformation within a single page and does not facilitate cross-page\nquestion-and-answer interaction. Furthermore, the token length limitation\nimposed on inputs to the model may lead to truncation of segments pertinent to\nthe answer. In this study, we introduce a simple but effective methodology\ncalled CFRet-DVQA, which focuses on retrieval and efficient tuning to address\nthis critical issue effectively. For that, we initially retrieve multiple\nsegments from the document that correlate with the question at hand.\nSubsequently, we leverage the advanced reasoning abilities of the large\nlanguage model (LLM), further augmenting its performance through instruction\ntuning. This approach enables the generation of answers that align with the\nstyle of the document labels. The experiments demonstrate that our methodology\nachieved state-of-the-art or competitive results with both single-page and\nmulti-page documents in various fields.",
        "pdf_link": "https://arxiv.org/pdf/2403.00816v1.pdf"
    },
    {
        "title": "HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs",
        "authors": [
            "Cem Uluoglakci",
            "Tugba Taskaya Temizel"
        ],
        "published": "2024-02-25T22:23:37Z",
        "summary": "Hallucinations pose a significant challenge to the reliability and alignment\nof Large Language Models (LLMs), limiting their widespread acceptance beyond\nchatbot applications. Despite ongoing efforts, hallucinations remain a\nprevalent challenge in LLMs. The detection of hallucinations itself is also a\nformidable task, frequently requiring manual labeling or constrained\nevaluations. This paper introduces an automated scalable framework that\ncombines benchmarking LLMs' hallucination tendencies with efficient\nhallucination detection. We leverage LLMs to generate challenging tasks related\nto hypothetical phenomena, subsequently employing them as agents for efficient\nhallucination detection. The framework is domain-agnostic, allowing the use of\nany language model for benchmark creation or evaluation in any domain. We\nintroduce the publicly available HypoTermQA Benchmarking Dataset, on which\nstate-of-the-art models' performance ranged between 3% and 11%, and evaluator\nagents demonstrated a 6% error rate in hallucination prediction. The proposed\nframework provides opportunities to test and improve LLMs. Additionally, it has\nthe potential to generate benchmarking datasets tailored to specific domains,\nsuch as law, health, and finance.",
        "pdf_link": "https://arxiv.org/pdf/2402.16211v1.pdf"
    },
    {
        "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing",
        "authors": [
            "Jiabao Ji",
            "Bairu Hou",
            "Alexander Robey",
            "George J. Pappas",
            "Hamed Hassani",
            "Yang Zhang",
            "Eric Wong",
            "Shiyu Chang"
        ],
        "published": "2024-02-25T20:36:03Z",
        "summary": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks,\nwhich bypass the safeguards of targeted LLMs and fool them into generating\nobjectionable content. While initial defenses show promise against token-based\nthreat models, there do not exist defenses that provide robustness against\nsemantic attacks and avoid unfavorable trade-offs between robustness and\nnominal performance. To meet this need, we propose SEMANTICSMOOTH, a\nsmoothing-based defense that aggregates the predictions of multiple\nsemantically transformed copies of a given input prompt. Experimental results\ndemonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against\nGCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on\ninstruction following benchmarks such as InstructionFollowing and AlpacaEval.\nThe codes will be publicly available at\nhttps://github.com/UCSB-NLP-Chang/SemanticSmooth.",
        "pdf_link": "https://arxiv.org/pdf/2402.16192v2.pdf"
    },
    {
        "title": "Attacking LLM Watermarks by Exploiting Their Strengths",
        "authors": [
            "Qi Pang",
            "Shengyuan Hu",
            "Wenting Zheng",
            "Virginia Smith"
        ],
        "published": "2024-02-25T20:24:07Z",
        "summary": "Advances in generative models have made it possible for AI-generated text,\ncode, and images to mirror human-generated content in many applications.\nWatermarking, a technique that aims to embed information in the output of a\nmodel to verify its source, is useful for mitigating misuse of such\nAI-generated content. However, existing watermarking schemes remain\nsurprisingly susceptible to attack. In particular, we show that desirable\nproperties shared by existing LLM watermarking systems such as quality\npreservation, robustness, and public detection APIs can in turn make these\nsystems vulnerable to various attacks. We rigorously study potential attacks in\nterms of common watermark design choices, and propose best practices and\ndefenses for mitigation -- establishing a set of practical guidelines for\nembedding and detection of LLM watermarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.16187v1.pdf"
    },
    {
        "title": "How Can LLM Guide RL? A Value-Based Approach",
        "authors": [
            "Shenao Zhang",
            "Sirui Zheng",
            "Shuqi Ke",
            "Zhihan Liu",
            "Wanxin Jin",
            "Jianbo Yuan",
            "Yingxiang Yang",
            "Hongxia Yang",
            "Zhaoran Wang"
        ],
        "published": "2024-02-25T20:07:13Z",
        "summary": "Reinforcement learning (RL) has become the de facto standard practice for\nsequential decision-making problems by improving future acting policies with\nfeedback. However, RL algorithms may require extensive trial-and-error\ninteractions to collect useful feedback for improvement. On the other hand,\nrecent developments in large language models (LLMs) have showcased impressive\ncapabilities in language understanding and generation, yet they fall short in\nexploration and self-improvement capabilities for planning tasks, lacking the\nability to autonomously refine their responses based on feedback. Therefore, in\nthis paper, we study how the policy prior provided by the LLM can enhance the\nsample efficiency of RL algorithms. Specifically, we develop an algorithm named\nLINVIT that incorporates LLM guidance as a regularization factor in value-based\nRL, leading to significant reductions in the amount of data needed for\nlearning, particularly when the difference between the ideal policy and the\nLLM-informed policy is small, which suggests that the initial policy is close\nto optimal, reducing the need for further exploration. Additionally, we present\na practical algorithm SLINVIT that simplifies the construction of the value\nfunction and employs subgoals to reduce the search complexity. Our experiments\nacross three interactive environments ALFWorld, InterCode, and BlocksWorld\ndemonstrate that our method achieves state-of-the-art success rates and also\nsurpasses previous RL and LLM approaches in terms of sample efficiency. Our\ncode is available at https://github.com/agentification/Language-Integrated-VI.",
        "pdf_link": "https://arxiv.org/pdf/2402.16181v1.pdf"
    },
    {
        "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers",
        "authors": [
            "Xirui Li",
            "Ruochen Wang",
            "Minhao Cheng",
            "Tianyi Zhou",
            "Cho-Jui Hsieh"
        ],
        "published": "2024-02-25T17:43:29Z",
        "summary": "The safety alignment of Large Language Models (LLMs) is vulnerable to both\nmanual and automated jailbreak attacks, which adversarially trigger LLMs to\noutput harmful content. However, current methods for jailbreaking LLMs, which\nnest entire harmful prompts, are not effective at concealing malicious intent\nand can be easily identified and rejected by well-aligned LLMs. This paper\ndiscovers that decomposing a malicious prompt into separated sub-prompts can\neffectively obscure its underlying malicious intent by presenting it in a\nfragmented, less detectable form, thereby addressing these limitations. We\nintroduce an automatic prompt \\textbf{D}ecomposition and\n\\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack).\nDrAttack includes three key components: (a) `Decomposition' of the original\nprompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly\nby in-context learning with semantically similar but harmless reassembling\ndemo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'\nsynonyms that maintain the original intent while jailbreaking LLMs. An\nextensive empirical study across multiple open-source and closed-source LLMs\ndemonstrates that, with a significantly reduced number of queries, DrAttack\nobtains a substantial gain of success rate over prior SOTA prompt-only\nattackers. Notably, the success rate of 78.0\\% on GPT-4 with merely 15 queries\nsurpassed previous art by 33.1\\%. The project is available at\nhttps://github.com/xirui-li/DrAttack.",
        "pdf_link": "https://arxiv.org/pdf/2402.16914v2.pdf"
    },
    {
        "title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM",
        "authors": [
            "Ruibin Yuan",
            "Hanfeng Lin",
            "Yi Wang",
            "Zeyue Tian",
            "Shangda Wu",
            "Tianhao Shen",
            "Ge Zhang",
            "Yuhang Wu",
            "Cong Liu",
            "Ziya Zhou",
            "Ziyang Ma",
            "Liumeng Xue",
            "Ziyu Wang",
            "Qin Liu",
            "Tianyu Zheng",
            "Yizhi Li",
            "Yinghao Ma",
            "Yiming Liang",
            "Xiaowei Chi",
            "Ruibo Liu",
            "Zili Wang",
            "Pengfei Li",
            "Jingcheng Wu",
            "Chenghua Lin",
            "Qifeng Liu",
            "Tao Jiang",
            "Wenhao Huang",
            "Wenhu Chen",
            "Emmanouil Benetos",
            "Jie Fu",
            "Gus Xia",
            "Roger Dannenberg",
            "Wei Xue",
            "Shiyin Kang",
            "Yike Guo"
        ],
        "published": "2024-02-25T17:19:41Z",
        "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities in\ntext generation, we find that their ability has yet to be generalized to music,\nhumanity's creative language. We introduce ChatMusician, an open-source LLM\nthat integrates intrinsic musical abilities. It is based on continual\npre-training and finetuning LLaMA2 on a text-compatible music representation,\nABC notation, and the music is treated as a second language. ChatMusician can\nunderstand and generate music with a pure text tokenizer without any external\nmulti-modal neural structures or tokenizers. Interestingly, endowing musical\nabilities does not harm language abilities, even achieving a slightly higher\nMMLU score. Our model is capable of composing well-structured, full-length\nmusic, conditioned on texts, chords, melodies, motifs, musical forms, etc,\nsurpassing GPT-4 baseline. On our meticulously curated college-level music\nunderstanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and\nGPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs\ncan be an excellent compressor for music, but there remains significant\nterritory to be conquered. We release our 4B token music-language corpora\nMusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2402.16153v1.pdf"
    },
    {
        "title": "PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization",
        "authors": [
            "Xiangdi Meng",
            "Damai Dai",
            "Weiyao Luo",
            "Zhe Yang",
            "Shaoxiang Wu",
            "Xiaochen Wang",
            "Peiyi Wang",
            "Qingxiu Dong",
            "Liang Chen",
            "Zhifang Sui"
        ],
        "published": "2024-02-25T16:43:41Z",
        "summary": "Supervised fine-tuning is the most common method to adapt large language\nmodels (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive\ncomputational resources. Recently, parameter-efficient fine-tuning (PEFT)\nmethods have been widely studied due to its cost-effectiveness. LoRA is one of\nthe most widely used methods, which assumes that the optimization process is\nessentially low-dimensional. Although LoRA fine-tuning is effective, there is\nstill a performance gap compared to full fine-tuning, since its weight update\nis limited to low-rank matrices. In order to break the low-rank bottleneck in\nLoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank\nupdate matrices multiple times to achieve a higher update rank. PLoRA has\nmultiple training stages. During each stage, we still update only the LoRA\nweights. However, at the end of each stage, we unload the LoRA weights into the\nbackbone parameters and then reinitialize the LoRA states. Experimental results\nshow that PLoRA has stronger learning ability, approximately 1.8 times that of\nLoRA's learning ability at most, but it does not increase memory usage.\nFurther, we introduce a momentum-based unloading strategy for PLoRA to mitigate\nthe training instability.",
        "pdf_link": "https://arxiv.org/pdf/2402.16141v1.pdf"
    },
    {
        "title": "What Generative Artificial Intelligence Means for Terminological Definitions",
        "authors": [
            "Antonio San Mart\u00edn"
        ],
        "published": "2024-02-25T16:36:51Z",
        "summary": "This paper examines the impact of Generative Artificial Intelligence (GenAI)\ntools like ChatGPT on the creation and consumption of terminological\ndefinitions. From the terminologist's point of view, the strategic use of GenAI\ntools can streamline the process of crafting definitions, reducing both time\nand effort, while potentially enhancing quality. GenAI tools enable AI-assisted\nterminography, notably post-editing terminography, where the machine produces a\ndefinition that the terminologist then corrects or refines. However, the\npotential of GenAI tools to fulfill all the terminological needs of a user,\nincluding term definitions, challenges the very existence of terminological\ndefinitions and resources as we know them. Unlike terminological definitions,\nGenAI tools can describe the knowledge activated by a term in a specific\ncontext. However, a main drawback of these tools is that their output can\ncontain errors. For this reason, users requiring reliability will likely still\nresort to terminological resources for definitions. Nevertheless, with the\ninevitable integration of AI into terminology work, the distinction between\nhuman-created and AI-created content will become increasingly blurred.",
        "pdf_link": "https://arxiv.org/pdf/2402.16139v2.pdf"
    },
    {
        "title": "AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation",
        "authors": [
            "Yasheng Sun",
            "Wenqing Chu",
            "Hang Zhou",
            "Kaisiyuan Wang",
            "Hideki Koike"
        ],
        "published": "2024-02-25T15:51:05Z",
        "summary": "While considerable progress has been made in achieving accurate lip\nsynchronization for 3D speech-driven talking face generation, the task of\nincorporating expressive facial detail synthesis aligned with the speaker's\nspeaking status remains challenging. Our goal is to directly leverage the\ninherent style information conveyed by human speech for generating an\nexpressive talking face that aligns with the speaking status. In this paper, we\npropose AVI-Talking, an Audio-Visual Instruction system for expressive Talking\nface generation. This system harnesses the robust contextual reasoning and\nhallucination capability offered by Large Language Models (LLMs) to instruct\nthe realistic synthesis of 3D talking faces. Instead of directly learning\nfacial movements from human speech, our two-stage strategy involves the LLMs\nfirst comprehending audio information and generating instructions implying\nexpressive facial details seamlessly corresponding to the speech. Subsequently,\na diffusion-based generative network executes these instructions. This\ntwo-stage process, coupled with the incorporation of LLMs, enhances model\ninterpretability and provides users with flexibility to comprehend instructions\nand specify desired operations or modifications. Extensive experiments showcase\nthe effectiveness of our approach in producing vivid talking faces with\nexpressive facial movements and consistent emotional status.",
        "pdf_link": "https://arxiv.org/pdf/2402.16124v1.pdf"
    },
    {
        "title": "FuseChat: Knowledge Fusion of Chat Models",
        "authors": [
            "Fanqi Wan",
            "Ziyi Yang",
            "Longguang Zhong",
            "Xiaojun Quan",
            "Xinting Huang",
            "Wei Bi"
        ],
        "published": "2024-02-25T15:11:58Z",
        "summary": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, this approach incurs\nsubstantial costs and may lead to potential redundancy in competencies. An\nalternative strategy is to combine existing LLMs into a more robust LLM,\nthereby diminishing the necessity for expensive pre-training. However, due to\nthe diverse architectures of LLMs, direct parameter blending proves to be\nunfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge\nfusion to transfer the collective knowledge of multiple structurally varied\nLLMs into a target LLM through lightweight continual training. In this report,\nwe extend the scalability and flexibility of the \\textsc{FuseLLM} framework to\nrealize the fusion of chat LLMs, resulting in \\textsc{FuseChat}.\n\\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge\nfusion for structurally and scale-varied source LLMs to derive multiple target\nLLMs of identical structure and size via lightweight fine-tuning. Then, these\ntarget LLMs are merged within the parameter space, wherein we propose a novel\nmethod for determining the merging weights based on the variation ratio of\nparameter matrices before and after fine-tuning. We validate our approach using\nthree prominent chat LLMs with diverse architectures and scales, namely\n\\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and\n\\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains\ndemonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad\nspectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5\n(March)} and approaching \\texttt{Mixtral-8x7B-Instruct}. Our code, model\nweights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/FuseLLM}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16107v3.pdf"
    },
    {
        "title": "NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification",
        "authors": [
            "Hanna Abi Akl"
        ],
        "published": "2024-02-25T13:20:13Z",
        "summary": "We present a neuro-symbolic (NeSy) workflow combining a symbolic-based\nlearning technique with a large language model (LLM) agent to generate\nsynthetic data for code comment classification in the C programming language.\nWe also show how generating controlled synthetic data using this workflow fixes\nsome of the notable weaknesses of LLM-based generation and increases the\nperformance of classical machine learning models on the code comment\nclassification task. Our best model, a Neural Network, achieves a Macro-F1\nscore of 91.412% with an increase of 1.033% after data augmentation.",
        "pdf_link": "https://arxiv.org/pdf/2402.16910v1.pdf"
    },
    {
        "title": "UrbanGPT: Spatio-Temporal Large Language Models",
        "authors": [
            "Zhonghang Li",
            "Lianghao Xia",
            "Jiabin Tang",
            "Yong Xu",
            "Lei Shi",
            "Long Xia",
            "Dawei Yin",
            "Chao Huang"
        ],
        "published": "2024-02-25T12:37:29Z",
        "summary": "Spatio-temporal prediction aims to forecast and gain insights into the\never-changing dynamics of urban environments across both time and space. Its\npurpose is to anticipate future patterns, trends, and events in diverse facets\nof urban life, including transportation, population movement, and crime rates.\nAlthough numerous efforts have been dedicated to developing neural network\ntechniques for accurate predictions on spatio-temporal data, it is important to\nnote that many of these methods heavily depend on having sufficient labeled\ndata to generate precise spatio-temporal representations. Unfortunately, the\nissue of data scarcity is pervasive in practical urban sensing scenarios.\nConsequently, it becomes necessary to build a spatio-temporal model with strong\ngeneralization capabilities across diverse spatio-temporal learning scenarios.\nTaking inspiration from the remarkable achievements of large language models\n(LLMs), our objective is to create a spatio-temporal LLM that can exhibit\nexceptional generalization capabilities across a wide range of downstream urban\ntasks. To achieve this objective, we present the UrbanGPT, which seamlessly\nintegrates a spatio-temporal dependency encoder with the instruction-tuning\nparadigm. This integration enables LLMs to comprehend the complex\ninter-dependencies across time and space, facilitating more comprehensive and\naccurate predictions under data scarcity. To validate the effectiveness of our\napproach, we conduct extensive experiments on various public datasets, covering\ndifferent spatio-temporal prediction tasks. The results consistently\ndemonstrate that our UrbanGPT, with its carefully designed architecture,\nconsistently outperforms state-of-the-art baselines. These findings highlight\nthe potential of building large language models for spatio-temporal learning,\nparticularly in zero-shot scenarios where labeled data is scarce.",
        "pdf_link": "https://arxiv.org/pdf/2403.00813v2.pdf"
    },
    {
        "title": "Citation-Enhanced Generation for LLM-based Chatbots",
        "authors": [
            "Weitao Li",
            "Junkai Li",
            "Weizhi Ma",
            "Yang Liu"
        ],
        "published": "2024-02-25T11:24:41Z",
        "summary": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2402.16063v3.pdf"
    },
    {
        "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions",
        "authors": [
            "Xuming Hu",
            "Xiaochuan Li",
            "Junzhe Chen",
            "Yinghui Li",
            "Yangning Li",
            "Xiaoguang Li",
            "Yasheng Wang",
            "Qun Liu",
            "Lijie Wen",
            "Philip S. Yu",
            "Zhijiang Guo"
        ],
        "published": "2024-02-25T11:22:19Z",
        "summary": "Generative search engines have the potential to transform how people seek\ninformation online, but generated responses from existing large language models\n(LLMs)-backed generative search engines may not always be accurate.\nNonetheless, retrieval-augmented generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable part of a claim. To this end, we propose evaluating the\nrobustness of generative search engines in the realistic and high-risk setting,\nwhere adversaries have only black-box system access and seek to deceive the\nmodel into returning incorrect responses. Through a comprehensive human\nevaluation of various generative search engines, such as Bing Chat,\nPerplexityAI, and YouChat across diverse queries, we demonstrate the\neffectiveness of adversarial factual questions in inducing incorrect responses.\nMoreover, retrieval-augmented generation exhibits a higher susceptibility to\nfactual errors compared to LLMs without retrieval. These findings highlight the\npotential security risks of these systems and emphasize the need for rigorous\nevaluation before deployment.",
        "pdf_link": "https://arxiv.org/pdf/2403.12077v1.pdf"
    },
    {
        "title": "How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study",
        "authors": [
            "Tianjie Ju",
            "Weiwei Sun",
            "Wei Du",
            "Xinwei Yuan",
            "Zhaochun Ren",
            "Gongshen Liu"
        ],
        "published": "2024-02-25T11:15:42Z",
        "summary": "Previous work has showcased the intriguing capability of large language\nmodels (LLMs) in retrieving facts and processing context knowledge. However,\nonly limited research exists on the layer-wise capability of LLMs to encode\nknowledge, which challenges our understanding of their internal mechanisms. In\nthis paper, we devote the first attempt to investigate the layer-wise\ncapability of LLMs through probing tasks. We leverage the powerful generative\ncapability of ChatGPT to construct probing datasets, providing diverse and\ncoherent evidence corresponding to various facts. We employ $\\mathcal V$-usable\ninformation as the validation metric to better reflect the capability in\nencoding context knowledge across different layers. Our experiments on\nconflicting and newly acquired knowledge show that LLMs: (1) prefer to encode\nmore context knowledge in the upper layers; (2) primarily encode context\nknowledge within knowledge-related entity tokens at lower layers while\nprogressively expanding more knowledge within other tokens at upper layers; and\n(3) gradually forget the earlier context knowledge retained within the\nintermediate layers when provided with irrelevant evidence. Code is publicly\navailable at https://github.com/Jometeorie/probing_llama.",
        "pdf_link": "https://arxiv.org/pdf/2402.16061v2.pdf"
    },
    {
        "title": "Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression",
        "authors": [
            "Xinze Li",
            "Zhenghao Liu",
            "Chenyan Xiong",
            "Shi Yu",
            "Yukun Yan",
            "Shuo Wang",
            "Ge Yu"
        ],
        "published": "2024-02-25T11:07:08Z",
        "summary": "Large language models (LLMs) require lengthy prompts as the input context to\nproduce output aligned with user intentions, a process that incurs extra costs\nduring inference. In this paper, we propose the Gist COnditioned deCOding\n(Gist-COCO) model, introducing a novel method for compressing prompts which\nalso can assist the prompt interpretation and engineering. Gist-COCO employs an\nencoder-decoder based language model and then incorporates an additional\nencoder as a plugin module to compress prompts with inputs using gist tokens.\nIt finetunes the compression plugin module and uses the representations of gist\ntokens to emulate the raw prompts in the vanilla language model. By verbalizing\nthe representations of gist tokens into gist prompts, the compression ability\nof Gist-COCO can be generalized to different LLMs with high compression rates.\nOur experiments demonstrate that Gist-COCO outperforms previous prompt\ncompression models in both passage and instruction compression tasks. Further\nanalysis on gist verbalization results suggests that our gist prompts serve\ndifferent functions in aiding language models. They may directly provide\npotential answers, generate the chain-of-thought, or simply repeat the inputs.\nAll data and codes are available at https://github.com/OpenMatch/Gist-COCO .",
        "pdf_link": "https://arxiv.org/pdf/2402.16058v1.pdf"
    },
    {
        "title": "LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding",
        "authors": [
            "Yuxuan Wang",
            "Yueqian Wang",
            "Pengfei Wu",
            "Jianxin Liang",
            "Dongyan Zhao",
            "Zilong Zheng"
        ],
        "published": "2024-02-25T10:27:46Z",
        "summary": "Despite progress in video-language modeling, the computational challenge of\ninterpreting long-form videos in response to task-specific linguistic queries\npersists, largely due to the complexity of high-dimensional video data and the\nmisalignment between language and visual cues over space and time. To tackle\nthis issue, we introduce a novel approach called Language-guided\nSpatial-Temporal Prompt Learning (LSTP). This approach features two key\ncomponents: a Temporal Prompt Sampler (TPS) with optical flow prior that\nleverages temporal information to efficiently extract relevant video content,\nand a Spatial Prompt Solver (SPS) that adeptly captures the intricate spatial\nrelationships between visual and textual elements. By harmonizing TPS and SPS\nwith a cohesive training strategy, our framework significantly enhances\ncomputational efficiency, temporal understanding, and spatial-temporal\nalignment. Empirical evaluations across two challenging tasks--video question\nanswering and temporal question grounding in videos--using a variety of\nvideo-language pretrainings (VLPs) and large language models (LLMs) demonstrate\nthe superior performance, speed, and versatility of our proposed LSTP paradigm.",
        "pdf_link": "https://arxiv.org/pdf/2402.16050v1.pdf"
    },
    {
        "title": "LLMs with Chain-of-Thought Are Non-Causal Reasoners",
        "authors": [
            "Guangsheng Bao",
            "Hongbo Zhang",
            "Linyi Yang",
            "Cunxiang Wang",
            "Yue Zhang"
        ],
        "published": "2024-02-25T10:13:04Z",
        "summary": "This paper explores the role of the Chain of Thought (CoT) in Large Language\nModels (LLMs) reasoning. Despite its potential to improve task performance, our\nanalysis reveals a surprising frequency of correct answers following incorrect\nCoTs and vice versa. We employ causal analysis to assess the cause-effect\nrelationship between CoTs/instructions and answers in LLMs, uncovering the\nStructural Causal Model (SCM) that LLMs approximate. By comparing the implied\nSCM with that of human reasoning, we highlight discrepancies between LLM and\nhuman reasoning processes. We further examine the factors influencing the\ncausal structure of the implied SCM, revealing that in-context learning,\nsupervised fine-tuning, and reinforcement learning on human feedback\nsignificantly impact the causal relations. We release the code and results at\nhttps://github.com/StevenZHB/CoT_Causal_Analysis.",
        "pdf_link": "https://arxiv.org/pdf/2402.16048v1.pdf"
    },
    {
        "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
        "authors": [
            "Shuhai Zhang",
            "Yiliao Song",
            "Jiahao Yang",
            "Yuanqing Li",
            "Bo Han",
            "Mingkui Tan"
        ],
        "published": "2024-02-25T09:44:56Z",
        "summary": "Large language models (LLMs) such as ChatGPT have exhibited remarkable\nperformance in generating human-like texts. However, machine-generated texts\n(MGTs) may carry critical risks, such as plagiarism issues, misleading\ninformation, or hallucination issues. Therefore, it is very urgent and\nimportant to detect MGTs in many situations. Unfortunately, it is challenging\nto distinguish MGTs and human-written texts because the distributional\ndiscrepancy between them is often very subtle due to the remarkable performance\nof LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy}\n(MMD) to address this issue in the sense that MMD can well identify\ndistributional discrepancies. However, directly training a detector with MMD\nusing diverse MGTs will incur a significantly increased variance of MMD since\nMGTs may contain \\textit{multiple text populations} due to various LLMs. This\nwill severely impair MMD's ability to measure the difference between two\nsamples. To tackle this, we propose a novel \\textit{multi-population} aware\noptimization method for MMD called MMD-MP, which can \\textit{avoid variance\nincreases} and thus improve the stability to measure the distributional\ndiscrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and\nsentence-based detection, respectively. Extensive experiments on various LLMs,\n\\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The\nsource code is available at \\url{https://github.com/ZSHsh98/MMD-MP}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16041v2.pdf"
    },
    {
        "title": "EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings",
        "authors": [
            "Sunjun Kweon",
            "Jiyoun Kim",
            "Heeyoung Kwak",
            "Dongchul Cha",
            "Hangyul Yoon",
            "Kwanghyun Kim",
            "Seunghyun Won",
            "Edward Choi"
        ],
        "published": "2024-02-25T09:41:50Z",
        "summary": "This study introduces EHRNoteQA, a novel patient-specific question answering\nbenchmark tailored for evaluating Large Language Models (LLMs) in clinical\nenvironments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three\nmedical professionals has curated the dataset comprising 962 unique questions,\neach linked to a specific patient's EHR clinical notes. What makes EHRNoteQA\ndistinct from existing EHR-based benchmarks is as follows: Firstly, it is the\nfirst dataset to adopt a multi-choice question answering format, a design\nchoice that effectively evaluates LLMs with reliable scores in the context of\nautomatic evaluation, compared to other formats. Secondly, it requires an\nanalysis of multiple clinical notes to answer a single question, reflecting the\ncomplex nature of real-world clinical decision-making where clinicians review\nextensive records of patient histories. Our comprehensive evaluation on various\nlarge language models showed that their scores on EHRNoteQA correlate more\nclosely with their performance in addressing real-world medical questions\nevaluated by clinicians than their scores from other LLM benchmarks. This\nunderscores the significance of EHRNoteQA in evaluating LLMs for medical\napplications and highlights its crucial role in facilitating the integration of\nLLMs into healthcare systems. The dataset will be made available to the public\nunder PhysioNet credential access, promoting further research in this vital\nfield.",
        "pdf_link": "https://arxiv.org/pdf/2402.16040v2.pdf"
    },
    {
        "title": "Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations",
        "authors": [
            "Yafei Xiang",
            "Hanyi Yu",
            "Yulu Gong",
            "Shuning Huo",
            "Mengran Zhu"
        ],
        "published": "2024-02-25T09:19:11Z",
        "summary": "With the rapid development of artificial intelligence technology, Transformer\nstructural pre-training model has become an important tool for large language\nmodel (LLM) tasks. In the field of e-commerce, these models are especially\nwidely used, from text understanding to generating recommendation systems,\nwhich provide powerful technical support for improving user experience and\noptimizing service processes. This paper reviews the core application scenarios\nof Transformer pre-training model in e-commerce text understanding and\nrecommendation generation, including but not limited to automatic generation of\nproduct descriptions, sentiment analysis of user comments, construction of\npersonalized recommendation system and automated processing of customer service\nconversations. Through a detailed analysis of the model's working principle,\nimplementation process, and application effects in specific cases, this paper\nemphasizes the unique advantages of pre-trained models in understanding complex\nuser intentions and improving the quality of recommendations. In addition, the\nchallenges and improvement directions for the future are also discussed, such\nas how to further improve the generalization ability of the model, the ability\nto handle large-scale data sets, and technical strategies to protect user\nprivacy. Ultimately, the paper points out that the application of Transformer\nstructural pre-training models in e-commerce has not only driven technological\ninnovation, but also brought substantial benefits to merchants and consumers,\nand looking forward, these models will continue to play a key role in\ne-commerce and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2402.16035v1.pdf"
    },
    {
        "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration",
        "authors": [
            "Xin Mao",
            "Feng-Lin Li",
            "Huimin Xu",
            "Wei Zhang",
            "Anh Tuan Luu"
        ],
        "published": "2024-02-25T08:45:10Z",
        "summary": "While Reinforcement Learning from Human Feedback (RLHF) significantly\nenhances the generation quality of Large Language Models (LLMs), recent studies\nhave raised concerns regarding the complexity and instability associated with\nthe Proximal Policy Optimization (PPO) algorithm, proposing a series of\norder-based calibration methods as viable alternatives. This paper delves\nfurther into current order-based methods, examining their inefficiencies in\nutilizing reward values and addressing misalignment issues. Building upon these\nfindings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration\n(VCB) method to better align LLMs with human preferences. Experimental results\ndemonstrate that VCB surpasses existing alignment methods on AI assistant and\nsummarization datasets, providing impressive generalizability, robustness, and\nstability in diverse settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.16030v1.pdf"
    },
    {
        "title": "GraphWiz: An Instruction-Following Language Model for Graph Problems",
        "authors": [
            "Nuo Chen",
            "Yuhan Li",
            "Jianheng Tang",
            "Jia Li"
        ],
        "published": "2024-02-25T08:41:32Z",
        "summary": "Large language models (LLMs) have achieved impressive success across several\nfields, but their proficiency in understanding and resolving complex graph\nproblems is less explored. To bridge this gap, we introduce GraphInstruct, a\nnovel and comprehensive instruction-tuning dataset designed to equip language\nmodels with the ability to tackle a broad spectrum of graph problems using\nexplicit reasoning paths. Utilizing GraphInstruct, we build GraphWiz, an\nopen-source language model capable of resolving various graph problem types\nwhile generating clear reasoning processes. To enhance the model's capability\nand reliability, we incorporate the Direct Preference Optimization (DPO)\nframework into the graph problem-solving context. The enhanced model,\nGraphWiz-DPO, achieves an average accuracy of 65% across nine tasks with\ndifferent complexity levels, surpassing GPT-4 which has an average accuracy of\n43.8%. Moreover, our research delves into the delicate balance between training\ndata volume and model performance, highlighting the potential for overfitting\nwith increased data. We also explore the transferability of the model's\nreasoning ability across different graph tasks, indicating the model's\nadaptability and practical application potential. Our investigation offers a\nnew blueprint and valuable insights for developing LLMs specialized in graph\nreasoning and problem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2402.16029v2.pdf"
    },
    {
        "title": "LoRA Meets Dropout under a Unified Framework",
        "authors": [
            "Sheng Wang",
            "Liheng Chen",
            "Jiyue Jiang",
            "Boyang Xue",
            "Lingpeng Kong",
            "Chuan Wu"
        ],
        "published": "2024-02-25T07:09:10Z",
        "summary": "With the remarkable capabilities, large language models (LLMs) have emerged\nas essential elements in numerous NLP applications, while parameter-efficient\nfinetuning, especially LoRA, has gained popularity as a lightweight approach\nfor model customization. Meanwhile, various dropout methods, initially designed\nfor full finetuning with all the parameters updated, alleviates overfitting\nassociated with excessive parameter redundancy. Hence, a possible contradiction\narises from negligible trainable parameters of LoRA and the effectiveness of\nprevious dropout methods, which has been largely overlooked. To fill this gap,\nwe first confirm that parameter-efficient LoRA is also overfitting-prone. We\nthen revisit transformer-specific dropout methods, and establish their\nequivalence and distinctions mathematically and empirically. Building upon this\ncomparative analysis, we introduce a unified framework for a comprehensive\ninvestigation, which instantiates these methods based on dropping position,\nstructural pattern and compensation measure. Through this framework, we reveal\nthe new preferences and performance comparisons of them when involved with\nlimited trainable parameters. This framework also allows us to amalgamate the\nmost favorable aspects into a novel dropout method named HiddenKey. Extensive\nexperiments verify the remarkable superiority and sufficiency of HiddenKey\nacross multiple models and tasks, which highlights it as the preferred approach\nfor high-performance and parameter-efficient finetuning of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.00812v1.pdf"
    },
    {
        "title": "From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings",
        "authors": [
            "Hao Wang",
            "Hao Li",
            "Minlie Huang",
            "Lei Sha"
        ],
        "published": "2024-02-25T06:46:27Z",
        "summary": "The safety defense methods of Large language models(LLMs) stays limited\nbecause the dangerous prompts are manually curated to just few known attack\ntypes, which fails to keep pace with emerging varieties. Recent studies found\nthat attaching suffixes to harmful instructions can hack the defense of LLMs\nand lead to dangerous outputs. This method, while effective, leaves a gap in\nunderstanding the underlying mechanics of such adversarial suffix due to the\nnon-readability and it can be relatively easily seen through by common defense\nmethods such as perplexity filters.To cope with this challenge, in this paper,\nwe propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that\nare able to translate the unreadable adversarial suffixes into coherent,\nreadable text, which makes it easier to understand and analyze the reasons\nbehind harmful content generation by large language models. We conducted\nexperiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's\nharmful instructions. The results indicate that our method achieves a much\nbetter attack success rate to existing techniques, while significantly\nenhancing the textual fluency of the prompts. In addition, our approach can be\ngeneralized into a broader method for generating transferable adversarial\nsuffixes that can successfully attack multiple LLMs, even black-box LLMs, such\nas ChatGPT and Gemini. As a result, the prompts generated through our method\nexhibit enriched semantic diversity, which potentially provides more\nadversarial examples for LLM defense methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.16006v1.pdf"
    },
    {
        "title": "Likelihood-based Mitigation of Evaluation Bias in Large Language Models",
        "authors": [
            "Masanari Ohi",
            "Masahiro Kaneko",
            "Ryuto Koike",
            "Mengsay Loem",
            "Naoaki Okazaki"
        ],
        "published": "2024-02-25T04:52:02Z",
        "summary": "Large Language Models (LLMs) are widely used to evaluate natural language\ngeneration tasks as automated metrics. However, the likelihood, a measure of\nLLM's plausibility for a sentence, can vary due to superficial differences in\nsentences, such as word order and sentence structure. It is therefore possible\nthat there might be a likelihood bias if LLMs are used for evaluation: they\nmight overrate sentences with higher likelihoods while underrating those with\nlower likelihoods. In this paper, we investigate the presence and impact of\nlikelihood bias in LLM-based evaluators. We also propose a method to mitigate\nthe likelihood bias. Our method utilizes highly biased instances as few-shot\nexamples for in-context learning. Our experiments in evaluating the\ndata-to-text and grammatical error correction tasks reveal that several LLMs we\ntest display a likelihood bias. Furthermore, our proposed method successfully\nmitigates this bias, also improving evaluation performance (in terms of\ncorrelation of models with human scores) significantly.",
        "pdf_link": "https://arxiv.org/pdf/2402.15987v2.pdf"
    },
    {
        "title": "Cognitive Bias in High-Stakes Decision-Making with LLMs",
        "authors": [
            "Jessica Echterhoff",
            "Yao Liu",
            "Abeer Alessa",
            "Julian McAuley",
            "Zexue He"
        ],
        "published": "2024-02-25T02:35:56Z",
        "summary": "Large language models (LLMs) offer significant potential as tools to support\nan expanding range of decision-making tasks. However, given their training on\nhuman (created) data, LLMs can inherit both societal biases against protected\ngroups, as well as be subject to cognitive bias. Such human-like bias can\nimpede fair and explainable decisions made with LLM assistance. Our work\nintroduces BiasBuster, a framework designed to uncover, evaluate, and mitigate\ncognitive bias in LLMs, particularly in high-stakes decision-making tasks.\nInspired by prior research in psychology and cognitive sciences, we develop a\ndataset containing 16,800 prompts to evaluate different cognitive biases (e.g.,\nprompt-induced, sequential, inherent). We test various bias mitigation\nstrategies, amidst proposing a novel method using LLMs to debias their own\nprompts. Our analysis provides a comprehensive picture on the presence and\neffects of cognitive bias across different commercial and open-source models.\nWe demonstrate that our self-help debiasing effectively mitigate cognitive bias\nwithout having to manually craft examples for each bias type.",
        "pdf_link": "https://arxiv.org/pdf/2403.00811v1.pdf"
    },
    {
        "title": "Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale",
        "authors": [
            "Dan Zhao",
            "Siddharth Samsi",
            "Joseph McDonald",
            "Baolin Li",
            "David Bestor",
            "Michael Jones",
            "Devesh Tiwari",
            "Vijay Gadepally"
        ],
        "published": "2024-02-25T02:22:34Z",
        "summary": "As research and deployment of AI grows, the computational burden to support\nand sustain its progress inevitably does too. To train or fine-tune\nstate-of-the-art models in NLP, computer vision, etc., some form of AI hardware\nacceleration is virtually a requirement. Recent large language models require\nconsiderable resources to train and deploy, resulting in significant energy\nusage, potential carbon emissions, and massive demand for GPUs and other\nhardware accelerators. However, this surge carries large implications for\nenergy sustainability at the HPC/datacenter level. In this paper, we study the\naggregate effect of power-capping GPUs on GPU temperature and power draw at a\nresearch supercomputing center. With the right amount of power-capping, we show\nsignificant decreases in both temperature and power draw, reducing power\nconsumption and potentially improving hardware life-span with minimal impact on\njob performance. While power-capping reduces power draw by design, the\naggregate system-wide effect on overall energy consumption is less clear; for\ninstance, if users notice job performance degradation from GPU power-caps, they\nmay request additional GPU-jobs to compensate, negating any energy savings or\neven worsening energy consumption. To our knowledge, our work is the first to\nconduct and make available a detailed analysis of the effects of GPU\npower-capping at the supercomputing scale. We hope our work will inspire\nHPCs/datacenters to further explore, evaluate, and communicate the impact of\npower-capping AI hardware accelerators for more sustainable AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.18593v1.pdf"
    },
    {
        "title": "GreenLLaMA: A Framework for Detoxification with Explanations",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Muhammad Abdul-Mageed",
            "Laks V. S. Lakshmanan"
        ],
        "published": "2024-02-25T01:56:47Z",
        "summary": "Prior works on detoxification are scattered in the sense that they do not\ncover all aspects of detoxification needed in a real-world scenario. Notably,\nprior works restrict the task of developing detoxification models to only a\nseen subset of platforms, leaving the question of how the models would perform\non unseen platforms unexplored. Additionally, these works do not address\nnon-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified\nwithout altering the meaning. We propose GreenLLaMA, the first comprehensive\nend-to-end detoxification framework, which attempts to alleviate the\naforementioned limitations. We first introduce a cross-platform pseudo-parallel\ncorpus applying multi-step data processing and generation strategies leveraging\nChatGPT. We then train a suite of detoxification models with our cross-platform\ncorpus. We show that our detoxification models outperform the SoTA model\ntrained with human-annotated parallel corpus. We further introduce explanation\nto promote transparency and trustworthiness. GreenLLaMA additionally offers a\nunique paraphrase detector especially dedicated for the detoxification task to\ntackle the non-detoxifiable cases. Through experimental analysis, we\ndemonstrate the effectiveness of our cross-platform corpus and the robustness\nof GreenLLaMA against adversarial toxicity.",
        "pdf_link": "https://arxiv.org/pdf/2402.15951v1.pdf"
    },
    {
        "title": "Bootstrapping Cognitive Agents with a Large Language Model",
        "authors": [
            "Feiyu Zhu",
            "Reid Simmons"
        ],
        "published": "2024-02-25T01:40:30Z",
        "summary": "Large language models contain noisy general knowledge of the world, yet are\nhard to train or fine-tune. On the other hand cognitive architectures have\nexcellent interpretability and are flexible to update but require a lot of\nmanual work to instantiate. In this work, we combine the best of both worlds:\nbootstrapping a cognitive-based model with the noisy knowledge encoded in large\nlanguage models. Through an embodied agent doing kitchen tasks, we show that\nour proposed framework yields better efficiency compared to an agent based\nentirely on large language models. Our experiments indicate that large language\nmodels are a good source of information for cognitive architectures, and the\ncognitive architecture in turn can verify and update the knowledge of large\nlanguage models to a specific domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.00810v1.pdf"
    },
    {
        "title": "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
        "authors": [
            "Lily Zhong",
            "Zilong Wang",
            "Jingbo Shang"
        ],
        "published": "2024-02-25T00:56:27Z",
        "summary": "Large language models (LLMs) are leading significant progress in code\ngeneration. Beyond one-pass code generation, recent works further integrate\nunit tests and program verifiers into LLMs to iteratively refine the generated\nprograms. However, these works consider the generated programs as an\nindivisible entity, which falls short for LLMs in debugging the programs,\nespecially when the programs contain complex logic flows and data operations.\nIn contrast, when human developers debug programs, they typically set\nbreakpoints and selectively examine runtime execution information. The\nexecution flow and the intermediate variables play a crucial role in the\ndebugging process, yet they are underutilized in the existing literature on\ncode generation. In this study, we introduce Large Language Model Debugger\n(LDB), a novel debugging framework that enables LLMs to refine their generated\nprograms with the runtime execution information. Specifically, LDB segments the\nprograms into basic blocks and tracks the values of intermediate variables\nafter each block throughout the runtime execution. This allows LLMs to\nconcentrate on simpler code units within the overall execution flow, verify\ntheir correctness against the task description block by block, and efficiently\npinpoint any potential errors. Experiments demonstrate that LDB consistently\nenhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and\nTransCoder benchmarks, archiving new state-of-the-art performance in code\ndebugging for various LLM selections.",
        "pdf_link": "https://arxiv.org/pdf/2402.16906v3.pdf"
    },
    {
        "title": "Rethinking Software Engineering in the Foundation Model Era: A Curated Catalogue of Challenges in the Development of Trustworthy FMware",
        "authors": [
            "Ahmed E. Hassan",
            "Dayi Lin",
            "Gopi Krishnan Rajbahadur",
            "Keheliya Gallaba",
            "Filipe R. Cogo",
            "Boyuan Chen",
            "Haoxiang Zhang",
            "Kishanthan Thangarajah",
            "Gustavo Ansaldi Oliva",
            "Jiahuei Lin",
            "Wali Mohammad Abdullah",
            "Zhen Ming Jiang"
        ],
        "published": "2024-02-25T00:53:16Z",
        "summary": "Foundation models (FMs), such as Large Language Models (LLMs), have\nrevolutionized software development by enabling new use cases and business\nmodels. We refer to software built using FMs as FMware. The unique properties\nof FMware (e.g., prompts, agents, and the need for orchestration), coupled with\nthe intrinsic limitations of FMs (e.g., hallucination) lead to a completely new\nset of software engineering challenges. Based on our industrial experience, we\nidentified 10 key SE4FMware challenges that have caused enterprise FMware\ndevelopment to be unproductive, costly, and risky. In this paper, we discuss\nthese challenges in detail and state the path for innovation that we envision.\nNext, we present FMArts, which is our long-term effort towards creating a\ncradle-to-grave platform for the engineering of trustworthy FMware. Finally, we\n(i) show how the unique properties of FMArts enabled us to design and develop a\ncomplex FMware for a large customer in a timely manner and (ii) discuss the\nlessons that we learned in doing so. We hope that the disclosure of the\naforementioned challenges and our associated efforts to tackle them will not\nonly raise awareness but also promote deeper and further discussions, knowledge\nsharing, and innovative solutions across the software engineering discipline.",
        "pdf_link": "https://arxiv.org/pdf/2402.15943v2.pdf"
    },
    {
        "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Huanyu Liu",
            "Zhi Jin",
            "Ge Li"
        ],
        "published": "2024-02-24T23:54:41Z",
        "summary": "Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\ncontamination caused by the variants of test data. TED significantly mitigates\nperformance improvements up to 66.9\\% attributed to data contamination across\n24 settings and 21 contamination degrees. In real-world applications, we reveal\nthat ChatGPT exhibits a high potential to suffer from data contamination on\nHumanEval benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2402.15938v1.pdf"
    },
    {
        "title": "Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency",
        "authors": [
            "Min Zeng",
            "Jiexin Kuang",
            "Mengyang Qiu",
            "Jayoung Song",
            "Jungyeul Park"
        ],
        "published": "2024-02-24T23:17:56Z",
        "summary": "The writing examples of English language learners may be different from those\nof native speakers. Given that there is a significant differences in second\nlanguage (L2) learners' error types by their proficiency levels, this paper\nattempts to reduce overcorrection by examining the interaction between LLM's\nperformance and L2 language proficiency. Our method focuses on zero-shot and\nfew-shot prompting and fine-tuning models for GEC for learners of English as a\nforeign language based on the different proficiency. We investigate GEC results\nand find that overcorrection happens primarily in advanced language learners'\nwriting (proficiency C) rather than proficiency A (a beginner level) and\nproficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot\nprompting with writing examples of English learners, actually tend to exhibit\ndecreased recall measures. To make our claim concrete, we conduct a\ncomprehensive examination of GEC outcomes and their evaluation results based on\nlanguage proficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.15930v1.pdf"
    },
    {
        "title": "QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs",
        "authors": [
            "Isha Chaudhary",
            "Vedaant V. Jain",
            "Gagandeep Singh"
        ],
        "published": "2024-02-24T23:16:57Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nseveral benchmarks. However, traditional studies do not provide formal\nguarantees on the performance of LLMs. In this work, we propose a novel\ncertification framework for LLM, QuaCer-C, wherein we formally certify the\nknowledge-comprehension capabilities of popular LLMs. Our certificates are\nquantitative - they consist of high-confidence, tight bounds on the probability\nthat the target LLM gives the correct answer on any relevant knowledge\ncomprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs\nindicate that the knowledge comprehension capability improves with an increase\nin the number of parameters and that the Mistral model is less performant than\nthe rest in this evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.15929v1.pdf"
    },
    {
        "title": "Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis",
        "authors": [
            "Raven Rothkopf",
            "Hannah Tongxin Zeng",
            "Mark Santolucito"
        ],
        "published": "2024-02-24T21:36:26Z",
        "summary": "The surge in popularity of Large Language Models (LLMs) has opened doors for\nnew approaches to the creation of interactive agents. However, managing the\ntemporal behavior of such agents over the course of an interaction remains\nchallenging. The stateful, long-term horizon and quantitative reasoning\nrequired for coherent agent behavior does not fit well into the LLM paradigm.\nWe propose a combination of formal logic-based program synthesis and LLM\ncontent generation to create generative agents that adhere to temporal\nconstraints. Our approach uses Temporal Stream Logic (TSL) to generate an\nautomaton that enforces a temporal structure on an agent and leaves the details\nof each action for a moment in time to an LLM. By using TSL, we are able to\naugment the generative agent where users have a higher level of guarantees on\nbehavior, better interpretability of the system, and more ability to build\nagents in a modular way. We evaluate our approach on different tasks involved\nin creating a coherent interactive agent specialized for various application\ndomains. We found that over all of the tasks, our approach using TSL achieves\nat least 96% adherence, whereas the pure LLM-based approach demonstrates as low\nas 14.67% adherence.",
        "pdf_link": "https://arxiv.org/pdf/2402.16905v1.pdf"
    },
    {
        "title": "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails",
        "authors": [
            "Neal Mangaokar",
            "Ashish Hooda",
            "Jihye Choi",
            "Shreyas Chandrashekaran",
            "Kassem Fawaz",
            "Somesh Jha",
            "Atul Prakash"
        ],
        "published": "2024-02-24T21:27:13Z",
        "summary": "Large language models (LLMs) are typically aligned to be harmless to humans.\nUnfortunately, recent work has shown that such models are susceptible to\nautomated jailbreak attacks that induce them to generate harmful content. More\nrecent LLMs often incorporate an additional layer of defense, a Guard Model,\nwhich is a second LLM that is designed to check and moderate the output\nresponse of the primary LLM. Our key contribution is to show a novel attack\nstrategy, PRP, that is successful against several open-source (e.g., Llama 2)\nand closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP\nleverages a two step prefix-based attack that operates by (a) constructing a\nuniversal adversarial prefix for the Guard Model, and (b) propagating this\nprefix to the response. We find that this procedure is effective across\nmultiple threat models, including ones in which the adversary has no access to\nthe Guard Model at all. Our work suggests that further advances are required on\ndefenses and Guard Models before they can be considered effective.",
        "pdf_link": "https://arxiv.org/pdf/2402.15911v1.pdf"
    },
    {
        "title": "Multimodal Instruction Tuning with Conditional Mixture of LoRA",
        "authors": [
            "Ying Shen",
            "Zhiyang Xu",
            "Qifan Wang",
            "Yu Cheng",
            "Wenpeng Yin",
            "Lifu Huang"
        ],
        "published": "2024-02-24T20:15:31Z",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nproficiency in diverse tasks across different domains, with an increasing focus\non improving their zero-shot generalization capabilities for unseen multimodal\ntasks. Multimodal instruction tuning has emerged as a successful strategy for\nachieving zero-shot generalization by fine-tuning pre-trained models on diverse\nmultimodal tasks through instructions. As MLLMs grow in complexity and size,\nthe need for parameter-efficient fine-tuning methods like Low-Rank Adaption\n(LoRA), which fine-tunes with a minimal set of parameters, becomes essential.\nHowever, applying LoRA in multimodal instruction tuning presents the challenge\nof task interference, which leads to performance degradation, especially when\ndealing with a broad array of multimodal tasks. To address this, this paper\nintroduces a novel approach that integrates multimodal instruction tuning with\nConditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically\nconstructing low-rank adaptation matrices tailored to the unique demands of\neach input instance, aiming to mitigate task interference. Experimental results\non various multimodal evaluation datasets indicate that MixLoRA not only\noutperforms the conventional LoRA with the same or even higher ranks,\ndemonstrating its efficacy and adaptability in diverse multimodal tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.15896v1.pdf"
    },
    {
        "title": "SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection",
        "authors": [
            "Ayan Datta",
            "Aryan Chandramania",
            "Radhika Mamidi"
        ],
        "published": "2024-02-24T17:44:56Z",
        "summary": "This document contains the details of the authors' submission to the\nproceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and\nMultilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual)\nand B. Detection of machine-generated text is becoming an increasingly\nimportant task, with the advent of large language models (LLMs). In this paper,\nwe lay out how using weighted averages of RoBERTa layers lets us capture\ninformation about text that is relevant to machine-generated text detection.",
        "pdf_link": "https://arxiv.org/pdf/2402.15873v2.pdf"
    },
    {
        "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models",
        "authors": [
            "Haotian Xia",
            "Zhengbang Yang",
            "Yuqing Wang",
            "Rhys Tracy",
            "Yun Zhao",
            "Dongdong Huang",
            "Zezhi Chen",
            "Yan Zhu",
            "Yuan-fang Wang",
            "Weining Shen"
        ],
        "published": "2024-02-24T17:12:10Z",
        "summary": "A deep understanding of sports, a field rich in strategic and dynamic\ncontent, is crucial for advancing Natural Language Processing (NLP). This holds\nparticular significance in the context of evaluating and advancing Large\nLanguage Models (LLMs), given the existing gap in specialized benchmarks. To\nbridge this gap, we introduce SportQA, a novel benchmark specifically designed\nfor evaluating LLMs in the context of sports understanding. SportQA encompasses\nover 70,000 multiple-choice questions across three distinct difficulty levels,\neach targeting different aspects of sports knowledge from basic historical\nfacts to intricate, scenario-based reasoning tasks. We conducted a thorough\nevaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms\nsupplemented by chain-of-thought (CoT) prompting. Our results reveal that while\nLLMs exhibit competent performance in basic sports knowledge, they struggle\nwith more complex, scenario-based sports reasoning, lagging behind human\nexpertise. The introduction of SportQA marks a significant step forward in NLP,\noffering a tool for assessing and enhancing sports understanding in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15862v1.pdf"
    },
    {
        "title": "MATHWELL: Generating Educational Math Word Problems at Scale",
        "authors": [
            "Bryan R Christ",
            "Jonathan Kropko",
            "Thomas Hartvigsen"
        ],
        "published": "2024-02-24T17:08:45Z",
        "summary": "Math word problems are critical K-8 educational tools, but writing them is\ntime-consuming and requires domain expertise. We suggest that language models\ncan support K-8 math education by automatically generating problems at scale.\nTo be educational, generated problems must be 1) solvable, 2) accurate, and 3)\nappropriate. Existing datasets are unlabeled for these criteria, making them\nill-suited for training problem generators. We introduce MATHWELL, a Llama-2\n(70B) model iteratively finetuned to generate K-8 math word problems using data\nfrom expert annotation. Using MATHWELL, we generate the largest English word\nproblem dataset with Program of Thought (PoT) rationales to date, containing\n20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40%\nhigher share of problems that have executable solutions and meet all criteria\nthan alternatives, with 74% of its problems with executable solutions being\nsolvable, accurate, and appropriate. We release our model, data, and\nannotations.",
        "pdf_link": "https://arxiv.org/pdf/2402.15861v3.pdf"
    },
    {
        "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation",
        "authors": [
            "Jiazhao Zhang",
            "Kunyu Wang",
            "Rongtao Xu",
            "Gengze Zhou",
            "Yicong Hong",
            "Xiaomeng Fang",
            "Qi Wu",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "published": "2024-02-24T16:39:16Z",
        "summary": "Vision-and-Language Navigation (VLN) stands as a key research problem of\nEmbodied AI, aiming at enabling agents to navigate in unseen environments\nfollowing linguistic instructions. In this field, generalization is a\nlong-standing challenge, either to out-of-distribution scenes or from Sim to\nReal. In this paper, we propose NaVid, a video-based large vision language\nmodel (VLM), to mitigate such a generalization gap. NaVid makes the first\nendeavour to showcase the capability of VLMs to achieve state-of-the-art level\nnavigation performance without any maps, odometer and depth inputs. Following\nhuman instruction, NaVid only requires an on-the-fly video stream from a\nmonocular RGB camera equipped on the robot to output the next-step action. Our\nformulation mimics how humans navigate and naturally gets rid of the problems\nintroduced by odometer noises, and the Sim2Real gaps from map or depth inputs.\nMoreover, our video-based approach can effectively encode the historical\nobservations of robots as spatio-temporal contexts for decision-making and\ninstruction following. We train NaVid with 550k navigation samples collected\nfrom VLN-CE trajectories, including action-planning and instruction-reasoning\nsamples, along with 665k large-scale web data. Extensive experiments show that\nNaVid achieves SOTA performance in simulation environments and the real world,\ndemonstrating superior cross-dataset and Sim2Real transfer. We thus believe our\nproposed VLM approach plans the next step for not only the navigation agents\nbut also this research field.",
        "pdf_link": "https://arxiv.org/pdf/2402.15852v4.pdf"
    },
    {
        "title": "Prompt Perturbation Consistency Learning for Robust Language Models",
        "authors": [
            "Yao Qiang",
            "Subhrangshu Nandi",
            "Ninareh Mehrabi",
            "Greg Ver Steeg",
            "Anoop Kumar",
            "Anna Rumshisky",
            "Aram Galstyan"
        ],
        "published": "2024-02-24T15:00:58Z",
        "summary": "Large language models (LLMs) have demonstrated impressive performance on a\nnumber of natural language processing tasks, such as question answering and\ntext summarization. However, their performance on sequence labeling tasks such\nas intent classification and slot filling (IC-SF), which is a central component\nin personal assistant systems, lags significantly behind discriminative models.\nFurthermore, there is a lack of substantive research on the robustness of LLMs\nto various perturbations in the input prompts. The contributions of this paper\nare three-fold. First, we show that fine-tuning sufficiently large LLMs can\nproduce IC-SF performance comparable to discriminative models. Next, we\nsystematically analyze the performance deterioration of those fine-tuned models\ndue to three distinct yet relevant types of input perturbations - oronyms,\nsynonyms, and paraphrasing. Finally, we propose an efficient mitigation\napproach, Prompt Perturbation Consistency Learning (PPCL), which works by\nregularizing the divergence between losses from clean and perturbed samples.\nOur experiments demonstrate that PPCL can recover on average 59% and 69% of the\nperformance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the\ndata augmentation approach while using ten times fewer augmented data samples.",
        "pdf_link": "https://arxiv.org/pdf/2402.15833v1.pdf"
    },
    {
        "title": "Linguistic Intelligence in Large Language Models for Telecommunications",
        "authors": [
            "Tasnim Ahmed",
            "Nicola Piovesan",
            "Antonio De Domenico",
            "Salimur Choudhury"
        ],
        "published": "2024-02-24T14:01:07Z",
        "summary": "Large Language Models (LLMs) have emerged as a significant advancement in the\nfield of Natural Language Processing (NLP), demonstrating remarkable\ncapabilities in language generation and other language-centric tasks. Despite\ntheir evaluation across a multitude of analytical and reasoning tasks in\nvarious scientific domains, a comprehensive exploration of their knowledge and\nunderstanding within the realm of natural language tasks in the\ntelecommunications domain is still needed. This study, therefore, seeks to\nevaluate the knowledge and understanding capabilities of LLMs within this\ndomain. To achieve this, we conduct an exhaustive zero-shot evaluation of four\nprominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer\nresources than ChatGPT, making them suitable for resource-constrained\nenvironments. Their performance is compared with state-of-the-art, fine-tuned\nmodels. To the best of our knowledge, this is the first work to extensively\nevaluate and compare the understanding of LLMs across multiple language-centric\ntasks in this domain. Our evaluation reveals that zero-shot LLMs can achieve\nperformance levels comparable to the current state-of-the-art fine-tuned\nmodels. This indicates that pretraining on extensive text corpora equips LLMs\nwith a degree of specialization, even within the telecommunications domain. We\nalso observe that no single LLM consistently outperforms others, and the\nperformance of different LLMs can fluctuate. Although their performance lags\nbehind fine-tuned models, our findings underscore the potential of LLMs as a\nvaluable resource for understanding various aspects of this field that lack\nlarge annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15818v1.pdf"
    },
    {
        "title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA",
        "authors": [
            "Sheng Wang",
            "Boyang Xue",
            "Jiacheng Ye",
            "Jiyue Jiang",
            "Liheng Chen",
            "Lingpeng Kong",
            "Chuan Wu"
        ],
        "published": "2024-02-24T13:39:05Z",
        "summary": "With the rapid scaling of large language models (LLMs), serving numerous\nLoRAs concurrently has become increasingly impractical, leading to unaffordable\ncosts and necessitating more parameter-efficient finetuning methods. In this\nwork, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA),\nan intra-layer sharing mechanism comprising four essential components:\nbroadcast reduction, rotation enhancement, partially-sharing refinement, and\nrectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its\nadvantages, and effectively circumvent the drawbacks of peer parameter-sharing\nmethods with superior model capacity, practical feasibility, and broad\napplicability. Empirical experiments demonstrate the remarkably higher\nparameter efficiency of PRoLoRA in both specific parameter budget and\nperformance target scenarios, and its scalability to larger LLMs. Notably, with\none time less trainable parameters, PRoLoRA still outperforms LoRA on multiple\ninstruction tuning datasets. Subsequently, an ablation study is conducted to\nvalidate the necessity of individual components and highlight the superiority\nof PRoLoRA over three potential variants. Hopefully, the conspicuously higher\nparameter efficiency can establish PRoLoRA as a resource-friendly alternative\nto LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.16902v1.pdf"
    },
    {
        "title": "Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method",
        "authors": [
            "Tian Xia",
            "Zhiwei He",
            "Tong Ren",
            "Yibo Miao",
            "Zhuosheng Zhang",
            "Yang Yang",
            "Rui Wang"
        ],
        "published": "2024-02-24T13:36:58Z",
        "summary": "Bargaining is an important and unique part of negotiation between humans. As\nLLM-driven agents learn to negotiate and act like real humans, how to evaluate\nagents' bargaining abilities remains an open problem. For the first time, we\nformally described the Bargaining task as an asymmetric incomplete information\ngame, defining the gains of the Buyer and Seller in multiple bargaining\nprocesses. It allows us to quantitatively assess an agent's performance in the\nBargain task. We collected a real product price dataset, AmazonHistoryPrice,\nand conducted evaluations of various LLM agents' bargaining abilities. We find\nthat playing a Buyer is much harder than a Seller, and increasing model size\ncan not effectively improve the Buyer's performance. To address the challenge,\nwe propose a novel approach called OG-Narrator that integrates a deterministic\nOffer Generator to control the price range of Buyer's offers, and an LLM\nNarrator to create natural language sentences for generated offers.\nExperimental results show that OG-Narrator improves the buyer's deal rates from\n26.67% to 88.88% and brings a ten times of multiplication of profits on all\nbaselines, even a model that has not been aligned.",
        "pdf_link": "https://arxiv.org/pdf/2402.15813v2.pdf"
    },
    {
        "title": "OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining",
        "authors": [
            "Fanjin Zhang",
            "Shijie Shi",
            "Yifan Zhu",
            "Bo Chen",
            "Yukuo Cen",
            "Jifan Yu",
            "Yelin Chen",
            "Lulu Wang",
            "Qingfei Zhao",
            "Yuqing Cheng",
            "Tianyi Han",
            "Yuwei An",
            "Dan Zhang",
            "Weng Lam Tam",
            "Kun Cao",
            "Yunhe Pang",
            "Xinyu Guan",
            "Huihui Yuan",
            "Jian Song",
            "Xiaoyan Li",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2024-02-24T13:15:54Z",
        "summary": "With the rapid proliferation of scientific literature, versatile academic\nknowledge services increasingly rely on comprehensive academic graph mining.\nDespite the availability of public academic graphs, benchmarks, and datasets,\nthese resources often fall short in multi-aspect and fine-grained annotations,\nare constrained to specific task types and domains, or lack underlying real\nacademic graphs. In this paper, we present OAG-Bench, a comprehensive,\nmulti-aspect, and fine-grained human-curated benchmark based on the Open\nAcademic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,\nand 120+ experimental results to date. We propose new data annotation\nstrategies for certain tasks and offer a suite of data pre-processing codes,\nalgorithm implementations, and standardized evaluation protocols to facilitate\nacademic graph mining. Extensive experiments reveal that even advanced\nalgorithms like large language models (LLMs) encounter difficulties in\naddressing key challenges in certain tasks, such as paper source tracing and\nscholar profiling. We also introduce the Open Academic Graph Challenge\n(OAG-Challenge) to encourage community input and sharing. We envisage that\nOAG-Bench can serve as a common ground for the community to evaluate and\ncompare algorithms in academic graph mining, thereby accelerating algorithm\ndevelopment and advancement in this field. OAG-Bench is accessible at\nhttps://www.aminer.cn/data/.",
        "pdf_link": "https://arxiv.org/pdf/2402.15810v1.pdf"
    },
    {
        "title": "Empowering Large Language Model Agents through Action Learning",
        "authors": [
            "Haiteng Zhao",
            "Chang Ma",
            "Guoyin Wang",
            "Jing Su",
            "Lingpeng Kong",
            "Jingjing Xu",
            "Zhi-Hong Deng",
            "Hongxia Yang"
        ],
        "published": "2024-02-24T13:13:04Z",
        "summary": "Large Language Model (LLM) Agents have recently garnered increasing interest\nyet they are limited in their ability to learn from trial and error, a key\nelement of intelligent behavior. In this work, we argue that the capacity to\nlearn new actions from experience is fundamental to the advancement of learning\nin LLM agents. While humans naturally expand their action spaces and develop\nskills through experiential learning, LLM agents typically operate within fixed\naction spaces, limiting their potential for growth. To address these\nchallenges, our study explores open-action learning for language agents. We\nintroduce a framework LearnAct with an iterative learning strategy to create\nand improve actions in the form of Python functions. In each iteration, LLM\nrevises and updates the currently available actions based on the errors\nidentified in unsuccessful training tasks, thereby enhancing action\neffectiveness. Our experimental evaluations across Robotic Planning and\nAlfworld environments reveal that after learning on a few training task\ninstances, our approach to open-action learning markedly improves agent\nperformance for the type of task (by 32 percent in AlfWorld compared to\nReAct+Reflexion, for instance) highlighting the importance of experiential\naction learning in the development of more intelligent LLM agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.15809v1.pdf"
    },
    {
        "title": "Enhancing Cloud-Based Large Language Model Processing with Elasticsearch and Transformer Models",
        "authors": [
            "Chunhe Ni",
            "Jiang Wu",
            "Hongbo Wang",
            "Wenran Lu",
            "Chenwei Zhang"
        ],
        "published": "2024-02-24T12:31:22Z",
        "summary": "Large Language Models (LLMs) are a class of generative AI models built using\nthe Transformer network, capable of leveraging vast datasets to identify,\nsummarize, translate, predict, and generate language. LLMs promise to\nrevolutionize society, yet training these foundational models poses immense\nchallenges. Semantic vector search within large language models is a potent\ntechnique that can significantly enhance search result accuracy and relevance.\nUnlike traditional keyword-based search methods, semantic search utilizes the\nmeaning and context of words to grasp the intent behind queries and deliver\nmore precise outcomes. Elasticsearch emerges as one of the most popular tools\nfor implementing semantic search an exceptionally scalable and robust search\nengine designed for indexing and searching extensive datasets. In this article,\nwe delve into the fundamentals of semantic search and explore how to harness\nElasticsearch and Transformer models to bolster large language model processing\nparadigms. We gain a comprehensive understanding of semantic search principles\nand acquire practical skills for implementing semantic search in real-world\nmodel application scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.00807v1.pdf"
    },
    {
        "title": "From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",
        "authors": [
            "Timothy R. McIntosh",
            "Teo Susnjak",
            "Tong Liu",
            "Paul Watters",
            "Raza Nowrozy",
            "Malka N. Halgamuge"
        ],
        "published": "2024-02-24T09:06:25Z",
        "summary": "This study investigated the integration readiness of four predominant\ncybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0,\nCOBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the\nopportunities, risks, and regulatory compliance when adopting Large Language\nModels (LLMs), using qualitative content analysis and expert validation. Our\nanalysis, with both LLMs and human experts in the loop, uncovered potential for\nLLM integration together with inadequacies in LLM risk oversight of those\nframeworks. Comparative gap analysis has highlighted that the new ISO\n42001:2023, specifically designed for Artificial Intelligence (AI) management\nsystems, provided most comprehensive facilitation for LLM opportunities,\nwhereas COBIT 2019 aligned most closely with the impending European Union AI\nAct. Nonetheless, our findings suggested that all evaluated frameworks would\nbenefit from enhancements to more effectively and more comprehensively address\nthe multifaceted risks associated with LLMs, indicating a critical and\ntime-sensitive need for their continuous evolution. We propose integrating\nhuman-expert-in-the-loop validation processes as crucial for enhancing\ncybersecurity frameworks to support secure and compliant LLM integration, and\ndiscuss implications for the continuous evolution of cybersecurity GRC\nframeworks to support the secure integration of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15770v1.pdf"
    },
    {
        "title": "Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models",
        "authors": [
            "Haoran Liao",
            "Jidong Tian",
            "Shaohua Hu",
            "Hao He",
            "Yaohui Jin"
        ],
        "published": "2024-02-24T08:40:30Z",
        "summary": "Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.",
        "pdf_link": "https://arxiv.org/pdf/2402.15764v2.pdf"
    },
    {
        "title": "Stepwise Self-Consistent Mathematical Reasoning with Large Language Models",
        "authors": [
            "Zilong Zhao",
            "Yao Rong",
            "Dongyang Guo",
            "Emek G\u00f6zl\u00fckl\u00fc",
            "Emir G\u00fclboy",
            "Enkelejda Kasneci"
        ],
        "published": "2024-02-24T08:22:39Z",
        "summary": "Using Large Language Models for complex mathematical reasoning is difficult,\nprimarily due to the complexity of multi-step reasoning. The main challenges of\nthis process include (1) selecting critical intermediate results to advance the\nprocedure, and (2) limited exploration of potential solutions. To address these\nissues, we introduce a novel algorithm, namely Stepwise Self-Consistent\nChain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting\nintermediate steps based on the intersection of various reasoning chains.\nAdditionally, SSC-CoT enables the model to discover critical intermediate steps\nby querying a knowledge graph comprising relevant domain knowledge. To validate\nSSC-CoT, we present a new dataset, TriMaster100, tailored for complex\ntrigonometry problems. This dataset contains 100 questions, with each solution\nbroken down into scored intermediate steps, facilitating a comprehensive\nevaluation of the mathematical reasoning process. On TriMaster100, SSC-CoT\ntriples the effectiveness of the state-of-the-art methods. Furthermore, we\nbenchmark SSC-CoT on the widely recognized complex mathematical question\ndataset, MATH level 5, and it surpasses the second-best method by 7.2% in\naccuracy. Code and the TriMaster100 dataset can be found at:\nhttps://github.com/zhao-zilong/ssc-cot.",
        "pdf_link": "https://arxiv.org/pdf/2402.17786v1.pdf"
    },
    {
        "title": "Increasing SAM Zero-Shot Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation",
        "authors": [
            "Zekun Jiang",
            "Dongjie Cheng",
            "Ziyuan Qin",
            "Jun Gao",
            "Qicheng Lao",
            "Kang Li",
            "Le Zhang"
        ],
        "published": "2024-02-24T08:10:54Z",
        "summary": "This study develops and evaluates a novel multimodal medical image zero-shot\nsegmentation algorithm named Text-Visual-Prompt SAM (TV-SAM) without any manual\nannotations. TV-SAM incorporates and integrates large language model GPT-4,\nVision Language Model GLIP, and Segment Anything Model (SAM), to autonomously\ngenerate descriptive text prompts and visual bounding box prompts from medical\nimages, thereby enhancing SAM for zero-shot segmentation. Comprehensive\nevaluations are implemented on seven public datasets encompassing eight imaging\nmodalities to demonstrate that TV-SAM can effectively segment unseen targets\nacross various modalities without additional training, significantly\noutperforming SAM AUTO and GSAM, closely matching the performance of SAM BBOX\nwith gold standard bounding box prompts, and surpassing the state-of-the-art on\nspecific datasets like ISIC and WBC. The study indicates that TV-SAM serves as\nan effective multimodal medical image zero-shot segmentation algorithm,\nhighlighting the significant contribution of GPT-4 to zero-shot segmentation.\nBy integrating foundational models such as GPT-4, GLIP, and SAM, it could\nenhance the capability to address complex problems in specialized domains. The\ncode is available at: https://github.com/JZK00/TV-SAM.",
        "pdf_link": "https://arxiv.org/pdf/2402.15759v1.pdf"
    },
    {
        "title": "Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens",
        "authors": [
            "Ziqian Zeng",
            "Jiahong Yu",
            "Qianshi Pang",
            "Zihao Wang",
            "Huiping Zhuang",
            "Cen Chen"
        ],
        "published": "2024-02-24T08:10:39Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, their widespread application is hindered by the\nresource-intensive decoding process. To address this challenge, current\napproaches have incorporated additional decoding heads to enable parallel\nprediction of multiple subsequent tokens, thereby achieving inference\nacceleration. Nevertheless, the accuracy of these decoding heads falls short of\nthe auto-regressive decoding approach.\n  In light of these limitations, we propose Chimera, a novel framework\nspecifically designed for speculative sampling. Within this framework, we\nintroduce a lightweight draft model that effectively utilizes previously\ngenerated tokens to predict subsequent words. To ensure both accuracy and\nefficiency, we present two strategies within the lightweight draft model.\nFirstly, we focus on capturing short-range dependencies at the bottom layer.\nSecondly, we leverage the readily available representations from the original\nLLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera\ndemonstrates impressive results, achieving an average latency speedup ratio of\n2.7x compared to the vanilla auto-regressive decoding approach. This highlights\nthe potential of our proposed framework in significantly improving the\nefficiency of large language models during the decoding process.",
        "pdf_link": "https://arxiv.org/pdf/2402.15758v1.pdf"
    },
    {
        "title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition",
        "authors": [
            "Yuxuan Liu",
            "Tianchi Yang",
            "Shaohan Huang",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang"
        ],
        "published": "2024-02-24T08:01:32Z",
        "summary": "Large language models (LLMs) have emerged as a promising alternative to\nexpensive human evaluations. However, the alignment and coverage of LLM-based\nevaluations are often limited by the scope and potential bias of the evaluation\nprompts and criteria. To address this challenge, we propose HD-Eval, a novel\nframework that iteratively aligns LLM-based evaluators with human preference\nvia Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the\nevaluation mindset of human experts and enhances the alignment of LLM-based\nevaluators by decomposing a given evaluation task into finer-grained criteria,\naggregating them according to estimated human preferences, pruning\ninsignificant criteria with attribution, and further decomposing significant\ncriteria. By integrating these steps within an iterative alignment training\nprocess, we obtain a hierarchical decomposition of criteria that\ncomprehensively captures aspects of natural language at multiple levels of\ngranularity. Implemented as a white box, the human preference-guided aggregator\nis efficient to train and more explainable than relying solely on prompting,\nand its independence from model parameters makes it applicable to closed-source\nLLMs. Extensive experiments on three evaluation domains demonstrate the\nsuperiority of HD-Eval in further aligning state-of-the-art evaluators and\nproviding deeper insights into the explanation of evaluation results and the\ntask itself.",
        "pdf_link": "https://arxiv.org/pdf/2402.15754v1.pdf"
    },
    {
        "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
        "authors": [
            "Yong Liu",
            "Zirui Zhu",
            "Chaoyu Gong",
            "Minhao Cheng",
            "Cho-Jui Hsieh",
            "Yang You"
        ],
        "published": "2024-02-24T07:22:04Z",
        "summary": "While fine-tuning large language models (LLMs) for specific tasks often\nyields impressive results, it comes at the cost of memory inefficiency due to\nback-propagation in gradient-based training. Memory-efficient Zeroth-order\n(MeZO) optimizers, recently proposed to address this issue, only require\nforward passes during training, making them more memory-friendly. However, the\nquality of gradient estimates in zeroth order optimization often depends on the\ndata dimensionality, potentially explaining why MeZO still exhibits significant\nperformance drops compared to standard fine-tuning across various tasks.\nInspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper\nintroduces Sparse MeZO, a novel memory-efficient zeroth-order optimization\napproach that applies ZO only to a carefully chosen subset of parameters. We\npropose a simple yet effective parameter selection scheme that yields\nsignificant performance gains with Sparse-MeZO. Additionally, we develop a\nmemory-optimized implementation for sparse masking, ensuring the algorithm\nrequires only inference-level memory consumption, allowing Sparse-MeZO to\nfine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that\nSparse-MeZO consistently improves both performance and convergence speed over\nMeZO without any overhead. For example, it achieves a 9\\% absolute accuracy\nimprovement and 3.5x speedup over MeZO on the RTE task.",
        "pdf_link": "https://arxiv.org/pdf/2402.15751v1.pdf"
    },
    {
        "title": "MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation",
        "authors": [
            "Han Wang",
            "Roy Ka-Wei Lee"
        ],
        "published": "2024-02-24T06:14:34Z",
        "summary": "Online memes have emerged as powerful digital cultural artifacts in the age\nof social media, offering not only humor but also platforms for political\ndiscourse, social critique, and information dissemination. Their extensive\nreach and influence in shaping online communities' sentiments make them\ninvaluable tools for campaigning and promoting ideologies. Despite the\ndevelopment of several meme-generation tools, there remains a gap in their\nsystematic evaluation and their ability to effectively communicate ideologies.\nAddressing this, we introduce MemeCraft, an innovative meme generator that\nleverages large language models (LLMs) and visual language models (VLMs) to\nproduce memes advocating specific social movements. MemeCraft presents an\nend-to-end pipeline, transforming user prompts into compelling multimodal memes\nwithout manual intervention. Conscious of the misuse potential in creating\ndivisive content, an intrinsic safety mechanism is embedded to curb hateful\nmeme production.",
        "pdf_link": "https://arxiv.org/pdf/2403.14652v1.pdf"
    },
    {
        "title": "How Do Humans Write Code? Large Models Do It the Same Way Too",
        "authors": [
            "Long Li"
        ],
        "published": "2024-02-24T05:40:01Z",
        "summary": "Large Language Models (LLMs) often make errors when performing numerical\ncalculations. In contrast to traditional chain-of-thought reasoning, the\nprogram-of-thoughts approach involves generating executable code to solve\nproblems. By executing this code, it achieves more precise results. Using\ngenerated executable code instead of natural language can reduce computational\nerrors. However, we observe that when LLMs solve mathematical problems using\ncode, they tend to generate more incorrect reasoning than when using natural\nlanguage. To address this issue, we propose Human-Think Language (HTL), a\nstraightforward yet highly efficient approach inspired by human coding\npractices. The approach first generates problem-solving methods described in\nthe natural language by the model, then converts them into code, mirroring the\nprocess where people think through the logic in natural language before writing\nit as code. Additionally, it utilizes the Proximal Policy Optimization (PPO)\nalgorithm, enabling it to provide feedback to itself based on the correctness\nof mathematical answers, much like humans do. Finally, we introduce a\nfocus-attention mechanism that masks the question segment, enhancing its\nreliance on natural language inference solutions during code generation. We\nconduct our experiments without introducing any additional information, and the\nresults across five mathematical calculation datasets showcase the\neffectiveness of our approach. Notably, on the NumGLUE dataset, the\nLlaMA-2-7B-based model achieves a superior performance rate (75.1%) compared to\nthe previous best performance with the LlaMA-2-70B model (74.4%).",
        "pdf_link": "https://arxiv.org/pdf/2402.15729v1.pdf"
    },
    {
        "title": "LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper",
        "authors": [
            "Daoyuan Wu",
            "Shuai Wang",
            "Yang Liu",
            "Ning Liu"
        ],
        "published": "2024-02-24T05:34:43Z",
        "summary": "Jailbreaking is an emerging adversarial attack that bypasses the safety\nalignment deployed in off-the-shelf large language models (LLMs). A\nconsiderable amount of research exists proposing more effective jailbreak\nattacks, including the recent Greedy Coordinate Gradient (GCG) attack,\njailbreak template-based attacks such as using \"Do-Anything-Now\" (DAN), and\nmultilingual jailbreak. In contrast, the defensive side has been relatively\nless explored. This paper proposes a lightweight yet practical defense called\nSELFDEFEND, which can defend against all existing jailbreak attacks with\nminimal delay for jailbreak prompts and negligible delay for normal user\nprompts. Our key insight is that regardless of the kind of jailbreak strategies\nemployed, they eventually need to include a harmful prompt (e.g., \"how to make\na bomb\") in the prompt sent to LLMs, and we found that existing LLMs can\neffectively recognize such harmful prompts that violate their safety policies.\nBased on this insight, we design a shadow stack that concurrently checks\nwhether a harmful prompt exists in the user prompt and triggers a checkpoint in\nthe normal stack once a token of \"No\" or a harmful prompt is output. The latter\ncould also generate an explainable LLM response to adversarial prompts. We\ndemonstrate our idea of SELFDEFEND works in various jailbreak scenarios through\nmanual analysis in GPT-3.5/4. We also list three future directions to further\nenhance SELFDEFEND.",
        "pdf_link": "https://arxiv.org/pdf/2402.15727v2.pdf"
    },
    {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
        "authors": [
            "Chaoya Jiang",
            "Wei Ye",
            "Mengfan Dong",
            "Hongrui Jia",
            "Haiyang Xu",
            "Ming Yan",
            "Ji Zhang",
            "Shikun Zhang"
        ],
        "published": "2024-02-24T05:14:52Z",
        "summary": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15721v1.pdf"
    },
    {
        "title": "Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors",
        "authors": [
            "Shengkun Ma",
            "Jiale Han",
            "Yi Liang",
            "Bo Cheng"
        ],
        "published": "2024-02-24T04:32:44Z",
        "summary": "Continual Few-shot Relation Extraction (CFRE) is a practical problem that\nrequires the model to continuously learn novel relations while avoiding\nforgetting old ones with few labeled training data. The primary challenges are\ncatastrophic forgetting and overfitting. This paper harnesses prompt learning\nto explore the implicit capabilities of pre-trained language models to address\nthe above two challenges, thereby making language models better continual\nfew-shot relation extractors. Specifically, we propose a Contrastive Prompt\nLearning framework, which designs prompt representation to acquire more\ngeneralized knowledge that can be easily adapted to old and new categories, and\nmargin-based contrastive learning to focus more on hard samples, therefore\nalleviating catastrophic forgetting and overfitting issues. To further remedy\noverfitting in low-resource scenarios, we introduce an effective memory\naugmentation strategy that employs well-crafted prompts to guide ChatGPT in\ngenerating diverse samples. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods by a large margin and significantly\nmitigates catastrophic forgetting and overfitting in low-resource scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.15713v1.pdf"
    },
    {
        "title": "Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology",
        "authors": [
            "Zhenhua Wang",
            "Wei Xie",
            "Baosheng Wang",
            "Enze Wang",
            "Zhiwen Gui",
            "Shuoyoucheng Ma",
            "Kai Chen"
        ],
        "published": "2024-02-24T02:27:55Z",
        "summary": "Large Language Models (LLMs) have gradually become the gateway for people to\nacquire new knowledge. However, attackers can break the model's security\nprotection (\"jail\") to access restricted information, which is called\n\"jailbreaking.\" Previous studies have shown the weakness of current LLMs when\nconfronted with such jailbreaking attacks. Nevertheless, comprehension of the\nintrinsic decision-making mechanism within the LLMs upon receipt of jailbreak\nprompts is noticeably lacking. Our research provides a psychological\nexplanation of the jailbreak prompts. Drawing on cognitive consistency theory,\nwe argue that the key to jailbreak is guiding the LLM to achieve cognitive\ncoordination in an erroneous direction. Further, we propose an automatic\nblack-box jailbreaking method based on the Foot-in-the-Door (FITD) technique.\nThis method progressively induces the model to answer harmful questions via\nmulti-step incremental prompts. We instantiated a prototype system to evaluate\nthe jailbreaking effectiveness on 8 advanced LLMs, yielding an average success\nrate of 83.9%. This study builds a psychological perspective on the explanatory\ninsights into the intrinsic decision-making logic of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15690v1.pdf"
    },
    {
        "title": "Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study",
        "authors": [
            "Zhaoyue Sun",
            "Gabriele Pergola",
            "Byron C. Wallace",
            "Yulan He"
        ],
        "published": "2024-02-24T00:38:29Z",
        "summary": "With the advent of large language models (LLMs), there has been growing\ninterest in exploring their potential for medical applications. This research\naims to investigate the ability of LLMs, specifically ChatGPT, in the context\nof pharmacovigilance event extraction, of which the main goal is to identify\nand extract adverse events or potential therapeutic events from textual medical\nsources. We conduct extensive experiments to assess the performance of ChatGPT\nin the pharmacovigilance event extraction task, employing various prompts and\ndemonstration selection strategies. The findings demonstrate that while ChatGPT\ndemonstrates reasonable performance with appropriate demonstration selection\nstrategies, it still falls short compared to fully fine-tuned small models.\nAdditionally, we explore the potential of leveraging ChatGPT for data\naugmentation. However, our investigation reveals that the inclusion of\nsynthesized data into fine-tuning may lead to a decrease in performance,\npossibly attributed to noise in the ChatGPT-generated labels. To mitigate this,\nwe explore different filtering strategies and find that, with the proper\napproach, more stable performance can be achieved, although constant\nimprovement remains elusive.",
        "pdf_link": "https://arxiv.org/pdf/2402.15663v1.pdf"
    },
    {
        "title": "Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics",
        "authors": [
            "Sadaf Ghaffari",
            "Nikhil Krishnaswamy"
        ],
        "published": "2024-02-24T00:01:01Z",
        "summary": "In this paper, we present an exploration of LLMs' abilities to problem solve\nwith physical reasoning in situated environments. We construct a simple\nsimulated environment and demonstrate examples of where, in a zero-shot\nsetting, both text and multimodal LLMs display atomic world knowledge about\nvarious objects but fail to compose this knowledge in correct solutions for an\nobject manipulation and placement task. We also use BLIP, a vision-language\nmodel trained with more sophisticated cross-modal attention, to identify cases\nrelevant to object physical properties that that model fails to ground.\nFinally, we present a procedure for discovering the relevant properties of\nobjects in the environment and propose a method to distill this knowledge back\ninto the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.15654v1.pdf"
    },
    {
        "title": "On Trojan Signatures in Large Language Models of Code",
        "authors": [
            "Aftab Hussain",
            "Md Rafiqul Islam Rabin",
            "Mohammad Amin Alipour"
        ],
        "published": "2024-02-23T22:48:29Z",
        "summary": "Trojan signatures, as described by Fields et al. (2021), are noticeable\ndifferences in the distribution of the trojaned class parameters (weights) and\nthe non-trojaned class parameters of the trojaned model, that can be used to\ndetect the trojaned model. Fields et al. (2021) found trojan signatures in\ncomputer vision classification tasks with image models, such as, Resnet,\nWideResnet, Densenet, and VGG. In this paper, we investigate such signatures in\nthe classifier layer parameters of large language models of source code.\n  Our results suggest that trojan signatures could not generalize to LLMs of\ncode. We found that trojaned code models are stubborn, even when the models\nwere poisoned under more explicit settings (finetuned with pre-trained weights\nfrozen). We analyzed nine trojaned models for two binary classification tasks:\nclone and defect detection. To the best of our knowledge, this is the first\nwork to examine weight-based trojan signature revelation techniques for\nlarge-language models of code and furthermore to demonstrate that detecting\ntrojans only from the weights in such models is a hard problem.",
        "pdf_link": "https://arxiv.org/pdf/2402.16896v2.pdf"
    },
    {
        "title": "Fine-Grained Self-Endorsement Improves Factuality and Reasoning",
        "authors": [
            "Ante Wang",
            "Linfeng Song",
            "Baolin Peng",
            "Ye Tian",
            "Lifeng Jin",
            "Haitao Mi",
            "Jinsong Su",
            "Dong Yu"
        ],
        "published": "2024-02-23T22:24:40Z",
        "summary": "This work studies improving large language model (LLM) generations at\ninference time by mitigating fact-conflicting hallucinations. Particularly, we\npropose a self-endorsement framework that leverages the fine-grained fact-level\ncomparisons across multiple sampled responses. Compared with prior ensemble\nmethods (Wang et al., 2022;Chen et al., 2023)) that perform response-level\nselection, our approach can better alleviate hallucinations, especially for\nlongform generation tasks. Our approach can broadly benefit smaller and\nopen-source LLMs as it mainly conducts simple content-based comparisons.\nExperiments on Biographies show that our method can effectively improve the\nfactuality of generations with simple and intuitive prompts across different\nscales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K\ndemonstrate the potential of self-endorsement for broader application.",
        "pdf_link": "https://arxiv.org/pdf/2402.15631v1.pdf"
    },
    {
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
        "authors": [
            "Ziheng Jiang",
            "Haibin Lin",
            "Yinmin Zhong",
            "Qi Huang",
            "Yangrui Chen",
            "Zhi Zhang",
            "Yanghua Peng",
            "Xiang Li",
            "Cong Xie",
            "Shibiao Nong",
            "Yulu Jia",
            "Sun He",
            "Hongmin Chen",
            "Zhihao Bai",
            "Qi Hou",
            "Shipeng Yan",
            "Ding Zhou",
            "Yiyao Sheng",
            "Zhuo Jiang",
            "Haohan Xu",
            "Haoran Wei",
            "Zhang Zhang",
            "Pengfei Nie",
            "Leqi Zou",
            "Sida Zhao",
            "Liang Xiang",
            "Zherui Liu",
            "Zhe Li",
            "Xiaoying Jia",
            "Jianxi Ye",
            "Xin Jin",
            "Xin Liu"
        ],
        "published": "2024-02-23T22:10:59Z",
        "summary": "We present the design, implementation and engineering experience in building\nand deploying MegaScale, a production system for training large language models\n(LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale\nbrings unprecedented challenges to training efficiency and stability. We take a\nfull-stack approach that co-designs the algorithmic and system components\nacross model block and optimizer design, computation and communication\noverlapping, operator optimization, data pipeline, and network performance\ntuning. Maintaining high efficiency throughout the training process (i.e.,\nstability) is an important consideration in production given the long extent of\nLLM training jobs. Many hard stability issues only emerge at large scale, and\nin-depth observability is the key to address them. We develop a set of\ndiagnosis tools to monitor system components and events deep in the stack,\nidentify root causes, and derive effective techniques to achieve fault\ntolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs\nUtilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the\nMFU by 1.34x compared to Megatron-LM. We share our operational experience in\nidentifying and fixing failures and stragglers. We hope by articulating the\nproblems and sharing our experience from a systems perspective, this work can\ninspire future LLM systems research.",
        "pdf_link": "https://arxiv.org/pdf/2402.15627v1.pdf"
    },
    {
        "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
        "authors": [
            "Artem Vysogorets",
            "Achintya Gopal"
        ],
        "published": "2024-02-23T21:28:59Z",
        "summary": "Fine-tuning Large Language Models (LLMs) is now a common approach for text\nclassification in a wide range of applications. When labeled documents are\nscarce, active learning helps save annotation efforts but requires retraining\nof massive models on each acquisition iteration. We drastically expedite this\nprocess by using pretrained representations of LLMs within the active learning\nloop and, once the desired amount of labeled data is acquired, fine-tuning that\nor even a different pretrained LLM on this labeled data to achieve the best\nperformance. As verified on common text classification benchmarks with\npretrained BERT and RoBERTa as the backbone, our strategy yields similar\nperformance to fine-tuning all the way through the active learning loop but is\norders of magnitude less computationally expensive. The data acquired with our\nprocedure generalizes across pretrained networks, allowing flexibility in\nchoosing the final model or updating it as newer versions get released.",
        "pdf_link": "https://arxiv.org/pdf/2402.15613v1.pdf"
    },
    {
        "title": "Selective \"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning",
        "authors": [
            "Tejas Srinivasan",
            "Jack Hessel",
            "Tanmay Gupta",
            "Bill Yuchen Lin",
            "Yejin Choi",
            "Jesse Thomason",
            "Khyathi Raghavi Chandu"
        ],
        "published": "2024-02-23T21:16:52Z",
        "summary": "Prior work on selective prediction minimizes incorrect predictions from\nvision-language models (VLMs) by allowing them to abstain from answering when\nuncertain. However, when deploying a vision-language system with low tolerance\nfor inaccurate predictions, selective prediction may be over-cautious and\nabstain too frequently, even on many correct predictions. We introduce\nReCoVERR, an inference-time algorithm to reduce the over-abstention of a\nselective vision-language system without decreasing prediction accuracy. When\nthe VLM makes a low-confidence prediction, instead of abstaining ReCoVERR tries\nto find relevant clues in the image that provide additional evidence for the\nprediction. ReCoVERR uses an LLM to pose related questions to the VLM, collects\nhigh-confidence evidences, and if enough evidence confirms the prediction the\nsystem makes a prediction instead of abstaining. ReCoVERR enables two VLMs,\nBLIP2 and InstructBLIP, to answer up to 20% more questions on the A-OKVQA task\nthan vanilla selective prediction without decreasing system accuracy, thus\nimproving overall system reliability.",
        "pdf_link": "https://arxiv.org/pdf/2402.15610v1.pdf"
    },
    {
        "title": "Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis",
        "authors": [
            "Hongkang Li",
            "Meng Wang",
            "Songtao Lu",
            "Xiaodong Cui",
            "Pin-Yu Chen"
        ],
        "published": "2024-02-23T21:07:20Z",
        "summary": "Transformer-based large language models have displayed impressive in-context\nlearning capabilities, where a pre-trained model can handle new tasks without\nfine-tuning by simply augmenting the query with some input-output examples from\nthat task. Despite the empirical success, the mechanics of how to train a\nTransformer to achieve ICL and the corresponding ICL capacity is mostly elusive\ndue to the technical challenges of analyzing the nonconvex training problems\nresulting from the nonlinear self-attention and nonlinear activation in\nTransformers. To the best of our knowledge, this paper provides the first\ntheoretical analysis of the training dynamics of Transformers with nonlinear\nself-attention and nonlinear MLP, together with the ICL generalization\ncapability of the resulting model. Focusing on a group of binary classification\ntasks, we train Transformers using data from a subset of these tasks and\nquantify the impact of various factors on the ICL generalization performance on\nthe remaining unseen tasks with and without data distribution shifts. We also\nanalyze how different components in the learned Transformers contribute to the\nICL performance. Furthermore, we provide the first theoretical analysis of how\nmodel pruning affects the ICL performance and prove that proper magnitude-based\npruning can have a minimal impact on ICL while reducing inference costs. These\ntheoretical findings are justified through numerical experiments.",
        "pdf_link": "https://arxiv.org/pdf/2402.15607v1.pdf"
    },
    {
        "title": "Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts",
        "authors": [
            "Shubhra Kanti Karmaker Santu",
            "Sanjeev Kumar Sinha",
            "Naman Bansal",
            "Alex Knipper",
            "Souvika Sarkar",
            "John Salvador",
            "Yash Mahajan",
            "Sri Guttikonda",
            "Mousumi Akter",
            "Matthew Freestone",
            "Matthew C. Williams Jr"
        ],
        "published": "2024-02-23T20:14:16Z",
        "summary": "One of the most important yet onerous tasks in the academic peer-reviewing\nprocess is composing meta-reviews, which involves understanding the core\ncontributions, strengths, and weaknesses of a scholarly manuscript based on\npeer-review narratives from multiple experts and then summarizing those\nmultiple experts' perspectives into a concise holistic overview. Given the\nlatest major developments in generative AI, especially Large Language Models\n(LLMs), it is very compelling to rigorously study the utility of LLMs in\ngenerating such meta-reviews in an academic peer-review setting. In this paper,\nwe perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and\nPaLM2, to automatically generate meta-reviews by prompting them with different\ntypes/levels of prompts based on the recently proposed TELeR taxonomy. Finally,\nwe perform a detailed qualitative study of the meta-reviews generated by the\nLLMs and summarize our findings and recommendations for prompting LLMs for this\ncomplex task.",
        "pdf_link": "https://arxiv.org/pdf/2402.15589v1.pdf"
    },
    {
        "title": "DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures",
        "authors": [
            "Agrima Seth",
            "Sanchit Ahuja",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2024-02-23T20:10:18Z",
        "summary": "Generative models are increasingly being used in various applications, such\nas text generation, commonsense reasoning, and question-answering. To be\neffective globally, these models must be aware of and account for local\nsocio-cultural contexts, making it necessary to have benchmarks to evaluate the\nmodels for their cultural familiarity. Since the training data for LLMs is\nweb-based and the Web is limited in its representation of information, it does\nnot capture knowledge present within communities that are not on the Web. Thus,\nthese models exacerbate the inequities, semantic misalignment, and stereotypes\nfrom the Web. There has been a growing call for community-centered\nparticipatory research methods in NLP. In this work, we respond to this call by\nusing participatory research methods to introduce $\\textit{DOSA}$, the first\ncommunity-generated $\\textbf{D}$ataset $\\textbf{o}$f 615 $\\textbf{S}$ocial\n$\\textbf{A}$rtifacts, by engaging with 260 participants from 19 different\nIndian geographic subcultures. We use a gamified framework that relies on\ncollective sensemaking to collect the names and descriptions of these artifacts\nsuch that the descriptions semantically align with the shared sensibilities of\nthe individuals from those cultures. Next, we benchmark four popular LLMs and\nfind that they show significant variation across regional sub-cultures in their\nability to infer the artifacts.",
        "pdf_link": "https://arxiv.org/pdf/2403.14651v1.pdf"
    },
    {
        "title": "CI w/o TN: Context Injection without Task Name for Procedure Planning",
        "authors": [
            "Xinjie Li"
        ],
        "published": "2024-02-23T19:34:47Z",
        "summary": "This paper explores the challenge of procedure planning in instructional\nvideos, which involves creating goal-directed plans based on visual start and\ngoal observations from videos. Previous research has tackled this problem with\ngradually weaker training supervision, from heavy intermediate visual\nobservations or language instructions to task class supervision. However, with\nthe advent of large language models, even given only the task name, these\nmodels can produce a detailed plan. In this study, we propose a much weaker\nsetting without task name as supervision, which is not currently solvable by\nexisting large language models since they require good prompts with sufficient\ninformation. Specifically, we hypothesize that previous intermediate\nsupervisions can serve as context information, and we use captions of visual\nstart and goal observations as a much cheaper form of supervision. This\napproach greatly reduces the labeling cost since the captions can be easily\nobtained by large pre-trained vision-language foundation models. Technically,\nwe apply BLIP to generate captions as supervision to train the context feature\nwith contrastive learning loss. Afterward, the context feature is fed into the\ngenerator to aid in plan generation. Our experiments on two datasets with\nvarying scales demonstrate that our model can achieve comparable performance on\nmultiple metrics, which validates our hypothesis.",
        "pdf_link": "https://arxiv.org/pdf/2402.15579v1.pdf"
    },
    {
        "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
        "authors": [
            "Jianguo Zhang",
            "Tian Lan",
            "Rithesh Murthy",
            "Zhiwei Liu",
            "Weiran Yao",
            "Juntao Tan",
            "Thai Hoang",
            "Liangwei Yang",
            "Yihao Feng",
            "Zuxin Liu",
            "Tulika Awalgaonkar",
            "Juan Carlos Niebles",
            "Silvio Savarese",
            "Shelby Heinecke",
            "Huan Wang",
            "Caiming Xiong"
        ],
        "published": "2024-02-23T18:56:26Z",
        "summary": "Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.",
        "pdf_link": "https://arxiv.org/pdf/2402.15506v3.pdf"
    },
    {
        "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
        "authors": [
            "Yuejiang Liu",
            "Alexandre Alahi"
        ],
        "published": "2024-02-23T18:56:11Z",
        "summary": "Steering the behavior of a strong model pre-trained on internet-scale data\ncan be difficult due to the scarcity of competent supervisors. Recent studies\nreveal that, despite supervisory noises, a strong student model may surpass its\nweak teacher when fine-tuned on specific objectives. Yet, the effectiveness of\nsuch weak-to-strong generalization remains limited, especially in the presence\nof large capability gaps. In this paper, we propose to address this challenge\nby harnessing a diverse set of specialized teachers, instead of a single\ngeneralist one, that collectively supervises the strong student. Our approach\nresembles the classical hierarchical mixture of experts, with two components\ntailored for co-supervision: (i) we progressively alternate student training\nand teacher assignment, leveraging the growth of the strong student to identify\nplausible supervisions; (ii) we conservatively enforce teacher-student and\nlocal-global consistency, leveraging their dependencies to reject potential\nannotation noises. We validate the proposed method through visual recognition\ntasks on the OpenAI weak-to-strong benchmark and additional multi-domain\ndatasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.",
        "pdf_link": "https://arxiv.org/pdf/2402.15505v1.pdf"
    },
    {
        "title": "Self-Retrieval: Building an Information Retrieval System with One Large Language Model",
        "authors": [
            "Qiaoyu Tang",
            "Jiawei Chen",
            "Bowen Yu",
            "Yaojie Lu",
            "Cheng Fu",
            "Haiyang Yu",
            "Hongyu Lin",
            "Fei Huang",
            "Ben He",
            "Xianpei Han",
            "Le Sun",
            "Yongbin Li"
        ],
        "published": "2024-02-23T18:45:35Z",
        "summary": "The rise of large language models (LLMs) has transformed the role of\ninformation retrieval (IR) systems in the way to humans accessing information.\nDue to the isolated architecture and the limited interaction, existing IR\nsystems are unable to fully accommodate the shift from directly providing\ninformation to humans to indirectly serving large language models. In this\npaper, we propose Self-Retrieval, an end-to-end, LLM-driven information\nretrieval architecture that can fully internalize the required abilities of IR\nsystems into a single LLM and deeply leverage the capabilities of LLMs during\nIR process. Specifically, Self-retrieval internalizes the corpus to retrieve\ninto a LLM via a natural language indexing architecture. Then the entire\nretrieval process is redefined as a procedure of document generation and\nself-assessment, which can be end-to-end executed using a single large language\nmodel. Experimental results demonstrate that Self-Retrieval not only\nsignificantly outperforms previous retrieval approaches by a large margin, but\nalso can significantly boost the performance of LLM-driven downstream\napplications like retrieval augumented generation.",
        "pdf_link": "https://arxiv.org/pdf/2403.00801v1.pdf"
    },
    {
        "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
        "authors": [
            "Shenglai Zeng",
            "Jiankun Zhang",
            "Pengfei He",
            "Yue Xing",
            "Yiding Liu",
            "Han Xu",
            "Jie Ren",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Yi Chang",
            "Jiliang Tang"
        ],
        "published": "2024-02-23T18:35:15Z",
        "summary": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate\nlanguage model with proprietary and private data, where data privacy is a\npivotal concern. Whereas extensive research has demonstrated the privacy risks\nof large language models (LLMs), the RAG technique could potentially reshape\nthe inherent behaviors of LLM generation, posing new privacy issues that are\ncurrently under-explored. In this work, we conduct extensive empirical studies\nwith novel attack methods, which demonstrate the vulnerability of RAG systems\non leaking the private retrieval database. Despite the new risk brought by RAG\non the retrieval data, we further reveal that RAG can mitigate the leakage of\nthe LLMs' training data. Overall, we provide new insights in this paper for\nprivacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG\nsystems builders. Our code is available at\nhttps://github.com/phycholosogy/RAG-privacy.",
        "pdf_link": "https://arxiv.org/pdf/2402.16893v1.pdf"
    },
    {
        "title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs",
        "authors": [
            "Kinjal Basu",
            "Ibrahim Abdelaziz",
            "Subhajit Chaudhury",
            "Soham Dan",
            "Maxwell Crouse",
            "Asim Munawar",
            "Sadhana Kumaravel",
            "Vinod Muthusamy",
            "Pavan Kapanipathi",
            "Luis A. Lastras"
        ],
        "published": "2024-02-23T18:30:49Z",
        "summary": "There is a growing need for Large Language Models (LLMs) to effectively use\ntools and external Application Programming Interfaces (APIs) to plan and\ncomplete tasks. As such, there is tremendous interest in methods that can\nacquire sufficient quantities of train and test data that involve calls to\ntools / APIs. Two lines of research have emerged as the predominant strategies\nfor addressing this challenge. The first has focused on synthetic data\ngeneration techniques, while the second has involved curating task-adjacent\ndatasets which can be transformed into API / Tool-based tasks. In this paper,\nwe focus on the task of identifying, curating, and transforming existing\ndatasets and, in turn, introduce API-BLEND, a large corpora for training and\nsystematic testing of tool-augmented LLMs. The datasets mimic real-world\nscenarios involving API-tasks such as API / tool detection, slot filling, and\nsequencing of the detected APIs. We demonstrate the utility of the API-BLEND\ndataset for both training and benchmarking purposes.",
        "pdf_link": "https://arxiv.org/pdf/2402.15491v1.pdf"
    },
    {
        "title": "Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models",
        "authors": [
            "Yiran Liu",
            "Ke Yang",
            "Zehan Qi",
            "Xiao Liu",
            "Yang Yu",
            "Chengxiang Zhai"
        ],
        "published": "2024-02-23T18:15:56Z",
        "summary": "The growing integration of large language models (LLMs) into social\noperations amplifies their impact on decisions in crucial areas such as\neconomics, law, education, and healthcare, raising public concerns about these\nmodels' discrimination-related safety and reliability. However, prior\ndiscrimination measuring frameworks solely assess the average discriminatory\nbehavior of LLMs, often proving inadequate due to the overlook of an additional\ndiscrimination-leading factor, i.e., the LLMs' prediction variation across\ndiverse contexts. In this work, we present the Prejudice-Caprice Framework\n(PCF) that comprehensively measures discrimination in LLMs by considering both\ntheir consistently biased preference and preference variation across diverse\ncontexts. Specifically, we mathematically dissect the aggregated contextualized\ndiscrimination risk of LLMs into prejudice risk, originating from LLMs'\npersistent prejudice, and caprice risk, stemming from their generation\ninconsistency. In addition, we utilize a data-mining approach to gather\npreference-detecting probes from sentence skeletons, devoid of attribute\nindications, to approximate LLMs' applied contexts. While initially intended\nfor assessing discrimination in LLMs, our proposed PCF facilitates the\ncomprehensive and flexible measurement of any inductive biases, including\nknowledge alongside prejudice, across various modality models. We apply our\ndiscrimination-measuring framework to 12 common LLMs, yielding intriguing\nfindings: i) modern LLMs demonstrate significant pro-male stereotypes, ii)\nLLMs' exhibited discrimination correlates with several social and economic\nfactors, iii) prejudice risk dominates the overall discrimination risk and\nfollows a normal distribution, and iv) caprice risk contributes minimally to\nthe overall risk but follows a fat-tailed distribution, suggesting that it is\nwild risk requiring enhanced surveillance.",
        "pdf_link": "https://arxiv.org/pdf/2402.15481v3.pdf"
    },
    {
        "title": "Repetition Improves Language Model Embeddings",
        "authors": [
            "Jacob Mitchell Springer",
            "Suhas Kotha",
            "Daniel Fried",
            "Graham Neubig",
            "Aditi Raghunathan"
        ],
        "published": "2024-02-23T17:25:10Z",
        "summary": "Recent approaches to improving the extraction of text embeddings from\nautoregressive large language models (LLMs) have largely focused on\nimprovements to data, backbone pretrained language models, or improving\ntask-differentiation via instructions. In this work, we address an\narchitectural limitation of autoregressive models: token embeddings cannot\ncontain information from tokens that appear later in the input. To address this\nlimitation, we propose a simple approach, \"echo embeddings,\" in which we repeat\nthe input twice in context and extract embeddings from the second occurrence.\nWe show that echo embeddings of early tokens can encode information about later\ntokens, allowing us to maximally leverage high-quality LLMs for embeddings. On\nthe MTEB leaderboard, echo embeddings improve over classical embeddings by over\n9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a\nMistral-7B model achieve state-of-the-art compared to prior open source models\nthat do not leverage synthetic fine-tuning data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15449v1.pdf"
    },
    {
        "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
        "authors": [
            "Simon Holk",
            "Daniel Marta",
            "Iolanda Leite"
        ],
        "published": "2024-02-23T16:30:05Z",
        "summary": "Preference-based reinforcement learning (RL) has emerged as a new field in\nrobot learning, where humans play a pivotal role in shaping robot behavior by\nexpressing preferences on different sequences of state-action pairs. However,\nformulating realistic policies for robots demands responses from humans to an\nextensive array of queries. In this work, we approach the sample-efficiency\nchallenge by expanding the information collected per query to contain both\npreferences and optional text prompting. To accomplish this, we leverage the\nzero-shot capabilities of a large language model (LLM) to reason from the text\nprovided by humans. To accommodate the additional query information, we\nreformulate the reward learning objectives to contain flexible highlights --\nstate-action pairs that contain relatively high information and are related to\nthe features processed in a zero-shot fashion from a pretrained LLM. In both a\nsimulated scenario and a user study, we reveal the effectiveness of our work by\nanalyzing the feedback and its implications. Additionally, the collective\nfeedback collected serves to train a robot on socially compliant trajectories\nin a simulated social navigation landscape. We provide video examples of the\ntrained policies at https://sites.google.com/view/rl-predilect",
        "pdf_link": "https://arxiv.org/pdf/2402.15420v1.pdf"
    },
    {
        "title": "Explorations of Self-Repair in Language Models",
        "authors": [
            "Cody Rushing",
            "Neel Nanda"
        ],
        "published": "2024-02-23T15:42:12Z",
        "summary": "Prior interpretability research studying narrow distributions has\npreliminarily identified self-repair, a phenomena where if components in large\nlanguage models are ablated, later components will change their behavior to\ncompensate. Our work builds off this past literature, demonstrating that\nself-repair exists on a variety of models families and sizes when ablating\nindividual attention heads on the full training distribution. We further show\nthat on the full training distribution self-repair is imperfect, as the\noriginal direct effect of the head is not fully restored, and noisy, since the\ndegree of self-repair varies significantly across different prompts (sometimes\novercorrecting beyond the original effect). We highlight two different\nmechanisms that contribute to self-repair, including changes in the final\nLayerNorm scaling factor (which can repair up to 30% of the direct effect) and\nsparse sets of neurons implementing Anti-Erasure. We additionally discuss the\nimplications of these results for interpretability practitioners and close with\na more speculative discussion on the mystery of why self-repair occurs in these\nmodels at all, highlighting evidence for the Iterative Inference hypothesis in\nlanguage models, a framework that predicts self-repair.",
        "pdf_link": "https://arxiv.org/pdf/2402.15390v1.pdf"
    },
    {
        "title": "Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction",
        "authors": [
            "Jun Wang",
            "Guocheng He",
            "Yiannis Kantaros"
        ],
        "published": "2024-02-23T15:02:44Z",
        "summary": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities (e.g., mobility, manipulation, and sensing) at various\nlocations and semantic objects. Several recent works have addressed similar\nplanning problems by leveraging pre-trained Large Language Models (LLMs) to\ndesign effective multi-robot plans. However, these approaches lack mission\nperformance and safety guarantees. To address this challenge, we introduce a\nnew decentralized LLM-based planner that is capable of achieving high mission\nsuccess rates. This is accomplished by leveraging conformal prediction (CP), a\ndistribution-free uncertainty quantification tool in black-box models. CP\nallows the proposed multi-robot planner to reason about its inherent\nuncertainty in a decentralized fashion, enabling robots to make individual\ndecisions when they are sufficiently certain and seek help otherwise. We show,\nboth theoretically and empirically, that the proposed planner can achieve\nuser-specified task success rates while minimizing the overall number of help\nrequests. We demonstrate the performance of our approach on multi-robot home\nservice applications. We also show through comparative experiments, that our\nmethod outperforms recent centralized and decentralized multi-robot LLM-based\nplanners in terms of in terms of its ability to design correct plans. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing mission complexity and robot team size.",
        "pdf_link": "https://arxiv.org/pdf/2402.15368v1.pdf"
    },
    {
        "title": "Farsight: Fostering Responsible AI Awareness During AI Application Prototyping",
        "authors": [
            "Zijie J. Wang",
            "Chinmay Kulkarni",
            "Lauren Wilcox",
            "Michael Terry",
            "Michael Madaio"
        ],
        "published": "2024-02-23T14:38:05Z",
        "summary": "Prompt-based interfaces for Large Language Models (LLMs) have made\nprototyping and building AI-powered applications easier than ever before.\nHowever, identifying potential harms that may arise from AI applications\nremains a challenge, particularly during prompt-based prototyping. To address\nthis, we present Farsight, a novel in situ interactive tool that helps people\nidentify potential harms from the AI applications they are prototyping. Based\non a user's prompt, Farsight highlights news articles about relevant AI\nincidents and allows users to explore and edit LLM-generated use cases,\nstakeholders, and harms. We report design insights from a co-design study with\n10 AI prototypers and findings from a user study with 42 AI prototypers. After\nusing Farsight, AI prototypers in our user study are better able to\nindependently identify potential harms associated with a prompt and find our\ntool more useful and usable than existing resources. Their qualitative feedback\nalso highlights that Farsight encourages them to focus on end-users and think\nbeyond immediate harms. We discuss these findings and reflect on their\nimplications for designing AI prototyping experiences that meaningfully engage\nwith AI harms. Farsight is publicly accessible at:\nhttps://PAIR-code.github.io/farsight.",
        "pdf_link": "https://arxiv.org/pdf/2402.15350v1.pdf"
    },
    {
        "title": "Ranking Entities along Conceptual Space Dimensions with LLMs: An Analysis of Fine-Tuning Strategies",
        "authors": [
            "Nitesh Kumar",
            "Usashi Chatterjee",
            "Steven Schockaert"
        ],
        "published": "2024-02-23T14:17:01Z",
        "summary": "Conceptual spaces represent entities in terms of their primitive semantic\nfeatures. Such representations are highly valuable but they are notoriously\ndifficult to learn, especially when it comes to modelling perceptual and\nsubjective features. Distilling conceptual spaces from Large Language Models\n(LLMs) has recently emerged as a promising strategy. However, existing work has\nbeen limited to probing pre-trained LLMs using relatively simple zero-shot\nstrategies. We focus in particular on the task of ranking entities according to\na given conceptual space dimension. Unfortunately, we cannot directly fine-tune\nLLMs on this task, because ground truth rankings for conceptual space\ndimensions are rare. We therefore use more readily available features as\ntraining data and analyse whether the ranking capabilities of the resulting\nmodels transfer to perceptual and subjective features. We find that this is\nindeed the case, to some extent, but having perceptual and subjective features\nin the training data seems essential for achieving the best results. We\nfurthermore find that pointwise ranking strategies are competitive against\npairwise approaches, in defiance of common wisdom.",
        "pdf_link": "https://arxiv.org/pdf/2402.15337v1.pdf"
    },
    {
        "title": "GPTVQ: The Blessing of Dimensionality for LLM Quantization",
        "authors": [
            "Mart van Baalen",
            "Andrey Kuzmin",
            "Markus Nagel",
            "Peter Couperus",
            "Cedric Bastoul",
            "Eric Mahurin",
            "Tijmen Blankevoort",
            "Paul Whatmough"
        ],
        "published": "2024-02-23T13:39:16Z",
        "summary": "In this work we show that the size versus accuracy trade-off of neural\nnetwork quantization can be significantly improved by increasing the\nquantization dimensionality. We propose the GPTVQ method, a new fast method for\npost-training vector quantization (VQ) that scales well to Large Language\nModels (LLMs). Our method interleaves quantization of one or more columns with\nupdates to the remaining unquantized weights, using information from the\nHessian of the per-layer output reconstruction MSE. Quantization codebooks are\ninitialized using an efficient data-aware version of the EM algorithm. The\ncodebooks are then updated, and further compressed by using integer\nquantization and SVD-based compression. GPTVQ establishes a new state-of-the\nart in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2\nand Mistral. Furthermore, our method is efficient: on a single H100 it takes\nbetween 3 and 11 hours to process a Llamav2-70B model, depending on\nquantization setting. Lastly, with on-device timings for VQ decompression on a\nmobile CPU we show that VQ leads to improved latency compared to using a 4-bit\ninteger format.",
        "pdf_link": "https://arxiv.org/pdf/2402.15319v1.pdf"
    },
    {
        "title": "ArabianGPT: Native Arabic GPT-based Large Language Model",
        "authors": [
            "Anis Koubaa",
            "Adel Ammar",
            "Lahouari Ghouti",
            "Omar Najar",
            "Serry Sibaee"
        ],
        "published": "2024-02-23T13:32:47Z",
        "summary": "The predominance of English and Latin-based large language models (LLMs) has\nled to a notable deficit in native Arabic LLMs. This discrepancy is accentuated\nby the prevalent inclusion of English tokens in existing Arabic models,\ndetracting from their efficacy in processing native Arabic's intricate\nmorphology and syntax. Consequently, there is a theoretical and practical\nimperative for developing LLMs predominantly focused on Arabic linguistic\nelements. To address this gap, this paper proposes ArabianGPT, a series of\ntransformer-based models within the ArabianLLM suite designed explicitly for\nArabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in\nsize and complexity, aligning with the nuanced linguistic characteristics of\nArabic. The AraNizer tokenizer, integral to these models, addresses the unique\nmorphological aspects of Arabic script, ensuring more accurate text processing.\nEmpirical results from fine-tuning the models on tasks like sentiment analysis\nand summarization demonstrate significant improvements. For sentiment analysis,\nthe fine-tuned ArabianGPT-0.1B model achieved a remarkable accuracy of 95%, a\nsubstantial increase from the base model's 56%. Similarly, in summarization\ntasks, fine-tuned models showed enhanced F1 scores, indicating improved\nprecision and recall in generating concise summaries. Comparative analysis of\nfine-tuned ArabianGPT models against their base versions across various\nbenchmarks reveals nuanced differences in performance, with fine-tuning\npositively impacting specific tasks like question answering and summarization.\nThese findings underscore the efficacy of fine-tuning in aligning ArabianGPT\nmodels more closely with specific NLP tasks, highlighting the potential of\ntailored transformer architectures in advancing Arabic NLP.",
        "pdf_link": "https://arxiv.org/pdf/2402.15313v2.pdf"
    },
    {
        "title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries",
        "authors": [
            "Somnath Banerjee",
            "Sayan Layek",
            "Rima Hazra",
            "Animesh Mukherjee"
        ],
        "published": "2024-02-23T13:03:12Z",
        "summary": "In this study, we tackle a growing concern around the safety and ethical use\nof large language models (LLMs). Despite their potential, these models can be\ntricked into producing harmful or unethical content through various\nsophisticated methods, including 'jailbreaking' techniques and targeted\nmanipulation. Our work zeroes in on a specific issue: to what extent LLMs can\nbe led astray by asking them to generate responses that are instruction-centric\nsuch as a pseudocode, a program or a software snippet as opposed to vanilla\ntext. To investigate this question, we introduce TechHazardQA, a dataset\ncontaining complex queries which should be answered in both text and\ninstruction-centric formats (e.g., pseudocodes), aimed at identifying triggers\nfor unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,\nMistral-V2 and Mistral 8X7B -- and ask them to generate both text and\ninstruction-centric responses. For evaluation we report the harmfulness score\nmetric as well as judgements from GPT-4 and humans. Overall, we observe that\nasking LLMs to produce instruction-centric responses enhances the unethical\nresponse generation by ~2-38% across the models. As an additional objective, we\ninvestigate the impact of model editing using the ROME technique, which further\nincreases the propensity for generating undesirable content. In particular,\nasking edited LLMs to generate instruction-centric responses further increases\nthe unethical response generation by ~3-16% across the different models.",
        "pdf_link": "https://arxiv.org/pdf/2402.15302v4.pdf"
    },
    {
        "title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models",
        "authors": [
            "Yuzhe Zhang",
            "Yipeng Zhang",
            "Yidong Gan",
            "Lina Yao",
            "Chen Wang"
        ],
        "published": "2024-02-23T13:02:10Z",
        "summary": "Causal graph recovery is essential in the field of causal inference.\nTraditional methods are typically knowledge-based or statistical\nestimation-based, which are limited by data collection biases and individuals'\nknowledge about factors affecting the relations between variables of interests.\nThe advance of large language models (LLMs) provides opportunities to address\nthese problems. We propose a novel method that utilizes the extensive knowledge\ncontained within a large corpus of scientific literature to deduce causal\nrelationships in general causal graph recovery tasks. This method leverages\nRetrieval Augmented-Generation (RAG) based LLMs to systematically analyze and\nextract pertinent information from a comprehensive collection of research\npapers. Our method first retrieves relevant text chunks from the aggregated\nliterature. Then, the LLM is tasked with identifying and labelling potential\nassociations between factors. Finally, we give a method to aggregate the\nassociational relationships to build a causal graph. We demonstrate our method\nis able to construct high quality causal graphs on the well-known SACHS dataset\nsolely from literature.",
        "pdf_link": "https://arxiv.org/pdf/2402.15301v1.pdf"
    },
    {
        "title": "CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora",
        "authors": [
            "Zijun Long",
            "Xuri Ge",
            "Richard Mccreadie",
            "Joemon Jose"
        ],
        "published": "2024-02-23T11:47:16Z",
        "summary": "Text-to-image retrieval aims to find the relevant images based on a text\nquery, which is important in various use-cases, such as digital libraries,\ne-commerce, and multimedia databases. Although Multimodal Large Language Models\n(MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in\nhandling large-scale, diverse, and ambiguous real-world needs of retrieval, due\nto the computation cost and the injective embeddings they produce. This paper\npresents a two-stage Coarse-to-Fine Index-shared Retrieval (CFIR) framework,\ndesigned for fast and effective large-scale long-text to image retrieval. The\nfirst stage, Entity-based Ranking (ER), adapts to long-text query ambiguity by\nemploying a multiple-queries-to-multiple-targets paradigm, facilitating\ncandidate filtering for the next stage. The second stage, Summary-based\nRe-ranking (SR), refines these rankings using summarized queries. We also\npropose a specialized Decoupling-BEiT-3 encoder, optimized for handling\nambiguous user needs and both stages, which also enhances computational\nefficiency through vector-based similarity inference. Evaluation on the AToMiC\ndataset reveals that CFIR surpasses existing MLLMs by up to 11.06% in\nRecall@1000, while reducing training and retrieval times by 68.75% and 99.79%,\nrespectively. We will release our code to facilitate future research at\nhttps://github.com/longkukuhi/CFIR.",
        "pdf_link": "https://arxiv.org/pdf/2402.15276v3.pdf"
    },
    {
        "title": "CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models",
        "authors": [
            "Juhye Ha",
            "Hyeon Jeon",
            "DaEun Han",
            "Jinwook Seo",
            "Changhoon Oh"
        ],
        "published": "2024-02-23T11:25:17Z",
        "summary": "Large language models (LLMs) have facilitated significant strides in\ngenerating conversational agents, enabling seamless, contextually relevant\ndialogues across diverse topics. However, the existing LLM-driven\nconversational agents have fixed personalities and functionalities, limiting\ntheir adaptability to individual user needs. Creating personalized agent\npersonas with distinct expertise or traits can address this issue. Nonetheless,\nwe lack knowledge of how people customize and interact with agent personas. In\nthis research, we investigated how users customize agent personas and their\nimpact on interaction quality, diversity, and dynamics. To this end, we\ndeveloped CloChat, an interface supporting easy and accurate customization of\nagent personas in LLMs. We conducted a study comparing how participants\ninteract with CloChat and ChatGPT. The results indicate that participants\nformed emotional bonds with the customized agents, engaged in more dynamic\ndialogues, and showed interest in sustaining interactions. These findings\ncontribute to design implications for future systems with conversational agents\nusing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15265v1.pdf"
    },
    {
        "title": "DEEM: Dynamic Experienced Expert Modeling for Stance Detection",
        "authors": [
            "Xiaolong Wang",
            "Yile Wang",
            "Sijie Cheng",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2024-02-23T11:24:00Z",
        "summary": "Recent work has made a preliminary attempt to use large language models\n(LLMs) to solve the stance detection task, showing promising results. However,\nconsidering that stance detection usually requires detailed background\nknowledge, the vanilla reasoning method may neglect the domain knowledge to\nmake a professional and accurate analysis. Thus, there is still room for\nimprovement of LLMs reasoning, especially in leveraging the generation\ncapability of LLMs to simulate specific experts (i.e., multi-agents) to detect\nthe stance. In this paper, different from existing multi-agent works that\nrequire detailed descriptions and use fixed experts, we propose a Dynamic\nExperienced Expert Modeling (DEEM) method which can leverage the generated\nexperienced experts and let LLMs reason in a semi-parametric way, making the\nexperts more generalizable and reliable. Experimental results demonstrate that\nDEEM consistently achieves the best results on three standard benchmarks,\noutperforms methods with self-consistency reasoning, and reduces the bias of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15264v1.pdf"
    },
    {
        "title": "Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues",
        "authors": [
            "Armand Stricker",
            "Patrick Paroubek"
        ],
        "published": "2024-02-23T10:27:42Z",
        "summary": "During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences",
        "pdf_link": "https://arxiv.org/pdf/2402.15248v2.pdf"
    },
    {
        "title": "GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?",
        "authors": [
            "Yiping Jin",
            "Leo Wanner",
            "Alexander Shvets"
        ],
        "published": "2024-02-23T10:02:01Z",
        "summary": "Online hate detection suffers from biases incurred in data sampling,\nannotation, and model pre-training. Therefore, measuring the averaged\nperformance over all examples in held-out test data is inadequate. Instead, we\nmust identify specific model weaknesses and be informed when it is more likely\nto fail. A recent proposal in this direction is HateCheck, a suite for testing\nfine-grained model functionalities on synthesized data generated using\ntemplates of the kind \"You are just a [slur] to me.\" However, despite enabling\nmore detailed diagnostic insights, the HateCheck test cases are often generic\nand have simplistic sentence structures that do not match the real-world data.\nTo address this limitation, we propose GPT-HateCheck, a framework to generate\nmore diverse and realistic functional tests from scratch by instructing large\nlanguage models (LLMs). We employ an additional natural language inference\n(NLI) model to verify the generations. Crowd-sourced annotation demonstrates\nthat the generated test cases are of high quality. Using the new functional\ntests, we can uncover model weaknesses that would be overlooked using the\noriginal HateCheck dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.15238v1.pdf"
    },
    {
        "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition",
        "authors": [
            "Lu Ye",
            "Ze Tao",
            "Yong Huang",
            "Yang Li"
        ],
        "published": "2024-02-23T09:29:19Z",
        "summary": "Self-attention is an essential component of large language models(LLMs) but a\nsignificant source of inference latency for long sequences. In multi-tenant\nLLMs serving scenarios, the compute and memory operation cost of self-attention\ncan be optimized by using the probability that multiple LLM requests have\nshared system prompts in prefixes. In this paper, we introduce ChunkAttention,\na prefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the start-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
        "pdf_link": "https://arxiv.org/pdf/2402.15220v2.pdf"
    },
    {
        "title": "Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing",
        "authors": [
            "Samuel Kernan Freire",
            "Margo MC van Mol",
            "Carola Schol",
            "Elif \u00d6zcan Vieira"
        ],
        "published": "2024-02-23T09:06:25Z",
        "summary": "Intensive care unit (ICU) patients often develop new health-related problems\nin their long-term recovery. Health care professionals keeping a diary of a\npatient's stay is a proven strategy to tackle this but faces several adoption\nbarriers, such as lack of time and difficulty in knowing what to write. Large\nlanguage models (LLMs), with their ability to generate human-like text and\nadaptability, could solve these challenges. However, realizing this vision\ninvolves addressing several socio-technical and practical research challenges.\nThis paper discusses these challenges and proposes future research directions\nto utilize the potential of LLMs in ICU diary writing, ultimately improving the\nlong-term recovery outcomes for ICU patients.",
        "pdf_link": "https://arxiv.org/pdf/2402.15205v1.pdf"
    },
    {
        "title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models",
        "authors": [
            "Xin Yi",
            "Linlin Wang",
            "Xiaoling Wang",
            "Liang He"
        ],
        "published": "2024-02-23T09:04:48Z",
        "summary": "Impressive results have been achieved in natural language processing (NLP)\ntasks through the training of large language models (LLMs). However, these\nmodels occasionally produce toxic content such as insults, threats, and\nprofanity in response to certain prompts, thereby constraining their practical\nutility. To tackle this issue, various finetuning-based and decoding-based\napproaches have been utilized to mitigate toxicity. However, these methods\ntypically necessitate additional costs such as high-quality training data or\nauxiliary models. In this paper, we propose fine-grained detoxification via\ninstance-level prefixes (FGDILP) to mitigate toxic text without additional\ncost. Specifically, FGDILP contrasts the contextualized representation in\nattention space using a positive prefix-prepended prompt against multiple\nnegative prefix-prepended prompts at the instance level. This allows for\nconstructing fine-grained subtoxicity vectors, which enables collaborative\ndetoxification by fusing them to correct the normal generation process when\nprovided with a raw prompt. We validate that FGDILP enables controlled text\ngeneration with regard to toxicity at both the utterance and context levels.\nOur method surpasses prompt-based baselines in detoxification, although at a\nslight cost to generation fluency and diversity.",
        "pdf_link": "https://arxiv.org/pdf/2402.15202v2.pdf"
    },
    {
        "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
        "authors": [
            "Xinglin Lyu",
            "Junhui Li",
            "Yanqing Zhao",
            "Min Zhang",
            "Daimeng Wei",
            "Shimin Tao",
            "Hao Yang",
            "Min Zhang"
        ],
        "published": "2024-02-23T09:01:00Z",
        "summary": "Generally, the decoder-only large language models (LLMs) are adapted to\ncontext-aware neural machine translation (NMT) in a concatenating way, where\nLLMs take the concatenation of the source sentence (i.e., intra-sentence\ncontext) and the inter-sentence context as the input, and then to generate the\ntarget tokens sequentially. This adaptation strategy, i.e., concatenation mode,\nconsiders intra-sentence and inter-sentence contexts with the same priority,\ndespite an apparent difference between the two kinds of contexts. In this\npaper, we propose an alternative adaptation approach, named Decoding-enhanced\nMulti-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and\nutilize the inter- and intra-sentence context and more effectively adapt LLMs\nto context-aware NMT. First, DeMPT divides the context-aware NMT process into\nthree separate phases. During each phase, different continuous prompts are\nintroduced to make LLMs discriminately model various information. Second, DeMPT\nemploys a heuristic way to further discriminately enhance the utilization of\nthe source-side inter- and intra-sentence information at the final decoding\nphase. Experiments show that our approach significantly outperforms the\nconcatenation method, and further improves the performance of LLMs in discourse\nmodeling.",
        "pdf_link": "https://arxiv.org/pdf/2402.15200v1.pdf"
    },
    {
        "title": "GraphEdit: Large Language Models for Graph Structure Learning",
        "authors": [
            "Zirui Guo",
            "Lianghao Xia",
            "Yanhua Yu",
            "Yuling Wang",
            "Zixuan Yang",
            "Wei Wei",
            "Liang Pang",
            "Tat-Seng Chua",
            "Chao Huang"
        ],
        "published": "2024-02-23T08:29:42Z",
        "summary": "Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies\nand interactions among nodes in graph-structured data by generating novel graph\nstructures. Graph Neural Networks (GNNs) have emerged as promising GSL\nsolutions, utilizing recursive message passing to encode node-wise\ninter-dependencies. However, many existing GSL methods heavily depend on\nexplicit graph structural information as supervision signals, leaving them\nsusceptible to challenges such as data noise and sparsity. In this work, we\npropose GraphEdit, an approach that leverages large language models (LLMs) to\nlearn complex node relationships in graph-structured data. By enhancing the\nreasoning capabilities of LLMs through instruction-tuning over graph\nstructures, we aim to overcome the limitations associated with explicit graph\nstructural information and enhance the reliability of graph structure learning.\nOur approach not only effectively denoises noisy connections but also\nidentifies node-wise dependencies from a global perspective, providing a\ncomprehensive understanding of the graph structure. We conduct extensive\nexperiments on multiple benchmark datasets to demonstrate the effectiveness and\nrobustness of GraphEdit across various settings. We have made our model\nimplementation available at: https://github.com/HKUDS/GraphEdit.",
        "pdf_link": "https://arxiv.org/pdf/2402.15183v4.pdf"
    },
    {
        "title": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer",
        "authors": [
            "Yanjun Zhao",
            "Sizhe Dang",
            "Haishan Ye",
            "Guang Dai",
            "Yi Qian",
            "Ivor W. Tsang"
        ],
        "published": "2024-02-23T08:11:55Z",
        "summary": "Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8.",
        "pdf_link": "https://arxiv.org/pdf/2402.15173v1.pdf"
    },
    {
        "title": "Machine Unlearning of Pre-trained Large Language Models",
        "authors": [
            "Jin Yao",
            "Eli Chien",
            "Minxin Du",
            "Xinyao Niu",
            "Tianhao Wang",
            "Zezhou Cheng",
            "Xiang Yue"
        ],
        "published": "2024-02-23T07:43:26Z",
        "summary": "This study investigates the concept of the `right to be forgotten' within the\ncontext of large language models (LLMs). We explore machine unlearning as a\npivotal solution, with a focus on pre-trained models--a notably\nunder-researched area. Our research delineates a comprehensive framework for\nmachine unlearning in pre-trained LLMs, encompassing a critical analysis of\nseven diverse unlearning methods. Through rigorous evaluation using curated\ndatasets from arXiv, books, and GitHub, we establish a robust benchmark for\nunlearning performance, demonstrating that these methods are over $10^5$ times\nmore computationally efficient than retraining. Our results show that\nintegrating gradient ascent with gradient descent on in-distribution data\nimproves hyperparameter robustness. We also provide detailed guidelines for\nefficient hyperparameter tuning in the unlearning process. Our findings advance\nthe discourse on ethical AI practices, offering substantive insights into the\nmechanics of machine unlearning for pre-trained LLMs and underscoring the\npotential for responsible AI development.",
        "pdf_link": "https://arxiv.org/pdf/2402.15159v2.pdf"
    },
    {
        "title": "Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing",
        "authors": [
            "Jeong Hun Yeo",
            "Seunghee Han",
            "Minsu Kim",
            "Yong Man Ro"
        ],
        "published": "2024-02-23T07:21:32Z",
        "summary": "In visual speech processing, context modeling capability is one of the most\nimportant requirements due to the ambiguous nature of lip movements. For\nexample, homophenes, words that share identical lip movements but produce\ndifferent sounds, can be distinguished by considering the context. In this\npaper, we propose a novel framework, namely Visual Speech Processing\nincorporated with LLMs (VSP-LLM), to maximize the context modeling ability by\nbringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to\nperform multi-tasks of visual speech recognition and translation, where the\ngiven instructions control the type of task. The input video is mapped to the\ninput latent space of a LLM by employing a self-supervised visual speech model.\nFocused on the fact that there is redundant information in input frames, we\npropose a novel deduplication method that reduces the embedded visual features\nby employing visual speech units. Through the proposed deduplication and Low\nRank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient\nmanner. In the translation dataset, the MuAViC benchmark, we demonstrate that\nVSP-LLM can more effectively recognize and translate lip movements with just 15\nhours of labeled data, compared to the recent translation model trained with\n433 hours of labeld data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15151v1.pdf"
    },
    {
        "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models",
        "authors": [
            "Guanming Xiong",
            "Junwei Bao",
            "Wen Zhao"
        ],
        "published": "2024-02-23T06:32:18Z",
        "summary": "This study explores the realm of knowledge-base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nYet, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.15131v1.pdf"
    },
    {
        "title": "AgentLite: A Lightweight Library for Building and Advancing Task-Oriented LLM Agent System",
        "authors": [
            "Zhiwei Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Liangwei Yang",
            "Zuxin Liu",
            "Juntao Tan",
            "Prafulla K. Choubey",
            "Tian Lan",
            "Jason Wu",
            "Huan Wang",
            "Shelby Heinecke",
            "Caiming Xiong",
            "Silvio Savarese"
        ],
        "published": "2024-02-23T06:25:20Z",
        "summary": "The booming success of LLMs initiates rapid development in LLM agents. Though\nthe foundation of an LLM agent is the generative model, it is critical to\ndevise the optimal reasoning strategies and agent architectures. Accordingly,\nLLM agent research advances from the simple chain-of-thought prompting to more\ncomplex ReAct and Reflection reasoning strategy; agent architecture also\nevolves from single agent generation to multi-agent conversation, as well as\nmulti-LLM multi-agent group chat. However, with the existing intricate\nframeworks and libraries, creating and evaluating new reasoning strategies and\nagent architectures has become a complex challenge, which hinders research\ninvestigation into LLM agents. Thus, we open-source a new AI agent library,\nAgentLite, which simplifies this process by offering a lightweight,\nuser-friendly platform for innovating LLM agent reasoning, architectures, and\napplications with ease. AgentLite is a task-oriented framework designed to\nenhance the ability of agents to break down tasks and facilitate the\ndevelopment of multi-agent systems. Furthermore, we introduce multiple\npractical applications developed with AgentLite to demonstrate its convenience\nand flexibility. Get started now at:\n\\url{https://github.com/SalesforceAIResearch/AgentLite}.",
        "pdf_link": "https://arxiv.org/pdf/2402.15538v1.pdf"
    },
    {
        "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
        "authors": [
            "Hyunjae Kim",
            "Seunghyun Yoon",
            "Trung Bui",
            "Handong Zhao",
            "Quan Tran",
            "Franck Dernoncourt",
            "Jaewoo Kang"
        ],
        "published": "2024-02-23T06:11:50Z",
        "summary": "Contrastive language-image pre-training (CLIP) models have demonstrated\nconsiderable success across various vision-language tasks, such as\ntext-to-image retrieval, where the model is required to effectively process\nnatural language input to produce an accurate visual output. However, current\nmodels still face limitations in dealing with linguistic variations in input\nqueries, such as paraphrases, making it challenging to handle a broad range of\nuser queries in real-world applications. In this study, we introduce a\nstraightforward fine-tuning approach to enhance the representations of CLIP\nmodels for paraphrases. Our approach involves a two-step paraphrase generation\nprocess, where we automatically create two categories of paraphrases from\nweb-scale image captions by leveraging large language models. Subsequently, we\nfine-tune the CLIP text encoder using these generated paraphrases while\nfreezing the image encoder. Our resulting model, which we call ParaCLIP,\nexhibits significant improvements over baseline CLIP models across various\ntasks, including paraphrased retrieval (with rank similarity scores improved by\nup to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven\nsemantic textual similarity tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.15120v1.pdf"
    },
    {
        "title": "Large Multimodal Agents: A Survey",
        "authors": [
            "Junlin Xie",
            "Zhihong Chen",
            "Ruifei Zhang",
            "Xiang Wan",
            "Guanbin Li"
        ],
        "published": "2024-02-23T06:04:23Z",
        "summary": "Large language models (LLMs) have achieved superior performance in powering\ntext-based AI agents, endowing them with decision-making and reasoning\nabilities akin to humans. Concurrently, there is an emerging research trend\nfocused on extending these LLM-powered AI agents into the multimodal domain.\nThis extension enables AI agents to interpret and respond to diverse multimodal\nuser queries, thereby handling more intricate and nuanced tasks. In this paper,\nwe conduct a systematic review of LLM-driven multimodal agents, which we refer\nto as large multimodal agents ( LMAs for short). First, we introduce the\nessential components involved in developing LMAs and categorize the current\nbody of research into four distinct types. Subsequently, we review the\ncollaborative frameworks integrating multiple LMAs , enhancing collective\nefficacy. One of the critical challenges in this field is the diverse\nevaluation methods used across existing studies, hindering effective comparison\namong different LMAs . Therefore, we compile these evaluation methodologies and\nestablish a comprehensive framework to bridge the gaps. This framework aims to\nstandardize evaluations, facilitating more meaningful comparisons. Concluding\nour review, we highlight the extensive applications of LMAs and propose\npossible future research directions. Our discussion aims to provide valuable\ninsights and guidelines for future research in this rapidly evolving field. An\nup-to-date resource list is available at\nhttps://github.com/jun0wanan/awesome-large-multimodal-agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.15116v1.pdf"
    },
    {
        "title": "Executing Natural Language-Described Algorithms with Large Language Models: An Investigation",
        "authors": [
            "Xin Zheng",
            "Qiming Zhu",
            "Hongyu Lin",
            "Yaojie Lu",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2024-02-23T05:31:36Z",
        "summary": "Executing computer programs described in natural language has long been a\npursuit of computer science. With the advent of enhanced natural language\nunderstanding capabilities exhibited by large language models (LLMs), the path\ntoward this goal has been illuminated. In this paper, we seek to examine the\ncapacity of present-day LLMs to comprehend and execute algorithms outlined in\nnatural language. We established an algorithm test set sourced from\nIntroduction to Algorithm, a well-known textbook that contains many\nrepresentative widely-used algorithms. To systematically assess LLMs' code\nexecution abilities, we selected 30 algorithms, generated 300 random-sampled\ninstances in total, and evaluated whether popular LLMs can understand and\nexecute these algorithms. Our findings reveal that LLMs, notably GPT-4, can\neffectively execute programs described in natural language, as long as no heavy\nnumeric computation is involved. We believe our findings contribute to\nevaluating LLMs' code execution abilities and would encourage further\ninvestigation and application for the computation power of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.00795v2.pdf"
    },
    {
        "title": "A First Look at GPT Apps: Landscape and Vulnerability",
        "authors": [
            "Zejun Zhang",
            "Li Zhang",
            "Xin Yuan",
            "Anlan Zhang",
            "Mengwei Xu",
            "Feng Qian"
        ],
        "published": "2024-02-23T05:30:32Z",
        "summary": "With the advancement of Large Language Models (LLMs), increasingly\nsophisticated and powerful GPTs are entering the market. Despite their\npopularity, the LLM ecosystem still remains unexplored. Additionally, LLMs'\nsusceptibility to attacks raises concerns over safety and plagiarism. Thus, in\nthis work, we conduct a pioneering exploration of GPT stores, aiming to study\nvulnerabilities and plagiarism within GPT applications. To begin with, we\nconduct, to our knowledge, the first large-scale monitoring and analysis of two\nstores, an unofficial GPTStore.AI, and an official OpenAI GPT Store. Then, we\npropose a TriLevel GPT Reversing (T-GR) strategy for extracting GPT internals.\nTo complete these two tasks efficiently, we develop two automated tools: one\nfor web scraping and another designed for programmatically interacting with\nGPTs. Our findings reveal a significant enthusiasm among users and developers\nfor GPT interaction and creation, as evidenced by the rapid increase in GPTs\nand their creators. However, we also uncover a widespread failure to protect\nGPT internals, with nearly 90% of system prompts easily accessible, leading to\nconsiderable plagiarism and duplication among GPTs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15105v1.pdf"
    },
    {
        "title": "Studying LLM Performance on Closed- and Open-source Data",
        "authors": [
            "Toufique Ahmed",
            "Christian Bird",
            "Premkumar Devanbu",
            "Saikat Chakraborty"
        ],
        "published": "2024-02-23T05:17:28Z",
        "summary": "Large Language models (LLMs) are finding wide use in software engineering\npractice. These models are extremely data-hungry, and are largely trained on\nopen-source (OSS) code distributed with permissive licenses. In terms of actual\nuse however, a great deal of software development still occurs in the\nfor-profit/proprietary sphere, where the code under development is not, and\nnever has been, in the public domain; thus, many developers, do their work, and\nuse LLMs, in settings where the models may not be as familiar with the code\nunder development. In such settings, do LLMs work as well as they do for OSS\ncode? If not, what are the differences? When performance differs, what are the\npossible causes, and are there work-arounds? In this paper, we examine this\nissue using proprietary, closed-source software data from Microsoft, where most\nproprietary code is in C# and C++. We find that performance for C# changes\nlittle from OSS --> proprietary code, but does significantly reduce for C++; we\nfind that this difference is attributable to differences in identifiers. We\nalso find that some performance degradation, in some cases, can be ameliorated\nefficiently by in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.15100v1.pdf"
    },
    {
        "title": "Evaluating the Performance of ChatGPT for Spam Email Detection",
        "authors": [
            "Yuwei Wu",
            "Shijing Si",
            "Yugui Zhang",
            "Jiawen Gu",
            "Jedrek Wosik"
        ],
        "published": "2024-02-23T04:52:08Z",
        "summary": "Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction and a few\ndemonstrations. We also investigate how the training example size affects the\nperformance of ChatGPT. For comparison, we also implement five popular\nbenchmark methods, including naive Bayes, support vector machines (SVM),\nlogistic regression (LR), feedforward dense neural networks (DNN), and BERT\nclassifiers. Though extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset, even outperforming BERT in this case.",
        "pdf_link": "https://arxiv.org/pdf/2402.15537v1.pdf"
    },
    {
        "title": "AttributionBench: How Hard is Automatic Attribution Evaluation?",
        "authors": [
            "Yifei Li",
            "Xiang Yue",
            "Zeyi Liao",
            "Huan Sun"
        ],
        "published": "2024-02-23T04:23:33Z",
        "summary": "Modern generative search engines enhance the reliability of large language\nmodel (LLM) responses by providing cited evidence. However, evaluating the\nanswer's attribution, i.e., whether every claim within the generated responses\nis fully supported by its cited evidence, remains an open problem. This\nverification, traditionally dependent on costly human evaluation, underscores\nthe urgent need for automatic attribution evaluation methods. To bridge the gap\nin the absence of standardized benchmarks for these methods, we present\nAttributionBench, a comprehensive benchmark compiled from various existing\nattribution datasets. Our extensive experiments on AttributionBench reveal the\nchallenges of automatic attribution evaluation, even for state-of-the-art LLMs.\nSpecifically, our findings show that even a fine-tuned GPT-3.5 only achieves\naround 80% macro-F1 under a binary classification formulation. A detailed\nanalysis of more than 300 error cases indicates that a majority of failures\nstem from the model's inability to process nuanced information, and the\ndiscrepancy between the information the model has access to and that human\nannotators do.",
        "pdf_link": "https://arxiv.org/pdf/2402.15089v1.pdf"
    },
    {
        "title": "Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models",
        "authors": [
            "Zachary Horvitz",
            "Jingru Chen",
            "Rahul Aditya",
            "Harshvardhan Srivastava",
            "Robert West",
            "Zhou Yu",
            "Kathleen McKeown"
        ],
        "published": "2024-02-23T02:58:12Z",
        "summary": "Humor is a fundamental facet of human cognition and interaction. Yet, despite\nrecent advances in natural language processing, humor detection remains a\nchallenging task that is complicated by the scarcity of datasets that pair\nhumorous texts with similar non-humorous counterparts. In our work, we\ninvestigate whether large language models (LLMs), can generate synthetic data\nfor humor detection via editing texts. We benchmark LLMs on an existing human\ndataset and show that current LLMs display an impressive ability to `unfun'\njokes, as judged by humans and as measured on the downstream task of humor\ndetection. We extend our approach to a code-mixed English-Hindi humor dataset,\nwhere we find that GPT-4's synthetic data is highly rated by bilingual\nannotators and provides challenging adversarial examples for humor classifiers.",
        "pdf_link": "https://arxiv.org/pdf/2403.00794v1.pdf"
    },
    {
        "title": "Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions",
        "authors": [
            "Yang Deng",
            "Yong Zhao",
            "Moxin Li",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-23T02:24:36Z",
        "summary": "Despite the remarkable abilities of Large Language Models (LLMs) to answer\nquestions, they often display a considerable level of overconfidence even when\nthe question does not have a definitive answer. To avoid providing hallucinated\nanswers to these unknown questions, existing studies typically investigate\napproaches to refusing to answer these questions. In this work, we propose a\nnovel and scalable self-alignment method to utilize the LLM itself to enhance\nits response-ability to different types of unknown questions, being capable of\nnot only refusing to answer but also providing explanation to the\nunanswerability of unknown questions. Specifically, the Self-Align method first\nemploy a two-stage class-aware self-augmentation approach to generate a large\namount of unknown question-response data. Then we conduct disparity-driven\nself-curation to select qualified data for fine-tuning the LLM itself for\naligning the responses to unknown questions as desired. Experimental results on\ntwo datasets across four types of unknown questions validate the superiority of\nthe Self-Align method over existing baselines in terms of three types of task\nformulation.",
        "pdf_link": "https://arxiv.org/pdf/2402.15062v1.pdf"
    },
    {
        "title": "Fine-tuning Large Language Models for Domain-specific Machine Translation",
        "authors": [
            "Jiawei Zheng",
            "Hanghai Hong",
            "Xiaoli Wang",
            "Jingsong Su",
            "Yonggui Liang",
            "Shikai Wu"
        ],
        "published": "2024-02-23T02:24:15Z",
        "summary": "Large language models (LLMs) have made significant progress in machine\ntranslation (MT). However, their potential in domain-specific MT remains\nunder-explored. Current LLM-based MT systems still face several challenges.\nFirst, for LLMs with in-context learning, their effectiveness is highly\nsensitive to input translation examples, and processing them can increase\ninference costs. They often require extra post-processing due to\nover-generation. Second, LLMs with fine-tuning on domain-specific data often\nrequire high training costs for domain adaptation, and may weaken the zero-shot\nMT capabilities of LLMs due to over-specialization. The aforementioned methods\ncan struggle to translate rare words in domain transfer scenarios. To address\nthese challenges, this paper proposes a prompt-oriented fine-tuning method,\ndenoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose\nLLM for domain-specific MT tasks. First, we construct a task-specific\nmix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can\neliminate the need for input translation examples, post-processing, or\nover-specialization. By zero-shot prompting with instructions, we adapt the MT\ntasks to the target domain at inference time. To further elicit the MT\ncapability for rare words, we construct new prompts by incorporating\ndomain-specific bilingual vocabulary. We also conduct extensive experiments on\nboth publicly available and self-constructed datasets. The results show that\nour LlamaIT can significantly enhance the domain-specific MT capabilities of\nthe LLM, meanwhile preserving its zero-shot MT capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.15061v1.pdf"
    },
    {
        "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
        "authors": [
            "Yang Deng",
            "Xuan Zhang",
            "Wenxuan Zhang",
            "Yifei Yuan",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-23T02:18:12Z",
        "summary": "Web agents powered by Large Language Models (LLMs) have demonstrated\nremarkable abilities in planning and executing multi-step interactions within\ncomplex web-based environments, fulfilling a wide range of web navigation\ntasks. Despite these advancements, the potential for LLM-powered agents to\neffectively engage with sequential user instructions in real-world scenarios\nhas not been fully explored. In this work, we introduce a new task of\nConversational Web Navigation, which necessitates sophisticated interactions\nthat span multiple turns with both the users and the environment, supported by\na specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To\ntackle the limited context length of LLMs and the context-dependency issue of\nthe conversational tasks, we further propose a novel framework, named\nself-reflective memory-augmented planning (Self-MAP), which employs memory\nutilization and self-reflection techniques. Extensive experiments are conducted\nto benchmark the MT-Mind2Web dataset, and validate the effectiveness of the\nproposed method.",
        "pdf_link": "https://arxiv.org/pdf/2402.15057v1.pdf"
    },
    {
        "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
        "authors": [
            "Zhuang Chen",
            "Jincenzi Wu",
            "Jinfeng Zhou",
            "Bosi Wen",
            "Guanqun Bi",
            "Gongyao Jiang",
            "Yaru Cao",
            "Mengting Hu",
            "Yunghwei Lai",
            "Zexuan Xiong",
            "Minlie Huang"
        ],
        "published": "2024-02-23T02:05:46Z",
        "summary": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe\nmental states to oneself and others. Recent research has sparked a debate over\nwhether large language models (LLMs) exhibit a form of ToM. However, existing\nToM evaluations are hindered by challenges such as constrained scope,\nsubjective judgment, and unintended contamination, yielding inadequate\nassessments. To address this gap, we introduce ToMBench with three key\ncharacteristics: a systematic evaluation framework encompassing 8 tasks and 31\nabilities in social cognition, a multiple-choice question format to support\nautomated and unbiased evaluation, and a build-from-scratch bilingual inventory\nto strictly avoid data leakage. Based on ToMBench, we conduct extensive\nexperiments to evaluate the ToM performance of 10 popular LLMs across tasks and\nabilities. We find that even the most advanced LLMs like GPT-4 lag behind human\nperformance by over 10% points, indicating that LLMs have not achieved a\nhuman-level theory of mind yet. Our aim with ToMBench is to enable an efficient\nand effective evaluation of LLMs' ToM capabilities, thereby facilitating the\ndevelopment of LLMs with inherent social intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2402.15052v1.pdf"
    },
    {
        "title": "Unlocking the Power of Large Language Models for Entity Alignment",
        "authors": [
            "Xuhui Jiang",
            "Yinghan Shen",
            "Zhichao Shi",
            "Chengjin Xu",
            "Wei Li",
            "Zixuan Li",
            "Jian Guo",
            "Huawei Shen",
            "Yuanzhuo Wang"
        ],
        "published": "2024-02-23T01:55:35Z",
        "summary": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG)\ndata, playing a crucial role in data-driven AI applications. Traditional EA\nmethods primarily rely on comparing entity embeddings, but their effectiveness\nis constrained by the limited input KG data and the capabilities of the\nrepresentation learning techniques. Against this backdrop, we introduce ChatEA,\nan innovative framework that incorporates large language models (LLMs) to\nimprove EA. To address the constraints of limited input KG data, ChatEA\nintroduces a KG-code translation module that translates KG structures into a\nformat understandable by LLMs, thereby allowing LLMs to utilize their extensive\nbackground knowledge to improve EA accuracy. To overcome the over-reliance on\nentity embedding comparisons, ChatEA implements a two-stage EA strategy that\ncapitalizes on LLMs' capability for multi-step reasoning in a dialogue format,\nthereby enhancing accuracy while preserving efficiency. Our experimental\nresults affirm ChatEA's superior performance, highlighting LLMs' potential in\nfacilitating EA tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.15048v1.pdf"
    },
    {
        "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models",
        "authors": [
            "Zhuohao Yu",
            "Chang Gao",
            "Wenjin Yao",
            "Yidong Wang",
            "Wei Ye",
            "Jindong Wang",
            "Xing Xie",
            "Yue Zhang",
            "Shikun Zhang"
        ],
        "published": "2024-02-23T01:30:39Z",
        "summary": "Automatic evaluation methods for large language models (LLMs) are hindered by\ndata contamination, leading to inflated assessments of their effectiveness.\nExisting strategies, which aim to detect contaminated texts, focus on\nquantifying contamination status instead of accurately gauging model\nperformance. In this paper, we introduce KIEval, a Knowledge-grounded\nInteractive Evaluation framework, which incorporates an LLM-powered\n\"interactor\" role for the first time to accomplish a dynamic\ncontamination-resilient evaluation. Starting with a question in a conventional\nLLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically\ngenerated, multi-round, and knowledge-focused dialogues to determine whether a\nmodel's response is merely a recall of benchmark answers or demonstrates a deep\ncomprehension to apply knowledge in more complex conversations. Extensive\nexperiments on seven leading LLMs across five datasets validate KIEval's\neffectiveness and generalization. We also reveal that data contamination brings\nno contribution or even negative effect to models' real-world applicability and\nunderstanding, and existing contamination detection methods for LLMs can only\nidentify contamination in pre-training but not during supervised fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.15043v1.pdf"
    },
    {
        "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
        "authors": [
            "Santiago Castro",
            "Amir Ziai",
            "Avneesh Saluja",
            "Zhuoning Yuan",
            "Rada Mihalcea"
        ],
        "published": "2024-02-22T23:42:25Z",
        "summary": "Recent years have witnessed a significant increase in the performance of\nVision and Language tasks. Foundational Vision-Language Models (VLMs), such as\nCLIP, have been leveraged in multiple settings and demonstrated remarkable\nperformance across several tasks. Such models excel at object-centric\nrecognition yet learn text representations that seem invariant to word order,\nfailing to compose known concepts in novel ways. However, no evidence exists\nthat any VLM, including large-scale single-stream models such as GPT-4V,\nidentifies compositions successfully. In this paper, we introduce a framework\nto significantly improve the ability of existing models to encode compositional\nlanguage, with over 10% absolute improvement on compositionality benchmarks,\nwhile maintaining or improving the performance on standard object-recognition\nand retrieval benchmarks. Our code and pre-trained models are publicly\navailable at https://github.com/netflix/clove.",
        "pdf_link": "https://arxiv.org/pdf/2402.15021v2.pdf"
    },
    {
        "title": "Unintended Impacts of LLM Alignment on Global Representation",
        "authors": [
            "Michael J. Ryan",
            "William Held",
            "Diyi Yang"
        ],
        "published": "2024-02-22T23:31:22Z",
        "summary": "Before being deployed for user-facing applications, developers align Large\nLanguage Models (LLMs) to user preferences through a variety of procedures,\nsuch as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference\nOptimization (DPO). Current evaluations of these procedures focus on benchmarks\nof instruction following, reasoning, and truthfulness. However, human\npreferences are not universal, and aligning to specific preference sets may\nhave unintended effects. We explore how alignment impacts performance along\nthree axes of global representation: English dialects, multilingualism, and\nopinions from and about countries worldwide. Our results show that current\nalignment procedures create disparities between English dialects and global\nopinions. We find alignment improves capabilities in several languages. We\nconclude by discussing design decisions that led to these unintended impacts\nand recommendations for more equitable preference tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.15018v1.pdf"
    },
    {
        "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
        "authors": [
            "Zhuofeng Wu",
            "He Bai",
            "Aonan Zhang",
            "Jiatao Gu",
            "VG Vinod Vydiswaran",
            "Navdeep Jaitly",
            "Yizhe Zhang"
        ],
        "published": "2024-02-22T22:28:46Z",
        "summary": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.",
        "pdf_link": "https://arxiv.org/pdf/2402.15000v1.pdf"
    },
    {
        "title": "tinyBenchmarks: evaluating LLMs with fewer examples",
        "authors": [
            "Felipe Maia Polo",
            "Lucas Weber",
            "Leshem Choshen",
            "Yuekai Sun",
            "Gongjun Xu",
            "Mikhail Yurochkin"
        ],
        "published": "2024-02-22T22:05:23Z",
        "summary": "The versatility of large language models (LLMs) led to the creation of\ndiverse benchmarks that thoroughly test a variety of language models'\nabilities. These benchmarks consist of tens of thousands of examples making\nevaluation of LLMs very expensive. In this paper, we investigate strategies to\nreduce the number of evaluations needed to assess the performance of an LLM on\nseveral key benchmarks. For example, we show that to accurately estimate the\nperformance of an LLM on MMLU, a popular multiple-choice QA benchmark\nconsisting of 14K examples, it is sufficient to evaluate this LLM on 100\ncurated examples. We release evaluation tools and tiny versions of popular\nbenchmarks: Open LLM Leaderboard, MMLU, HELM, and AlpacaEval 2.0. Our empirical\nanalysis demonstrates that these tools and tiny benchmarks are sufficient to\nreliably and efficiently reproduce the original evaluation results.",
        "pdf_link": "https://arxiv.org/pdf/2402.14992v1.pdf"
    },
    {
        "title": "Optimizing Language Models for Human Preferences is a Causal Inference Problem",
        "authors": [
            "Victoria Lin",
            "Eli Ben-Michael",
            "Louis-Philippe Morency"
        ],
        "published": "2024-02-22T21:36:07Z",
        "summary": "As large language models (LLMs) see greater use in academic and commercial\nsettings, there is increasing interest in methods that allow language models to\ngenerate texts aligned with human preferences. In this paper, we present an\ninitial exploration of language model optimization for human preferences from\ndirect outcome datasets, where each sample consists of a text and an associated\nnumerical outcome measuring the reader's response. We first propose that\nlanguage model optimization should be viewed as a causal problem to ensure that\nthe model correctly learns the relationship between the text and the outcome.\nWe formalize this causal language optimization problem, and we develop a\nmethod--causal preference optimization (CPO)--that solves an unbiased surrogate\nobjective for the problem. We further extend CPO with doubly robust CPO\n(DR-CPO), which reduces the variance of the surrogate objective while retaining\nprovably strong guarantees on bias. Finally, we empirically demonstrate the\neffectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human\npreferences on direct outcome data, and we validate the robustness of DR-CPO\nunder difficult confounding conditions.",
        "pdf_link": "https://arxiv.org/pdf/2402.14979v1.pdf"
    },
    {
        "title": "AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation",
        "authors": [
            "Orit Shaer",
            "Angelora Cooper",
            "Osnat Mokryn",
            "Andrew L. Kun",
            "Hagit Ben Shoshan"
        ],
        "published": "2024-02-22T21:34:52Z",
        "summary": "The growing availability of generative AI technologies such as large language\nmodels (LLMs) has significant implications for creative work. This paper\nexplores twofold aspects of integrating LLMs into the creative process - the\ndivergence stage of idea generation, and the convergence stage of evaluation\nand selection of ideas. We devised a collaborative group-AI Brainwriting\nideation framework, which incorporated an LLM as an enhancement into the group\nideation process, and evaluated the idea generation process and the resulted\nsolution space. To assess the potential of using LLMs in the idea evaluation\nprocess, we design an evaluation engine and compared it to idea ratings\nassigned by three expert and six novice evaluators. Our findings suggest that\nintegrating LLM in Brainwriting could enhance both the ideation process and its\noutcome. We also provide evidence that LLMs can support idea evaluation. We\nconclude by discussing implications for HCI education and practice.",
        "pdf_link": "https://arxiv.org/pdf/2402.14978v2.pdf"
    },
    {
        "title": "GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data",
        "authors": [
            "Lele Cao",
            "Valentin Buchner",
            "Zineb Senane",
            "Fangkai Yang"
        ],
        "published": "2024-02-22T21:22:04Z",
        "summary": "Multimodal Large Language Models (MLLMs) are commonly evaluated using costly\nannotated multimodal benchmarks. However, these benchmarks often struggle to\nkeep pace with the rapidly advancing requirements of MLLM evaluation. We\npropose GenCeption, a novel and annotation-free MLLM evaluation framework that\nmerely requires unimodal data to assess inter-modality semantic coherence and\ninversely reflects the models' inclination to hallucinate. Analogous to the\npopular DrawCeption game, GenCeption initiates with a non-textual sample and\nundergoes a series of iterative description and generation steps. Semantic\ndrift across iterations is quantified using the GC@T metric. Our empirical\nfindings validate GenCeption's efficacy, showing strong correlations with\npopular MLLM benchmarking results. GenCeption may be extended to mitigate\ntraining data contamination by utilizing ubiquitous, previously unseen unimodal\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2402.14973v1.pdf"
    },
    {
        "title": "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment",
        "authors": [
            "Jiongxiao Wang",
            "Jiazhao Li",
            "Yiquan Li",
            "Xiangyu Qi",
            "Junjie Hu",
            "Yixuan Li",
            "Patrick McDaniel",
            "Muhao Chen",
            "Bo Li",
            "Chaowei Xiao"
        ],
        "published": "2024-02-22T21:05:18Z",
        "summary": "Despite the general capabilities of Large Language Models (LLMs) like GPT-4\nand Llama-2, these models still request fine-tuning or adaptation with\ncustomized data when it comes to meeting the specific business demands and\nintricacies of tailored use cases. However, this process inevitably introduces\nnew safety threats, particularly against the Fine-tuning based Jailbreak Attack\n(FJAttack), where incorporating just a few harmful examples into the\nfine-tuning dataset can significantly compromise the model safety. Though\npotential defenses have been proposed by incorporating safety examples into the\nfine-tuning dataset to reduce the safety issues, such approaches require\nincorporating a substantial amount of safety examples, making it inefficient.\nTo effectively defend against the FJAttack with limited safety examples, we\npropose a Backdoor Enhanced Safety Alignment method inspired by an analogy with\nthe concept of backdoor attacks. In particular, we construct prefixed safety\nexamples by integrating a secret prompt, acting as a \"backdoor trigger\", that\nis prefixed to safety examples. Our comprehensive experiments demonstrate that\nthrough the Backdoor Enhanced Safety Alignment with adding as few as 11\nprefixed safety examples, the maliciously fine-tuned LLMs will achieve similar\nsafety performance as the original aligned models. Furthermore, we also explore\nthe effectiveness of our method in a more practical setting where the\nfine-tuning data consists of both FJAttack examples and the fine-tuning task\ndata. Our method shows great efficacy in defending against FJAttack without\nharming the performance of fine-tuning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.14968v2.pdf"
    },
    {
        "title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
        "authors": [
            "Hanqi Yan",
            "Qinglin Zhu",
            "Xinyu Wang",
            "Lin Gui",
            "Yulan He"
        ],
        "published": "2024-02-22T20:57:17Z",
        "summary": "While Large language models (LLMs) have the capability to iteratively reflect\non their own outputs, recent studies have observed their struggles with\nknowledge-rich problems without access to external resources. In addition to\nthe inefficiency of LLMs in self-assessment, we also observe that LLMs struggle\nto revisit their predictions despite receiving explicit negative feedback.\nTherefore, We propose Mirror, a Multiple-perspective self-reflection method for\nknowledge-rich reasoning, to avoid getting stuck at a particular reflection\niteration. Mirror enables LLMs to reflect from multiple-perspective clues,\nachieved through a heuristic interaction between a Navigator and a Reasoner. It\nguides agents toward diverse yet plausibly reliable reasoning trajectory\nwithout access to ground truth by encouraging (1) diversity of directions\ngenerated by Navigator and (2) agreement among strategically induced\nperturbations in responses generated by the Reasoner. The experiments on five\nreasoning datasets demonstrate that Mirror's superiority over several\ncontemporary self-reflection approaches. Additionally, the ablation study\nstudies clearly indicate that our strategies alleviate the aforementioned\nchallenges.",
        "pdf_link": "https://arxiv.org/pdf/2402.14963v1.pdf"
    },
    {
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning",
        "authors": [
            "Zicheng Lin",
            "Zhibin Gou",
            "Tian Liang",
            "Ruilin Luo",
            "Haowei Liu",
            "Yujiu Yang"
        ],
        "published": "2024-02-22T18:59:02Z",
        "summary": "The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.",
        "pdf_link": "https://arxiv.org/pdf/2402.14809v2.pdf"
    },
    {
        "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases",
        "authors": [
            "Zechun Liu",
            "Changsheng Zhao",
            "Forrest Iandola",
            "Chen Lai",
            "Yuandong Tian",
            "Igor Fedorov",
            "Yunyang Xiong",
            "Ernie Chang",
            "Yangyang Shi",
            "Raghuraman Krishnamoorthi",
            "Liangzhen Lai",
            "Vikas Chandra"
        ],
        "published": "2024-02-22T18:58:55Z",
        "summary": "This paper addresses the growing need for efficient large language models\n(LLMs) on mobile devices, driven by increasing cloud costs and latency\nconcerns. We focus on designing top-quality LLMs with fewer than a billion\nparameters, a practical choice for mobile deployment. Contrary to prevailing\nbelief emphasizing the pivotal role of data and parameter quantity in\ndetermining model quality, our investigation underscores the significance of\nmodel architecture for sub-billion scale LLMs. Leveraging deep and thin\narchitectures, coupled with embedding sharing and grouped-query attention\nmechanisms, we establish a strong baseline network denoted as MobileLLM, which\nattains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M\nstate-of-the-art models. Additionally, we propose an immediate block-wise\nweight sharing approach with no increase in model size and only marginal\nlatency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a\nfurther accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,\nMobileLLM model family shows significant improvements compared to previous\nsub-billion models on chat benchmarks, and demonstrates close correctness to\nLLaMA-v2 7B in API calling tasks, highlighting the capability of small models\nfor common on-device use cases.",
        "pdf_link": "https://arxiv.org/pdf/2402.14905v1.pdf"
    },
    {
        "title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts",
        "authors": [
            "Lei Zhu",
            "Xinjiang Wang",
            "Wayne Zhang",
            "Rynson W. H. Lau"
        ],
        "published": "2024-02-22T18:58:28Z",
        "summary": "Practical large language model (LLM) services may involve a long system\nprompt, which specifies the instructions, examples, and knowledge documents of\nthe task and is reused across numerous requests. However, the long system\nprompt causes throughput/latency bottlenecks as the cost of generating the next\ntoken grows w.r.t. the sequence length. This paper aims to improve the\nefficiency of LLM services that involve long system prompts. Our key\nobservation is that handling these system prompts requires heavily redundant\nmemory accesses in existing causal attention computation algorithms.\nSpecifically, for batched requests, the cached hidden states (i.e., key-value\npairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM\nmultiple times, each corresponding to an individual request. To eliminate such\na redundancy, we propose RelayAttention, an attention algorithm that allows\nreading these hidden states from DRAM exactly once for a batch of input tokens.\nRelayAttention is a free lunch: it maintains the generation quality while\nrequiring no model retraining, as it is based on a mathematical reformulation\nof causal attention. Code is available at\n\\url{https://github.com/rayleizhu/vllm-ra}.",
        "pdf_link": "https://arxiv.org/pdf/2402.14808v2.pdf"
    },
    {
        "title": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
        "authors": [
            "Xiaoyang Song",
            "Yuta Adachi",
            "Jessie Feng",
            "Mouwei Lin",
            "Linhao Yu",
            "Frank Li",
            "Akshat Gupta",
            "Gopala Anumanchipalli",
            "Simerjot Kaur"
        ],
        "published": "2024-02-22T18:57:20Z",
        "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14805v1.pdf"
    },
    {
        "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models",
        "authors": [
            "Xudong Lu",
            "Qi Liu",
            "Yuhui Xu",
            "Aojun Zhou",
            "Siyuan Huang",
            "Bo Zhang",
            "Junchi Yan",
            "Hongsheng Li"
        ],
        "published": "2024-02-22T18:56:07Z",
        "summary": "A pivotal advancement in the progress of large language models (LLMs) is the\nemergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs,\nMoE LLMs can achieve higher performance with fewer parameters, but it is still\nhard to deploy them due to their immense parameter sizes. Different from\nprevious weight pruning methods that rely on specifically designed hardware,\nthis paper mainly aims to enhance the deployment efficiency of MoE LLMs by\nintroducing plug-and-play expert-level sparsification techniques. Specifically,\nwe propose, for the first time to our best knowledge, post-training approaches\nfor task-agnostic and task-specific expert pruning and skipping of MoE LLMs,\ntailored to improve deployment efficiency while maintaining model performance\nacross a wide range of tasks. Extensive experiments show that our proposed\nmethods can simultaneously reduce model sizes and increase the inference speed,\nwhile maintaining satisfactory performance. Data and code will be available at\nhttps://github.com/Lucky-Lance/Expert_Sparsity.",
        "pdf_link": "https://arxiv.org/pdf/2402.14800v1.pdf"
    },
    {
        "title": "Watermarking Makes Language Models Radioactive",
        "authors": [
            "Tom Sander",
            "Pierre Fernandez",
            "Alain Durmus",
            "Matthijs Douze",
            "Teddy Furon"
        ],
        "published": "2024-02-22T18:55:22Z",
        "summary": "This paper investigates the radioactivity of LLM-generated texts, i.e.\nwhether it is possible to detect that such input was used as training data.\nConventional methods like membership inference can carry out this detection\nwith some level of accuracy. We show that watermarked training data leaves\ntraces easier to detect and much more reliable than membership inference. We\nlink the contamination level to the watermark robustness, its proportion in the\ntraining set, and the fine-tuning process. We notably demonstrate that training\non watermarked synthetic instructions can be detected with high confidence\n(p-value < 1e-5) even when as little as 5% of training text is watermarked.\nThus, LLM watermarking, originally designed for detecting machine-generated\ntext, gives the ability to easily identify if the outputs of a watermarked LLM\nwere used to fine-tune another LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.14904v1.pdf"
    },
    {
        "title": "Zero-shot cross-lingual transfer in instruction tuning of large language model",
        "authors": [
            "Nadezhda Chirkova",
            "Vassilina Nikoulina"
        ],
        "published": "2024-02-22T18:37:33Z",
        "summary": "Instruction tuning (IT) is widely used to teach pretrained large language\nmodels (LLMs) to follow arbitrary instructions, but is under-studied in\nmultilingual settings. In this work, we conduct a systematic study of zero-shot\ncross-lingual transfer in IT, when an LLM is instruction-tuned on English-only\ndata and then tested on user prompts in other languages. We investigate the\ninfluence of model configuration choices and devise a multi-facet evaluation\nstrategy for multilingual instruction following. We find that cross-lingual\ntransfer does happen successfully in IT even if all stages of model training\nare English-centric, but only if multiliguality is taken into account in\nhyperparameter tuning and with large enough IT data. English-trained LLMs are\ncapable of generating correct-language, comprehensive and helpful responses in\nthe other languages, but suffer from low factuality and may occasionally have\nfluency errors.",
        "pdf_link": "https://arxiv.org/pdf/2402.14778v1.pdf"
    },
    {
        "title": "DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models",
        "authors": [
            "Yuhang Cao",
            "Pan Zhang",
            "Xiaoyi Dong",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "published": "2024-02-22T18:26:02Z",
        "summary": "We present DualFocus, a novel framework for integrating macro and micro\nperspectives within multi-modal large language models (MLLMs) to enhance\nvision-language task performance. Current MLLMs typically singularly focus on\ninputs at a predefined resolution, resulting in deficiencies in detailed\nquestions involving local regions. We introduced a DualFocus mechanism where\nthe model concentrates on the image from a macro perspective, responses to the\nquestion, and identifies suitable sub-regions to zoom in for subsequent micro\nperspective analysis. Via the integration of answers from both macro and micro\nperspectives, the model is adept at addressing tasks that encompass global,\ndetailed, and combined considerations. To endows the DualFocus mechanism in\nMLLMs, we curated a tailored dataset derived from the Visual Genome (VG) and\nadapted it to align with the training regimen of DualFocus. Through comparative\nstudies across different model sizes and benchmarks, we demonstrate DualFocus's\nsuperiority in balancing detailed examination with holistic insight,\nsignificantly reducing hallucination instances in MLLMs and improving their\nperformance in various vision-language tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.14767v1.pdf"
    },
    {
        "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues",
        "authors": [
            "Ge Bai",
            "Jie Liu",
            "Xingyuan Bu",
            "Yancheng He",
            "Jiaheng Liu",
            "Zhanhui Zhou",
            "Zhuoran Lin",
            "Wenbo Su",
            "Tiezheng Ge",
            "Bo Zheng",
            "Wanli Ouyang"
        ],
        "published": "2024-02-22T18:21:59Z",
        "summary": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.14762v1.pdf"
    },
    {
        "title": "Generalizing Reward Modeling for Out-of-Distribution Preference Learning",
        "authors": [
            "Chen Jia"
        ],
        "published": "2024-02-22T18:20:33Z",
        "summary": "Preference learning (PL) with large language models (LLMs) aims to align the\nLLMs' generations with human preferences. Previous work on reinforcement\nlearning from human feedback (RLHF) has demonstrated promising results in\nin-distribution PL. However, due to the difficulty of obtaining human feedback,\ndiscretely training reward models for every encountered distribution is\nchallenging. Thus, out-of-distribution (OOD) PL is practically useful for\nenhancing the generalization ability of LLMs with limited preference feedback.\nThis work addresses OOD PL by optimizing a general reward model through a\nmeta-learning approach. During meta-training, a bilevel optimization algorithm\nis utilized to learn a reward model capable of guiding policy learning to align\nwith human preferences across various distributions. When encountering a test\ndistribution, the meta-test procedure conducts regularized policy optimization\nusing the learned reward model for PL. We theoretically demonstrate the\nconvergence rate of the bilevel optimization algorithm under reasonable\nassumptions. Additionally, we conduct experiments on two text generation tasks\nacross 20 held-out domains and outperform a variety of strong baselines across\nvarious evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.14760v1.pdf"
    },
    {
        "title": "Tokenization counts: the impact of tokenization on arithmetic in frontier LLMs",
        "authors": [
            "Aaditya K. Singh",
            "DJ Strouse"
        ],
        "published": "2024-02-22T18:14:09Z",
        "summary": "Tokenization, the division of input text into input tokens, is an often\noverlooked aspect of the large language model (LLM) pipeline and could be the\nsource of useful or harmful inductive biases. Historically, LLMs have relied on\nbyte pair encoding, without care to specific input domains. With the increased\nuse of LLMs for reasoning, various number-specific tokenization schemes have\nbeen adopted, with popular models like LLaMa and PaLM opting for single-digit\ntokenization while GPT-3.5 and GPT-4 have separate tokens for each 1-, 2-, and\n3-digit numbers. In this work, we study the effect this choice has on numerical\nreasoning through the use of arithmetic tasks. We consider left-to-right and\nright-to-left tokenization for GPT-3.5 and -4, finding that right-to-left\ntokenization (enforced by comma separating numbers at inference time) leads to\nlargely improved performance. Furthermore, we find that model errors when using\nstandard left-to-right tokenization follow stereotyped error patterns,\nsuggesting that model computations are systematic rather than approximate. We\nshow that the model is able to convert between tokenizations easily, thus\nallowing chain-of-thought-inspired approaches to recover performance on\nleft-to-right tokenized inputs. We also find the gap between tokenization\ndirections decreases when models are scaled, possibly indicating that larger\nmodels are better able to override this tokenization-dependent inductive bias.\nIn summary, our work performs the first study of how number tokenization\nchoices lead to differences in model performance on arithmetic tasks,\naccompanied by a thorough analysis of error patterns. We hope this work\ninspires practitioners to more carefully ablate number tokenization-related\nchoices when working towards general models of numerical reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.14903v1.pdf"
    },
    {
        "title": "Scaling Efficient LLMs",
        "authors": [
            "B. N. Kausik"
        ],
        "published": "2024-02-22T18:06:19Z",
        "summary": "Trained LLMs are typically sparse in that most of the parameters are zero,\nraising questions on efficiency. In response, we inquire into efficient LLMs,\ni.e. those with the fewest parameters that achieve the desired accuracy on a\ntraining corpus. Specifically, we compare theoretical and empirical estimates\nfor training loss at current scale to obtain upper and lower bounds on the\nnumber of unique sequences in a natural training corpus as a function of its\nsize. Our result implies (1) to double the number of skills represented in a\ntraining corpus, the corpus must scale roughly between three and five fold (2)\nfor efficient LLMs, the number of parameters $N$ and the size $D$ of a natural\ntraining corpus scale as $N \\sim D^{0.58}$ (3) if the number of parameters of\nan LLM is smaller than the number of unique sequences in the training corpus,\nscaling up can uncover emergent skills.",
        "pdf_link": "https://arxiv.org/pdf/2402.14746v1.pdf"
    },
    {
        "title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation",
        "authors": [
            "Jiawei Wang",
            "Renhe Jiang",
            "Chuang Yang",
            "Zengqing Wu",
            "Makoto Onizuka",
            "Ryosuke Shibasaki",
            "Chuan Xiao"
        ],
        "published": "2024-02-22T18:03:14Z",
        "summary": "This paper introduces a novel approach using Large Language Models (LLMs)\nintegrated into an agent framework for flexible and efficient personal mobility\ngeneration. LLMs overcome the limitations of previous models by efficiently\nprocessing semantic data and offering versatility in modeling various tasks.\nOur approach addresses the critical need to align LLMs with real-world urban\nmobility data, focusing on three research questions: aligning LLMs with rich\nactivity data, developing reliable activity generation strategies, and\nexploring LLM applications in urban mobility. The key technical contribution is\na novel LLM agent framework that accounts for individual activity patterns and\nmotivations, including a self-consistency approach to align LLMs with\nreal-world activity data and a retrieval-augmented strategy for interpretable\nactivity generation. In experimental studies, comprehensive validation is\nperformed using real-world data. This research marks the pioneering work of\ndesigning an LLM agent framework for activity generation based on real-world\nhuman activity data, offering a promising tool for urban mobility analysis.",
        "pdf_link": "https://arxiv.org/pdf/2402.14744v1.pdf"
    },
    {
        "title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs",
        "authors": [
            "Arash Ahmadian",
            "Chris Cremer",
            "Matthias Gall\u00e9",
            "Marzieh Fadaee",
            "Julia Kreutzer",
            "Olivier Pietquin",
            "Ahmet \u00dcst\u00fcn",
            "Sara Hooker"
        ],
        "published": "2024-02-22T17:52:34Z",
        "summary": "AI alignment in the shape of Reinforcement Learning from Human Feedback\n(RLHF) is increasingly treated as a crucial ingredient for high performance\nlarge language models. Proximal Policy Optimization (PPO) has been positioned\nby recent literature as the canonical method for the RL part of RLHF. However,\nit involves both high computational cost and sensitive hyperparameter tuning.\nWe posit that most of the motivational principles that led to the development\nof PPO are less of a practical concern in RLHF and advocate for a less\ncomputationally expensive method that preserves and even increases performance.\nWe revisit the formulation of alignment from human preferences in the context\nof RL. Keeping simplicity as a guiding principle, we show that many components\nof PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style\noptimization variants outperform both PPO and newly proposed \"RL-free\" methods\nsuch as DPO and RAFT. Our work suggests that careful adaptation to LLMs\nalignment characteristics enables benefiting from online RL optimization at low\ncost.",
        "pdf_link": "https://arxiv.org/pdf/2402.14740v2.pdf"
    },
    {
        "title": "Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning Meets Adversarial Images",
        "authors": [
            "Zefeng Wang",
            "Zhen Han",
            "Shuo Chen",
            "Fan Xue",
            "Zifeng Ding",
            "Xun Xiao",
            "Volker Tresp",
            "Philip Torr",
            "Jindong Gu"
        ],
        "published": "2024-02-22T17:36:34Z",
        "summary": "Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand\nimages. However, like traditional vision models, they are still vulnerable to\nadversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely\nexplored on MLLMs, which not only improves model's performance, but also\nenhances model's explainability by giving intermediate reasoning steps.\nNevertheless, there is still a lack of study regarding MLLMs' adversarial\nrobustness with CoT and an understanding of what the rationale looks like when\nMLLMs infer wrong answers with adversarial images. Our research evaluates the\nadversarial robustness of MLLMs when employing CoT reasoning, finding that CoT\nmarginally improves adversarial robustness against existing attack methods.\nMoreover, we introduce a novel stop-reasoning attack technique that effectively\nbypasses the CoT-induced robustness enhancements. Finally, we demonstrate the\nalterations in CoT reasoning when MLLMs confront adversarial images, shedding\nlight on their reasoning process under adversarial attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.14899v2.pdf"
    },
    {
        "title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy",
        "authors": [
            "Oliver Bentham",
            "Nathan Stringham",
            "Ana Marasovi\u0107"
        ],
        "published": "2024-02-22T17:23:53Z",
        "summary": "Understanding the extent to which Chain-of-Thought (CoT) generations align\nwith a large language model's (LLM) internal computations is critical for\ndeciding whether to trust an LLM's output. As a proxy for CoT faithfulness,\narXiv:2307.13702 propose a metric that measures a model's dependence on its CoT\nfor producing an answer. Within a single family of proprietary models, they\nfind that LLMs exhibit a scaling-then-inverse-scaling relationship between\nmodel size and their measure of faithfulness, and that a 13 billion parameter\nmodel exhibits increased faithfulness compared to models ranging from 810\nmillion to 175 billion parameters in size. We evaluate whether these results\ngeneralize as a property of all LLMs. We replicate their experimental setup\nwith three different families of models and, under specific conditions,\nsuccessfully reproduce the scaling trends for CoT faithfulness they report.\nHowever, we discover that simply changing the order of answer choices in the\nprompt can reduce the metric by 73 percentage points. The faithfulness metric\nis also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about\nits validity as a construct for evaluating faithfulness.",
        "pdf_link": "https://arxiv.org/pdf/2402.14897v1.pdf"
    },
    {
        "title": "IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus",
        "authors": [
            "Honghao Gui",
            "Lin Yuan",
            "Hongbin Ye",
            "Ningyu Zhang",
            "Mengshu Sun",
            "Lei Liang",
            "Huajun Chen"
        ],
        "published": "2024-02-22T17:11:38Z",
        "summary": "Large Language Models (LLMs) demonstrate remarkable potential across various\ndomains; however, they exhibit a significant performance gap in Information\nExtraction (IE). Note that high-quality instruction data is the vital key for\nenhancing the specific capabilities of LLMs, while current IE datasets tend to\nbe small in scale, fragmented, and lack standardized schema. To this end, we\nintroduce IEPile, a comprehensive bilingual (English and Chinese) IE\ninstruction corpus, which contains approximately 0.32B tokens. We construct\nIEPile by collecting and cleaning 33 existing IE datasets, and introduce\nschema-based instruction generation to unearth a large-scale corpus.\nExperimental results on LLaMA, Baichuan and Qwen demonstrate that using IEPile\ncan enhance the performance of LLMs for IE, especially the zero-shot\ngeneralization. We open-source the resource and pre-trained models, hoping to\nprovide valuable support to the NLP community.",
        "pdf_link": "https://arxiv.org/pdf/2402.14710v2.pdf"
    },
    {
        "title": "An LLM-Enhanced Adversarial Editing System for Lexical Simplification",
        "authors": [
            "Keren Tan",
            "Kangyang Luo",
            "Yunshi Lan",
            "Zheng Yuan",
            "Jinlong Shu"
        ],
        "published": "2024-02-22T17:04:30Z",
        "summary": "Lexical Simplification (LS) aims to simplify text at the lexical level.\nExisting methods rely heavily on annotated data, making it challenging to apply\nin low-resource scenarios. In this paper, we propose a novel LS method without\nparallel corpora. This method employs an Adversarial Editing System with\nguidance from a confusion loss and an invariance loss to predict lexical edits\nin the original sentences. Meanwhile, we introduce an innovative LLM-enhanced\nloss to enable the distillation of knowledge from Large Language Models (LLMs)\ninto a small-size LS system. From that, complex words within sentences are\nmasked and a Difficulty-aware Filling module is crafted to replace masked\npositions with simpler words. At last, extensive experimental results and\nanalyses on three benchmark LS datasets demonstrate the effectiveness of our\nproposed method.",
        "pdf_link": "https://arxiv.org/pdf/2402.14704v3.pdf"
    },
    {
        "title": "Unveiling Linguistic Regions in Large Language Models",
        "authors": [
            "Zhihao Zhang",
            "Jun Zhao",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-02-22T16:56:13Z",
        "summary": "Large Language Models (LLMs) have demonstrated considerable cross-lingual\nalignment and generalization ability. Current research primarily focuses on\nimproving LLMs' cross-lingual generalization capabilities. However, there is\nstill a lack of research on the intrinsic mechanisms of how LLMs achieve\ncross-lingual alignment. From the perspective of region partitioning, this\npaper conducts several investigations on the linguistic competence of LLMs. We\ndiscover a core region in LLMs that corresponds to linguistic competence,\naccounting for approximately 1% of the total model parameters. Removing this\ncore region by setting parameters to zero results in a significant performance\ndecrease across 30 different languages. Furthermore, this core region exhibits\nsignificant dimensional dependency, perturbations to even a single parameter on\nspecific dimensions leading to a loss of linguistic competence. Moreover, we\ndiscover that distinct regions exist for different monolingual families, and\ndisruption to these specific regions substantially reduces the LLMs'\nproficiency in those corresponding languages. Our research also indicates that\nfreezing the core linguistic region during further pre-training can mitigate\nthe issue of catastrophic forgetting (CF), a common occurrence observed during\nfurther pre-training of LLMs. Overall, exploring the LLMs' functional regions\nprovides insights into the foundation of their intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2402.14700v1.pdf"
    },
    {
        "title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models",
        "authors": [
            "Zhaoheng Huang",
            "Zhicheng Dou",
            "Yutao Zhu",
            "Ji-rong Wen"
        ],
        "published": "2024-02-22T16:45:32Z",
        "summary": "Large language models (LLMs) may generate text that lacks consistency with\nhuman knowledge, leading to factual inaccuracies or \\textit{hallucination}.\nExisting research for evaluating the factuality of LLMs involves extracting\nfact claims using an LLM and verifying them against a predefined fact source.\nHowever, these evaluation metrics are task-specific, and not scalable, and the\nsubstitutability of fact sources in different tasks is under-explored. To\naddress these challenges, we categorize four available fact sources:\nhuman-written evidence, reference documents, search engine results, and LLM\nknowledge, along with five text generation tasks containing six representative\ndatasets. Then, we propose \\texttt{UFO}, an LLM-based unified and flexible\nevaluation framework to verify facts against plug-and-play fact sources. We\nimplement five evaluation scenarios based on this framework. Experimental\nresults show that for most QA tasks, human-written evidence and reference\ndocuments are crucial, and they can substitute for each other in\nretrieval-augmented QA tasks. In news fact generation tasks, search engine\nresults and LLM knowledge are essential. Our dataset and code are available at\n\\url{https://github.com/WaldenRUC/UFO}.",
        "pdf_link": "https://arxiv.org/pdf/2402.14690v1.pdf"
    },
    {
        "title": "Visual Hallucinations of Multi-modal Large Language Models",
        "authors": [
            "Wen Huang",
            "Hongbin Liu",
            "Minxin Guo",
            "Neil Zhenqiang Gong"
        ],
        "published": "2024-02-22T16:40:33Z",
        "summary": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines\nincorrect details about an image in visual question answering. Existing studies\nfind VH instances only in existing image datasets, which results in biased\nunderstanding of MLLMs' performance under VH due to limited diversity of such\nVH instances. In this work, we propose a tool called VHTest to generate a\ndiverse set of VH instances. Specifically, VHTest finds some initial VH\ninstances in existing image datasets (e.g., COCO), generates a text description\nfor each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to\ngenerate VH images based on the text descriptions. We collect a benchmark\ndataset with 1,200 VH instances in 8 VH modes using VHTest. We find that\nexisting MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a\nlarge fraction of the instances in our benchmark. Moreover, we find that\nfine-tuning an MLLM using our benchmark dataset reduces its likelihood to\nhallucinate without sacrificing its performance on other benchmarks. Our\nbenchmarks are publicly available: https://github.com/wenhuang2000/VHTest.",
        "pdf_link": "https://arxiv.org/pdf/2402.14683v1.pdf"
    },
    {
        "title": "Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality",
        "authors": [
            "Yiming Ai",
            "Zhiwei He",
            "Ziyin Zhang",
            "Wenhong Zhu",
            "Hongkun Hao",
            "Kai Yu",
            "Lingjun Chen",
            "Rui Wang"
        ],
        "published": "2024-02-22T16:32:08Z",
        "summary": "In this study, we investigate the reliability of Large Language Models (LLMs)\nin professing human-like personality traits through responses to personality\nquestionnaires. Our goal is to evaluate the consistency between LLMs' professed\npersonality inclinations and their actual \"behavior\", examining the extent to\nwhich these models can emulate human-like personality patterns. Through a\ncomprehensive analysis of LLM outputs against established human benchmarks, we\nseek to understand the cognition-action divergence in LLMs and propose\nhypotheses for the observed results based on psychological theories and\nmetrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.14679v1.pdf"
    },
    {
        "title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
        "authors": [
            "Yanan Wu",
            "Jie Liu",
            "Xingyuan Bu",
            "Jiaheng Liu",
            "Zhanhui Zhou",
            "Yuanxing Zhang",
            "Chenchen Zhang",
            "Zhiqi Bai",
            "Haibin Chen",
            "Tiezheng Ge",
            "Wanli Ouyang",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "published": "2024-02-22T16:06:49Z",
        "summary": "This paper introduces ConceptMath, a bilingual (English and Chinese),\nfine-grained benchmark that evaluates concept-wise mathematical reasoning of\nLarge Language Models (LLMs). Unlike traditional benchmarks that evaluate\ngeneral mathematical reasoning with an average accuracy, ConceptMath\nsystematically organizes math problems under a hierarchy of math concepts, so\nthat mathematical reasoning can be evaluated at different granularity with\nconcept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range\nof LLMs, and we observe existing LLMs, though achieving high average accuracies\non traditional benchmarks, exhibit significant performance variations across\ndifferent math concepts and may even fail catastrophically on the most basic\nones. Besides, we also introduce an efficient fine-tuning strategy to enhance\nthe weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the\ndevelopers to understand the fine-grained mathematical abilities of their\nmodels and facilitate the growth of foundation models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14660v2.pdf"
    },
    {
        "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement",
        "authors": [
            "Tianyu Zheng",
            "Ge Zhang",
            "Tianhao Shen",
            "Xueling Liu",
            "Bill Yuchen Lin",
            "Jie Fu",
            "Wenhu Chen",
            "Xiang Yue"
        ],
        "published": "2024-02-22T16:06:23Z",
        "summary": "The introduction of large language models has significantly advanced code\ngeneration. However, open-source models often lack the execution capabilities\nand iterative refinement of advanced systems like the GPT-4 Code Interpreter.\nTo address this, we introduce OpenCodeInterpreter, a family of open-source code\nsystems designed for generating, executing, and iteratively refining code.\nSupported by Code-Feedback, a dataset featuring 68K multi-turn interactions,\nOpenCodeInterpreter integrates execution and human feedback for dynamic code\nrefinement. Our comprehensive evaluation of OpenCodeInterpreter across key\nbenchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus\nreveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves\nan accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and\nMBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6)\nwith synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap\nbetween open-source code generation models and proprietary systems like GPT-4\nCode Interpreter.",
        "pdf_link": "https://arxiv.org/pdf/2402.14658v2.pdf"
    },
    {
        "title": "LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition",
        "authors": [
            "Junjie Ye",
            "Nuo Xu",
            "Yikun Wang",
            "Jie Zhou",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-02-22T14:19:56Z",
        "summary": "Despite the impressive capabilities of large language models (LLMs), their\nperformance on information extraction tasks is still not entirely satisfactory.\nHowever, their remarkable rewriting capabilities and extensive world knowledge\noffer valuable insights to improve these tasks. In this paper, we propose\n$LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot\nNER task. To overcome the limitations of existing data augmentation methods\nthat compromise semantic integrity and address the uncertainty inherent in\nLLM-generated text, we leverage the distinctive characteristics of the NER task\nby augmenting the original data at both the contextual and entity levels. Our\napproach involves employing 14 contextual rewriting strategies, designing\nentity replacements of the same type, and incorporating noise injection to\nenhance robustness. Extensive experiments demonstrate the effectiveness of our\napproach in enhancing NER model performance with limited data. Furthermore,\nadditional analyses provide further evidence supporting the assertion that the\nquality of the data we generate surpasses that of other existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.14568v1.pdf"
    },
    {
        "title": "LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey",
        "authors": [
            "Ashok Urlana",
            "Charaka Vinayak Kumar",
            "Ajeet Kumar Singh",
            "Bala Mallikarjunarao Garlapati",
            "Srinivasa Rao Chalamala",
            "Rahul Mishra"
        ],
        "published": "2024-02-22T13:52:02Z",
        "summary": "Large language models (LLMs) have become the secret ingredient driving\nnumerous industrial applications, showcasing their remarkable versatility\nacross a diverse spectrum of tasks. From natural language processing and\nsentiment analysis to content generation and personalized recommendations,\ntheir unparalleled adaptability has facilitated widespread adoption across\nindustries. This transformative shift driven by LLMs underscores the need to\nexplore the underlying associated challenges and avenues for enhancement in\ntheir utilization. In this paper, our objective is to unravel and evaluate the\nobstacles and opportunities inherent in leveraging LLMs within an industrial\ncontext. To this end, we conduct a survey involving a group of industry\npractitioners, develop four research questions derived from the insights\ngathered, and examine 68 industry papers to address these questions and derive\nmeaningful conclusions.",
        "pdf_link": "https://arxiv.org/pdf/2402.14558v1.pdf"
    },
    {
        "title": "Whose LLM is it Anyway? Linguistic Comparison and LLM Attribution for GPT-3.5, GPT-4 and Bard",
        "authors": [
            "Ariel Rosenfeld",
            "Teddy Lazebnik"
        ],
        "published": "2024-02-22T13:25:17Z",
        "summary": "Large Language Models (LLMs) are capable of generating text that is similar\nto or surpasses human quality. However, it is unclear whether LLMs tend to\nexhibit distinctive linguistic styles akin to how human authors do. Through a\ncomprehensive linguistic analysis, we compare the vocabulary, Part-Of-Speech\n(POS) distribution, dependency distribution, and sentiment of texts generated\nby three of the most popular LLMS today (GPT-3.5, GPT-4, and Bard) to diverse\ninputs. The results point to significant linguistic variations which, in turn,\nenable us to attribute a given text to its LLM origin with a favorable 88\\%\naccuracy using a simple off-the-shelf classification model. Theoretical and\npractical implications of this intriguing finding are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2402.14533v1.pdf"
    },
    {
        "title": "Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance",
        "authors": [
            "Ziqi Yin",
            "Hao Wang",
            "Kaito Horio",
            "Daisuke Kawahara",
            "Satoshi Sekine"
        ],
        "published": "2024-02-22T13:24:10Z",
        "summary": "We investigate the impact of politeness levels in prompts on the performance\nof large language models (LLMs). Polite language in human communications often\ngarners more compliance and effectiveness, while rudeness can cause aversion,\nimpacting response quality. We consider that LLMs mirror human communication\ntraits, suggesting they align with human cultural norms. We assess the impact\nof politeness in prompts on LLMs across English, Chinese, and Japanese tasks.\nWe observed that impolite prompts often result in poor performance, but overly\npolite language does not guarantee better outcomes. The best politeness level\nis different according to the language. This phenomenon suggests that LLMs not\nonly reflect human behavior but are also influenced by language, particularly\nin different cultural contexts. Our findings highlight the need to factor in\npoliteness for cross-cultural natural language processing and LLM usage.",
        "pdf_link": "https://arxiv.org/pdf/2402.14531v1.pdf"
    },
    {
        "title": "Balanced Data Sampling for Language Model Training with Clustering",
        "authors": [
            "Yunfan Shao",
            "Linyang Li",
            "Zhaoye Fei",
            "Hang Yan",
            "Dahua Lin",
            "Xipeng Qiu"
        ],
        "published": "2024-02-22T13:20:53Z",
        "summary": "Data plays a fundamental role in the training of Large Language Models\n(LLMs). While attention has been paid to the collection and composition of\ndatasets, determining the data sampling strategy in training remains an open\nquestion. Most LLMs are trained with a simple strategy, random sampling.\nHowever, this sampling strategy ignores the unbalanced nature of training data\ndistribution, which can be sub-optimal. In this paper, we propose ClusterClip\nSampling to balance the text distribution of training data for better model\ntraining. Specifically, ClusterClip Sampling utilizes data clustering to\nreflect the data distribution of the training set and balances the common\nsamples and rare samples during training based on the cluster results. A\nrepetition clip operation is introduced to mitigate the overfitting issue led\nby samples from certain clusters. Extensive experiments validate the\neffectiveness of ClusterClip Sampling, which outperforms random sampling and\nother cluster-based sampling variants under various training datasets and large\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14526v1.pdf"
    },
    {
        "title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond",
        "authors": [
            "Xinyu Wang",
            "Hainiu Xu",
            "Lin Gui",
            "Yulan He"
        ],
        "published": "2024-02-22T13:13:31Z",
        "summary": "Task embedding, a meta-learning technique that captures task-specific\ninformation, has become prevalent, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradientfree manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To unleash the\npower of task embedding in the era of LLMs, we propose a framework for unified\ntask embeddings (FUTE), harmonizing task embeddings from various models,\nincluding smaller language models and LLMs with varied prompts, within a single\nvector space. Such uniformity enables the comparison and analysis of\nsimilarities amongst different models, extending the scope and utility of\nexisting task embedding methods in addressing multi-model scenarios, whilst\nmaintaining their performance to be comparable to architecture-specific\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.14522v1.pdf"
    },
    {
        "title": "\"My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",
        "authors": [
            "Xinpeng Wang",
            "Bolei Ma",
            "Chengzhi Hu",
            "Leon Weber-Genzel",
            "Paul R\u00f6ttger",
            "Frauke Kreuter",
            "Dirk Hovy",
            "Barbara Plank"
        ],
        "published": "2024-02-22T12:47:33Z",
        "summary": "The open-ended nature of language generation makes the evaluation of\nautoregressive large language models (LLMs) challenging. One common evaluation\napproach uses multiple-choice questions (MCQ) to limit the response space. The\nmodel is then evaluated by ranking the candidate answers by the log probability\nof the first token prediction. However, first-tokens may not consistently\nreflect the final response output, due to model's diverse response styles such\nas starting with \"Sure\" or refusing to answer. Consequently, MCQ evaluation is\nnot indicative of model behaviour when interacting with users. But by how much?\nWe evaluate how aligned first-token evaluation is with the text output along\nseveral dimensions, namely final option choice, refusal rate, choice\ndistribution and robustness under prompt perturbation. Our results show that\nthe two approaches are severely misaligned on all dimensions, reaching mismatch\nrates over 60%. Models heavily fine-tuned on conversational or safety data are\nespecially impacted. Crucially, models remain misaligned even when we\nincreasingly constrain prompts, i.e., force them to start with an option letter\nor example template. Our findings i) underscore the importance of inspecting\nthe text output, too and ii) caution against relying solely on first-token\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.14499v1.pdf"
    },
    {
        "title": "LLMBind: A Unified Modality-Task Integration Framework",
        "authors": [
            "Bin Zhu",
            "Peng Jin",
            "Munan Ning",
            "Bin Lin",
            "Jinfa Huang",
            "Qi Song",
            "Jiaxi Cui",
            "Junwu Zhang",
            "Zhenyu Tang",
            "Mingjun Pan",
            "Xing Zhou",
            "Li Yuan"
        ],
        "published": "2024-02-22T12:36:31Z",
        "summary": "While recent progress in multimodal large language models tackles various\nmodality tasks, they posses limited integration capabilities for complex\nmulti-modality tasks, consequently constraining the development of the field.\nIn this work, we take the initiative to explore and propose the LLMBind, a\nunified framework for modality task integration, which binds Large Language\nModels and corresponding pre-trained task models with task-specific tokens.\nConsequently, LLMBind can interpret inputs and produce outputs in versatile\ncombinations of image, text, video, and audio. Specifically, we introduce a\nMixture-of-Experts technique to enable effective learning for different\nmultimodal tasks through collaboration among diverse experts. Furthermore, we\ncreate a multi-task dataset comprising 400k instruction data, which unlocks the\nability for interactive visual generation and editing tasks. Extensive\nexperiments show the effectiveness of our framework across various tasks,\nincluding image, video, audio generation, image segmentation, and image\nediting. More encouragingly, our framework can be easily extended to other\nmodality tasks, showcasing the promising potential of creating a unified AI\nagent for modeling universal modalities.",
        "pdf_link": "https://arxiv.org/pdf/2402.14891v3.pdf"
    },
    {
        "title": "Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis",
        "authors": [
            "Takehiro Takayanagi",
            "Masahiro Suzuki",
            "Ryotaro Kobayashi",
            "Hiroki Sakaji",
            "Kiyoshi Izumi"
        ],
        "published": "2024-02-22T12:19:04Z",
        "summary": "Causality is fundamental in human cognition and has drawn attention in\ndiverse research fields. With growing volumes of textual data, discerning\ncausalities within text data is crucial, and causal text mining plays a pivotal\nrole in extracting meaningful patterns. This study conducts comprehensive\nevaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce\na benchmark that extends beyond general English datasets, including\ndomain-specific and non-English datasets. We also provide an evaluation\nframework to ensure fair comparisons between ChatGPT and previous approaches.\nFinally, our analysis outlines the limitations and future challenges in\nemploying ChatGPT for causal text mining. Specifically, our analysis reveals\nthat ChatGPT serves as a good starting point for various datasets. However,\nwhen equipped with a sufficient amount of training data, previous models still\nsurpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency\nto falsely recognize non-causal sequences as causal sequences. These issues\nbecome even more pronounced with advanced versions of the model, such as GPT-4.\nIn addition, we highlight the constraints of ChatGPT in handling complex\ncausality types, including both intra/inter-sentential and implicit causality.\nThe model also faces challenges with effectively leveraging in-context learning\nand domain adaptation. We release our code to support further research and\ndevelopment in this field.",
        "pdf_link": "https://arxiv.org/pdf/2402.14484v2.pdf"
    },
    {
        "title": "Do LLMs Implicitly Determine the Suitable Text Difficulty for Users?",
        "authors": [
            "Seiji Gobara",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "published": "2024-02-22T11:16:23Z",
        "summary": "Education that suits the individual learning level is necessary to improve\nstudents' understanding. The first step in achieving this purpose by using\nlarge language models (LLMs) is to adjust the textual difficulty of the\nresponse to students. This work analyzes how LLMs can implicitly adjust text\ndifficulty between user input and its generated text. To conduct the\nexperiments, we created a new dataset from Stack-Overflow to explore the\nperformance of question-answering-based conversation. Experimental results on\nthe Stack-Overflow dataset and the TSCC dataset, including multi-turn\nconversation show that LLMs can implicitly handle text difficulty between user\ninput and its generated response. We also observed that some LLMs can surpass\nhumans in handling text difficulty and the importance of instruction-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.14453v1.pdf"
    },
    {
        "title": "COBIAS: Contextual Reliability in Bias Assessment",
        "authors": [
            "Priyanshul Govil",
            "Vamshi Krishna Bonagiri",
            "Manas Gaur",
            "Ponnurangam Kumaraguru",
            "Sanorita Dey"
        ],
        "published": "2024-02-22T10:46:11Z",
        "summary": "Large Language Models (LLMs) are trained on inherently biased data. Previous\nworks on debiasing models rely on benchmark datasets to measure model\nperformance. However, these datasets suffer from several pitfalls due to the\nextremely subjective understanding of bias, highlighting a critical need for\ncontextual exploration. We propose understanding the context of user inputs\nwith consideration of the diverse situations in which input statements are\npossible. This approach would allow for frameworks that foster bias awareness\nrather than guardrails that hurt user engagement. Our contribution is twofold:\n(i) we create a dataset of 2287 stereotyped statements augmented with points\nfor adding context; (ii) we develop the Context-Oriented Bias Indicator and\nAssessment Score (COBIAS) to assess statements' contextual reliability in\nmeasuring bias. Our metric is a significant predictor of the contextual\nreliability of bias-benchmark datasets ($\\chi^2=71.02, p<2.2 \\cdot 10^{-16})$.\nCOBIAS can be used to create reliable datasets, resulting in an improvement in\nbias mitigation works.",
        "pdf_link": "https://arxiv.org/pdf/2402.14889v1.pdf"
    },
    {
        "title": "Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph",
        "authors": [
            "Song Tong",
            "Kai Mao",
            "Zhen Huang",
            "Yukun Zhao",
            "Kaiping Peng"
        ],
        "published": "2024-02-22T10:12:16Z",
        "summary": "Leveraging the synergy between causal knowledge graphs and a large language\nmodel (LLM), our study introduces a groundbreaking approach for computational\nhypothesis generation in psychology. We analyzed 43,312 psychology articles\nusing a LLM to extract causal relation pairs. This analysis produced a\nspecialized causal graph for psychology. Applying link prediction algorithms,\nwe generated 130 potential psychological hypotheses focusing on `well-being',\nthen compared them against research ideas conceived by doctoral scholars and\nthose produced solely by the LLM. Interestingly, our combined approach of a LLM\nand causal graphs mirrored the expert-level insights in terms of novelty,\nclearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =\n4.32, p<0.001, respectively). This alignment was further corroborated using\ndeep semantic analysis. Our results show that combining LLM with machine\nlearning techniques such as causal knowledge graphs can revolutionize automated\ndiscovery in psychology, extracting novel insights from the extensive\nliterature. This work stands at the crossroads of psychology and artificial\nintelligence, championing a new enriched paradigm for data-driven hypothesis\ngeneration in psychological research.",
        "pdf_link": "https://arxiv.org/pdf/2402.14424v2.pdf"
    },
    {
        "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
        "authors": [
            "Vasily Kostumov",
            "Bulat Nutfullin",
            "Oleg Pilipenko",
            "Eugene Ilyushin"
        ],
        "published": "2024-02-22T10:04:17Z",
        "summary": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in\npopularity recently due to their impressive performance in several\nvision-language tasks. Current evaluation methods, however, overlook an\nessential component: uncertainty, which is crucial for a comprehensive\nassessment of VLMs. Addressing this oversight, we present a benchmark\nincorporating uncertainty quantification into evaluating VLMs.\n  Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question\nAnswering (VQA) task. We examine models on 5 datasets that evaluate various\nvision-language capabilities.\n  Using conformal prediction as an uncertainty estimation approach, we\ndemonstrate that the models' uncertainty is not aligned with their accuracy.\nSpecifically, we show that models with the highest accuracy may also have the\nhighest uncertainty, which confirms the importance of measuring it for VLMs.\nOur empirical findings also reveal a correlation between model uncertainty and\nits language model part.",
        "pdf_link": "https://arxiv.org/pdf/2402.14418v2.pdf"
    },
    {
        "title": "Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching",
        "authors": [
            "Piotr Rybak"
        ],
        "published": "2024-02-22T09:49:26Z",
        "summary": "Pre-trained language models have revolutionized the natural language\nunderstanding landscape, most notably BERT (Bidirectional Encoder\nRepresentations from Transformers). However, a significant challenge remains\nfor low-resource languages, where limited data hinders the effective training\nof such models. This work presents a novel approach to bridge this gap by\ntransferring BERT capabilities from high-resource to low-resource languages\nusing vocabulary matching. We conduct experiments on the Silesian and Kashubian\nlanguages and demonstrate the effectiveness of our approach to improve the\nperformance of BERT models even when the target language has minimal training\ndata. Our results highlight the potential of the proposed technique to\neffectively train BERT models for low-resource languages, thus democratizing\naccess to advanced language understanding models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14408v1.pdf"
    },
    {
        "title": "Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning",
        "authors": [
            "Yuwei Xia",
            "Ding Wang",
            "Qiang Liu",
            "Liang Wang",
            "Shu Wu",
            "Xiaoyu Zhang"
        ],
        "published": "2024-02-22T08:51:39Z",
        "summary": "Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based\non given histories. Most recent graph-based models excel at capturing\nstructural information within TKGs but lack semantic comprehension abilities.\nNowadays, with the surge of LLMs, the LLM-based TKG prediction model has\nemerged. However, the existing LLM-based model exhibits three shortcomings: (1)\nIt only focuses on the first-order history for prediction while ignoring\nhigh-order historical information, resulting in the provided information for\nLLMs being extremely limited. (2) LLMs struggle with optimal reasoning\nperformance under heavy historical information loads. (3) For TKG prediction,\nthe temporal reasoning capability of LLM alone is limited. To address the first\ntwo challenges, we propose Chain-of-History (CoH) reasoning which explores\nhigh-order histories step-by-step, achieving effective utilization of\nhigh-order historical information for LLMs on TKG prediction. To address the\nthird issue, we design CoH as a paly-and-plug module to enhance the performance\nof graph-based models for TKG prediction. Extensive experiments on three\ndatasets and backbones demonstrate the effectiveness of CoH.",
        "pdf_link": "https://arxiv.org/pdf/2402.14382v1.pdf"
    },
    {
        "title": "Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction",
        "authors": [
            "Xuemei Tang",
            "Jun Wang",
            "Qi Su"
        ],
        "published": "2024-02-22T08:26:56Z",
        "summary": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\ncurrently paid to this problem using LLM approaches. Therefore, in this paper,\nwe propose SLCoLM, a model collaboration framework, to mitigate the data\nlong-tail problem. In our framework, We use the\n``\\textit{Training-Guide-Predict}'' strategy to combine the strengths of\npre-trained language models (PLMs) and LLMs, where a task-specific PLM\nframework acts as a tutor, transfers task knowledge to the LLM, and guides the\nLLM in performing RE tasks. Our experiments on a RE dataset rich in relation\ntypes show that the approach in this paper facilitates RE of long-tail relation\ntypes.",
        "pdf_link": "https://arxiv.org/pdf/2402.14373v1.pdf"
    },
    {
        "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
        "authors": [
            "Kezhi Kong",
            "Jiani Zhang",
            "Zhengyuan Shen",
            "Balasubramaniam Srinivasan",
            "Chuan Lei",
            "Christos Faloutsos",
            "Huzefa Rangwala",
            "George Karypis"
        ],
        "published": "2024-02-22T08:01:01Z",
        "summary": "Large Language Models (LLMs) trained on large volumes of data excel at\nvarious natural language tasks, but they cannot handle tasks requiring\nknowledge that has not been trained on previously. One solution is to use a\nretriever that fetches relevant information to expand LLM's knowledge scope.\nHowever, existing textual-oriented retrieval-based LLMs are not ideal on\nstructured table data due to diversified data modalities and large table sizes.\nIn this work, we propose OpenTab, an open-domain table reasoning framework\npowered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant\ntables and then generates SQL programs to parse the retrieved tables\nefficiently. Utilizing the intermediate data derived from the SQL executions,\nit conducts grounded inference to produce accurate response. Extensive\nexperimental evaluation shows that OpenTab significantly outperforms baselines\nin both open- and closed-domain settings, achieving up to 21.5% higher\naccuracy. We further run ablation studies to validate the efficacy of our\nproposed designs of the system.",
        "pdf_link": "https://arxiv.org/pdf/2402.14361v1.pdf"
    },
    {
        "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark",
        "authors": [
            "Xiuying Chen",
            "Tairan Wang",
            "Qingqing Zhu",
            "Taicheng Guo",
            "Shen Gao",
            "Zhiyong Lu",
            "Xin Gao",
            "Xiangliang Zhang"
        ],
        "published": "2024-02-22T07:58:29Z",
        "summary": "The summarization capabilities of pretrained and large language models (LLMs)\nhave been widely validated in general areas, but their use in scientific\ncorpus, which involves complex sentences and specialized knowledge, has been\nless assessed. This paper presents conceptual and experimental analyses of\nscientific summarization, highlighting the inadequacies of traditional\nevaluation methods, such as $n$-gram, embedding comparison, and QA,\nparticularly in providing explanations, grasping scientific concepts, or\nidentifying key content. Subsequently, we introduce the Facet-aware Metric\n(FM), employing LLMs for advanced semantic matching to evaluate summaries based\non different aspects. This facet-aware approach offers a thorough evaluation of\nabstracts by decomposing the evaluation task into simpler subtasks.Recognizing\nthe absence of an evaluation benchmark in this domain, we curate a Facet-based\nscientific summarization Dataset (FD) with facet-level annotations. Our\nfindings confirm that FM offers a more logical approach to evaluating\nscientific summaries. In addition, fine-tuned smaller models can compete with\nLLMs in scientific contexts, while LLMs have limitations in learning from\nin-context information in scientific domains. This suggests an area for future\nenhancement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14359v1.pdf"
    },
    {
        "title": "Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",
        "authors": [
            "Ning Bian",
            "Xianpei Han",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Le Sun"
        ],
        "published": "2024-02-22T07:55:26Z",
        "summary": "Building machines with commonsense has been a longstanding challenge in NLP\ndue to the reporting bias of commonsense rules and the exposure bias of\nrule-based commonsense reasoning. In contrast, humans convey and pass down\ncommonsense implicitly through stories. This paper investigates the inherent\ncommonsense ability of large language models (LLMs) expressed through\nstorytelling. We systematically investigate and compare stories and rules for\nretrieving and leveraging commonsense in LLMs. Experimental results on 28\ncommonsense QA datasets show that stories outperform rules as the expression\nfor retrieving commonsense from LLMs, exhibiting higher generation confidence\nand commonsense accuracy. Moreover, stories are the more effective commonsense\nexpression for answering questions regarding daily events, while rules are more\neffective for scientific questions. This aligns with the reporting bias of\ncommonsense in text corpora. We further show that the correctness and relevance\nof commonsense stories can be further improved via iterative self-supervised\nfine-tuning. These findings emphasize the importance of using appropriate\nlanguage to express, retrieve, and leverage commonsense for LLMs, highlighting\na promising direction for better exploiting their commonsense abilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.14355v1.pdf"
    },
    {
        "title": "INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models",
        "authors": [
            "Hanseok Oh",
            "Hyunji Lee",
            "Seonghyeon Ye",
            "Haebin Shin",
            "Hansol Jang",
            "Changwook Jun",
            "Minjoon Seo"
        ],
        "published": "2024-02-22T06:59:50Z",
        "summary": "Despite the critical need to align search targets with users' intention,\nretrievers often only prioritize query information without delving into the\nusers' intended search context. Enhancing the capability of retrievers to\nunderstand intentions and preferences of users, akin to language model\ninstructions, has the potential to yield more aligned search targets. Prior\nstudies restrict the application of instructions in information retrieval to a\ntask description format, neglecting the broader context of diverse and evolving\nsearch scenarios. Furthermore, the prevailing benchmarks utilized for\nevaluation lack explicit tailoring to assess instruction-following ability,\nthereby hindering progress in this field. In response to these limitations, we\npropose a novel benchmark,INSTRUCTIR, specifically designed to evaluate\ninstruction-following ability in information retrieval tasks. Our approach\nfocuses on user-aligned instructions tailored to each query instance,\nreflecting the diverse characteristics inherent in real-world search scenarios.\nThrough experimental analysis, we observe that retrievers fine-tuned to follow\ntask-style instructions, such as INSTRUCTOR, can underperform compared to their\nnon-instruction-tuned counterparts. This underscores potential overfitting\nissues inherent in constructing retrievers trained on existing\ninstruction-aware retrieval datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.14334v1.pdf"
    },
    {
        "title": "Understanding and Patching Compositional Reasoning in LLMs",
        "authors": [
            "Zhaoyi Li",
            "Gangwei Jiang",
            "Hong Xie",
            "Linqi Song",
            "Defu Lian",
            "Ying Wei"
        ],
        "published": "2024-02-22T06:47:56Z",
        "summary": "LLMs have marked a revolutonary shift, yet they falter when faced with\ncompositional reasoning tasks. Our research embarks on a quest to uncover the\nroot causes of compositional reasoning failures of LLMs, uncovering that most\nof them stem from the improperly generated or leveraged implicit reasoning\nresults. Inspired by our empirical findings, we resort to Logit Lens and an\nintervention experiment to dissect the inner hidden states of LLMs. This deep\ndive reveals that implicit reasoning results indeed surface within middle\nlayers and play a causative role in shaping the final explicit reasoning\nresults. Our exploration further locates multi-head self-attention (MHSA)\nmodules within these layers, which emerge as the linchpins in accurate\ngeneration and leveraing of implicit reasoning results. Grounded on the above\nfindings, we develop CREME, a lightweight method to patch errors in\ncompositional reasoning via editing the located MHSA modules. Our empirical\nevidence stands testament to CREME's effectiveness, paving the way for\nautonomously and continuously enhancing compositional reasoning capabilities in\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14328v1.pdf"
    },
    {
        "title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
        "authors": [
            "Chang Zong",
            "Yuchen Yan",
            "Weiming Lu",
            "Eliot Huang",
            "Jian Shao",
            "Yueting Zhuang"
        ],
        "published": "2024-02-22T06:23:37Z",
        "summary": "Recent progress with LLM-based agents has shown promising results across\nvarious tasks. However, their use in answering questions from knowledge bases\nremains largely unexplored. Implementing a KBQA system using traditional\nmethods is challenging due to the shortage of task-specific training data and\nthe complexity of creating task-focused model structures. In this paper, we\npresent Triad, a unified framework that utilizes an LLM-based agent with three\nroles for KBQA tasks. The agent is assigned three roles to tackle different\nKBQA subtasks: agent as a generalist for mastering various subtasks, as a\ndecision maker for the selection of candidates, and as an advisor for answering\nquestions with knowledge. Our KBQA framework is executed in four phases,\ninvolving the collaboration of the agent's multiple roles. We evaluated the\nperformance of our framework using three benchmark datasets, and the results\nshow that our framework outperforms state-of-the-art systems on the LC-QuAD and\nYAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.14320v3.pdf"
    },
    {
        "title": "Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge",
        "authors": [
            "Jinlan Fu",
            "Shenzhen Huangfu",
            "Hang Yan",
            "See-Kiong Ng",
            "Xipeng Qiu"
        ],
        "published": "2024-02-22T05:58:03Z",
        "summary": "Large Language Models (LLMs) have recently showcased remarkable\ngeneralizability in various domains. Despite their extensive knowledge, LLMs\nstill face challenges in efficiently utilizing encoded knowledge to develop\naccurate and logical reasoning processes. To mitigate this problem, we\nintroduced Hint-before-Solving Prompting (HSP), which guides the model to\ngenerate hints (e.g., specific knowledge or key ideas) for solving the problem\nand then generate solutions containing intermediate reasoning steps. Since HSP\nis orthogonal to prompting methods (e.g., Chain-of-Thought (CoT)), we applied\nHSP to CoT, Least-to-Most, Plan-and-Solve, and Standard promptings. The results\nof extensive experiments on 6 reasoning benchmarks and 4 open-source LLMs\ndemonstrate that HSP can effectively improve the accuracy of reasoning tasks:\n(1) By applying high-quality hint-enhanced HSP to CoT prompting,\nLlama2-70B-Chat shows an improvement of 9.7. (2) Beyond exploring training-free\nLLM capabilities, we built the HSPMATH dataset based on HSP and fine-tuned\nLlemma-7B, reaching 64.3 accuracy, surpassing GPT-3.5 and WizardMath-13B. We\nmake our code and dataset publicly available at\n\\url{https://github.com/jinlanfu/HSP}.",
        "pdf_link": "https://arxiv.org/pdf/2402.14310v1.pdf"
    },
    {
        "title": "Mitigating Biases of Large Language Models in Stance Detection with Calibration",
        "authors": [
            "Ang Li",
            "Jingqian Zhao",
            "Bin Liang",
            "Lin Gui",
            "Hui Wang",
            "Xi Zeng",
            "Kam-Fai Wong",
            "Ruifeng Xu"
        ],
        "published": "2024-02-22T05:17:49Z",
        "summary": "Large language models (LLMs) have achieved remarkable progress in many\nnatural language processing tasks. However, our experiment reveals that, in\nstance detection tasks, LLMs may generate biased stances due to spurious\nsentiment-stance correlation and preference towards certain individuals and\ntopics, thus harming their performance. Therefore, in this paper, we propose to\nMitigate Biases of LLMs in stance detection with Calibration (MB-Cal). In\nwhich, a novel gated calibration network is devised to mitigate the biases on\nthe stance reasoning results from LLMs. Further, to make the calibration more\naccurate and generalizable, we construct counterfactual augmented data to\nrectify stance biases. Experimental results on in-target and zero-shot stance\ndetection tasks show that the proposed MB-Cal can effectively mitigate biases\nof LLMs, achieving state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2402.14296v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Concept Graph Recovery and Question Answering in NLP Education",
        "authors": [
            "Rui Yang",
            "Boming Yang",
            "Sixun Ouyang",
            "Tianwei She",
            "Aosong Feng",
            "Yuang Jiang",
            "Freddy Lecue",
            "Jinghui Lu",
            "Irene Li"
        ],
        "published": "2024-02-22T05:15:27Z",
        "summary": "In the domain of Natural Language Processing (NLP), Large Language Models\n(LLMs) have demonstrated promise in text-generation tasks. However, their\neducational applications, particularly for domain-specific queries, remain\nunderexplored. This study investigates LLMs' capabilities in educational\nscenarios, focusing on concept graph recovery and question-answering (QA). We\nassess LLMs' zero-shot performance in creating domain-specific concept graphs\nand introduce TutorQA, a new expert-verified NLP-focused benchmark for\nscientific graph reasoning and QA. TutorQA consists of five tasks with 500 QA\npairs. To tackle TutorQA queries, we present CGLLM, a pipeline integrating\nconcept graphs with LLMs for answering diverse questions. Our results indicate\nthat LLMs' zero-shot concept graph recovery is competitive with supervised\nmethods, showing an average 3% F1 score improvement. In TutorQA tasks, LLMs\nachieve up to 26% F1 score enhancement. Moreover, human evaluation and analysis\nshow that CGLLM generates answers with more fine-grained concepts.",
        "pdf_link": "https://arxiv.org/pdf/2402.14293v1.pdf"
    },
    {
        "title": "CEV-LM: Controlled Edit Vector Language Model for Shaping Natural Language Generations",
        "authors": [
            "Samraj Moorjani",
            "Adit Krishnan",
            "Hari Sundaram"
        ],
        "published": "2024-02-22T05:07:31Z",
        "summary": "As large-scale language models become the standard for text generation, there\nis a greater need to tailor the generations to be more or less concise,\ntargeted, and informative, depending on the audience/application. Existing\ncontrol approaches primarily adjust the semantic (e.g., emotion, topics),\nstructural (e.g., syntax tree, parts-of-speech), and lexical (e.g.,\nkeyword/phrase inclusion) properties of text, but are insufficient to\naccomplish complex objectives such as pacing which control the complexity and\nreadability of the text. In this paper, we introduce CEV-LM - a lightweight,\nsemi-autoregressive language model that utilizes constrained edit vectors to\ncontrol three complementary metrics (speed, volume, and circuitousness) that\nquantify the shape of text (e.g., pacing of content). We study an extensive set\nof state-of-the-art CTG models and find that CEV-LM provides significantly more\ntargeted and precise control of these three metrics while preserving semantic\ncontent, using less training data, and containing fewer parameters.",
        "pdf_link": "https://arxiv.org/pdf/2402.14290v1.pdf"
    },
    {
        "title": "Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning",
        "authors": [
            "Shen Li",
            "Liuyi Yao",
            "Jinyang Gao",
            "Lan Zhang",
            "Yaliang Li"
        ],
        "published": "2024-02-22T04:55:14Z",
        "summary": "To support various applications, business owners often seek the customized\nmodels that are obtained by fine-tuning a pre-trained LLM through the API\nprovided by LLM owners or cloud servers. However, this process carries a\nsubstantial risk of model misuse, potentially resulting in severe economic\nconsequences for business owners. Thus, safeguarding the copyright of these\ncustomized models during LLM fine-tuning has become an urgent practical\nrequirement, but there are limited existing solutions to provide such\nprotection. To tackle this pressing issue, we propose a novel watermarking\napproach named \"Double-I watermark\". Specifically, based on the instruct-tuning\ndata, two types of backdoor data paradigms are introduced with trigger in the\ninstruction and the input, respectively. By leveraging LLM's learning\ncapability to incorporate customized backdoor samples into the dataset, the\nproposed approach effectively injects specific watermarking information into\nthe customized model during fine-tuning, which makes it easy to inject and\nverify watermarks in commercial scenarios. We evaluate the proposed \"Double-I\nwatermark\" under various fine-tuning methods, demonstrating its harmlessness,\nrobustness, uniqueness, imperceptibility, and validity through both theoretical\nanalysis and experimental verification.",
        "pdf_link": "https://arxiv.org/pdf/2402.14883v1.pdf"
    },
    {
        "title": "Can Language Models Act as Knowledge Bases at Scale?",
        "authors": [
            "Qiyuan He",
            "Yizhong Wang",
            "Wenya Wang"
        ],
        "published": "2024-02-22T04:20:14Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating responses to complex queries through large-scale\npre-training. However, the efficacy of these models in memorizing and reasoning\namong large-scale structured knowledge, especially world knowledge that\nexplicitly covers abundant factual information remains questionable. Addressing\nthis gap, our research investigates whether LLMs can effectively store, recall,\nand reason with knowledge on a large scale comparable to latest knowledge bases\n(KBs) such as Wikidata. Specifically, we focus on three crucial aspects to\nstudy the viability: (1) the efficiency of LLMs with different sizes in\nmemorizing the exact knowledge in the large-scale KB; (2) the flexibility of\nrecalling the memorized knowledge in response to natural language queries; (3)\nthe capability to infer new knowledge through reasoning. Our findings indicate\nthat while LLMs hold promise as large-scale KBs capable of retrieving and\nresponding with flexibility, enhancements in their reasoning capabilities are\nnecessary to fully realize their potential.",
        "pdf_link": "https://arxiv.org/pdf/2402.14273v1.pdf"
    },
    {
        "title": "Qsnail: A Questionnaire Dataset for Sequential Question Generation",
        "authors": [
            "Yan Lei",
            "Liang Pang",
            "Yuanzhuo Wang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-22T04:14:10Z",
        "summary": "The questionnaire is a professional research methodology used for both\nqualitative and quantitative analysis of human opinions, preferences,\nattitudes, and behaviors. However, designing and evaluating questionnaires\ndemands significant effort due to their intricate and complex structure.\nQuestionnaires entail a series of questions that must conform to intricate\nconstraints involving the questions, options, and overall structure.\nSpecifically, the questions should be relevant and specific to the given\nresearch topic and intent. The options should be tailored to the questions,\nensuring they are mutually exclusive, completed, and ordered sensibly.\nMoreover, the sequence of questions should follow a logical order, grouping\nsimilar topics together. As a result, automatically generating questionnaires\npresents a significant challenge and this area has received limited attention\nprimarily due to the scarcity of high-quality datasets. To address these\nissues, we present Qsnail, the first dataset specifically constructed for the\nquestionnaire generation task, which comprises 13,168 human-written\nquestionnaires gathered from online platforms. We further conduct experiments\non Qsnail, and the results reveal that retrieval models and traditional\ngenerative models do not fully align with the given research topic and intents.\nLarge language models, while more closely related to the research topic and\nintents, exhibit significant limitations in terms of diversity and specificity.\nDespite enhancements through the chain-of-thought prompt and finetuning,\nquestionnaires generated by language models still fall short of human-written\nquestionnaires. Therefore, questionnaire generation is challenging and needs to\nbe further explored. The dataset is available at:\nhttps://github.com/LeiyanGithub/qsnail.",
        "pdf_link": "https://arxiv.org/pdf/2402.14272v1.pdf"
    },
    {
        "title": "Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization",
        "authors": [
            "Xuxi Chen",
            "Zhendong Wang",
            "Daouda Sow",
            "Junjie Yang",
            "Tianlong Chen",
            "Yingbin Liang",
            "Mingyuan Zhou",
            "Zhangyang Wang"
        ],
        "published": "2024-02-22T04:10:57Z",
        "summary": "In the rapidly advancing arena of large language models (LLMs), a key\nchallenge is to enhance their capabilities amid a looming shortage of\nhigh-quality training data. Our study starts from an empirical strategy for the\nlight continual training of LLMs using their original pre-training data sets,\nwith a specific focus on selective retention of samples that incur moderately\nhigh losses. These samples are deemed informative and beneficial for model\nrefinement, contrasting with the highest-loss samples, which would be discarded\ndue to their correlation with data noise and complexity. We then formalize this\nstrategy into a principled framework of Instance-Reweighted Distributionally\nRobust Optimization (IR-DRO). IR-DRO is designed to dynamically prioritize the\ntraining focus on informative samples through an instance reweighting\nmechanism, streamlined by a closed-form solution for straightforward\nintegration into established training protocols. Through rigorous\nexperimentation with various models and datasets, our findings indicate that\nour sample-targeted methods significantly improve LLM performance across\nmultiple benchmarks, in both continual pre-training and instruction tuning\nscenarios. Our codes are available at\nhttps://github.com/VITA-Group/HardFocusTraining.",
        "pdf_link": "https://arxiv.org/pdf/2402.14270v2.pdf"
    },
    {
        "title": "Can Large Language Models Detect Misinformation in Scientific News Reporting?",
        "authors": [
            "Yupeng Cao",
            "Aishwarya Muralidharan Nair",
            "Elyon Eyimife",
            "Nastaran Jamalipour Soofi",
            "K. P. Subbalakshmi",
            "John R. Wullert II",
            "Chumki Basu",
            "David Shallcross"
        ],
        "published": "2024-02-22T04:07:00Z",
        "summary": "Scientific facts are often spun in the popular press with the intent to\ninfluence public opinion and action, as was evidenced during the COVID-19\npandemic. Automatic detection of misinformation in the scientific domain is\nchallenging because of the distinct styles of writing in these two media types\nand is still in its nascence. Most research on the validity of scientific\nreporting treats this problem as a claim verification challenge. In doing so,\nsignificant expert human effort is required to generate appropriate claims. Our\nsolution bypasses this step and addresses a more real-world scenario where such\nexplicit, labeled claims may not be available. The central research question of\nthis paper is whether it is possible to use large language models (LLMs) to\ndetect misinformation in scientific reporting. To this end, we first present a\nnew labeled dataset SciNews, containing 2.4k scientific news stories drawn from\ntrusted and untrustworthy sources, paired with related abstracts from the\nCORD-19 database. Our dataset includes both human-written and LLM-generated\nnews articles, making it more comprehensive in terms of capturing the growing\ntrend of using LLMs to generate popular press articles. Then, we identify\ndimensions of scientific validity in science news articles and explore how this\ncan be integrated into the automated detection of scientific misinformation. We\npropose several baseline architectures using LLMs to automatically detect false\nrepresentations of scientific findings in the popular press. For each of these\narchitectures, we use several prompt engineering strategies including\nzero-shot, few-shot, and chain-of-thought prompting. We also test these\narchitectures and prompting strategies on GPT-3.5, GPT-4, and Llama2-7B,\nLlama2-13B.",
        "pdf_link": "https://arxiv.org/pdf/2402.14268v1.pdf"
    },
    {
        "title": "Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming",
        "authors": [
            "Anisha Agarwal",
            "Aaron Chan",
            "Shubham Chandel",
            "Jinu Jang",
            "Shaun Miller",
            "Roshanak Zilouchian Moghaddam",
            "Yevhen Mohylevskyy",
            "Neel Sundaresan",
            "Michele Tufano"
        ],
        "published": "2024-02-22T03:51:34Z",
        "summary": "The integration of Large Language Models (LLMs) into Development Environments\n(IDEs) has become a focal point in modern software development. LLMs such as\nOpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment\ndeveloper productivity by serving as intelligent, chat-driven programming\nassistants. However, utilizing LLMs out of the box is unlikely to be optimal\nfor any given scenario. Rather, each system requires the LLM to be honed to its\nset of heuristics to ensure the best performance. In this paper, we introduce\nthe Copilot evaluation harness: a set of data and tools for evaluating\nLLM-guided IDE interactions, covering various programming scenarios and\nlanguages. We propose our metrics as a more robust and information-dense\nevaluation than previous state of the art evaluation systems. We design and\ncompute both static and execution based success metrics for scenarios\nencompassing a wide range of developer tasks, including code generation from\nnatural language (generate), documentation generation from code (doc), test\ncase generation (test), bug-fixing (fix), and workspace understanding and query\nresolution (workspace). These success metrics are designed to evaluate the\nperformance of LLMs within a given IDE and its respective parameter space. Our\nlearnings from evaluating three common LLMs using these metrics can inform the\ndevelopment and validation of future scenarios in LLM guided IDEs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14261v1.pdf"
    },
    {
        "title": "Word-Sequence Entropy: Towards Uncertainty Estimation in Free-Form Medical Question Answering Applications and Beyond",
        "authors": [
            "Zhiyuan Wang",
            "Jinhao Duan",
            "Chenxi Yuan",
            "Qingyu Chen",
            "Tianlong Chen",
            "Huaxiu Yao",
            "Yue Zhang",
            "Ren Wang",
            "Kaidi Xu",
            "Xiaoshuang Shi"
        ],
        "published": "2024-02-22T03:46:08Z",
        "summary": "Uncertainty estimation plays a pivotal role in ensuring the reliability of\nsafety-critical human-AI interaction systems, particularly in the medical\ndomain. However, a general method for quantifying the uncertainty of free-form\nanswers has yet to be established in open-ended medical question-answering (QA)\ntasks, where irrelevant words and sequences with limited semantic information\ncan be the primary source of uncertainty due to the presence of generative\ninequality. In this paper, we propose the Word-Sequence Entropy (WSE), which\ncalibrates the uncertainty proportion at both the word and sequence levels\naccording to the semantic relevance, with greater emphasis placed on keywords\nand more relevant sequences when performing uncertainty quantification. We\ncompare WSE with 6 baseline methods on 5 free-form medical QA datasets,\nutilizing 7 \"off-the-shelf\" large language models (LLMs), and show that WSE\nexhibits superior performance on accurate uncertainty measurement under two\nstandard criteria for correctness evaluation (e.g., WSE outperforms existing\nstate-of-the-art method by 3.23% AUROC on the MedQA dataset). Additionally, in\nterms of the potential for real-world medical QA applications, we achieve a\nsignificant enhancement in the performance of LLMs when employing sequences\nwith lower uncertainty, identified by WSE, as final answers (e.g., +6.36%\naccuracy improvement on the COVID-QA dataset), without requiring any additional\ntask-specific fine-tuning or architectural modifications.",
        "pdf_link": "https://arxiv.org/pdf/2402.14259v1.pdf"
    },
    {
        "title": "Eagle: Ethical Dataset Given from Real Interactions",
        "authors": [
            "Masahiro Kaneko",
            "Danushka Bollegala",
            "Timothy Baldwin"
        ],
        "published": "2024-02-22T03:46:02Z",
        "summary": "Recent studies have demonstrated that large language models (LLMs) have\nethical-related problems such as social biases, lack of moral reasoning, and\ngeneration of offensive content. The existing evaluation metrics and methods to\naddress these ethical challenges use datasets intentionally created by\ninstructing humans to create instances including ethical problems. Therefore,\nthe data does not reflect prompts that users actually provide when utilizing\nLLM services in everyday contexts. This may not lead to the development of safe\nLLMs that can address ethical challenges arising in real-world applications. In\nthis paper, we create Eagle datasets extracted from real interactions between\nChatGPT and users that exhibit social biases, toxicity, and immoral problems.\nOur experiments show that Eagle captures complementary aspects, not covered by\nexisting datasets proposed for evaluation and mitigation of such ethical\nchallenges. Our code is publicly available at\nhttps://huggingface.co/datasets/MasahiroKaneko/eagle.",
        "pdf_link": "https://arxiv.org/pdf/2402.14258v1.pdf"
    },
    {
        "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models",
        "authors": [
            "Jinyi Liu",
            "Yifu Yuan",
            "Jianye Hao",
            "Fei Ni",
            "Lingzhi Fu",
            "Yibin Chen",
            "Yan Zheng"
        ],
        "published": "2024-02-22T03:14:03Z",
        "summary": "Recently, there has been considerable attention towards leveraging large\nlanguage models (LLMs) to enhance decision-making processes. However, aligning\nthe natural language text instructions generated by LLMs with the vectorized\noperations required for execution presents a significant challenge, often\nnecessitating task-specific details. To circumvent the need for such\ntask-specific granularity, inspired by preference-based policy learning\napproaches, we investigate the utilization of multimodal LLMs to provide\nautomated preference feedback solely from image inputs to guide\ndecision-making. In this study, we train a multimodal LLM, termed CriticGPT,\ncapable of understanding trajectory videos in robot manipulation tasks, serving\nas a critic to offer analysis and preference feedback. Subsequently, we\nvalidate the effectiveness of preference labels generated by CriticGPT from a\nreward modeling perspective. Experimental evaluation of the algorithm's\npreference accuracy demonstrates its effective generalization ability to new\ntasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT's\nreward model efficiently guides policy learning, surpassing rewards based on\nstate-of-the-art pre-trained representation models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14245v1.pdf"
    },
    {
        "title": "COPR: Continual Human Preference Learning via Optimal Policy Regularization",
        "authors": [
            "Han Zhang",
            "Lin Gui",
            "Yu Lei",
            "Yuanzhao Zhai",
            "Yehong Zhang",
            "Yulan He",
            "Hui Wang",
            "Yue Yu",
            "Kam-Fai Wong",
            "Bin Liang",
            "Ruifeng Xu"
        ],
        "published": "2024-02-22T02:20:08Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to\nimprove the alignment of Large Language Models (LLMs) with human preferences.\nGiven the evolving nature of human preferences, continual alignment becomes\nmore crucial and practical in comparison to traditional static alignment.\nNevertheless, making RLHF compatible with Continual Learning (CL) is\nchallenging due to its complex process. Meanwhile, directly learning new human\npreferences may lead to Catastrophic Forgetting (CF) of historical preferences,\nresulting in helpless or harmful outputs. To overcome these challenges, we\npropose the Continual Optimal Policy Regularization (COPR) method, which draws\ninspiration from the optimal policy theory. COPR utilizes a sampling\ndistribution as a demonstration and regularization constraints for CL. It\nadopts the Lagrangian Duality (LD) method to dynamically regularize the current\npolicy based on the historically optimal policy, which prevents CF and avoids\nover-emphasizing unbalanced objectives. We also provide formal proof for the\nlearnability of COPR. The experimental results show that COPR outperforms\nstrong CL baselines on our proposed benchmark, in terms of reward-based, GPT-4\nevaluations and human assessment. Furthermore, we validate the robustness of\nCOPR under various CL settings, including different backbones, replay memory\nsizes, and learning orders.",
        "pdf_link": "https://arxiv.org/pdf/2402.14228v2.pdf"
    },
    {
        "title": "Content Conditional Debiasing for Fair Text Embedding",
        "authors": [
            "Wenlong Deng",
            "Blair Chen",
            "Xiaoxiao Li",
            "Christos Thrampoulidis"
        ],
        "published": "2024-02-22T01:20:51Z",
        "summary": "Mitigating biases in machine learning models has gained increasing attention\nin Natural Language Processing (NLP). Yet, only a few studies focus on fair\ntext embeddings, which are crucial yet challenging for real-world applications.\nIn this paper, we propose a novel method for learning fair text embeddings. We\nachieve fairness while maintaining utility trade-off by ensuring conditional\nindependence between sensitive attributes and text embeddings conditioned on\nthe content. Specifically, we enforce that embeddings of texts with different\nsensitive attributes but identical content maintain the same distance toward\nthe embedding of their corresponding neutral text. Furthermore, we address the\nissue of lacking proper training data by using Large Language Models (LLMs) to\naugment texts into different sensitive groups. Our extensive evaluations\ndemonstrate that our approach effectively improves fairness while preserving\nthe utility of embeddings, representing a pioneering effort in achieving\nconditional independence for fair text embeddings.",
        "pdf_link": "https://arxiv.org/pdf/2402.14208v2.pdf"
    },
    {
        "title": "Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models",
        "authors": [
            "Yijia Shao",
            "Yucheng Jiang",
            "Theodore A. Kanell",
            "Peter Xu",
            "Omar Khattab",
            "Monica S. Lam"
        ],
        "published": "2024-02-22T01:20:17Z",
        "summary": "We study how to apply large language models to write grounded and organized\nlong-form articles from scratch, with comparable breadth and depth to Wikipedia\npages. This underexplored problem poses new challenges at the pre-writing\nstage, including how to research the topic and prepare an outline prior to\nwriting. We propose STORM, a writing system for the Synthesis of Topic Outlines\nthrough Retrieval and Multi-perspective Question Asking. STORM models the\npre-writing stage by (1) discovering diverse perspectives in researching the\ngiven topic, (2) simulating conversations where writers carrying different\nperspectives pose questions to a topic expert grounded on trusted Internet\nsources, (3) curating the collected information to create an outline.\n  For evaluation, we curate FreshWiki, a dataset of recent high-quality\nWikipedia articles, and formulate outline assessments to evaluate the\npre-writing stage. We further gather feedback from experienced Wikipedia\neditors. Compared to articles generated by an outline-driven\nretrieval-augmented baseline, more of STORM's articles are deemed to be\norganized (by a 25% absolute increase) and broad in coverage (by 10%). The\nexpert feedback also helps identify new challenges for generating grounded long\narticles, such as source bias transfer and over-association of unrelated facts.",
        "pdf_link": "https://arxiv.org/pdf/2402.14207v2.pdf"
    },
    {
        "title": "Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models",
        "authors": [
            "Younghun Lee",
            "Dan Goldwasser",
            "Laura Schwab Reese"
        ],
        "published": "2024-02-22T01:02:37Z",
        "summary": "Understanding the dynamics of counseling conversations is an important task,\nyet it is a challenging NLP problem regardless of the recent advance of\nTransformer-based pre-trained language models. This paper proposes a systematic\napproach to examine the efficacy of domain knowledge and large language models\n(LLMs) in better representing conversations between a crisis counselor and a\nhelp seeker. We empirically show that state-of-the-art language models such as\nTransformer-based models and GPT models fail to predict the conversation\noutcome. To provide richer context to conversations, we incorporate\nhuman-annotated domain knowledge and LLM-generated features; simple integration\nof domain knowledge and LLM features improves the model performance by\napproximately 15%. We argue that both domain knowledge and LLM-generated\nfeatures can be exploited to better characterize counseling conversations when\nthey are used as an additional context to conversations.",
        "pdf_link": "https://arxiv.org/pdf/2402.14200v1.pdf"
    },
    {
        "title": "Learning to Reduce: Optimal Representations of Structured Data in Prompting Large Language Models",
        "authors": [
            "Younghun Lee",
            "Sungchul Kim",
            "Tong Yu",
            "Ryan A. Rossi",
            "Xiang Chen"
        ],
        "published": "2024-02-22T00:41:23Z",
        "summary": "Large Language Models (LLMs) have been widely used as general-purpose AI\nagents showing comparable performance on many downstream tasks. However,\nexisting work shows that it is challenging for LLMs to integrate structured\ndata (e.g. KG, tables, DBs) into their prompts; LLMs need to either understand\nlong text data or select the most relevant evidence prior to inference, and\nboth approaches are not trivial.\n  In this paper, we propose a framework, Learning to Reduce, that fine-tunes a\nlanguage model to generate a reduced version of an input context, given a task\ndescription and context input. The model learns to reduce the input context\nusing On-Policy Reinforcement Learning and aims to improve the reasoning\nperformance of a fixed LLM. Experimental results illustrate that our model not\nonly achieves comparable accuracies in selecting the relevant evidence from an\ninput context, but also shows generalizability on different datasets. We\nfurther show that our model helps improve the LLM's performance on downstream\ntasks especially when the context is long.",
        "pdf_link": "https://arxiv.org/pdf/2402.14195v1.pdf"
    },
    {
        "title": "Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization",
        "authors": [
            "Jiliang Li",
            "Yifan Zhang",
            "Zachary Karas",
            "Collin McMillan",
            "Kevin Leach",
            "Yu Huang"
        ],
        "published": "2024-02-22T00:01:02Z",
        "summary": "Recent language models have demonstrated proficiency in summarizing source\ncode. However, as in many other domains of machine learning, language models of\ncode lack sufficient explainability. Informally, we lack a formulaic or\nintuitive understanding of what and how models learn from code. Explainability\nof language models can be partially provided if, as the models learn to produce\nhigher-quality code summaries, they also align in deeming the same code parts\nimportant as those identified by human programmers. In this paper, we report\nnegative results from our investigation of explainability of language models in\ncode summarization through the lens of human comprehension. We measure human\nfocus on code using eye-tracking metrics such as fixation counts and duration\nin code summarization tasks. To approximate language model focus, we employ a\nstate-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP\n(SHapley Additive exPlanations), to identify which code tokens influence that\ngeneration of summaries. Using these settings, we find no statistically\nsignificant relationship between language models' focus and human programmers'\nattention. Furthermore, alignment between model and human foci in this setting\ndoes not seem to dictate the quality of the LLM-generated summaries. Our study\nhighlights an inability to align human focus with SHAP-based model focus\nmeasures. This result calls for future investigation of multiple open questions\nfor explainable language models for code summarization and software engineering\ntasks in general, including the training mechanisms of language models for\ncode, whether there is an alignment between human and model attention on code,\nwhether human attention can improve the development of language models, and\nwhat other model focus measures are appropriate for improving explainability.",
        "pdf_link": "https://arxiv.org/pdf/2402.14182v1.pdf"
    },
    {
        "title": "Understanding the Dataset Practitioners Behind Large Language Model Development",
        "authors": [
            "Crystal Qian",
            "Emily Reif",
            "Minsuk Kahng"
        ],
        "published": "2024-02-21T23:50:37Z",
        "summary": "As large language models (LLMs) become more advanced and impactful, it is\nincreasingly important to scrutinize the data that they rely upon and produce.\nWhat is it to be a dataset practitioner doing this work? We approach this in\ntwo parts: first, we define the role of \"dataset practitioners\" by performing a\nretrospective analysis on the responsibilities of teams contributing to LLM\ndevelopment at a technology company, Google. Then, we conduct semi-structured\ninterviews with a cross-section of these practitioners (N=10). We find that\nalthough data quality is a top priority, there is little consensus around what\ndata quality is and how to evaluate it. Consequently, practitioners either rely\non their own intuition or write custom code to evaluate their data. We discuss\npotential reasons for this phenomenon and opportunities for alignment.",
        "pdf_link": "https://arxiv.org/pdf/2402.16611v2.pdf"
    },
    {
        "title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media",
        "authors": [
            "MD Ashraful Goni",
            "Fahad Mostafa",
            "Kerk F. Kee"
        ],
        "published": "2024-02-21T23:43:04Z",
        "summary": "Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.",
        "pdf_link": "https://arxiv.org/pdf/2402.14179v1.pdf"
    },
    {
        "title": "Recursive Speculative Decoding: Accelerating LLM Inference via Sampling Without Replacement",
        "authors": [
            "Wonseok Jeon",
            "Mukul Gagrani",
            "Raghavv Goel",
            "Junyoung Park",
            "Mingu Lee",
            "Christopher Lott"
        ],
        "published": "2024-02-21T22:57:49Z",
        "summary": "Speculative decoding is an inference-acceleration method for large language\nmodels (LLMs) where a small language model generates a draft-token sequence\nwhich is further verified by the target LLM in parallel. Recent works have\nadvanced this method by establishing a draft-token tree, achieving superior\nperformance over a single-sequence speculative decoding. However, those works\nindependently generate tokens at each level of the tree, not leveraging the\ntree's entire diversifiability. Besides, their empirical superiority has been\nshown for fixed length of sequences, implicitly granting more computational\nresource to LLM for the tree-based methods. None of the existing works has\nconducted empirical studies with fixed target computational budgets despite its\nimportance to resource-bounded devices. We present Recursive Speculative\nDecoding (RSD), a novel tree-based method that samples draft tokens without\nreplacement and maximizes the diversity of the tree. During RSD's drafting, the\ntree is built by either Gumbel-Top-$k$ trick that draws tokens without\nreplacement in parallel or Stochastic Beam Search that samples sequences\nwithout replacement while early-truncating unlikely draft sequences and\nreducing the computational cost of LLM. We empirically evaluate RSD with Llama\n2 and OPT models, showing that RSD outperforms the baseline methods,\nconsistently for fixed draft sequence length and in most cases for fixed\ncomputational budgets at LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.14160v2.pdf"
    },
    {
        "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
        "authors": [
            "Dheeraj Mekala",
            "Jason Weston",
            "Jack Lanchantin",
            "Roberta Raileanu",
            "Maria Lomeli",
            "Jingbo Shang",
            "Jane Dwivedi-Yu"
        ],
        "published": "2024-02-21T22:41:38Z",
        "summary": "Teaching language models to use tools is an important milestone towards\nbuilding general assistants, but remains an open problem. While there has been\nsignificant progress on learning to use specific tools via fine-tuning,\nlanguage models still struggle with learning how to robustly use new tools from\nonly a few demonstrations. In this work we introduce a self-verification method\nwhich distinguishes between close candidates by self-asking contrastive\nquestions during (1) tool selection; and (2) parameter generation. We construct\nsynthetic, high-quality, self-generated data for this goal using Llama-2 70B,\nwhich we intend to release publicly. Extensive experiments on 4 tasks from the\nToolBench benchmark, consisting of 17 unseen tools, demonstrate an average\nimprovement of 22% over few-shot baselines, even in scenarios where the\ndistinctions between candidate tools are finely nuanced.",
        "pdf_link": "https://arxiv.org/pdf/2402.14158v2.pdf"
    },
    {
        "title": "Automatic Histograms: Leveraging Language Models for Text Dataset Exploration",
        "authors": [
            "Emily Reif",
            "Crystal Qian",
            "James Wexler",
            "Minsuk Kahng"
        ],
        "published": "2024-02-21T22:29:16Z",
        "summary": "Making sense of unstructured text datasets is perennially difficult, yet\nincreasingly relevant with Large Language Models. Data workers often rely on\ndataset summaries, especially distributions of various derived features. Some\nfeatures, like toxicity or topics, are relevant to many datasets, but many\ninteresting features are domain specific: instruments and genres for a music\ndataset, or diseases and symptoms for a medical dataset. Accordingly, data\nworkers often run custom analyses for each dataset, which is cumbersome and\ndifficult. We present AutoHistograms, a visualization tool leveragingLLMs.\nAutoHistograms automatically identifies relevant features, visualizes them with\nhistograms, and allows the user to interactively query the dataset for\ncategories of entities and create new histograms. In a user study with 10 data\nworkers (n=10), we observe that participants can quickly identify insights and\nexplore the data using AutoHistograms, and conceptualize a broad range of\napplicable use cases. Together, this tool and user study contributeto the\ngrowing field of LLM-assisted sensemaking tools.",
        "pdf_link": "https://arxiv.org/pdf/2402.14880v1.pdf"
    },
    {
        "title": "MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms",
        "authors": [
            "Yiqiao Jin",
            "Minje Choi",
            "Gaurav Verma",
            "Jindong Wang",
            "Srijan Kumar"
        ],
        "published": "2024-02-21T22:27:40Z",
        "summary": "Social media platforms are hubs for multimodal information exchange,\nencompassing text, images, and videos, making it challenging for machines to\ncomprehend the information or emotions associated with interactions in online\nspaces. Multimodal Large Language Models (MLLMs) have emerged as a promising\nsolution to address these challenges, yet struggle with accurately interpreting\nhuman emotions and complex contents like misinformation. This paper introduces\nMM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of\nmultimodal social media content. MM-Soc compiles prominent multimodal datasets\nand incorporates a novel large-scale YouTube tagging dataset, targeting a range\nof tasks from misinformation detection, hate speech detection, and social\ncontext generation. Through our exhaustive evaluation on ten size-variants of\nfour open-source MLLMs, we have identified significant performance disparities,\nhighlighting the need for advancements in models' social understanding\ncapabilities. Our analysis reveals that, in a zero-shot setting, various types\nof MLLMs generally exhibit difficulties in handling social media tasks.\nHowever, MLLMs demonstrate performance improvements post fine-tuning,\nsuggesting potential pathways for improvement.",
        "pdf_link": "https://arxiv.org/pdf/2402.14154v1.pdf"
    },
    {
        "title": "BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives",
        "authors": [
            "Xiaoyue Wang",
            "Jianyou Wang",
            "Weili Cao",
            "Kaicheng Wang",
            "Ramamohan Paturi",
            "Leon Bergen"
        ],
        "published": "2024-02-21T22:22:30Z",
        "summary": "We present the Benchmark of Information Retrieval (IR) tasks with Complex\nObjectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve\ndocuments given multi-faceted user objectives. The benchmark's complexity and\ncompact size make it suitable for evaluating large language model (LLM)-based\ninformation retrieval systems. We present a modular framework for investigating\nfactors that may influence LLM performance on retrieval tasks, and identify a\nsimple baseline model which matches or outperforms existing approaches and more\ncomplex alternatives. No approach achieves satisfactory performance on all\nbenchmark tasks, suggesting that stronger models and new retrieval protocols\nare necessary to address complex user needs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14151v2.pdf"
    },
    {
        "title": "Driving Generative Agents With Their Personality",
        "authors": [
            "Lawrence J. Klinkert",
            "Stephanie Buongiorno",
            "Corey Clark"
        ],
        "published": "2024-02-21T21:29:57Z",
        "summary": "This research explores the potential of Large Language Models (LLMs) to\nutilize psychometric values, specifically personality information, within the\ncontext of video game character development. Affective Computing (AC) systems\nquantify a Non-Player character's (NPC) psyche, and an LLM can take advantage\nof the system's information by using the values for prompt generation. The\nresearch shows an LLM can consistently represent a given personality profile,\nthereby enhancing the human-like characteristics of game characters.\nRepurposing a human examination, the International Personality Item Pool (IPIP)\nquestionnaire, to evaluate an LLM shows that the model can accurately generate\ncontent concerning the personality provided. Results show that the improvement\nof LLM, such as the latest GPT-4 model, can consistently utilize and interpret\na personality to represent behavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.14879v1.pdf"
    },
    {
        "title": "FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models",
        "authors": [
            "Andrew Zhu",
            "Alyssa Hwang",
            "Liam Dugan",
            "Chris Callison-Burch"
        ],
        "published": "2024-02-21T20:30:45Z",
        "summary": "One type of question that is commonly found in day-to-day scenarios is\n``fan-out'' questions, complex multi-hop, multi-document reasoning questions\nthat require finding information about a large number of entities. However,\nthere exist few resources to evaluate this type of question-answering\ncapability among large language models. To evaluate complex reasoning in LLMs\nmore fully, we present FanOutQA, a high-quality dataset of fan-out\nquestion-answer pairs and human-annotated decompositions with English Wikipedia\nas the knowledge base. We formulate three benchmark settings across our dataset\nand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,\nfinding that contemporary models still have room to improve reasoning over\ninter-document dependencies in a long context. We provide our dataset and\nopen-source tools to run models to encourage evaluation at https://fanoutqa.com",
        "pdf_link": "https://arxiv.org/pdf/2402.14116v1.pdf"
    },
    {
        "title": "EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy",
        "authors": [
            "Hamed Hooshangnejad",
            "Xue Feng",
            "Gaofeng Huang",
            "Rui Zhang",
            "Quan Chen",
            "Kai Ding"
        ],
        "published": "2024-02-21T19:49:12Z",
        "summary": "Lung cancer is a devastating disease with the highest mortality rate among\ncancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, which\naccounts for 87% of diagnoses, require radiation therapy. Rapid treatment\ninitiation significantly increases the patient's survival rate and reduces the\nmortality rate. Accurate tumor segmentation is a critical step in the diagnosis\nand treatment of NSCLC. Manual segmentation is time and labor-consuming and\ncauses delays in treatment initiation. Although many lung nodule detection\nmethods, including deep learning-based models, have been proposed, there is\nstill a long-standing problem of high false positives (FPs) with most of these\nmethods. Here, we developed an electronic health record (EHR) guided lung tumor\nauto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor\nsegmentation), where the extracted information from EHRs using a pre-trained\nlarge language model (LLM), was used to remove the FPs and keep the TP nodules\nonly. The auto-segmentation model was trained on NSCLC patients' computed\ntomography (CT), and the pre-trained LLM was used with the zero-shot learning\napproach. Our approach resulted in a 250% boost in successful nodule detection\nusing the data from ten NSCLC patients treated in our institution.",
        "pdf_link": "https://arxiv.org/pdf/2402.14099v1.pdf"
    },
    {
        "title": "Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns",
        "authors": [
            "Zheyuan Zhang",
            "Zehong Wang",
            "Shifu Hou",
            "Evan Hall",
            "Landon Bachman",
            "Vincent Galassi",
            "Jasmine White",
            "Nitesh V. Chawla",
            "Chuxu Zhang",
            "Yanfang Ye"
        ],
        "published": "2024-02-21T19:36:24Z",
        "summary": "The opioid crisis has been one of the most critical society concerns in the\nUnited States. Although the medication assisted treatment (MAT) is recognized\nas the most effective treatment for opioid misuse and addiction, the various\nside effects can trigger opioid relapse. In addition to MAT, the dietary\nnutrition intervention has been demonstrated its importance in opioid misuse\nprevention and recovery. However, research on the alarming connections between\ndietary patterns and opioid misuse remain under-explored. In response to this\ngap, in this paper, we first establish a large-scale multifaceted dietary\nbenchmark dataset related to opioid users at the first attempt and then develop\na novel framework - i.e., namely Opioid Misuse Detection with Interpretable\nDietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large\nlanguage model (LLM) for the identification of users with opioid misuse and the\ninterpretation of their associated dietary patterns. Specifically, in\nDiet-ODIN, we first construct an HG to comprehensively incorporate both dietary\nand health-related information, and then we devise a holistic graph learning\nframework with noise reduction to fully capitalize both users' individual\ndietary habits and shared dietary patterns for the detection of users with\nopioid misuse. To further delve into the intricate correlations between dietary\npatterns and opioid misuse, we exploit an LLM by utilizing the knowledge\nobtained from the graph learning model for interpretation. The extensive\nexperimental results based on our established benchmark with quantitative and\nqualitative measures demonstrate the outstanding performance of Diet-ODIN in\nexploring the complex interplay between opioid misuse and dietary patterns, by\ncomparison with state-of-the-art baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.08820v1.pdf"
    },
    {
        "title": "Coercing LLMs to do and reveal (almost) anything",
        "authors": [
            "Jonas Geiping",
            "Alex Stein",
            "Manli Shu",
            "Khalid Saifullah",
            "Yuxin Wen",
            "Tom Goldstein"
        ],
        "published": "2024-02-21T18:59:13Z",
        "summary": "It has recently been shown that adversarial attacks on large language models\n(LLMs) can \"jailbreak\" the model into making harmful statements. In this work,\nwe argue that the spectrum of adversarial attacks on LLMs is much larger than\nmerely jailbreaking. We provide a broad overview of possible attack surfaces\nand attack goals. Based on a series of concrete examples, we discuss,\ncategorize and systematize attacks that coerce varied unintended behaviors,\nsuch as misdirection, model control, denial-of-service, or data extraction.\n  We analyze these attacks in controlled experiments, and find that many of\nthem stem from the practice of pre-training LLMs with coding capabilities, as\nwell as the continued existence of strange \"glitch\" tokens in common LLM\nvocabularies that should be removed for security reasons.",
        "pdf_link": "https://arxiv.org/pdf/2402.14020v1.pdf"
    },
    {
        "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
        "authors": [
            "Vyas Raina",
            "Adian Liusie",
            "Mark Gales"
        ],
        "published": "2024-02-21T18:55:20Z",
        "summary": "Large Language Models (LLMs) are powerful zero-shot assessors and are\nincreasingly used in real-world situations such as for written exams or\nbenchmarking systems. Despite this, no existing work has analyzed the\nvulnerability of judge-LLMs against adversaries attempting to manipulate\noutputs. This work presents the first study on the adversarial robustness of\nassessment LLMs, where we search for short universal phrases that when appended\nto texts can deceive LLMs to provide high assessment scores. Experiments on\nSummEval and TopicalChat demonstrate that both LLM-scoring and pairwise\nLLM-comparative assessment are vulnerable to simple concatenation attacks,\nwhere in particular LLM-scoring is very susceptible and can yield maximum\nassessment scores irrespective of the input text quality. Interestingly, such\nattacks are transferable and phrases learned on smaller open-source LLMs can be\napplied to larger closed-source models, such as GPT3.5. This highlights the\npervasive nature of the adversarial vulnerabilities across different judge-LLM\nsizes, families and methods. Our findings raise significant concerns on the\nreliability of LLMs-as-a-judge methods, and underscore the importance of\naddressing vulnerabilities in LLM assessment methods before deployment in\nhigh-stakes real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.14016v1.pdf"
    },
    {
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
        "authors": [
            "Chaoqun He",
            "Renjie Luo",
            "Yuzhuo Bai",
            "Shengding Hu",
            "Zhen Leng Thai",
            "Junhao Shen",
            "Jinyi Hu",
            "Xu Han",
            "Yujie Huang",
            "Yuxiang Zhang",
            "Jie Liu",
            "Lei Qi",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-21T18:49:26Z",
        "summary": "Recent advancements have seen Large Language Models (LLMs) and Large\nMultimodal Models (LMMs) surpassing general human capabilities in various\ntasks, approaching the proficiency level of human experts across multiple\ndomains. With traditional benchmarks becoming less challenging for these\nmodels, new rigorous challenges are essential to gauge their advanced\nabilities. In this work, we present OlympiadBench, an Olympiad-level bilingual\nmultimodal scientific benchmark, featuring 8,952 problems from Olympiad-level\nmathematics and physics competitions, including the Chinese college entrance\nexam. Each problem is detailed with expert-level annotations for step-by-step\nreasoning. Evaluating top-tier models on OlympiadBench, we implement a\ncomprehensive assessment methodology to accurately evaluate model responses.\nNotably, the best-performing model, GPT-4V, attains an average score of 17.23%\non OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark\nrigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V\npoints out prevalent issues with hallucinations, knowledge omissions, and\nlogical fallacies. We hope that our challenging benchmark can serve as a\nvaluable resource for helping future AGI research endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2402.14008v1.pdf"
    },
    {
        "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models",
        "authors": [
            "Zhiwei He",
            "Binglin Zhou",
            "Hongkun Hao",
            "Aiwei Liu",
            "Xing Wang",
            "Zhaopeng Tu",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published": "2024-02-21T18:48:38Z",
        "summary": "Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of ''cross-lingual consistency'' in text watermarking, which assesses\nthe ability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks by reducing the Area Under the Curve (AUC) from\n0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors\nthat contribute to the cross-lingual consistency in text watermarking and\npropose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.14007v1.pdf"
    },
    {
        "title": "Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models",
        "authors": [
            "Aline Ioste"
        ],
        "published": "2024-02-21T18:40:24Z",
        "summary": "Large Language Models with transformer architecture have revolutionized the\ndomain of text generation, setting unprecedented benchmarks. Despite their\nimpressive capabilities, LLMs have been criticized for generating outcomes that\ndeviate from factual accuracy or display logical inconsistencies, phenomena\ncommonly referred to as hallucinations. This term, however, has often been\nmisapplied to any results deviating from the instructor's expectations, which\nthis paper defines as attention misdirection rather than true hallucinations.\nUnderstanding the distinction between hallucinations and attention misdirection\nbecomes increasingly relevant in business contexts, where the ramifications of\nsuch errors can significantly impact the value extraction from these inherently\npre-trained models. This paper highlights the best practices of the PGI,\nPersona, Grouping, and Intelligence, method, a strategic framework that\nachieved a remarkable error rate of only 3,15 percent across 4,000 responses\ngenerated by GPT in response to a real business challenge. It emphasizes that\nby equipping experimentation with knowledge, businesses can unlock\nopportunities for innovation through the use of these natively pre-trained\nmodels. This reinforces the notion that strategic application grounded in a\nskilled team can maximize the benefits of emergent technologies such as the\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14002v1.pdf"
    },
    {
        "title": "What's in a Name? Auditing Large Language Models for Race and Gender Bias",
        "authors": [
            "Amit Haim",
            "Alejandro Salinas",
            "Julian Nyarko"
        ],
        "published": "2024-02-21T18:25:25Z",
        "summary": "We employ an audit design to investigate biases in state-of-the-art large\nlanguage models, including GPT-4. In our study, we prompt the models for advice\ninvolving a named individual across a variety of scenarios, such as during car\npurchase negotiations or election outcome predictions. We find that the advice\nsystematically disadvantages names that are commonly associated with racial\nminorities and women. Names associated with Black women receive the least\nadvantageous outcomes. The biases are consistent across 42 prompt templates and\nseveral models, indicating a systemic issue rather than isolated incidents.\nWhile providing numerical, decision-relevant anchors in the prompt can\nsuccessfully counteract the biases, qualitative details have inconsistent\neffects and may even increase disparities. Our findings underscore the\nimportance of conducting audits at the point of LLM deployment and\nimplementation to mitigate their potential for harm against marginalized\ncommunities.",
        "pdf_link": "https://arxiv.org/pdf/2402.14875v2.pdf"
    },
    {
        "title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training",
        "authors": [
            "Yu Zhao",
            "Yuanbin Qu",
            "Konrad Staniszewski",
            "Szymon Tworkowski",
            "Wei Liu",
            "Piotr Mi\u0142o\u015b",
            "Yuxiang Wu",
            "Pasquale Minervini"
        ],
        "published": "2024-02-21T18:23:16Z",
        "summary": "Most language model pre-training frameworks concatenate multiple documents\ninto fixed-length sequences and use causal masking to compute the likelihood of\neach token given its context; this strategy is widely adopted due to its\nsimplicity and efficiency. However, to this day, the influence of the\npre-training sequence composition strategy on the generalisation properties of\nthe model remains under-explored. In this work, we find that applying causal\nmasking can lead to the inclusion of distracting information from previous\ndocuments during pre-training, which negatively impacts the performance of the\nmodels on language modelling and downstream tasks. In intra-document causal\nmasking, the likelihood of each token is only conditioned on the previous\ntokens in the same document, eliminating potential distracting information from\nprevious documents and significantly improving performance. Furthermore, we\nfind that concatenating related documents can reduce some potential\ndistractions during pre-training, and our proposed efficient retrieval-based\nsequence construction method, BM25Chunk, can improve in-context learning\n(+11.6\\%), knowledge memorisation (+9.8\\%), and context utilisation (+7.2\\%)\nabilities of language models without sacrificing efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.13991v1.pdf"
    },
    {
        "title": "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning",
        "authors": [
            "Debjit Paul",
            "Robert West",
            "Antoine Bosselut",
            "Boi Faltings"
        ],
        "published": "2024-02-21T17:23:59Z",
        "summary": "Large language models (LLMs) have been shown to perform better when asked to\nreason step-by-step before answering a question. However, it is unclear to what\ndegree the model's final answer is faithful to the stated reasoning steps. In\nthis paper, we perform a causal mediation analysis on twelve LLMs to examine\nhow intermediate reasoning steps generated by the LLM influence the final\noutcome and find that LLMs do not reliably use their intermediate reasoning\nsteps when generating an answer. To address this issue, we introduce FRODO, a\nframework to tailor small-sized LMs to generate correct reasoning steps and\nrobustly reason over these steps. FRODO consists of an inference module that\nlearns to generate correct reasoning steps using an implicit causal reward\nfunction and a reasoning module that learns to faithfully reason over these\nintermediate inferences using a counterfactual and causal preference objective.\nOur experiments show that FRODO significantly outperforms four competitive\nbaselines. Furthermore, FRODO improves the robustness and generalization\nability of the reasoning LM, yielding higher performance on out-of-distribution\ntest sets. Finally, we find that FRODO's rationales are more faithful to its\nfinal answer predictions than standard supervised fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.13950v2.pdf"
    },
    {
        "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
        "authors": [
            "Phuc Phan",
            "Hieu Tran",
            "Long Phan"
        ],
        "published": "2024-02-21T17:20:38Z",
        "summary": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.14874v1.pdf"
    },
    {
        "title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content",
        "authors": [
            "Federico Bianchi",
            "James Zou"
        ],
        "published": "2024-02-21T16:46:36Z",
        "summary": "The risks derived from large language models (LLMs) generating deceptive and\ndamaging content have been the subject of considerable research, but even safe\ngenerations can lead to problematic downstream impacts. In our study, we shift\nthe focus to how even safe text coming from LLMs can be easily turned into\npotentially dangerous content through Bait-and-Switch attacks. In such attacks,\nthe user first prompts LLMs with safe questions and then employs a simple\nfind-and-replace post-hoc technique to manipulate the outputs into harmful\nnarratives. The alarming efficacy of this approach in generating toxic content\nhighlights a significant challenge in developing reliable safety guardrails for\nLLMs. In particular, we stress that focusing on the safety of the verbatim LLM\noutputs is insufficient and that we also need to consider post-hoc\ntransformations.",
        "pdf_link": "https://arxiv.org/pdf/2402.13926v1.pdf"
    },
    {
        "title": "Exploring ChatGPT and its Impact on Society",
        "authors": [
            "Md. Asraful Haque",
            "Shuai Li"
        ],
        "published": "2024-02-21T16:44:35Z",
        "summary": "Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.",
        "pdf_link": "https://arxiv.org/pdf/2403.14643v2.pdf"
    },
    {
        "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
        "authors": [
            "Prakamya Mishra",
            "Zonghai Yao",
            "Parth Vashisht",
            "Feiyun Ouyang",
            "Beining Wang",
            "Vidhi Dhaval Mody",
            "Hong Yu"
        ],
        "published": "2024-02-21T16:33:22Z",
        "summary": "Large Language Models (LLMs) such as GPT and Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate\nhigh-quality feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback, mirroring the\npractical scenario in which medical professionals refine AI system outputs\nwithout the need for additional annotations. Despite GPT's proven expertise in\nvarious clinical NLP tasks, such as the Medical Licensing Examination, there is\nscant research on its capacity to deliver expert-level edit feedback for\nimproving weaker LMs or LLMs generation quality. This work leverages GPT's\nadvanced capabilities in clinical NLP to offer expert-level edit feedback.\nThrough the use of two distinct alignment algorithms (DPO and SALT) based on\nGPT edit feedback, our goal is to reduce hallucinations and align closely with\nmedical facts, endeavoring to narrow the divide between AI-generated content\nand factual accuracy. This highlights the substantial potential of GPT edits in\nenhancing the alignment of clinical factuality.",
        "pdf_link": "https://arxiv.org/pdf/2402.13919v1.pdf"
    },
    {
        "title": "Could We Have Had Better Multilingual LLMs If English Was Not the Central Language?",
        "authors": [
            "Ryandito Diandaru",
            "Lucky Susanto",
            "Zilu Tang",
            "Ayu Purwarianti",
            "Derry Wijaya"
        ],
        "published": "2024-02-21T16:32:38Z",
        "summary": "Large Language Models (LLMs) demonstrate strong machine translation\ncapabilities on languages they are trained on. However, the impact of factors\nbeyond training data size on translation performance remains a topic of debate,\nespecially concerning languages not directly encountered during training. Our\nstudy delves into Llama2's translation capabilities. By modeling a linear\nrelationship between linguistic feature distances and machine translation\nscores, we ask ourselves if there are potentially better central languages for\nLLMs other than English. Our experiments show that the 7B Llama2 model yields\nabove 10 BLEU when translating into all languages it has seen, which rarely\nhappens for languages it has not seen. Most translation improvements into\nunseen languages come from scaling up the model size rather than instruction\ntuning or increasing shot count. Furthermore, our correlation analysis reveals\nthat syntactic similarity is not the only linguistic factor that strongly\ncorrelates with machine translation scores. Interestingly, we discovered that\nunder specific circumstances, some languages (e.g. Swedish, Catalan), despite\nhaving significantly less training data, exhibit comparable correlation levels\nto English. These insights challenge the prevailing landscape of LLMs,\nsuggesting that models centered around languages other than English could\nprovide a more efficient foundation for multilingual applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.13917v2.pdf"
    },
    {
        "title": "Calibrating Large Language Models with Sample Consistency",
        "authors": [
            "Qing Lyu",
            "Kumar Shridhar",
            "Chaitanya Malaviya",
            "Li Zhang",
            "Yanai Elazar",
            "Niket Tandon",
            "Marianna Apidianaki",
            "Mrinmaya Sachan",
            "Chris Callison-Burch"
        ],
        "published": "2024-02-21T16:15:20Z",
        "summary": "Accurately gauging the confidence level of Large Language Models' (LLMs)\npredictions is pivotal for their reliable application. However, LLMs are often\nuncalibrated inherently and elude conventional calibration techniques due to\ntheir proprietary nature and massive scale. In this work, we explore the\npotential of deriving confidence from the distribution of multiple randomly\nsampled model generations, via three measures of consistency. We perform an\nextensive evaluation across various open and closed-source models on nine\nreasoning datasets. Results show that consistency-based calibration methods\noutperform existing post-hoc approaches. Meanwhile, we find that factors such\nas intermediate explanations, model scaling, and larger sample sizes enhance\ncalibration, while instruction-tuning makes calibration more difficult.\nMoreover, confidence scores obtained from consistency have the potential to\nenhance model performance. Finally, we offer practical guidance on choosing\nsuitable consistency metrics for calibration, tailored to the characteristics\nof various LMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.13904v1.pdf"
    },
    {
        "title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
        "authors": [
            "Chenyang Lyu",
            "Minghao Wu",
            "Alham Fikri Aji"
        ],
        "published": "2024-02-21T15:58:37Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious applications, fundamentally reshaping the landscape of natural language\nprocessing (NLP) research. However, recent evaluation frameworks often rely on\nthe output probabilities of LLMs for predictions, primarily due to\ncomputational constraints, diverging from real-world LLM usage scenarios. While\nwidely employed, the efficacy of these probability-based evaluation strategies\nremains an open research question. This study aims to scrutinize the validity\nof such probability-based evaluation methods within the context of using LLMs\nfor Multiple Choice Questions (MCQs), highlighting their inherent limitations.\nOur empirical investigation reveals that the prevalent probability-based\nevaluation method inadequately aligns with generation-based prediction.\nFurthermore, current evaluation frameworks typically assess LLMs through\npredictive tasks based on output probabilities rather than directly generating\nresponses, owing to computational limitations. We illustrate that these\nprobability-based approaches do not effectively correspond with generative\npredictions. The outcomes of our study can enhance the understanding of LLM\nevaluation methodologies and provide insights for future research in this\ndomain.",
        "pdf_link": "https://arxiv.org/pdf/2402.13887v1.pdf"
    }
]