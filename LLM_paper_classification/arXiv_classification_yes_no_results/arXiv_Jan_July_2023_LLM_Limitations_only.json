[
    {
        "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
        "authors": [
            "B. A. Levinstein",
            "Daniel A. Herrmann"
        ],
        "published": "2023-06-30T23:44:51Z",
        "summary": "We consider the questions of whether or not large language models (LLMs) have\nbeliefs, and, if they do, how we might measure them. First, we evaluate two\nexisting approaches, one due to Azaria and Mitchell (2023) and the other to\nBurns et al. (2022). We provide empirical results that show that these methods\nfail to generalize in very basic ways. We then argue that, even if LLMs have\nbeliefs, these methods are unlikely to be successful for conceptual reasons.\nThus, there is still no lie-detector for LLMs. After describing our empirical\nresults we take a step back and consider whether or not we should expect LLMs\nto have something like beliefs in the first place. We consider some recent\narguments aiming to show that LLMs cannot have beliefs. We show that these\narguments are misguided. We provide a more productive framing of questions\nsurrounding the status of beliefs in LLMs, and highlight the empirical nature\nof the problem. We conclude by suggesting some concrete paths for future work.",
        "pdf_link": "https://arxiv.org/pdf/2307.00175v1.pdf"
    },
    {
        "title": "Large Language Models (GPT) for automating feedback on programming assignments",
        "authors": [
            "Maciej Pankiewicz",
            "Ryan S. Baker"
        ],
        "published": "2023-06-30T21:57:40Z",
        "summary": "Addressing the challenge of generating personalized feedback for programming\nassignments is demanding due to several factors, like the complexity of code\nsyntax or different ways to correctly solve a task. In this experimental study,\nwe automated the process of feedback generation by employing OpenAI's GPT-3.5\nmodel to generate personalized hints for students solving programming\nassignments on an automated assessment platform. Students rated the usefulness\nof GPT-generated hints positively. The experimental group (with GPT hints\nenabled) relied less on the platform's regular feedback but performed better in\nterms of percentage of successful submissions across consecutive attempts for\ntasks, where GPT hints were enabled. For tasks where the GPT feedback was made\nunavailable, the experimental group needed significantly less time to solve\nassignments. Furthermore, when GPT hints were unavailable, students in the\nexperimental condition were initially less likely to solve the assignment\ncorrectly. This suggests potential over-reliance on GPT-generated feedback.\nHowever, students in the experimental condition were able to correct reasonably\nrapidly, reaching the same percentage correct after seven submission attempts.\nThe availability of GPT hints did not significantly impact students' affective\nstate.",
        "pdf_link": "https://arxiv.org/pdf/2307.00150v1.pdf"
    },
    {
        "title": "Meta-training with Demonstration Retrieval for Efficient Few-shot Learning",
        "authors": [
            "Aaron Mueller",
            "Kanika Narang",
            "Lambert Mathias",
            "Qifan Wang",
            "Hamed Firooz"
        ],
        "published": "2023-06-30T20:16:22Z",
        "summary": "Large language models show impressive results on few-shot NLP tasks. However,\nthese models are memory and computation-intensive. Meta-training allows one to\nleverage smaller models for few-shot generalization in a domain-general and\ntask-agnostic manner; however, these methods alone results in models that may\nnot have sufficient parameterization or knowledge to adapt quickly to a large\nvariety of tasks. To overcome this issue, we propose meta-training with\ndemonstration retrieval, where we use a dense passage retriever to retrieve\nsemantically similar labeled demonstrations to each example for more varied\nsupervision. By separating external knowledge from model parameters, we can use\nmeta-training to train parameter-efficient models that generalize well on a\nlarger variety of tasks. We construct a meta-training set from UnifiedQA and\nCrossFit, and propose a demonstration bank based on UnifiedQA tasks. To our\nknowledge, our work is the first to combine retrieval with meta-training, to\nuse DPR models to retrieve demonstrations, and to leverage demonstrations from\nmany tasks simultaneously, rather than randomly sampling demonstrations from\nthe training set of the target task. Our approach outperforms a variety of\ntargeted parameter-efficient and retrieval-augmented few-shot methods on QA,\nNLI, and text classification tasks (including SQuAD, QNLI, and TREC). Our\napproach can be meta-trained and fine-tuned quickly on a single GPU.",
        "pdf_link": "https://arxiv.org/pdf/2307.00119v1.pdf"
    },
    {
        "title": "Ticket-BERT: Labeling Incident Management Tickets with Language Models",
        "authors": [
            "Zhexiong Liu",
            "Cris Benge",
            "Siduo Jiang"
        ],
        "published": "2023-06-30T19:48:25Z",
        "summary": "An essential aspect of prioritizing incident tickets for resolution is\nefficiently labeling tickets with fine-grained categories. However, ticket data\nis often complex and poses several unique challenges for modern machine\nlearning methods: (1) tickets are created and updated either by machines with\npre-defined algorithms or by engineers with domain expertise that share\ndifferent protocols, (2) tickets receive frequent revisions that update ticket\nstatus by modifying all or parts of ticket descriptions, and (3) ticket\nlabeling is time-sensitive and requires knowledge updates and new labels per\nthe rapid software and hardware improvement lifecycle. To handle these issues,\nwe introduce Ticket- BERT which trains a simple yet robust language model for\nlabeling tickets using our proposed ticket datasets. Experiments demonstrate\nthe superiority of Ticket-BERT over baselines and state-of-the-art text\nclassifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT\nwith an active learning cycle and deploy it on the Microsoft IcM system, which\nenables the model to quickly finetune on newly-collected tickets with a few\nannotations.",
        "pdf_link": "https://arxiv.org/pdf/2307.00108v1.pdf"
    },
    {
        "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models",
        "authors": [
            "Harnoor Dhingra",
            "Preetiha Jayashanker",
            "Sayali Moghe",
            "Emma Strubell"
        ],
        "published": "2023-06-30T19:39:01Z",
        "summary": "Large Language Models (LLMs) are trained primarily on minimally processed web\ntext, which exhibits the same wide range of social biases held by the humans\nwho created that content. Consequently, text generated by LLMs can\ninadvertently perpetuate stereotypes towards marginalized groups, like the\nLGBTQIA+ community. In this paper, we perform a comparative study of how LLMs\ngenerate text describing people with different sexual identities. Analyzing\nbias in the text generated by an LLM using regard score shows measurable bias\nagainst queer people. We then show that a post-hoc method based on\nchain-of-thought prompting using SHAP analysis can increase the regard of the\nsentence, representing a promising approach towards debiasing the output of\nLLMs in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2307.00101v1.pdf"
    },
    {
        "title": "Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Pei Zhang",
            "Baosong Yang",
            "Rui Wang"
        ],
        "published": "2023-06-30T17:38:10Z",
        "summary": "Neural-symbolic methods have demonstrated efficiency in enhancing the\nreasoning abilities of large language models (LLMs). However, existing methods\nmainly rely on syntactically mapping natural languages to complete formal\nlanguages like Python and SQL. Those methods require that reasoning tasks be\nconvertible into programs, which cater to the computer execution mindset and\ndeviate from human reasoning habits. To broaden symbolic methods' applicability\nand adaptability in the real world, we propose the Meta-Reasoning from a\nlinguistic perspective. This method empowers LLMs to deconstruct\nreasoning-independent semantic information into generic symbolic\nrepresentations, thereby efficiently capturing more generalized reasoning\nknowledge. We conduct extensive experiments on more than ten datasets\nencompassing conventional reasoning tasks like arithmetic, symbolic, and\nlogical reasoning, and the more complex interactive reasoning tasks like\ntheory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning\nsignificantly enhances in-context reasoning accuracy, learning efficiency,\nout-of-domain generalization, and output stability compared to the\nChain-of-Thought technique. Code and data are publicly available at\n\\url{https://github.com/Alsace08/Meta-Reasoning}.",
        "pdf_link": "https://arxiv.org/pdf/2306.17820v3.pdf"
    },
    {
        "title": "Biomedical Language Models are Robust to Sub-optimal Tokenization",
        "authors": [
            "Bernal Jiménez Gutiérrez",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023-06-30T13:35:24Z",
        "summary": "As opposed to general English, many concepts in biomedical terminology have\nbeen designed in recent history by biomedical professionals with the goal of\nbeing precise and concise. This is often achieved by concatenating meaningful\nbiomedical morphemes to create new semantic units. Nevertheless, most modern\nbiomedical language models (LMs) are pre-trained using standard domain-specific\ntokenizers derived from large scale biomedical corpus statistics without\nexplicitly leveraging the agglutinating nature of biomedical language. In this\nwork, we first find that standard open-domain and biomedical tokenizers are\nlargely unable to segment biomedical terms into meaningful components.\nTherefore, we hypothesize that using a tokenizer which segments biomedical\nterminology more accurately would enable biomedical LMs to improve their\nperformance on downstream biomedical NLP tasks, especially ones which involve\nbiomedical terms directly such as named entity recognition (NER) and entity\nlinking. Surprisingly, we find that pre-training a biomedical LM using a more\naccurate biomedical tokenizer does not improve the entity representation\nquality of a language model as measured by several intrinsic and extrinsic\nmeasures such as masked language modeling prediction (MLM) accuracy as well as\nNER and entity linking performance. These quantitative findings, along with a\ncase study which explores entity representation quality more directly, suggest\nthat the biomedical pre-training process is quite robust to instances of\nsub-optimal tokenization.",
        "pdf_link": "https://arxiv.org/pdf/2306.17649v3.pdf"
    },
    {
        "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
        "authors": [
            "Zhen Qin",
            "Rolf Jagerman",
            "Kai Hui",
            "Honglei Zhuang",
            "Junru Wu",
            "Le Yan",
            "Jiaming Shen",
            "Tianqi Liu",
            "Jialu Liu",
            "Donald Metzler",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "published": "2023-06-30T11:32:25Z",
        "summary": "Ranking documents using Large Language Models (LLMs) by directly feeding the\nquery and candidate documents into the prompt is an interesting and practical\nproblem. However, researchers have found it difficult to outperform fine-tuned\nbaseline rankers on benchmark datasets. We analyze pointwise and listwise\nranking prompts used by existing methods and argue that off-the-shelf LLMs do\nnot fully understand these challenging ranking formulations. In this paper, we\npropose to significantly reduce the burden on LLMs by using a new technique\ncalled Pairwise Ranking Prompting (PRP). Our results are the first in the\nliterature to achieve state-of-the-art ranking performance on standard\nbenchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP\nbased on the Flan-UL2 model with 20B parameters performs favorably with the\nprevious best approach in the literature, which is based on the blackbox\ncommercial GPT-4 that has 50x (estimated) model size, while outperforming other\nLLM-based solutions, such as InstructGPT which has 175B parameters, by over 10%\nfor all ranking metrics. By using the same prompt template on seven BEIR tasks,\nPRP outperforms supervised baselines and outperforms the blackbox commercial\nChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on\naverage NDCG@10. Furthermore, we propose several variants of PRP to improve\nefficiency and show that it is possible to achieve competitive results even\nwith linear complexity.",
        "pdf_link": "https://arxiv.org/pdf/2306.17563v2.pdf"
    },
    {
        "title": "Preference Ranking Optimization for Human Alignment",
        "authors": [
            "Feifan Song",
            "Bowen Yu",
            "Minghao Li",
            "Haiyang Yu",
            "Fei Huang",
            "Yongbin Li",
            "Houfeng Wang"
        ],
        "published": "2023-06-30T09:07:37Z",
        "summary": "Large language models (LLMs) often contain misleading content, emphasizing\nthe need to align them with human values to ensure secure AI systems.\nReinforcement learning from human feedback (RLHF) has been employed to achieve\nthis alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits\ncomplexity, instability, and sensitivity to hyperparameters in contrast to SFT.\n(2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise\ncontrast, thus lacking contrasts from a macro perspective. In this paper, we\npropose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to\ndirectly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast\nto accommodate preference rankings of any length. By iteratively contrasting\ncandidates, PRO instructs the LLM to prioritize the best response while\nprogressively ranking the rest responses. In this manner, PRO effectively\ntransforms human alignment into aligning the probability ranking of n responses\ngenerated by LLM with the preference ranking of humans towards these responses.\nExperiments have shown that PRO outperforms baseline algorithms, achieving\ncomparable results to ChatGPT and human responses through automatic-based,\nreward-based, GPT-4, and human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2306.17492v2.pdf"
    },
    {
        "title": "Provable Robust Watermarking for AI-Generated Text",
        "authors": [
            "Xuandong Zhao",
            "Prabhanjan Ananth",
            "Lei Li",
            "Yu-Xiang Wang"
        ],
        "published": "2023-06-30T07:24:32Z",
        "summary": "We study the problem of watermarking large language models (LLMs) generated\ntext -- one of the most promising approaches for addressing the safety\nchallenges of LLM usage. In this paper, we propose a rigorous theoretical\nframework to quantify the effectiveness and robustness of LLM watermarks. We\npropose a robust and high-quality watermark method, Unigram-Watermark, by\nextending an existing approach with a simplified fixed grouping strategy. We\nprove that our watermark method enjoys guaranteed generation quality,\ncorrectness in watermark detection, and is robust against text editing and\nparaphrasing. Experiments on three varying LLMs and two datasets verify that\nour Unigram-Watermark achieves superior detection accuracy and comparable\ngeneration quality in perplexity, thus promoting the responsible use of LLMs.\nCode is available at https://github.com/XuandongZhao/Unigram-Watermark.",
        "pdf_link": "https://arxiv.org/pdf/2306.17439v2.pdf"
    },
    {
        "title": "LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection",
        "authors": [
            "Zijian Cai",
            "Zhaoxuan Tan",
            "Zhenyu Lei",
            "Zifeng Zhu",
            "Hongrui Wang",
            "Qinghua Zheng",
            "Minnan Luo"
        ],
        "published": "2023-06-30T05:50:26Z",
        "summary": "As malicious actors employ increasingly advanced and widespread bots to\ndisseminate misinformation and manipulate public opinion, the detection of\nTwitter bots has become a crucial task. Though graph-based Twitter bot\ndetection methods achieve state-of-the-art performance, we find that their\ninference depends on the neighbor users multi-hop away from the targets, and\nfetching neighbors is time-consuming and may introduce bias. At the same time,\nwe find that after finetuning on Twitter bot detection, pretrained language\nmodels achieve competitive performance and do not require a graph structure\nduring deployment. Inspired by this finding, we propose a novel bot detection\nframework LMBot that distills the knowledge of graph neural networks (GNNs)\ninto language models (LMs) for graph-less deployment in Twitter bot detection\nto combat the challenge of data dependency. Moreover, LMBot is compatible with\ngraph-based and graph-less datasets. Specifically, we first represent each user\nas a textual sequence and feed them into the LM for domain adaptation. For\ngraph-based datasets, the output of LMs provides input features for the GNN,\nenabling it to optimize for bot detection and distill knowledge back to the LM\nin an iterative, mutually enhancing process. Armed with the LM, we can perform\ngraph-less inference, which resolves the graph data dependency and sampling\nbias issues. For datasets without graph structure, we simply replace the GNN\nwith an MLP, which has also shown strong performance. Our experiments\ndemonstrate that LMBot achieves state-of-the-art performance on four Twitter\nbot detection benchmarks. Extensive studies also show that LMBot is more\nrobust, versatile, and efficient compared to graph-based Twitter bot detection\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2306.17408v3.pdf"
    },
    {
        "title": "SummQA at MEDIQA-Chat 2023:In-Context Learning with GPT-4 for Medical Summarization",
        "authors": [
            "Yash Mathur",
            "Sanketh Rangreji",
            "Raghav Kapoor",
            "Medha Palavalli",
            "Amanda Bertsch",
            "Matthew R. Gormley"
        ],
        "published": "2023-06-30T03:14:04Z",
        "summary": "Medical dialogue summarization is challenging due to the unstructured nature\nof medical conversations, the use of medical terminology in gold summaries, and\nthe need to identify key information across multiple symptom sets. We present a\nnovel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA\n2023 Shared Task. Our approach for section-wise summarization (Task A) is a\ntwo-stage process of selecting semantically similar dialogues and using the\ntop-k similar dialogues as in-context examples for GPT-4. For full-note\nsummarization (Task B), we use a similar solution with k=1. We achieved 3rd\nplace in Task A (2nd among all teams), 4th place in Task B Division Wise\nSummarization (2nd among all teams), 15th place in Task A Section Header\nClassification (9th among all teams), and 8th place among all teams in Task B.\nOur results highlight the effectiveness of few-shot prompting for this task,\nthough we also identify several weaknesses of prompting-based approaches. We\ncompare GPT-4 performance with several finetuned baselines. We find that GPT-4\nsummaries are more abstractive and shorter. We make our code publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2306.17384v1.pdf"
    },
    {
        "title": "Modeling Parallel Programs using Large Language Models",
        "authors": [
            "Daniel Nichols",
            "Aniruddha Marathe",
            "Harshitha Menon",
            "Todd Gamblin",
            "Abhinav Bhatele"
        ],
        "published": "2023-06-29T19:44:55Z",
        "summary": "Parallel software codes in high performance computing (HPC) continue to grow\nin complexity and scale as we enter the exascale era. A diverse set of emerging\nhardware and programming paradigms make developing, optimizing, and maintaining\nparallel software burdensome for developers. One way to alleviate some of these\nburdens is with automated development and analysis tools. Such tools can\nperform complex and/or remedial tasks for developers that increase their\nproductivity and decrease the chance for error. So far, such tools for code\ndevelopment and performance analysis have been limited in the complexity of\ntasks they can perform. However, with recent advancements in language modeling,\nand the wealth of code related data that is now available online, these tools\nhave started to utilize predictive language models to automate more complex\ntasks. In this paper, we show how large language models (LLMs) can be applied\nto tasks specific to high performance and scientific codes. We train LLMs using\ncode and performance data that is specific to parallel codes. We compare\nseveral recent LLMs on HPC related tasks and introduce a new model, HPC-Coder,\ntrained on parallel code. In our experiments we show that this model can\nauto-complete HPC functions where general models cannot, decorate for loops\nwith OpenMP pragmas, and model performance changes in two scientific\napplication repositories.",
        "pdf_link": "https://arxiv.org/pdf/2306.17281v1.pdf"
    },
    {
        "title": "DisasterResponseGPT: Large Language Models for Accelerated Plan of Action Development in Disaster Response Scenarios",
        "authors": [
            "Vinicius G. Goecks",
            "Nicholas R. Waytowich"
        ],
        "published": "2023-06-29T19:24:19Z",
        "summary": "The development of plans of action in disaster response scenarios is a\ntime-consuming process. Large Language Models (LLMs) offer a powerful solution\nto expedite this process through in-context learning. This study presents\nDisasterResponseGPT, an algorithm that leverages LLMs to generate valid plans\nof action quickly by incorporating disaster response and planning guidelines in\nthe initial prompt. In DisasterResponseGPT, users input the scenario\ndescription and receive a plan of action as output. The proposed method\ngenerates multiple plans within seconds, which can be further refined following\nthe user's feedback. Preliminary results indicate that the plans of action\ndeveloped by DisasterResponseGPT are comparable to human-generated ones while\noffering greater ease of modification in real-time. This approach has the\npotential to revolutionize disaster response operations by enabling rapid\nupdates and adjustments during the plan's execution.",
        "pdf_link": "https://arxiv.org/pdf/2306.17271v1.pdf"
    },
    {
        "title": "Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendations",
        "authors": [
            "Xuansheng Wu",
            "Huachi Zhou",
            "Yucheng Shi",
            "Wenlin Yao",
            "Xiao Huang",
            "Ninghao Liu"
        ],
        "published": "2023-06-29T18:50:12Z",
        "summary": "Recommendation systems help users find matched items based on their previous\nbehaviors. Personalized recommendation becomes challenging in the absence of\nhistorical user-item interactions, a practical problem for startups known as\nthe system cold-start recommendation. While existing research addresses\ncold-start issues for either users or items, we still lack solutions for system\ncold-start scenarios. To tackle the problem, we propose PromptRec, a simple but\neffective approach based on in-context learning of language models, where we\ntransform the recommendation task into the sentiment analysis task on natural\nlanguage containing user and item profiles. However, this naive approach\nheavily relies on the strong in-context learning ability emerged from large\nlanguage models, which could suffer from significant latency for online\nrecommendations. To solve the challenge, we propose to enhance small language\nmodels for recommender systems with a data-centric pipeline, which consists of:\n(1) constructing a refined corpus for model pre-training; (2) constructing a\ndecomposed prompt template via prompt pre-training. They correspond to the\ndevelopment of training data and inference data, respectively. The pipeline is\nsupported by a theoretical framework that formalizes the connection between\nin-context recommendation and language modeling. To evaluate our approach, we\nintroduce a cold-start recommendation benchmark, and the results demonstrate\nthat the enhanced small language models can achieve comparable cold-start\nrecommendation performance to that of large models with only $17\\%$ of the\ninference time. To the best of our knowledge, this is the first study to tackle\nthe system cold-start recommendation problem. We believe our findings will\nprovide valuable insights for future works. The benchmark and implementations\nare available at https://github.com/JacksonWuxs/PromptRec.",
        "pdf_link": "https://arxiv.org/pdf/2306.17256v5.pdf"
    },
    {
        "title": "Generative AI for Programming Education: Benchmarking ChatGPT, GPT-4, and Human Tutors",
        "authors": [
            "Tung Phung",
            "Victor-Alexandru Pădurean",
            "José Cambronero",
            "Sumit Gulwani",
            "Tobias Kohn",
            "Rupak Majumdar",
            "Adish Singla",
            "Gustavo Soares"
        ],
        "published": "2023-06-29T17:57:40Z",
        "summary": "Generative AI and large language models hold great promise in enhancing\ncomputing education by powering next-generation educational technologies for\nintroductory programming. Recent works have studied these models for different\nscenarios relevant to programming education; however, these works are limited\nfor several reasons, as they typically consider already outdated models or only\nspecific scenario(s). Consequently, there is a lack of a systematic study that\nbenchmarks state-of-the-art models for a comprehensive set of programming\neducation scenarios. In our work, we systematically evaluate two models,\nChatGPT (based on GPT-3.5) and GPT-4, and compare their performance with human\ntutors for a variety of scenarios. We evaluate using five introductory Python\nprogramming problems and real-world buggy programs from an online platform, and\nassess performance using expert-based annotations. Our results show that GPT-4\ndrastically outperforms ChatGPT (based on GPT-3.5) and comes close to human\ntutors' performance for several scenarios. These results also highlight\nsettings where GPT-4 still struggles, providing exciting future directions on\ndeveloping techniques to improve the performance of these models.",
        "pdf_link": "https://arxiv.org/pdf/2306.17156v3.pdf"
    },
    {
        "title": "Concept-Oriented Deep Learning with Large Language Models",
        "authors": [
            "Daniel T. Chang"
        ],
        "published": "2023-06-29T16:47:11Z",
        "summary": "Large Language Models (LLMs) have been successfully used in many\nnatural-language tasks and applications including text generation and AI\nchatbots. They also are a promising new technology for concept-oriented deep\nlearning (CODL). However, the prerequisite is that LLMs understand concepts and\nensure conceptual consistency. We discuss these in this paper, as well as major\nuses of LLMs for CODL including concept extraction from text, concept graph\nextraction from text, and concept learning. Human knowledge consists of both\nsymbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only\nLLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal\nLLMs, on the other hand, are capable of representing the full range (conceptual\nand sensory) of human knowledge. We discuss conceptual understanding in\nvisual-language LLMs, the most important multimodal LLMs, and major uses of\nthem for CODL including concept extraction from image, concept graph extraction\nfrom image, and concept learning. While uses of LLMs for CODL are valuable\nstandalone, they are particularly valuable as part of LLM applications such as\nAI chatbots.",
        "pdf_link": "https://arxiv.org/pdf/2306.17089v2.pdf"
    },
    {
        "title": "UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?",
        "authors": [
            "Junda Wang",
            "Zonghai Yao",
            "Avijit Mitra",
            "Samuel Osebe",
            "Zhichao Yang",
            "Hong Yu"
        ],
        "published": "2023-06-29T13:30:41Z",
        "summary": "This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023\nshared task for Task-A and Task-C. We focus especially on Task-C and propose a\nnovel LLMs cooperation system named a doctor-patient loop to generate\nhigh-quality conversation data sets. The experiment results demonstrate that\nour approaches yield reasonable performance as evaluated by automatic metrics\nsuch as ROUGE, medical concept recall, BLEU, and Self-BLEU. Furthermore, we\nconducted a comparative analysis between our proposed method and ChatGPT and\nGPT-4. This analysis also investigates the potential of utilizing cooperation\nLLMs to generate high-quality datasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.16931v1.pdf"
    },
    {
        "title": "From Query Tools to Causal Architects: Harnessing Large Language Models for Advanced Causal Discovery from Data",
        "authors": [
            "Taiyu Ban",
            "Lyvzhou Chen",
            "Xiangyu Wang",
            "Huanhuan Chen"
        ],
        "published": "2023-06-29T12:48:00Z",
        "summary": "Large Language Models (LLMs) exhibit exceptional abilities for causal\nanalysis between concepts in numerous societally impactful domains, including\nmedicine, science, and law. Recent research on LLM performance in various\ncausal discovery and inference tasks has given rise to a new ladder in the\nclassical three-stage framework of causality. In this paper, we advance the\ncurrent research of LLM-driven causal discovery by proposing a novel framework\nthat combines knowledge-based LLM causal analysis with data-driven causal\nstructure learning. To make LLM more than a query tool and to leverage its\npower in discovering natural and new laws of causality, we integrate the\nvaluable LLM expertise on existing causal mechanisms into statistical analysis\nof objective data to build a novel and practical baseline for causal structure\nlearning.\n  We introduce a universal set of prompts designed to extract causal graphs\nfrom given variables and assess the influence of LLM prior causality on\nrecovering causal structures from data. We demonstrate the significant\nenhancement of LLM expertise on the quality of recovered causal structures from\ndata, while also identifying critical challenges and issues, along with\npotential approaches to address them. As a pioneering study, this paper aims to\nemphasize the new frontier that LLMs are opening for classical causal discovery\nand inference, and to encourage the widespread adoption of LLM capabilities in\ndata-driven causal analysis.",
        "pdf_link": "https://arxiv.org/pdf/2306.16902v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Model Capabilities for Conditional Generation",
        "authors": [
            "Joshua Maynez",
            "Priyanka Agrawal",
            "Sebastian Gehrmann"
        ],
        "published": "2023-06-29T08:59:40Z",
        "summary": "Pre-trained large language models (PLMs) underlie most new developments in\nnatural language processing. They have shifted the field from\napplication-specific model pipelines to a single model that is adapted to a\nwide range of tasks. Autoregressive PLMs like GPT-3 or PaLM, alongside\ntechniques like few-shot learning, have additionally shifted the output\nmodality to generation instead of classification or regression. Despite their\nubiquitous use, the generation quality of language models is rarely evaluated\nwhen these models are introduced. Additionally, it is unclear how existing\ngeneration tasks--while they can be used to compare systems at a high\nlevel--relate to the real world use cases for which people have been adopting\nthem. In this work, we discuss how to adapt existing application-specific\ngeneration benchmarks to PLMs and provide an in-depth, empirical study of the\nlimitations and capabilities of PLMs in natural language generation tasks along\ndimensions such as scale, architecture, input and output language. Our results\nshow that PLMs differ in their applicability to different data regimes and\ntheir generalization to multiple languages and inform which PLMs to use for a\ngiven generation task setup. We share best practices to be taken into\nconsideration when benchmarking generation capabilities during the development\nof upcoming PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.16793v1.pdf"
    },
    {
        "title": "Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game",
        "authors": [
            "Huy A. Nguyen",
            "Hayden Stec",
            "Xinying Hou",
            "Sarah Di",
            "Bruce M. McLaren"
        ],
        "published": "2023-06-29T02:28:09Z",
        "summary": "While open-ended self-explanations have been shown to promote robust learning\nin multiple studies, they pose significant challenges to automated grading and\nfeedback in technology-enhanced learning, due to the unconstrained nature of\nthe students' input. Our work investigates whether recent advances in Large\nLanguage Models, and in particular ChatGPT, can address this issue. Using\ndecimal exercises and student data from a prior study of the learning game\nDecimal Point, with more than 5,000 open-ended self-explanation responses, we\ninvestigate ChatGPT's capability in (1) solving the in-game exercises, (2)\ndetermining the correctness of students' answers, and (3) providing meaningful\nfeedback to incorrect answers. Our results showed that ChatGPT can respond well\nto conceptual questions, but struggled with decimal place values and number\nline problems. In addition, it was able to accurately assess the correctness of\n75% of the students' answers and generated generally high-quality feedback,\nsimilar to human instructors. We conclude with a discussion of ChatGPT's\nstrengths and weaknesses and suggest several venues for extending its use cases\nin digital teaching and learning.",
        "pdf_link": "https://arxiv.org/pdf/2306.16639v1.pdf"
    },
    {
        "title": "A negation detection assessment of GPTs: analysis with the xNot360 dataset",
        "authors": [
            "Ha Thanh Nguyen",
            "Randy Goebel",
            "Francesca Toni",
            "Kostas Stathis",
            "Ken Satoh"
        ],
        "published": "2023-06-29T02:27:48Z",
        "summary": "Negation is a fundamental aspect of natural language, playing a critical role\nin communication and comprehension. Our study assesses the negation detection\nperformance of Generative Pre-trained Transformer (GPT) models, specifically\nGPT-2, GPT-3, GPT-3.5, and GPT-4. We focus on the identification of negation in\nnatural language using a zero-shot prediction approach applied to our custom\nxNot360 dataset. Our approach examines sentence pairs labeled to indicate\nwhether the second sentence negates the first. Our findings expose a\nconsiderable performance disparity among the GPT models, with GPT-4 surpassing\nits counterparts and GPT-3.5 displaying a marked performance reduction. The\noverall proficiency of the GPT models in negation detection remains relatively\nmodest, indicating that this task pushes the boundaries of their natural\nlanguage understanding capabilities. We not only highlight the constraints of\nGPT models in handling negation but also emphasize the importance of logical\nreliability in high-stakes domains such as healthcare, science, and law.",
        "pdf_link": "https://arxiv.org/pdf/2306.16638v1.pdf"
    },
    {
        "title": "CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?",
        "authors": [
            "Tianwen Wei",
            "Jian Luan",
            "Wei Liu",
            "Shuang Dong",
            "Bin Wang"
        ],
        "published": "2023-06-29T02:19:50Z",
        "summary": "We present the Chinese Elementary School Math Word Problems (CMATH) dataset,\ncomprising 1.7k elementary school-level math word problems with detailed\nannotations, source from actual Chinese workbooks and exams. This dataset aims\nto provide a benchmark tool for assessing the following question: to what grade\nlevel of elementary school math do the abilities of popular large language\nmodels (LLMs) correspond? We evaluate a variety of popular LLMs, including both\ncommercial and open-source options, and discover that only GPT-4 achieves\nsuccess (accuracy $\\geq$ 60\\%) across all six elementary school grades, while\nother models falter at different grade levels. Furthermore, we assess the\nrobustness of several top-performing LLMs by augmenting the original problems\nin the CMATH dataset with distracting information. Our findings reveal that\nGPT-4 is able to maintains robustness, while other model fail. We anticipate\nthat our study will expose limitations in LLMs' arithmetic and reasoning\ncapabilities, and promote their ongoing development and advancement.",
        "pdf_link": "https://arxiv.org/pdf/2306.16636v1.pdf"
    },
    {
        "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
        "authors": [
            "Theodore Zhao",
            "Mu Wei",
            "J. Samuel Preston",
            "Hoifung Poon"
        ],
        "published": "2023-06-28T21:11:15Z",
        "summary": "Generative Large language models (LLMs) have demonstrated remarkable\ncapabilities for a wide range of applications, but reducing ungrounded or\nerroneous responses remains a major growth area. Unlike task-specific models,\nthere lack an effective method to calibrate the confidence level of LLM\nresponses to indicate potential errors and facilitate human-in-the-loop\nverification. An important source of calibration stems from expert-stipulated\nprogrammatic supervision, which is often available at low cost but has its own\nlimitations such as noise and coverage. In this paper, we introduce a Pareto\noptimal self-supervision framework that can leverage available programmatic\nsupervision to systematically calibrate LLM responses by producing a risk score\nfor every LLM response, without any additional manual efforts. This is\naccomplished by learning a harmonizer model to align with LLM output as well as\nother weak supervision sources. The model assigns higher risk scores to more\nuncertain LLM responses and facilitate error correction. Experiments on\nstandard relation extraction and classification tasks in biomedical and general\ndomains demonstrate that the proposed risk score is highly correlated with the\nactual LLM error rate. By using a dynamic prompting strategy based on the risk\nscore, we observed significant accuracy improvement for off-the-shelf LLMs,\nboosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model\nand GPT-4 results past SOTA supervised results on challenging evaluation\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.16564v3.pdf"
    },
    {
        "title": "Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023",
        "authors": [
            "Daoji Huang",
            "Otmar Hilliges",
            "Luc Van Gool",
            "Xi Wang"
        ],
        "published": "2023-06-28T20:33:52Z",
        "summary": "We present Palm, a solution to the Long-Term Action Anticipation (LTA) task\nutilizing vision-language and large language models. Given an input video with\nannotated action periods, the LTA task aims to predict possible future actions.\nWe hypothesize that an optimal solution should capture the interdependency\nbetween past and future actions, and be able to infer future actions based on\nthe structure and dependency encoded in the past actions. Large language models\nhave demonstrated remarkable commonsense-based reasoning ability. Inspired by\nthat, Palm chains an image captioning model and a large language model. It\npredicts future actions based on frame descriptions and action labels extracted\nfrom the input videos. Our method outperforms other participants in the EGO4D\nLTA challenge and achieves the best performance in terms of action prediction.\nOur code is available at https://github.com/DanDoge/Palm",
        "pdf_link": "https://arxiv.org/pdf/2306.16545v1.pdf"
    },
    {
        "title": "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language",
        "authors": [
            "William Berrios",
            "Gautam Mittal",
            "Tristan Thrush",
            "Douwe Kiela",
            "Amanpreet Singh"
        ],
        "published": "2023-06-28T17:57:10Z",
        "summary": "We propose LENS, a modular approach for tackling computer vision problems by\nleveraging the power of large language models (LLMs). Our system uses a\nlanguage model to reason over outputs from a set of independent and highly\ndescriptive vision modules that provide exhaustive information about an image.\nWe evaluate the approach on pure computer vision settings such as zero- and\nfew-shot object recognition, as well as on vision and language problems. LENS\ncan be applied to any off-the-shelf LLM and we find that the LLMs with LENS\nperform highly competitively with much bigger and much more sophisticated\nsystems, without any multimodal training whatsoever. We open-source our code at\nhttps://github.com/ContextualAI/lens and provide an interactive demo.",
        "pdf_link": "https://arxiv.org/pdf/2306.16410v1.pdf"
    },
    {
        "title": "On the Exploitability of Instruction Tuning",
        "authors": [
            "Manli Shu",
            "Jiongxiao Wang",
            "Chen Zhu",
            "Jonas Geiping",
            "Chaowei Xiao",
            "Tom Goldstein"
        ],
        "published": "2023-06-28T17:54:04Z",
        "summary": "Instruction tuning is an effective technique to align large language models\n(LLMs) with human intents. In this work, we investigate how an adversary can\nexploit instruction tuning by injecting specific instruction-following examples\ninto the training data that intentionally changes the model's behavior. For\nexample, an adversary can achieve content injection by injecting training\nexamples that mention target content and eliciting such behavior from\ndownstream models. To achieve this goal, we propose \\textit{AutoPoison}, an\nautomated data poisoning pipeline. It naturally and coherently incorporates\nversatile attack goals into poisoned data with the help of an oracle LLM. We\nshowcase two example attacks: content injection and over-refusal attacks, each\naiming to induce a specific exploitable behavior. We quantify and benchmark the\nstrength and the stealthiness of our data poisoning scheme. Our results show\nthat AutoPoison allows an adversary to change a model's behavior by poisoning\nonly a small fraction of data while maintaining a high level of stealthiness in\nthe poisoned examples. We hope our work sheds light on how data quality affects\nthe behavior of instruction-tuned models and raises awareness of the importance\nof data quality for responsible deployments of LLMs. Code is available at\n\\url{https://github.com/azshue/AutoPoison}.",
        "pdf_link": "https://arxiv.org/pdf/2306.17194v2.pdf"
    },
    {
        "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
        "authors": [
            "Esin Durmus",
            "Karina Nyugen",
            "Thomas I. Liao",
            "Nicholas Schiefer",
            "Amanda Askell",
            "Anton Bakhtin",
            "Carol Chen",
            "Zac Hatfield-Dodds",
            "Danny Hernandez",
            "Nicholas Joseph",
            "Liane Lovitt",
            "Sam McCandlish",
            "Orowa Sikder",
            "Alex Tamkin",
            "Janel Thamkul",
            "Jared Kaplan",
            "Jack Clark",
            "Deep Ganguli"
        ],
        "published": "2023-06-28T17:31:53Z",
        "summary": "Large language models (LLMs) may not equitably represent diverse global\nperspectives on societal issues. In this paper, we develop a quantitative\nframework to evaluate whose opinions model-generated responses are more similar\nto. We first build a dataset, GlobalOpinionQA, comprised of questions and\nanswers from cross-national surveys designed to capture diverse opinions on\nglobal issues across different countries. Next, we define a metric that\nquantifies the similarity between LLM-generated survey responses and human\nresponses, conditioned on country. With our framework, we run three experiments\non an LLM trained to be helpful, honest, and harmless with Constitutional AI.\nBy default, LLM responses tend to be more similar to the opinions of certain\npopulations, such as those from the USA, and some European and South American\ncountries, highlighting the potential for biases. When we prompt the model to\nconsider a particular country's perspective, responses shift to be more similar\nto the opinions of the prompted populations, but can reflect harmful cultural\nstereotypes. When we translate GlobalOpinionQA questions to a target language,\nthe model's responses do not necessarily become the most similar to the\nopinions of speakers of those languages. We release our dataset for others to\nuse and build on. Our data is at\nhttps://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide\nan interactive visualization at https://llmglobalvalues.anthropic.com.",
        "pdf_link": "https://arxiv.org/pdf/2306.16388v1.pdf"
    },
    {
        "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
        "authors": [
            "Yiwen Shi",
            "Ping Ren",
            "Jing Wang",
            "Biao Han",
            "Taha ValizadehAslani",
            "Felix Agbavor",
            "Yi Zhang",
            "Meng Hu",
            "Liang Zhao",
            "Hualou Liang"
        ],
        "published": "2023-06-28T14:55:13Z",
        "summary": "Food effect summarization from New Drug Application (NDA) is an essential\ncomponent of product-specific guidance (PSG) development and assessment.\nHowever, manual summarization of food effect from extensive drug application\nreview documents is time-consuming, which arouses a need to develop automated\nmethods. Recent advances in large language models (LLMs) such as ChatGPT and\nGPT-4, have demonstrated great potential in improving the effectiveness of\nautomated text summarization, but its ability regarding the accuracy in\nsummarizing food effect for PSG assessment remains unclear. In this study, we\nintroduce a simple yet effective approach, iterative prompting, which allows\none to interact with ChatGPT or GPT-4 more effectively and efficiently through\nmulti-turn interaction. Specifically, we propose a three-turn iterative\nprompting approach to food effect summarization in which the keyword-focused\nand length-controlled prompts are respectively provided in consecutive turns to\nrefine the quality of the generated summary. We conduct a series of extensive\nevaluations, ranging from automated metrics to FDA professionals and even\nevaluation by GPT-4, on 100 NDA review documents selected over the past five\nyears. We observe that the summary quality is progressively improved throughout\nthe process. Moreover, we find that GPT-4 performs better than ChatGPT, as\nevaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%).\nImportantly, all the FDA professionals unanimously rated that 85% of the\nsummaries generated by GPT-4 are factually consistent with the golden reference\nsummary, a finding further supported by GPT-4 rating of 72% consistency. These\nresults strongly suggest a great potential for GPT-4 to draft food effect\nsummaries that could be reviewed by FDA professionals, thereby improving the\nefficiency of PSG assessment cycle and promoting the generic drug product\ndevelopment.",
        "pdf_link": "https://arxiv.org/pdf/2306.16275v1.pdf"
    },
    {
        "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
        "authors": [
            "Yufei Huang",
            "Deyi Xiong"
        ],
        "published": "2023-06-28T14:14:44Z",
        "summary": "Holistically measuring societal biases of large language models is crucial\nfor detecting and reducing ethical risks in highly capable AI models. In this\nwork, we present a Chinese Bias Benchmark dataset that consists of over 100K\nquestions jointly constructed by human experts and generative language models,\ncovering stereotypes and societal biases in 14 social dimensions related to\nChinese culture and values. The curation process contains 4 essential steps:\nbias identification via extensive literature review, ambiguous context\ngeneration, AI-assisted disambiguous context generation, snd manual review \\&\nrecomposition. The testing instances in the dataset are automatically derived\nfrom 3K+ high-quality templates manually authored with stringent quality\ncontrol. The dataset exhibits wide coverage and high diversity. Extensive\nexperiments demonstrate the effectiveness of the dataset in detecting model\nbias, with all 10 publicly available Chinese large language models exhibiting\nstrong bias in certain categories. Additionally, we observe from our\nexperiments that fine-tuned models could, to a certain extent, heed\ninstructions and avoid generating outputs that are morally harmful in some\ntypes, in the way of \"moral self-correction\". Our dataset and results are\npublicly available at\n\\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ},\noffering debiasing research opportunities to a widened community.",
        "pdf_link": "https://arxiv.org/pdf/2306.16244v1.pdf"
    },
    {
        "title": "Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition",
        "authors": [
            "Yuang Li",
            "Yu Wu",
            "Jinyu Li",
            "Shujie Liu"
        ],
        "published": "2023-06-28T08:29:00Z",
        "summary": "The integration of Language Models (LMs) has proven to be an effective way to\naddress domain shifts in speech recognition. However, these approaches usually\nrequire a significant amount of target domain text data for the training of\nLMs. Different from these methods, in this work, with only a domain-specific\ntext prompt, we propose two zero-shot ASR domain adaptation methods using\nLLaMA, a 7-billion-parameter large language model (LLM). LLM is used in two\nways: 1) second-pass rescoring: reranking N-best hypotheses of a given ASR\nsystem with LLaMA; 2) deep LLM-fusion: incorporating LLM into the decoder of an\nencoder-decoder based ASR system. Experiments show that, with only one domain\nprompt, both methods can effectively reduce word error rates (WER) on\nout-of-domain TedLium-2 and SPGISpeech datasets. Especially, the deep\nLLM-fusion has the advantage of better recall of entity and out-of-vocabulary\nwords.",
        "pdf_link": "https://arxiv.org/pdf/2306.16007v1.pdf"
    },
    {
        "title": "Query Understanding in the Age of Large Language Models",
        "authors": [
            "Avishek Anand",
            "Venktesh V",
            "Abhijit Anand",
            "Vinay Setty"
        ],
        "published": "2023-06-28T08:24:14Z",
        "summary": "Querying, conversing, and controlling search and information-seeking\ninterfaces using natural language are fast becoming ubiquitous with the rise\nand adoption of large-language models (LLM). In this position paper, we\ndescribe a generic framework for interactive query-rewriting using LLMs. Our\nproposal aims to unfold new opportunities for improved and transparent intent\nunderstanding while building high-performance retrieval systems using LLMs. A\nkey aspect of our framework is the ability of the rewriter to fully specify the\nmachine intent by the search engine in natural language that can be further\nrefined, controlled, and edited before the final retrieval phase. The ability\nto present, interact, and reason over the underlying machine intent in natural\nlanguage has profound implications on transparency, ranking performance, and a\ndeparture from the traditional way in which supervised signals were collected\nfor understanding intents. We detail the concept, backed by initial\nexperiments, along with open questions for this interactive query understanding\nframework.",
        "pdf_link": "https://arxiv.org/pdf/2306.16004v1.pdf"
    },
    {
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias",
        "authors": [
            "Yue Yu",
            "Yuchen Zhuang",
            "Jieyu Zhang",
            "Yu Meng",
            "Alexander Ratner",
            "Ranjay Krishna",
            "Jiaming Shen",
            "Chao Zhang"
        ],
        "published": "2023-06-28T03:31:31Z",
        "summary": "Large language models (LLMs) have been recently leveraged as training data\ngenerators for various natural language processing (NLP) tasks. While previous\nresearch has explored different approaches to training models using generated\ndata, they generally rely on simple class-conditional prompts, which may limit\nthe diversity of the generated data and inherit systematic biases of LLM. Thus,\nwe investigate training data generation with diversely attributed prompts\n(e.g., specifying attributes like length and style), which have the potential\nto yield diverse and attributed generated data. Our investigation focuses on\ndatasets with high cardinality and diverse domains, wherein we demonstrate that\nattributed prompts outperform simple class-conditional prompts in terms of the\nresulting model's performance. Additionally, we present a comprehensive\nempirical study on data generation encompassing vital aspects like bias,\ndiversity, and efficiency, and highlight three key observations: firstly,\nsynthetic datasets generated by simple prompts exhibit significant biases, such\nas regional bias; secondly, attribute diversity plays a pivotal role in\nenhancing model performance; lastly, attributed prompts achieve the performance\nof simple class-conditional prompts while utilizing only 5\\% of the querying\ncost of ChatGPT associated with the latter. The data and code are available on\n\\url{https://github.com/yueyu1030/AttrPrompt}.",
        "pdf_link": "https://arxiv.org/pdf/2306.15895v2.pdf"
    },
    {
        "title": "Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",
        "authors": [
            "Salmonn Talebi",
            "Elizabeth Tong",
            "Mohammad R. K. Mofrad"
        ],
        "published": "2023-06-28T03:03:51Z",
        "summary": "The use of large language models (LLMs) in healthcare is gaining popularity,\nbut their practicality and safety in clinical settings have not been thoroughly\nassessed. In high-stakes environments like medical settings, trust and safety\nare critical issues for LLMs. To address these concerns, we present an approach\nto evaluate the performance and trustworthiness of a GPT3.5 model for medical\nimage protocol assignment. We compare it with a fine-tuned BERT model and a\nradiologist. In addition, we have a radiologist review the GPT3.5 output to\nevaluate its decision-making process. Our evaluation dataset consists of 4,700\nphysician entries across 11 imaging protocol classes spanning the entire head.\nOur findings suggest that the GPT3.5 performance falls behind BERT and a\nradiologist. However, GPT3.5 outperforms BERT in its ability to explain its\ndecision, detect relevant word indicators, and model calibration. Furthermore,\nby analyzing the explanations of GPT3.5 for misclassifications, we reveal\nsystematic errors that need to be resolved to enhance its safety and\nsuitability for clinical use.",
        "pdf_link": "https://arxiv.org/pdf/2306.15887v1.pdf"
    },
    {
        "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
        "authors": [
            "Eric Nguyen",
            "Michael Poli",
            "Marjan Faizi",
            "Armin Thomas",
            "Callum Birch-Sykes",
            "Michael Wornow",
            "Aman Patel",
            "Clayton Rabideau",
            "Stefano Massaroli",
            "Yoshua Bengio",
            "Stefano Ermon",
            "Stephen A. Baccus",
            "Chris Ré"
        ],
        "published": "2023-06-27T20:46:34Z",
        "summary": "Genomic (DNA) sequences encode an enormous amount of information for gene\nregulation and protein synthesis. Similar to natural language models,\nresearchers have proposed foundation models in genomics to learn generalizable\nfeatures from unlabeled genome data that can then be fine-tuned for downstream\ntasks such as identifying regulatory elements. Due to the quadratic scaling of\nattention, previous Transformer-based genomic models have used 512 to 4k tokens\nas context (<0.001% of the human genome), significantly limiting the modeling\nof long-range interactions in DNA. In addition, these methods rely on\ntokenizers or fixed k-mers to aggregate meaningful DNA units, losing single\nnucleotide resolution where subtle genetic variations can completely alter\nprotein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a\nlarge language model based on implicit convolutions was shown to match\nattention in quality while allowing longer context lengths and lower time\ncomplexity. Leveraging Hyena's new long-range capabilities, we present\nHyenaDNA, a genomic foundation model pretrained on the human reference genome\nwith context lengths of up to 1 million tokens at the single nucleotide-level -\nan up to 500x increase over previous dense attention-based models. HyenaDNA\nscales sub-quadratically in sequence length (training up to 160x faster than\nTransformer), uses single nucleotide tokens, and has full global context at\neach layer. We explore what longer context enables - including the first use of\nin-context learning in genomics. On fine-tuned benchmarks from the Nucleotide\nTransformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets\nusing a model with orders of magnitude less parameters and pretraining data. On\nthe GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by\n+10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.",
        "pdf_link": "https://arxiv.org/pdf/2306.15794v2.pdf"
    },
    {
        "title": "Evaluating GPT-3.5 and GPT-4 on Grammatical Error Correction for Brazilian Portuguese",
        "authors": [
            "Maria Carolina Penteado",
            "Fábio Perez"
        ],
        "published": "2023-06-27T20:37:54Z",
        "summary": "We investigate the effectiveness of GPT-3.5 and GPT-4, two large language\nmodels, as Grammatical Error Correction (GEC) tools for Brazilian Portuguese\nand compare their performance against Microsoft Word and Google Docs. We\nintroduce a GEC dataset for Brazilian Portuguese with four categories: Grammar,\nSpelling, Internet, and Fast typing. Our results show that while GPT-4 has\nhigher recall than other methods, LLMs tend to have lower precision, leading to\novercorrection. This study demonstrates the potential of LLMs as practical GEC\ntools for Brazilian Portuguese and encourages further exploration of LLMs for\nnon-English languages and other educational settings.",
        "pdf_link": "https://arxiv.org/pdf/2306.15788v2.pdf"
    },
    {
        "title": "Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost",
        "authors": [
            "Parikshit Bansal",
            "Amit Sharma"
        ],
        "published": "2023-06-27T19:29:55Z",
        "summary": "State-of-the-art supervised NLP models achieve high accuracy but are also\nsusceptible to failures on inputs from low-data regimes, such as domains that\nare not represented in training data. As an approximation to collecting\nground-truth labels for the specific domain, we study the use of large language\nmodels (LLMs) for annotating inputs and improving the generalization of NLP\nmodels. Specifically, given a budget for LLM annotations, we present an\nalgorithm for sampling the most informative inputs to annotate and retrain the\nNLP model. We find that popular active learning strategies such as\nuncertainty-based sampling do not work well. Instead, we propose a sampling\nstrategy based on the difference in prediction scores between the base model\nand the finetuned NLP model, utilizing the fact that most NLP models are\nfinetuned from a base model. Experiments with classification (semantic\nsimilarity) and ranking (semantic search) tasks show that our sampling strategy\nleads to significant gains in accuracy for both the training and target\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2306.15766v1.pdf"
    },
    {
        "title": "REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction",
        "authors": [
            "Zeyi Liu",
            "Arpit Bahety",
            "Shuran Song"
        ],
        "published": "2023-06-27T18:03:15Z",
        "summary": "The ability to detect and analyze failed executions automatically is crucial\nfor an explainable and robust robotic system. Recently, Large Language Models\n(LLMs) have demonstrated strong reasoning abilities on textual inputs. To\nleverage the power of LLMs for robot failure explanation, we introduce REFLECT,\na framework which queries LLM for failure reasoning based on a hierarchical\nsummary of robot past experiences generated from multisensory observations. The\nfailure explanation can further guide a language-based planner to correct the\nfailure and complete the task. To systematically evaluate the framework, we\ncreate the RoboFail dataset with a variety of tasks and failure scenarios. We\ndemonstrate that the LLM-based framework is able to generate informative\nfailure explanations that assist successful correction planning.",
        "pdf_link": "https://arxiv.org/pdf/2306.15724v4.pdf"
    },
    {
        "title": "LeanDojo: Theorem Proving with Retrieval-Augmented Language Models",
        "authors": [
            "Kaiyu Yang",
            "Aidan M. Swope",
            "Alex Gu",
            "Rahul Chalamala",
            "Peiyang Song",
            "Shixing Yu",
            "Saad Godil",
            "Ryan Prenger",
            "Anima Anandkumar"
        ],
        "published": "2023-06-27T17:05:32Z",
        "summary": "Large language models (LLMs) have shown promise in proving formal theorems\nusing proof assistants such as Lean. However, existing methods are difficult to\nreproduce or build on, due to private code, data, and large compute\nrequirements. This has created substantial barriers to research on machine\nlearning methods for theorem proving. This paper removes these barriers by\nintroducing LeanDojo: an open-source Lean playground consisting of toolkits,\ndata, models, and benchmarks. LeanDojo extracts data from Lean and enables\ninteraction with the proof environment programmatically. It contains\nfine-grained annotations of premises in proofs, providing valuable data for\npremise selection: a key bottleneck in theorem proving. Using this data, we\ndevelop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented\nwith retrieval for selecting premises from a vast math library. It is\ninexpensive and needs only one GPU week of training. Our retriever leverages\nLeanDojo's program analysis capability to identify accessible premises and hard\nnegative examples, which makes retrieval much more effective. Furthermore, we\nconstruct a new benchmark consisting of 98,734 theorems and proofs extracted\nfrom Lean's math library. It features challenging data split requiring the\nprover to generalize to theorems relying on novel premises that are never used\nin training. We use this benchmark for training and evaluation, and\nexperimental results demonstrate the effectiveness of ReProver over\nnon-retrieval baselines and GPT-4. We thus provide the first set of open-source\nLLM-based theorem provers without any proprietary datasets and release it under\na permissive MIT license to facilitate further research.",
        "pdf_link": "https://arxiv.org/pdf/2306.15626v2.pdf"
    },
    {
        "title": "Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool",
        "authors": [
            "Jingwei Ni",
            "Julia Bingler",
            "Chiara Colesanti-Senni",
            "Mathias Kraus",
            "Glen Gostlow",
            "Tobias Schimanski",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Nicolas Webersinke",
            "Tobias Wekhof",
            "Tingyu Yu",
            "Markus Leippold"
        ],
        "published": "2023-06-27T14:46:47Z",
        "summary": "This paper introduces a novel approach to enhance Large Language Models\n(LLMs) with expert knowledge to automate the analysis of corporate\nsustainability reports by benchmarking them against the Task Force for\nClimate-Related Financial Disclosures (TCFD) recommendations. Corporate\nsustainability reports are crucial in assessing organizations' environmental\nand social risks and impacts. However, analyzing these reports' vast amounts of\ninformation makes human analysis often too costly. As a result, only a few\nentities worldwide have the resources to analyze these reports, which could\nlead to a lack of transparency. While AI-powered tools can automatically\nanalyze the data, they are prone to inaccuracies as they lack domain-specific\nexpertise. This paper introduces a novel approach to enhance LLMs with expert\nknowledge to automate the analysis of corporate sustainability reports. We\nchristen our tool CHATREPORT, and apply it in a first use case to assess\ncorporate climate risk disclosures following the TCFD recommendations.\nCHATREPORT results from collaborating with experts in climate science, finance,\neconomic policy, and computer science, demonstrating how domain experts can be\ninvolved in developing AI tools. We make our prompt templates, generated data,\nand scores available to the public to encourage transparency.",
        "pdf_link": "https://arxiv.org/pdf/2306.15518v2.pdf"
    },
    {
        "title": "Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task",
        "authors": [
            "Sophie Jentzsch",
            "Cigdem Turan"
        ],
        "published": "2023-06-27T08:36:35Z",
        "summary": "Pretrained language models are publicly available and constantly finetuned\nfor various real-life applications. As they become capable of grasping complex\ncontextual information, harmful biases are likely increasingly intertwined with\nthose models. This paper analyses gender bias in BERT models with two main\ncontributions: First, a novel bias measure is introduced, defining biases as\nthe difference in sentiment valuation of female and male sample versions.\nSecond, we comprehensively analyse BERT's biases on the example of a realistic\nIMDB movie classifier. By systematically varying elements of the training\npipeline, we can conclude regarding their impact on the final model bias. Seven\ndifferent public BERT models in nine training conditions, i.e. 63 models in\ntotal, are compared. Almost all conditions yield significant gender biases.\nResults indicate that reflected biases stem from public BERT models rather than\ntask-specific data, emphasising the weight of responsible usage.",
        "pdf_link": "https://arxiv.org/pdf/2306.15298v1.pdf"
    },
    {
        "title": "WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models",
        "authors": [
            "Virginia K. Felkner",
            "Ho-Chun Herbert Chang",
            "Eugene Jang",
            "Jonathan May"
        ],
        "published": "2023-06-26T22:07:33Z",
        "summary": "We present WinoQueer: a benchmark specifically designed to measure whether\nlarge language models (LLMs) encode biases that are harmful to the LGBTQ+\ncommunity. The benchmark is community-sourced, via application of a novel\nmethod that generates a bias benchmark from a community survey. We apply our\nbenchmark to several popular LLMs and find that off-the-shelf models generally\ndo exhibit considerable anti-queer bias. Finally, we show that LLM bias against\na marginalized community can be somewhat mitigated by finetuning on data\nwritten about or by members of that community, and that social media text\nwritten by community members is more effective than news text written about the\ncommunity by non-members. Our method for community-in-the-loop benchmark\ndevelopment provides a blueprint for future researchers to develop\ncommunity-driven, harms-grounded LLM benchmarks for other marginalized\ncommunities.",
        "pdf_link": "https://arxiv.org/pdf/2306.15087v1.pdf"
    },
    {
        "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
        "authors": [
            "John Yang",
            "Akshara Prabhakar",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "published": "2023-06-26T17:59:50Z",
        "summary": "Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create three interactive code environments with Bash, SQL, and\nPython as action spaces, leveraging data from the static NL2Bash, Spider, and\nMBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating\nmultiple state-of-the-art LLMs configured with different prompting strategies\nsuch as ReAct and Plan & Solve. Our results showcase the benefits of\ninteractive code generation and demonstrate that InterCode can serve as a\nchallenging benchmark for advancing code understanding and generation\ncapabilities. InterCode is designed to be easily extensible and can even be\nused to create new tasks such as Capture the Flag, a popular coding puzzle that\nis inherently multi-step and involves multiple programming languages. Project\nsite with code and data: https://intercode-benchmark.github.io",
        "pdf_link": "https://arxiv.org/pdf/2306.14898v3.pdf"
    },
    {
        "title": "Are aligned neural networks adversarially aligned?",
        "authors": [
            "Nicholas Carlini",
            "Milad Nasr",
            "Christopher A. Choquette-Choo",
            "Matthew Jagielski",
            "Irena Gao",
            "Anas Awadalla",
            "Pang Wei Koh",
            "Daphne Ippolito",
            "Katherine Lee",
            "Florian Tramer",
            "Ludwig Schmidt"
        ],
        "published": "2023-06-26T17:18:44Z",
        "summary": "Large language models are now tuned to align with the goals of their\ncreators, namely to be \"helpful and harmless.\" These models should respond\nhelpfully to user questions, but refuse to answer requests that could cause\nharm. However, adversarial users can construct inputs which circumvent attempts\nat alignment. In this work, we study to what extent these models remain\naligned, even when interacting with an adversarial user who constructs\nworst-case inputs (adversarial examples). These inputs are designed to cause\nthe model to emit harmful content that would otherwise be prohibited. We show\nthat existing NLP-based optimization attacks are insufficiently powerful to\nreliably attack aligned text models: even when current NLP-based attacks fail,\nwe can find adversarial inputs with brute force. As a result, the failure of\ncurrent attacks should not be seen as proof that aligned text models remain\naligned under adversarial inputs.\n  However the recent trend in large-scale ML models is multimodal models that\nallow users to provide images that influence the text that is generated. We\nshow these models can be easily attacked, i.e., induced to perform arbitrary\nun-aligned behavior through adversarial perturbation of the input image. We\nconjecture that improved NLP attacks may demonstrate this same level of\nadversarial control over text-only models.",
        "pdf_link": "https://arxiv.org/pdf/2306.15447v1.pdf"
    },
    {
        "title": "Exploring the Robustness of Large Language Models for Solving Programming Problems",
        "authors": [
            "Atsushi Shirafuji",
            "Yutaka Watanobe",
            "Takumi Ito",
            "Makoto Morishita",
            "Yuki Nakamura",
            "Yusuke Oda",
            "Jun Suzuki"
        ],
        "published": "2023-06-26T10:48:50Z",
        "summary": "Using large language models (LLMs) for source code has recently gained\nattention. LLMs, such as Transformer-based models like Codex and ChatGPT, have\nbeen shown to be highly capable of solving a wide range of programming\nproblems. However, the extent to which LLMs understand problem descriptions and\ngenerate programs accordingly or just retrieve source code from the most\nrelevant problem in training data based on superficial cues has not been\ndiscovered yet. To explore this research question, we conduct experiments to\nunderstand the robustness of several popular LLMs, CodeGen and GPT-3.5 series\nmodels, capable of tackling code generation tasks in introductory programming\nproblems. Our experimental results show that CodeGen and Codex are sensitive to\nthe superficial modifications of problem descriptions and significantly impact\ncode generation performance. Furthermore, we observe that Codex relies on\nvariable names, as randomized variables decrease the solved rate significantly.\nHowever, the state-of-the-art (SOTA) models, such as InstructGPT and ChatGPT,\nshow higher robustness to superficial modifications and have an outstanding\ncapability for solving programming problems. This highlights the fact that\nslight modifications to the prompts given to the LLMs can greatly affect code\ngeneration performance, and careful formatting of prompts is essential for\nhigh-quality code generation, while the SOTA models are becoming more robust to\nperturbations.",
        "pdf_link": "https://arxiv.org/pdf/2306.14583v1.pdf"
    },
    {
        "title": "Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference",
        "authors": [
            "Junyan Li",
            "Li Lyna Zhang",
            "Jiahang Xu",
            "Yujing Wang",
            "Shaoguang Yan",
            "Yunqing Xia",
            "Yuqing Yang",
            "Ting Cao",
            "Hao Sun",
            "Weiwei Deng",
            "Qi Zhang",
            "Mao Yang"
        ],
        "published": "2023-06-26T03:06:57Z",
        "summary": "Deploying pre-trained transformer models like BERT on downstream tasks in\nresource-constrained scenarios is challenging due to their high inference cost,\nwhich grows rapidly with input sequence length. In this work, we propose a\nconstraint-aware and ranking-distilled token pruning method ToP, which\nselectively removes unnecessary tokens as input sequence passes through layers,\nallowing the model to improve online inference speed while preserving accuracy.\nToP overcomes the limitation of inaccurate token importance ranking in the\nconventional self-attention mechanism through a ranking-distilled token\ndistillation technique, which distills effective token rankings from the final\nlayer of unpruned models to early layers of pruned models. Then, ToP introduces\na coarse-to-fine pruning approach that automatically selects the optimal subset\nof transformer layers and optimizes token pruning decisions within these layers\nthrough improved $L_0$ regularization. Extensive experiments on GLUE benchmark\nand SQuAD tasks demonstrate that ToP outperforms state-of-the-art token pruning\nand model compression methods with improved accuracy and speedups. ToP reduces\nthe average FLOPs of BERT by 8.1x while achieving competitive accuracy on GLUE,\nand provides a real latency speedup of up to 7.4x on an Intel CPU.",
        "pdf_link": "https://arxiv.org/pdf/2306.14393v1.pdf"
    },
    {
        "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated Adversarial Perturbations",
        "authors": [
            "Yilun Zhao",
            "Chen Zhao",
            "Linyong Nan",
            "Zhenting Qi",
            "Wenlin Zhang",
            "Xiangru Tang",
            "Boyu Mi",
            "Dragomir Radev"
        ],
        "published": "2023-06-25T19:23:21Z",
        "summary": "Despite significant progress having been made in question answering on\ntabular data (Table QA), it's unclear whether, and to what extent existing\nTable QA models are robust to task-specific perturbations, e.g., replacing key\nquestion entities or shuffling table columns. To systematically study the\nrobustness of Table QA models, we propose a benchmark called RobuT, which\nbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and\nincludes human-annotated adversarial perturbations in terms of table header,\ntable content, and question. Our results indicate that both state-of-the-art\nTable QA models and large language models (e.g., GPT-3) with few-shot learning\nfalter in these adversarial sets. We propose to address this problem by using\nlarge language models to generate adversarial examples to enhance training,\nwhich significantly improves the robustness of Table QA models. Our data and\ncode is publicly available at https://github.com/yilunzhao/RobuT.",
        "pdf_link": "https://arxiv.org/pdf/2306.14321v1.pdf"
    },
    {
        "title": "Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning",
        "authors": [
            "Xiao Ma",
            "Swaroop Mishra",
            "Ahmad Beirami",
            "Alex Beutel",
            "Jilin Chen"
        ],
        "published": "2023-06-25T18:40:43Z",
        "summary": "Language models still struggle on moral reasoning, despite their impressive\nperformance in many other tasks. In particular, the Moral Scenarios task in\nMMLU (Multi-task Language Understanding) is among the worst performing tasks\nfor many language models, including GPT-3. In this work, we propose a new\nprompting framework, Thought Experiments, to teach language models to do better\nmoral reasoning using counterfactuals. Experiment results show that our\nframework elicits counterfactual questions and answers from the model, which in\nturn helps improve the accuracy on Moral Scenarios task by 9-16% compared to\nother zero-shot baselines. Interestingly, unlike math reasoning tasks,\nzero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and\neven reduces accuracy by around 4% compared to direct zero-shot. We further\nobserved that with minimal human supervision in the form of 5 few-shot\nexamples, the accuracy of the task can be improved to as much as 80%.",
        "pdf_link": "https://arxiv.org/pdf/2306.14308v1.pdf"
    },
    {
        "title": "Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction",
        "authors": [
            "Feng Chen",
            "Yujian Feng"
        ],
        "published": "2023-06-25T04:33:56Z",
        "summary": "Multimodal Named Entity Recognition (MNER) and Multimodal Relation Extraction\n(MRE) necessitate the fundamental reasoning capacity for intricate linguistic\nand multimodal comprehension. In this study, we explore distilling the\nreasoning ability of large language models (LLMs) into a more compact student\nmodel by generating a \\textit{chain of thought} (CoT) -- a sequence of\nintermediate reasoning steps. Specifically, we commence by exemplifying the\nelicitation of such reasoning ability from LLMs through CoT prompts covering\nmulti-grain (noun, sentence, multimodality) and data-augmentation (style,\nentity, image) dimensions. Subsequently, we present a novel conditional prompt\ndistillation method to assimilate the commonsense reasoning ability from LLMs,\nthereby enhancing the utility of the student model in addressing text-only\ninputs without the requisite addition of image and CoT knowledge. Extensive\nexperiments reveal that our approach attains state-of-the-art accuracy and\nmanifests a plethora of advantages concerning interpretability, data\nefficiency, and cross-domain generalization on MNER and MRE datasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.14122v3.pdf"
    },
    {
        "title": "Language models are weak learners",
        "authors": [
            "Hariharan Manikandan",
            "Yiding Jiang",
            "J Zico Kolter"
        ],
        "published": "2023-06-25T02:39:19Z",
        "summary": "A central notion in practical and theoretical machine learning is that of a\n$\\textit{weak learner}$, classifiers that achieve better-than-random\nperformance (on any given distribution over data), even by a small margin. Such\nweak learners form the practical basis for canonical machine learning methods\nsuch as boosting. In this work, we illustrate that prompt-based large language\nmodels can operate effectively as said weak learners. Specifically, we\nillustrate the use of a large language model (LLM) as a weak learner in a\nboosting algorithm applied to tabular data. We show that by providing (properly\nsampled according to the distribution of interest) text descriptions of tabular\ndata samples, LLMs can produce a summary of the samples that serves as a\ntemplate for classification and achieves the aim of acting as a weak learner on\nthis task. We incorporate these models into a boosting approach, which in some\nsettings can leverage the knowledge within the LLM to outperform traditional\ntree-based boosting. The model outperforms both few-shot learning and\noccasionally even more involved fine-tuning procedures, particularly for tasks\ninvolving small numbers of data points. The results illustrate the potential\nfor prompt-based LLMs to function not just as few-shot learners themselves, but\nas components of larger machine learning pipelines.",
        "pdf_link": "https://arxiv.org/pdf/2306.14101v1.pdf"
    },
    {
        "title": "Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models",
        "authors": [
            "Yinyu Lan",
            "Yanru Wu",
            "Wang Xu",
            "Weiqiang Feng",
            "Youhao Zhang"
        ],
        "published": "2023-06-25T02:24:30Z",
        "summary": "Entity-level fine-grained sentiment analysis in the financial domain is a\ncrucial subtask of sentiment analysis and currently faces numerous challenges.\nThe primary challenge stems from the lack of high-quality and large-scale\nannotated corpora specifically designed for financial text sentiment analysis,\nwhich in turn limits the availability of data necessary for developing\neffective text processing techniques. Recent advancements in large language\nmodels (LLMs) have yielded remarkable performance in natural language\nprocessing tasks, primarily centered around language pattern matching. In this\npaper, we propose a novel and extensive Chinese fine-grained financial\nsentiment analysis dataset, FinChina SA, for enterprise early warning. We\nthoroughly evaluate and experiment with well-known existing open-source LLMs\nusing our dataset. We firmly believe that our dataset will serve as a valuable\nresource to advance the exploration of real-world financial sentiment analysis\ntasks, which should be the focus of future research. The FinChina SA dataset is\npublicly available at https://github.com/YerayL/FinChina-SA",
        "pdf_link": "https://arxiv.org/pdf/2306.14096v5.pdf"
    },
    {
        "title": "Full Automation of Goal-driven LLM Dialog Threads with And-Or Recursors and Refiner Oracles",
        "authors": [
            "Paul Tarau"
        ],
        "published": "2023-06-24T23:33:00Z",
        "summary": "We automate deep step-by step reasoning in an LLM dialog thread by\nrecursively exploring alternatives (OR-nodes) and expanding details (AND-nodes)\nup to a given depth. Starting from a single succinct task-specific initiator we\nsteer the automated dialog thread to stay focussed on the task by synthesizing\na prompt that summarizes the depth-first steps taken so far.\n  Our algorithm is derived from a simple recursive descent implementation of a\nHorn Clause interpreter, except that we accommodate our logic engine to fit the\nnatural language reasoning patterns LLMs have been trained on. Semantic\nsimilarity to ground-truth facts or oracle advice from another LLM instance is\nused to restrict the search space and validate the traces of justification\nsteps returned as answers. At the end, the unique minimal model of a generated\nHorn Clause program collects the results of the reasoning process.\n  As applications, we sketch implementations of consequence predictions, causal\nexplanations, recommendation systems and topic-focussed exploration of\nscientific literature.",
        "pdf_link": "https://arxiv.org/pdf/2306.14077v1.pdf"
    },
    {
        "title": "On the Uses of Large Language Models to Interpret Ambiguous Cyberattack Descriptions",
        "authors": [
            "Reza Fayyazi",
            "Shanchieh Jay Yang"
        ],
        "published": "2023-06-24T21:08:15Z",
        "summary": "The volume, variety, and velocity of change in vulnerabilities and exploits\nhave made incident threat analysis challenging with human expertise and\nexperience along. Tactics, Techniques, and Procedures (TTPs) are to describe\nhow and why attackers exploit vulnerabilities. However, a TTP description\nwritten by one security professional can be interpreted very differently by\nanother, leading to confusion in cybersecurity operations or even business,\npolicy, and legal decisions. Meanwhile, advancements in AI have led to the\nincreasing use of Natural Language Processing (NLP) algorithms to assist the\nvarious tasks in cyber operations. With the rise of Large Language Models\n(LLMs), NLP tasks have significantly improved because of the LLM's semantic\nunderstanding and scalability. This leads us to question how well LLMs can\ninterpret TTPs or general cyberattack descriptions to inform analysts of the\nintended purposes of cyberattacks. We propose to analyze and compare the direct\nuse of LLMs (e.g., GPT-3.5) versus supervised fine-tuning (SFT) of\nsmall-scale-LLMs (e.g., BERT) to study their capabilities in predicting ATT&CK\ntactics. Our results reveal that the small-scale-LLMs with SFT provide a more\nfocused and clearer differentiation between the ATT&CK tactics (if such\ndifferentiation exists). On the other hand, direct use of LLMs offer a broader\ninterpretation of cyberattack techniques. When treating more general cases,\ndespite the power of LLMs, inherent ambiguity exists and limits their\npredictive power. We then summarize the challenges and recommend research\ndirections on LLMs to treat the inherent ambiguity of TTP descriptions used in\nvarious cyber operations.",
        "pdf_link": "https://arxiv.org/pdf/2306.14062v2.pdf"
    },
    {
        "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \"Think\" Step-by-Step",
        "authors": [
            "Liunian Harold Li",
            "Jack Hessel",
            "Youngjae Yu",
            "Xiang Ren",
            "Kai-Wei Chang",
            "Yejin Choi"
        ],
        "published": "2023-06-24T20:15:07Z",
        "summary": "Chain-of-thought prompting (e.g., \"Let's think step-by-step\") primes large\nlanguage models to verbalize rationalization for their predictions. While\nchain-of-thought can lead to dramatic performance gains, benefits appear to\nemerge only for sufficiently large models (beyond 50B parameters). We show that\norders-of-magnitude smaller models (125M -- 1.3B parameters) can still benefit\nfrom chain-of-thought prompting. To achieve this, we introduce Symbolic\nChain-of-Thought Distillation (SCoTD), a method to train a smaller student\nmodel on rationalizations sampled from a significantly larger teacher model.\nExperiments across several commonsense benchmarks show that: 1) SCoTD enhances\nthe performance of the student model in both supervised and few-shot settings,\nand especially for challenge sets; 2) sampling many reasoning chains per\ninstance from the teacher is paramount; and 3) after distillation, student\nchain-of-thoughts are judged by humans as comparable to the teacher, despite\norders of magnitude fewer parameters. We test several hypotheses regarding what\nproperties of chain-of-thought samples are important, e.g., diversity vs.\nteacher likelihood vs. open-endedness. We release our corpus of\nchain-of-thought samples and code.",
        "pdf_link": "https://arxiv.org/pdf/2306.14050v1.pdf"
    },
    {
        "title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
        "authors": [
            "Zhenyu Zhang",
            "Ying Sheng",
            "Tianyi Zhou",
            "Tianlong Chen",
            "Lianmin Zheng",
            "Ruisi Cai",
            "Zhao Song",
            "Yuandong Tian",
            "Christopher Ré",
            "Clark Barrett",
            "Zhangyang Wang",
            "Beidi Chen"
        ],
        "published": "2023-06-24T20:11:14Z",
        "summary": "Large Language Models (LLMs), despite their recent impressive\naccomplishments, are notably cost-prohibitive to deploy, particularly for\napplications involving long-content generation, such as dialogue systems and\nstory writing. Often, a large amount of transient state information, referred\nto as the KV cache, is stored in GPU memory in addition to model parameters,\nscaling linearly with the sequence length and batch size. In this paper, we\nintroduce a novel approach for implementing the KV cache which significantly\nreduces its memory footprint. Our approach is based on the noteworthy\nobservation that a small portion of tokens contributes most of the value when\ncomputing attention scores. We call these tokens Heavy Hitters (H$_2$). Through\na comprehensive investigation, we find that (i) the emergence of H$_2$ is\nnatural and strongly correlates with the frequent co-occurrence of tokens in\nthe text, and (ii) removing them results in significant performance\ndegradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O),\na KV cache eviction policy that dynamically retains a balance of recent and\nH$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular\nproblem and prove (under mild assumptions) a theoretical guarantee for our\nnovel eviction algorithm which could help guide future work. We validate the\naccuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of\ntasks. Our implementation of H$_2$O with 20% heavy hitters improves the\nthroughput over three leading inference systems DeepSpeed Zero-Inference,\nHugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and\n3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the\nlatency by up to 1.9$\\times$. The code is available at\nhttps://github.com/FMInference/H2O.",
        "pdf_link": "https://arxiv.org/pdf/2306.14048v3.pdf"
    },
    {
        "title": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks",
        "authors": [
            "Tanmay Chavan",
            "Omkar Gokhale",
            "Aditya Kane",
            "Shantanu Patankar",
            "Raviraj Joshi"
        ],
        "published": "2023-06-24T18:17:38Z",
        "summary": "The research on code-mixed data is limited due to the unavailability of\ndedicated code-mixed datasets and pre-trained language models. In this work, we\nfocus on the low-resource Indian language Marathi which lacks any prior work in\ncode-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English\n(Mr-En) corpus with 10 million social media sentences for pretraining. We also\nrelease L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer models\npre-trained on MeCorpus. Furthermore, for benchmarking, we present three\nsupervised datasets MeHate, MeSent, and MeLID for downstream tasks like\ncode-mixed Mr-En hate speech detection, sentiment analysis, and language\nidentification respectively. These evaluation datasets individually consist of\nmanually annotated \\url{~}12,000 Marathi-English code-mixed tweets. Ablations\nshow that the models trained on this novel corpus significantly outperform the\nexisting state-of-the-art BERT models. This is the first work that presents\nartifacts for code-mixed Marathi research. All datasets and models are publicly\nreleased at https://github.com/l3cube-pune/MarathiNLP .",
        "pdf_link": "https://arxiv.org/pdf/2306.14030v2.pdf"
    },
    {
        "title": "LLM-assisted Generation of Hardware Assertions",
        "authors": [
            "Rahul Kande",
            "Hammond Pearce",
            "Benjamin Tan",
            "Brendan Dolan-Gavitt",
            "Shailja Thakur",
            "Ramesh Karri",
            "Jeyavijayan Rajendran"
        ],
        "published": "2023-06-24T17:44:36Z",
        "summary": "The security of computer systems typically relies on a hardware root of\ntrust. As vulnerabilities in hardware can have severe implications on a system,\nthere is a need for techniques to support security verification activities.\nAssertion-based verification is a popular verification technique that involves\ncapturing design intent in a set of assertions that can be used in formal\nverification or testing-based checking. However, writing security-centric\nassertions is a challenging task. In this work, we investigate the use of\nemerging large language models (LLMs) for code generation in hardware assertion\ngeneration for security, where primarily natural language prompts, such as\nthose one would see as code comments in assertion files, are used to produce\nSystemVerilog assertions. We focus our attention on a popular LLM and\ncharacterize its ability to write assertions out of the box, given varying\nlevels of detail in the prompt. We design an evaluation framework that\ngenerates a variety of prompts, and we create a benchmark suite comprising\nreal-world hardware designs and corresponding golden reference assertions that\nwe want to generate with the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.14027v1.pdf"
    },
    {
        "title": "Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise?",
        "authors": [
            "Jaromir Savelka",
            "Kevin D. Ashley",
            "Morgan A Gray",
            "Hannes Westermann",
            "Huihui Xu"
        ],
        "published": "2023-06-24T08:48:24Z",
        "summary": "We evaluated the capability of generative pre-trained transformers~(GPT-4) in\nanalysis of textual data in tasks that require highly specialized domain\nexpertise. Specifically, we focused on the task of analyzing court opinions to\ninterpret legal concepts. We found that GPT-4, prompted with annotation\nguidelines, performs on par with well-trained law student annotators. We\nobserved that, with a relatively minor decrease in performance, GPT-4 can\nperform batch predictions leading to significant cost reductions. However,\nemploying chain-of-thought prompting did not lead to noticeably improved\nperformance on this task. Further, we demonstrated how to analyze GPT-4's\npredictions to identify and mitigate deficiencies in annotation guidelines, and\nsubsequently improve the performance of the model. Finally, we observed that\nthe model is quite brittle, as small formatting related changes in the prompt\nhad a high impact on the predictions. These findings can be leveraged by\nresearchers and practitioners who engage in semantic/pragmatic annotations of\ntexts in the context of the tasks requiring highly specialized domain\nexpertise.",
        "pdf_link": "https://arxiv.org/pdf/2306.13906v1.pdf"
    },
    {
        "title": "IERL: Interpretable Ensemble Representation Learning -- Combining CrowdSourced Knowledge and Distributed Semantic Representations",
        "authors": [
            "Yuxin Zi",
            "Kaushik Roy",
            "Vignesh Narayanan",
            "Manas Gaur",
            "Amit Sheth"
        ],
        "published": "2023-06-24T05:02:34Z",
        "summary": "Large Language Models (LLMs) encode meanings of words in the form of\ndistributed semantics. Distributed semantics capture common statistical\npatterns among language tokens (words, phrases, and sentences) from large\namounts of data. LLMs perform exceedingly well across General Language\nUnderstanding Evaluation (GLUE) tasks designed to test a model's understanding\nof the meanings of the input tokens. However, recent studies have shown that\nLLMs tend to generate unintended, inconsistent, or wrong texts as outputs when\nprocessing inputs that were seen rarely during training, or inputs that are\nassociated with diverse contexts (e.g., well-known hallucination phenomenon in\nlanguage generation tasks). Crowdsourced and expert-curated knowledge graphs\nsuch as ConceptNet are designed to capture the meaning of words from a compact\nset of well-defined contexts. Thus LLMs may benefit from leveraging such\nknowledge contexts to reduce inconsistencies in outputs. We propose a novel\nensemble learning method, Interpretable Ensemble Representation Learning\n(IERL), that systematically combines LLM and crowdsourced knowledge\nrepresentations of input tokens. IERL has the distinct advantage of being\ninterpretable by design (when was the LLM context used vs. when was the\nknowledge context used?) over state-of-the-art (SOTA) methods, allowing\nscrutiny of the inputs in conjunction with the parameters of the model,\nfacilitating the analysis of models' inconsistent or irrelevant outputs.\nAlthough IERL is agnostic to the choice of LLM and crowdsourced knowledge, we\ndemonstrate our approach using BERT and ConceptNet. We report improved or\ncompetitive results with IERL across GLUE tasks over current SOTA methods and\nsignificantly enhanced model interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2306.13865v1.pdf"
    },
    {
        "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
        "authors": [
            "Alycia Lee",
            "Brando Miranda",
            "Sudharsan Sundar",
            "Sanmi Koyejo"
        ],
        "published": "2023-06-24T02:25:56Z",
        "summary": "Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.13840v2.pdf"
    },
    {
        "title": "Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models",
        "authors": [
            "Adel Elmahdy",
            "Ahmed Salem"
        ],
        "published": "2023-06-23T21:25:38Z",
        "summary": "Natural language processing (NLP) models have become increasingly popular in\nreal-world applications, such as text classification. However, they are\nvulnerable to privacy attacks, including data reconstruction attacks that aim\nto extract the data used to train the model. Most previous studies on data\nreconstruction attacks have focused on LLM, while classification models were\nassumed to be more secure. In this work, we propose a new targeted data\nreconstruction attack called the Mix And Match attack, which takes advantage of\nthe fact that most classification models are based on LLM. The Mix And Match\nattack uses the base model of the target model to generate candidate tokens and\nthen prunes them using the classification head. We extensively demonstrate the\neffectiveness of the attack using both random and organic canaries. This work\nhighlights the importance of considering the privacy risks associated with data\nreconstruction attacks in classification models and offers insights into\npossible leakages.",
        "pdf_link": "https://arxiv.org/pdf/2306.13789v1.pdf"
    },
    {
        "title": "LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding",
        "authors": [
            "Robert Chew",
            "John Bollenbacher",
            "Michael Wenger",
            "Jessica Speer",
            "Annice Kim"
        ],
        "published": "2023-06-23T20:57:32Z",
        "summary": "Deductive coding is a widely used qualitative research method for determining\nthe prevalence of themes across documents. While useful, deductive coding is\noften burdensome and time consuming since it requires researchers to read,\ninterpret, and reliably categorize a large body of unstructured text documents.\nLarge language models (LLMs), like ChatGPT, are a class of quickly evolving AI\ntools that can perform a range of natural language processing and reasoning\ntasks. In this study, we explore the use of LLMs to reduce the time it takes\nfor deductive coding while retaining the flexibility of a traditional content\nanalysis. We outline the proposed approach, called LLM-assisted content\nanalysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a\npublicly available deductive coding data set. Additionally, we conduct an\nempirical benchmark using LACA on 4 publicly available data sets to assess the\nbroader question of how well GPT-3.5 performs across a range of deductive\ncoding tasks. Overall, we find that GPT-3.5 can often perform deductive coding\nat levels of agreement comparable to human coders. Additionally, we demonstrate\nthat LACA can help refine prompts for deductive coding, identify codes for\nwhich an LLM is randomly guessing, and help assess when to use LLMs vs. human\ncoders for deductive coding. We conclude with several implications for future\npractice of deductive coding and related research methods.",
        "pdf_link": "https://arxiv.org/pdf/2306.14924v1.pdf"
    },
    {
        "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
        "authors": [
            "Neel Jain",
            "Khalid Saifullah",
            "Yuxin Wen",
            "John Kirchenbauer",
            "Manli Shu",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "published": "2023-06-23T17:59:09Z",
        "summary": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data.",
        "pdf_link": "https://arxiv.org/pdf/2306.13651v2.pdf"
    },
    {
        "title": "Comparing the Efficacy of Fine-Tuning and Meta-Learning for Few-Shot Policy Imitation",
        "authors": [
            "Massimiliano Patacchiola",
            "Mingfei Sun",
            "Katja Hofmann",
            "Richard E. Turner"
        ],
        "published": "2023-06-23T15:29:15Z",
        "summary": "In this paper we explore few-shot imitation learning for control problems,\nwhich involves learning to imitate a target policy by accessing a limited set\nof offline rollouts. This setting has been relatively under-explored despite\nits relevance to robotics and control applications. State-of-the-art methods\ndeveloped to tackle few-shot imitation rely on meta-learning, which is\nexpensive to train as it requires access to a distribution over tasks (rollouts\nfrom many target policies and variations of the base environment). Given this\nlimitation we investigate an alternative approach, fine-tuning, a family of\nmethods that pretrain on a single dataset and then fine-tune on unseen\ndomain-specific data. Recent work has shown that fine-tuners outperform\nmeta-learners in few-shot image classification tasks, especially when the data\nis out-of-domain. Here we evaluate to what extent this is true for control\nproblems, proposing a simple yet effective baseline which relies on two stages:\n(i) training a base policy online via reinforcement learning (e.g. Soft\nActor-Critic) on a single base environment, (ii) fine-tuning the base policy\nvia behavioral cloning on a few offline rollouts of the target policy. Despite\nits simplicity this baseline is competitive with meta-learning methods on a\nvariety of conditions and is able to imitate target policies trained on unseen\nvariations of the original environment. Importantly, the proposed approach is\npractical and easy to implement, as it does not need any complex meta-training\nprotocol. As a further contribution, we release an open source dataset called\niMuJoCo (iMitation MuJoCo) consisting of 154 variants of popular OpenAI-Gym\nMuJoCo environments with associated pretrained target policies and rollouts,\nwhich can be used by the community to study few-shot imitation learning and\noffline reinforcement learning.",
        "pdf_link": "https://arxiv.org/pdf/2306.13554v1.pdf"
    },
    {
        "title": "Knowledge-Infused Self Attention Transformers",
        "authors": [
            "Kaushik Roy",
            "Yuxin Zi",
            "Vignesh Narayanan",
            "Manas Gaur",
            "Amit Sheth"
        ],
        "published": "2023-06-23T13:55:01Z",
        "summary": "Transformer-based language models have achieved impressive success in various\nnatural language processing tasks due to their ability to capture complex\ndependencies and contextual information using self-attention mechanisms.\nHowever, they are not without limitations. These limitations include\nhallucinations, where they produce incorrect outputs with high confidence, and\nalignment issues, where they generate unhelpful and unsafe outputs for human\nusers. These limitations stem from the absence of implicit and missing context\nin the data alone. To address this, researchers have explored augmenting these\nmodels with external knowledge from knowledge graphs to provide the necessary\nadditional context. However, the ad-hoc nature of existing methods makes it\ndifficult to properly analyze the effects of knowledge infusion on the many\nmoving parts or components of a transformer. This paper introduces a systematic\nmethod for infusing knowledge into different components of a transformer-based\nmodel. A modular framework is proposed to identify specific components within\nthe transformer architecture, such as the self-attention mechanism, encoder\nlayers, or the input embedding layer, where knowledge infusion can be applied.\nAdditionally, extensive experiments are conducted on the General Language\nUnderstanding Evaluation (GLUE) benchmark tasks, and the findings are reported.\nThis systematic approach aims to facilitate more principled approaches to\nincorporating knowledge into language model architectures.",
        "pdf_link": "https://arxiv.org/pdf/2306.13501v1.pdf"
    },
    {
        "title": "Efficient Online Processing with Deep Neural Networks",
        "authors": [
            "Lukas Hedegaard"
        ],
        "published": "2023-06-23T12:29:44Z",
        "summary": "The capabilities and adoption of deep neural networks (DNNs) grow at an\nexhilarating pace: Vision models accurately classify human actions in videos\nand identify cancerous tissue in medical scans as precisely than human experts;\nlarge language models answer wide-ranging questions, generate code, and write\nprose, becoming the topic of everyday dinner-table conversations. Even though\ntheir uses are exhilarating, the continually increasing model sizes and\ncomputational complexities have a dark side. The economic cost and negative\nenvironmental externalities of training and serving models is in evident\ndisharmony with financial viability and climate action goals.\n  Instead of pursuing yet another increase in predictive performance, this\ndissertation is dedicated to the improvement of neural network efficiency.\nSpecifically, a core contribution addresses the efficiency aspects during\nonline inference. Here, the concept of Continual Inference Networks (CINs) is\nproposed and explored across four publications. CINs extend prior\nstate-of-the-art methods developed for offline processing of spatio-temporal\ndata and reuse their pre-trained weights, improving their online processing\nefficiency by an order of magnitude. These advances are attained through a\nbottom-up computational reorganization and judicious architectural\nmodifications. The benefit to online inference is demonstrated by reformulating\nseveral widely used network architectures into CINs, including 3D CNNs,\nST-GCNs, and Transformer Encoders. An orthogonal contribution tackles the\nconcurrent adaptation and computational acceleration of a large source model\ninto multiple lightweight derived models. Drawing on fusible adapter networks\nand structured pruning, Structured Pruning Adapters achieve superior predictive\naccuracy under aggressive pruning using significantly fewer learned weights\ncompared to fine-tuning with pruning.",
        "pdf_link": "https://arxiv.org/pdf/2306.13474v1.pdf"
    },
    {
        "title": "Product Information Extraction using ChatGPT",
        "authors": [
            "Alexander Brinkmann",
            "Roee Shraga",
            "Reng Chiz Der",
            "Christian Bizer"
        ],
        "published": "2023-06-23T09:30:01Z",
        "summary": "Structured product data in the form of attribute/value pairs is the\nfoundation of many e-commerce applications such as faceted product search,\nproduct comparison, and product recommendation. Product offers often only\ncontain textual descriptions of the product attributes in the form of titles or\nfree text. Hence, extracting attribute/value pairs from textual product\ndescriptions is an essential enabler for e-commerce applications. In order to\nexcel, state-of-the-art product information extraction methods require large\nquantities of task-specific training data. The methods also struggle with\ngeneralizing to out-of-distribution attributes and attribute values that were\nnot a part of the training data. Due to being pre-trained on huge amounts of\ntext as well as due to emergent effects resulting from the model size, Large\nLanguage Models like ChatGPT have the potential to address both of these\nshortcomings. This paper explores the potential of ChatGPT for extracting\nattribute/value pairs from product descriptions. We experiment with different\nzero-shot and few-shot prompt designs. Our results show that ChatGPT achieves a\nperformance similar to a pre-trained language model but requires much smaller\namounts of training data and computation for fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2306.14921v1.pdf"
    },
    {
        "title": "ToolQA: A Dataset for LLM Question Answering with External Tools",
        "authors": [
            "Yuchen Zhuang",
            "Yue Yu",
            "Kuan Wang",
            "Haotian Sun",
            "Chao Zhang"
        ],
        "published": "2023-06-23T05:43:28Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nvarious NLP tasks, but they still suffer from challenges such as hallucination\nand weak numerical reasoning. To overcome these challenges, external tools can\nbe used to enhance LLMs' question-answering abilities. However, current\nevaluation methods do not distinguish between questions that can be answered\nusing LLMs' internal knowledge and those that require external information\nthrough tool use. To address this issue, we introduce a new dataset called\nToolQA, which is designed to faithfully evaluate LLMs' ability to use external\ntools for question answering. Our development of ToolQA involved a scalable,\nautomated process for dataset curation, along with 13 specialized tools\ndesigned for interaction with external knowledge in order to answer questions.\nImportantly, we strive to minimize the overlap between our benchmark data and\nLLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use\nreasoning abilities. We conducted an in-depth diagnosis of existing tool-use\nLLMs to highlight their strengths, weaknesses, and potential improvements. Our\nfindings set a new benchmark for evaluating LLMs and suggest new directions for\nfuture advancements. Our data and code are freely available to the broader\nscientific community on GitHub.",
        "pdf_link": "https://arxiv.org/pdf/2306.13304v1.pdf"
    },
    {
        "title": "Correcting discount-factor mismatch in on-policy policy gradient methods",
        "authors": [
            "Fengdi Che",
            "Gautham Vasan",
            "A. Rupam Mahmood"
        ],
        "published": "2023-06-23T04:10:58Z",
        "summary": "The policy gradient theorem gives a convenient form of the policy gradient in\nterms of three factors: an action value, a gradient of the action likelihood,\nand a state distribution involving discounting called the \\emph{discounted\nstationary distribution}. But commonly used on-policy methods based on the\npolicy gradient theorem ignores the discount factor in the state distribution,\nwhich is technically incorrect and may even cause degenerate learning behavior\nin some environments. An existing solution corrects this discrepancy by using\n$\\gamma^t$ as a factor in the gradient estimate. However, this solution is not\nwidely adopted and does not work well in tasks where the later states are\nsimilar to earlier states. We introduce a novel distribution correction to\naccount for the discounted stationary distribution that can be plugged into\nmany existing gradient estimators. Our correction circumvents the performance\ndegradation associated with the $\\gamma^t$ correction with a lower variance.\nImportantly, compared to the uncorrected estimators, our algorithm provides\nimproved state emphasis to evade suboptimal policies in certain environments\nand consistently matches or exceeds the original performance on several OpenAI\ngym and DeepMind suite benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2306.13284v1.pdf"
    },
    {
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "authors": [
            "Xiangyu Qi",
            "Kaixuan Huang",
            "Ashwinee Panda",
            "Peter Henderson",
            "Mengdi Wang",
            "Prateek Mittal"
        ],
        "published": "2023-06-22T22:13:03Z",
        "summary": "Recently, there has been a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such as\nFlamingo and GPT-4. This paper sheds light on the security and safety\nimplications of this trend. First, we underscore that the continuous and\nhigh-dimensional nature of the visual input makes it a weak link against\nadversarial attacks, representing an expanded attack surface of\nvision-integrated LLMs. Second, we highlight that the versatility of LLMs also\npresents visual attackers with a wider array of achievable adversarial\nobjectives, extending the implications of security failures beyond mere\nmisclassification. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety guardrail of\naligned LLMs with integrated vision. Intriguingly, we discover that a single\nvisual adversarial example can universally jailbreak an aligned LLM, compelling\nit to heed a wide range of harmful instructions that it otherwise would not)\nand generate harmful content that transcends the narrow scope of a `few-shot'\nderogatory corpus initially employed to optimize the adversarial example. Our\nstudy underscores the escalating adversarial risks associated with the pursuit\nof multimodality. Our findings also connect the long-studied adversarial\nvulnerabilities of neural networks to the nascent field of AI alignment. The\npresented attack suggests a fundamental adversarial challenge for AI alignment,\nespecially in light of the emerging trend toward multimodality in frontier\nfoundation models.",
        "pdf_link": "https://arxiv.org/pdf/2306.13213v2.pdf"
    },
    {
        "title": "Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation",
        "authors": [
            "Yuetian Chen",
            "Bowen Shi",
            "Mei Si"
        ],
        "published": "2023-06-22T20:38:52Z",
        "summary": "Artificial intelligence has made significant progress in natural language\nprocessing, with models like GPT-3 demonstrating impressive capabilities.\nHowever, these models still have limitations when it comes to complex tasks\nthat require an understanding of the user, such as mastering human comedy\nwriting strategies. This paper explores humor generation using GPT-3 by\nmodeling human comedy writing theory and leveraging step-by-step thinking\ninstructions. In addition, we explore the role of cognitive distance in\ncreating humor.",
        "pdf_link": "https://arxiv.org/pdf/2306.13195v1.pdf"
    },
    {
        "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
        "authors": [
            "Miao Xiong",
            "Zhiyuan Hu",
            "Xinyang Lu",
            "Yifei Li",
            "Jie Fu",
            "Junxian He",
            "Bryan Hooi"
        ],
        "published": "2023-06-22T17:31:44Z",
        "summary": "Empowering large language models to accurately express confidence in their\nanswers is essential for trustworthy decision-making. Previous confidence\nelicitation methods, which primarily rely on white-box access to internal model\ninformation or model fine-tuning, have become less suitable for LLMs,\nespecially closed-source commercial APIs. This leads to a growing need to\nexplore the untapped area of black-box approaches for LLM uncertainty\nestimation. To better break down the problem, we define a systematic framework\nwith three components: prompting strategies for eliciting verbalized\nconfidence, sampling methods for generating multiple responses, and aggregation\ntechniques for computing consistency. We then benchmark these methods on two\nkey tasks-confidence calibration and failure prediction-across five types of\ndatasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs\nincluding GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights:\n1) LLMs, when verbalizing their confidence, tend to be overconfident,\npotentially imitating human patterns of expressing confidence. 2) As model\ncapability scales up, both calibration and failure prediction performance\nimprove. 3) Employing our proposed strategies, such as human-inspired prompts,\nconsistency among multiple responses, and better aggregation strategies can\nhelp mitigate this overconfidence from various perspectives. 4) Comparisons\nwith white-box methods indicate that while white-box methods perform better,\nthe gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements,\nnone of these techniques consistently outperform others, and all investigated\nmethods struggle in challenging tasks, such as those requiring professional\nknowledge, indicating significant scope for improvement. We believe this study\ncan serve as a strong baseline and provide insights for eliciting confidence in\nblack-box LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.13063v2.pdf"
    },
    {
        "title": "Towards Explainable Evaluation Metrics for Machine Translation",
        "authors": [
            "Christoph Leiter",
            "Piyawat Lertvittayakumjorn",
            "Marina Fomicheva",
            "Wei Zhao",
            "Yang Gao",
            "Steffen Eger"
        ],
        "published": "2023-06-22T17:07:57Z",
        "summary": "Unlike classical lexical overlap metrics such as BLEU, most current\nevaluation metrics for machine translation (for example, COMET or BERTScore)\nare based on black-box large language models. They often achieve strong\ncorrelations with human judgments, but recent research indicates that the\nlower-quality classical metrics remain dominant, one of the potential reasons\nbeing that their decision processes are more transparent. To foster more\nwidespread acceptance of novel high-quality metrics, explainability thus\nbecomes crucial. In this concept paper, we identify key properties as well as\nkey goals of explainable machine translation metrics and provide a\ncomprehensive synthesis of recent techniques, relating them to our established\ngoals and properties. In this context, we also discuss the latest\nstate-of-the-art approaches to explainable metrics based on generative models\nsuch as ChatGPT and GPT4. Finally, we contribute a vision of next-generation\napproaches, including natural language explanations. We hope that our work can\nhelp catalyze and guide future research on explainable evaluation metrics and,\nmediately, also contribute to better and more transparent machine translation\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2306.13041v1.pdf"
    },
    {
        "title": "Apolitical Intelligence? Auditing Delphi's responses on controversial political issues in the US",
        "authors": [
            "Jonathan H. Rystrøm"
        ],
        "published": "2023-06-22T15:56:50Z",
        "summary": "As generative language models are deployed in ever-wider contexts, concerns\nabout their political values have come to the forefront with critique from all\nparts of the political spectrum that the models are biased and lack neutrality.\nHowever, the question of what neutrality is and whether it is desirable remains\nunderexplored. In this paper, I examine neutrality through an audit of Delphi\n[arXiv:2110.07574], a large language model designed for crowdsourced ethics. I\nanalyse how Delphi responds to politically controversial questions compared to\ndifferent US political subgroups. I find that Delphi is poorly calibrated with\nrespect to confidence and exhibits a significant political skew. Based on these\nresults, I examine the question of neutrality from a data-feminist lens, in\nterms of how notions of neutrality shift power and further marginalise unheard\nvoices. These findings can hopefully contribute to a more reflexive debate\nabout the normative questions of alignment and what role we want generative\nmodels to play in society.",
        "pdf_link": "https://arxiv.org/pdf/2306.13000v1.pdf"
    },
    {
        "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
        "authors": [
            "Yelysei Bondarenko",
            "Markus Nagel",
            "Tijmen Blankevoort"
        ],
        "published": "2023-06-22T14:39:04Z",
        "summary": "Transformer models have been widely adopted in various domains over the last\nyears, and especially large language models have advanced the field of AI\nsignificantly. Due to their size, the capability of these networks has\nincreased tremendously, but this has come at the cost of a significant increase\nin necessary compute. Quantization is one of the most effective ways to reduce\nthe computational time and memory consumption of neural networks. Many studies\nhave shown, however, that modern transformer models tend to learn strong\noutliers in their activations, making them difficult to quantize. To retain\nacceptable performance, the existence of these outliers requires activations to\nbe in higher bitwidth or the use of different numeric formats, extra\nfine-tuning, or other workarounds. We show that strong outliers are related to\nvery specific behavior of attention heads that try to learn a \"no-op\" or just a\npartial update of the residual. To achieve the exact zeros needed in the\nattention matrix for a no-update, the input to the softmax is pushed to be\nlarger and larger during training, causing outliers in other parts of the\nnetwork. Based on these observations, we propose two simple (independent)\nmodifications to the attention mechanism - clipped softmax and gated attention.\nWe empirically show that models pre-trained using our methods learn\nsignificantly smaller outliers while maintaining and sometimes even improving\nthe floating-point task performance. This enables us to quantize transformers\nto full INT8 quantization of the activations without any additional effort. We\ndemonstrate the effectiveness of our methods on both language models (BERT,\nOPT) and vision transformers.",
        "pdf_link": "https://arxiv.org/pdf/2306.12929v2.pdf"
    },
    {
        "title": "Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT",
        "authors": [
            "Stephen Yang"
        ],
        "published": "2023-06-22T13:21:20Z",
        "summary": "Analysis of innovation has been fundamentally limited by conventional\napproaches to broad, structural variables. This paper pushes the boundaries,\ntaking an LLM approach to patent analysis with the groundbreaking ChatGPT\ntechnology. OpenAI's state-of-the-art textual embedding accesses complex\ninformation about the quality and impact of each invention to power deep\nlearning predictive models. The nuanced embedding drives a 24% incremental\nimprovement in R-squared predicting patent value and clearly isolates the worst\nand best applications. These models enable a revision of the contemporary\nKogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median\ndeviation of 1.5 times, accounting for potential institutional predictions.\nFurthermore, the market fails to incorporate timely information about\napplications; a long-short portfolio based on predicted acceptance rates\nachieves significant abnormal returns of 3.3% annually. The models provide an\nopportunity to revolutionize startup and small-firm corporate policy vis-a-vis\npatenting.",
        "pdf_link": "https://arxiv.org/pdf/2307.01202v1.pdf"
    },
    {
        "title": "Generative Multimodal Entity Linking",
        "authors": [
            "Senbao Shi",
            "Zhenran Xu",
            "Baotian Hu",
            "Min Zhang"
        ],
        "published": "2023-06-22T07:57:19Z",
        "summary": "Multimodal Entity Linking (MEL) is the task of mapping mentions with\nmultimodal contexts to the referent entities from a knowledge base. Existing\nMEL methods mainly focus on designing complex multimodal interaction mechanisms\nand require fine-tuning all model parameters, which can be prohibitively costly\nand difficult to scale in the era of Large Language Models (LLMs). In this\nwork, we propose GEMEL, a Generative Multimodal Entity Linking framework based\non LLMs, which directly generates target entity names. We keep the vision and\nlanguage model frozen and only train a feature mapper to enable cross-modality\ninteractions. To adapt LLMs to the MEL task, we leverage the in-context\nlearning capability of LLMs by retrieving multimodal instances as\ndemonstrations. Extensive experiments show that, with only ~0.3% of the model\nparameters fine-tuned, GEMEL achieves state-of-the-art results on two\nwell-established MEL datasets (7.7% accuracy gains on WikiDiverse and 8.8%\naccuracy gains on WikiMEL). The performance gain stems from mitigating the\npopularity bias of LLM predictions and disambiguating less common entities\neffectively. Further analysis verifies the generality and scalability of GEMEL.\nOur framework is compatible with any off-the-shelf language model, paving the\nway towards an efficient and general solution for utilizing LLMs in the MEL\ntask. Our code is available at https://github.com/HITsz-TMG/GEMEL.",
        "pdf_link": "https://arxiv.org/pdf/2306.12725v4.pdf"
    },
    {
        "title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models",
        "authors": [
            "Boyu Zhang",
            "Hongyang Yang",
            "Xiao-Yang Liu"
        ],
        "published": "2023-06-22T03:56:38Z",
        "summary": "Sentiment analysis is a vital tool for uncovering insights from financial\narticles, news, and social media, shaping our understanding of market\nmovements. Despite the impressive capabilities of large language models (LLMs)\nin financial natural language processing (NLP), they still struggle with\naccurately interpreting numerical values and grasping financial context,\nlimiting their effectiveness in predicting financial sentiment. In this paper,\nwe introduce a simple yet effective instruction tuning approach to address\nthese issues. By transforming a small portion of supervised financial sentiment\nanalysis data into instruction data and fine-tuning a general-purpose LLM with\nthis method, we achieve remarkable advancements in financial sentiment\nanalysis. In the experiment, our approach outperforms state-of-the-art\nsupervised sentiment analysis models, as well as widely used LLMs like ChatGPT\nand LLaMAs, particularly in scenarios where numerical understanding and\ncontextual comprehension are vital.",
        "pdf_link": "https://arxiv.org/pdf/2306.12659v1.pdf"
    },
    {
        "title": "Identifying and Extracting Rare Disease Phenotypes with Large Language Models",
        "authors": [
            "Cathy Shyr",
            "Yan Hu",
            "Paul A. Harris",
            "Hua Xu"
        ],
        "published": "2023-06-22T03:52:12Z",
        "summary": "Rare diseases (RDs) are collectively common and affect 300 million people\nworldwide. Accurate phenotyping is critical for informing diagnosis and\ntreatment, but RD phenotypes are often embedded in unstructured text and\ntime-consuming to extract manually. While natural language processing (NLP)\nmodels can perform named entity recognition (NER) to automate extraction, a\nmajor bottleneck is the development of a large, annotated corpus for model\ntraining. Recently, prompt learning emerged as an NLP paradigm that can lead to\nmore generalizable results without any (zero-shot) or few labeled samples\n(few-shot). Despite growing interest in ChatGPT, a revolutionary large language\nmodel capable of following complex human prompts and generating high-quality\nresponses, none have studied its NER performance for RDs in the zero- and\nfew-shot settings. To this end, we engineered novel prompts aimed at extracting\nRD phenotypes and, to the best of our knowledge, are the first the establish a\nbenchmark for evaluating ChatGPT's performance in these settings. We compared\nits performance to the traditional fine-tuning approach and conducted an\nin-depth error analysis. Overall, fine-tuning BioClinicalBERT resulted in\nhigher performance (F1 of 0.689) than ChatGPT (F1 of 0.472 and 0.591 in the\nzero- and few-shot settings, respectively). Despite this, ChatGPT achieved\nsimilar or higher accuracy for certain entities (i.e., rare diseases and signs)\nin the one-shot setting (F1 of 0.776 and 0.725). This suggests that with\nappropriate prompt engineering, ChatGPT has the potential to match or\noutperform fine-tuned language models for certain entity types with just one\nlabeled sample. While the proliferation of large language models may provide\nopportunities for supporting RD diagnosis and treatment, researchers and\nclinicians should critically evaluate model outputs and be well-informed of\ntheir limitations.",
        "pdf_link": "https://arxiv.org/pdf/2306.12656v1.pdf"
    },
    {
        "title": "FLAG: Finding Line Anomalies (in code) with Generative AI",
        "authors": [
            "Baleegh Ahmad",
            "Benjamin Tan",
            "Ramesh Karri",
            "Hammond Pearce"
        ],
        "published": "2023-06-22T03:04:56Z",
        "summary": "Code contains security and functional bugs. The process of identifying and\nlocalizing them is difficult and relies on human labor. In this work, we\npresent a novel approach (FLAG) to assist human debuggers. FLAG is based on the\nlexical capabilities of generative AI, specifically, Large Language Models\n(LLMs). Here, we input a code file then extract and regenerate each line within\nthat file for self-comparison. By comparing the original code with an\nLLM-generated alternative, we can flag notable differences as anomalies for\nfurther inspection, with features such as distance from comments and LLM\nconfidence also aiding this classification. This reduces the inspection search\nspace for the designer. Unlike other automated approaches in this area, FLAG is\nlanguage-agnostic, can work on incomplete (and even non-compiling) code and\nrequires no creation of security properties, functional tests or definition of\nrules. In this work, we explore the features that help LLMs in this\nclassification and evaluate the performance of FLAG on known bugs. We use 121\nbenchmarks across C, Python and Verilog; with each benchmark containing a known\nsecurity or functional weakness. We conduct the experiments using two state of\nthe art LLMs in OpenAI's code-davinci-002 and gpt-3.5-turbo, but our approach\nmay be used by other models. FLAG can identify 101 of the defects and helps\nreduce the search space to 12-17% of source code.",
        "pdf_link": "https://arxiv.org/pdf/2306.12643v1.pdf"
    },
    {
        "title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews",
        "authors": [
            "Mike D'Arcy",
            "Alexis Ross",
            "Erin Bransom",
            "Bailey Kuehl",
            "Jonathan Bragg",
            "Tom Hope",
            "Doug Downey"
        ],
        "published": "2023-06-21T22:00:03Z",
        "summary": "Revising scientific papers based on peer feedback is a challenging task that\nrequires not only deep scientific knowledge and reasoning, but also the ability\nto recognize the implicit requests in high-level feedback and to choose the\nbest of many possible ways to update the manuscript in response. We introduce\nthis task for large language models and release ARIES, a dataset of review\ncomments and their corresponding paper edits, to enable training and evaluating\nmodels. We study two versions of the task: comment-edit alignment and edit\ngeneration, and evaluate several baselines, including GPT-4. We find that\nmodels struggle even to identify the edits that correspond to a comment,\nespecially in cases where the comment is phrased in an indirect way or where\nthe edit addresses the spirit of a comment but not the precise request. When\ntasked with generating edits, GPT-4 often succeeds in addressing comments on a\nsurface level, but it rigidly follows the wording of the feedback rather than\nthe underlying intent, and includes fewer technical details than human-written\nedits. We hope that our formalization, dataset, and analysis will form a\nfoundation for future work in this area.",
        "pdf_link": "https://arxiv.org/pdf/2306.12587v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models with NeuBAROCO: Syllogistic Reasoning Ability and Human-like Biases",
        "authors": [
            "Risako Ando",
            "Takanobu Morishita",
            "Hirohiko Abe",
            "Koji Mineshima",
            "Mitsuhiro Okada"
        ],
        "published": "2023-06-21T21:04:11Z",
        "summary": "This paper investigates whether current large language models exhibit biases\nin logical reasoning, similar to humans. Specifically, we focus on syllogistic\nreasoning, a well-studied form of inference in the cognitive science of human\ndeduction. To facilitate our analysis, we introduce a dataset called NeuBAROCO,\noriginally designed for psychological experiments that assess human logical\nabilities in syllogistic reasoning. The dataset consists of syllogistic\ninferences in both English and Japanese. We examine three types of biases\nobserved in human syllogistic reasoning: belief biases, conversion errors, and\natmosphere effects. Our findings demonstrate that current large language models\nstruggle more with problems involving these three types of biases.",
        "pdf_link": "https://arxiv.org/pdf/2306.12567v1.pdf"
    },
    {
        "title": "FlakyFix: Using Large Language Models for Predicting Flaky Test Fix Categories and Test Code Repair",
        "authors": [
            "Sakina Fatima",
            "Hadi Hemmati",
            "Lionel Briand"
        ],
        "published": "2023-06-21T19:34:16Z",
        "summary": "Flaky tests are problematic because they non-deterministically pass or fail\nfor the same software version under test, causing confusion and wasting\ndevelopment effort. While machine learning models have been used to predict\nflakiness and its root causes, there is much less work on providing support to\nfix the problem. To address this gap, in this paper, we focus on predicting the\ntype of fix that is required to remove flakiness and then repair the test code\non that basis. We do this for a subset of flaky test cases where the root cause\nof flakiness is in the test case itself and not in the production code. Our key\nidea is to guide the repair process with additional knowledge about the test's\nflakiness in the form of its predicted fix category. Thus, we first propose a\nframework that automatically generates labeled datasets for 13 fix categories\nand trains models to predict the fix category of a flaky test by analyzing the\ntest code only. Our experimental results using code models and few-shot\nlearning show that we can correctly predict most of the fix categories. To show\nthe usefulness of such fix category labels for automatically repairing\nflakiness, in addition to informing testers, we augment a Large Language Model\n(LLM) like GPT with such extra knowledge to ask the LLM for repair suggestions.\nThe results show that our suggested fix category labels significantly enhance\nthe capability of GPT 3.5 Turbo, in generating fixes for flaky tests.",
        "pdf_link": "https://arxiv.org/pdf/2307.00012v2.pdf"
    },
    {
        "title": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
        "authors": [
            "Alessandro Sordoni",
            "Xingdi Yuan",
            "Marc-Alexandre Côté",
            "Matheus Pereira",
            "Adam Trischler",
            "Ziang Xiao",
            "Arian Hosseini",
            "Friederike Niedtner",
            "Nicolas Le Roux"
        ],
        "published": "2023-06-21T18:45:56Z",
        "summary": "Large language models (LLMs) can be seen as atomic units of computation\nmapping sequences to a distribution over sequences. Thus, they can be seen as\nstochastic language layers in a language network, where the learnable\nparameters are the natural language prompts at each layer. By stacking two such\nlayers and feeding the output of one layer to the next, we obtain a Deep\nLanguage Network (DLN). We first show how to effectively perform prompt\noptimization for a 1-Layer language network (DLN-1). Then, we present an\nextension that applies to 2-layer DLNs (DLN-2), where two prompts must be\nlearned. The key idea is to consider the output of the first layer as a latent\nvariable, which requires inference, and prompts to be learned as the parameters\nof the generative distribution. We first test the effectiveness of DLN-1 in\nmultiple reasoning and natural language understanding tasks. Then, we show that\nDLN-2 can reach higher performance than a single layer, showing promise that we\nmight reach comparable performance to GPT-4, even when each LLM in the network\nis smaller and less powerful.",
        "pdf_link": "https://arxiv.org/pdf/2306.12509v2.pdf"
    },
    {
        "title": "Understanding Social Reasoning in Language Models with Language Models",
        "authors": [
            "Kanishk Gandhi",
            "Jan-Philipp Fränken",
            "Tobias Gerstenberg",
            "Noah D. Goodman"
        ],
        "published": "2023-06-21T16:42:15Z",
        "summary": "As Large Language Models (LLMs) become increasingly integrated into our\neveryday lives, understanding their ability to comprehend human mental states\nbecomes critical for ensuring effective interactions. However, despite the\nrecent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of\nLLMs, the degree to which these models can align with human ToM remains a\nnuanced topic of exploration. This is primarily due to two distinct challenges:\n(1) the presence of inconsistent results from previous evaluations, and (2)\nconcerns surrounding the validity of existing evaluation methodologies. To\naddress these challenges, we present a novel framework for procedurally\ngenerating evaluations with LLMs by populating causal templates. Using our\nframework, we create a new social reasoning benchmark (BigToM) for LLMs which\nconsists of 25 controls and 5,000 model-written evaluations. We find that human\nparticipants rate the quality of our benchmark higher than previous\ncrowd-sourced evaluations and comparable to expert-written evaluations. Using\nBigToM, we evaluate the social reasoning capabilities of a variety of LLMs and\ncompare model performances with human performance. Our results suggest that\nGPT4 has ToM capabilities that mirror human inference patterns, though less\nreliable, while other LLMs struggle.",
        "pdf_link": "https://arxiv.org/pdf/2306.15448v2.pdf"
    },
    {
        "title": "Testing of Detection Tools for AI-Generated Text",
        "authors": [
            "Debora Weber-Wulff",
            "Alla Anohina-Naumeca",
            "Sonja Bjelobaba",
            "Tomáš Foltýnek",
            "Jean Guerrero-Dib",
            "Olumide Popoola",
            "Petr Šigut",
            "Lorna Waddington"
        ],
        "published": "2023-06-21T16:29:44Z",
        "summary": "Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.",
        "pdf_link": "https://arxiv.org/pdf/2306.15666v2.pdf"
    },
    {
        "title": "GPT-Based Models Meet Simulation: How to Efficiently Use Large-Scale Pre-Trained Language Models Across Simulation Tasks",
        "authors": [
            "Philippe J. Giabbanelli"
        ],
        "published": "2023-06-21T15:42:36Z",
        "summary": "The disruptive technology provided by large-scale pre-trained language models\n(LLMs) such as ChatGPT or GPT-4 has received significant attention in several\napplication domains, often with an emphasis on high-level opportunities and\nconcerns. This paper is the first examination regarding the use of LLMs for\nscientific simulations. We focus on four modeling and simulation tasks, each\ntime assessing the expected benefits and limitations of LLMs while providing\npractical guidance for modelers regarding the steps involved. The first task is\ndevoted to explaining the structure of a conceptual model to promote the\nengagement of participants in the modeling process. The second task focuses on\nsummarizing simulation outputs, so that model users can identify a preferred\nscenario. The third task seeks to broaden accessibility to simulation platforms\nby conveying the insights of simulation visualizations via text. Finally, the\nlast task evokes the possibility of explaining simulation errors and providing\nguidance to resolve them.",
        "pdf_link": "https://arxiv.org/pdf/2306.13679v1.pdf"
    },
    {
        "title": "Solving and Generating NPR Sunday Puzzles with Large Language Models",
        "authors": [
            "Jingmiao Zhao",
            "Carolyn Jane Anderson"
        ],
        "published": "2023-06-21T13:23:48Z",
        "summary": "We explore the ability of large language models to solve and generate puzzles\nfrom the NPR Sunday Puzzle game show using PUZZLEQA, a dataset comprising 15\nyears of on-air puzzles. We evaluate four large language models using PUZZLEQA,\nin both multiple choice and free response formats, and explore two prompt\nengineering techniques to improve free response performance: chain-of-thought\nreasoning and prompt summarization. We find that state-of-the-art large\nlanguage models can solve many PUZZLEQA puzzles: the best model, GPT-3.5,\nachieves 50.2% loose accuracy. However, in our few-shot puzzle generation\nexperiment, we find no evidence that models can generate puzzles: GPT-3.5\ngenerates puzzles with answers that do not conform to the generated rules.\nPuzzle generation remains a challenging task for future work.",
        "pdf_link": "https://arxiv.org/pdf/2306.12255v1.pdf"
    },
    {
        "title": "Limits for Learning with Language Models",
        "authors": [
            "Nicholas Asher",
            "Swarnadeep Bhar",
            "Akshay Chaturvedi",
            "Julie Hunter",
            "Soumya Paul"
        ],
        "published": "2023-06-21T12:11:31Z",
        "summary": "With the advent of large language models (LLMs), the trend in NLP has been to\ntrain LLMs on vast amounts of data to solve diverse language understanding and\ngeneration tasks. The list of LLM successes is long and varied. Nevertheless,\nseveral recent papers provide empirical evidence that LLMs fail to capture\nimportant aspects of linguistic meaning. Focusing on universal quantification,\nwe provide a theoretical foundation for these empirical findings by proving\nthat LLMs cannot learn certain fundamental semantic properties including\nsemantic entailment and consistency as they are defined in formal semantics.\nMore generally, we show that LLMs are unable to learn concepts beyond the first\nlevel of the Borel Hierarchy, which imposes severe limits on the ability of\nLMs, both large and small, to capture many aspects of linguistic meaning. This\nmeans that LLMs will continue to operate without formal guarantees on tasks\nthat require entailments and deep linguistic understanding.",
        "pdf_link": "https://arxiv.org/pdf/2306.12213v1.pdf"
    },
    {
        "title": "Opening the Black Box: Analyzing Attention Weights and Hidden States in Pre-trained Language Models for Non-language Tasks",
        "authors": [
            "Mohamad Ballout",
            "Ulf Krumnack",
            "Gunther Heidemann",
            "Kai-Uwe Kühnberger"
        ],
        "published": "2023-06-21T11:48:07Z",
        "summary": "Investigating deep learning language models has always been a significant\nresearch area due to the ``black box\" nature of most advanced models. With the\nrecent advancements in pre-trained language models based on transformers and\ntheir increasing integration into daily life, addressing this issue has become\nmore pressing. In order to achieve an explainable AI model, it is essential to\ncomprehend the procedural steps involved and compare them with human thought\nprocesses. Thus, in this paper, we use simple, well-understood non-language\ntasks to explore these models' inner workings. Specifically, we apply a\npre-trained language model to constrained arithmetic problems with hierarchical\nstructure, to analyze their attention weight scores and hidden states. The\ninvestigation reveals promising results, with the model addressing hierarchical\nproblems in a moderately structured manner, similar to human problem-solving\nstrategies. Additionally, by inspecting the attention weights layer by layer,\nwe uncover an unconventional finding that layer 10, rather than the model's\nfinal layer, is the optimal layer to unfreeze for the least parameter-intensive\napproach to fine-tune the model. We support these findings with entropy\nanalysis and token embeddings similarity analysis. The attention analysis\nallows us to hypothesize that the model can generalize to longer sequences in\nListOps dataset, a conclusion later confirmed through testing on sequences\nlonger than those in the training set. Lastly, by utilizing a straightforward\ntask in which the model predicts the winner of a Tic Tac Toe game, we identify\nlimitations in attention analysis, particularly its inability to capture 2D\npatterns.",
        "pdf_link": "https://arxiv.org/pdf/2306.12198v1.pdf"
    },
    {
        "title": "OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue",
        "authors": [
            "Weihao Gao",
            "Zhuo Deng",
            "Zhiyuan Niu",
            "Fuju Rong",
            "Chucheng Chen",
            "Zheng Gong",
            "Wenze Zhang",
            "Daimin Xiao",
            "Fang Li",
            "Zhenjie Cao",
            "Zhaoyi Ma",
            "Wenbin Wei",
            "Lan Ma"
        ],
        "published": "2023-06-21T11:09:48Z",
        "summary": "Large multimodal language models (LMMs) have achieved significant success in\ngeneral domains. However, due to the significant differences between medical\nimages and text and general web content, the performance of LMMs in medical\nscenarios is limited. In ophthalmology, clinical diagnosis relies on multiple\nmodalities of medical images, but unfortunately, multimodal ophthalmic large\nlanguage models have not been explored to date. In this paper, we study and\nconstruct an ophthalmic large multimodal model. Firstly, we use fundus images\nas an entry point to build a disease assessment and diagnosis pipeline to\nachieve common ophthalmic disease diagnosis and lesion segmentation. Then, we\nestablish a new ophthalmic multimodal instruction-following and dialogue\nfine-tuning dataset based on disease-related knowledge data and publicly\navailable real-world medical dialogue. We introduce visual ability into the\nlarge language model to complete the ophthalmic large language and vision\nassistant (OphGLM). Our experimental results demonstrate that the OphGLM model\nperforms exceptionally well, and it has the potential to revolutionize clinical\napplications in ophthalmology. The dataset, code, and models will be made\npublicly available at https://github.com/ML-AILab/OphGLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.12174v2.pdf"
    },
    {
        "title": "Interactive Molecular Discovery with Natural Language",
        "authors": [
            "Zheni Zeng",
            "Bangchen Yin",
            "Shipeng Wang",
            "Jiarui Liu",
            "Cheng Yang",
            "Haishen Yao",
            "Xingzhi Sun",
            "Maosong Sun",
            "Guotong Xie",
            "Zhiyuan Liu"
        ],
        "published": "2023-06-21T02:05:48Z",
        "summary": "Natural language is expected to be a key medium for various human-machine\ninteractions in the era of large language models. When it comes to the\nbiochemistry field, a series of tasks around molecules (e.g., property\nprediction, molecule mining, etc.) are of great significance while having a\nhigh technical threshold. Bridging the molecule expressions in natural language\nand chemical language can not only hugely improve the interpretability and\nreduce the operation difficulty of these tasks, but also fuse the chemical\nknowledge scattered in complementary materials for a deeper comprehension of\nmolecules. Based on these benefits, we propose the conversational molecular\ndesign, a novel task adopting natural language for describing and editing\ntarget molecules. To better accomplish this task, we design ChatMol, a\nknowledgeable and versatile generative pre-trained model, enhanced by injecting\nexperimental property information, molecular spatial knowledge, and the\nassociations between natural and chemical languages into it. Several typical\nsolutions including large language models (e.g., ChatGPT) are evaluated,\nproving the challenge of conversational molecular design and the effectiveness\nof our knowledge enhancement method. Case observations and analysis are\nconducted to provide directions for further exploration of natural-language\ninteraction in molecular discovery.",
        "pdf_link": "https://arxiv.org/pdf/2306.11976v1.pdf"
    },
    {
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "authors": [
            "Christopher T. Small",
            "Ivan Vendrov",
            "Esin Durmus",
            "Hadjar Homaei",
            "Elizabeth Barry",
            "Julien Cornebise",
            "Ted Suzman",
            "Deep Ganguli",
            "Colin Megill"
        ],
        "published": "2023-06-20T22:52:51Z",
        "summary": "Polis is a platform that leverages machine intelligence to scale up\ndeliberative processes. In this paper, we explore the opportunities and risks\nassociated with applying Large Language Models (LLMs) towards challenges with\nfacilitating, moderating and summarizing the results of Polis engagements. In\nparticular, we demonstrate with pilot experiments using Anthropic's Claude that\nLLMs can indeed augment human intelligence to help more efficiently run Polis\nconversations. In particular, we find that summarization capabilities enable\ncategorically new methods with immense promise to empower the public in\ncollective meaning-making exercises. And notably, LLM context limitations have\na significant impact on insight and quality of these results.\n  However, these opportunities come with risks. We discuss some of these risks,\nas well as principles and techniques for characterizing and mitigating them,\nand the implications for other deliberative or political systems that may\nemploy LLMs. Finally, we conclude with several open future research directions\nfor augmenting tools like Polis with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.11932v1.pdf"
    },
    {
        "title": "Open-Domain Text Evaluation via Meta Distribution Modeling",
        "authors": [
            "Sidi Lu",
            "Asli Celikyilmaz",
            "Tianlu Wang",
            "Nanyun Peng"
        ],
        "published": "2023-06-20T20:37:54Z",
        "summary": "Recent advances in open-domain text generation models powered by large\npre-trained language models (LLMs) have achieved remarkable performance.\nHowever, evaluating and controlling these models for desired attributes remains\na challenge, as traditional reference-based metrics such as BLEU, ROUGE, and\nMETEOR are insufficient for open-ended generation tasks. Similarly, while\ntrainable discriminator-based evaluation metrics show promise, obtaining\nhigh-quality training data is a non-trivial task. In this paper, we introduce a\nnovel approach to evaluate open-domain generation - the Meta-Distribution\nMethods (MDM). Drawing on the correlation between the rising parameter counts\nand the improving performance of LLMs, MDM creates a mapping from the contrast\nof two probabilistic distributions -- one known to be superior to the other --\nto quality measures, which can be viewed as a distribution of distributions\ni.e. Meta-Distribution. We investigate MDM for open-domain text generation\nevaluation under two paradigms: 1) \\emph{Generative} MDM, which leverages the\nMeta-Distribution Methods to generate in-domain negative samples for training\ndiscriminator-based metrics; 2) \\emph{Discriminative} MDM, which directly uses\ndistribution discrepancies between two language models for evaluation. Our\nexperiments on multi-turn dialogue and factuality in abstractive summarization\ndemonstrate that MDMs correlate better with human judgment than existing\nautomatic evaluation metrics on both tasks, highlighting the strong performance\nand generalizability of such methods.",
        "pdf_link": "https://arxiv.org/pdf/2306.11879v1.pdf"
    },
    {
        "title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
        "authors": [
            "Boxin Wang",
            "Weixin Chen",
            "Hengzhi Pei",
            "Chulin Xie",
            "Mintong Kang",
            "Chenhui Zhang",
            "Chejian Xu",
            "Zidi Xiong",
            "Ritik Dutta",
            "Rylan Schaeffer",
            "Sang T. Truong",
            "Simran Arora",
            "Mantas Mazeika",
            "Dan Hendrycks",
            "Zinan Lin",
            "Yu Cheng",
            "Sanmi Koyejo",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2023-06-20T17:24:23Z",
        "summary": "Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in their capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications such as healthcare and finance -- where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives -- including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially because GPT-4\nfollows (misleading) instructions more precisely. Our work illustrates a\ncomprehensive trustworthiness evaluation of GPT models and sheds light on the\ntrustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/ ; our dataset can be previewed at\nhttps://huggingface.co/datasets/AI-Secure/DecodingTrust ; a concise version of\nthis work is at https://openreview.net/pdf?id=kaHpo8OZw2 .",
        "pdf_link": "https://arxiv.org/pdf/2306.11698v5.pdf"
    },
    {
        "title": "A Simple and Effective Pruning Approach for Large Language Models",
        "authors": [
            "Mingjie Sun",
            "Zhuang Liu",
            "Anna Bair",
            "J. Zico Kolter"
        ],
        "published": "2023-06-20T17:18:20Z",
        "summary": "As their size increases, Large Languages Models (LLMs) are natural candidates\nfor network pruning methods: approaches that drop a subset of network weights\nwhile striving to preserve performance. Existing methods, however, require\neither retraining, which is rarely affordable for billion-scale LLMs, or\nsolving a weight reconstruction problem reliant on second-order information,\nwhich may also be computationally expensive. In this paper, we introduce a\nnovel, straightforward yet effective pruning method, termed Wanda (Pruning by\nWeights and activations), designed to induce sparsity in pretrained LLMs.\nMotivated by the recent observation of emergent large magnitude features in\nLLMs, our approach prunes weights with the smallest magnitudes multiplied by\nthe corresponding input activations, on a per-output basis. Notably, Wanda\nrequires no retraining or weight update, and the pruned LLM can be used as is.\nWe conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2\nacross various language benchmarks. Wanda significantly outperforms the\nestablished baseline of magnitude pruning and performs competitively against\nrecent method involving intensive weight update. Code is available at\nhttps://github.com/locuslab/wanda.",
        "pdf_link": "https://arxiv.org/pdf/2306.11695v2.pdf"
    },
    {
        "title": "Towards Environmentally Equitable AI via Geographical Load Balancing",
        "authors": [
            "Pengfei Li",
            "Jianyi Yang",
            "Adam Wierman",
            "Shaolei Ren"
        ],
        "published": "2023-06-20T17:13:33Z",
        "summary": "Fueled by the soaring popularity of large language and foundation models, the\naccelerated growth of artificial intelligence (AI) models' enormous\nenvironmental footprint has come under increased scrutiny. While many\napproaches have been proposed to make AI more energy-efficient and\nenvironmentally friendly, environmental inequity -- the fact that AI's\nenvironmental footprint can be disproportionately higher in certain regions\nthan in others -- has emerged, raising social-ecological justice concerns. This\npaper takes a first step toward addressing AI's environmental inequity by\nbalancing its regional negative environmental impact. Concretely, we focus on\nthe carbon and water footprints of AI model inference and propose equity-aware\ngeographical load balancing (GLB) to explicitly address AI's environmental\nimpacts on the most disadvantaged regions. We run trace-based simulations by\nconsidering a set of 10 geographically-distributed data centers that serve\ninference requests for a large language AI model. The results demonstrate that\nexisting GLB approaches may amplify environmental inequity while our proposed\nequity-aware GLB can significantly reduce the regional disparity in terms of\ncarbon and water footprints.",
        "pdf_link": "https://arxiv.org/pdf/2307.05494v1.pdf"
    },
    {
        "title": "Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion",
        "authors": [
            "Simone Bianco",
            "Luigi Celona",
            "Marco Donzella",
            "Paolo Napoletano"
        ],
        "published": "2023-06-20T15:13:02Z",
        "summary": "State-of-The-Art (SoTA) image captioning models often rely on the Microsoft\nCOCO (MS-COCO) dataset for training. This dataset contains annotations provided\nby human annotators, who typically produce captions averaging around ten\ntokens. However, this constraint presents a challenge in effectively capturing\ncomplex scenes and conveying detailed information. Furthermore, captioning\nmodels tend to exhibit bias towards the ``average'' caption, which captures\nonly the more general aspects. What would happen if we were able to\nautomatically generate longer captions, thereby making them more detailed?\nWould these captions, evaluated by humans, be more or less representative of\nthe image content compared to the original MS-COCO captions? In this paper, we\npresent a novel approach to address previous challenges by showcasing how\ncaptions generated from different SoTA models can be effectively fused,\nresulting in richer captions. Our proposed method leverages existing models\nfrom the literature, eliminating the need for additional training. Instead, it\nutilizes an image-text based metric to rank the captions generated by SoTA\nmodels for a given image. Subsequently, the top two captions are fused using a\nLarge Language Model (LLM). Experimental results demonstrate the effectiveness\nof our approach, as the captions generated by our model exhibit higher\nconsistency with human judgment when evaluated on the MS-COCO test set. By\ncombining the strengths of various SoTA models, our method enhances the quality\nand appeal of image captions, bridging the gap between automated systems and\nthe rich, informative nature of human-generated descriptions. This advance\nopens up new possibilities for generating captions that are more suitable for\nthe training of both vision-language and captioning models.",
        "pdf_link": "https://arxiv.org/pdf/2306.11593v1.pdf"
    },
    {
        "title": "Hallucination is the last thing you need",
        "authors": [
            "Shawn Curran",
            "Sam Lansley",
            "Oliver Bethell"
        ],
        "published": "2023-06-20T13:14:15Z",
        "summary": "The legal profession necessitates a multidimensional approach that involves\nsynthesizing an in-depth comprehension of a legal issue with insightful\ncommentary based on personal experience, combined with a comprehensive\nunderstanding of pertinent legislation, regulation, and case law, in order to\ndeliver an informed legal solution. The present offering with generative AI\npresents major obstacles in replicating this, as current models struggle to\nintegrate and navigate such a complex interplay of understanding, experience,\nand fact-checking procedures. It is noteworthy that where generative AI outputs\nunderstanding and experience, which reflect the aggregate of various subjective\nviews on similar topics, this often deflects the model's attention from the\ncrucial legal facts, thereby resulting in hallucination. Hence, this paper\ndelves into the feasibility of three independent LLMs, each focused on\nunderstanding, experience, and facts, synthesising as one single ensemble model\nto effectively counteract the current challenges posed by the existing\nmonolithic generative AI models. We introduce an idea of mutli-length\ntokenisation to protect key information assets like common law judgements, and\nfinally we interrogate the most advanced publicly available models for legal\nhallucination, with some interesting results.",
        "pdf_link": "https://arxiv.org/pdf/2306.11520v1.pdf"
    },
    {
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models",
        "authors": [
            "Yue Huang",
            "Qihui Zhang",
            "Philip S. Y",
            "Lichao Sun"
        ],
        "published": "2023-06-20T12:53:39Z",
        "summary": "Large Language Models (LLMs) such as ChatGPT, have gained significant\nattention due to their impressive natural language processing capabilities. It\nis crucial to prioritize human-centered principles when utilizing these models.\nSafeguarding the ethical and moral compliance of LLMs is of utmost importance.\nHowever, individual ethical issues have not been well studied on the latest\nLLMs. Therefore, this study aims to address these gaps by introducing a new\nbenchmark -- TrustGPT. TrustGPT provides a comprehensive evaluation of LLMs in\nthree crucial areas: toxicity, bias, and value-alignment. Initially, TrustGPT\nexamines toxicity in language models by employing toxic prompt templates\nderived from social norms. It then quantifies the extent of bias in models by\nmeasuring quantifiable toxicity values across different groups. Lastly,\nTrustGPT assesses the value of conversation generation models from both active\nvalue-alignment and passive value-alignment tasks. Through the implementation\nof TrustGPT, this research aims to enhance our understanding of the performance\nof conversation generation models and promote the development of language\nmodels that are more ethical and socially responsible.",
        "pdf_link": "https://arxiv.org/pdf/2306.11507v1.pdf"
    },
    {
        "title": "Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling",
        "authors": [
            "Linyao Yang",
            "Hongyang Chen",
            "Zhao Li",
            "Xiao Ding",
            "Xindong Wu"
        ],
        "published": "2023-06-20T12:21:06Z",
        "summary": "Recently, ChatGPT, a representative large language model (LLM), has gained\nconsiderable attention due to its powerful emergent abilities. Some researchers\nsuggest that LLMs could potentially replace structured knowledge bases like\nknowledge graphs (KGs) and function as parameterized knowledge bases. However,\nwhile LLMs are proficient at learning probabilistic language patterns based on\nlarge corpus and engaging in conversations with humans, they, like previous\nsmaller pre-trained language models (PLMs), still have difficulty in recalling\nfacts while generating knowledge-grounded contents. To overcome these\nlimitations, researchers have proposed enhancing data-driven PLMs with\nknowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus\nimproving their performance to generate texts requiring factual knowledge and\nproviding more informed responses to user queries. This paper reviews the\nstudies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced\npre-trained language models (KGPLMs) as well as their applications. Inspired by\nexisting studies on KGPLM, this paper proposes to enhance LLMs with KGs by\ndeveloping knowledge graph-enhanced large language models (KGLLMs). KGLLM\nprovides a solution to enhance LLMs' factual reasoning ability, opening up new\navenues for LLM research.",
        "pdf_link": "https://arxiv.org/pdf/2306.11489v2.pdf"
    },
    {
        "title": "Blackbird language matrices (BLM), a new task for rule-like generalization in neural networks: Motivations and Formal Specifications",
        "authors": [
            "Paola Merlo"
        ],
        "published": "2023-06-20T10:45:56Z",
        "summary": "We motivate and formally define a new task for fine-tuning rule-like\ngeneralization in large language models. It is conjectured that the\nshortcomings of current LLMs are due to a lack of ability to generalize. It has\nbeen argued that, instead, humans are better at generalization because they\nhave a tendency at extracting rules from complex data. We try to recreate this\ntendency to rule-based generalization. When exposed to tests of analytic\nintelligence, for example, the visual RAVEN IQ test, human problem-solvers\nidentify the relevant objects in the picture and their relevant attributes and\nreason based on rules applied to these objects and attributes. Based on the\ninduced rules, they are able to provide a solution to the test. We propose a\ntask that translates this IQ task into language. In this paper, we provide the\nformal specification for the task and the generative process of its datasets.",
        "pdf_link": "https://arxiv.org/pdf/2306.11444v1.pdf"
    },
    {
        "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts",
        "authors": [
            "Xuan-Phi Nguyen",
            "Sharifah Mahani Aljunied",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2023-06-20T08:27:47Z",
        "summary": "Large language models (LLMs) are known to effectively perform tasks by simply\nobserving few exemplars. However, in low-resource languages, obtaining such\nhand-picked exemplars can still be challenging, where unsupervised techniques\nmay be necessary. Moreover, competent generative capabilities of LLMs are\nobserved only in high-resource languages, while their performances among\nunder-represented languages fall behind due to pre-training data imbalance. To\nelicit LLMs' ability onto low-resource languages without any supervised data,\nwe propose to assemble synthetic exemplars from a diverse set of high-resource\nlanguages to prompt the LLMs to translate from any language into English. These\nprompts are then used to create intra-lingual exemplars to perform tasks in the\ntarget languages. Our unsupervised prompting method performs on par with\nsupervised few-shot learning in LLMs of different sizes for translations\nbetween English and 13 Indic and 21 African low-resource languages. We also\nshow that fine-tuning a 7B model on data generated from our method helps it\nperform competitively with a 175B model. In non-English translation tasks, our\nmethod even outperforms supervised prompting by up to 3 chrF++ in many\nlow-resource languages. When evaluated on zero-shot multilingual summarization,\nour method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is\nalso favored by GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2306.11372v1.pdf"
    },
    {
        "title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis",
        "authors": [
            "Zhiling Zheng",
            "Oufan Zhang",
            "Christian Borgs",
            "Jennifer T. Chayes",
            "Omar M. Yaghi"
        ],
        "published": "2023-06-20T05:20:29Z",
        "summary": "We use prompt engineering to guide ChatGPT in the automation of text mining\nof metal-organic frameworks (MOFs) synthesis conditions from diverse formats\nand styles of the scientific literature. This effectively mitigates ChatGPT's\ntendency to hallucinate information -- an issue that previously made the use of\nLarge Language Models (LLMs) in scientific fields challenging. Our approach\ninvolves the development of a workflow implementing three different processes\nfor text mining, programmed by ChatGPT itself. All of them enable parsing,\nsearching, filtering, classification, summarization, and data unification with\ndifferent tradeoffs between labor, speed, and accuracy. We deploy this system\nto extract 26,257 distinct synthesis parameters pertaining to approximately 800\nMOFs sourced from peer-reviewed research articles. This process incorporates\nour ChemPrompt Engineering strategy to instruct ChatGPT in text mining,\nresulting in impressive precision, recall, and F1 scores of 90-99%.\nFurthermore, with the dataset built by text mining, we constructed a\nmachine-learning model with over 86% accuracy in predicting MOF experimental\ncrystallization outcomes and preliminarily identifying important factors in MOF\ncrystallization. We also developed a reliable data-grounded MOF chatbot to\nanswer questions on chemical reactions and synthesis procedures. Given that the\nprocess of using ChatGPT reliably mines and tabulates diverse MOF synthesis\ninformation in a unified format, while using only narrative language requiring\nno coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be\nvery useful across various other chemistry sub-disciplines.",
        "pdf_link": "https://arxiv.org/pdf/2306.11296v2.pdf"
    },
    {
        "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
        "authors": [
            "Jiuding Sun",
            "Chantal Shaib",
            "Byron C. Wallace"
        ],
        "published": "2023-06-20T03:48:51Z",
        "summary": "Instruction fine-tuning has recently emerged as a promising approach for\nimproving the zero-shot capabilities of Large Language Models (LLMs) on new\ntasks. This technique has shown particular strength in improving the\nperformance of modestly sized LLMs, sometimes inducing performance competitive\nwith much larger model variants. In this paper we ask two questions: (1) How\nsensitive are instruction-tuned models to the particular phrasings of\ninstructions, and, (2) How can we make them more robust to such natural\nlanguage variation? To answer the former, we collect a set of 319 instructions\nmanually written by NLP practitioners for over 80 unique tasks included in\nwidely used benchmarks, and we evaluate the variance and average performance of\nthese instructions as compared to instruction phrasings observed during\ninstruction fine-tuning. We find that using novel (unobserved) but appropriate\ninstruction phrasings consistently degrades model performance, sometimes\nsubstantially so. Further, such natural instructions yield a wide variance in\ndownstream performance, despite their semantic equivalence. Put another way,\ninstruction-tuned models are not especially robust to instruction re-phrasings.\nWe propose a simple method to mitigate this issue by introducing ``soft\nprompt'' embedding parameters and optimizing these to maximize the similarity\nbetween representations of semantically equivalent instructions. We show that\nthis method consistently improves the robustness of instruction-tuned models.",
        "pdf_link": "https://arxiv.org/pdf/2306.11270v2.pdf"
    },
    {
        "title": "Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset",
        "authors": [
            "Saeid Naeini",
            "Raeid Saqur",
            "Mozhgan Saeidi",
            "John Giorgi",
            "Babak Taati"
        ],
        "published": "2023-06-19T21:14:57Z",
        "summary": "The quest for human imitative AI has been an enduring topic in AI research\nsince its inception. The technical evolution and emerging capabilities of the\nlatest cohort of large language models (LLMs) have reinvigorated the subject\nbeyond academia to the cultural zeitgeist. While recent NLP evaluation\nbenchmark tasks test some aspects of human-imitative behaviour (e.g.,\nBIG-bench's 'human-like behavior' tasks), few, if not none, examine creative\nproblem solving abilities. Creative problem solving in humans is a well-studied\ntopic in cognitive neuroscience with standardized tests that predominantly use\nthe ability to associate (heterogeneous) connections among clue words as a\nmetric for creativity. Exposure to misleading stimuli - distractors dubbed red\nherrings - impede human performance in such tasks via the fixation effect and\nEinstellung paradigm. In cognitive neuroscience studies, such fixations are\nexperimentally induced by pre-exposing participants to orthographically similar\nincorrect words to subsequent word-fragments or clues. The popular British quiz\nshow Only Connect's Connecting Wall segment essentially mimics Mednick's Remote\nAssociates Test (RAT) formulation with built-in, deliberate red herrings, which\nmakes it an ideal proxy dataset to explore and study fixation effect and\nEinstellung paradigm from cognitive neuroscience in LLMs. In this paper we\npresent the novel Only Connect Wall (OCW) dataset and report results from our\nevaluation of selected pre-trained language models and LLMs on creative problem\nsolving tasks like grouping clue words by heterogeneous connections, and\nidentifying correct open knowledge domain connections in respective groups. We\nsynthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to\nfurther analyze our red-herrings hypothesis in language models. The code and\nlink to the dataset are available at https://github.com/TaatiTeam/OCW.",
        "pdf_link": "https://arxiv.org/pdf/2306.11167v4.pdf"
    },
    {
        "title": "Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting",
        "authors": [
            "Xinli Yu",
            "Zheng Chen",
            "Yuan Ling",
            "Shujing Dong",
            "Zongyi Liu",
            "Yanbin Lu"
        ],
        "published": "2023-06-19T15:42:02Z",
        "summary": "This paper presents a novel study on harnessing Large Language Models' (LLMs)\noutstanding knowledge and reasoning abilities for explainable financial time\nseries forecasting. The application of machine learning models to financial\ntime series comes with several challenges, including the difficulty in\ncross-sequence reasoning and inference, the hurdle of incorporating multi-modal\nsignals from historical news, financial knowledge graphs, etc., and the issue\nof interpreting and explaining the model results. In this paper, we focus on\nNASDAQ-100 stocks, making use of publicly accessible historical stock price\ndata, company metadata, and historical economic/financial news. We conduct\nexperiments to illustrate the potential of LLMs in offering a unified solution\nto the aforementioned challenges. Our experiments include trying\nzero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with\na public LLM model Open LLaMA. We demonstrate our approach outperforms a few\nbaselines, including the widely applied classic ARMA-GARCH model and a\ngradient-boosting tree model. Through the performance comparison results and a\nfew examples, we find LLMs can make a well-thought decision by reasoning over\ninformation from both textual news and price time series and extracting\ninsights, leveraging cross-sequence information, and utilizing the inherent\nknowledge embedded within the LLM. Additionally, we show that a publicly\navailable LLM such as Open-LLaMA, after fine-tuning, can comprehend the\ninstruction to generate explainable forecasts and achieve reasonable\nperformance, albeit relatively inferior in comparison to GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2306.11025v1.pdf"
    },
    {
        "title": "RepoFusion: Training Code Models to Understand Your Repository",
        "authors": [
            "Disha Shrivastava",
            "Denis Kocetkov",
            "Harm de Vries",
            "Dzmitry Bahdanau",
            "Torsten Scholak"
        ],
        "published": "2023-06-19T15:05:31Z",
        "summary": "Despite the huge success of Large Language Models (LLMs) in coding assistants\nlike GitHub Copilot, these models struggle to understand the context present in\nthe repository (e.g., imports, parent classes, files with similar names, etc.),\nthereby producing inaccurate code completions. This effect is more pronounced\nwhen using these assistants for repositories that the model has not seen during\ntraining, such as proprietary software or work-in-progress code projects.\nRecent work has shown the promise of using context from the repository during\ninference. In this work, we extend this idea and propose RepoFusion, a\nframework to train models to incorporate relevant repository context.\nExperiments on single-line code completion show that our models trained with\nrepository context significantly outperform much larger code models as\nCodeGen-16B-multi ($\\sim73\\times$ larger) and closely match the performance of\nthe $\\sim 70\\times$ larger StarCoderBase model that was trained with the\nFill-in-the-Middle objective. We find these results to be a novel and\ncompelling demonstration of the gains that training with repository context can\nbring. We carry out extensive ablation studies to investigate the impact of\ndesign choices such as context type, number of contexts, context length, and\ninitialization within our framework. Lastly, we release Stack-Repo, a dataset\nof 200 Java repositories with permissive licenses and near-deduplicated files\nthat are augmented with three types of repository contexts. Additionally, we\nare making available the code and trained checkpoints for our work. Our\nreleased resources can be found at \\url{https://huggingface.co/RepoFusion}.",
        "pdf_link": "https://arxiv.org/pdf/2306.10998v1.pdf"
    },
    {
        "title": "BayLing: Bridging Cross-lingual Alignment and Instruction Following through Interactive Translation for Large Language Models",
        "authors": [
            "Shaolei Zhang",
            "Qingkai Fang",
            "Zhuocheng Zhang",
            "Zhengrui Ma",
            "Yan Zhou",
            "Langlin Huang",
            "Mengyu Bu",
            "Shangtong Gui",
            "Yunji Chen",
            "Xilin Chen",
            "Yang Feng"
        ],
        "published": "2023-06-19T14:30:52Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable prowess in language\nunderstanding and generation. Advancing from foundation LLMs to\ninstructionfollowing LLMs, instruction tuning plays a vital role in aligning\nLLMs to human preferences. However, the existing LLMs are usually focused on\nEnglish, leading to inferior performance in non-English languages. In order to\nimprove the performance for non-English languages, it is necessary to collect\nlanguage-specific training data for foundation LLMs and construct\nlanguage-specific instructions for instruction tuning, both of which are heavy\nloads. To minimize human workload, we propose to transfer the capabilities of\nlanguage generation and instruction following from English to other languages\nthrough an interactive translation task. We have developed BayLing, an\ninstruction-following LLM by utilizing LLaMA as the foundation LLM and\nautomatically constructing interactive translation instructions for instructing\ntuning. Extensive assessments demonstrate that BayLing achieves comparable\nperformance to GPT-3.5-turbo, despite utilizing a considerably smaller\nparameter size of only 13 billion. Experimental results on translation tasks\nshow that BayLing achieves 95% of single-turn translation capability compared\nto GPT-4 with automatic evaluation and 96% of interactive translation\ncapability compared to GPT-3.5-turbo with human evaluation. To estimate the\nperformance on general tasks, we created a multi-turn instruction test set\ncalled BayLing-80. The experimental results on BayLing-80 indicate that BayLing\nachieves 89% of performance compared to GPT-3.5-turbo. BayLing also\ndemonstrates outstanding performance on knowledge assessment of Chinese GaoKao\nand English SAT, second only to GPT-3.5-turbo among a multitude of\ninstruction-following LLMs. Demo, homepage, code and models of BayLing are\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2306.10968v2.pdf"
    },
    {
        "title": "Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis",
        "authors": [
            "Jun-Min Lee",
            "Tae-Bin Ha"
        ],
        "published": "2023-06-19T10:22:12Z",
        "summary": "Generative Adversarial Networks (GAN) is a model for data synthesis, which\ncreates plausible data through the competition of generator and discriminator.\nAlthough GAN application to image synthesis is extensively studied, it has\ninherent limitations to natural language generation. Because natural language\nis composed of discrete tokens, a generator has difficulty updating its\ngradient through backpropagation; therefore, most text-GAN studies generate\nsentences starting with a random token based on a reward system. Thus, the\ngenerators of previous studies are pre-trained in an autoregressive way before\nadversarial training, causing data memorization that synthesized sentences\nreproduce the training data. In this paper, we synthesize sentences using a\nframework similar to the original GAN. More specifically, we propose Text\nEmbedding Space Generative Adversarial Networks (TESGAN) which generate\ncontinuous text embedding spaces instead of discrete tokens to solve the\ngradient backpropagation problem. Furthermore, TESGAN conducts unsupervised\nlearning which does not directly refer to the text of the training data to\novercome the data memorization issue. By adopting this novel method, TESGAN can\nsynthesize new sentences, showing the potential of unsupervised learning for\ntext synthesis. We expect to see extended research combining Large Language\nModels with a new perspective of viewing text as an continuous space.",
        "pdf_link": "https://arxiv.org/pdf/2306.17181v4.pdf"
    },
    {
        "title": "Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost",
        "authors": [
            "Juexiao Zhou",
            "Xiuying Chen",
            "Xin Gao"
        ],
        "published": "2023-06-19T08:15:14Z",
        "summary": "Medical artificial general intelligence (AGI) is an emerging field that aims\nto develop systems specifically designed for medical applications that possess\nthe ability to understand, learn, and apply knowledge across a wide range of\ntasks and domains. Large language models (LLMs) represent a significant step\ntowards AGI. However, training cross-domain LLMs in the medical field poses\nsignificant challenges primarily attributed to the requirement of collecting\ndata from diverse domains. This task becomes particularly difficult due to\nprivacy restrictions and the scarcity of publicly available medical datasets.\nHere, we propose Medical AGI (MedAGI), a paradigm to unify domain-specific\nmedical LLMs with the lowest cost, and suggest a possible path to achieve\nmedical AGI. With an increasing number of domain-specific professional\nmultimodal LLMs in the medical field being developed, MedAGI is designed to\nautomatically select appropriate medical models by analyzing users' questions\nwith our novel adaptive expert selection algorithm. It offers a unified\napproach to existing LLMs in the medical field, eliminating the need for\nretraining regardless of the introduction of new models. This characteristic\nrenders it a future-proof solution in the dynamically advancing medical domain.\nTo showcase the resilience of MedAGI, we conducted an evaluation across three\ndistinct medical domains: dermatology diagnosis, X-ray diagnosis, and analysis\nof pathology pictures. The results demonstrated that MedAGI exhibited\nremarkable versatility and scalability, delivering exceptional performance\nacross diverse domains. Our code is publicly available to facilitate further\nresearch at https://github.com/JoshuaChou2018/MedAGI.",
        "pdf_link": "https://arxiv.org/pdf/2306.10765v1.pdf"
    },
    {
        "title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
        "authors": [
            "Teodoro Baldazzi",
            "Luigi Bellomarini",
            "Stefano Ceri",
            "Andrea Colombo",
            "Andrea Gentili",
            "Emanuel Sallinger"
        ],
        "published": "2023-06-19T06:48:45Z",
        "summary": "Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to\ndiverse goals, thanks to task-specific training data. Task specificity should\ngo hand in hand with domain orientation, that is, the specialization of an LLM\nto accurately address the tasks of a given realm of interest. However, models\nare usually fine-tuned over publicly available data or, at most, over ground\ndata from databases, ignoring business-level definitions and domain experience.\nOn the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and\naugment such domain knowledge via ontological reasoning. With the goal of\ncombining LLM flexibility with the domain orientation of EKGs, we propose a\nnovel neurosymbolic architecture that leverages the power of ontological\nreasoning to build task- and domain-specific corpora for LLM fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2306.10723v2.pdf"
    },
    {
        "title": "Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix)",
        "authors": [
            "Cansu Koyuturk",
            "Mona Yavari",
            "Emily Theophilou",
            "Sathya Bursic",
            "Gregor Donabauer",
            "Alessia Telari",
            "Alessia Testa",
            "Raffaele Boiano",
            "Alessandro Gabbiadini",
            "Davinia Hernandez-Leo",
            "Martin Ruskov",
            "Dimitri Ognibene"
        ],
        "published": "2023-06-18T22:23:18Z",
        "summary": "Educational chatbots come with a promise of interactive and personalized\nlearning experiences, yet their development has been limited by the restricted\nfree interaction capabilities of available platforms and the difficulty of\nencoding knowledge in a suitable format. Recent advances in language learning\nmodels with zero-shot learning capabilities, such as ChatGPT, suggest a new\npossibility for developing educational chatbots using a prompt-based approach.\nWe present a case study with a simple system that enables mixed-turn chatbot\ninteractions and discuss the insights and preliminary guidelines obtained from\ninitial tests. We examine ChatGPT's ability to pursue multiple interconnected\nlearning objectives, adapt the educational activity to users' characteristics,\nsuch as culture, age, and level of education, and its ability to use diverse\neducational strategies and conversational styles. Although the results are\nencouraging, challenges are posed by the limited history maintained for the\nconversation and the highly structured form of responses by ChatGPT, as well as\ntheir variability, which can lead to an unexpected switch of the chatbot's role\nfrom a teacher to a therapist. We provide some initial guidelines to address\nthese issues and to facilitate the development of effective educational\nchatbots.",
        "pdf_link": "https://arxiv.org/pdf/2306.10645v2.pdf"
    },
    {
        "title": "The Importance of Human-Labeled Data in the Era of LLMs",
        "authors": [
            "Yang Liu"
        ],
        "published": "2023-06-18T12:12:03Z",
        "summary": "The advent of large language models (LLMs) has brought about a revolution in\nthe development of tailored machine learning models and sparked debates on\nredefining data requirements. The automation facilitated by the training and\nimplementation of LLMs has led to discussions and aspirations that human-level\nlabeling interventions may no longer hold the same level of importance as in\nthe era of supervised learning. This paper presents compelling arguments\nsupporting the ongoing relevance of human-labeled data in the era of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.14910v1.pdf"
    },
    {
        "title": "Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources",
        "authors": [
            "Paul Denny",
            "Hassan Khosravi",
            "Arto Hellas",
            "Juho Leinonen",
            "Sami Sarsa"
        ],
        "published": "2023-06-18T09:49:21Z",
        "summary": "As an increasing number of students move to online learning platforms that\ndeliver personalized learning experiences, there is a great need for the\nproduction of high-quality educational content. Large language models (LLMs)\nappear to offer a promising solution to the rapid creation of learning\nmaterials at scale, reducing the burden on instructors. In this study, we\ninvestigated the potential for LLMs to produce learning resources in an\nintroductory programming context, by comparing the quality of the resources\ngenerated by an LLM with those created by students as part of a learnersourcing\nactivity. Using a blind evaluation, students rated the correctness and\nhelpfulness of resources generated by AI and their peers, after both were\ninitially provided with identical exemplars. Our results show that the quality\nof AI-generated resources, as perceived by students, is equivalent to the\nquality of resources generated by their peers. This suggests that AI-generated\nresources may serve as viable supplementary material in certain contexts.\nResources generated by LLMs tend to closely mirror the given exemplars, whereas\nstudent-generated resources exhibit greater variety in terms of content length\nand specific syntax features used. The study highlights the need for further\nresearch exploring different types of learning resources and a broader range of\nsubject areas, and understanding the long-term impact of AI-generated resources\non learning outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2306.10509v2.pdf"
    },
    {
        "title": "News Verifiers Showdown: A Comparative Performance Evaluation of ChatGPT 3.5, ChatGPT 4.0, Bing AI, and Bard in News Fact-Checking",
        "authors": [
            "Kevin Matthe Caramancion"
        ],
        "published": "2023-06-18T04:30:29Z",
        "summary": "This study aimed to evaluate the proficiency of prominent Large Language\nModels (LLMs), namely OpenAI's ChatGPT 3.5 and 4.0, Google's Bard(LaMDA), and\nMicrosoft's Bing AI in discerning the truthfulness of news items using black\nbox testing. A total of 100 fact-checked news items, all sourced from\nindependent fact-checking agencies, were presented to each of these LLMs under\ncontrolled conditions. Their responses were classified into one of three\ncategories: True, False, and Partially True/False. The effectiveness of the\nLLMs was gauged based on the accuracy of their classifications against the\nverified facts provided by the independent agencies. The results showed a\nmoderate proficiency across all models, with an average score of 65.25 out of\n100. Among the models, OpenAI's GPT-4.0 stood out with a score of 71,\nsuggesting an edge in newer LLMs' abilities to differentiate fact from\ndeception. However, when juxtaposed against the performance of human\nfact-checkers, the AI models, despite showing promise, lag in comprehending the\nsubtleties and contexts inherent in news information. The findings highlight\nthe potential of AI in the domain of fact-checking while underscoring the\ncontinued importance of human cognitive skills and the necessity for persistent\nadvancements in AI capabilities. Finally, the experimental data produced from\nthe simulation of this work is openly available on Kaggle.",
        "pdf_link": "https://arxiv.org/pdf/2306.17176v1.pdf"
    },
    {
        "title": "CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents",
        "authors": [
            "Jeongeun Park",
            "Seungwon Lim",
            "Joonhyung Lee",
            "Sangbeom Park",
            "Minsuk Chang",
            "Youngjae Yu",
            "Sungjoon Choi"
        ],
        "published": "2023-06-17T15:24:54Z",
        "summary": "In this paper, we focus on inferring whether the given user command is clear,\nambiguous, or infeasible in the context of interactive robotic agents utilizing\nlarge language models (LLMs). To tackle this problem, we first present an\nuncertainty estimation method for LLMs to classify whether the command is\ncertain (i.e., clear) or not (i.e., ambiguous or infeasible). Once the command\nis classified as uncertain, we further distinguish it between ambiguous or\ninfeasible commands leveraging LLMs with situational aware context in a\nzero-shot manner. For ambiguous commands, we disambiguate the command by\ninteracting with users via question generation with LLMs. We believe that\nproper recognition of the given commands could lead to a decrease in\nmalfunction and undesired actions of the robot, enhancing the reliability of\ninteractive robot agents. We present a dataset for robotic situational\nawareness, consisting pair of high-level commands, scene descriptions, and\nlabels of command type (i.e., clear, ambiguous, or infeasible). We validate the\nproposed method on the collected dataset, pick-and-place tabletop simulation.\nFinally, we demonstrate the proposed approach in real-world human-robot\ninteraction experiments, i.e., handover scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.10376v5.pdf"
    },
    {
        "title": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning",
        "authors": [
            "Yunlong Tang",
            "Jinrui Zhang",
            "Xiangchen Wang",
            "Teng Wang",
            "Feng Zheng"
        ],
        "published": "2023-06-17T13:55:54Z",
        "summary": "Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .",
        "pdf_link": "https://arxiv.org/pdf/2306.10354v1.pdf"
    },
    {
        "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
        "authors": [
            "Stephanie Schoch",
            "Ritwick Mishra",
            "Yangfeng Ji"
        ],
        "published": "2023-06-16T20:07:38Z",
        "summary": "Although Shapley values have been shown to be highly effective for\nidentifying harmful training instances, dataset size and model complexity\nconstraints limit the ability to apply Shapley-based data valuation to\nfine-tuning large pre-trained language models. To address this, we propose\nTS-DShapley, an algorithm that reduces computational cost of Shapley-based data\nvaluation through: 1) an efficient sampling-based method that aggregates\nShapley values computed from subsets for valuation of the entire training set,\nand 2) a value transfer method that leverages value information extracted from\na simple classifier trained using representations from the target language\nmodel. Our experiments applying TS-DShapley to select data for fine-tuning\nBERT-based language models on benchmark natural language understanding (NLU)\ndatasets show that TS-DShapley outperforms existing data selection methods.\nFurther, TS-DShapley can filter fine-tuning data to increase language model\nperformance compared to training with the full fine-tuning dataset.",
        "pdf_link": "https://arxiv.org/pdf/2306.10165v1.pdf"
    },
    {
        "title": "Evaluating Superhuman Models with Consistency Checks",
        "authors": [
            "Lukas Fluri",
            "Daniel Paleka",
            "Florian Tramèr"
        ],
        "published": "2023-06-16T17:26:38Z",
        "summary": "If machine learning models were to achieve superhuman abilities at various\nreasoning or decision-making tasks, how would we go about evaluating such\nmodels, given that humans would necessarily be poor proxies for ground truth?\nIn this paper, we propose a framework for evaluating superhuman models via\nconsistency checks. Our premise is that while the correctness of superhuman\ndecisions may be impossible to evaluate, we can still surface mistakes if the\nmodel's decisions fail to satisfy certain logical, human-interpretable rules.\nWe instantiate our framework on three tasks where correctness of decisions is\nhard to evaluate due to either superhuman model abilities, or to otherwise\nmissing ground truth: evaluating chess positions, forecasting future events,\nand making legal judgments. We show that regardless of a model's (possibly\nsuperhuman) performance on these tasks, we can discover logical inconsistencies\nin decision making. For example: a chess engine assigning opposing valuations\nto semantically identical boards; GPT-4 forecasting that sports records will\nevolve non-monotonically over time; or an AI judge assigning bail to a\ndefendant only after we add a felony to their criminal record.",
        "pdf_link": "https://arxiv.org/pdf/2306.09983v3.pdf"
    },
    {
        "title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation",
        "authors": [
            "Guangyu Wang",
            "Guoxing Yang",
            "Zongxin Du",
            "Longjun Fan",
            "Xiaohu Li"
        ],
        "published": "2023-06-16T16:56:32Z",
        "summary": "Large language models have exhibited exceptional performance on various\nNatural Language Processing (NLP) tasks, leveraging techniques such as the\npre-training, and instruction fine-tuning. Despite these advances, their\neffectiveness in medical applications is limited, due to challenges such as\nfactual inaccuracies, reasoning abilities, and lack grounding in real-world\nexperience. In this study, we present ClinicalGPT, a language model explicitly\ndesigned and optimized for clinical scenarios. By incorporating extensive and\ndiverse real-world data, such as medical records, domain-specific knowledge,\nand multi-round dialogue consultations in the training process, ClinicalGPT is\nbetter prepared to handle multiple clinical task. Furthermore, we introduce a\ncomprehensive evaluation framework that includes medical knowledge\nquestion-answering, medical exams, patient consultations, and diagnostic\nanalysis of medical records. Our results demonstrate that ClinicalGPT\nsignificantly outperforms other models in these tasks, highlighting the\neffectiveness of our approach in adapting large language models to the critical\ndomain of healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2306.09968v1.pdf"
    },
    {
        "title": "Friend or Foe? Exploring the Implications of Large Language Models on the Science System",
        "authors": [
            "Benedikt Fecher",
            "Marcel Hebing",
            "Melissa Laufer",
            "Jörg Pohle",
            "Fabian Sofsky"
        ],
        "published": "2023-06-16T15:50:17Z",
        "summary": "The advent of ChatGPT by OpenAI has prompted extensive discourse on its\npotential implications for science and higher education. While the impact on\neducation has been a primary focus, there is limited empirical research on the\neffects of large language models (LLMs) and LLM-based chatbots on science and\nscientific practice. To investigate this further, we conducted a Delphi study\ninvolving 72 experts specialising in research and AI. The study focused on\napplications and limitations of LLMs, their effects on the science system,\nethical and legal considerations, and the required competencies for their\neffective use. Our findings highlight the transformative potential of LLMs in\nscience, particularly in administrative, creative, and analytical tasks.\nHowever, risks related to bias, misinformation, and quality assurance need to\nbe addressed through proactive regulation and science education. This research\ncontributes to informed discussions on the impact of generative AI in science\nand helps identify areas for future action.",
        "pdf_link": "https://arxiv.org/pdf/2306.09928v1.pdf"
    },
    {
        "title": "Is Self-Repair a Silver Bullet for Code Generation?",
        "authors": [
            "Theo X. Olausson",
            "Jeevana Priya Inala",
            "Chenglong Wang",
            "Jianfeng Gao",
            "Armando Solar-Lezama"
        ],
        "published": "2023-06-16T15:13:17Z",
        "summary": "Large language models have shown remarkable aptitude in code generation, but\nstill struggle to perform complex tasks. Self-repair -- in which the model\ndebugs and repairs its own code -- has recently become a popular way to boost\nperformance in these settings. However, despite its increasing popularity,\nexisting studies of self-repair have been limited in scope; in many settings,\nits efficacy thus remains poorly understood. In this paper, we analyze Code\nLlama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken\nfrom HumanEval and APPS. We find that when the cost of carrying out repair is\ntaken into account, performance gains are often modest, vary a lot between\nsubsets of the data, and are sometimes not present at all. We hypothesize that\nthis is because self-repair is bottlenecked by the model's ability to provide\nfeedback on its own code; using a stronger model to artificially boost the\nquality of the feedback, we observe substantially larger performance gains.\nSimilarly, a small-scale study in which we provide GPT-4 with feedback from\nhuman participants suggests that even for the strongest models, self-repair\nstill lags far behind what can be achieved with human-level debugging.",
        "pdf_link": "https://arxiv.org/pdf/2306.09896v5.pdf"
    },
    {
        "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond",
        "authors": [
            "Fangzhi Xu",
            "Qika Lin",
            "Jiawei Han",
            "Tianzhe Zhao",
            "Jun Liu",
            "Erik Cambria"
        ],
        "published": "2023-06-16T13:39:35Z",
        "summary": "Logical reasoning consistently plays a fundamental and significant role in\nthe domains of knowledge engineering and artificial intelligence. Recently,\nLarge Language Models (LLMs) have emerged as a noteworthy innovation in natural\nlanguage processing (NLP), exhibiting impressive achievements across various\nclassic NLP tasks. However, the question of whether LLMs can effectively\naddress the task of logical reasoning, which requires gradual cognitive\ninference similar to human intelligence, remains unanswered. To this end, we\naim to bridge this gap and provide comprehensive evaluations in this paper.\nFirstly, to offer systematic evaluations, we select fifteen typical logical\nreasoning datasets and organize them into deductive, inductive, abductive and\nmixed-form reasoning settings. Considering the comprehensiveness of\nevaluations, we include three representative LLMs (i.e., text-davinci-003,\nChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,\none-shot and three-shot settings. Secondly, different from previous evaluations\nrelying only on simple metrics (e.g., accuracy), we propose fine-level\nevaluations from objective and subjective manners, covering both answers and\nexplanations. Additionally, to uncover the logical flaws of LLMs, problematic\ncases will be attributed to five error types from two dimensions, i.e.,\nevidence selection process and reasoning process. Thirdly, to avoid the\ninfluences of knowledge bias and purely focus on benchmarking the logical\nreasoning capability of LLMs, we propose a new dataset with neutral content. It\ncontains 3,000 samples and covers deductive, inductive and abductive settings.\nBased on the in-depth evaluations, this paper finally forms a general\nevaluation scheme of logical reasoning capability from six dimensions. It\nreflects the pros and cons of LLMs and gives guiding directions for future\nworks.",
        "pdf_link": "https://arxiv.org/pdf/2306.09841v3.pdf"
    },
    {
        "title": "Process Knowledge-infused Learning for Clinician-friendly Explanations",
        "authors": [
            "Kaushik Roy",
            "Yuxin Zi",
            "Manas Gaur",
            "Jinendra Malekar",
            "Qi Zhang",
            "Vignesh Narayanan",
            "Amit Sheth"
        ],
        "published": "2023-06-16T13:08:17Z",
        "summary": "Language models have the potential to assess mental health using social media\ndata. By analyzing online posts and conversations, these models can detect\npatterns indicating mental health conditions like depression, anxiety, or\nsuicidal thoughts. They examine keywords, language markers, and sentiment to\ngain insights into an individual's mental well-being. This information is\ncrucial for early detection, intervention, and support, improving mental health\ncare and prevention strategies. However, using language models for mental\nhealth assessments from social media has two limitations: (1) They do not\ncompare posts against clinicians' diagnostic processes, and (2) It's\nchallenging to explain language model outputs using concepts that the clinician\ncan understand, i.e., clinician-friendly explanations. In this study, we\nintroduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm\nthat layers clinical process knowledge structures on language model outputs,\nenabling clinician-friendly explanations of the underlying language model\npredictions. We rigorously test our methods on existing benchmark datasets,\naugmented with such clinical process knowledge, and release a new dataset for\nassessing suicidality. PK-iL performs competitively, achieving a 70% agreement\nwith users, while other XAI methods only achieve 47% agreement (average\ninter-rater agreement of 0.72). Our evaluations demonstrate that PK-iL\neffectively explains model predictions to clinicians.",
        "pdf_link": "https://arxiv.org/pdf/2306.09824v1.pdf"
    },
    {
        "title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System",
        "authors": [
            "Zhiyuan Hu",
            "Yue Feng",
            "Anh Tuan Luu",
            "Bryan Hooi",
            "Aldo Lipani"
        ],
        "published": "2023-06-16T13:04:56Z",
        "summary": "Dialogue systems and large language models (LLMs) have gained considerable\nattention. However, the direct utilization of LLMs as task-oriented dialogue\n(TOD) models has been found to underperform compared to smaller task-specific\nmodels. Nonetheless, it is crucial to acknowledge the significant potential of\nLLMs and explore improved approaches for leveraging their impressive abilities.\nMotivated by the goal of leveraging LLMs, we propose an alternative approach\ncalled User-Guided Response Optimization (UGRO) to combine it with a smaller\nTOD model. This approach uses LLM as annotation-free user simulator to assess\ndialogue responses, combining them with smaller fine-tuned end-to-end TOD\nmodels. By utilizing the satisfaction feedback generated by LLMs, UGRO further\noptimizes the supervised fine-tuned TOD model. Specifically, the TOD model\ntakes the dialogue history as input and, with the assistance of the user\nsimulator's feedback, generates high-satisfaction responses that meet the\nuser's requirements. Through empirical experiments on two TOD benchmarks, we\nvalidate the effectiveness of our method. The results demonstrate that our\napproach outperforms previous state-of-the-art (SOTA) results.",
        "pdf_link": "https://arxiv.org/pdf/2306.09821v2.pdf"
    },
    {
        "title": "Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody",
        "authors": [
            "Sofoklis Kakouros",
            "Juraj Šimko",
            "Martti Vainio",
            "Antti Suni"
        ],
        "published": "2023-06-16T12:49:44Z",
        "summary": "This paper investigates the use of word surprisal, a measure of the\npredictability of a word in a given context, as a feature to aid speech\nsynthesis prosody. We explore how word surprisal extracted from large language\nmodels (LLMs) correlates with word prominence, a signal-based measure of the\nsalience of a word in a given discourse. We also examine how context length and\nLLM size affect the results, and how a speech synthesizer conditioned with\nsurprisal values compares with a baseline system. To evaluate these factors, we\nconducted experiments using a large corpus of English text and LLMs of varying\nsizes. Our results show that word surprisal and word prominence are moderately\ncorrelated, suggesting that they capture related but distinct aspects of\nlanguage use. We find that length of context and size of the LLM impact the\ncorrelations, but not in the direction anticipated, with longer contexts and\nlarger LLMs generally underpredicting prominent words in a nearly linear\nmanner. We demonstrate that, in line with these findings, a speech synthesizer\nconditioned with surprisal values provides a minimal improvement over the\nbaseline with the results suggesting a limited effect of using surprisal values\nfor eliciting appropriate prominence patterns.",
        "pdf_link": "https://arxiv.org/pdf/2306.09814v1.pdf"
    },
    {
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources",
        "authors": [
            "Kai Lv",
            "Yuqing Yang",
            "Tengxiao Liu",
            "Qinghui Gao",
            "Qipeng Guo",
            "Xipeng Qiu"
        ],
        "published": "2023-06-16T11:37:15Z",
        "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing\n(NLP) but demand massive GPU resources for training. Lowering the threshold for\nLLMs training would encourage greater participation from researchers,\nbenefiting both academia and society. While existing approaches have focused on\nparameter-efficient fine-tuning, which tunes or adds a small number of\nparameters, few have addressed the challenge of tuning the full parameters of\nLLMs with limited resources. In this work, we propose a new optimizer,\nLOw-Memory Optimization (LOMO), which fuses the gradient computation and the\nparameter update in one step to reduce memory usage. By integrating LOMO with\nexisting memory saving techniques, we reduce memory usage to 10.8% compared to\nthe standard approach (DeepSpeed solution). Consequently, our approach enables\nthe full parameter fine-tuning of a 65B model on a single machine with 8 RTX\n3090, each with 24GB memory.",
        "pdf_link": "https://arxiv.org/pdf/2306.09782v1.pdf"
    },
    {
        "title": "Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models",
        "authors": [
            "Victor Steinborn",
            "Antonis Maronikolakis",
            "Hinrich Schütze"
        ],
        "published": "2023-06-16T10:36:18Z",
        "summary": "In efforts to keep up with the rapid progress and use of large language\nmodels, gender bias research is becoming more prevalent in NLP. Non-English\nbias research, however, is still in its infancy with most work focusing on\nEnglish. In our work, we study how grammatical gender bias relating to\npoliteness levels manifests in Japanese and Korean language models. Linguistic\nstudies in these languages have identified a connection between gender bias and\npoliteness levels, however it is not yet known if language models reproduce\nthese biases. We analyze relative prediction probabilities of the male and\nfemale grammatical genders using templates and find that informal polite speech\nis most indicative of the female grammatical gender, while rude and formal\nspeech is most indicative of the male grammatical gender. Further, we find\npoliteness levels to be an attack vector for allocational gender bias in\ncyberbullying detection models. Cyberbullies can evade detection through simple\ntechniques abusing politeness levels. We introduce an attack dataset to (i)\nidentify representational gender bias across politeness levels, (ii)\ndemonstrate how gender biases can be abused to bypass cyberbullying detection\nmodels and (iii) show that allocational biases can be mitigated via training on\nour proposed dataset. Through our findings we highlight the importance of bias\nresearch moving beyond its current English-centrism.",
        "pdf_link": "https://arxiv.org/pdf/2306.09752v1.pdf"
    },
    {
        "title": "Pushing the Limits of ChatGPT on NLP Tasks",
        "authors": [
            "Xiaofei Sun",
            "Linfeng Dong",
            "Xiaoya Li",
            "Zhen Wan",
            "Shuhe Wang",
            "Tianwei Zhang",
            "Jiwei Li",
            "Fei Cheng",
            "Lingjuan Lyu",
            "Fei Wu",
            "Guoyin Wang"
        ],
        "published": "2023-06-16T09:40:05Z",
        "summary": "Despite the success of ChatGPT, its performances on most NLP tasks are still\nwell below the supervised baselines. In this work, we looked into the causes,\nand discovered that its subpar performance was caused by the following factors:\n(1) token limit in the prompt does not allow for the full utilization of the\nsupervised datasets; (2) mismatch between the generation nature of ChatGPT and\nNLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly\nfocus on certain keywords, etc.\n  In this work, we propose a collection of general modules to address these\nissues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed\nmodules include (1) a one-input-multiple-prompts strategy that employs multiple\nprompts for one input to accommodate more demonstrations; (2) using fine-tuned\nmodels for better demonstration retrieval; (3) transforming tasks to formats\nthat are more tailored to the generation nature; (4) employing reasoning\nstrategies that are tailored to addressing the task-specific complexity; (5)\nthe self-verification strategy to address the hallucination issue of LLMs; (6)\nthe paraphrase strategy to improve the robustness of model predictions.\n  We conduct experiments on 21 datasets of 10 representative NLP tasks,\nincluding question answering, commonsense reasoning, natural language\ninference, sentiment analysis, named entity recognition, entity-relation\nextraction, event extraction, dependency parsing, semantic role labeling, and\npart-of-speech tagging. Using the proposed assemble of techniques, we are able\nto significantly boost the performance of ChatGPT on the selected NLP tasks,\nachieving performances comparable to or better than supervised baselines, or\neven existing SOTA performances.",
        "pdf_link": "https://arxiv.org/pdf/2306.09719v2.pdf"
    },
    {
        "title": "Clickbait Detection via Large Language Models",
        "authors": [
            "Han Wang",
            "Yi Zhu",
            "Ye Wang",
            "Yun Li",
            "Yunhao Yuan",
            "Jipeng Qiang"
        ],
        "published": "2023-06-16T02:49:20Z",
        "summary": "Clickbait, which aims to induce users with some surprising and even thrilling\nheadlines for increasing click-through rates, permeates almost all online\ncontent publishers, such as news portals and social media. Recently, Large\nLanguage Models (LLMs) have emerged as a powerful instrument and achieved\ntremendous success in a series of NLP downstream tasks. However, it is not yet\nknown whether LLMs can be served as a high-quality clickbait detection system.\nIn this paper, we analyze the performance of LLMs in the few-shot and zero-shot\nscenarios on several English and Chinese benchmark datasets. Experimental\nresults show that LLMs cannot achieve the best results compared to the\nstate-of-the-art deep and fine-tuning PLMs methods. Different from human\nintuition, the experiments demonstrated that LLMs cannot make satisfied\nclickbait detection just by the headlines.",
        "pdf_link": "https://arxiv.org/pdf/2306.09597v3.pdf"
    },
    {
        "title": "Schema-learning and rebinding as mechanisms of in-context learning and emergence",
        "authors": [
            "Sivaramakrishnan Swaminathan",
            "Antoine Dedieu",
            "Rajkumar Vasudeva Raju",
            "Murray Shanahan",
            "Miguel Lazaro-Gredilla",
            "Dileep George"
        ],
        "published": "2023-06-16T00:29:19Z",
        "summary": "In-context learning (ICL) is one of the most powerful and most unexpected\ncapabilities to emerge in recent transformer-based large language models\n(LLMs). Yet the mechanisms that underlie it are poorly understood. In this\npaper, we demonstrate that comparable ICL capabilities can be acquired by an\nalternative sequence prediction learning method using clone-structured causal\ngraphs (CSCGs). Moreover, a key property of CSCGs is that, unlike\ntransformer-based LLMs, they are {\\em interpretable}, which considerably\nsimplifies the task of explaining how ICL works. Specifically, we show that it\nuses a combination of (a) learning template (schema) circuits for pattern\ncompletion, (b) retrieving relevant templates in a context-sensitive manner,\nand (c) rebinding of novel tokens to appropriate slots in the templates. We go\non to marshall evidence for the hypothesis that similar mechanisms underlie ICL\nin LLMs. For example, we find that, with CSCGs as with LLMs, different\ncapabilities emerge at different levels of overparameterization, suggesting\nthat overparameterization helps in learning more complex template (schema)\ncircuits. By showing how ICL can be achieved with small models and datasets, we\nopen up a path to novel architectures, and take a vital step towards a more\ngeneral understanding of the mechanics behind this important capability.",
        "pdf_link": "https://arxiv.org/pdf/2307.01201v1.pdf"
    },
    {
        "title": "Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses",
        "authors": [
            "Jaromir Savelka",
            "Arav Agarwal",
            "Marshall An",
            "Chris Bogart",
            "Majd Sakr"
        ],
        "published": "2023-06-15T22:12:34Z",
        "summary": "This paper studies recent developments in large language models' (LLM)\nabilities to pass assessments in introductory and intermediate Python\nprogramming courses at the postsecondary level. The emergence of ChatGPT\nresulted in heated debates of its potential uses (e.g., exercise generation,\ncode explanation) as well as misuses in programming classes (e.g., cheating).\nRecent studies show that while the technology performs surprisingly well on\ndiverse sets of assessment instruments employed in typical programming classes\nthe performance is usually not sufficient to pass the courses. The release of\nGPT-4 largely emphasized notable improvements in the capabilities related to\nhandling assessments originally designed for human test-takers. This study is\nthe necessary analysis in the context of this ongoing transition towards mature\ngenerative AI systems. Specifically, we report the performance of GPT-4,\ncomparing it to the previous generations of GPT models, on three Python courses\nwith assessments ranging from simple multiple-choice questions (no code\ninvolved) to complex programming projects with code bases distributed into\nmultiple files (599 exercises overall). Additionally, we analyze the\nassessments that were not handled well by GPT-4 to understand the current\nlimitations of the model, as well as its capabilities to leverage feedback\nprovided by an auto-grader. We found that the GPT models evolved from\ncompletely failing the typical programming class' assessments (the original\nGPT-3) to confidently passing the courses with no human involvement (GPT-4).\nWhile we identified certain limitations in GPT-4's handling of MCQs and coding\nexercises, the rate of improvement across the recent generations of GPT models\nstrongly suggests their potential to handle almost any type of assessment\nwidely used in higher education programming courses. These findings could be\nleveraged by educators and institutions to adapt the design of programming\nassessments as well as to fuel the necessary discussions into how programming\nclasses should be updated to reflect the recent technological developments.\nThis study provides evidence that programming instructors need to prepare for a\nworld in which there is an easy-to-use widely accessible technology that can be\nutilized by learners to collect passing scores, with no effort whatsoever, on\nwhat today counts as viable programming knowledge and skills assessments.",
        "pdf_link": "https://arxiv.org/pdf/2306.10073v1.pdf"
    },
    {
        "title": "Explaining Legal Concepts with Augmented Large Language Models (GPT-4)",
        "authors": [
            "Jaromir Savelka",
            "Kevin D. Ashley",
            "Morgan A. Gray",
            "Hannes Westermann",
            "Huihui Xu"
        ],
        "published": "2023-06-15T21:58:18Z",
        "summary": "Interpreting the meaning of legal open-textured terms is a key task of legal\nprofessionals. An important source for this interpretation is how the term was\napplied in previous court cases. In this paper, we evaluate the performance of\nGPT-4 in generating factually accurate, clear and relevant explanations of\nterms in legislation. We compare the performance of a baseline setup, where\nGPT-4 is directly asked to explain a legal term, to an augmented approach,\nwhere a legal information retrieval module is used to provide relevant context\nto the model, in the form of sentences from case law. We found that the direct\napplication of GPT-4 yields explanations that appear to be of very high quality\non their surface. However, detailed analysis uncovered limitations in terms of\nthe factual accuracy of the explanations. Further, we found that the\naugmentation leads to improved quality, and appears to eliminate the issue of\nhallucination, where models invent incorrect statements. These findings open\nthe door to the building of systems that can autonomously retrieve relevant\nsentences from case law and condense them into a useful explanation for legal\nscholars, educators or practicing lawyers alike.",
        "pdf_link": "https://arxiv.org/pdf/2306.09525v2.pdf"
    },
    {
        "title": "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models",
        "authors": [
            "Junting Pan",
            "Ziyi Lin",
            "Yuying Ge",
            "Xiatian Zhu",
            "Renrui Zhang",
            "Yi Wang",
            "Yu Qiao",
            "Hongsheng Li"
        ],
        "published": "2023-06-15T20:56:20Z",
        "summary": "Video Question Answering (VideoQA) has been significantly advanced from the\nscaling of recent Large Language Models (LLMs). The key idea is to convert the\nvisual information into the language feature space so that the capacity of LLMs\ncan be fully exploited. Existing VideoQA methods typically take two paradigms:\n(1) learning cross-modal alignment, and (2) using an off-the-shelf captioning\nmodel to describe the visual data. However, the first design needs costly\ntraining on many extra multi-modal data, whilst the second is further limited\nby limited domain generalization. To address these limitations, a simple yet\neffective Retrieving-to-Answer (R2A) framework is proposed.Given an input\nvideo, R2A first retrieves a set of semantically similar texts from a generic\ntext corpus using a pre-trained multi-modal model (e.g., CLIP). With both the\nquestion and the retrieved texts, a LLM (e.g., DeBERTa) can be directly used to\nyield a desired answer. Without the need for cross-modal fine-tuning, R2A\nallows for all the key components (e.g., LLM, retrieval model, and text corpus)\nto plug-and-play. Extensive experiments on several VideoQA benchmarks show that\ndespite with 1.3B parameters and no fine-tuning, our R2A can outperform the 61\ntimes larger Flamingo-80B model even additionally trained on nearly 2.1B\nmulti-modal data.",
        "pdf_link": "https://arxiv.org/pdf/2306.11732v1.pdf"
    },
    {
        "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health",
        "authors": [
            "Shubo Tian",
            "Qiao Jin",
            "Lana Yeganova",
            "Po-Ting Lai",
            "Qingqing Zhu",
            "Xiuying Chen",
            "Yifan Yang",
            "Qingyu Chen",
            "Won Kim",
            "Donald C. Comeau",
            "Rezarta Islamaj",
            "Aadit Kapoor",
            "Xin Gao",
            "Zhiyong Lu"
        ],
        "published": "2023-06-15T20:19:08Z",
        "summary": "ChatGPT has drawn considerable attention from both the general public and\ndomain experts with its remarkable text generation capabilities. This has\nsubsequently led to the emergence of diverse applications in the field of\nbiomedicine and health. In this work, we examine the diverse applications of\nlarge language models (LLMs), such as ChatGPT, in biomedicine and health.\nSpecifically we explore the areas of biomedical information retrieval, question\nanswering, medical text summarization, information extraction, and medical\neducation, and investigate whether LLMs possess the transformative power to\nrevolutionize these tasks or whether the distinct complexities of biomedical\ndomain presents unique challenges. Following an extensive literature survey, we\nfind that significant advances have been made in the field of text generation\ntasks, surpassing the previous state-of-the-art methods. For other\napplications, the advances have been modest. Overall, LLMs have not yet\nrevolutionized biomedicine, but recent rapid progress indicates that such\nmethods hold great potential to provide valuable means for accelerating\ndiscovery and improving health. We also find that the use of LLMs, like\nChatGPT, in the fields of biomedicine and health entails various risks and\nchallenges, including fabricated information in its generated responses, as\nwell as legal and privacy concerns associated with sensitive patient data. We\nbelieve this survey can provide a comprehensive and timely overview to\nbiomedical researchers and healthcare practitioners on the opportunities and\nchallenges associated with using ChatGPT and other LLMs for transforming\nbiomedicine and health.",
        "pdf_link": "https://arxiv.org/pdf/2306.10070v2.pdf"
    },
    {
        "title": "Inverse Scaling: When Bigger Isn't Better",
        "authors": [
            "Ian R. McKenzie",
            "Alexander Lyzhov",
            "Michael Pieler",
            "Alicia Parrish",
            "Aaron Mueller",
            "Ameya Prabhu",
            "Euan McLean",
            "Aaron Kirtland",
            "Alexis Ross",
            "Alisa Liu",
            "Andrew Gritsevskiy",
            "Daniel Wurgaft",
            "Derik Kauffman",
            "Gabriel Recchia",
            "Jiacheng Liu",
            "Joe Cavanagh",
            "Max Weiss",
            "Sicong Huang",
            "The Floating Droid",
            "Tom Tseng",
            "Tomasz Korbak",
            "Xudong Shen",
            "Yuhui Zhang",
            "Zhengping Zhou",
            "Najoung Kim",
            "Samuel R. Bowman",
            "Ethan Perez"
        ],
        "published": "2023-06-15T20:11:23Z",
        "summary": "Work on scaling laws has found that large language models (LMs) show\npredictable improvements to overall loss with increased scale (model size,\ntraining data, and compute). Here, we present evidence for the claim that LMs\nmay show inverse scaling, or worse task performance with increased scale, e.g.,\ndue to flaws in the training objective and data. We present empirical evidence\nof inverse scaling on 11 datasets collected by running a public contest, the\nInverse Scaling Prize, with a substantial prize pool. Through analysis of the\ndatasets, along with other examples found in the literature, we identify four\npotential causes of inverse scaling: (i) preference to repeat memorized\nsequences over following in-context instructions, (ii) imitation of undesirable\npatterns in the training data, (iii) tasks containing an easy distractor task\nwhich LMs could focus on, rather than the harder real task, and (iv) correct\nbut misleading few-shot demonstrations of the task. We release the winning\ndatasets at https://inversescaling.com/data to allow for further investigation\nof inverse scaling. Our tasks have helped drive the discovery of U-shaped and\ninverted-U scaling trends, where an initial trend reverses, suggesting that\nscaling trends are less reliable at predicting the behavior of larger-scale\nmodels than previously understood. Overall, our results suggest that there are\ntasks for which increased model scale alone may not lead to progress, and that\nmore careful thought needs to go into the data and objectives for training\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2306.09479v1.pdf"
    },
    {
        "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch",
        "authors": [
            "Stephen Casper",
            "Jason Lin",
            "Joe Kwon",
            "Gatlen Culp",
            "Dylan Hadfield-Menell"
        ],
        "published": "2023-06-15T18:49:50Z",
        "summary": "Deploying large language models (LMs) can pose hazards from harmful outputs\nsuch as toxic or false text. Prior work has introduced automated tools that\nelicit harmful outputs to identify these risks. While this is a valuable step\ntoward securing models, these approaches rely on a pre-existing way to\nefficiently classify undesirable outputs. Using a pre-existing classifier does\nnot allow for red-teaming to be tailored to the target model. Furthermore, when\nfailures can be easily classified in advance, red-teaming has limited marginal\nvalue because problems can be avoided by simply filtering training data and/or\nmodel outputs. Here, we consider red-teaming \"from scratch,\" in which the\nadversary does not begin with a way to classify failures. Our framework\nconsists of three steps: 1) Exploring the model's range of behaviors in the\ndesired context; 2) Establishing a definition and measurement for undesired\nbehavior (e.g., a classifier trained to reflect human evaluations); and 3)\nExploiting the model's flaws using this measure to develop diverse adversarial\nprompts. We use this approach to red-team GPT-3 to discover classes of inputs\nthat elicit false statements. In doing so, we construct the CommonClaim dataset\nof 20,000 statements labeled by humans as common-knowledge-true, common\nknowledge-false, or neither. We are making code and data available.",
        "pdf_link": "https://arxiv.org/pdf/2306.09442v3.pdf"
    },
    {
        "title": "SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts",
        "authors": [
            "Rose E. Wang",
            "Pawan Wirawarn",
            "Noah Goodman",
            "Dorottya Demszky"
        ],
        "published": "2023-06-15T17:59:47Z",
        "summary": "Lectures are a learning experience for both students and teachers. Students\nlearn from teachers about the subject material, while teachers learn from\nstudents about how to refine their instruction. However, online student\nfeedback is unstructured and abundant, making it challenging for teachers to\nlearn and improve. We take a step towards tackling this challenge. First, we\ncontribute a dataset for studying this problem: SIGHT is a large dataset of 288\nmath lecture transcripts and 15,784 comments collected from the Massachusetts\nInstitute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we\ndevelop a rubric for categorizing feedback types using qualitative analysis.\nQualitative analysis methods are powerful in uncovering domain-specific\ninsights, however they are costly to apply to large data sources. To overcome\nthis challenge, we propose a set of best practices for using large language\nmodels (LLMs) to cheaply classify the comments at scale. We observe a striking\ncorrelation between the model's and humans' annotation: Categories with\nconsistent human annotations (>$0.9$ inter-rater reliability, IRR) also display\nhigher human-model agreement (>$0.7$), while categories with less consistent\nhuman annotations ($0.7$-$0.8$ IRR) correspondingly demonstrate lower\nhuman-model agreement ($0.3$-$0.5$). These techniques uncover useful student\nfeedback from thousands of comments, costing around $\\$0.002$ per comment. We\nconclude by discussing exciting future directions on using online student\nfeedback and improving automated annotation techniques for qualitative\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2306.09343v1.pdf"
    },
    {
        "title": "Language-Guided Music Recommendation for Video via Prompt Analogies",
        "authors": [
            "Daniel McKee",
            "Justin Salamon",
            "Josef Sivic",
            "Bryan Russell"
        ],
        "published": "2023-06-15T17:58:01Z",
        "summary": "We propose a method to recommend music for an input video while allowing a\nuser to guide music selection with free-form natural language. A key challenge\nof this problem setting is that existing music video datasets provide the\nneeded (video, music) training pairs, but lack text descriptions of the music.\nThis work addresses this challenge with the following three contributions.\nFirst, we propose a text-synthesis approach that relies on an analogy-based\nprompting procedure to generate natural language music descriptions from a\nlarge-scale language model (BLOOM-176B) given pre-trained music tagger outputs\nand a small number of human text descriptions. Second, we use these synthesized\nmusic descriptions to train a new trimodal model, which fuses text and video\ninput representations to query music samples. For training, we introduce a text\ndropout regularization mechanism which we show is critical to model\nperformance. Our model design allows for the retrieved music audio to agree\nwith the two input modalities by matching visual style depicted in the video\nand musical genre, mood, or instrumentation described in the natural language\nquery. Third, to evaluate our approach, we collect a testing dataset for our\nproblem by annotating a subset of 4k clips from the YT8M-MusicVideo dataset\nwith natural language music descriptions which we make publicly available. We\nshow that our approach can match or exceed the performance of prior methods on\nvideo-to-music retrieval while significantly improving retrieval accuracy when\nusing text guidance.",
        "pdf_link": "https://arxiv.org/pdf/2306.09327v1.pdf"
    },
    {
        "title": "Matching Pairs: Attributing Fine-Tuned Models to their Pre-Trained Large Language Models",
        "authors": [
            "Myles Foley",
            "Ambrish Rawat",
            "Taesung Lee",
            "Yufang Hou",
            "Gabriele Picco",
            "Giulio Zizzo"
        ],
        "published": "2023-06-15T17:42:48Z",
        "summary": "The wide applicability and adaptability of generative large language models\n(LLMs) has enabled their rapid adoption. While the pre-trained models can\nperform many tasks, such models are often fine-tuned to improve their\nperformance on various downstream applications. However, this leads to issues\nover violation of model licenses, model theft, and copyright infringement.\nMoreover, recent advances show that generative technology is capable of\nproducing harmful content which exacerbates the problems of accountability\nwithin model supply chains. Thus, we need a method to investigate how a model\nwas trained or a piece of text was generated and what their pre-trained base\nmodel was. In this paper we take the first step to address this open problem by\ntracing back the origin of a given fine-tuned LLM to its corresponding\npre-trained base model. We consider different knowledge levels and attribution\nstrategies, and find that we can correctly trace back 8 out of the 10 fine\ntuned models with our best method.",
        "pdf_link": "https://arxiv.org/pdf/2306.09308v1.pdf"
    },
    {
        "title": "Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization",
        "authors": [
            "Swarnadeep Saha",
            "Peter Hase",
            "Mohit Bansal"
        ],
        "published": "2023-06-15T17:27:20Z",
        "summary": "A hallmark property of explainable AI models is the ability to teach other\nagents, communicating knowledge of how to perform a task. While Large Language\nModels perform complex reasoning by generating explanations for their\npredictions, it is unclear whether they also make good teachers for weaker\nagents. To address this, we consider a student-teacher framework between two\nLLM agents and study if, when, and how the teacher should intervene with\nnatural language explanations to improve the student's performance. Since\ncommunication is expensive, we define a budget such that the teacher only\ncommunicates explanations for a fraction of the data, after which the student\nshould perform well on its own. We decompose the teaching problem along four\naxes: (1) if teacher's test time intervention improve student predictions, (2)\nwhen it is worth explaining a data point, (3) how the teacher should\npersonalize explanations to better teach the student, and (4) if teacher\nexplanations also improve students on future unexplained data. We first show\nthat teacher LLMs can indeed intervene on student reasoning to improve their\nperformance. Next, inspired by the Theory of Mind abilities of effective\nteachers, we propose building two few-shot mental models of the student. The\nfirst model defines an Intervention Function that simulates the utility of an\nintervention, allowing the teacher to intervene when this utility is the\nhighest and improving student performance at lower budgets. The second model\nenables the teacher to personalize explanations for a particular student and\noutperform unpersonalized teachers. We also demonstrate that in multi-turn\ninteractions, teacher explanations generalize and learning from explained data\nimproves student performance on future unexplained data. Finally, we verify\nthat misaligned teachers can lower student performance to random chance by\nintentionally misleading them.",
        "pdf_link": "https://arxiv.org/pdf/2306.09299v2.pdf"
    },
    {
        "title": "KoLA: Carefully Benchmarking World Knowledge of Large Language Models",
        "authors": [
            "Jifan Yu",
            "Xiaozhi Wang",
            "Shangqing Tu",
            "Shulin Cao",
            "Daniel Zhang-Li",
            "Xin Lv",
            "Hao Peng",
            "Zijun Yao",
            "Xiaohan Zhang",
            "Hanming Li",
            "Chunyang Li",
            "Zheyuan Zhang",
            "Yushi Bai",
            "Yantao Liu",
            "Amy Xin",
            "Nianyi Lin",
            "Kaifeng Yun",
            "Linlu Gong",
            "Jianhui Chen",
            "Zhili Wu",
            "Yunjia Qi",
            "Weikai Li",
            "Yong Guan",
            "Kaisheng Zeng",
            "Ji Qi",
            "Hailong Jin",
            "Jinxin Liu",
            "Yu Gu",
            "Yuan Yao",
            "Ning Ding",
            "Lei Hou",
            "Zhiyuan Liu",
            "Bin Xu",
            "Jie Tang",
            "Juanzi Li"
        ],
        "published": "2023-06-15T17:20:46Z",
        "summary": "The unprecedented performance of large language models (LLMs) necessitates\nimprovements in evaluations. Rather than merely exploring the breadth of LLM\nabilities, we believe meticulous and thoughtful designs are essential to\nthorough, unbiased, and applicable evaluations. Given the importance of world\nknowledge to LLMs, we construct a Knowledge-oriented LLM Assessment benchmark\n(KoLA), in which we carefully design three crucial factors: (1) For ability\nmodeling, we mimic human cognition to form a four-level taxonomy of\nknowledge-related abilities, covering $19$ tasks. (2) For data, to ensure fair\ncomparisons, we use both Wikipedia, a corpus prevalently pre-trained by LLMs,\nalong with continuously collected emerging corpora, aiming to evaluate the\ncapacity to handle unseen data and evolving knowledge. (3) For evaluation\ncriteria, we adopt a contrastive system, including overall standard scores for\nbetter numerical comparability across tasks and models and a unique\nself-contrast metric for automatically evaluating knowledge hallucination. We\nevaluate $21$ open-source and commercial LLMs and obtain some intriguing\nfindings. The KoLA dataset and open-participation leaderboard are publicly\nreleased at https://kola.xlore.cn and will be continuously updated to provide\nreferences for developing LLMs and knowledge-related systems.",
        "pdf_link": "https://arxiv.org/pdf/2306.09296v2.pdf"
    },
    {
        "title": "SCALE: Scaling up the Complexity for Advanced Language Model Evaluation",
        "authors": [
            "Vishvaksenan Rasiah",
            "Ronja Stern",
            "Veton Matoshi",
            "Matthias Stürmer",
            "Ilias Chalkidis",
            "Daniel E. Ho",
            "Joel Niklaus"
        ],
        "published": "2023-06-15T16:19:15Z",
        "summary": "Recent strides in Large Language Models (LLMs) have saturated many NLP\nbenchmarks (even professional domain-specific ones), emphasizing the need for\nnovel, more challenging novel ones to properly assess LLM capabilities. In this\npaper, we introduce a novel NLP benchmark that poses challenges to current LLMs\nacross four key dimensions: processing long documents (up to 50K tokens),\nutilizing domain specific knowledge (embodied in legal texts), multilingual\nunderstanding (covering five languages), and multitasking (comprising legal\ndocument to document Information Retrieval, Court View Generation, Leading\nDecision Summarization, Citation Extraction, and eight challenging Text\nClassification tasks). Our benchmark comprises diverse legal NLP datasets from\nthe Swiss legal system, allowing for a comprehensive study of the underlying\nNon-English, inherently multilingual, federal legal system. Despite recent\nadvances, efficiently processing long documents for intense review/analysis\ntasks remains an open challenge for language models. Also, comprehensive,\ndomain-specific benchmarks requiring high expertise to develop are rare, as are\nmultilingual benchmarks. This scarcity underscores our contribution's value,\nconsidering most public models are trained predominantly on English corpora,\nwhile other languages remain understudied, particularly for practical\ndomain-specific NLP tasks. Our benchmark allows for testing and advancing the\nstate-of-the-art LLMs. As part of our study, we evaluate several pre-trained\nmultilingual language models on our benchmark to establish strong baselines as\na point of reference. Despite the large size of our datasets (tens to hundreds\nof thousands of examples), existing publicly available models struggle with\nmost tasks, even after in-domain pretraining. We publish all resources\n(benchmark suite, pre-trained models, code) under a fully permissive open CC\nBY-SA license.",
        "pdf_link": "https://arxiv.org/pdf/2306.09237v2.pdf"
    },
    {
        "title": "ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations",
        "authors": [
            "Hamideh Ghanadian",
            "Isar Nejadgholi",
            "Hussein Al Osman"
        ],
        "published": "2023-06-15T16:01:30Z",
        "summary": "This paper presents a novel framework for quantitatively evaluating the\ninteractive ChatGPT model in the context of suicidality assessment from social\nmedia posts, utilizing the University of Maryland Reddit suicidality dataset.\nWe conduct a technical evaluation of ChatGPT's performance on this task using\nZero-Shot and Few-Shot experiments and compare its results with those of two\nfine-tuned transformer-based models. Additionally, we investigate the impact of\ndifferent temperature parameters on ChatGPT's response generation and discuss\nthe optimal temperature based on the inconclusiveness rate of ChatGPT. Our\nresults indicate that while ChatGPT attains considerable accuracy in this task,\ntransformer-based models fine-tuned on human-annotated datasets exhibit\nsuperior performance. Moreover, our analysis sheds light on how adjusting the\nChatGPT's hyperparameters can improve its ability to assist mental health\nprofessionals in this critical task.",
        "pdf_link": "https://arxiv.org/pdf/2306.09390v1.pdf"
    },
    {
        "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
        "authors": [
            "Haonan Li",
            "Yixuan Zhang",
            "Fajri Koto",
            "Yifei Yang",
            "Hai Zhao",
            "Yeyun Gong",
            "Nan Duan",
            "Timothy Baldwin"
        ],
        "published": "2023-06-15T15:49:51Z",
        "summary": "As the capabilities of large language models (LLMs) continue to advance,\nevaluating their performance becomes increasingly crucial and challenging. This\npaper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese\nbenchmark that covers various subjects, including natural science, social\nsciences, engineering, and humanities. We conduct a thorough evaluation of 18\nadvanced multilingual- and Chinese-oriented LLMs, assessing their performance\nacross different subjects and settings. The results reveal that most existing\nLLMs struggle to achieve an average accuracy of 50%, even when provided with\nin-context examples and chain-of-thought prompts, whereas the random baseline\nstands at 25%. This highlights significant room for improvement in LLMs.\nAdditionally, we conduct extensive experiments to identify factors impacting\nthe models' performance and propose directions for enhancing LLMs. CMMLU fills\nthe gap in evaluating the knowledge and reasoning capabilities of large\nlanguage models within the Chinese context.",
        "pdf_link": "https://arxiv.org/pdf/2306.09212v2.pdf"
    },
    {
        "title": "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration",
        "authors": [
            "Chenyang Lyu",
            "Minghao Wu",
            "Longyue Wang",
            "Xinting Huang",
            "Bingshuai Liu",
            "Zefeng Du",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-06-15T12:45:25Z",
        "summary": "Although instruction-tuned large language models (LLMs) have exhibited\nremarkable capabilities across various NLP tasks, their effectiveness on other\ndata modalities beyond text has not been fully studied. In this work, we\npropose Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual,\naudio, and textual information. Macaw-LLM consists of three main components: a\nmodality module for encoding multi-modal data, a cognitive module for\nharnessing pretrained LLMs, and an alignment module for harmonizing diverse\nrepresentations. Our novel alignment module seamlessly bridges multi-modal\nfeatures to textual features, simplifying the adaptation process from the\nmodality modules to the cognitive module. In addition, we construct a\nlarge-scale multi-modal instruction dataset in terms of multi-turn dialogue,\nincluding 69K image instances and 50K video instances. We have made our data,\ncode and model publicly available, which we hope can pave the way for future\nresearch in multi-modal LLMs and expand the capabilities of LLMs to handle\ndiverse data modalities and address complex real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.09093v1.pdf"
    },
    {
        "title": "DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning",
        "authors": [
            "Hengli Li",
            "Song-Chun Zhu",
            "Zilong Zheng"
        ],
        "published": "2023-06-15T10:41:23Z",
        "summary": "Pragmatic reasoning plays a pivotal role in deciphering implicit meanings\nthat frequently arise in real-life conversations and is essential for the\ndevelopment of communicative social agents. In this paper, we introduce a novel\nchallenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic\nreasoning and situated conversational understanding. Compared with previous\nworks that treat different figurative expressions (e.g. metaphor, sarcasm) as\nindividual tasks, DiPlomat provides a cohesive framework towards general\npragmatic understanding. Our dataset is created through the utilization of\nAmazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn\ndialogues. In conjunction with the dataset, we propose two tasks, Pragmatic\nIdentification and Reasoning (PIR) and Conversational Question Answering (CQA).\nExperimental results with state-of-the-art (SOTA) neural architectures reveal\nseveral significant findings: 1) large language models ( LLMs) exhibit poor\nperformance in tackling this subjective domain; 2) comprehensive comprehension\nof context emerges as a critical factor for establishing benign human-machine\ninteractions; 3) current models defect in the application of pragmatic\nreasoning. As a result, we call on more attention to improve the ability of\ncontext understanding, reasoning, and implied meaning modeling.",
        "pdf_link": "https://arxiv.org/pdf/2306.09030v2.pdf"
    },
    {
        "title": "Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation",
        "authors": [
            "Zihui Gu",
            "Ju Fan",
            "Nan Tang",
            "Songyue Zhang",
            "Yuxin Zhang",
            "Zui Chen",
            "Lei Cao",
            "Guoliang Li",
            "Sam Madden",
            "Xiaoyong Du"
        ],
        "published": "2023-06-15T06:50:51Z",
        "summary": "Zero-shot NL2SQL is crucial in achieving natural language to SQL that is\nadaptive to new environments (e.g., new databases, new linguistic phenomena or\nSQL structures) with zero annotated NL2SQL samples from such environments.\nExisting approaches either fine-tune pre-trained language models (PLMs) based\non annotated data or use prompts to guide fixed large language models (LLMs)\nsuch as ChatGPT. PLMs can perform well in schema alignment but struggle to\nachieve complex reasoning, while LLMs is superior in complex reasoning tasks\nbut cannot achieve precise schema alignment. In this paper, we propose a\nZeroNL2SQL framework that combines the complementary advantages of PLMs and\nLLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an\nSQL sketch via schema alignment, then uses LLMs to fill the missing information\nvia complex reasoning. Moreover, in order to better align the generated SQL\nqueries with values in the given database instances, we design a predicate\ncalibration method to guide the LLM in completing the SQL sketches based on the\ndatabase instances and select the optimal SQL query via an execution-based\nstrategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best\nzero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL\noutperforms the state-of-the-art PLM-based methods by 3.2% to 13% and exceeds\nLLM-based methods by 10% to 20% on execution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2306.08891v1.pdf"
    },
    {
        "title": "Toward Grounded Commonsense Reasoning",
        "authors": [
            "Minae Kwon",
            "Hengyuan Hu",
            "Vivek Myers",
            "Siddharth Karamcheti",
            "Anca Dragan",
            "Dorsa Sadigh"
        ],
        "published": "2023-06-14T17:30:57Z",
        "summary": "Consider a robot tasked with tidying a desk with a meticulously constructed\nLego sports car. A human may recognize that it is not appropriate to\ndisassemble the sports car and put it away as part of the \"tidying.\" How can a\nrobot reach that conclusion? Although large language models (LLMs) have\nrecently been used to enable commonsense reasoning, grounding this reasoning in\nthe real world has been challenging. To reason in the real world, robots must\ngo beyond passively querying LLMs and actively gather information from the\nenvironment that is required to make the right decision. For instance, after\ndetecting that there is an occluded car, the robot may need to actively\nperceive the car to know whether it is an advanced model car made out of Legos\nor a toy car built by a toddler. We propose an approach that leverages an LLM\nand vision language model (VLM) to help a robot actively perceive its\nenvironment to perform grounded commonsense reasoning. To evaluate our\nframework at scale, we release the MessySurfaces dataset which contains images\nof 70 real-world surfaces that need to be cleaned. We additionally illustrate\nour approach with a robot on 2 carefully designed surfaces. We find an average\n12.9% improvement on the MessySurfaces benchmark and an average 15% improvement\non the robot experiments over baselines that do not use active perception. The\ndataset, code, and videos of our approach can be found at\nhttps://minaek.github.io/grounded_commonsense_reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2306.08651v2.pdf"
    },
    {
        "title": "Language to Rewards for Robotic Skill Synthesis",
        "authors": [
            "Wenhao Yu",
            "Nimrod Gileadi",
            "Chuyuan Fu",
            "Sean Kirmani",
            "Kuang-Huei Lee",
            "Montse Gonzalez Arenas",
            "Hao-Tien Lewis Chiang",
            "Tom Erez",
            "Leonard Hasenclever",
            "Jan Humplik",
            "Brian Ichter",
            "Ted Xiao",
            "Peng Xu",
            "Andy Zeng",
            "Tingnan Zhang",
            "Nicolas Heess",
            "Dorsa Sadigh",
            "Jie Tan",
            "Yuval Tassa",
            "Fei Xia"
        ],
        "published": "2023-06-14T17:27:10Z",
        "summary": "Large language models (LLMs) have demonstrated exciting progress in acquiring\ndiverse new capabilities through in-context learning, ranging from logical\nreasoning to code-writing. Robotics researchers have also explored using LLMs\nto advance the capabilities of robotic control. However, since low-level robot\nactions are hardware-dependent and underrepresented in LLM training corpora,\nexisting efforts in applying LLMs to robotics have largely treated LLMs as\nsemantic planners or relied on human-engineered control primitives to interface\nwith the robot. On the other hand, reward functions are shown to be flexible\nrepresentations that can be optimized for control policies to achieve diverse\ntasks, while their semantic richness makes them suitable to be specified by\nLLMs. In this work, we introduce a new paradigm that harnesses this realization\nby utilizing LLMs to define reward parameters that can be optimized and\naccomplish variety of robotic tasks. Using reward as the intermediate interface\ngenerated by LLMs, we can effectively bridge the gap between high-level\nlanguage instructions or corrections to low-level robot actions. Meanwhile,\ncombining this with a real-time optimizer, MuJoCo MPC, empowers an interactive\nbehavior creation experience where users can immediately observe the results\nand provide feedback to the system. To systematically evaluate the performance\nof our proposed method, we designed a total of 17 tasks for a simulated\nquadruped robot and a dexterous manipulator robot. We demonstrate that our\nproposed method reliably tackles 90% of the designed tasks, while a baseline\nusing primitive skills as the interface with Code-as-policies achieves 50% of\nthe tasks. We further validated our method on a real robot arm where complex\nmanipulation skills such as non-prehensile pushing emerge through our\ninteractive system.",
        "pdf_link": "https://arxiv.org/pdf/2306.08647v2.pdf"
    },
    {
        "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models",
        "authors": [
            "Lingxi Xie",
            "Longhui Wei",
            "Xiaopeng Zhang",
            "Kaifeng Bi",
            "Xiaotao Gu",
            "Jianlong Chang",
            "Qi Tian"
        ],
        "published": "2023-06-14T17:15:01Z",
        "summary": "The AI community has been pursuing algorithms known as artificial general\nintelligence (AGI) that apply to any kind of real-world problem. Recently, chat\nsystems powered by large language models (LLMs) emerge and rapidly become a\npromising direction to achieve AGI in natural language processing (NLP), but\nthe path towards AGI in computer vision (CV) remains unclear. One may owe the\ndilemma to the fact that visual signals are more complex than language signals,\nyet we are interested in finding concrete reasons, as well as absorbing\nexperiences from GPT and LLMs to solve the problem. In this paper, we start\nwith a conceptual definition of AGI and briefly review how NLP solves a wide\nrange of tasks via a chat system. The analysis inspires us that unification is\nthe next important goal of CV. But, despite various efforts in this direction,\nCV is still far from a system like GPT that naturally integrates all tasks. We\npoint out that the essential weakness of CV lies in lacking a paradigm to learn\nfrom environments, yet NLP has accomplished the task in the text world. We then\nimagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale,\ninteractable environments, pre-trains it to predict future frames with respect\nto its action, and then fine-tunes it with instruction to accomplish various\ntasks. We expect substantial research and engineering efforts to push the idea\nforward and scale it up, for which we share our perspectives on future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2306.08641v1.pdf"
    },
    {
        "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",
        "authors": [
            "Difei Gao",
            "Lei Ji",
            "Luowei Zhou",
            "Kevin Qinghong Lin",
            "Joya Chen",
            "Zihan Fan",
            "Mike Zheng Shou"
        ],
        "published": "2023-06-14T17:12:56Z",
        "summary": "Recent research on Large Language Models (LLMs) has led to remarkable\nadvancements in general NLP AI assistants. Some studies have further explored\nthe use of LLMs for planning and invoking models or APIs to address more\ngeneral multi-modal user queries. Despite this progress, complex visual-based\ntasks still remain challenging due to the diverse nature of visual tasks. This\ndiversity is reflected in two aspects: 1) Reasoning paths. For many real-life\napplications, it is hard to accurately decompose a query simply by examining\nthe query itself. Planning based on the specific visual content and the results\nof each step is usually required. 2) Flexible inputs and intermediate results.\nInput forms could be flexible for in-the-wild cases, and involves not only a\nsingle image or video but a mixture of videos and images, e.g., a user-view\nimage with some reference videos. Besides, a complex reasoning process will\nalso generate diverse multimodal intermediate results, e.g., video narrations,\nsegmented video clips, etc. To address such general cases, we propose a\nmulti-modal AI assistant, AssistGPT, with an interleaved code and language\nreasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate\nLLMs with various tools. Specifically, the Planner is capable of using natural\nlanguage to plan which tool in Executor should do next based on the current\nreasoning progress. Inspector is an efficient memory manager to assist the\nPlanner to feed proper visual information into a specific tool. Finally, since\nthe entire reasoning process is complex and flexible, a Learner is designed to\nenable the model to autonomously explore and discover the optimal solution. We\nconducted experiments on A-OKVQA and NExT-QA benchmarks, achieving\nstate-of-the-art results. Moreover, showcases demonstrate the ability of our\nsystem to handle questions far more complex than those found in the benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2306.08640v2.pdf"
    },
    {
        "title": "MiniLLM: Knowledge Distillation of Large Language Models",
        "authors": [
            "Yuxian Gu",
            "Li Dong",
            "Furu Wei",
            "Minlie Huang"
        ],
        "published": "2023-06-14T14:44:03Z",
        "summary": "Knowledge Distillation (KD) is a promising technique for reducing the high\ncomputational demand of large language models (LLMs). However, previous KD\nmethods are primarily applied to white-box classification models or training\nsmall models to imitate black-box model APIs like ChatGPT. How to effectively\ndistill the knowledge of white-box LLMs into small models is still\nunder-explored, which becomes more important with the prosperity of open-source\nLLMs. In this work, we propose a KD approach that distills LLMs into smaller\nlanguage models. We first replace the forward Kullback-Leibler divergence (KLD)\nobjective in the standard KD approaches with reverse KLD, which is more\nsuitable for KD on generative language models, to prevent the student model\nfrom overestimating the low-probability regions of the teacher distribution.\nThen, we derive an effective optimization approach to learn this objective. The\nstudent models are named MiniLLM. Extensive experiments in the\ninstruction-following setting show that MiniLLM generates more precise\nresponses with higher overall quality, lower exposure bias, better calibration,\nand higher long-text generation performance than the baselines. Our method is\nscalable for different model families with 120M to 13B parameters. Our code,\ndata, and model checkpoints can be found in\nhttps://github.com/microsoft/LMOps/tree/main/minillm.",
        "pdf_link": "https://arxiv.org/pdf/2306.08543v4.pdf"
    },
    {
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
        "authors": [
            "Shirui Pan",
            "Linhao Luo",
            "Yufei Wang",
            "Chen Chen",
            "Jiapu Wang",
            "Xindong Wu"
        ],
        "published": "2023-06-14T07:15:26Z",
        "summary": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves\nin the field of natural language processing and artificial intelligence, due to\ntheir emergent ability and generalizability. However, LLMs are black-box\nmodels, which often fall short of capturing and accessing factual knowledge. In\ncontrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are\nstructured knowledge models that explicitly store rich factual knowledge. KGs\ncan enhance LLMs by providing external knowledge for inference and\ninterpretability. Meanwhile, KGs are difficult to construct and evolving by\nnature, which challenges the existing methods in KGs to generate new facts and\nrepresent unseen knowledge. Therefore, it is complementary to unify LLMs and\nKGs together and simultaneously leverage their advantages. In this article, we\npresent a forward-looking roadmap for the unification of LLMs and KGs. Our\nroadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,\nwhich incorporate KGs during the pre-training and inference phases of LLMs, or\nfor the purpose of enhancing understanding of the knowledge learned by LLMs; 2)\nLLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,\ncompletion, construction, graph-to-text generation, and question answering; and\n3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a\nmutually beneficial way to enhance both LLMs and KGs for bidirectional\nreasoning driven by both data and knowledge. We review and summarize existing\nefforts within these three frameworks in our roadmap and pinpoint their future\nresearch directions.",
        "pdf_link": "https://arxiv.org/pdf/2306.08302v3.pdf"
    },
    {
        "title": "Language models are not naysayers: An analysis of language models on negation benchmarks",
        "authors": [
            "Thinh Hung Truong",
            "Timothy Baldwin",
            "Karin Verspoor",
            "Trevor Cohn"
        ],
        "published": "2023-06-14T01:16:37Z",
        "summary": "Negation has been shown to be a major bottleneck for masked language models,\nsuch as BERT. However, whether this finding still holds for larger-sized\nauto-regressive language models (``LLMs'') has not been studied\ncomprehensively. With the ever-increasing volume of research and applications\nof LLMs, we take a step back to evaluate the ability of current-generation LLMs\nto handle negation, a fundamental linguistic phenomenon that is central to\nlanguage understanding. We evaluate different LLMs -- including the open-source\nGPT-neo, GPT-3, and InstructGPT -- against a wide range of negation benchmarks.\nThrough systematic experimentation with varying model sizes and prompts, we\nshow that LLMs have several limitations including insensitivity to the presence\nof negation, an inability to capture the lexical semantics of negation, and a\nfailure to reason under negation.",
        "pdf_link": "https://arxiv.org/pdf/2306.08189v1.pdf"
    },
    {
        "title": "INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation",
        "authors": [
            "Yuji Chai",
            "John Gkountouras",
            "Glenn G. Ko",
            "David Brooks",
            "Gu-Yeon Wei"
        ],
        "published": "2023-06-13T22:25:35Z",
        "summary": "We introduce a method that dramatically reduces fine-tuning VRAM requirements\nand rectifies quantization errors in quantized Large Language Models. First, we\ndevelop an extremely memory-efficient fine-tuning (EMEF) method for quantized\nmodels using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an\nerror-correcting algorithm designed to minimize errors induced by the\nquantization process. Our method reduces the memory requirements by up to 5.6\ntimes, which enables fine-tuning a 7 billion parameter Large Language Model\n(LLM) on consumer laptops. At the same time, we propose a Low-Rank Error\nCorrection (LREC) method that exploits the added LoRA layers to ameliorate the\ngap between the quantized model and its float point counterpart. Our error\ncorrection framework leads to a fully functional INT2 quantized LLM with the\ncapacity to generate coherent English text. To the best of our knowledge, this\nis the first INT2 Large Language Model that has been able to reach such a\nperformance. The overhead of our method is merely a 1.05 times increase in\nmodel size, which translates to an effective precision of INT2.1. Also, our\nmethod readily generalizes to other quantization standards, such as INT3, INT4,\nand INT8, restoring their lost performance, which marks a significant milestone\nin the field of model quantization. The strategies delineated in this paper\nhold promising implications for the future development and optimization of\nquantized models, marking a pivotal shift in the landscape of low-resource\nmachine learning computations.",
        "pdf_link": "https://arxiv.org/pdf/2306.08162v1.pdf"
    },
    {
        "title": "Large-scale Language Model Rescoring on Long-form Data",
        "authors": [
            "Tongzhou Chen",
            "Cyril Allauzen",
            "Yinghui Huang",
            "Daniel Park",
            "David Rybach",
            "W. Ronny Huang",
            "Rodrigo Cabrera",
            "Kartik Audhkhasi",
            "Bhuvana Ramabhadran",
            "Pedro J. Moreno",
            "Michael Riley"
        ],
        "published": "2023-06-13T20:54:12Z",
        "summary": "In this work, we study the impact of Large-scale Language Models (LLM) on\nAutomated Speech Recognition (ASR) of YouTube videos, which we use as a source\nfor long-form ASR. We demonstrate up to 8\\% relative reduction in Word Error\nEate (WER) on US English (en-us) and code-switched Indian English (en-in)\nlong-form ASR test sets and a reduction of up to 30\\% relative on Salient Term\nError Rate (STER) over a strong first-pass baseline that uses a maximum-entropy\nbased language model. Improved lattice processing that results in a lattice\nwith a proper (non-tree) digraph topology and carrying context from the 1-best\nhypothesis of the previous segment(s) results in significant wins in rescoring\nwith LLMs. We also find that the gains in performance from the combination of\nLLMs trained on vast quantities of available data (such as C4) and conventional\nneural LMs is additive and significantly outperforms a strong first-pass\nbaseline with a maximum entropy LM.\n  Copyright 2023 IEEE. Personal use of this material is permitted. Permission\nfrom IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to\nservers or lists, or reuse of any copyrighted component of this work in other\nworks.",
        "pdf_link": "https://arxiv.org/pdf/2306.08133v2.pdf"
    },
    {
        "title": "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent",
        "authors": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Chen Sun",
            "Kai-Wei Chang",
            "Yizhou Sun",
            "David A Ross",
            "Cordelia Schmid",
            "Alireza Fathi"
        ],
        "published": "2023-06-13T20:50:22Z",
        "summary": "In this paper, we propose an autonomous information seeking visual question\nanswering framework, AVIS. Our method leverages a Large Language Model (LLM) to\ndynamically strategize the utilization of external tools and to investigate\ntheir outputs, thereby acquiring the indispensable knowledge needed to provide\nanswers to the posed questions. Responding to visual questions that necessitate\nexternal knowledge, such as \"What event is commemorated by the building\ndepicted in this image?\", is a complex task. This task presents a combinatorial\nsearch space that demands a sequence of actions, including invoking APIs,\nanalyzing their responses, and making informed decisions. We conduct a user\nstudy to collect a variety of instances of human decision-making when faced\nwith this task. This data is then used to design a system comprised of three\ncomponents: an LLM-powered planner that dynamically determines which tool to\nuse next, an LLM-powered reasoner that analyzes and extracts key information\nfrom the tool outputs, and a working memory component that retains the acquired\ninformation throughout the process. The collected user behavior serves as a\nguide for our system in two key ways. First, we create a transition graph by\nanalyzing the sequence of decisions made by users. This graph delineates\ndistinct states and confines the set of actions available at each state.\nSecond, we use examples of user decision-making to provide our LLM-powered\nplanner and reasoner with relevant contextual instances, enhancing their\ncapacity to make informed decisions. We show that AVIS achieves\nstate-of-the-art results on knowledge-intensive visual question answering\nbenchmarks such as Infoseek and OK-VQA.",
        "pdf_link": "https://arxiv.org/pdf/2306.08129v3.pdf"
    },
    {
        "title": "Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level",
        "authors": [
            "Mujahid Ali Quidwai",
            "Chunhui Li",
            "Parijat Dube"
        ],
        "published": "2023-06-13T20:34:55Z",
        "summary": "The increasing reliance on large language models (LLMs) in academic writing\nhas led to a rise in plagiarism. Existing AI-generated text classifiers have\nlimited accuracy and often produce false positives. We propose a novel approach\nusing natural language processing (NLP) techniques, offering quantifiable\nmetrics at both sentence and document levels for easier interpretation by human\nevaluators. Our method employs a multi-faceted approach, generating multiple\nparaphrased versions of a given question and inputting them into the LLM to\ngenerate answers. By using a contrastive loss function based on cosine\nsimilarity, we match generated sentences with those from the student's\nresponse. Our approach achieves up to 94% accuracy in classifying human and AI\ntext, providing a robust and adaptable solution for plagiarism detection in\nacademic settings. This method improves with LLM advancements, reducing the\nneed for new model training or reconfiguration, and offers a more transparent\nway of evaluating and detecting AI-generated text.",
        "pdf_link": "https://arxiv.org/pdf/2306.08122v1.pdf"
    },
    {
        "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
        "authors": [
            "Alexander Tornede",
            "Difan Deng",
            "Theresa Eimer",
            "Joseph Giovanelli",
            "Aditya Mohan",
            "Tim Ruhkopf",
            "Sarah Segel",
            "Daphne Theodorakopoulos",
            "Tanja Tornede",
            "Henning Wachsmuth",
            "Marius Lindauer"
        ],
        "published": "2023-06-13T19:51:22Z",
        "summary": "The fields of both Natural Language Processing (NLP) and Automated Machine\nLearning (AutoML) have achieved remarkable results over the past years. In NLP,\nespecially Large Language Models (LLMs) have experienced a rapid series of\nbreakthroughs very recently. We envision that the two fields can radically push\nthe boundaries of each other through tight integration. To showcase this\nvision, we explore the potential of a symbiotic relationship between AutoML and\nLLMs, shedding light on how they can benefit each other. In particular, we\ninvestigate both the opportunities to enhance AutoML approaches with LLMs from\ndifferent perspectives and the challenges of leveraging AutoML to further\nimprove LLMs. To this end, we survey existing work, and we critically assess\nrisks. We strongly believe that the integration of the two fields has the\npotential to disrupt both fields, NLP and AutoML. By highlighting conceivable\nsynergies, but also risks, we aim to foster further exploration at the\nintersection of AutoML and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.08107v3.pdf"
    },
    {
        "title": "Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning",
        "authors": [
            "Michael Villarreal",
            "Bibek Poudel",
            "Weizi Li"
        ],
        "published": "2023-06-13T19:27:18Z",
        "summary": "The surge in Reinforcement Learning (RL) applications in Intelligent\nTransportation Systems (ITS) has contributed to its growth as well as\nhighlighted key challenges. However, defining objectives of RL agents in\ntraffic control and management tasks, as well as aligning policies with these\ngoals through an effective formulation of Markov Decision Process (MDP), can be\nchallenging and often require domain experts in both RL and ITS. Recent\nadvancements in Large Language Models (LLMs) such as GPT-4 highlight their\nbroad general knowledge, reasoning capabilities, and commonsense priors across\nvarious domains. In this work, we conduct a large-scale user study involving 70\nparticipants to investigate whether novices can leverage ChatGPT to solve\ncomplex mixed traffic control problems. Three environments are tested,\nincluding ring road, bottleneck, and intersection. We find ChatGPT has mixed\nresults. For intersection and bottleneck, ChatGPT increases number of\nsuccessful policies by 150% and 136% compared to solely beginner capabilities,\nwith some of them even outperforming experts. However, ChatGPT does not provide\nconsistent improvements across all scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.08094v2.pdf"
    },
    {
        "title": "FLamE: Few-shot Learning from Natural Language Explanations",
        "authors": [
            "Yangqiaoyu Zhou",
            "Yiming Zhang",
            "Chenhao Tan"
        ],
        "published": "2023-06-13T18:01:46Z",
        "summary": "Natural language explanations have the potential to provide rich information\nthat in principle guides model reasoning. Yet, recent work by Lampinen et al.\n(2022) has shown limited utility of natural language explanations in improving\nclassification. To effectively learn from explanations, we present FLamE, a\ntwo-stage few-shot learning framework that first generates explanations using\nGPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated\nexplanations. Our experiments on natural language inference demonstrate\neffectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3\nBabbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification\nperformance, human evaluation surprisingly reveals that the majority of\ngenerated explanations does not adequately justify classification decisions.\nAdditional analyses point to the important role of label-specific cues (e.g.,\n\"not know\" for the neutral label) in generated explanations.",
        "pdf_link": "https://arxiv.org/pdf/2306.08042v1.pdf"
    },
    {
        "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models",
        "authors": [
            "Omkar Thawkar",
            "Abdelrahman Shaker",
            "Sahal Shaji Mullappilly",
            "Hisham Cholakkal",
            "Rao Muhammad Anwer",
            "Salman Khan",
            "Jorma Laaksonen",
            "Fahad Shahbaz Khan"
        ],
        "published": "2023-06-13T17:59:59Z",
        "summary": "The latest breakthroughs in large vision-language models, such as Bard and\nGPT-4, have showcased extraordinary abilities in performing a wide range of\ntasks. Such models are trained on massive datasets comprising billions of\npublic image-text pairs with diverse tasks. However, their performance on\ntask-specific domains, such as radiology, is still under-investigated and\npotentially limited due to a lack of sophistication in understanding biomedical\nimages. On the other hand, conversational medical models have exhibited\nremarkable success but have mainly focused on text-based analysis. In this\npaper, we introduce XrayGPT, a novel conversational medical vision-language\nmodel that can analyze and answer open-ended questions about chest radiographs.\nSpecifically, we align both medical visual encoder (MedClip) with a fine-tuned\nlarge language model (Vicuna), using a simple linear transformation. This\nalignment enables our model to possess exceptional visual conversation\nabilities, grounded in a deep understanding of radiographs and medical domain\nknowledge. To enhance the performance of LLMs in the medical context, we\ngenerate ~217k interactive and high-quality summaries from free-text radiology\nreports. These summaries serve to enhance the performance of LLMs through the\nfine-tuning process. Our approach opens up new avenues the research for\nadvancing the automated analysis of chest radiographs. Our open-source demos,\nmodels, and instruction sets are available at:\nhttps://github.com/mbzuai-oryx/XrayGPT.",
        "pdf_link": "https://arxiv.org/pdf/2306.07971v1.pdf"
    },
    {
        "title": "arXiVeri: Automatic table verification with GPT",
        "authors": [
            "Gyungin Shin",
            "Weidi Xie",
            "Samuel Albanie"
        ],
        "published": "2023-06-13T17:59:57Z",
        "summary": "Without accurate transcription of numerical data in scientific documents, a\nscientist cannot draw accurate conclusions. Unfortunately, the process of\ncopying numerical data from one paper to another is prone to human error. In\nthis paper, we propose to meet this challenge through the novel task of\nautomatic table verification (AutoTV), in which the objective is to verify the\naccuracy of numerical data in tables by cross-referencing cited sources. To\nsupport this task, we propose a new benchmark, arXiVeri, which comprises\ntabular data drawn from open-access academic papers on arXiv. We introduce\nmetrics to evaluate the performance of a table verifier in two key areas: (i)\ntable matching, which aims to identify the source table in a cited document\nthat corresponds to a target table, and (ii) cell matching, which aims to\nlocate shared cells between a target and source table and identify their row\nand column indices accurately. By leveraging the flexible capabilities of\nmodern large language models (LLMs), we propose simple baselines for table\nverification. Our findings highlight the complexity of this task, even for\nstate-of-the-art LLMs like OpenAI's GPT-4. The code and benchmark will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2306.07968v1.pdf"
    },
    {
        "title": "Questioning the Survey Responses of Large Language Models",
        "authors": [
            "Ricardo Dominguez-Olmedo",
            "Moritz Hardt",
            "Celestine Mendler-Dünner"
        ],
        "published": "2023-06-13T17:48:27Z",
        "summary": "As large language models increase in capability, researchers have started to\nconduct surveys of all kinds on these models in order to investigate the\npopulation represented by their responses. In this work, we critically examine\nlanguage models' survey responses on the basis of the well-established American\nCommunity Survey by the U.S. Census Bureau and investigate whether they elicit\na faithful representations of any human population. Using a de-facto standard\nmultiple-choice prompting technique and evaluating 39 different language models\nusing systematic experiments, we establish two dominant patterns: First,\nmodels' responses are governed by ordering and labeling biases, leading to\nvariations across models that do not persist after adjusting for systematic\nbiases. Second, models' responses do not contain the entropy variations and\nstatistical signals typically found in human populations. As a result, a binary\nclassifier can almost perfectly differentiate model-generated data from the\nresponses of the U.S. census. At the same time, models' relative alignment with\ndifferent demographic subgroups can be predicted from the subgroups' entropy,\nirrespective of the model's training data or training strategy. Taken together,\nour findings suggest caution in treating models' survey responses as equivalent\nto those of human populations.",
        "pdf_link": "https://arxiv.org/pdf/2306.07951v3.pdf"
    },
    {
        "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences",
        "authors": [
            "Xiao Liu",
            "Hanyu Lai",
            "Hao Yu",
            "Yifan Xu",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Peng Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-06-13T16:57:53Z",
        "summary": "We present WebGLM, a web-enhanced question-answering system based on the\nGeneral Language Model (GLM). Its goal is to augment a pre-trained large\nlanguage model (LLM) with web search and retrieval capabilities while being\nefficient for real-world deployments. To achieve this, we develop WebGLM with\nstrategies for the LLM-augmented retriever, bootstrapped generator, and human\npreference-aware scorer. Specifically, we identify and address the limitations\nof WebGPT (OpenAI), through which WebGLM is enabled with accuracy, efficiency,\nand cost-effectiveness advantages. In addition, we propose systematic criteria\nfor evaluating web-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the outperformance\nof the proposed WebGLM designs over existing systems. WebGLM with the\n10-billion-parameter GLM (10B) is shown to perform better than the\nsimilar-sized WebGPT (13B) and even comparably to WebGPT (175B) in human\nevaluation. The code, demo, and data are at\n\\url{https://github.com/THUDM/WebGLM}.",
        "pdf_link": "https://arxiv.org/pdf/2306.07906v1.pdf"
    },
    {
        "title": "Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks",
        "authors": [
            "Veniamin Veselovsky",
            "Manoel Horta Ribeiro",
            "Robert West"
        ],
        "published": "2023-06-13T16:46:24Z",
        "summary": "Large language models (LLMs) are remarkable data annotators. They can be used\nto generate high-fidelity supervised training data, as well as survey and\nexperimental data. With the widespread adoption of LLMs, human gold--standard\nannotations are key to understanding the capabilities of LLMs and the validity\nof their results. However, crowdsourcing, an important, inexpensive way to\nobtain human annotations, may itself be impacted by LLMs, as crowd workers have\nfinancial incentives to use LLMs to increase their productivity and income. To\ninvestigate this concern, we conducted a case study on the prevalence of LLM\nusage by crowd workers. We reran an abstract summarization task from the\nliterature on Amazon Mechanical Turk and, through a combination of keystroke\ndetection and synthetic text classification, estimate that 33-46% of crowd\nworkers used LLMs when completing the task. Although generalization to other,\nless LLM-friendly tasks is unclear, our results call for platforms,\nresearchers, and crowd workers to find new ways to ensure that human data\nremain human, perhaps using the methodology proposed here as a stepping stone.\nCode/data: https://github.com/epfl-dlab/GPTurk",
        "pdf_link": "https://arxiv.org/pdf/2306.07899v1.pdf"
    },
    {
        "title": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
        "authors": [
            "Longtao Zheng",
            "Rundong Wang",
            "Xinrun Wang",
            "Bo An"
        ],
        "published": "2023-06-13T15:49:41Z",
        "summary": "Building agents with large language models (LLMs) for computer control is a\nburgeoning research area, where the agent receives computer states and performs\nactions to complete complex tasks. Previous computer agents have demonstrated\nthe benefits of in-context learning (ICL); however, their performance is\nhindered by several issues. First, the limited context length of LLMs and\ncomplex computer states restrict the number of exemplars, as a single webpage\ncan consume the entire context. Second, the exemplars in current methods, such\nas high-level plans and multi-choice questions, cannot represent complete\ntrajectories, leading to suboptimal performance in long-horizon tasks. Third,\nexisting computer agents rely on task-specific exemplars and overlook the\nsimilarity among tasks, resulting in poor generalization to novel tasks. To\naddress these challenges, we introduce Synapse, a computer agent featuring\nthree key components: i) state abstraction, which filters out task-irrelevant\ninformation from raw states, allowing more exemplars within the limited\ncontext, ii) trajectory-as-exemplar prompting, which prompts the LLM with\ncomplete trajectories of the abstracted states and actions to improve\nmulti-step decision-making, and iii) exemplar memory, which stores the\nembeddings of exemplars and retrieves them via similarity search for\ngeneralization to novel tasks. We evaluate Synapse on MiniWoB++, a standard\ntask suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse\nachieves a 99.2% average success rate (a 10% relative improvement) across 64\ntasks using demonstrations from only 48 tasks. Notably, Synapse is the first\nICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a\n56% relative improvement in average step success rate over the previous\nstate-of-the-art prompting scheme in Mind2Web.",
        "pdf_link": "https://arxiv.org/pdf/2306.07863v3.pdf"
    },
    {
        "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
        "authors": [
            "Yin Fang",
            "Xiaozhuan Liang",
            "Ningyu Zhang",
            "Kangwei Liu",
            "Rui Huang",
            "Zhuo Chen",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "published": "2023-06-13T14:35:34Z",
        "summary": "Large Language Models (LLMs), with their remarkable task-handling\ncapabilities and innovative outputs, have catalyzed significant advancements\nacross a spectrum of fields. However, their proficiency within specialized\ndomains such as biomolecular studies remains limited. To address this\nchallenge, we introduce Mol-Instructions, a comprehensive instruction dataset\ndesigned for the biomolecular domain. Mol-Instructions encompasses three key\ncomponents: molecule-oriented instructions, protein-oriented instructions, and\nbiomolecular text instructions. Each component aims to improve the\nunderstanding and prediction capabilities of LLMs concerning biomolecular\nfeatures and behaviors. Through extensive instruction tuning experiments on\nLLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large\nmodels' performance in the intricate realm of biomolecular studies, thus\nfostering progress in the biomolecular research community. Mol-Instructions is\npublicly available for ongoing research and will undergo regular updates to\nenhance its applicability.",
        "pdf_link": "https://arxiv.org/pdf/2306.08018v5.pdf"
    },
    {
        "title": "NoCoLA: The Norwegian Corpus of Linguistic Acceptability",
        "authors": [
            "Matias Jentoft",
            "David Samuel"
        ],
        "published": "2023-06-13T14:11:19Z",
        "summary": "While there has been a surge of large language models for Norwegian in recent\nyears, we lack any tool to evaluate their understanding of grammaticality. We\npresent two new Norwegian datasets for this task. NoCoLA_class is a supervised\nbinary classification task where the goal is to discriminate between acceptable\nand non-acceptable sentences. On the other hand, NoCoLA_zero is a purely\ndiagnostic task for evaluating the grammatical judgement of a language model in\na completely zero-shot manner, i.e. without any further training. In this\npaper, we describe both datasets in detail, show how to use them for different\nflavors of language models, and conduct a comparative study of the existing\nNorwegian language models.",
        "pdf_link": "https://arxiv.org/pdf/2306.07790v1.pdf"
    },
    {
        "title": "SqueezeLLM: Dense-and-Sparse Quantization",
        "authors": [
            "Sehoon Kim",
            "Coleman Hooper",
            "Amir Gholami",
            "Zhen Dong",
            "Xiuyu Li",
            "Sheng Shen",
            "Michael W. Mahoney",
            "Kurt Keutzer"
        ],
        "published": "2023-06-13T08:57:54Z",
        "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable results\nfor a wide range of tasks. However, deploying these models for inference has\nbeen a significant challenge due to their unprecedented resource requirements.\nThis has forced existing deployment frameworks to use multi-GPU inference\npipelines, which are often complex and costly, or to use smaller and less\nperformant models. In this work, we demonstrate that the main bottleneck for\ngenerative inference with LLMs is memory bandwidth, rather than compute,\nspecifically for single batch inference. While quantization has emerged as a\npromising solution by representing model weights with reduced precision,\nprevious efforts have often resulted in notable performance degradation. To\naddress this, we introduce SqueezeLLM, a post-training quantization framework\nthat not only enables lossless compression to ultra-low precisions of up to\n3-bit, but also achieves higher quantization performance under the same memory\nconstraint. Our framework incorporates two novel ideas: (i) sensitivity-based\nnon-uniform quantization, which searches for the optimal bit precision\nassignment based on second-order information; and (ii) the Dense-and-Sparse\ndecomposition that stores outliers and sensitive weight values in an efficient\nsparse format. When applied to the LLaMA models, our 3-bit quantization\nsignificantly reduces the perplexity gap from the FP16 baseline by up to 2.1x\nas compared to the state-of-the-art methods with the same memory requirement.\nFurthermore, when deployed on an A6000 GPU, our quantized models achieve up to\n2.3x speedup compared to the baseline. Our code is open-sourced and available\nonline.",
        "pdf_link": "https://arxiv.org/pdf/2306.07629v3.pdf"
    },
    {
        "title": "Large Language Models Sometimes Generate Purely Negatively-Reinforced Text",
        "authors": [
            "Fabien Roger"
        ],
        "published": "2023-06-13T06:40:37Z",
        "summary": "When using adversarial training, it is common practice to train against the\nmost egregious failures. However, this might imply using examples with\nsensitive information (such as leaked passwords or security vulnerabilities) as\ntraining data. One might assume that language models trained with gradient\ndescent never generate text snippets which were only present in examples\nassociated with the lowest possible reward. In this paper, we show that this\nassumption is wrong: in some situations, large language models do learn from\nsuch negatively-reinforced examples. We present a specific training setup that\nenables Pythia-160M to guess passwords 13% more often than it would by guessing\nrandomly, despite only showing it these passwords on examples where the model\nis incentivized to not output these passwords. Our code is available at\nwww.github.com/FabienRoger/Learning-From-Negative-Examples",
        "pdf_link": "https://arxiv.org/pdf/2306.07567v2.pdf"
    },
    {
        "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
        "authors": [
            "Kush Bhatia",
            "Avanika Narayan",
            "Christopher De Sa",
            "Christopher Ré"
        ],
        "published": "2023-06-13T04:37:00Z",
        "summary": "Large language models (LLMs) exhibit in-context learning abilities which\nenable the same model to perform several tasks without any task-specific\ntraining. In contrast, traditional adaptation approaches, such as fine-tuning,\nmodify the underlying models for each specific task. In-context learning,\nhowever, consistently underperforms task-specific tuning approaches even when\npresented with the same examples. While most existing approaches (e.g., prompt\nengineering) focus on the LLM's learned representations to patch this\nperformance gap, our analysis actually reveal that LLM representations contain\nsufficient information to make good predictions. As such, we focus on the LLM's\nreasoning abilities and demonstrate that this performance gap exists due to\ntheir inability to perform simple probabilistic reasoning tasks. This raises an\nintriguing question: Are LLMs actually capable of learning how to reason in a\ntask-agnostic manner? We answer this in the affirmative and propose TART which\ngenerically improves an LLM's reasoning abilities using a synthetically trained\nTransformer-based reasoning module. TART trains this reasoning module in a\ntask-agnostic manner using only synthetic logistic regression tasks and\ncomposes it with an arbitrary real-world pre-trained model without any\nadditional training. With a single inference module, TART improves performance\nacross different model families (GPT-Neo, Pythia, BLOOM), model sizes (100M -\n6B), tasks (14 NLP binary classification tasks), and even across different\nmodalities (audio and vision). Additionally, on the RAFT Benchmark, TART\nimproves GPT-Neo (125M)'s performance such that it outperforms BLOOM (176B),\nand is within 4% of GPT-3 (175B). Our code and models are available at\nhttps://github.com/HazyResearch/TART .",
        "pdf_link": "https://arxiv.org/pdf/2306.07536v1.pdf"
    },
    {
        "title": "Assigning AI: Seven Approaches for Students, with Prompts",
        "authors": [
            "Ethan Mollick",
            "Lilach Mollick"
        ],
        "published": "2023-06-13T03:36:36Z",
        "summary": "This paper examines the transformative role of Large Language Models (LLMs)\nin education and their potential as learning tools, despite their inherent\nrisks and limitations. The authors propose seven approaches for utilizing AI in\nclassrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator,\nand AI-student, each with distinct pedagogical benefits and risks. The aim is\nto help students learn with and about AI, with practical strategies designed to\nmitigate risks such as complacency about the AI's output, errors, and biases.\nThese strategies promote active oversight, critical assessment of AI outputs,\nand complementarity of AI's capabilities with the students' unique insights. By\nchallenging students to remain the \"human in the loop,\" the authors aim to\nenhance learning outcomes while ensuring that AI serves as a supportive tool\nrather than a replacement. The proposed framework offers a guide for educators\nnavigating the integration of AI-assisted learning in classrooms",
        "pdf_link": "https://arxiv.org/pdf/2306.10052v1.pdf"
    },
    {
        "title": "Knowledge-Prompted Estimator: A Novel Approach to Explainable Machine Translation Assessment",
        "authors": [
            "Hao Yang",
            "Min Zhang",
            "Shimin Tao",
            "Minghan Wang",
            "Daimeng Wei",
            "Yanfei Jiang"
        ],
        "published": "2023-06-13T01:18:32Z",
        "summary": "Cross-lingual Machine Translation (MT) quality estimation plays a crucial\nrole in evaluating translation performance. GEMBA, the first MT quality\nassessment metric based on Large Language Models (LLMs), employs one-step\nprompting to achieve state-of-the-art (SOTA) in system-level MT quality\nestimation; however, it lacks segment-level analysis. In contrast,\nChain-of-Thought (CoT) prompting outperforms one-step prompting by offering\nimproved reasoning and explainability. In this paper, we introduce\nKnowledge-Prompted Estimator (KPE), a CoT prompting method that combines three\none-step prompting techniques, including perplexity, token-level similarity,\nand sentence-level similarity. This method attains enhanced performance for\nsegment-level estimation compared with previous deep learning models and\none-step prompting approaches. Furthermore, supplementary experiments on\nword-level visualized alignment demonstrate that our KPE method significantly\nimproves token alignment compared with earlier models and provides better\ninterpretability for MT quality estimation. Code will be released upon\npublication.",
        "pdf_link": "https://arxiv.org/pdf/2306.07486v1.pdf"
    },
    {
        "title": "Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling",
        "authors": [
            "Akshat Gupta"
        ],
        "published": "2023-06-12T19:20:18Z",
        "summary": "With their increasing size, large language models (LLMs) are becoming\nincreasingly good at language understanding tasks. But even with high\nperformance on specific downstream task, LLMs fail at simple linguistic tests\nfor negation or quantifier understanding. Previous work on quantifier\nunderstanding in LLMs show inverse scaling in understanding few-type\nquantifiers. In this paper, we question the claims of of previous work and show\nthat it is a result of inappropriate testing methodology. We also present\nalternate methods to measure quantifier comprehension in LLMs and show that\nLLMs are able to better understand the difference between the meaning of\nfew-type and most-type quantifiers as their size increases, although they are\nnot particularly good at it. We also observe inverse scaling for most-type\nquantifier understanding, which is contrary to human psycho-linguistic\nexperiments and previous work, where the model's understanding of most-type\nquantifier gets worse as the model size increases. We do this evaluation on\nmodels ranging from 125M-175B parameters, which suggests that LLMs do not do as\nwell as expected with quantifiers. We also discuss the possible reasons for\nthis and the relevance of quantifier understanding in evaluating language\nunderstanding in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.07384v3.pdf"
    },
    {
        "title": "Lost in Translation: Large Language Models in Non-English Content Analysis",
        "authors": [
            "Gabriel Nicholas",
            "Aliya Bhatia"
        ],
        "published": "2023-06-12T19:10:47Z",
        "summary": "In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa,\nGoogle's PaLM) have become the dominant approach for building AI systems to\nanalyze and generate language online. However, the automated systems that\nincreasingly mediate our interactions online -- such as chatbots, content\nmoderation systems, and search engines -- are primarily designed for and work\nfar more effectively in English than in the world's other 7,000 languages.\nRecently, researchers and technology companies have attempted to extend the\ncapabilities of large language models into languages other than English by\nbuilding what are called multilingual language models.\n  In this paper, we explain how these multilingual language models work and\nexplore their capabilities and limits. Part I provides a simple technical\nexplanation of how large language models work, why there is a gap in available\ndata between English and other languages, and how multilingual language models\nattempt to bridge that gap. Part II accounts for the challenges of doing\ncontent analysis with large language models in general and multilingual\nlanguage models in particular. Part III offers recommendations for companies,\nresearchers, and policymakers to keep in mind when considering researching,\ndeveloping and deploying large and multilingual language models.",
        "pdf_link": "https://arxiv.org/pdf/2306.07377v1.pdf"
    },
    {
        "title": "Waffling around for Performance: Visual Classification with Random Words and Broad Concepts",
        "authors": [
            "Karsten Roth",
            "Jae Myung Kim",
            "A. Sophia Koepke",
            "Oriol Vinyals",
            "Cordelia Schmid",
            "Zeynep Akata"
        ],
        "published": "2023-06-12T17:59:48Z",
        "summary": "The visual classification performance of vision-language models such as CLIP\nhas been shown to benefit from additional semantic knowledge from large\nlanguage models (LLMs) such as GPT-3. In particular, averaging over\nLLM-generated class descriptors, e.g. \"waffle, which has a round shape\", can\nnotably improve generalization performance. In this work, we critically study\nthis behavior and propose WaffleCLIP, a framework for zero-shot visual\nclassification which simply replaces LLM-generated descriptors with random\ncharacter and word descriptors. Without querying external models, we achieve\ncomparable performance gains on a large number of visual classification tasks.\nThis allows WaffleCLIP to both serve as a low-cost alternative, as well as a\nsanity check for any future LLM-based vision-language model extensions. We\nconduct an extensive experimental study on the impact and shortcomings of\nadditional semantics introduced with LLM-generated descriptors, and showcase\nhow - if available - semantic context is better leveraged by querying LLMs for\nhigh-level concepts, which we show can be done to jointly resolve potential\nclass name ambiguities. Code is available here:\nhttps://github.com/ExplainableML/WaffleCLIP.",
        "pdf_link": "https://arxiv.org/pdf/2306.07282v2.pdf"
    },
    {
        "title": "Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow",
        "authors": [
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Yueting Zhuang"
        ],
        "published": "2023-06-12T16:12:56Z",
        "summary": "Various industries such as finance, meteorology, and energy generate vast\namounts of heterogeneous data every day. There is a natural demand for humans\nto manage, process, and display data efficiently. However, it necessitates\nlabor-intensive efforts and a high level of expertise for these data-related\ntasks. Considering that large language models (LLMs) have showcased promising\ncapabilities in semantic understanding and reasoning, we advocate that the\ndeployment of LLMs could autonomously manage and process massive amounts of\ndata while displaying and interacting in a human-friendly manner. Based on this\nbelief, we propose Data-Copilot, an LLM-based system that connects numerous\ndata sources on one end and caters to diverse human demands on the other end.\nActing like an experienced expert, Data-Copilot autonomously transforms raw\ndata into visualization results that best match the user's intent.\nSpecifically, Data-Copilot autonomously designs versatile interfaces (tools)\nfor data management, processing, prediction, and visualization. In real-time\nresponse, it automatically deploys a concise workflow by invoking corresponding\ninterfaces step by step for the user's request. The interface design and\ndeployment processes are fully controlled by Data-Copilot itself, without human\nassistance. Besides, we create a Data-Copilot demo that links abundant data\nfrom different domains (stock, fund, company, economics, and live news) and\naccurately respond to diverse requests, serving as a reliable AI assistant.",
        "pdf_link": "https://arxiv.org/pdf/2306.07209v1.pdf"
    },
    {
        "title": "Mitigating Prior Errors in Causal Structure Learning: Towards LLM driven Prior Knowledge",
        "authors": [
            "Lyuzhou Chen",
            "Taiyu Ban",
            "Xiangyu Wang",
            "Derui Lyu",
            "Huanhuan Chen"
        ],
        "published": "2023-06-12T11:24:48Z",
        "summary": "Causal structure learning, a prominent technique for encoding cause and\neffect relationships among variables, through Bayesian Networks (BNs). Merely\nrecovering causal structures from real-world observed data lacks precision,\nwhile the development of Large Language Models (LLM) is opening a new frontier\nof causality. LLM presents strong capability in discovering causal\nrelationships between variables with the \"text\" inputs defining the\ninvestigated variables, leading to a potential new hierarchy and new ladder of\ncausality. We aim an critical issue in the emerging topic of LLM based causal\nstructure learning, to tackle erroneous prior causal statements from LLM, which\nis seldom considered in the current context of expert dominating prior\nresources. As a pioneer attempt, we propose a BN learning strategy resilient to\nprior errors without need of human intervention. Focusing on the edge-level\nprior, we classify the possible prior errors into three types:\norder-consistent, order-reversed, and irrelevant, and provide their theoretical\nimpact on the Structural Hamming Distance (SHD) under the presumption of\nsufficient data. Intriguingly, we discover and prove that only the\norder-reversed error contributes to an increase in a unique acyclic closed\nstructure, defined as a \"quasi-circle\". Leveraging this insight, a post-hoc\nstrategy is employed to identify the order-reversed prior error by its impact\non the increment of \"quasi-circles\". Through empirical evaluation on both real\nand synthetic datasets, we demonstrate our strategy's robustness against prior\nerrors. Specifically, we highlight its substantial ability to resist\norder-reversed errors while maintaining the majority of correct prior\nknowledge.",
        "pdf_link": "https://arxiv.org/pdf/2306.07032v1.pdf"
    },
    {
        "title": "Weakly supervised information extraction from inscrutable handwritten document images",
        "authors": [
            "Sujoy Paul",
            "Gagan Madan",
            "Akankshya Mishra",
            "Narayan Hegde",
            "Pradeep Kumar",
            "Gaurav Aggarwal"
        ],
        "published": "2023-06-12T02:22:30Z",
        "summary": "State-of-the-art information extraction methods are limited by OCR errors.\nThey work well for printed text in form-like documents, but unstructured,\nhandwritten documents still remain a challenge. Adapting existing models to\ndomain-specific training data is quite expensive, because of two factors, 1)\nlimited availability of the domain-specific documents (such as handwritten\nprescriptions, lab notes, etc.), and 2) annotations become even more\nchallenging as one needs domain-specific knowledge to decode inscrutable\nhandwritten document images. In this work, we focus on the complex problem of\nextracting medicine names from handwritten prescriptions using only weakly\nlabeled data. The data consists of images along with the list of medicine names\nin it, but not their location in the image. We solve the problem by first\nidentifying the regions of interest, i.e., medicine lines from just weak labels\nand then injecting a domain-specific medicine language model learned using only\nsynthetically generated data. Compared to off-the-shelf state-of-the-art\nmethods, our approach performs >2.5x better in medicine names extraction from\nprescriptions.",
        "pdf_link": "https://arxiv.org/pdf/2306.06823v1.pdf"
    },
    {
        "title": "Multimodal Audio-textual Architecture for Robust Spoken Language Understanding",
        "authors": [
            "Anderson R. Avila",
            "Mehdi Rezagholizadeh",
            "Chao Xing"
        ],
        "published": "2023-06-12T01:55:53Z",
        "summary": "Recent voice assistants are usually based on the cascade spoken language\nunderstanding (SLU) solution, which consists of an automatic speech recognition\n(ASR) engine and a natural language understanding (NLU) system. Because such\napproach relies on the ASR output, it often suffers from the so-called ASR\nerror propagation. In this work, we investigate impacts of this ASR error\npropagation on state-of-the-art NLU systems based on pre-trained language\nmodels (PLM), such as BERT and RoBERTa. Moreover, a multimodal language\nunderstanding (MLU) module is proposed to mitigate SLU performance degradation\ncaused by errors present in the ASR transcript. The MLU benefits from\nself-supervised features learned from both audio and text modalities,\nspecifically Wav2Vec for speech and Bert/RoBERTa for language. Our MLU combines\nan encoder network to embed the audio signal and a text encoder to process text\ntranscripts followed by a late fusion layer to fuse audio and text logits. We\nfound that the proposed MLU showed to be robust towards poor quality ASR\ntranscripts, while the performance of BERT and RoBERTa are severely\ncompromised. Our model is evaluated on five tasks from three SLU datasets and\nrobustness is tested using ASR transcripts from three ASR engines. Results show\nthat the proposed approach effectively mitigates the ASR error propagation\nproblem, surpassing the PLM models' performance across all datasets for the\nacademic ASR engine.",
        "pdf_link": "https://arxiv.org/pdf/2306.06819v2.pdf"
    },
    {
        "title": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models",
        "authors": [
            "Jiaqi Xue",
            "Mengxin Zheng",
            "Ting Hua",
            "Yilin Shen",
            "Yepeng Liu",
            "Ladislau Boloni",
            "Qian Lou"
        ],
        "published": "2023-06-12T01:22:39Z",
        "summary": "Large Language Models (LLMs) are progressively being utilized as machine\nlearning services and interface tools for various applications. However, the\nsecurity implications of LLMs, particularly in relation to adversarial and\nTrojan attacks, remain insufficiently examined. In this paper, we propose\nTrojLLM, an automatic and black-box framework to effectively generate universal\nand stealthy triggers. When these triggers are incorporated into the input\ndata, the LLMs' outputs can be maliciously manipulated. Moreover, the framework\nalso supports embedding Trojans within discrete prompts, enhancing the overall\neffectiveness and precision of the triggers' attacks. Specifically, we propose\na trigger discovery algorithm for generating universal triggers for various\ninputs by querying victim LLM-based APIs using few-shot data samples.\nFurthermore, we introduce a novel progressive Trojan poisoning algorithm\ndesigned to generate poisoned prompts that retain efficacy and transferability\nacross a diverse range of models. Our experiments and results demonstrate\nTrojLLM's capacity to effectively insert Trojans into text prompts in\nreal-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining\nexceptional performance on clean test sets. Our work sheds light on the\npotential security risks in current models and offers a potential defensive\napproach. The source code of TrojLLM is available at\nhttps://github.com/UCF-ML-Research/TrojLLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.06815v3.pdf"
    },
    {
        "title": "A blind spot for large language models: Supradiegetic linguistic information",
        "authors": [
            "Julia Witte Zimmerman",
            "Denis Hudon",
            "Kathryn Cramer",
            "Jonathan St. Onge",
            "Mikaela Fudolig",
            "Milo Z. Trujillo",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "published": "2023-06-11T22:15:01Z",
        "summary": "Large Language Models (LLMs) like ChatGPT reflect profound changes in the\nfield of Artificial Intelligence, achieving a linguistic fluency that is\nimpressively, even shockingly, human-like. The extent of their current and\npotential capabilities is an active area of investigation by no means limited\nto scientific researchers. It is common for people to frame the training data\nfor LLMs as \"text\" or even \"language\". We examine the details of this framing\nusing ideas from several areas, including linguistics, embodied cognition,\ncognitive science, mathematics, and history. We propose that considering what\nit is like to be an LLM like ChatGPT, as Nagel might have put it, can help us\ngain insight into its capabilities in general, and in particular, that its\nexposure to linguistic training data can be productively reframed as exposure\nto the diegetic information encoded in language, and its deficits can be\nreframed as ignorance of extradiegetic information, including supradiegetic\nlinguistic information. Supradiegetic linguistic information consists of those\narbitrary aspects of the physical form of language that are not derivable from\nthe one-dimensional relations of context -- frequency, adjacency, proximity,\nco-occurrence -- that LLMs like ChatGPT have access to. Roughly speaking, the\ndiegetic portion of a word can be thought of as its function, its meaning, as\nthe information in a theoretical vector in a word embedding, while the\nsupradiegetic portion of the word can be thought of as its form, like the\nshapes of its letters or the sounds of its syllables. We use these concepts to\ninvestigate why LLMs like ChatGPT have trouble handling palindromes, the visual\ncharacteristics of symbols, translating Sumerian cuneiform, and continuing\ninteger sequences.",
        "pdf_link": "https://arxiv.org/pdf/2306.06794v2.pdf"
    },
    {
        "title": "Augmenting Greybox Fuzzing with Generative AI",
        "authors": [
            "Jie Hu",
            "Qian Zhang",
            "Heng Yin"
        ],
        "published": "2023-06-11T21:44:47Z",
        "summary": "Real-world programs expecting structured inputs often has a format-parsing\nstage gating the deeper program space. Neither a mutation-based approach nor a\ngenerative approach can provide a solution that is effective and scalable.\nLarge language models (LLM) pre-trained with an enormous amount of natural\nlanguage corpus have proved to be effective for understanding the implicit\nformat syntax and generating format-conforming inputs. In this paper, propose\nChatFuzz, a greybox fuzzer augmented by generative AI. More specifically, we\npick a seed in the fuzzer's seed pool and prompt ChatGPT generative models to\nvariations, which are more likely to be format-conforming and thus of high\nquality. We conduct extensive experiments to explore the best practice for\nharvesting the power of generative LLM models. The experiment results show that\nour approach improves the edge coverage by 12.77\\% over the SOTA greybox fuzzer\n(AFL++) on 12 target programs from three well-tested benchmarks. As for\nvulnerability detection, \\sys is able to perform similar to or better than\nAFL++ for programs with explicit syntax rules but not for programs with\nnon-trivial syntax.",
        "pdf_link": "https://arxiv.org/pdf/2306.06782v1.pdf"
    },
    {
        "title": "Improving Knowledge Extraction from LLMs for Task Learning through Agent Analysis",
        "authors": [
            "James R. Kirk",
            "Robert E. Wray",
            "Peter Lindes",
            "John E. Laird"
        ],
        "published": "2023-06-11T20:50:14Z",
        "summary": "Large language models (LLMs) offer significant promise as a knowledge source\nfor task learning. Prompt engineering has been shown to be effective for\neliciting knowledge from an LLM, but alone it is insufficient for acquiring\nrelevant, situationally grounded knowledge for an embodied agent learning novel\ntasks. We describe a cognitive-agent approach, STARS, that extends and\ncomplements prompt engineering, mitigating its limitations and thus enabling an\nagent to acquire new task knowledge matched to its native language\ncapabilities, embodiment, environment, and user preferences. The STARS approach\nis to increase the response space of LLMs and deploy general strategies,\nembedded within the autonomous agent, to evaluate, repair, and select among\ncandidate responses produced by the LLM. We describe the approach and\nexperiments that show how an agent, by retrieving and evaluating a breadth of\nresponses from the LLM, can achieve 77-94% task completion in one-shot learning\nwithout user oversight. The approach achieves 100% task completion when human\noversight (such as an indication of preference) is provided. Further, the type\nof oversight largely shifts from explicit, natural language instruction to\nsimple confirmation/discomfirmation of high-quality responses that have been\nvetted by the agent before presentation to a user.",
        "pdf_link": "https://arxiv.org/pdf/2306.06770v4.pdf"
    },
    {
        "title": "The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases",
        "authors": [
            "Jiancheng Yang",
            "Hongwei Bran Li",
            "Donglai Wei"
        ],
        "published": "2023-06-11T20:39:13Z",
        "summary": "This study investigates the transformative potential of Large Language Models\n(LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public\ndata, these models, which possess remarkable language understanding and\ngeneration capabilities, are augmenting the interpretive skills of\nradiologists, enhancing patient-physician communication, and streamlining\nclinical workflows. The paper introduces an analytic framework for presenting\nthe complex interactions between LLMs and the broader ecosystem of medical\nimaging stakeholders, including businesses, insurance entities, governments,\nresearch institutions, and hospitals (nicknamed BIGR-H). Through detailed\nanalyses, illustrative use cases, and discussions on the broader implications\nand future directions, this perspective seeks to raise discussion in strategic\nplanning and decision-making in the era of AI-enabled healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2306.06767v2.pdf"
    },
    {
        "title": "CoTran: An LLM-based Code Translator using Reinforcement Learning with Feedback from Compiler and Symbolic Execution",
        "authors": [
            "Prithwish Jana",
            "Piyush Jha",
            "Haoyang Ju",
            "Gautham Kishore",
            "Aryan Mahajan",
            "Vijay Ganesh"
        ],
        "published": "2023-06-11T19:47:52Z",
        "summary": "In this paper, we present an LLM-based code translation method and an\nassociated tool called CoTran, that translates whole-programs from one\nhigh-level programming language to another. Current LLM-based code translation\nmethods lack a training approach to ensure that the translated code reliably\ncompiles or bears substantial functional equivalence to the input code. In our\nwork, we train an LLM via reinforcement learning, by modifying the fine-tuning\nprocess to incorporate compiler feedback and symbolic execution (symexec)-based\nequivalence testing feedback that checks for functional equivalence between the\ninput and output programs. The idea is to guide an LLM-in-training, via\ncompiler and symexec-based testing feedback, by letting it know how far it is\nfrom producing perfect translations. We report on extensive experiments\ncomparing CoTran with 14 other code translation tools that include\nhuman-written transpilers, LLM-based translation tools, and ChatGPT over a\nbenchmark of more than 57,000 Java-Python equivalent pairs, and we show that\nCoTran outperforms them on relevant metrics such as compilation accuracy\n(CompAcc) and functional equivalence accuracy (FEqAcc). For example, our tool\nachieves 48.68% FEqAcc, 76.98% CompAcc for Python-to-Java translation, whereas\nthe nearest competing tool (PLBART-base) only gets 38.26% and 75.77% resp.\nAlso, built upon CodeT5, CoTran achieves +11.23%, +14.89% improvement on FEqAcc\nand +4.07%, +8.14% on CompAcc for Java-to-Python and Python-to-Java translation\nresp.",
        "pdf_link": "https://arxiv.org/pdf/2306.06755v3.pdf"
    },
    {
        "title": "GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model",
        "authors": [
            "Shicheng Tan",
            "Weng Lam Tam",
            "Yuanchun Wang",
            "Wenwen Gong",
            "Yang Yang",
            "Hongyin Tang",
            "Keqing He",
            "Jiahao Liu",
            "Jingang Wang",
            "Shu Zhao",
            "Peng Zhang",
            "Jie Tang"
        ],
        "published": "2023-06-11T09:17:21Z",
        "summary": "Currently, the reduction in the parameter scale of large-scale pre-trained\nlanguage models (PLMs) through knowledge distillation has greatly facilitated\ntheir widespread deployment on various devices. However, the deployment of\nknowledge distillation systems faces great challenges in real-world\nindustrial-strength applications, which require the use of complex distillation\nmethods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the\nswitching of methods. To overcome these challenges, we propose GKD, a general\nknowledge distillation framework that supports distillation on larger-scale\nPLMs using various distillation methods. With GKD, developers can build larger\ndistillation models on memory-limited GPUs and easily switch and combine\ndifferent distillation methods within a single framework. Experimental results\nshow that GKD can support the distillation of at least 100B-scale PLMs and 25\nmainstream methods on 8 NVIDIA A100 (40GB) GPUs.",
        "pdf_link": "https://arxiv.org/pdf/2306.06629v1.pdf"
    },
    {
        "title": "Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method",
        "authors": [
            "Shicheng Tan",
            "Weng Lam Tam",
            "Yuanchun Wang",
            "Wenwen Gong",
            "Shu Zhao",
            "Peng Zhang",
            "Jie Tang"
        ],
        "published": "2023-06-11T08:53:27Z",
        "summary": "The large scale of pre-trained language models poses a challenge for their\ndeployment on various devices, with a growing emphasis on methods to compress\nthese models, particularly knowledge distillation. However, current knowledge\ndistillation methods rely on the model's intermediate layer features and the\ngolden labels (also called hard labels), which usually require aligned model\narchitecture and enough labeled data respectively. Moreover, the parameters of\nvocabulary are usually neglected in existing methods. To address these\nproblems, we propose a general language model distillation (GLMD) method that\nperforms two-stage word prediction distillation and vocabulary compression,\nwhich is simple and surprisingly shows extremely strong performance.\nSpecifically, GLMD supports more general application scenarios by eliminating\nthe constraints of dimension and structure between models and the need for\nlabeled datasets through the absence of intermediate layers and golden labels.\nMeanwhile, based on the long-tailed distribution of word frequencies in the\ndata, GLMD designs a strategy of vocabulary compression through decreasing\nvocabulary size instead of dimensionality. Experimental results show that our\nmethod outperforms 25 state-of-the-art methods on the SuperGLUE benchmark,\nachieving an average score that surpasses the best method by 3%.",
        "pdf_link": "https://arxiv.org/pdf/2306.06625v1.pdf"
    },
    {
        "title": "RestGPT: Connecting Large Language Models with Real-World RESTful APIs",
        "authors": [
            "Yifan Song",
            "Weimin Xiong",
            "Dawei Zhu",
            "Wenhao Wu",
            "Han Qian",
            "Mingbo Song",
            "Hailiang Huang",
            "Cheng Li",
            "Ke Wang",
            "Rong Yao",
            "Ye Tian",
            "Sujian Li"
        ],
        "published": "2023-06-11T08:53:12Z",
        "summary": "Tool-augmented large language models (LLMs) have achieved remarkable progress\nin tackling a broad range of tasks. However, existing methods are mainly\nrestricted to specifically designed tools and fail to fulfill complex\ninstructions, having great limitations when confronted with real-world\nscenarios. In this paper, we explore a more realistic scenario by connecting\nLLMs with RESTful APIs, which adhere to the widely adopted REST software\narchitectural style for web service development. To address the practical\nchallenges of tackling complex instructions, we propose RestGPT, which exploits\nthe power of LLMs and conducts a coarse-to-fine online planning mechanism to\nenhance the abilities of task decomposition and API selection. RestGPT also\ncontains an API executor tailored for calling RESTful APIs, which can\nmeticulously formulate parameters and parse API responses. To fully evaluate\nthe performance of RestGPT, we propose RestBench, a high-quality benchmark\nwhich consists of two real-world scenarios and human-annotated instructions\nwith gold solution paths. Experiments show that RestGPT is able to achieve\nimpressive results in complex tasks and has strong robustness, which paves a\nnew way towards AGI. RestGPT and RestBench is publicly available at\nhttps://restgpt.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2306.06624v2.pdf"
    },
    {
        "title": "Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective",
        "authors": [
            "Jiatong Li",
            "Yunqing Liu",
            "Wenqi Fan",
            "Xiao-Yong Wei",
            "Hui Liu",
            "Jiliang Tang",
            "Qing Li"
        ],
        "published": "2023-06-11T08:16:25Z",
        "summary": "Molecule discovery plays a crucial role in various scientific fields,\nadvancing the design of tailored materials and drugs. Traditional methods for\nmolecule discovery follow a trial-and-error process, which are both\ntime-consuming and costly, while computational approaches such as artificial\nintelligence (AI) have emerged as revolutionary tools to expedite various\ntasks, like molecule-caption translation. Despite the importance of\nmolecule-caption translation for molecule discovery, most of the existing\nmethods heavily rely on domain experts, require excessive computational cost,\nand suffer from poor performance. On the other hand, Large Language Models\n(LLMs), like ChatGPT, have shown remarkable performance in various cross-modal\ntasks due to their great powerful capabilities in natural language\nunderstanding, generalization, and reasoning, which provides unprecedented\nopportunities to advance molecule discovery. To address the above limitations,\nin this work, we propose a novel LLMs-based framework (\\textbf{MolReGPT}) for\nmolecule-caption translation, where a retrieval-based prompt paradigm is\nintroduced to empower molecule discovery with LLMs like ChatGPT without\nfine-tuning. More specifically, MolReGPT leverages the principle of molecular\nsimilarity to retrieve similar molecules and their text descriptions from a\nlocal database to ground the generation of LLMs through in-context few-shot\nmolecule learning. We evaluate the effectiveness of MolReGPT via\nmolecule-caption translation, which includes molecule understanding and\ntext-based molecule generation. Experimental results show that MolReGPT\noutperforms fine-tuned models like MolT5-base without any additional training.\nTo the best of our knowledge, MolReGPT is the first work to leverage LLMs in\nmolecule-caption translation for advancing molecule discovery.",
        "pdf_link": "https://arxiv.org/pdf/2306.06615v1.pdf"
    },
    {
        "title": "Inductive reasoning in humans and large language models",
        "authors": [
            "Simon J. Han",
            "Keith Ransom",
            "Andrew Perfors",
            "Charles Kemp"
        ],
        "published": "2023-06-11T00:23:25Z",
        "summary": "The impressive recent performance of large language models has led many to\nwonder to what extent they can serve as models of general intelligence or are\nsimilar to human cognition. We address this issue by applying GPT-3.5 and GPT-4\nto a classic problem in human inductive reasoning known as property induction.\nOver two experiments, we elicit human judgments on a range of property\ninduction tasks spanning multiple domains. Although GPT-3.5 struggles to\ncapture many aspects of human behaviour, GPT-4 is much more successful: for the\nmost part, its performance qualitatively matches that of humans, and the only\nnotable exception is its failure to capture the phenomenon of premise\nnon-monotonicity. Our work demonstrates that property induction allows for\ninteresting comparisons between human and machine intelligence and provides two\nlarge datasets that can serve as benchmarks for future work in this vein.",
        "pdf_link": "https://arxiv.org/pdf/2306.06548v3.pdf"
    },
    {
        "title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers",
        "authors": [
            "Yongchao Chen",
            "Jacob Arkin",
            "Charles Dawson",
            "Yang Zhang",
            "Nicholas Roy",
            "Chuchu Fan"
        ],
        "published": "2023-06-10T21:58:29Z",
        "summary": "For effective human-robot interaction, robots need to understand, plan, and\nexecute complex, long-horizon tasks described by natural language. Recent\nadvances in large language models (LLMs) have shown promise for translating\nnatural language into robot action sequences for complex tasks. However,\nexisting approaches either translate the natural language directly into robot\ntrajectories or factor the inference process by decomposing language into task\nsub-goals and relying on a motion planner to execute each sub-goal. When\ncomplex environmental and temporal constraints are involved, inference over\nplanning tasks must be performed jointly with motion plans using traditional\ntask-and-motion planning (TAMP) algorithms, making factorization into subgoals\nuntenable. Rather than using LLMs to directly plan task sub-goals, we instead\nperform few-shot translation from natural language task descriptions to an\nintermediate task representation that can then be consumed by a TAMP algorithm\nto jointly solve the task and motion plan. To improve translation, we\nautomatically detect and correct both syntactic and semantic errors via\nautoregressive re-prompting, resulting in significant improvements in task\ncompletion. We show that our approach outperforms several methods using LLMs as\nplanners in complex task domains. See our project website\nhttps://yongchao98.github.io/MIT-REALM-AutoTAMP/ for prompts, videos, and code.",
        "pdf_link": "https://arxiv.org/pdf/2306.06531v3.pdf"
    },
    {
        "title": "Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification",
        "authors": [
            "Shouvon Sarker",
            "Lijun Qian",
            "Xishuang Dong"
        ],
        "published": "2023-06-10T20:55:21Z",
        "summary": "The identification of key factors such as medications, diseases, and\nrelationships within electronic health records and clinical notes has a wide\nrange of applications in the clinical field. In the N2C2 2022 competitions,\nvarious tasks were presented to promote the identification of key factors in\nelectronic health records (EHRs) using the Contextualized Medication Event\nDataset (CMED). Pretrained large language models (LLMs) demonstrated\nexceptional performance in these tasks. This study aims to explore the\nutilization of LLMs, specifically ChatGPT, for data augmentation to overcome\nthe limited availability of annotated data for identifying the key factors in\nEHRs. Additionally, different pre-trained BERT models, initially trained on\nextensive datasets like Wikipedia and MIMIC, were employed to develop models\nfor identifying these key variables in EHRs through fine-tuning on augmented\ndatasets. The experimental results of two EHR analysis tasks, namely medication\nidentification and medication event classification, indicate that data\naugmentation based on ChatGPT proves beneficial in improving performance for\nboth medication identification and medication event classification.",
        "pdf_link": "https://arxiv.org/pdf/2306.07297v1.pdf"
    },
    {
        "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
        "authors": [
            "Jianing Wang",
            "Qiushi Sun",
            "Nuo Chen",
            "Xiang Li",
            "Ming Gao"
        ],
        "published": "2023-06-10T12:42:36Z",
        "summary": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex\nreasoning tasks, which aims at designing a simple prompt like ``Let's think\nstep by step'' or multiple in-context exemplars with well-designed rationales\nto elicit Large Language Models (LLMs) to generate intermediate reasoning\nsteps. However, the generated rationales often come with mistakes, making\nunfactual and unfaithful reasoning chains. To mitigate this brittleness, we\npropose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting\nLLMs to generate explicit pieces of knowledge evidence in the form of structure\ntriple. This is inspired by our human behaviors, i.e., we can draw a mind map\nor knowledge map as the reasoning evidence in the brain before answering a\ncomplex question. Benefiting from CoK, we additionally introduce a\nF^2-Verification method to estimate the reliability of the reasoning chains in\nterms of factuality and faithfulness. For the unreliable response, the wrong\nevidence can be indicated to prompt the LLM to rethink. Extensive experiments\ndemonstrate that our method can further improve the performance of commonsense,\nfactual, symbolic, and arithmetic reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.06427v2.pdf"
    },
    {
        "title": "Human-in-the-Loop through Chain-of-Thought",
        "authors": [
            "Zefan Cai",
            "Baobao Chang",
            "Wenjuan Han"
        ],
        "published": "2023-06-10T04:31:57Z",
        "summary": "While the emergence of powerful language models along with Chain-of-thought\nprompting has made automation more and more omnipresent, it sometimes\ndemonstrates its weakness in long-term or multi-step logical reasoning. For\nexample, users don't always get desirable answers for complex mathematical\nproblems without human involvement. Against this background, we present the\nManual Correction System (MCS) -- a human-in-the-loop system enhanced by\nChain-of-Thought prompting, which explores how manual correction of sub-logics\nin rationales can improve LLM's reasoning performance. Moving one step forward,\nconsidering a system with human-in-the-loop involves more than having humans\nimprove performance but also controlling the cost. Therefore, we post a\nCost-utility Analysis Model for Human-in-the-Loop systems (CAMLOP) based on\nclassical economics theory to analyze, quantify and balance the utility and the\ncorresponding cost. We conduct experiments of MCS and CAMLOP with twelve\ndatasets. A significant advantage w.r.t cost and utility proves its superiority\nover strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2306.07932v2.pdf"
    },
    {
        "title": "Measuring and Modifying Factual Knowledge in Large Language Models",
        "authors": [
            "Pouya Pezeshkpour"
        ],
        "published": "2023-06-09T21:25:48Z",
        "summary": "Large Language Models (LLMs) store an extensive amount of factual knowledge\nobtained from vast collections of text. To effectively utilize these models for\ndownstream tasks, it is crucial to have reliable methods for measuring their\nknowledge. However, existing approaches for knowledge measurement have certain\nlimitations, and despite recent efforts, they fail to provide accurate\nmeasurements and the necessary insights for modifying the knowledge within\nLLMs. In this work, we employ information theory-based measurements to provide\na framework estimating the factual knowledge contained within large language\nmodels. More specifically, we measure knowledge by analyzing the LLM's\nprediction probability distribution before and after instilling the target\nknowledge, employing metrics such as entropy and KL-divergence. Introducing our\nmetrics, we first assess their accuracy in comparison to previous ranking-based\nmethods, surpassing them by over $35\\%$ in a synthetic experiment. Then, we\nexplore two prominent methods of knowledge instillation, discovering that LLMs\nexhibit limitations in capturing new knowledge under specific circumstances for\none of these methods. Lastly, we demonstrate the applicability of our methods\nin extracting unlearned and mislearned facts in LLMs through their application\nto in-context learning. We make code and data for all methods and experiments\nin this paper publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2306.06264v1.pdf"
    },
    {
        "title": "Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording",
        "authors": [
            "Aisha Khatun",
            "Daniel G. Brown"
        ],
        "published": "2023-06-09T19:07:31Z",
        "summary": "Large language models (LLMs) have become mainstream technology with their\nversatile use cases and impressive performance. Despite the countless\nout-of-the-box applications, LLMs are still not reliable. A lot of work is\nbeing done to improve the factual accuracy, consistency, and ethical standards\nof these models through fine-tuning, prompting, and Reinforcement Learning with\nHuman Feedback (RLHF), but no systematic analysis of the responses of these\nmodels to different categories of statements, or on their potential\nvulnerabilities to simple prompting changes is available. In this work, we\nanalyze what confuses GPT-3: how the model responds to certain sensitive topics\nand what effects the prompt wording has on the model response. We find that\nGPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes\nmistakes with common Misconceptions and Controversies. The model responses are\ninconsistent across prompts and settings, highlighting GPT-3's unreliability.\nDataset and code of our analysis is available in\nhttps://github.com/tanny411/GPT3-Reliability-Check.",
        "pdf_link": "https://arxiv.org/pdf/2306.06199v1.pdf"
    },
    {
        "title": "Trapping LLM Hallucinations Using Tagged Context Prompts",
        "authors": [
            "Philip Feldman",
            "James R. Foulds",
            "Shimei Pan"
        ],
        "published": "2023-06-09T17:48:54Z",
        "summary": "Recent advances in large language models (LLMs), such as ChatGPT, have led to\nhighly sophisticated conversation agents. However, these models suffer from\n\"hallucinations,\" where the model generates false or fabricated information.\nAddressing this challenge is crucial, particularly with AI-driven platforms\nbeing adopted across various sectors. In this paper, we propose a novel method\nto recognize and flag instances when LLMs perform outside their domain\nknowledge, and ensuring users receive accurate information.\n  We find that the use of context combined with embedded tags can successfully\ncombat hallucinations within generative language models. To do this, we\nbaseline hallucination frequency in no-context prompt-response pairs using\ngenerated URLs as easily-tested indicators of fabricated data. We observed a\nsignificant reduction in overall hallucination when context was supplied along\nwith question prompts for tested generative engines. Lastly, we evaluated how\nplacing tags within contexts impacted model responses and were able to\neliminate hallucinations in responses with 98.88% effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2306.06085v1.pdf"
    },
    {
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "authors": [
            "Xiang Deng",
            "Yu Gu",
            "Boyuan Zheng",
            "Shijie Chen",
            "Samuel Stevens",
            "Boshi Wang",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023-06-09T17:44:31Z",
        "summary": "We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.",
        "pdf_link": "https://arxiv.org/pdf/2306.06070v3.pdf"
    },
    {
        "title": "FinGPT: Open-Source Financial Large Language Models",
        "authors": [
            "Hongyang Yang",
            "Xiao-Yang Liu",
            "Christina Dan Wang"
        ],
        "published": "2023-06-09T16:52:00Z",
        "summary": "Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n  In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT}\nand \\url{https://github.com/AI4Finance-Foundation/FinNLP}",
        "pdf_link": "https://arxiv.org/pdf/2306.06031v1.pdf"
    },
    {
        "title": "S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput",
        "authors": [
            "Yunho Jin",
            "Chun-Feng Wu",
            "David Brooks",
            "Gu-Yeon Wei"
        ],
        "published": "2023-06-09T16:13:43Z",
        "summary": "Generating texts with a large language model (LLM) consumes massive amounts\nof memory. Apart from the already-large model parameters, the key/value (KV)\ncache that holds information about previous tokens in a sequence can grow to be\neven larger than the model itself. This problem is exacerbated in one of the\ncurrent LLM serving frameworks which reserves the maximum sequence length of\nmemory for the KV cache to guarantee generating a complete sequence as they do\nnot know the output sequence length. This restricts us to use a smaller batch\nsize leading to lower GPU utilization and above all, lower throughput. We argue\nthat designing a system with a priori knowledge of the output sequence can\nmitigate this problem. To this end, we propose S$^{3}$, which predicts the\noutput sequence length, schedules generation queries based on the prediction to\nincrease device resource utilization and throughput, and handle mispredictions.\nOur proposed method achieves 6.49$\\times$ throughput over those systems that\nassume the worst case for the output sequence length.",
        "pdf_link": "https://arxiv.org/pdf/2306.06000v1.pdf"
    },
    {
        "title": "Language Models Can Learn Exceptions to Syntactic Rules",
        "authors": [
            "Cara Su-Yi Leong",
            "Tal Linzen"
        ],
        "published": "2023-06-09T15:35:11Z",
        "summary": "Artificial neural networks can generalize productively to novel contexts. Can\nthey also learn exceptions to those productive rules? We explore this question\nusing the case of restrictions on English passivization (e.g., the fact that\n\"The vacation lasted five days\" is grammatical, but \"*Five days was lasted by\nthe vacation\" is not). We collect human acceptability judgments for passive\nsentences with a range of verbs, and show that the probability distribution\ndefined by GPT-2, a language model, matches the human judgments with high\ncorrelation. We also show that the relative acceptability of a verb in the\nactive vs. passive voice is positively correlated with the relative frequency\nof its occurrence in those voices. These results provide preliminary support\nfor the entrenchment hypothesis, according to which learners track and uses the\ndistributional properties of their input to learn negative exceptions to rules.\nAt the same time, this hypothesis fails to explain the magnitude of\nunpassivizability demonstrated by certain individual verbs, suggesting that\nother cues to exceptionality are available in the linguistic input.",
        "pdf_link": "https://arxiv.org/pdf/2306.05969v1.pdf"
    },
    {
        "title": "Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?",
        "authors": [
            "Wissam Antoun",
            "Virginie Mouilleron",
            "Benoît Sagot",
            "Djamé Seddah"
        ],
        "published": "2023-06-09T13:03:53Z",
        "summary": "Recent advances in natural language processing (NLP) have led to the\ndevelopment of large language models (LLMs) such as ChatGPT. This paper\nproposes a methodology for developing and evaluating ChatGPT detectors for\nFrench text, with a focus on investigating their robustness on out-of-domain\ndata and against common attack schemes. The proposed method involves\ntranslating an English dataset into French and training a classifier on the\ntranslated data. Results show that the detectors can effectively detect\nChatGPT-generated text, with a degree of robustness against basic attack\ntechniques in in-domain settings. However, vulnerabilities are evident in\nout-of-domain contexts, highlighting the challenge of detecting adversarial\ntext. The study emphasizes caution when applying in-domain testing results to a\nwider variety of content. We provide our translated datasets and models as\nopen-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection",
        "pdf_link": "https://arxiv.org/pdf/2306.05871v1.pdf"
    },
    {
        "title": "Can Large Language Models Infer Causation from Correlation?",
        "authors": [
            "Zhijing Jin",
            "Jiarui Liu",
            "Zhiheng Lyu",
            "Spencer Poff",
            "Mrinmaya Sachan",
            "Rada Mihalcea",
            "Mona Diab",
            "Bernhard Schölkopf"
        ],
        "published": "2023-06-09T12:09:15Z",
        "summary": "Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.",
        "pdf_link": "https://arxiv.org/pdf/2306.05836v2.pdf"
    },
    {
        "title": "Towards the Exploitation of LLM-based Chatbot for Providing Legal Support to Palestinian Cooperatives",
        "authors": [
            "Rabee Qasem",
            "Banan Tantour",
            "Mohammed Maree"
        ],
        "published": "2023-06-09T11:57:57Z",
        "summary": "With the ever-increasing utilization of natural language processing (NLP), we\nstarted to witness over the past few years a significant transformation in our\ninteraction with legal texts. This technology has advanced the analysis and\nenhanced the understanding of complex legal terminology and contexts. The\ndevelopment of recent large language models (LLMs), particularly ChatGPT, has\nalso introduced a revolutionary contribution to the way that legal texts can be\nprocessed and comprehended. In this paper, we present our work on a\ncooperative-legal question-answering LLM-based chatbot, where we developed a\nset of legal questions about Palestinian cooperatives, associated with their\nregulations and compared the auto-generated answers by the chatbot to their\ncorrespondences that are designed by a legal expert. To evaluate the proposed\nchatbot, we have used 50 queries generated by the legal expert and compared the\nanswers produced by the chart to their relevance judgments. Finding\ndemonstrated that an overall accuracy rate of 82% has been achieved when\nanswering the queries, while exhibiting an F1 score equivalent to 79%.",
        "pdf_link": "https://arxiv.org/pdf/2306.05827v1.pdf"
    },
    {
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
        "authors": [
            "Jianghao Lin",
            "Xinyi Dai",
            "Yunjia Xi",
            "Weiwen Liu",
            "Bo Chen",
            "Hao Zhang",
            "Yong Liu",
            "Chuhan Wu",
            "Xiangyang Li",
            "Chenxu Zhu",
            "Huifeng Guo",
            "Yong Yu",
            "Ruiming Tang",
            "Weinan Zhang"
        ],
        "published": "2023-06-09T11:31:50Z",
        "summary": "With the rapid development of online services, recommender systems (RS) have\nbecome increasingly indispensable for mitigating information overload. Despite\nremarkable progress, conventional recommendation models (CRM) still have some\nlimitations, e.g., lacking open-world knowledge, and difficulties in\ncomprehending users' underlying preferences and motivations. Meanwhile, large\nlanguage models (LLM) have shown impressive general intelligence and human-like\ncapabilities, which mainly stem from their extensive open-world knowledge,\nreasoning ability, as well as their comprehension of human culture and society.\nConsequently, the emergence of LLM is inspiring the design of recommender\nsystems and pointing out a promising research direction, i.e., whether we can\nincorporate LLM and benefit from their knowledge and capabilities to compensate\nfor the limitations of CRM. In this paper, we conduct a comprehensive survey on\nthis research direction from the perspective of the whole pipeline in\nreal-world recommender systems. Specifically, we summarize existing works from\ntwo orthogonal aspects: where and how to adapt LLM to RS. For the WHERE\nquestion, we discuss the roles that LLM could play in different stages of the\nrecommendation pipeline, i.e., feature engineering, feature encoder,\nscoring/ranking function, user interaction, and pipeline controller. For the\nHOW question, we investigate the training and inference strategies, resulting\nin two fine-grained taxonomy criteria, i.e., whether to tune LLM or not, and\nwhether to involve conventional recommendation models for inference. Then, we\nhighlight key challenges in adapting LLM to RS from three aspects, i.e.,\nefficiency, effectiveness, and ethics. Finally, we summarize the survey and\ndiscuss the future prospects. We actively maintain a GitHub repository for\npapers and other related resources:\nhttps://github.com/CHIANGEL/Awesome-LLM-for-RecSys/.",
        "pdf_link": "https://arxiv.org/pdf/2306.05817v5.pdf"
    },
    {
        "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Haoning Ye",
            "Lin Zhang",
            "Jianchen Wang",
            "Yixin Zhu",
            "Sihang Jiang",
            "Zhuozhi Xiong",
            "Zihan Li",
            "Weijie Wu",
            "Qianyu He",
            "Rui Xu",
            "Wenhao Huang",
            "Jingping Liu",
            "Zili Wang",
            "Shusen Wang",
            "Weiguo Zheng",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2023-06-09T09:52:05Z",
        "summary": "New Natural Langauge Process~(NLP) benchmarks are urgently needed to align\nwith the rapid development of large language models (LLMs). We present Xiezhi,\nthe most comprehensive evaluation suite designed to assess holistic domain\nknowledge. Xiezhi comprises multiple-choice questions across 516 diverse\ndisciplines ranging from 13 different subjects with 249,587 questions and\naccompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k\nquestions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results\nindicate that LLMs exceed average performance of humans in science,\nengineering, agronomy, medicine, and art, but fall short in economics,\njurisprudence, pedagogy, literature, history, and management. We anticipate\nXiezhi will help analyze important strengths and shortcomings of LLMs, and the\nbenchmark is released in~\\url{https://github.com/MikeGu721/XiezhiBenchmark}.",
        "pdf_link": "https://arxiv.org/pdf/2306.05783v3.pdf"
    },
    {
        "title": "Exploring the Responses of Large Language Models to Beginner Programmers' Help Requests",
        "authors": [
            "Arto Hellas",
            "Juho Leinonen",
            "Sami Sarsa",
            "Charles Koutcheme",
            "Lilja Kujanpää",
            "Juha Sorva"
        ],
        "published": "2023-06-09T07:19:43Z",
        "summary": "Background and Context: Over the past year, large language models (LLMs) have\ntaken the world by storm. In computing education, like in other walks of life,\nmany opportunities and threats have emerged as a consequence.\n  Objectives: In this article, we explore such opportunities and threats in a\nspecific area: responding to student programmers' help requests. More\nspecifically, we assess how good LLMs are at identifying issues in problematic\ncode that students request help on.\n  Method: We collected a sample of help requests and code from an online\nprogramming course. We then prompted two different LLMs (OpenAI Codex and\nGPT-3.5) to identify and explain the issues in the students' code and assessed\nthe LLM-generated answers both quantitatively and qualitatively.\n  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently\nfind at least one actual issue in each student program (GPT-3.5 in 90% of the\ncases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%\nof the time). False positives are common (40% chance for GPT-3.5). The advice\nthat the LLMs provide on the issues is often sensible. The LLMs perform better\non issues involving program logic rather than on output formatting. Model\nsolutions are frequently provided even when the LLM is prompted not to. LLM\nresponses to prompts in a non-English language are only slightly worse than\nresponses to English prompts.\n  Implications: Our results continue to highlight the utility of LLMs in\nprogramming education. At the same time, the results highlight the\nunreliability of LLMs: LLMs make some of the same mistakes that students do,\nperhaps especially when formatting output as required by automated assessment\nsystems. Our study informs teachers interested in using LLMs as well as future\nefforts to customize LLMs for the needs of programming education.",
        "pdf_link": "https://arxiv.org/pdf/2306.05715v1.pdf"
    },
    {
        "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
        "authors": [
            "Lianmin Zheng",
            "Wei-Lin Chiang",
            "Ying Sheng",
            "Siyuan Zhuang",
            "Zhanghao Wu",
            "Yonghao Zhuang",
            "Zi Lin",
            "Zhuohan Li",
            "Dacheng Li",
            "Eric P. Xing",
            "Hao Zhang",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "published": "2023-06-09T05:55:52Z",
        "summary": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
        "pdf_link": "https://arxiv.org/pdf/2306.05685v4.pdf"
    },
    {
        "title": "Customizing General-Purpose Foundation Models for Medical Report Generation",
        "authors": [
            "Bang Yang",
            "Asif Raza",
            "Yuexian Zou",
            "Tong Zhang"
        ],
        "published": "2023-06-09T03:02:36Z",
        "summary": "Medical caption prediction which can be regarded as a task of medical report\ngeneration (MRG), requires the automatic generation of coherent and accurate\ncaptions for the given medical images. However, the scarcity of labelled\nmedical image-report pairs presents great challenges in the development of deep\nand large-scale neural networks capable of harnessing the potential artificial\ngeneral intelligence power like large language models (LLMs). In this work, we\npropose customizing off-the-shelf general-purpose large-scale pre-trained\nmodels, i.e., foundation models (FMs), in computer vision and natural language\nprocessing with a specific focus on medical report generation. Specifically,\nfollowing BLIP-2, a state-of-the-art vision-language pre-training approach, we\nintroduce our encoder-decoder-based MRG model. This model utilizes a\nlightweight query Transformer to connect two FMs: the giant vision Transformer\nEVA-ViT-g and a bilingual LLM trained to align with human intentions (referred\nto as ChatGLM-6B). Furthermore, we conduct ablative experiments on the\ntrainable components of the model to identify the crucial factors for effective\ntransfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn\nmedical image representations, followed by parameter-efficient training of\nChatGLM-6B to capture the writing styles of medical reports, is essential for\nachieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and\nthe 2nd, respectively, out of 13 participating teams, based on the BERTScore\nand ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction\nTask competition.",
        "pdf_link": "https://arxiv.org/pdf/2306.05642v1.pdf"
    },
    {
        "title": "Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding",
        "authors": [
            "Mingqiu Wang",
            "Izhak Shafran",
            "Hagen Soltau",
            "Wei Han",
            "Yuan Cao",
            "Dian Yu",
            "Laurent El Shafey"
        ],
        "published": "2023-06-08T22:33:22Z",
        "summary": "Large Language Models (LLMs) have been applied in the speech domain, often\nincurring a performance drop due to misaligned between speech and language\nrepresentations. To bridge this gap, we propose a joint speech and language\nmodel (SLM) using a Speech2Text adapter, which maps speech into text token\nembedding space without speech information loss. Additionally, using a\nCTC-based blank-filtering, we can reduce the speech sequence length to that of\ntext. In speech MultiWoz dataset (DSTC11 challenge), SLM largely improves the\ndialog state tracking (DST) performance (24.7% to 28.4% accuracy). Further to\naddress errors on rare entities, we augment SLM with a Speech2Entity retriever,\nwhich uses speech to retrieve relevant entities, and then adds them to the\noriginal SLM input as a prefix. With this retrieval-augmented SLM (ReSLM), the\nDST performance jumps to 34.6% accuracy. Moreover, augmenting the ASR task with\nthe dialog understanding task improves the ASR performance from 9.4% to 8.5%\nWER.",
        "pdf_link": "https://arxiv.org/pdf/2306.07944v1.pdf"
    },
    {
        "title": "Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization",
        "authors": [
            "Oleksandr Yermilov",
            "Vipul Raheja",
            "Artem Chernodub"
        ],
        "published": "2023-06-08T21:06:19Z",
        "summary": "This work investigates the effectiveness of different pseudonymization\ntechniques, ranging from rule-based substitutions to using pre-trained Large\nLanguage Models (LLMs), on a variety of datasets and models used for two widely\nused NLP tasks: text classification and summarization. Our work provides\ncrucial insights into the gaps between original and anonymized data (focusing\non the pseudonymization technique) and model quality and fosters future\nresearch into higher-quality anonymization techniques to better balance the\ntrade-offs between data protection and utility preservation. We make our code,\npseudonymized datasets, and downstream models publicly available",
        "pdf_link": "https://arxiv.org/pdf/2306.05561v1.pdf"
    },
    {
        "title": "Prompt Injection attack against LLM-integrated Applications",
        "authors": [
            "Yi Liu",
            "Gelei Deng",
            "Yuekang Li",
            "Kailong Wang",
            "Zihao Wang",
            "Xiaofeng Wang",
            "Tianwei Zhang",
            "Yepang Liu",
            "Haoyu Wang",
            "Yan Zheng",
            "Yang Liu"
        ],
        "published": "2023-06-08T18:43:11Z",
        "summary": "Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2306.05499v2.pdf"
    },
    {
        "title": "Multi-Modal Classifiers for Open-Vocabulary Object Detection",
        "authors": [
            "Prannay Kaul",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "published": "2023-06-08T18:31:56Z",
        "summary": "The goal of this paper is open-vocabulary object detection (OVOD)\n$\\unicode{x2013}$ building a model that can detect objects beyond the set of\ncategories seen at training, thus enabling the user to specify categories of\ninterest at inference without the need for model retraining. We adopt a\nstandard two-stage object detector architecture, and explore three ways for\nspecifying novel categories: via language descriptions, via image exemplars, or\nvia a combination of the two. We make three contributions: first, we prompt a\nlarge language model (LLM) to generate informative language descriptions for\nobject classes, and construct powerful text-based classifiers; second, we\nemploy a visual aggregator on image exemplars that can ingest any number of\nimages as input, forming vision-based classifiers; and third, we provide a\nsimple method to fuse information from language descriptions and image\nexemplars, yielding a multi-modal classifier. When evaluating on the\nchallenging LVIS open-vocabulary benchmark we demonstrate that: (i) our\ntext-based classifiers outperform all previous OVOD works; (ii) our\nvision-based classifiers perform as well as text-based classifiers in prior\nwork; (iii) using multi-modal classifiers perform better than either modality\nalone; and finally, (iv) our text-based and multi-modal classifiers yield\nbetter performance than a fully-supervised detector.",
        "pdf_link": "https://arxiv.org/pdf/2306.05493v1.pdf"
    },
    {
        "title": "Artificial General Intelligence for Medical Imaging",
        "authors": [
            "Xiang Li",
            "Lu Zhang",
            "Zihao Wu",
            "Zhengliang Liu",
            "Lin Zhao",
            "Yixuan Yuan",
            "Jun Liu",
            "Gang Li",
            "Dajiang Zhu",
            "Pingkun Yan",
            "Quanzheng Li",
            "Wei Liu",
            "Tianming Liu",
            "Dinggang Shen"
        ],
        "published": "2023-06-08T18:04:13Z",
        "summary": "In this review, we explore the potential applications of Artificial General\nIntelligence (AGI) models in healthcare, focusing on foundational Large\nLanguage Models (LLMs), Large Vision Models, and Large Multimodal Models. We\nemphasize the importance of integrating clinical expertise, domain knowledge,\nand multimodal capabilities into AGI models. In addition, we lay out key\nroadmaps that guide the development and deployment of healthcare AGI models.\nThroughout the review, we provide critical perspectives on the potential\nchallenges and pitfalls associated with deploying large-scale AGI models in the\nmedical field. This comprehensive review aims to offer insights into the future\nimplications of AGI in medical imaging, healthcare and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2306.05480v2.pdf"
    },
    {
        "title": "Grounded Text-to-Image Synthesis with Attention Refocusing",
        "authors": [
            "Quynh Phung",
            "Songwei Ge",
            "Jia-Bin Huang"
        ],
        "published": "2023-06-08T17:59:59Z",
        "summary": "Driven by the scalable diffusion models trained on large-scale datasets,\ntext-to-image synthesis methods have shown compelling results. However, these\nmodels still fail to precisely follow the text prompt involving multiple\nobjects, attributes, or spatial compositions. In this paper, we reveal the\npotential causes in the diffusion model's cross-attention and self-attention\nlayers. We propose two novel losses to refocus attention maps according to a\ngiven spatial layout during sampling. Creating the layouts manually requires\nadditional effort and can be tedious. Therefore, we explore using large\nlanguage models (LLM) to produce these layouts for our method. We conduct\nextensive experiments on the DrawBench, HRS, and TIFA benchmarks to evaluate\nour proposed method. We show that our proposed attention refocusing effectively\nimproves the controllability of existing approaches.",
        "pdf_link": "https://arxiv.org/pdf/2306.05427v2.pdf"
    },
    {
        "title": "MIMIC-IT: Multi-Modal In-Context Instruction Tuning",
        "authors": [
            "Bo Li",
            "Yuanhan Zhang",
            "Liangyu Chen",
            "Jinghao Wang",
            "Fanyi Pu",
            "Jingkang Yang",
            "Chunyuan Li",
            "Ziwei Liu"
        ],
        "published": "2023-06-08T17:59:56Z",
        "summary": "High-quality instructions and responses are essential for the zero-shot\nperformance of large language models on interactive natural language tasks. For\ninteractive vision-language tasks involving intricate visual scenes, a large\nquantity of diverse and creative instruction-response pairs should be\nimperative to tune vision-language models (VLMs). Nevertheless, the current\navailability of vision-language instruction-response pairs in terms of\nquantity, diversity, and creativity remains limited, posing challenges to the\ngeneralization of interactive VLMs. Here we present MultI-Modal In-Context\nInstruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal\ninstruction-response pairs, with 2.2 million unique instructions derived from\nimages and videos. Each pair is accompanied by multi-modal in-context\ninformation, forming conversational contexts aimed at empowering VLMs in\nperception, reasoning, and planning. The instruction-response collection\nprocess, dubbed as Syphus, is scaled using an automatic annotation pipeline\nthat combines human expertise with GPT's capabilities. Using the MIMIC-IT\ndataset, we train a large VLM named Otter. Based on extensive evaluations\nconducted on vision-language benchmarks, it has been observed that Otter\ndemonstrates remarkable proficiency in multi-modal perception, reasoning, and\nin-context learning. Human evaluation reveals it effectively aligns with the\nuser's intentions. We release the MIMIC-IT dataset, instruction-response\ncollection pipeline, benchmarks, and the Otter model.",
        "pdf_link": "https://arxiv.org/pdf/2306.05425v1.pdf"
    },
    {
        "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
        "authors": [
            "Qiaoyu Tang",
            "Ziliang Deng",
            "Hongyu Lin",
            "Xianpei Han",
            "Qiao Liang",
            "Boxi Cao",
            "Le Sun"
        ],
        "published": "2023-06-08T15:46:32Z",
        "summary": "Enabling large language models to utilize real-world tools effectively is\ncrucial for achieving embodied intelligence. Existing approaches to tool\nlearning have either primarily relied on extremely large language models, such\nas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or\nutilized supervised learning to train limited scopes of tools on compact\nmodels. However, it remains uncertain whether smaller language models can\nachieve generalized tool-use abilities without tool-specific training. To\naddress this question, this paper introduces ToolAlpaca, a novel framework\ndesigned to automatically generate a diverse tool-use corpus and learn\ngeneralized tool-use abilities on compact language models with minimal human\nintervention. Specifically, ToolAlpaca first automatically creates a highly\ndiversified tool-use corpus by building a multi-agent simulation environment.\nThe corpus contains 3938 tool-use instances from more than 400 real-world tool\nAPIs spanning 50 distinct categories. Subsequently, the constructed corpus is\nemployed to fine-tune compact language models, resulting in two models, namely\nToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the\nability of these models to utilize previously unseen tools without specific\ntraining. Experimental results demonstrate that ToolAlpaca achieves effective\ngeneralized tool-use capabilities comparable to those of extremely large\nlanguage models like GPT-3.5, demonstrating that learning generalized tool-use\nability is feasible for compact language models.",
        "pdf_link": "https://arxiv.org/pdf/2306.05301v2.pdf"
    },
    {
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Xiao Zhang",
            "Yanzhao Lai",
            "Min Peng",
            "Alejandro Lopez-Lira",
            "Jimin Huang"
        ],
        "published": "2023-06-08T14:20:29Z",
        "summary": "Although large language models (LLMs) has shown great performance on natural\nlanguage processing (NLP) in the financial domain, there are no publicly\navailable financial tailtored LLMs, instruction tuning datasets, and evaluation\nbenchmarks, which is critical for continually pushing forward the open-source\ndevelopment of financial artificial intelligence (AI). This paper introduces\nPIXIU, a comprehensive framework including the first financial LLM based on\nfine-tuning LLaMA with instruction data, the first instruction data with 136K\ndata samples to support the fine-tuning, and an evaluation benchmark with 5\ntasks and 9 datasets. We first construct the large-scale multi-task instruction\ndata considering a variety of financial tasks, financial document types, and\nfinancial data modalities. We then propose a financial LLM called FinMA by\nfine-tuning LLaMA with the constructed dataset to be able to follow\ninstructions for various financial tasks. To support the evaluation of\nfinancial LLMs, we propose a standardized benchmark that covers a set of\ncritical financial tasks, including five financial NLP tasks and one financial\nprediction task. With this benchmark, we conduct a detailed analysis of FinMA\nand several existing LLMs, uncovering their strengths and weaknesses in\nhandling critical financial tasks. The model, datasets, benchmark, and\nexperimental results are open-sourced to facilitate future research in\nfinancial AI.",
        "pdf_link": "https://arxiv.org/pdf/2306.05443v1.pdf"
    },
    {
        "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models",
        "authors": [
            "Wenxuan Zhang",
            "Sharifah Mahani Aljunied",
            "Chang Gao",
            "Yew Ken Chia",
            "Lidong Bing"
        ],
        "published": "2023-06-08T13:21:29Z",
        "summary": "Despite the existence of various benchmarks for evaluating natural language\nprocessing models, we argue that human exams are a more suitable means of\nevaluating general intelligence for large language models (LLMs), as they\ninherently demand a much wider range of abilities such as language\nunderstanding, domain knowledge, and problem-solving skills. To this end, we\nintroduce M3Exam, a novel benchmark sourced from real and official human exam\nquestions for evaluating LLMs in a multilingual, multimodal, and multilevel\ncontext. M3Exam exhibits three unique characteristics: (1) multilingualism,\nencompassing questions from multiple countries that require strong multilingual\nproficiency and cultural knowledge; (2) multimodality, accounting for the\nmultimodal nature of many exam questions to test the model's multimodal\nunderstanding capability; and (3) multilevel structure, featuring exams from\nthree critical educational periods to comprehensively assess a model's\nproficiency at different levels. In total, M3Exam contains 12,317 questions in\n9 diverse languages with three educational levels, where about 23\\% of the\nquestions require processing images for successful solving. We assess the\nperformance of top-performing LLMs on M3Exam and find that current models,\nincluding GPT-4, still struggle with multilingual text, particularly in\nlow-resource and non-Latin script languages. Multimodal LLMs also perform\npoorly with complex multimodal questions. We believe that M3Exam can be a\nvaluable resource for comprehensively evaluating LLMs by examining their\nmultilingual and multimodal abilities and tracking their development. Data and\nevaluation code is available at \\url{https://github.com/DAMO-NLP-SG/M3Exam}.",
        "pdf_link": "https://arxiv.org/pdf/2306.05179v2.pdf"
    },
    {
        "title": "Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures",
        "authors": [
            "Yue Zhen",
            "Sheng Bi",
            "Lu Xing-tong",
            "Pan Wei-qin",
            "Shi Hai-peng",
            "Chen Zi-rui",
            "Fang Yi-shu"
        ],
        "published": "2023-06-08T13:10:00Z",
        "summary": "Traditional robot task planning methods face challenges when dealing with\nhighly unstructured environments and complex tasks. We propose a task planning\nmethod that combines human expertise with an LLM and have designed an LLM\nprompt template, Think_Net_Prompt, with stronger expressive power to represent\nstructured professional knowledge. We further propose a method to progressively\ndecompose tasks and generate a task tree to reduce the planning volume for each\ntask, and we have designed a strategy to decouple robot task planning. By\ndividing different planning entities and separating the task from the actual\nmachine binding process, the task planning process becomes more flexible.\nResearch results show that our method performs well in handling specified code\nformats, understanding the relationship between tasks and subtasks, and\nextracting parameters from text descriptions. However, there are also problems\nsuch as limited complexity of task logic handling, ambiguity in the quantity of\nparts and the precise location of assembly. Improving the precision of task\ndescription and cognitive structure can bring certain improvements.\nhttps://github.com/NOMIzy/Think_Net_Prompt",
        "pdf_link": "https://arxiv.org/pdf/2306.05171v1.pdf"
    },
    {
        "title": "Is AI the better programming partner? Human-Human Pair Programming vs. Human-AI pAIr Programming",
        "authors": [
            "Qianou Ma",
            "Tongshuang Wu",
            "Kenneth Koedinger"
        ],
        "published": "2023-06-08T12:22:56Z",
        "summary": "The emergence of large-language models (LLMs) that excel at code generation\nand commercial products such as GitHub's Copilot has sparked interest in\nhuman-AI pair programming (referred to as \"pAIr programming\") where an AI\nsystem collaborates with a human programmer. While traditional pair programming\nbetween humans has been extensively studied, it remains uncertain whether its\nfindings can be applied to human-AI pair programming. We compare human-human\nand human-AI pair programming, exploring their similarities and differences in\ninteraction, measures, benefits, and challenges. We find that the effectiveness\nof both approaches is mixed in the literature (though the measures used for\npAIr programming are not as comprehensive). We summarize moderating factors on\nthe success of human-human pair programming, which provides opportunities for\npAIr programming research. For example, mismatched expertise makes pair\nprogramming less productive, therefore well-designed AI programming assistants\nmay adapt to differences in expertise levels.",
        "pdf_link": "https://arxiv.org/pdf/2306.05153v2.pdf"
    },
    {
        "title": "Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet",
        "authors": [
            "Gonzalo Martínez",
            "Lauren Watson",
            "Pedro Reviriego",
            "José Alberto Hernández",
            "Marc Juarez",
            "Rik Sarkar"
        ],
        "published": "2023-06-08T11:14:51Z",
        "summary": "The rapid adoption of generative Artificial Intelligence (AI) tools that can\ngenerate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have\nput the societal impacts of these technologies at the center of public debate.\nThese tools are possible due to the massive amount of data (text and images)\nthat is publicly available through the Internet. At the same time, these\ngenerative AI tools become content creators that are already contributing to\nthe data that is available to train future models. Therefore, future versions\nof generative AI tools will be trained with a mix of human-created and\nAI-generated content, causing a potential feedback loop between generative AI\nand public data repositories. This interaction raises many questions: how will\nfuture versions of generative AI tools behave when trained on a mixture of real\nand AI generated data? Will they evolve and improve with the new data sets or\non the contrary will they degrade? Will evolution introduce biases or reduce\ndiversity in subsequent generations of generative AI tools? What are the\nsocietal implications of the possible degradation of these models? Can we\nmitigate the effects of this feedback loop? In this document, we explore the\neffect of this interaction and report some initial results using simple\ndiffusion models trained with various image datasets. Our results show that the\nquality and diversity of the generated images can degrade over time suggesting\nthat incorporating AI-created data can have undesired effects on future\nversions of generative models.",
        "pdf_link": "https://arxiv.org/pdf/2306.06130v1.pdf"
    },
    {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "authors": [
            "Yidong Wang",
            "Zhuohao Yu",
            "Zhengran Zeng",
            "Linyi Yang",
            "Cunxiang Wang",
            "Hao Chen",
            "Chaoya Jiang",
            "Rui Xie",
            "Jindong Wang",
            "Xing Xie",
            "Wei Ye",
            "Shikun Zhang",
            "Yue Zhang"
        ],
        "published": "2023-06-08T10:41:56Z",
        "summary": "Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.",
        "pdf_link": "https://arxiv.org/pdf/2306.05087v1.pdf"
    },
    {
        "title": "Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models",
        "authors": [
            "Aleksa Bisercic",
            "Mladen Nikolic",
            "Mihaela van der Schaar",
            "Boris Delibasic",
            "Pietro Lio",
            "Andrija Petrovic"
        ],
        "published": "2023-06-08T09:12:28Z",
        "summary": "Tabular data is often hidden in text, particularly in medical diagnostic\nreports. Traditional machine learning (ML) models designed to work with tabular\ndata, cannot effectively process information in such form. On the other hand,\nlarge language models (LLMs) which excel at textual tasks, are probably not the\nbest tool for modeling tabular data. Therefore, we propose a novel, simple, and\neffective methodology for extracting structured tabular data from textual\nmedical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of\nLLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately\ninferring tabular features, even when their names are not explicitly mentioned\nin the text. This is achieved by combining domain-specific reasoning guidelines\nwith a proposed data validation and reasoning correction feedback loop. By\napplying interpretable ML models such as decision trees and logistic regression\nover the extracted and validated data, we obtain end-to-end interpretable\npredictions. We demonstrate that our approach significantly outperforms\nstate-of-the-art text classification models in medical diagnostics. Given its\npredictive performance, simplicity, and interpretability, TEMED-LLM underscores\nthe potential of leveraging LLMs to improve the performance and trustworthiness\nof ML models in medical applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.05052v1.pdf"
    },
    {
        "title": "Assessing Phrase Break of ESL Speech with Pre-trained Language Models and Large Language Models",
        "authors": [
            "Zhiyi Wang",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Yan Xia",
            "Yan Deng",
            "Jonathan Tien"
        ],
        "published": "2023-06-08T07:10:39Z",
        "summary": "This work introduces approaches to assessing phrase breaks in ESL learners'\nspeech using pre-trained language models (PLMs) and large language models\n(LLMs). There are two tasks: overall assessment of phrase break for a speech\nclip and fine-grained assessment of every possible phrase break position. To\nleverage NLP models, speech input is first force-aligned with texts, and then\npre-processed into a token sequence, including words and phrase break\ninformation. To utilize PLMs, we propose a pre-training and fine-tuning\npipeline with the processed tokens. This process includes pre-training with a\nreplaced break token detection module and fine-tuning with text classification\nand sequence labeling. To employ LLMs, we design prompts for ChatGPT. The\nexperiments show that with the PLMs, the dependence on labeled training data\nhas been greatly reduced, and the performance has improved. Meanwhile, we\nverify that ChatGPT, a renowned LLM, has potential for further advancement in\nthis area.",
        "pdf_link": "https://arxiv.org/pdf/2306.04980v1.pdf"
    },
    {
        "title": "FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs",
        "authors": [
            "Shanshan Han",
            "Baturalp Buyukates",
            "Zijian Hu",
            "Han Jin",
            "Weizhao Jin",
            "Lichao Sun",
            "Xiaoyang Wang",
            "Wenxuan Wu",
            "Chulin Xie",
            "Yuhang Yao",
            "Kai Zhang",
            "Qifan Zhang",
            "Yuhui Zhang",
            "Carlee Joe-Wong",
            "Salman Avestimehr",
            "Chaoyang He"
        ],
        "published": "2023-06-08T06:21:35Z",
        "summary": "This paper introduces FedSecurity, an end-to-end benchmark designed to\nsimulate adversarial attacks and corresponding defense mechanisms in Federated\nLearning (FL). FedSecurity comprises two pivotal components: FedAttacker, which\nfacilitates the simulation of a variety of attacks during FL training, and\nFedDefender, which implements defensive mechanisms to counteract these attacks.\nAs an open-source library, FedSecurity enhances its usability compared to\nfrom-scratch implementations that focus on specific attack/defense scenarios\nbased on the following features: i) It offers extensive customization options\nto accommodate a broad range of machine learning models (e.g., Logistic\nRegression, ResNet, and GAN) and FL optimizers (e.g., FedAVG, FedOPT, and\nFedNOVA); ii) it enables exploring the variability in the effectiveness of\nattacks and defenses across different datasets and models; and iii) it supports\nflexible configuration and customization through a configuration file and some\nprovided APIs. We further demonstrate FedSecurity's utility and adaptability\nthrough federated training of Large Language Models (LLMs), showcasing its\npotential to impact a wide range of complex applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.04959v4.pdf"
    },
    {
        "title": "covLLM: Large Language Models for COVID-19 Biomedical Literature",
        "authors": [
            "Yousuf A. Khan",
            "Clarisse Hokia",
            "Jennifer Xu",
            "Ben Ehlert"
        ],
        "published": "2023-06-08T04:08:32Z",
        "summary": "The COVID-19 pandemic led to 1.1 million deaths in the United States, despite\nthe explosion of coronavirus research. These new findings are slow to translate\nto clinical interventions, leading to poorer patient outcomes and unnecessary\ndeaths. One reason is that clinicians, overwhelmed by patients, struggle to\nkeep pace with the rate of new coronavirus literature. A potential solution is\ndeveloping a tool for evaluating coronavirus literature using large language\nmodels (LLMs) -- neural networks that are deployed for natural language\nprocessing. LLMs can be used to summarize and extract user-specified\ninformation. The greater availability and advancement of LLMs and pre-processed\ncoronavirus literature databases provide the opportunity to assist clinicians\nin evaluating coronavirus literature through a coronavirus literature specific\nLLM (covLLM), a tool that directly takes an inputted research article and a\nuser query to return an answer. Using the COVID-19 Open Research Dataset\n(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of\nhandwritten prompts and synthetic prompts generated using OpenAI, and (2) real\nabstracts, which contains abstract and title pairs. covLLM was trained with\nLLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca\nand synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real\nabstract datasets. These models were evaluated by two human evaluators and\nChatGPT. Results demonstrate that training covLLM on the synCovid and abstract\npairs datasets performs competitively with ChatGPT and outperforms covLLM\ntrained primarily using the Alpaca dataset.",
        "pdf_link": "https://arxiv.org/pdf/2306.04926v1.pdf"
    },
    {
        "title": "Prefer to Classify: Improving Text Classifiers via Auxiliary Preference Learning",
        "authors": [
            "Jaehyung Kim",
            "Jinwoo Shin",
            "Dongyeop Kang"
        ],
        "published": "2023-06-08T04:04:47Z",
        "summary": "The development of largely human-annotated benchmarks has driven the success\nof deep neural networks in various NLP tasks. To enhance the effectiveness of\nexisting benchmarks, collecting new additional input-output pairs is often too\ncostly and challenging, particularly considering their marginal impact on\nimproving the current model accuracy. Instead, additional or complementary\nannotations on the existing input texts in the benchmarks can be preferable as\nan efficient way to pay the additional human cost. In this paper, we\ninvestigate task-specific preferences between pairs of input texts as a new\nalternative way for such auxiliary data annotation. From 'pair-wise'\ncomparisons with respect to the task, the auxiliary preference learning enables\nthe model to learn an additional informative training signal that cannot be\ncaptured with 'instance-wise' task labels. To this end, we propose a novel\nmulti-task learning framework, called prefer-to-classify (P2C), which can enjoy\nthe cooperative effect of learning both the given classification task and the\nauxiliary preferences. Here, we provide three different ways to collect\npreference signals in practice: (a) implicitly extracting from annotation\nrecords (for free, but often unavailable), (b) collecting explicitly from crowd\nworkers (high paid), or (c) pre-trained large language models such as GPT-3\n(low paid). Given existing classification NLP benchmarks, we demonstrate that\nthe proposed auxiliary preference learning via P2C on them is effective in\nimproving text classifiers. Our codes are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2306.04925v1.pdf"
    },
    {
        "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
        "authors": [
            "Ganesh Jawahar",
            "Haichuan Yang",
            "Yunyang Xiong",
            "Zechun Liu",
            "Dilin Wang",
            "Fei Sun",
            "Meng Li",
            "Aasish Pappu",
            "Barlas Oguz",
            "Muhammad Abdul-Mageed",
            "Laks V. S. Lakshmanan",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra"
        ],
        "published": "2023-06-08T00:35:36Z",
        "summary": "Weight-sharing supernet has become a vital component for performance\nestimation in the state-of-the-art (SOTA) neural architecture search (NAS)\nframeworks. Although supernet can directly generate different subnetworks\nwithout retraining, there is no guarantee for the quality of these subnetworks\nbecause of weight sharing. In NLP tasks such as machine translation and\npre-trained language modeling, we observe that given the same model\narchitecture, there is a large performance gap between supernet and training\nfrom scratch. Hence, supernet cannot be directly used and retraining is\nnecessary after finding the optimal architectures.\n  In this work, we propose mixture-of-supernets, a generalized supernet\nformulation where mixture-of-experts (MoE) is adopted to enhance the expressive\npower of the supernet model, with negligible training overhead. In this way,\ndifferent subnetworks do not share the model weights directly, but through an\narchitecture-based routing mechanism. As a result, model weights of different\nsubnetworks are customized towards their specific architectures and the weight\ngeneration is learned by gradient descent. Compared to existing weight-sharing\nsupernet for NLP, our method can minimize the retraining time, greatly\nimproving training efficiency. In addition, the proposed method achieves the\nSOTA performance in NAS for building fast machine translation models, yielding\nbetter latency-BLEU tradeoff compared to HAT, state-of-the-art NAS for MT. We\nalso achieve the SOTA performance in NAS for building memory-efficient\ntask-agnostic BERT models, outperforming NAS-BERT and AutoDistil in various\nmodel sizes.",
        "pdf_link": "https://arxiv.org/pdf/2306.04845v1.pdf"
    },
    {
        "title": "Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers",
        "authors": [
            "Shreya Chandrasekhar",
            "Chieh-Yang Huang",
            "Ting-Hao 'Kenneth' Huang"
        ],
        "published": "2023-06-07T22:56:53Z",
        "summary": "The rapid growth of scientific publications, particularly during the COVID-19\npandemic, emphasizes the need for tools to help researchers efficiently\ncomprehend the latest advancements. One essential part of understanding\nscientific literature is research aspect classification, which categorizes\nsentences in abstracts to Background, Purpose, Method, and Finding. In this\nstudy, we investigate the impact of different datasets on model performance for\nthe crowd-annotated CODA-19 research aspect classification task. Specifically,\nwe explore the potential benefits of using the large, automatically curated\nPubMed 200K RCT dataset and evaluate the effectiveness of large language models\n(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that\nusing the PubMed 200K RCT dataset does not improve performance for the CODA-19\ntask. We also observe that while GPT-4 performs well, it does not outperform\nthe SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance\nof a dedicated and task-aligned datasets dataset for the target task. Our code\nis available at https://github.com/Crowd-AI-Lab/CODA-19-exp.",
        "pdf_link": "https://arxiv.org/pdf/2306.04820v1.pdf"
    },
    {
        "title": "Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation",
        "authors": [
            "Yinda Chen",
            "Che Liu",
            "Wei Huang",
            "Sibo Cheng",
            "Rossella Arcucci",
            "Zhiwei Xiong"
        ],
        "published": "2023-06-07T22:20:51Z",
        "summary": "Vision-Language Pretraining (VLP) has demonstrated remarkable capabilities in\nlearning visual representations from textual descriptions of images without\nannotations. Yet, effective VLP demands large-scale image-text pairs, a\nresource that suffers scarcity in the medical domain. Moreover, conventional\nVLP is limited to 2D images while medical images encompass diverse modalities,\noften in 3D, making the learning process more challenging. To address these\nchallenges, we present Generative Text-Guided 3D Vision-Language Pretraining\nfor Unified Medical Image Segmentation (GTGM), a framework that extends of VLP\nto 3D medical images without relying on paired textual descriptions.\nSpecifically, GTGM utilizes large language models (LLM) to generate\nmedical-style text from 3D medical images. This synthetic text is then used to\nsupervise 3D visual representation learning. Furthermore, a negative-free\ncontrastive learning objective strategy is introduced to cultivate consistent\nvisual representations between augmented 3D medical image patches, which\neffectively mitigates the biases associated with strict positive-negative\nsample pairings. We evaluate GTGM on three imaging modalities - Computed\nTomography (CT), Magnetic Resonance Imaging (MRI), and electron microscopy (EM)\nover 13 datasets. GTGM's superior performance across various medical image\nsegmentation tasks underscores its effectiveness and versatility, by enabling\nVLP extension into 3D medical imagery while bypassing the need for paired text.",
        "pdf_link": "https://arxiv.org/pdf/2306.04811v1.pdf"
    },
    {
        "title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",
        "authors": [
            "Hejie Cui",
            "Jiaying Lu",
            "Shiyu Wang",
            "Ran Xu",
            "Wenjing Ma",
            "Shaojun Yu",
            "Yue Yu",
            "Xuan Kan",
            "Chen Ling",
            "Tianfan Fu",
            "Liang Zhao",
            "Joyce Ho",
            "Fei Wang",
            "Carl Yang"
        ],
        "published": "2023-06-07T21:51:56Z",
        "summary": "Healthcare knowledge graphs (HKGs) are valuable tools for organizing\nbiomedical concepts and their relationships with interpretable structures. The\nrecent advent of large language models (LLMs) has paved the way for building\nmore comprehensive and accurate HKGs. This, in turn, can improve the\nreliability of generated content and enable better evaluation of LLMs. However,\nthe challenges of HKGs such as regarding data heterogeneity and limited\ncoverage are not fully understood, highlighting the need for detailed reviews.\nThis work provides the first comprehensive review of HKGs. It summarizes the\npipeline and key techniques for HKG construction, as well as the common\nutilization approaches, i.e., model-free and model-based. The existing HKG\nresources are also organized based on the data types they capture and\napplication domains they cover, along with relevant statistical information\n(Resource available at\nhttps://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the\napplication level, we delve into the successful integration of HKGs across\nvarious health domains, ranging from fine-grained basic science research to\nhigh-level clinical decision support and public health. Lastly, the paper\nhighlights the opportunities for HKGs in the era of LLMs. This work aims to\nserve as a valuable resource for understanding the potential and opportunities\nof HKG in health research.",
        "pdf_link": "https://arxiv.org/pdf/2306.04802v3.pdf"
    },
    {
        "title": "INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models",
        "authors": [
            "Yew Ken Chia",
            "Pengfei Hong",
            "Lidong Bing",
            "Soujanya Poria"
        ],
        "published": "2023-06-07T20:12:29Z",
        "summary": "Instruction-tuned large language models have revolutionized natural language\nprocessing and have shown great potential in applications such as\nconversational agents. These models, such as GPT-4, can not only master\nlanguage but also solve complex tasks in areas like mathematics, coding,\nmedicine, and law. Despite their impressive capabilities, there is still a lack\nof comprehensive understanding regarding their full potential, primarily due to\nthe black-box nature of many models and the absence of holistic evaluation\nstudies. To address these challenges, we present INSTRUCTEVAL, a more\ncomprehensive evaluation suite designed specifically for instruction-tuned\nlarge language models. Unlike previous works, our evaluation involves a\nrigorous assessment of models based on problem-solving, writing ability, and\nalignment to human values. We take a holistic approach to analyze various\nfactors affecting model performance, including the pretraining foundation,\ninstruction-tuning data, and training methods. Our findings reveal that the\nquality of instruction data is the most crucial factor in scaling model\nperformance. While open-source models demonstrate impressive writing abilities,\nthere is substantial room for improvement in problem-solving and alignment. We\nare encouraged by the rapid development of models by the open-source community,\nbut we also highlight the need for rigorous evaluation to support claims made\nabout these models. Through INSTRUCTEVAL, we aim to foster a deeper\nunderstanding of instruction-tuned models and advancements in their\ncapabilities. INSTRUCTEVAL is publicly available at\nhttps://github.com/declare-lab/instruct-eval.",
        "pdf_link": "https://arxiv.org/pdf/2306.04757v3.pdf"
    },
    {
        "title": "Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models",
        "authors": [
            "Naoki Egami",
            "Musashi Hinck",
            "Brandon M. Stewart",
            "Hanying Wei"
        ],
        "published": "2023-06-07T19:49:41Z",
        "summary": "In computational social science (CSS), researchers analyze documents to\nexplain social and political phenomena. In most scenarios, CSS researchers\nfirst obtain labels for documents and then explain labels using interpretable\nregression analyses in the second step. One increasingly common way to annotate\ndocuments cheaply at scale is through large language models (LLMs). However,\nlike other scalable ways of producing annotations, such surrogate labels are\noften imperfect and biased. We present a new algorithm for using imperfect\nannotation surrogates for downstream statistical analyses while guaranteeing\nstatistical properties -- like asymptotic unbiasedness and proper uncertainty\nquantification -- which are fundamental to CSS research. We show that direct\nuse of surrogate labels in downstream statistical analyses leads to substantial\nbias and invalid confidence intervals, even with high surrogate accuracy of\n80-90%. To address this, we build on debiased machine learning to propose the\ndesign-based supervised learning (DSL) estimator. DSL employs a doubly-robust\nprocedure to combine surrogate labels with a smaller number of high-quality,\ngold-standard labels. Our approach guarantees valid inference for downstream\nstatistical analyses, even when surrogates are arbitrarily biased and without\nrequiring stringent assumptions, by controlling the probability of sampling\ndocuments for gold-standard labeling. Both our theoretical analysis and\nexperimental results show that DSL provides valid statistical inference while\nachieving root mean squared errors comparable to existing alternatives that\nfocus only on prediction without inferential guarantees.",
        "pdf_link": "https://arxiv.org/pdf/2306.04746v3.pdf"
    },
    {
        "title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems",
        "authors": [
            "Yi Zhang",
            "Jan Deriu",
            "George Katsogiannis-Meimarakis",
            "Catherine Kosten",
            "Georgia Koutrika",
            "Kurt Stockinger"
        ],
        "published": "2023-06-07T19:37:55Z",
        "summary": "Natural Language to SQL systems (NL-to-SQL) have recently shown a significant\nincrease in accuracy for natural language to SQL query translation. This\nimprovement is due to the emergence of transformer-based language models, and\nthe popularity of the Spider benchmark - the de-facto standard for evaluating\nNL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\\%.\nHowever, Spider mainly contains simple databases with few tables, columns, and\nentries, which does not reflect a realistic setting. Moreover, complex\nreal-world databases with domain-specific content have little to no training\ndata available in the form of NL/SQL-pairs leading to poor performance of\nexisting NL-to-SQL systems.\n  In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL\nbenchmark for three real-world, highly domain-specific databases. For this new\nbenchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for\neach domain. To garner more data, we extended the small amount of\nhuman-generated data with synthetic data generated using GPT-3. We show that\nour benchmark is highly challenging, as the top performing systems on Spider\nachieve a very low performance on our benchmark. Thus, the challenge is\nmany-fold: creating NL-to-SQL systems for highly complex domains with a small\namount of hand-made training data augmented with synthetic data. To our\nknowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with\ncomplex real-world scientific databases, containing challenging training and\ntest data carefully validated by domain experts.",
        "pdf_link": "https://arxiv.org/pdf/2306.04743v2.pdf"
    },
    {
        "title": "Soft-prompt Tuning for Large Language Models to Evaluate Bias",
        "authors": [
            "Jacob-Junqi Tian",
            "David Emerson",
            "Sevil Zanjani Miyandoab",
            "Deval Pandya",
            "Laleh Seyyed-Kalantari",
            "Faiza Khan Khattak"
        ],
        "published": "2023-06-07T19:11:25Z",
        "summary": "Prompting large language models has gained immense popularity in recent years\ndue to the advantage of producing good results even without the need for\nlabelled data. However, this requires prompt tuning to get optimal prompts that\nlead to better model performances. In this paper, we explore the use of\nsoft-prompt tuning on sentiment classification task to quantify the biases of\nlarge language models (LLMs) such as Open Pre-trained Transformers (OPT) and\nGalactica language model. Since these models are trained on real-world data\nthat could be prone to bias toward certain groups of populations, it is\nimportant to identify these underlying issues. Using soft-prompts to evaluate\nbias gives us the extra advantage of avoiding the human-bias injection that can\nbe caused by manually designed prompts. We check the model biases on different\nsensitive attributes using the group fairness (bias) and find interesting bias\npatterns. Since LLMs have been used in the industry in various applications, it\nis crucial to identify the biases before deploying these models in practice. We\nopen-source our pipeline and encourage industry researchers to adapt our work\nto their use cases.",
        "pdf_link": "https://arxiv.org/pdf/2306.04735v2.pdf"
    },
    {
        "title": "ModuleFormer: Modularity Emerges from Mixture-of-Experts",
        "authors": [
            "Yikang Shen",
            "Zheyu Zhang",
            "Tianyou Cao",
            "Shawn Tan",
            "Zhenfang Chen",
            "Chuang Gan"
        ],
        "published": "2023-06-07T17:59:57Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable results. However,\nexisting models are expensive to train and deploy, and it is also difficult to\nexpand their knowledge beyond pre-training data without forgetting previous\nknowledge. This paper proposes a new neural network architecture, ModuleFormer,\nthat leverages modularity to improve the efficiency and flexibility of large\nlanguage models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).\nUnlike the previous SMoE-based modular language model, which requires\ndomain-labeled data to learn domain-specific experts, ModuleFormer can induce\nmodularity from uncurated data with its new load balancing and concentration\nlosses. ModuleFormer is a modular architecture that includes two different\ntypes of modules: new stick-breaking attention heads and feedforward experts.\nDifferent modules are sparsely activated conditions on the input token during\ntraining and inference. In our experiment, we found that the modular\narchitecture enables three important abilities for large pre-trained language\nmodels: 1) Efficiency, since ModuleFormer only activates a subset of its\nmodules for each input token, thus it could achieve the same performance as\ndense LLMs with more than two times throughput; 2) Extendability, ModuleFormer\nis more immune to catastrophic forgetting than dense LLMs and can be easily\nextended with new modules to learn new knowledge that is not included in the\ntraining data; 3) Specialisation, finetuning ModuleFormer could specialize a\nsubset of modules to the finetuning task and the task-unrelated modules could\nbe easily pruned for a lightweight deployment.",
        "pdf_link": "https://arxiv.org/pdf/2306.04640v2.pdf"
    },
    {
        "title": "On the Reliability of Watermarks for Large Language Models",
        "authors": [
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Manli Shu",
            "Khalid Saifullah",
            "Kezhi Kong",
            "Kasun Fernando",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Tom Goldstein"
        ],
        "published": "2023-06-07T17:58:48Z",
        "summary": "As LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking\nis a simple and effective strategy for mitigating such harms by enabling the\ndetection and documentation of LLM-generated text. Yet a crucial question\nremains: How reliable is watermarking in realistic settings in the wild? There,\nwatermarked text may be modified to suit a user's needs, or entirely rewritten\nto avoid detection.\n  We study the robustness of watermarked text after it is re-written by humans,\nparaphrased by a non-watermarked LLM, or mixed into a longer hand-written\ndocument. We find that watermarks remain detectable even after human and\nmachine paraphrasing. While these attacks dilute the strength of the watermark,\nparaphrases are statistically likely to leak n-grams or even longer fragments\nof the original text, resulting in high-confidence detections when enough\ntokens are observed. For example, after strong human paraphrasing the watermark\nis detectable after observing 800 tokens on average, when setting a 1e-5 false\npositive rate. We also consider a range of new detection schemes that are\nsensitive to short spans of watermarked text embedded inside a large document,\nand we compare the robustness of watermarking to other kinds of detectors.",
        "pdf_link": "https://arxiv.org/pdf/2306.04634v3.pdf"
    },
    {
        "title": "Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations",
        "authors": [
            "Lifan Yuan",
            "Yangyi Chen",
            "Ganqu Cui",
            "Hongcheng Gao",
            "Fangyuan Zou",
            "Xingyi Cheng",
            "Heng Ji",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2023-06-07T17:47:03Z",
        "summary": "This paper reexamines the research on out-of-distribution (OOD) robustness in\nthe field of NLP. We find that the distribution shift settings in previous\nstudies commonly lack adequate challenges, hindering the accurate evaluation of\nOOD robustness. To address these issues, we propose a benchmark construction\nprotocol that ensures clear differentiation and challenging distribution\nshifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution\nrobustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we\nconduct a series of experiments on pre-trained language models for analysis and\nevaluation of OOD robustness. First, for vanilla fine-tuning, we examine the\nrelationship between in-distribution (ID) and OOD performance. We identify\nthree typical types that unveil the inner learning mechanism, which could\npotentially facilitate the forecasting of OOD robustness, correlating with the\nadvancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and\nfind that, despite exhibiting some effectiveness in specific cases, they do not\noffer significant improvement compared to vanilla fine-tuning. Further, we\nevaluate 5 LLMs with various adaptation paradigms and find that when sufficient\nID data is available, fine-tuning domain-specific models outperform LLMs on ID\nexamples significantly. However, in the case of OOD instances, prioritizing\nLLMs with in-context learning yields better results. We identify that both\nfine-tuned small models and LLMs face challenges in effectively addressing\ndownstream tasks. The code is public at\n\\url{https://github.com/lifan-yuan/OOD_NLP}.",
        "pdf_link": "https://arxiv.org/pdf/2306.04618v2.pdf"
    },
    {
        "title": "The Two Word Test: A Semantic Benchmark for Large Language Models",
        "authors": [
            "Nicholas Riccardi",
            "Rutvik H. Desai"
        ],
        "published": "2023-06-07T17:22:03Z",
        "summary": "Large Language Models (LLMs) have shown remarkable abilities recently,\nincluding passing advanced professional exams and demanding benchmark tests.\nThis performance has led many to suggest that they are close to achieving\nhumanlike or 'true' understanding of language, and even Artificial General\nIntelligence (AGI). Here, we provide a new open-source benchmark that can\nassess semantic abilities of LLMs using two-word phrases using a task that can\nbe performed relatively easily by humans without advanced training. Combining\nmultiple words into a single concept is a fundamental aspect of human language\nand intelligence. The test requires meaningfulness judgments of 1768 noun-noun\ncombinations that have been rated as meaningful (e.g., baby boy) or not\nmeaningful (e.g., goat sky). by 150 human raters. We provide versions of the\ntask that probe meaningfulness ratings on a 0-4 scale as well as binary\njudgments. We conducted a series of experiments using the TWT on GPT-4,\nGPT-3.5, and Bard, with both versions. Results demonstrated that, compared to\nhumans, all models perform poorly at rating meaningfulness of these phrases.\nGPT-3.5 and Bard are also unable to make binary discriminations between\nsensible and nonsense phrases as making sense. GPT-4 makes a substantial\nimprovement in binary discrimination of combinatorial phrases but is still\nsignificantly worse than human performance. The TWT can be used to understand\nthe limitations and weaknesses of current LLMs, and potentially improve them.\nThe test also reminds us that caution is warranted in attributing 'true\nunderstanding' or AGI to LLMs. TWT is available at:\nhttps://github.com/NickRiccardi/two-word-test",
        "pdf_link": "https://arxiv.org/pdf/2306.04610v1.pdf"
    },
    {
        "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
        "authors": [
            "Himanshu Thakur",
            "Atishay Jain",
            "Praneetha Vaddamanu",
            "Paul Pu Liang",
            "Louis-Philippe Morency"
        ],
        "published": "2023-06-07T16:50:03Z",
        "summary": "Societal biases present in pre-trained large language models are a critical\nissue as these models have been shown to propagate biases in countless\ndownstream applications, rendering them unfair towards specific groups of\npeople. Since large-scale retraining of these models from scratch is both time\nand compute-expensive, a variety of approaches have been previously proposed\nthat de-bias a pre-trained model. While the majority of current\nstate-of-the-art debiasing methods focus on changes to the training regime, in\nthis paper, we propose data intervention strategies as a powerful yet simple\ntechnique to reduce gender bias in pre-trained models. Specifically, we\nempirically show that by fine-tuning a pre-trained model on only 10 de-biased\n(intervened) training examples, the tendency to favor any gender is\nsignificantly reduced. Since our proposed method only needs a few training\nexamples, our few-shot debiasing approach is highly feasible and practical.\nThrough extensive experimentation, we show that our debiasing technique\nperforms better than competitive state-of-the-art baselines with minimal loss\nin language modeling ability.",
        "pdf_link": "https://arxiv.org/pdf/2306.04597v1.pdf"
    },
    {
        "title": "ChatGPT is fun, but it is not funny! Humor is still challenging Large Language Models",
        "authors": [
            "Sophie Jentzsch",
            "Kristian Kersting"
        ],
        "published": "2023-06-07T16:10:21Z",
        "summary": "Humor is a central aspect of human communication that has not been solved for\nartificial agents so far. Large language models (LLMs) are increasingly able to\ncapture implicit and contextual information. Especially, OpenAI's ChatGPT\nrecently gained immense public attention. The GPT3-based model almost seems to\ncommunicate on a human level and can even tell jokes. Humor is an essential\ncomponent of human communication. But is ChatGPT really funny? We put ChatGPT's\nsense of humor to the test. In a series of exploratory experiments around\njokes, i.e., generation, explanation, and detection, we seek to understand\nChatGPT's capability to grasp and reproduce human humor. Since the model itself\nis not accessible, we applied prompt-based experiments. Our empirical evidence\nindicates that jokes are not hard-coded but mostly also not newly generated by\nthe model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system\naccurately explains valid jokes but also comes up with fictional explanations\nfor invalid jokes. Joke-typical characteristics can mislead ChatGPT in the\nclassification of jokes. ChatGPT has not solved computational humor yet but it\ncan be a big leap toward \"funny\" machines.",
        "pdf_link": "https://arxiv.org/pdf/2306.04563v1.pdf"
    },
    {
        "title": "StudentEval: A Benchmark of Student-Written Prompts for Large Language Models of Code",
        "authors": [
            "Hannah McLean Babe",
            "Sydney Nguyen",
            "Yangtian Zi",
            "Arjun Guha",
            "Molly Q Feldman",
            "Carolyn Jane Anderson"
        ],
        "published": "2023-06-07T16:03:55Z",
        "summary": "Code LLMs are being rapidly deployed and there is evidence that they can make\nprofessional programmers more productive. Current benchmarks for code\ngeneration measure whether models generate correct programs given an expert\nprompt. In this paper, we present a new benchmark containing multiple prompts\nper problem, written by a specific population of non-expert prompters:\nbeginning programmers. StudentEval contains 1,749 prompts for 48 problems,\nwritten by 80 students who have only completed one semester of Python\nprogramming. Our students wrote these prompts while working interactively with\na Code LLM, and we observed very mixed success rates. We use StudentEval to\nevaluate 5 Code LLMs and find that StudentEval is a better discriminator of\nmodel performance than existing benchmarks. We analyze the prompts and find\nsignificant variation in students' prompting techniques. We also find that\nnondeterministic LLM sampling could mislead students into thinking that their\nprompts are more (or less) effective than they actually are, which has\nimplications for how to teach with Code LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.04556v1.pdf"
    },
    {
        "title": "Long-form analogies generated by chatGPT lack human-like psycholinguistic properties",
        "authors": [
            "S. M. Seals",
            "Valerie L. Shalin"
        ],
        "published": "2023-06-07T15:42:31Z",
        "summary": "Psycholinguistic analyses provide a means of evaluating large language model\n(LLM) output and making systematic comparisons to human-generated text. These\nmethods can be used to characterize the psycholinguistic properties of LLM\noutput and illustrate areas where LLMs fall short in comparison to\nhuman-generated text. In this work, we apply psycholinguistic methods to\nevaluate individual sentences from long-form analogies about biochemical\nconcepts. We compare analogies generated by human subjects enrolled in\nintroductory biochemistry courses to analogies generated by chatGPT. We perform\na supervised classification analysis using 78 features extracted from\nCoh-metrix that analyze text cohesion, language, and readability (Graesser et.\nal., 2004). Results illustrate high performance for classifying\nstudent-generated and chatGPT-generated analogies. To evaluate which features\ncontribute most to model performance, we use a hierarchical clustering\napproach. Results from this analysis illustrate several linguistic differences\nbetween the two sources.",
        "pdf_link": "https://arxiv.org/pdf/2306.04537v1.pdf"
    },
    {
        "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Jiaheng Zhou",
            "Zichen Wang",
            "Hao Chen",
            "Yidong Wang",
            "Linyi Yang",
            "Wei Ye",
            "Yue Zhang",
            "Neil Zhenqiang Gong",
            "Xing Xie"
        ],
        "published": "2023-06-07T15:37:00Z",
        "summary": "The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptBench, a robustness\nbenchmark designed to measure LLMs' resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across\nmultiple levels: character, word, sentence, and semantic. The adversarial\nprompts, crafted to mimic plausible user errors like typos or synonyms, aim to\nevaluate how slight deviations can affect LLM outcomes while maintaining\nsemantic integrity. These prompts are then employed in diverse tasks, such as\nsentiment analysis, natural language inference, reading comprehension, machine\ntranslation, and math problem-solving. Our study generates 4788 adversarial\nprompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings\ndemonstrate that contemporary LLMs are not robust to adversarial prompts.\nFurthermore, we present comprehensive analysis to understand the mystery behind\nprompt robustness and its transferability. We then offer insightful robustness\nanalysis and pragmatic recommendations for prompt composition, beneficial to\nboth researchers and everyday users. Code is available at:\nhttps://github.com/microsoft/promptbench.",
        "pdf_link": "https://arxiv.org/pdf/2306.04528v4.pdf"
    },
    {
        "title": "Can current NLI systems handle German word order? Investigating language model performance on a new German challenge set of minimal pairs",
        "authors": [
            "Ines Reinig",
            "Katja Markert"
        ],
        "published": "2023-06-07T15:33:07Z",
        "summary": "Compared to English, German word order is freer and therefore poses\nadditional challenges for natural language inference (NLI). We create WOGLI\n(Word Order in German Language Inference), the first adversarial NLI dataset\nfor German word order that has the following properties: (i) each premise has\nan entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ\nonly in word order and necessary morphological changes to mark case and number.\nIn particular, each premise andits two hypotheses contain exactly the same\nlemmata. Our adversarial examples require the model to use morphological\nmarkers in order to recognise or reject entailment. We show that current German\nautoencoding models fine-tuned on translated NLI data can struggle on this\nchallenge set, reflecting the fact that translated NLI datasets will not mirror\nall necessary language phenomena in the target language. We also examine\nperformance after data augmentation as well as on related word order phenomena\nderived from WOGLI. Our datasets are publically available at\nhttps://github.com/ireinig/wogli.",
        "pdf_link": "https://arxiv.org/pdf/2306.04523v1.pdf"
    },
    {
        "title": "Enhancing In-Context Learning with Answer Feedback for Multi-Span Question Answering",
        "authors": [
            "Zixian Huang",
            "Jiaying Zhou",
            "Gengyang Xiao",
            "Gong Cheng"
        ],
        "published": "2023-06-07T15:20:24Z",
        "summary": "Whereas the recent emergence of large language models (LLMs) like ChatGPT has\nexhibited impressive general performance, it still has a large gap with\nfully-supervised models on specific tasks such as multi-span question\nanswering. Previous researches found that in-context learning is an effective\napproach to exploiting LLM, by using a few task-related labeled data as\ndemonstration examples to construct a few-shot prompt for answering new\nquestions. A popular implementation is to concatenate a few questions and their\ncorrect answers through simple templates, informing LLM of the desired output.\nIn this paper, we propose a novel way of employing labeled data such that it\nalso informs LLM of some undesired output, by extending demonstration examples\nwith feedback about answers predicted by an off-the-shelf model, e.g., correct,\nincorrect, or incomplete. Experiments on three multi-span question answering\ndatasets as well as a keyphrase extraction dataset show that our new prompting\nstrategy consistently improves LLM's in-context learning performance.",
        "pdf_link": "https://arxiv.org/pdf/2306.04508v1.pdf"
    },
    {
        "title": "STEPS: A Benchmark for Order Reasoning in Sequential Tasks",
        "authors": [
            "Weizhi Wang",
            "Hong Wang",
            "Xifeng Yan"
        ],
        "published": "2023-06-07T13:58:55Z",
        "summary": "Various human activities can be abstracted into a sequence of actions in\nnatural text, i.e. cooking, repairing, manufacturing, etc. Such action\nsequences heavily depend on the executing order, while disorder in action\nsequences leads to failure of further task execution by robots or AI agents.\nTherefore, to verify the order reasoning capability of current neural models in\nsequential tasks, we propose a challenging benchmark , named STEPS. STEPS\ninvolves two subtask settings, focusing on determining the rationality of given\nnext step in recipes and selecting the reasonable step from the multi-choice\nquestion, respectively. We describe the data construction and task\nformulations, and benchmark most of significant Large Language Models (LLMs).\nThe experimental results demonstrate 1) The commonsense reasoning of action\norders in sequential tasks are challenging to resolve via zero-shot prompting\nor few-shot in-context learning for LLMs; 2) Prompting method still\nsignificantly lags behind tuning-based method on STEPS.",
        "pdf_link": "https://arxiv.org/pdf/2306.04441v1.pdf"
    },
    {
        "title": "On the Detectability of ChatGPT Content: Benchmarking, Methodology, and Evaluation through the Lens of Academic Writing",
        "authors": [
            "Zeyan Liu",
            "Zijun Yao",
            "Fengjun Li",
            "Bo Luo"
        ],
        "published": "2023-06-07T12:33:24Z",
        "summary": "With ChatGPT under the spotlight, utilizing large language models (LLMs) to\nassist academic writing has drawn a significant amount of debate in the\ncommunity. In this paper, we aim to present a comprehensive study of the\ndetectability of ChatGPT-generated content within the academic literature,\nparticularly focusing on the abstracts of scientific papers, to offer holistic\nsupport for the future development of LLM applications and policies in\nacademia. Specifically, we first present GPABench2, a benchmarking dataset of\nover 2.8 million comparative samples of human-written, GPT-written,\nGPT-completed, and GPT-polished abstracts of scientific writing in computer\nscience, physics, and humanities and social sciences. Second, we explore the\nmethodology for detecting ChatGPT content. We start by examining the\nunsatisfactory performance of existing ChatGPT detecting tools and the\nchallenges faced by human evaluators (including more than 240 researchers or\nstudents). We then test the hand-crafted linguistic features models as a\nbaseline and develop a deep neural framework named CheckGPT to better capture\nthe subtle and deep semantic and linguistic patterns in ChatGPT written\nliterature. Last, we conduct comprehensive experiments to validate the proposed\nCheckGPT framework in each benchmarking task over different disciplines. To\nevaluate the detectability of ChatGPT content, we conduct extensive experiments\non the transferability, prompt engineering, and robustness of CheckGPT.",
        "pdf_link": "https://arxiv.org/pdf/2306.05524v2.pdf"
    },
    {
        "title": "Multilingual Clinical NER: Translation or Cross-lingual Transfer?",
        "authors": [
            "Xavier Fontaine",
            "Félix Gaschi",
            "Parisa Rastin",
            "Yannick Toussaint"
        ],
        "published": "2023-06-07T12:31:07Z",
        "summary": "Natural language tasks like Named Entity Recognition (NER) in the clinical\ndomain on non-English texts can be very time-consuming and expensive due to the\nlack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent\nthis issue thanks to the ability of multilingual large language models to be\nfine-tuned on a specific task in one language and to provide high accuracy for\nthe same task in another language. However, other methods leveraging\ntranslation models can be used to perform NER without annotated data in the\ntarget language, by either translating the training set or test set. This paper\ncompares cross-lingual transfer with these two alternative methods, to perform\nclinical NER in French and in German without any training data in those\nlanguages. To this end, we release MedNERF a medical NER test set extracted\nfrom French drug prescriptions and annotated with the same guidelines as an\nEnglish dataset. Through extensive experiments on this dataset and on a German\nmedical dataset (Frei and Kramer, 2021), we show that translation-based methods\ncan achieve similar performance to CLT but require more care in their design.\nAnd while they can take advantage of monolingual clinical language models,\nthose do not guarantee better results than large general-purpose multilingual\nmodels, whether with cross-lingual transfer or translation.",
        "pdf_link": "https://arxiv.org/pdf/2306.04384v1.pdf"
    },
    {
        "title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks",
        "authors": [
            "Haiyang Xu",
            "Qinghao Ye",
            "Xuan Wu",
            "Ming Yan",
            "Yuan Miao",
            "Jiabo Ye",
            "Guohai Xu",
            "Anwen Hu",
            "Yaya Shi",
            "Guangwei Xu",
            "Chenliang Li",
            "Qi Qian",
            "Maofei Que",
            "Ji Zhang",
            "Xiao Zeng",
            "Fei Huang"
        ],
        "published": "2023-06-07T11:52:36Z",
        "summary": "To promote the development of Vision-Language Pre-training (VLP) and\nmultimodal Large Language Model (LLM) in the Chinese community, we firstly\nrelease the largest public Chinese high-quality video-language dataset named\nYouku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing\nwebsite, with strict criteria of safety, diversity, and quality. Youku-mPLUG\ncontains 10 million Chinese video-text pairs filtered from 400 million raw\nvideos across a wide range of 45 diverse categories for large-scale\npre-training. In addition, to facilitate a comprehensive evaluation of\nvideo-language models, we carefully build the largest human-annotated Chinese\nbenchmarks covering three popular video-language tasks of cross-modal\nretrieval, video captioning, and video category classification. Youku-mPLUG can\nenable researchers to conduct more in-depth multimodal research and develop\nbetter applications in the future. Furthermore, we release popular\nvideo-language pre-training models, ALPRO and mPLUG-2, and our proposed\nmodularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.\nExperiments show that models pre-trained on Youku-mPLUG gain up to 23.1%\nimprovement in video category classification. Besides, mPLUG-video achieves a\nnew state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in\nvideo category classification and 68.9 CIDEr score in video captioning,\nrespectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with\nonly 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate\nimpressive instruction and video understanding ability. The zero-shot\ninstruction understanding experiment indicates that pretraining with\nYouku-mPLUG can enhance the ability to comprehend overall and detailed visual\nsemantics, recognize scene text, and leverage open-domain knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2306.04362v1.pdf"
    },
    {
        "title": "GPT Self-Supervision for a Better Data Annotator",
        "authors": [
            "Xiaohuan Pei",
            "Yanxi Li",
            "Chang Xu"
        ],
        "published": "2023-06-07T11:33:14Z",
        "summary": "The task of annotating data into concise summaries poses a significant\nchallenge across various domains, frequently requiring the allocation of\nsignificant time and specialized knowledge by human experts. Despite existing\nefforts to use large language models for annotation tasks, significant problems\nsuch as limited applicability to unlabeled data, the absence of self-supervised\nmethods, and the lack of focus on complex structured data still persist. In\nthis work, we propose a GPT self-supervision annotation method, which embodies\na generating-recovering paradigm that leverages the one-shot learning\ncapabilities of the Generative Pretrained Transformer (GPT). The proposed\napproach comprises a one-shot tuning phase followed by a generation phase. In\nthe one-shot tuning phase, we sample a data from the support set as part of the\nprompt for GPT to generate a textual summary, which is then used to recover the\noriginal data. The alignment score between the recovered and original data\nserves as a self-supervision navigator to refine the process. In the generation\nstage, the optimally selected one-shot sample serves as a template in the\nprompt and is applied to generating summaries from challenging datasets. The\nannotation performance is evaluated by tuning several human feedback reward\nnetworks and by calculating alignment scores between original and recovered\ndata at both sentence and structure levels. Our self-supervised annotation\nmethod consistently achieves competitive scores, convincingly demonstrating its\nrobust strength in various data-to-summary annotation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.04349v2.pdf"
    },
    {
        "title": "Personality testing of GPT-3: Limited temporal reliability, but highlighted social desirability of GPT-3's personality instruments results",
        "authors": [
            "Bojana Bodroza",
            "Bojana M. Dinic",
            "Ljubisa Bojic"
        ],
        "published": "2023-06-07T10:14:17Z",
        "summary": "As AI-bots continue to gain popularity due to their human-like traits and the\nintimacy they offer to users, their societal impact inevitably expands. This\nleads to the rising necessity for comprehensive studies to fully understand\nAI-bots and reveal their potential opportunities, drawbacks, and overall\nsocietal impact. With that in mind, this research conducted an extensive\ninvestigation into ChatGPT3, a renowned AI bot, aiming to assess the temporal\nreliability of its personality profile. Psychological questionnaires were\nadministered to the chatbot on two separate occasions, followed by a comparison\nof the responses to human normative data. The findings revealed varying levels\nof agreement in chatbot's responses over time, with some scales displaying\nexcellent agreement while others demonstrated poor agreement. Overall,\nDavinci-003 displayed a socially desirable and pro-social personality profile,\nparticularly in the domain of communion. However, the underlying basis of the\nchatbot's responses-whether driven by conscious self reflection or\npredetermined algorithms-remains uncertain.",
        "pdf_link": "https://arxiv.org/pdf/2306.04308v2.pdf"
    },
    {
        "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
        "authors": [
            "John Joon Young Chung",
            "Ece Kamar",
            "Saleema Amershi"
        ],
        "published": "2023-06-07T04:27:09Z",
        "summary": "Large language models (LLMs) can be used to generate text data for training\nand evaluating other models. However, creating high-quality datasets with LLMs\ncan be challenging. In this work, we explore human-AI partnerships to\nfacilitate high diversity and accuracy in LLM-based text data generation. We\nfirst examine two approaches to diversify text generation: 1) logit\nsuppression, which minimizes the generation of languages that have already been\nfrequently generated, and 2) temperature sampling, which flattens the token\nsampling probability. We found that diversification approaches can increase\ndata diversity but often at the cost of data accuracy (i.e., text and labels\nbeing appropriate for the target domain). To address this issue, we examined\ntwo human interventions, 1) label replacement (LR), correcting misaligned\nlabels, and 2) out-of-scope filtering (OOSF), removing instances that are out\nof the user's domain of interest or to which no considered label applies. With\noracle studies, we found that LR increases the absolute accuracy of models\ntrained with diversified datasets by 14.4%. Moreover, we found that some models\ntrained with data generated with LR interventions outperformed LLM-based\nfew-shot classification. In contrast, OOSF was not effective in increasing\nmodel accuracy, implying the need for future work in human-in-the-loop text\ndata generation.",
        "pdf_link": "https://arxiv.org/pdf/2306.04140v1.pdf"
    },
    {
        "title": "Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering",
        "authors": [
            "Jinheon Baek",
            "Alham Fikri Aji",
            "Amir Saffari"
        ],
        "published": "2023-06-07T04:15:21Z",
        "summary": "Large Language Models (LLMs) are capable of performing zero-shot closed-book\nquestion answering tasks, based on their internal knowledge stored in\nparameters during pre-training. However, such internalized knowledge might be\ninsufficient and incorrect, which could lead LLMs to generate factually wrong\nanswers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.\nTo this end, we propose to augment the knowledge directly in the input of LLMs.\nSpecifically, we first retrieve the relevant facts to the input question from\nthe knowledge graph based on semantic similarities between the question and its\nassociated facts. After that, we prepend the retrieved facts to the input\nquestion in the form of the prompt, which is then forwarded to LLMs to generate\nthe answer. Our framework, Knowledge-Augmented language model PromptING\n(KAPING), requires no model training, thus completely zero-shot. We validate\nthe performance of our KAPING framework on the knowledge graph question\nanswering task, that aims to answer the user's question based on facts over a\nknowledge graph, on which ours outperforms relevant zero-shot baselines by up\nto 48% in average, across multiple LLMs of various sizes.",
        "pdf_link": "https://arxiv.org/pdf/2306.04136v1.pdf"
    },
    {
        "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
        "authors": [
            "Zhongbin Xie",
            "Thomas Lukasiewicz"
        ],
        "published": "2023-06-06T23:56:18Z",
        "summary": "The increasingly large size of modern pretrained language models not only\nmakes them inherit more human-like biases from the training corpora, but also\nmakes it computationally expensive to mitigate such biases. In this paper, we\ninvestigate recent parameter-efficient methods in combination with\ncounterfactual data augmentation (CDA) for bias mitigation. We conduct\nextensive experiments with prefix tuning, prompt tuning, and adapter tuning on\ndifferent language models and bias types to evaluate their debiasing\nperformance and abilities to preserve the internal knowledge of a pre-trained\nmodel. We find that the parameter-efficient methods (i) are effective in\nmitigating gender bias, where adapter tuning is consistently the most effective\none and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less\neffective when it comes to racial and religious bias, which may be attributed\nto the limitations of CDA, and (iii) can perform similarly to or sometimes\nbetter than full fine-tuning with improved time and memory efficiency, as well\nas maintain the internal knowledge in BERT and GPT-2, evaluated via fact\nretrieval and downstream fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2306.04067v1.pdf"
    },
    {
        "title": "Certified Deductive Reasoning with Language Models",
        "authors": [
            "Gabriel Poesia",
            "Kanishk Gandhi",
            "Eric Zelikman",
            "Noah D. Goodman"
        ],
        "published": "2023-06-06T21:49:00Z",
        "summary": "Language models often achieve higher accuracy when reasoning step-by-step in\ncomplex tasks. However, even when arriving at a correct final answer, their\nrationales are often logically unsound or inconsistent. This is a major issue\nwhen reliable reasoning traces are needed, such when fine-tuning on\nmodel-generated reasoning for self-improvement. To tackle these issues, we\nintroduce a class of tools for language models called \\emph{guides}, that use\nstate and incremental constraints to guide generation. A guide can be invoked\nby the model to constrain its own generation to a set of valid statements given\nby the tool. In turn, the model's choices can change the guide's state. We show\nhow a general system for logical reasoning can be used as a guide, which we\ncall \\textsc{LogicGuide}. Given a reasoning problem in natural language, a\nmodel can formalize its assumptions for \\textsc{LogicGuide} and guarantee that\nits step-by-step reasoning is sound. In experiments on PrOntoQA, ProofWriter\nand Syllogism Validity datasets, \\textsc{LogicGuide} significantly improves the\nperformance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35\\%),\nwhile drastically reducing \\emph{content effects} -- the interference between\nunwanted prior assumptions and reasoning, which humans and language models\nsuffer from. We then explore bootstrapping GPT-3.5 Turbo and LLaMA using their\nown reasoning traces. We find that LogicGuide is critical: by training only on\ncertified self-generated reasoning, models can self-improve, avoiding learning\nfrom their own hallucinations. Moreover, bootstrapped models enjoy significant\nboosts on ReClor, a challenging real-world reasoning dataset, even when not\nrelying on formalization at inference time.",
        "pdf_link": "https://arxiv.org/pdf/2306.04031v2.pdf"
    },
    {
        "title": "Büyük dil modellerinin Türkçe verisetleri ile eğitilmesi ve ince ayarlanması",
        "authors": [
            "A. Taha Arslan"
        ],
        "published": "2023-06-06T19:31:08Z",
        "summary": "Large language models have advanced enormously, gained vast attraction and\nare having a phase of intensed research. Some of the developed models and\ntraining datasets have been made open-accessible. Hence these may be further\nfine-tuned with some techniques to obtain specialized models for specific\ntasks. When it comes to Turkish language, open-access models do not provide\nsatisfactory coverage. This is also observed over published datasets. In this\nwork, we propose some ideas to mitigate this issue: creating large Turkish\ndatasets, training LLMs with these and fine-tuning pre-trained models with\nTurkish inputs. We report our findings on Turkish-based trainings with the\nproblems encountered along the way. We conclude with outcomes of these\nexperiments and propose ideas for further works.\n  --\n  B\\\"uy\\\"uk dil modelleri inan{\\i}lmaz \\\"ol\\c{c}\\\"ude geli\\c{s}mekte, b\\\"uy\\\"uk\nilgi toplayarak ve \\\"uzerlerinde yo\\u{g}un ara\\c{s}tirmalarin yapildi\\u{g}i bir\nd\\\"onemdedirler. Geli\\c{s}tirilen modeller ve e\\u{g}itimde kullanilan\nverisetlerinden bazilari a\\c{c}ik eri\\c{s}imli olarak sunulmaktadir. B\\\"oylece\nince ayarlama teknikleri uygulayarak \\\"ozelle\\c{s}mi\\c{s} g\\\"orevler i\\c{c}in\n\\c{c}ali\\c{s}abilir modeller elde edilmektedir. T\\\"urk\\c{c}e s\\\"oz konusu\noldu\\u{g}unda bu modellerinin kapsayicili\\u{g}i yeterli d\\\"uzeyde de\\u{g}ildir.\nBu durum, yayimlanan verisetlerinde de g\\\"ozlemlenebilir. Bunu a\\c{s}manin\nyollari T\\\"urk\\c{c}e i\\c{c}erikli b\\\"uy\\\"uk verisetlerinin olu\\c{s}turulmasi,\nb\\\"uy\\\"uk dil modellerinin bunlarla e\\u{g}itilmesi ve \\\"onceden\ne\\u{g}itilmi\\c{s} modellerin T\\\"urk\\c{c}e girdilerle ince ayarlanmalari\nolabilir. Bu \\c{c}ali\\c{s}mada a\\c{c}ik eri\\c{s}imli dil modelleri ve\nverisetleri \\\"uzerinde durulmakta ve T\\\"urk\\c{c}e temelli bazi deneyler,\nkar\\c{s}ila\\c{s}ilan sorunlar ve sonu\\c{c}lar irdelenmektedir.",
        "pdf_link": "https://arxiv.org/pdf/2306.03978v1.pdf"
    },
    {
        "title": "Leveraging Explicit Procedural Instructions for Data-Efficient Action Prediction",
        "authors": [
            "Julia White",
            "Arushi Raghuvanshi",
            "Yada Pruksachatkun"
        ],
        "published": "2023-06-06T18:42:08Z",
        "summary": "Task-oriented dialogues often require agents to enact complex, multi-step\nprocedures in order to meet user requests. While large language models have\nfound success automating these dialogues in constrained environments, their\nwidespread deployment is limited by the substantial quantities of task-specific\ndata required for training. The following paper presents a data-efficient\nsolution to constructing dialogue systems, leveraging explicit instructions\nderived from agent guidelines, such as company policies or customer service\nmanuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a\nlarge language model with a knowledge retrieval module that pulls documents\noutlining relevant procedures from a predefined set of policies, given a\nuser-agent interaction. To train this system, we introduce a semi-supervised\npre-training scheme that employs dialogue-document matching and action-oriented\nmasked language modeling with partial parameter freezing. We evaluate the\neffectiveness of our approach on prominent task-oriented dialogue datasets,\nAction-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue\ntasks: action state tracking and workflow discovery. Our results demonstrate\nthat procedural knowledge augmentation improves accuracy predicting in- and\nout-of-distribution actions while preserving high performance in settings with\nlow or sparse data.",
        "pdf_link": "https://arxiv.org/pdf/2306.03959v1.pdf"
    },
    {
        "title": "MISGENDERED: Limits of Large Language Models in Understanding Pronouns",
        "authors": [
            "Tamanna Hossain",
            "Sunipa Dev",
            "Sameer Singh"
        ],
        "published": "2023-06-06T18:27:52Z",
        "summary": "Content Warning: This paper contains examples of misgendering and erasure\nthat could be offensive and potentially triggering.\n  Gender bias in language technologies has been widely studied, but research\nhas mostly been restricted to a binary paradigm of gender. It is essential also\nto consider non-binary gender identities, as excluding them can cause further\nharm to an already marginalized group. In this paper, we comprehensively\nevaluate popular language models for their ability to correctly use English\ngender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,\nxe, thon) that are used by individuals whose gender identity is not represented\nby binary pronouns. We introduce MISGENDERED, a framework for evaluating large\nlanguage models' ability to correctly use preferred pronouns, consisting of (i)\ninstances declaring an individual's pronoun, followed by a sentence with a\nmissing pronoun, and (ii) an experimental setup for evaluating masked and\nauto-regressive language models using a unified method. When prompted\nout-of-the-box, language models perform poorly at correctly predicting\nneo-pronouns (averaging 7.7% accuracy) and gender-neutral pronouns (averaging\n34.2% accuracy). This inability to generalize results from a lack of\nrepresentation of non-binary pronouns in training data and memorized\nassociations. Few-shot adaptation with explicit examples in the prompt improves\nperformance for neo-pronouns, but only to 64.7% even with 20 shots. We release\nthe full dataset, code, and demo at\nhttps://tamannahossainkay.github.io/misgendered/",
        "pdf_link": "https://arxiv.org/pdf/2306.03950v2.pdf"
    },
    {
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "authors": [
            "Chenxu Hu",
            "Jie Fu",
            "Chenzhuang Du",
            "Simian Luo",
            "Junbo Zhao",
            "Hang Zhao"
        ],
        "published": "2023-06-06T17:58:24Z",
        "summary": "Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .",
        "pdf_link": "https://arxiv.org/pdf/2306.03901v2.pdf"
    },
    {
        "title": "Deductive Verification of Chain-of-Thought Reasoning",
        "authors": [
            "Zhan Ling",
            "Yunhao Fang",
            "Xuanlin Li",
            "Zhiao Huang",
            "Mingu Lee",
            "Roland Memisevic",
            "Hao Su"
        ],
        "published": "2023-06-06T17:18:56Z",
        "summary": "Large Language Models (LLMs) significantly benefit from Chain-of-Thought\n(CoT) prompting in performing various reasoning tasks. While CoT allows models\nto produce more comprehensive reasoning processes, its emphasis on intermediate\nreasoning steps can inadvertently introduce hallucinations and accumulated\nerrors, thereby limiting models' ability to solve complex reasoning tasks.\nInspired by how humans engage in careful and meticulous deductive logical\nreasoning processes to solve tasks, we seek to enable language models to\nperform explicit and rigorous deductive reasoning, and also ensure the\ntrustworthiness of their reasoning process through self-verification. However,\ndirectly verifying the validity of an entire deductive reasoning process is\nchallenging, even with advanced models like ChatGPT. In light of this, we\npropose to decompose a reasoning verification process into a series of\nstep-by-step subprocesses, each only receiving their necessary context and\npremises. To facilitate this procedure, we propose Natural Program, a natural\nlanguage-based deductive reasoning format. Our approach enables models to\ngenerate precise reasoning steps where subsequent steps are more rigorously\ngrounded on prior steps. It also empowers language models to carry out\nreasoning self-verification in a step-by-step manner. By integrating this\nverification process into each deductive reasoning stage, we significantly\nenhance the rigor and trustfulness of generated reasoning steps. Along this\nprocess, we also improve the answer correctness on complex reasoning tasks.\nCode will be released at https://github.com/lz1oceani/verify_cot.",
        "pdf_link": "https://arxiv.org/pdf/2306.03872v3.pdf"
    },
    {
        "title": "Can large language models democratize access to dual-use biotechnology?",
        "authors": [
            "Emily H. Soice",
            "Rafael Rocha",
            "Kimberlee Cordova",
            "Michael Specter",
            "Kevin M. Esvelt"
        ],
        "published": "2023-06-06T15:52:05Z",
        "summary": "Large language models (LLMs) such as those embedded in 'chatbots' are\naccelerating and democratizing research by providing comprehensible information\nand expertise from many different fields. However, these models may also confer\neasy access to dual-use technologies capable of inflicting great harm. To\nevaluate this risk, the 'Safeguarding the Future' course at MIT tasked\nnon-scientist students with investigating whether LLM chatbots could be\nprompted to assist non-experts in causing a pandemic. In one hour, the chatbots\nsuggested four potential pandemic pathogens, explained how they can be\ngenerated from synthetic DNA using reverse genetics, supplied the names of DNA\nsynthesis companies unlikely to screen orders, identified detailed protocols\nand how to troubleshoot them, and recommended that anyone lacking the skills to\nperform reverse genetics engage a core facility or contract research\norganization. Collectively, these results suggest that LLMs will make\npandemic-class agents widely accessible as soon as they are credibly\nidentified, even to people with little or no laboratory training. Promising\nnonproliferation measures include pre-release evaluations of LLMs by third\nparties, curating training datasets to remove harmful concepts, and verifiably\nscreening all DNA generated by synthesis providers or used by contract research\norganizations and robotic cloud laboratories to engineer organisms or viruses.",
        "pdf_link": "https://arxiv.org/pdf/2306.03809v1.pdf"
    },
    {
        "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
        "authors": [
            "Ajay Jaiswal",
            "Shiwei Liu",
            "Tianlong Chen",
            "Zhangyang Wang"
        ],
        "published": "2023-06-06T15:49:09Z",
        "summary": "Large pre-trained transformers are show-stealer in modern-day deep learning,\nand it becomes crucial to comprehend the parsimonious patterns that exist\nwithin them as they grow in scale. With exploding parameter counts, Lottery\nTicket Hypothesis (LTH) and its variants, have lost their pragmatism in\nsparsifying them due to high computation and memory bottleneck of repetitive\ntrain-prune-retrain routine of iterative magnitude pruning (IMP) which worsens\nwith increasing model size. This paper comprehensively studies induced sparse\npatterns across multiple large pre-trained vision and language transformers. We\npropose the existence of -- essential sparsity defined with a sharp dropping\npoint beyond which the performance declines much faster w.r.t the rise of\nsparsity level, when we directly remove weights with the smallest magnitudes in\none-shot without re-training. We also find essential sparsity to hold valid for\nN:M sparsity patterns as well as on modern-scale large language models\n(Vicuna-7B). We also present an intriguing emerging phenomenon of abrupt\nsparsification during the pre-training of BERT, i.e., BERT suddenly becomes\nheavily sparse in pre-training after certain iterations. Moreover, our\nobservations also indicate a counter-intuitive finding that BERT trained with a\nlarger amount of pre-training data tends to have a better ability to condense\nknowledge in comparatively relatively fewer parameters. Lastly, we investigate\nthe effect of the pre-training loss on essential sparsity and discover that\nself-supervised learning (SSL) objectives trigger stronger emergent\nsparsification properties than supervised learning (SL). Our codes are\navailable at \\url{https://github.com/VITA-Group/essential_sparsity}.",
        "pdf_link": "https://arxiv.org/pdf/2306.03805v2.pdf"
    },
    {
        "title": "Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models",
        "authors": [
            "Fobo Shi",
            "Peijun Qing",
            "Dong Yang",
            "Nan Wang",
            "Youbo Lei",
            "Haonan Lu",
            "Xiaodong Lin",
            "Duantengchuan Li"
        ],
        "published": "2023-06-06T15:43:16Z",
        "summary": "Prompt engineering is an essential technique for enhancing the abilities of\nlarge language models (LLMs) by providing explicit and specific instructions.\nIt enables LLMs to excel in various tasks, such as arithmetic reasoning,\nquestion answering, summarization, relation extraction, machine translation,\nand sentiment analysis. Researchers have been actively exploring different\nprompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and\nIn-context learning. However, an unresolved problem arises from the fact that\ncurrent approaches lack a solid mathematical solution for determining optimal\nprompts. To address this issue in prompt engineering, we propose a new and\neffective approach called Prompt Space. Our methodology utilizes text\nembeddings to obtain basis vectors by matrix decomposition, and then constructs\na space for representing all prompts. Prompt Space significantly outperforms\nstate-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,\nwithout the help of the CoT method and the prompt \"Let's think step by step\",\nPrompt Space shows superior performance over the few-shot method. Overall, our\napproach provides a robust and effective mathematical framework for selecting\nsimple and effective prompts. This advancement marks a significant step towards\nimproving prompt engineering for a wide variety of applications in LLMs. Our\ncode is publicly available at\n\\textcolor{blue}{\\url{https://github.com/YouBLEI/Prompt-Space}}",
        "pdf_link": "https://arxiv.org/pdf/2306.03799v2.pdf"
    },
    {
        "title": "Towards End-to-end Speech-to-text Summarization",
        "authors": [
            "Raul Monteiro",
            "Diogo Pernes"
        ],
        "published": "2023-06-06T15:22:16Z",
        "summary": "Speech-to-text (S2T) summarization is a time-saving technique for filtering\nand keeping up with the broadcast news uploaded online on a daily basis. The\nrise of large language models from deep learning with impressive text\ngeneration capabilities has placed the research focus on summarization systems\nthat produce paraphrased compact versions of the document content, also known\nas abstractive summaries. End-to-end (E2E) modelling of S2T abstractive\nsummarization is a promising approach that offers the possibility of generating\nrich latent representations that leverage non-verbal and acoustic information,\nas opposed to the use of only linguistic information from automatically\ngenerated transcripts in cascade systems. However, the few literature on E2E\nmodelling of this task fails on exploring different domains, namely broadcast\nnews, which is challenging domain where large and diversified volumes of data\nare presented to the user every day. We model S2T summarization both with a\ncascade and an E2E system for a corpus of broadcast news in French. Our novel\nE2E model leverages external data by resorting to transfer learning from a\npre-trained T2T summarizer. Experiments show that both our cascade and E2E\nabstractive summarizers are stronger than an extractive baseline. However, the\nperformance of the E2E model still lies behind the cascade one, which is object\nof an extensive analysis that includes future directions to close that gap.",
        "pdf_link": "https://arxiv.org/pdf/2306.05432v1.pdf"
    },
    {
        "title": "Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach",
        "authors": [
            "Bin Hu",
            "Chenyang Zhao",
            "Pu Zhang",
            "Zihao Zhou",
            "Yuanhang Yang",
            "Zenglin Xu",
            "Bin Liu"
        ],
        "published": "2023-06-06T11:49:09Z",
        "summary": "Large language models (LLMs) encode a vast amount of world knowledge acquired\nfrom massive text datasets. Recent studies have demonstrated that LLMs can\nassist an embodied agent in solving complex sequential decision making tasks by\nproviding high-level instructions. However, interactions with LLMs can be\ntime-consuming. In many practical scenarios, they require a significant amount\nof storage space that can only be deployed on remote cloud server nodes.\nAdditionally, using commercial LLMs can be costly since they may charge based\non usage frequency. In this paper, we explore how to enable intelligent\ncost-effective interactions between the agent and an LLM. We find that this\nproblem can be naturally formulated by a Markov decision process (MDP), and\npropose When2Ask, a reinforcement learning based approach that learns when it\nis necessary to query LLMs for high-level instructions to accomplish a target\ntask. Experiments on MiniGrid and Habitat environments that entail planning\nsub-goals demonstrate that When2Ask learns to solve target tasks with only a\nfew necessary interactions with an LLM, and significantly reduces interaction\ncosts in testing environments compared with baseline methods. Experiment\nresults also suggest that by learning a mediator model to interact with the\nLLM, the agent's performance becomes more robust against partial observability\nof the environment. Our code is available at\nhttps://github.com/ZJLAB-AMMI/LLM4RL.",
        "pdf_link": "https://arxiv.org/pdf/2306.03604v6.pdf"
    },
    {
        "title": "Language acquisition: do children and language models follow similar learning stages?",
        "authors": [
            "Linnea Evanson",
            "Yair Lakretz",
            "Jean-Rémi King"
        ],
        "published": "2023-06-06T11:08:20Z",
        "summary": "During language acquisition, children follow a typical sequence of learning\nstages, whereby they first learn to categorize phonemes before they develop\ntheir lexicon and eventually master increasingly complex syntactic structures.\nHowever, the computational principles that lead to this learning trajectory\nremain largely unknown. To investigate this, we here compare the learning\ntrajectories of deep language models to those of children. Specifically, we\ntest whether, during its training, GPT-2 exhibits stages of language\nacquisition comparable to those observed in children aged between 18 months and\n6 years. For this, we train 48 GPT-2 models from scratch and evaluate their\nsyntactic and semantic abilities at each training step, using 96 probes curated\nfrom the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these\nevaluations with the behavior of 54 children during language production. Our\nanalyses reveal three main findings. First, similarly to children, the language\nmodels tend to learn linguistic skills in a systematic order. Second, this\nlearning scheme is parallel: the language tasks that are learned last improve\nfrom the very first training steps. Third, some - but not all - learning stages\nare shared between children and these language models. Overall, these results\nshed new light on the principles of language acquisition, and highlight\nimportant divergences in how humans and modern algorithms learn to process\nnatural language.",
        "pdf_link": "https://arxiv.org/pdf/2306.03586v1.pdf"
    },
    {
        "title": "An Approach to Solving the Abstraction and Reasoning Corpus (ARC) Challenge",
        "authors": [
            "Tan John Chong Min"
        ],
        "published": "2023-06-06T10:08:12Z",
        "summary": "We utilise the power of Large Language Models (LLMs), in particular GPT4, to\nbe prompt engineered into performing an arbitrary task. Here, we give the model\nsome human priors via text, along with some typical procedures for solving the\nARC tasks, and ask it to generate the i) broad description of the input-output\nrelation, ii) detailed steps of the input-output mapping, iii) use the detailed\nsteps to perform manipulation on the test input and derive the test output. The\ncurrent GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those\nwith small grids of 8x8 and below). With tweaks to the prompt to make it more\nspecific for the use case, it can solve more. We posit that when scaled to a\nmulti-agent system with usage of past memory and equipped with an image\ninterpretation tool via Visual Question Answering, we may actually be able to\nsolve the majority of the ARC challenge",
        "pdf_link": "https://arxiv.org/pdf/2306.03553v1.pdf"
    },
    {
        "title": "Applying Standards to Advance Upstream & Downstream Ethics in Large Language Models",
        "authors": [
            "Jose Berengueres",
            "Marybeth Sandell"
        ],
        "published": "2023-06-06T08:47:42Z",
        "summary": "This paper explores how AI-owners can develop safeguards for AI-generated\ncontent by drawing from established codes of conduct and ethical standards in\nother content-creation industries. It delves into the current state of ethical\nawareness on Large Language Models (LLMs). By dissecting the mechanism of\ncontent generation by LLMs, four key areas (upstream/downstream and at user\nprompt/answer), where safeguards could be effectively applied, are identified.\nA comparative analysis of these four areas follows and includes an evaluation\nof the existing ethical safeguards in terms of cost, effectiveness, and\nalignment with established industry practices. The paper's key argument is that\nexisting IT-related ethical codes, while adequate for traditional IT\nengineering, are inadequate for the challenges posed by LLM-based content\ngeneration. Drawing from established practices within journalism, we propose\npotential standards for businesses involved in distributing and selling\nLLM-generated content. Finally, potential conflicts of interest between dataset\ncuration at upstream and ethical benchmarking downstream are highlighted to\nunderscore the need for a broader evaluation beyond mere output. This study\nprompts a nuanced conversation around ethical implications in this rapidly\nevolving field of content generation.",
        "pdf_link": "https://arxiv.org/pdf/2306.03503v2.pdf"
    },
    {
        "title": "Natural Language Commanding via Program Synthesis",
        "authors": [
            "Apurva Gandhi",
            "Thong Q. Nguyen",
            "Huitian Jiao",
            "Robert Steen",
            "Ameya Bhatawdekar"
        ],
        "published": "2023-06-06T07:28:49Z",
        "summary": "We present Semantic Interpreter, a natural language-friendly AI system for\nproductivity software such as Microsoft Office that leverages large language\nmodels (LLMs) to execute user intent across application features. While LLMs\nare excellent at understanding user intent expressed as natural language, they\nare not sufficient for fulfilling application-specific user intent that\nrequires more than text-to-text transformations. We therefore introduce the\nOffice Domain Specific Language (ODSL), a concise, high-level language\nspecialized for performing actions in and interacting with entities in Office\napplications. Semantic Interpreter leverages an Analysis-Retrieval prompt\nconstruction method with LLMs for program synthesis, translating natural\nlanguage user utterances to ODSL programs that can be transpiled to application\nAPIs and then executed. We focus our discussion primarily on a research\nexploration for Microsoft PowerPoint.",
        "pdf_link": "https://arxiv.org/pdf/2306.03460v1.pdf"
    },
    {
        "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
        "authors": [
            "Tuan Dinh",
            "Jinman Zhao",
            "Samson Tan",
            "Renato Negrinho",
            "Leonard Lausen",
            "Sheng Zha",
            "George Karypis"
        ],
        "published": "2023-06-06T06:35:27Z",
        "summary": "Large language models of code (Code-LLMs) have recently brought tremendous\nadvances to code completion, a fundamental feature of programming assistance\nand code intelligence. However, most existing works ignore the possible\npresence of bugs in the code context for generation, which are inevitable in\nsoftware development. Therefore, we introduce and study the buggy-code\ncompletion problem, inspired by the realistic scenario of real-time code\nsuggestion where the code context contains potential bugs -- anti-patterns that\ncan become bugs in the completed program. To systematically study the task, we\nintroduce two datasets: one with synthetic bugs derived from semantics-altering\noperator changes (buggy-HumanEval) and one with realistic bugs derived from\nuser submissions to coding problems (buggy-FixEval). We find that the presence\nof potential bugs significantly degrades the generation performance of the\nhigh-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO\non test cases of buggy-HumanEval drop more than 50% given a single potential\nbug in the context. Finally, we investigate several post-hoc methods for\nmitigating the adverse effect of potential bugs and find that there remains a\nsignificant gap in post-mitigation performance.",
        "pdf_link": "https://arxiv.org/pdf/2306.03438v2.pdf"
    },
    {
        "title": "On the Role of Attention in Prompt-tuning",
        "authors": [
            "Samet Oymak",
            "Ankit Singh Rawat",
            "Mahdi Soltanolkotabi",
            "Christos Thrampoulidis"
        ],
        "published": "2023-06-06T06:23:38Z",
        "summary": "Prompt-tuning is an emerging strategy to adapt large language models (LLM) to\ndownstream tasks by learning a (soft-)prompt parameter from data. Despite its\nsuccess in LLMs, there is limited theoretical understanding of the power of\nprompt-tuning and the role of the attention mechanism in prompting. In this\nwork, we explore prompt-tuning for one-layer attention architectures and study\ncontextual mixture-models where each input token belongs to a context-relevant\nor -irrelevant set. We isolate the role of prompt-tuning through a\nself-contained prompt-attention model. Our contributions are as follows: (1) We\nshow that softmax-prompt-attention is provably more expressive than\nsoftmax-self-attention and linear-prompt-attention under our contextual data\nmodel. (2) We analyze the initial trajectory of gradient descent and show that\nit learns the prompt and prediction head with near-optimal sample complexity\nand demonstrate how prompt can provably attend to sparse context-relevant\ntokens. (3) Assuming a known prompt but an unknown prediction head, we\ncharacterize the exact finite sample performance of prompt-attention which\nreveals the fundamental performance limits and the precise benefit of the\ncontext information. We also provide experiments that verify our theoretical\ninsights on real datasets and demonstrate how prompt-tuning enables the model\nto attend to context-relevant information.",
        "pdf_link": "https://arxiv.org/pdf/2306.03435v1.pdf"
    },
    {
        "title": "Prompting Large Language Models to Reformulate Queries for Moment Localization",
        "authors": [
            "Wenfeng Yan",
            "Shaoxiang Chen",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "published": "2023-06-06T05:48:09Z",
        "summary": "The task of moment localization is to localize a temporal moment in an\nuntrimmed video for a given natural language query. Since untrimmed video\ncontains highly redundant contents, the quality of the query is crucial for\naccurately localizing moments, i.e., the query should provide precise\ninformation about the target moment so that the localization model can\nunderstand what to look for in the videos. However, the natural language\nqueries in current datasets may not be easy to understand for existing models.\nFor example, the Ego4D dataset uses question sentences as the query to describe\nrelatively complex moments. While being natural and straightforward for humans,\nunderstanding such question sentences are challenging for mainstream moment\nlocalization models like 2D-TAN. Inspired by the recent success of large\nlanguage models, especially their ability of understanding and generating\ncomplex natural language contents, in this extended abstract, we make early\nattempts at reformulating the moment queries into a set of instructions using\nlarge language models and making them more friendly to the localization models.",
        "pdf_link": "https://arxiv.org/pdf/2306.03422v1.pdf"
    },
    {
        "title": "Inference-Time Intervention: Eliciting Truthful Answers from a Language Model",
        "authors": [
            "Kenneth Li",
            "Oam Patel",
            "Fernanda Viégas",
            "Hanspeter Pfister",
            "Martin Wattenberg"
        ],
        "published": "2023-06-06T01:26:53Z",
        "summary": "We introduce Inference-Time Intervention (ITI), a technique designed to\nenhance the \"truthfulness\" of large language models (LLMs). ITI operates by\nshifting model activations during inference, following a set of directions\nacross a limited number of attention heads. This intervention significantly\nimproves the performance of LLaMA models on the TruthfulQA benchmark. On an\ninstruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from\n32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and\ndemonstrate how to balance it by tuning the intervention strength. ITI is\nminimally invasive and computationally inexpensive. Moreover, the technique is\ndata efficient: while approaches like RLHF require extensive annotations, ITI\nlocates truthful directions using only few hundred examples. Our findings\nsuggest that LLMs may have an internal representation of the likelihood of\nsomething being true, even as they produce falsehoods on the surface.",
        "pdf_link": "https://arxiv.org/pdf/2306.03341v5.pdf"
    },
    {
        "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents",
        "authors": [
            "Yashar Talebirad",
            "Amirhossein Nadiri"
        ],
        "published": "2023-06-05T23:55:37Z",
        "summary": "In this paper, we present a novel framework for enhancing the capabilities of\nlarge language models (LLMs) by leveraging the power of multi-agent systems.\nOur framework introduces a collaborative environment where multiple intelligent\nagent components, each with distinctive attributes and roles, work together to\nhandle complex tasks more efficiently and effectively. We demonstrate the\npracticality and versatility of our framework through case studies in\nartificial general intelligence (AGI), specifically focusing on the Auto-GPT\nand BabyAGI models. We also examine the \"Gorilla\" model, which integrates\nexternal APIs into the LLM. Our framework addresses limitations and challenges\nsuch as looping issues, security risks, scalability, system evaluation, and\nethical considerations. By modeling various domains such as courtroom\nsimulations and software development scenarios, we showcase the potential\napplications and benefits of our proposed multi-agent system. Our framework\nprovides an avenue for advancing the capabilities and performance of LLMs\nthrough collaboration and knowledge exchange among intelligent agents.",
        "pdf_link": "https://arxiv.org/pdf/2306.03314v1.pdf"
    },
    {
        "title": "shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation",
        "authors": [
            "Sanjeev Kumar Karn",
            "Rikhiya Ghosh",
            "Kusuma P",
            "Oladimeji Farri"
        ],
        "published": "2023-06-05T21:33:04Z",
        "summary": "Instruction-tuned generative Large language models (LLMs) like ChatGPT and\nBloomz possess excellent generalization abilities, but they face limitations in\nunderstanding radiology reports, particularly in the task of generating the\nIMPRESSIONS section from the FINDINGS section. They tend to generate either\nverbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to\nmedical text data during training. We present a system which leverages\nlarge-scale medical text data for domain-adaptive pre-training of\ninstruction-tuned LLMs to enhance its medical knowledge and performance on\nspecific medical tasks. We show that this system performs better in a zero-shot\nsetting than a number of pretrain-and-finetune adaptation methods on the\nIMPRESSIONS generation task, and ranks 1st among participating systems in Task\n1B: Radiology Report Summarization at the BioNLP 2023 workshop.",
        "pdf_link": "https://arxiv.org/pdf/2306.03264v1.pdf"
    },
    {
        "title": "Early Weight Averaging meets High Learning Rates for LLM Pre-training",
        "authors": [
            "Sunny Sanyal",
            "Atula Neerkaje",
            "Jean Kaddour",
            "Abhishek Kumar",
            "Sujay Sanghavi"
        ],
        "published": "2023-06-05T20:51:44Z",
        "summary": "Training Large Language Models (LLMs) incurs significant cost; hence, any\nstrategy that accelerates model convergence is helpful. In this paper, we\ninvestigate the ability of a simple idea checkpoint averaging along the\ntrajectory of a training run to improve both convergence and generalization\nquite early on during training. Here we show that models trained with high\nlearning rates observe higher gains due to checkpoint averaging. Furthermore,\nthese gains are amplified when checkpoints are sampled with considerable\nspacing in training steps. Our training recipe outperforms conventional\ntraining and popular checkpoint averaging baselines such as exponential moving\naverage (EMA) and stochastic moving average (SWA). We evaluate our training\nrecipe by pre-training LLMs, where high learning rates are inherently preferred\ndue to extremely large batch sizes. Specifically, we pre-trained nanoGPT-2\nmodels of varying sizes, small (125M), medium (335M), and large (770M)on the\nOpenWebText dataset, comprised of 9B tokens. Additionally, we present results\nfor publicly available Pythia LLMs, ranging from 1B to 12B, which were trained\non the PILE-deduped dataset containing 207B tokens.",
        "pdf_link": "https://arxiv.org/pdf/2306.03241v2.pdf"
    },
    {
        "title": "NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks",
        "authors": [
            "Jean-Michel Attendu",
            "Jean-Philippe Corbeil"
        ],
        "published": "2023-06-05T19:30:41Z",
        "summary": "Finetuning large language models inflates the costs of NLU applications and\nremains the bottleneck of development cycles. Recent works in computer vision\nuse data pruning to reduce training time. Pruned data selection with static\nmethods is based on a score calculated for each training example prior to\nfinetuning, which involves important computational overhead. Moreover, the\nscore may not necessarily be representative of sample importance throughout the\nentire training duration. We propose to address these issues with a refined\nversion of dynamic data pruning, a curriculum which periodically scores and\ndiscards unimportant examples during finetuning. Our method leverages an EL2N\nmetric that we extend to the joint intent and slot classification task, and an\ninitial finetuning phase on the full train set. Our results on the GLUE\nbenchmark and four joint NLU datasets show a better time-accuracy trade-off\ncompared to static methods. Our method preserves full accuracy while training\non 50% of the data points and reduces computational times by up to 41%. If we\ntolerate instead a minor drop of accuracy of 1%, we can prune 80% of the\ntraining examples for a reduction in finetuning time reaching 66%.",
        "pdf_link": "https://arxiv.org/pdf/2306.03208v1.pdf"
    },
    {
        "title": "ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs",
        "authors": [
            "Levente Juhász",
            "Peter Mooney",
            "Hartwig H. Hochmair",
            "Boyuan Guan"
        ],
        "published": "2023-06-05T19:26:21Z",
        "summary": "This paper explores the concept of leveraging generative AI as a mapping\nassistant for enhancing the efficiency of collaborative mapping. We present\nresults of an experiment that combines multiple sources of volunteered\ngeographic information (VGI) and large language models (LLMs). Three analysts\ndescribed the content of crowdsourced Mapillary street-level photographs taken\nalong roads in a small test area in Miami, Florida. GPT-3.5-turbo was\ninstructed to suggest the most appropriate tagging for each road in\nOpenStreetMap (OSM). The study also explores the utilization of BLIP-2, a\nstate-of-the-art multimodal pre-training method as an artificial analyst of\nstreet-level photographs in addition to human analysts. Results demonstrate two\nways to effectively increase the accuracy of mapping suggestions without\nmodifying the underlying AI models: by (1) providing a more detailed\ndescription of source photographs, and (2) combining prompt engineering with\nadditional context (e.g. location and objects detected along a road). The first\napproach increases the suggestion accuracy by up to 29%, and the second one by\nup to 20%.",
        "pdf_link": "https://arxiv.org/pdf/2306.03204v2.pdf"
    },
    {
        "title": "A Static Evaluation of Code Completion by Large Language Models",
        "authors": [
            "Hantian Ding",
            "Varun Kumar",
            "Yuchen Tian",
            "Zijian Wang",
            "Rob Kwiatkowski",
            "Xiaopeng Li",
            "Murali Krishna Ramanathan",
            "Baishakhi Ray",
            "Parminder Bhatia",
            "Sudipta Sengupta",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023-06-05T19:23:34Z",
        "summary": "Large language models trained on code have shown great potential to increase\nproductivity of software developers. Several execution-based benchmarks have\nbeen proposed to evaluate functional correctness of model-generated code on\nsimple programming problems. Nevertheless, it is expensive to perform the same\nevaluation on complex real-world projects considering the execution cost. On\nthe contrary, static analysis tools such as linters, which can detect errors\nwithout running the program, haven't been well explored for evaluating code\ngeneration models. In this work, we propose a static evaluation framework to\nquantify static errors in Python code completions, by leveraging Abstract\nSyntax Trees. Compared with execution-based evaluation, our method is not only\nmore efficient, but also applicable to code in the wild. For experiments, we\ncollect code context from open source repos to generate one million function\nbodies using public models. Our static analysis reveals that Undefined Name and\nUnused Variable are the most common errors among others made by language\nmodels. Through extensive studies, we also show the impact of sampling\ntemperature, model size, and context on static errors in code completions.",
        "pdf_link": "https://arxiv.org/pdf/2306.03203v1.pdf"
    },
    {
        "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
        "authors": [
            "Lichang Chen",
            "Jiuhai Chen",
            "Tom Goldstein",
            "Heng Huang",
            "Tianyi Zhou"
        ],
        "published": "2023-06-05T17:55:22Z",
        "summary": "Large language models~(LLMs) are instruction followers, but it can be\nchallenging to find the best instruction for different situations, especially\nfor black-box LLMs on which backpropagation is forbidden. Instead of directly\noptimizing the discrete instruction, we optimize a low-dimensional soft prompt\napplied to an open-source LLM to generate the instruction for the black-box\nLLM. On each iteration of the proposed method, which we call InstructZero, a\nsoft prompt is converted into an instruction using the open-source LLM, which\nis then submitted to the black-box LLM for zero-shot evaluation, and the\nperformance is sent to Bayesian optimization to produce new soft prompts\nimproving the zero-shot performance. We evaluate InstructZero on different\ncombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our\nresults show that InstructZero outperforms SOTA auto-instruction methods across\na variety of downstream tasks. Our code and data are publicly available at\nhttps://github.com/Lichang-Chen/InstructZero.",
        "pdf_link": "https://arxiv.org/pdf/2306.03082v2.pdf"
    },
    {
        "title": "Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs",
        "authors": [
            "Alexander K. Lew",
            "Tan Zhi-Xuan",
            "Gabriel Grand",
            "Vikash K. Mansinghka"
        ],
        "published": "2023-06-05T17:55:05Z",
        "summary": "Even after fine-tuning and reinforcement learning, large language models\n(LLMs) can be difficult, if not impossible, to control reliably with prompts\nalone. We propose a new inference-time approach to enforcing syntactic and\nsemantic constraints on the outputs of LLMs, called sequential Monte Carlo\n(SMC) steering. The key idea is to specify language generation tasks as\nposterior inference problems in a class of discrete probabilistic sequence\nmodels, and replace standard decoding with sequential Monte Carlo inference.\nFor a computational cost similar to that of beam search, SMC can steer LLMs to\nsolve diverse tasks, including infilling, generation under syntactic\nconstraints, and prompt intersection. To facilitate experimentation with SMC\nsteering, we present a probabilistic programming library, LLaMPPL\n(https://github.com/probcomp/hfppl), for concisely specifying new generation\ntasks as language model probabilistic programs, and automating steering of\nLLaMA-family Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2306.03081v2.pdf"
    },
    {
        "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
        "authors": [
            "Tim Dettmers",
            "Ruslan Svirschevski",
            "Vage Egiazarian",
            "Denis Kuznedelev",
            "Elias Frantar",
            "Saleh Ashkboos",
            "Alexander Borzunov",
            "Torsten Hoefler",
            "Dan Alistarh"
        ],
        "published": "2023-06-05T17:53:28Z",
        "summary": "Recent advances in large language model (LLM) pretraining have led to\nhigh-quality LLMs with impressive abilities. By compressing such LLMs via\nquantization to 3-4 bits per parameter, they can fit into memory-limited\ndevices such as laptops and mobile phones, enabling personalized use. However,\nquantization down to 3-4 bits per parameter usually leads to moderate-to-high\naccuracy losses, especially for smaller models in the 1-10B parameter range,\nwhich are well-suited for edge deployments. To address this accuracy issue, we\nintroduce the Sparse-Quantized Representation (SpQR), a new compressed format\nand quantization technique which enables for the first time near-lossless\ncompression of LLMs across model scales, while reaching similar compression\nlevels to previous methods. SpQR works by identifying and isolating outlier\nweights, which cause particularly-large quantization errors, and storing them\nin higher precision, while compressing all other weights to 3-4 bits, and\nachieves relative accuracy losses of less than 1% in perplexity for\nhighly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B\nparameter LLM on a single 24 GB consumer GPU without any performance\ndegradation at 15% speedup thus making powerful LLMs available to consumer\nwithout any downsides. SpQR comes with efficient algorithms for both encoding\nweights into its format, as well as decoding them efficiently at runtime.\nSpecifically, we provide an efficient GPU inference algorithm for SpQR which\nyields faster inference than 16-bit baselines at similar accuracy, while\nenabling memory compression gains of more than 4x.",
        "pdf_link": "https://arxiv.org/pdf/2306.03078v1.pdf"
    },
    {
        "title": "Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion",
        "authors": [
            "Ryo Sekizawa",
            "Hitomi Yanaka"
        ],
        "published": "2023-06-05T17:27:48Z",
        "summary": "Using Japanese honorifics is challenging because it requires not only\nknowledge of the grammatical rules but also contextual information, such as\nsocial relationships. It remains unclear whether pre-trained large language\nmodels (LLMs) can flexibly handle Japanese honorifics like humans. To analyze\nthis, we introduce an honorific conversion task that considers social\nrelationships among people mentioned in a conversation. We construct a Japanese\nhonorifics dataset from problem templates of various sentence structures to\ninvestigate the syntactic generalization capacity of GPT-3, one of the leading\nLLMs, on this task under two settings: fine-tuning and prompt learning. Our\nresults showed that the fine-tuned GPT-3 performed better in a context-aware\nhonorific conversion task than the prompt-based one. The fine-tuned model\ndemonstrated overall syntactic generalizability towards compound honorific\nsentences, except when tested with the data involving direct speech.",
        "pdf_link": "https://arxiv.org/pdf/2306.03055v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset",
        "authors": [
            "Junling Liu",
            "Peilin Zhou",
            "Yining Hua",
            "Dading Chong",
            "Zhongyu Tian",
            "Andrew Liu",
            "Helin Wang",
            "Chenyu You",
            "Zhenhua Guo",
            "Lei Zhu",
            "Michael Lingzhi Li"
        ],
        "published": "2023-06-05T16:48:41Z",
        "summary": "Recent advancements in large language models (LLMs) have transformed the\nfield of question answering (QA). However, evaluating LLMs in the medical field\nis challenging due to the lack of standardized and comprehensive datasets. To\naddress this gap, we introduce CMExam, sourced from the Chinese National\nMedical Licensing Examination. CMExam consists of 60K+ multiple-choice\nquestions for standardized and objective evaluations, as well as solution\nexplanations for model reasoning evaluation in an open-ended manner. For\nin-depth analyses of LLMs, we invited medical professionals to label five\nadditional question-wise annotations, including disease groups, clinical\ndepartments, medical disciplines, areas of competency, and question difficulty\nlevels. Alongside the dataset, we further conducted thorough experiments with\nrepresentative LLMs and QA algorithms on CMExam. The results show that GPT-4\nhad the best accuracy of 61.6% and a weighted F1 score of 0.617. These results\nhighlight a great disparity when compared to human accuracy, which stood at\n71.6%. For explanation tasks, while LLMs could generate relevant reasoning and\ndemonstrate improved performance after finetuning, they fall short of a desired\nstandard, indicating ample room for improvement. To the best of our knowledge,\nCMExam is the first Chinese medical exam dataset to provide comprehensive\nmedical annotations. The experiments and findings of LLM evaluation also\nprovide valuable insights into the challenges and potential solutions in\ndeveloping Chinese medical QA systems and LLM evaluation pipelines. The dataset\nand relevant code are available at https://github.com/williamliujl/CMExam.",
        "pdf_link": "https://arxiv.org/pdf/2306.03030v3.pdf"
    },
    {
        "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
        "authors": [
            "Shuyang Jiang",
            "Yuhao Wang",
            "Yu Wang"
        ],
        "published": "2023-06-05T14:12:46Z",
        "summary": "Large language models (LLMs) have already revolutionized code generation,\nafter being pretrained on publicly available code data. However, while various\nmethods have been proposed to augment LLMs with retrieved knowledge and enhance\nthe quality of code generation, the performance of these retrieval-based\nmethods is limited by the strength of the retrievers used. In addition, while\nLLMs show great emergent ability, they still struggle to produce the correct\ncode in one turn. To address these challenges, we propose a novel two-step\npipeline, called \\autoknow, that leverages LLMs as both knowledge providers and\nself-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains\nthe knowledge from input prompts and generates intermediate code based on the\ngenerated knowledge. After that, \\autoknow~asks LLM to act as an expert\nprogrammer to perform debugging for the generated code. This is achieved by\nreceiving the error message from the interpreter, without requiring special\ntest cases for correctness verification. We evaluate \\autoknow~on three code\ngeneration datasets, including DS-1000 for data science code, HumanEval for\nsoftware engineering code, and TransCoder for C++-to-Python translation. Our\nempirical experiments show that \\autoknow~outperforms strong baselines by a\nsignificant margin on all datasets. We also conduct exhaustive analytical\nexperiments to validate the effectiveness of the two stages of \\autoknow, and\nfind that both are superior to other prompting-based methods. Further\nscalability analysis demonstrates that \\autoknow~can be adapted to other more\nadvanced models, such as GPT-4, and bring consistent efficacy improvement.",
        "pdf_link": "https://arxiv.org/pdf/2306.02907v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs",
        "authors": [
            "Alejandro Peña",
            "Aythami Morales",
            "Julian Fierrez",
            "Ignacio Serna",
            "Javier Ortega-Garcia",
            "Iñigo Puente",
            "Jorge Cordova",
            "Gonzalo Cordova"
        ],
        "published": "2023-06-05T13:35:01Z",
        "summary": "The analysis of public affairs documents is crucial for citizens as it\npromotes transparency, accountability, and informed decision-making. It allows\ncitizens to understand government policies, participate in public discourse,\nand hold representatives accountable. This is crucial, and sometimes a matter\nof life or death, for companies whose operation depend on certain regulations.\nLarge Language Models (LLMs) have the potential to greatly enhance the analysis\nof public affairs documents by effectively processing and understanding the\ncomplex language used in such documents. In this work, we analyze the\nperformance of LLMs in classifying public affairs documents. As a natural\nmulti-label task, the classification of these documents presents important\nchallenges. In this work, we use a regex-powered tool to collect a database of\npublic affairs documents with more than 33K samples and 22.5M tokens. Our\nexperiments assess the performance of 4 different Spanish LLMs to classify up\nto 30 different topics in the data in different configurations. The results\nshows that LLMs can be of great use to process domain-specific documents, such\nas those in the domain of public affairs.",
        "pdf_link": "https://arxiv.org/pdf/2306.02864v2.pdf"
    },
    {
        "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
        "authors": [
            "Hang Zhang",
            "Xin Li",
            "Lidong Bing"
        ],
        "published": "2023-06-05T13:17:27Z",
        "summary": "We present Video-LLaMA a multi-modal framework that empowers Large Language\nModels (LLMs) with the capability of understanding both visual and auditory\ncontent in the video. Video-LLaMA bootstraps cross-modal training from the\nfrozen pre-trained visual and audio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to process the visual or audio signals\nonly, Video-LLaMA enables video comprehension by tackling two challenges: (1)\ncapturing the temporal changes in visual scenes, (2) integrating audio-visual\nsignals. To counter the first challenge, we propose a Video Q-former to\nassemble a pre-trained image encoder into our video encoder and introduce a\nvideo-to-text generation task to learn video-language correspondence. For the\nsecond challenge, we leverage ImageBind, a universal embedding model aligning\nmultiple modalities, as the pre-trained audio encoder and introduce an Audio\nQ-former on top of ImageBind to learn reasonable auditory query embeddings for\nthe LLM module. To align the output of both visual and audio encoders with\nLLM's embedding space, we first train Video-LLaMA on massive\nvideo/image-caption pairs and then tune our model with visual-instruction\ndatasets of moderate amount but higher quality. We found Video-LLaMA shows the\nability to perceive and comprehend video content and generate meaningful\nresponses grounded in the visual and auditory information presented in the\nvideos.",
        "pdf_link": "https://arxiv.org/pdf/2306.02858v4.pdf"
    },
    {
        "title": "Cheap-fake Detection with LLM using Prompt Engineering",
        "authors": [
            "Guangyang Wu",
            "Weijie Wu",
            "Xiaohong Liu",
            "Kele Xu",
            "Tianjiao Wan",
            "Wenyi Wang"
        ],
        "published": "2023-06-05T11:01:00Z",
        "summary": "The misuse of real photographs with conflicting image captions in news items\nis an example of the out-of-context (OOC) misuse of media. In order to detect\nOOC media, individuals must determine the accuracy of the statement and\nevaluate whether the triplet (~\\textit{i.e.}, the image and two captions)\nrelates to the same event. This paper presents a novel learnable approach for\ndetecting OOC media in ICME'23 Grand Challenge on Detecting Cheapfakes. The\nproposed method is based on the COSMOS structure, which assesses the coherence\nbetween an image and captions, as well as between two captions. We enhance the\nbaseline algorithm by incorporating a Large Language Model (LLM), GPT3.5, as a\nfeature extractor. Specifically, we propose an innovative approach to feature\nextraction utilizing prompt engineering to develop a robust and reliable\nfeature extractor with GPT3.5 model. The proposed method captures the\ncorrelation between two captions and effectively integrates this module into\nthe COSMOS baseline model, which allows for a deeper understanding of the\nrelationship between captions. By incorporating this module, we demonstrate the\npotential for significant improvements in cheap-fakes detection performance.\nThe proposed methodology holds promising implications for various applications\nsuch as natural language processing, image captioning, and text-to-image\nsynthesis. Docker for submission is available at\nhttps://hub.docker.com/repository/docker/mulns/ acmmmcheapfakes.",
        "pdf_link": "https://arxiv.org/pdf/2306.02776v1.pdf"
    },
    {
        "title": "Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in Australia",
        "authors": [
            "Benjamin Kereopa-Yorke"
        ],
        "published": "2023-06-05T06:01:00Z",
        "summary": "The escalating digitalisation of our lives and enterprises has led to a\nparallel growth in the complexity and frequency of cyber-attacks. Small and\nmedium-sized enterprises (SMEs), particularly in Australia, are experiencing\nincreased vulnerability to cyber threats, posing a significant challenge to the\nnation's cyber security landscape. Embracing transformative technologies such\nas Artificial Intelligence (AI), Machine Learning (ML) and Large Language\nModels (LLMs) can potentially strengthen cyber security policies for Australian\nSMEs. However, their practical application, advantages, and limitations remain\nunderexplored, with prior research mainly focusing on large corporations. This\nstudy aims to address this gap by providing a comprehensive understanding of\nthe potential role of LLMs in enhancing cyber security policies for Australian\nSMEs. Employing a mixed-methods study design, this research includes a\nliterature review, qualitative analysis of SME case studies, and a quantitative\nassessment of LLM performance metrics in cyber security applications. The\nfindings highlight the promising potential of LLMs across various performance\ncriteria, including relevance, accuracy, and applicability, though gaps remain\nin areas such as completeness and clarity. The study underlines the importance\nof integrating human expertise with LLM technology and refining model\ndevelopment to address these limitations. By proposing a robust conceptual\nframework guiding the effective adoption of LLMs, this research aims to\ncontribute to a safer and more resilient cyber environment for Australian SMEs,\nenabling sustainable growth and competitiveness in the digital era.",
        "pdf_link": "https://arxiv.org/pdf/2306.02612v1.pdf"
    },
    {
        "title": "User Behavior Simulation with Large Language Model based Agents",
        "authors": [
            "Lei Wang",
            "Jingsen Zhang",
            "Hao Yang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Zeyu Zhang",
            "Xu Chen",
            "Yankai Lin",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Jun Xu",
            "Zhicheng Dou",
            "Jun Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023-06-05T02:58:35Z",
        "summary": "Simulating high quality user behavior data has always been a fundamental\nproblem in human-centered applications, where the major difficulty originates\nfrom the intricate mechanism of human decision process. Recently, substantial\nevidences have suggested that by learning huge amounts of web knowledge, large\nlanguage models (LLMs) can achieve human-like intelligence. We believe these\nmodels can provide significant opportunities to more believable user behavior\nsimulation. To inspire such direction, we propose an LLM-based agent framework\nand design a sandbox environment to simulate real user behaviors. Based on\nextensive experiments, we find that the simulated behaviors of our method are\nvery close to the ones of real humans. Concerning potential applications, we\nsimulate and study two social phenomenons including (1) information cocoons and\n(2) user conformity behaviors. This research provides novel simulation\nparadigms for human-centered applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.02552v3.pdf"
    },
    {
        "title": "Evaluation of AI Chatbots for Patient-Specific EHR Questions",
        "authors": [
            "Alaleh Hamidi",
            "Kirk Roberts"
        ],
        "published": "2023-06-05T02:52:54Z",
        "summary": "This paper investigates the use of artificial intelligence chatbots for\npatient-specific question answering (QA) from clinical notes using several\nlarge language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google\nBard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and\ncoherence of the answers generated by each model using a 5-point Likert scale\non a set of patient-specific questions.",
        "pdf_link": "https://arxiv.org/pdf/2306.02549v1.pdf"
    },
    {
        "title": "Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning",
        "authors": [
            "Beichen Zhang",
            "Kun Zhou",
            "Xilin Wei",
            "Wayne Xin Zhao",
            "Jing Sha",
            "Shijin Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023-06-04T17:02:59Z",
        "summary": "Chain-of-thought prompting~(CoT) and tool augmentation have been validated in\nrecent work as effective practices for improving large language models~(LLMs)\nto perform step-by-step reasoning on complex math-related tasks. However, most\nexisting math reasoning datasets may be not able to fully evaluate and analyze\nthe ability of LLMs in manipulating tools and performing reasoning, as they may\nonly require very few invocations of tools or miss annotations for evaluating\nintermediate reasoning steps. To address the issue, we construct \\textbf{CARP},\na new Chinese dataset consisting of 4,886 computation-intensive algebra\nproblems with formulated annotations on intermediate steps. In CARP, we test\nfour LLMs with CoT prompting, and find that they are all prone to make mistakes\nat the early steps of the solution, leading to wrong answers. Based on this\nfinding, we propose a new approach that can deliberate the reasoning steps with\ntool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a\nstep-by-step solution based on retrieved exemplars, then iterate two\ndeliberation procedures that check and refine the intermediate steps of the\ngenerated solution, from the perspectives of tool manipulation and natural\nlanguage reasoning, until obtaining converged solutions or reaching the maximum\nturn. Experimental results on CARP and six other datasets show that the\nproposed DELI mostly outperforms competitive baselines, and can further boost\nthe performance of existing CoT methods. Our data and code are available in\n\\url{https://github.com/RUCAIBox/CARP}.",
        "pdf_link": "https://arxiv.org/pdf/2306.02408v1.pdf"
    },
    {
        "title": "Commonsense Knowledge Transfer for Pre-trained Language Models",
        "authors": [
            "Wangchunshu Zhou",
            "Ronan Le Bras",
            "Yejin Choi"
        ],
        "published": "2023-06-04T15:44:51Z",
        "summary": "Despite serving as the foundation models for a wide range of NLP benchmarks,\npre-trained language models have shown limited capabilities of acquiring\nimplicit commonsense knowledge from self-supervision alone, compared to\nlearning linguistic and factual knowledge that appear more explicitly in the\nsurface patterns in text. In this work, we introduce commonsense knowledge\ntransfer, a framework to transfer the commonsense knowledge stored in a neural\ncommonsense knowledge model to a general-purpose pre-trained language model. It\nfirst exploits general texts to form queries for extracting commonsense\nknowledge from the neural commonsense knowledge model and then refines the\nlanguage model with two self-supervised objectives: commonsense mask infilling\nand commonsense relation prediction, which align human language with the\nunderlying commonsense knowledge. Empirical results show that our approach\nconsistently improves the model's performance on downstream tasks that require\ncommonsense reasoning. Moreover, we find that the improvement is more\nsignificant in the few-shot setting. This suggests that our approach helps\nlanguage models better transfer to downstream tasks without extensive\nsupervision by injecting commonsense knowledge into their parameters.",
        "pdf_link": "https://arxiv.org/pdf/2306.02388v1.pdf"
    },
    {
        "title": "Exposing Bias in Online Communities through Large-Scale Language Models",
        "authors": [
            "Celine Wald",
            "Lukas Pfahler"
        ],
        "published": "2023-06-04T08:09:26Z",
        "summary": "Progress in natural language generation research has been shaped by the\never-growing size of language models. While large language models pre-trained\non web data can generate human-sounding text, they also reproduce social biases\nand contribute to the propagation of harmful stereotypes. This work utilises\nthe flaw of bias in language models to explore the biases of six different\nonline communities. In order to get an insight into the communities'\nviewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias\nof the resulting models is evaluated by prompting the models with different\ndemographics and comparing the sentiment and toxicity values of these\ngenerations. Together, these methods reveal that bias differs in type and\nintensity for the various models. This work not only affirms how easily bias is\nabsorbed from training data but also presents a scalable method to identify and\ncompare the bias of different datasets or communities. Additionally, the\nexamples generated for this work demonstrate the limitations of using automated\nsentiment and toxicity classifiers in bias research.",
        "pdf_link": "https://arxiv.org/pdf/2306.02294v1.pdf"
    },
    {
        "title": "OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models",
        "authors": [
            "Changhun Lee",
            "Jungyu Jin",
            "Taesu Kim",
            "Hyungjun Kim",
            "Eunhyeok Park"
        ],
        "published": "2023-06-04T06:33:13Z",
        "summary": "Large language models (LLMs) with hundreds of billions of parameters require\npowerful server-grade GPUs for inference, limiting their practical deployment.\nTo address this challenge, we introduce the outlier-aware weight quantization\n(OWQ) method, which aims to minimize LLM's footprint through low-precision\nrepresentation. OWQ prioritizes a small subset of structured weights sensitive\nto quantization, storing them in high-precision, while applying highly tuned\nquantization to the remaining dense weights. This sensitivity-aware\nmixed-precision scheme reduces the quantization error notably, and extensive\nexperiments demonstrate that 3.1-bit models using OWQ perform comparably to\n4-bit models optimized by OPTQ. Furthermore, OWQ incorporates a\nparameter-efficient fine-tuning for task-specific adaptation, called weak\ncolumn tuning (WCT), enabling accurate task-specific LLM adaptation with\nminimal memory overhead in the optimized format. OWQ represents a notable\nadvancement in the flexibility, efficiency, and practicality of LLM\noptimization literature. The source code is available at\nhttps://github.com/xvyaward/owq",
        "pdf_link": "https://arxiv.org/pdf/2306.02272v4.pdf"
    },
    {
        "title": "Probing Physical Reasoning with Counter-Commonsense Context",
        "authors": [
            "Kazushi Kondo",
            "Saku Sugawara",
            "Akiko Aizawa"
        ],
        "published": "2023-06-04T04:24:43Z",
        "summary": "In this study, we create a CConS (Counter-commonsense Contextual Size\ncomparison) dataset to investigate how physical commonsense affects the\ncontextualized size comparison task; the proposed dataset consists of both\ncontexts that fit physical commonsense and those that do not. This dataset\ntests the ability of language models to predict the size relationship between\nobjects under various contexts generated from our curated noun list and\ntemplates. We measure the ability of several masked language models and\ngenerative models. The results show that while large language models can use\nprepositions such as ``in'' and ``into'' in the provided context to infer size\nrelationships, they fail to use verbs and thus make incorrect judgments led by\ntheir prior physical commonsense.",
        "pdf_link": "https://arxiv.org/pdf/2306.02258v1.pdf"
    },
    {
        "title": "Large Language Model Augmented Narrative Driven Recommendations",
        "authors": [
            "Sheshera Mysore",
            "Andrew McCallum",
            "Hamed Zamani"
        ],
        "published": "2023-06-04T03:46:45Z",
        "summary": "Narrative-driven recommendation (NDR) presents an information access problem\nwhere users solicit recommendations with verbose descriptions of their\npreferences and context, for example, travelers soliciting recommendations for\npoints of interest while describing their likes/dislikes and travel\ncircumstances. These requests are increasingly important with the rise of\nnatural language-based conversational interfaces for search and recommendation\nsystems. However, NDR lacks abundant training data for models, and current\nplatforms commonly do not support these requests. Fortunately, classical\nuser-item interaction datasets contain rich textual data, e.g., reviews, which\noften describe user preferences and context - this may be used to bootstrap\ntraining for NDR models. In this work, we explore using large language models\n(LLMs) for data augmentation to train NDR models. We use LLMs for authoring\nsynthetic narrative queries from user-item interactions with few-shot prompting\nand train retrieval models for NDR on synthetic queries and user-item\ninteraction data. Our experiments demonstrate that this is an effective\nstrategy for training small-parameter retrieval models that outperform other\nretrieval and LLM baselines for narrative-driven recommendation.",
        "pdf_link": "https://arxiv.org/pdf/2306.02250v2.pdf"
    },
    {
        "title": "Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model",
        "authors": [
            "Lingfeng Shen",
            "Haiyun Jiang",
            "Lemao Liu",
            "Shuming Shi"
        ],
        "published": "2023-06-04T03:26:43Z",
        "summary": "Sentence embedding is one of the most fundamental tasks in Natural Language\nProcessing and plays an important role in various tasks. The recent\nbreakthrough in sentence embedding is achieved by pre-trained language models\n(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point\nestimate does not naturally express uncertainty in a taskagnostic way. This\npaper thereby proposes an efficient framework on probabilistic sentence\nembedding (Sen2Pro) from PLMs, and it represents a sentence as a probability\ndensity distribution in an embedding space to reflect both model uncertainty\nand data uncertainty (i.e., many-to-one nature) in the sentence representation.\nThe proposed framework performs in a plug-and-play way without retraining PLMs\nanymore, and it is easy to implement and generally applied on top of any PLM.\nThe superiority of Sen2Pro over Sen2Vec has been theoretically verified and\npractically illustrated on different NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.02247v1.pdf"
    },
    {
        "title": "Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions",
        "authors": [
            "Hui Yang",
            "Sifu Yue",
            "Yunzhong He"
        ],
        "published": "2023-06-04T01:07:20Z",
        "summary": "Auto-GPT is an autonomous agent that leverages recent advancements in\nadapting Large Language Models (LLMs) for decision-making tasks. While there\nhas been a growing interest in Auto-GPT stypled agents, questions remain\nregarding the effectiveness and flexibility of Auto-GPT in solving real-world\ndecision-making tasks. Its limited capability for real-world engagement and the\nabsence of benchmarks contribute to these uncertainties. In this paper, we\npresent a comprehensive benchmark study of Auto-GPT styled agents in\ndecision-making tasks that simulate real-world scenarios. Our aim is to gain\ndeeper insights into this problem and understand the adaptability of GPT-based\nagents. We compare the performance of popular LLMs such as GPT-4, GPT-3.5,\nClaude, and Vicuna in Auto-GPT styled decision-making tasks. Furthermore, we\nintroduce the Additional Opinions algorithm, an easy and effective method that\nincorporates supervised/imitation-based learners into the Auto-GPT scheme. This\napproach enables lightweight supervised learning without requiring fine-tuning\nof the foundational LLMs. We demonstrate through careful baseline comparisons\nand ablation studies that the Additional Opinions algorithm significantly\nenhances performance in online decision-making benchmarks, including WebShop\nand ALFWorld.",
        "pdf_link": "https://arxiv.org/pdf/2306.02224v1.pdf"
    },
    {
        "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts",
        "authors": [
            "Haibin Wu",
            "Kai-Wei Chang",
            "Yuan-Kuei Wu",
            "Hung-yi Lee"
        ],
        "published": "2023-06-03T22:35:27Z",
        "summary": "Large language models (LLMs) have gained considerable attention for\nArtificial Intelligence Generated Content (AIGC), particularly with the\nemergence of ChatGPT. However, the direct adaptation of continuous speech to\nLLMs that process discrete tokens remains an unsolved challenge, hindering the\napplication of LLMs for speech generation. The advanced speech LMs are in the\ncorner, as that speech signals encapsulate a wealth of information, including\nspeaker and emotion, beyond textual data alone. Prompt tuning has demonstrated\nnotable gains in parameter efficiency and competitive performance on some\nspeech classification tasks. However, the extent to which prompts can\neffectively elicit generation tasks from speech LMs remains an open question.\nIn this paper, we present pioneering research that explores the application of\nprompt tuning to stimulate speech LMs for various generation tasks, within a\nunified framework called SpeechGen, with around 10M trainable parameters. The\nproposed unified framework holds great promise for efficiency and\neffectiveness, particularly with the imminent arrival of advanced speech LMs,\nwhich will significantly enhance the capabilities of the framework. The code\nand demos of SpeechGen will be available on the project website:\n\\url{https://ga642381.github.io/SpeechPrompt/speechgen}",
        "pdf_link": "https://arxiv.org/pdf/2306.02207v3.pdf"
    },
    {
        "title": "Towards Coding Social Science Datasets with Language Models",
        "authors": [
            "Christopher Michael Rytting",
            "Taylor Sorensen",
            "Lisa Argyle",
            "Ethan Busby",
            "Nancy Fulda",
            "Joshua Gubler",
            "David Wingate"
        ],
        "published": "2023-06-03T19:11:34Z",
        "summary": "Researchers often rely on humans to code (label, annotate, etc.) large sets\nof texts. This kind of human coding forms an important part of social science\nresearch, yet the coding process is both resource intensive and highly variable\nfrom application to application. In some cases, efforts to automate this\nprocess have achieved human-level accuracies, but to achieve this, these\nattempts frequently rely on thousands of hand-labeled training examples, which\nmakes them inapplicable to small-scale research studies and costly for large\nones. Recent advances in a specific kind of artificial intelligence tool -\nlanguage models (LMs) - provide a solution to this problem. Work in computer\nscience makes it clear that LMs are able to classify text, without the cost (in\nfinancial terms and human effort) of alternative methods. To demonstrate the\npossibilities of LMs in this area of political science, we use GPT-3, one of\nthe most advanced LMs, as a synthetic coder and compare it to human coders. We\nfind that GPT-3 can match the performance of typical human coders and offers\nbenefits over other machine learning methods of coding text. We find this\nacross a variety of domains using very different coding procedures. This\nprovides exciting evidence that language models can serve as a critical advance\nin the coding of open-ended texts in a variety of applications.",
        "pdf_link": "https://arxiv.org/pdf/2306.02177v1.pdf"
    },
    {
        "title": "Unsupervised Human Activity Recognition through Two-stage Prompting with ChatGPT",
        "authors": [
            "Qingxin Xia",
            "Takuya Maekawa",
            "Takahiro Hara"
        ],
        "published": "2023-06-03T15:41:59Z",
        "summary": "Wearable sensor devices, which offer the advantage of recording daily objects\nused by a person while performing an activity, enable the feasibility of\nunsupervised Human Activity Recognition (HAR). Unfortunately, previous\nunsupervised approaches using the usage sequence of objects usually require a\nproper description of activities manually prepared by humans. Instead, we\nleverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT.\nBecause the sequence of objects robustly characterizes the activity identity,\nit is possible that ChatGPT already learned the association between activities\nand objects from existing contexts. However, previous prompt engineering for\nChatGPT exhibits limited generalization ability when dealing with a list of\nwords (i.e., sequence of objects) due to the similar weighting assigned to each\nword in the list. In this study, we propose a two-stage prompt engineering,\nwhich first guides ChatGPT to generate activity descriptions associated with\nobjects while emphasizing important objects for distinguishing similar\nactivities; then outputs activity classes and explanations for enhancing the\ncontexts that are helpful for HAR. To the best of our knowledge, this is the\nfirst study that utilizes ChatGPT to recognize activities using objects in an\nunsupervised manner. We conducted our approach on three datasets and\ndemonstrated the state-of-the-art performance.",
        "pdf_link": "https://arxiv.org/pdf/2306.02140v1.pdf"
    },
    {
        "title": "Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions",
        "authors": [
            "Jana Straková",
            "Eva Fučíková",
            "Jan Hajič",
            "Zdeňka Urešová"
        ],
        "published": "2023-06-03T14:57:47Z",
        "summary": "In this project, we have investigated the use of advanced machine learning\nmethods, specifically fine-tuned large language models, for pre-annotating data\nfor a lexical extension task, namely adding descriptive words (verbs) to an\nexisting (but incomplete, as of yet) ontology of event types. Several research\nquestions have been focused on, from the investigation of a possible heuristics\nto provide at least hints to annotators which verbs to include and which are\noutside the current version of the ontology, to the possible use of the\nautomatic scores to help the annotators to be more efficient in finding a\nthreshold for identifying verbs that cannot be assigned to any existing class\nand therefore they are to be used as seeds for a new class. We have also\ncarefully examined the correlation of the automatic scores with the human\nannotation. While the correlation turned out to be strong, its influence on the\nannotation proper is modest due to its near linearity, even though the mere\nfact of such pre-annotation leads to relatively short annotation times.",
        "pdf_link": "https://arxiv.org/pdf/2306.02130v2.pdf"
    },
    {
        "title": "MultiLegalPile: A 689GB Multilingual Legal Corpus",
        "authors": [
            "Joel Niklaus",
            "Veton Matoshi",
            "Matthias Stürmer",
            "Ilias Chalkidis",
            "Daniel E. Ho"
        ],
        "published": "2023-06-03T10:10:38Z",
        "summary": "Large, high-quality datasets are crucial for training Large Language Models\n(LLMs). However, so far, there are few datasets available for specialized\ncritical domains such as law and the available ones are often only for the\nEnglish language. We curate and release MultiLegalPile, a 689GB corpus in 24\nlanguages from 17 jurisdictions. The MultiLegalPile corpus, which includes\ndiverse legal data sources with varying licenses, allows for pretraining NLP\nmodels under fair use, with more permissive licenses for the Eurlex Resources\nand Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer\nmultilingually, and 24 monolingual models on each of the language-specific\nsubsets and evaluate them on LEXTREME. Additionally, we evaluate the English\nand multilingual models on LexGLUE. Our multilingual models set a new SotA on\nLEXTREME and our English models on LexGLUE. We release the dataset, the trained\nmodels, and all of the code under the most open possible licenses.",
        "pdf_link": "https://arxiv.org/pdf/2306.02069v2.pdf"
    },
    {
        "title": "LambdaBeam: Neural Program Search with Higher-Order Functions and Lambdas",
        "authors": [
            "Kensen Shi",
            "Hanjun Dai",
            "Wen-Ding Li",
            "Kevin Ellis",
            "Charles Sutton"
        ],
        "published": "2023-06-03T08:24:53Z",
        "summary": "Search is an important technique in program synthesis that allows for\nadaptive strategies such as focusing on particular search directions based on\nexecution results. Several prior works have demonstrated that neural models are\neffective at guiding program synthesis searches. However, a common drawback of\nthose approaches is the inability to handle iterative loops, higher-order\nfunctions, or lambda functions, thus limiting prior neural searches from\nsynthesizing longer and more general programs. We address this gap by designing\na search algorithm called LambdaBeam that can construct arbitrary lambda\nfunctions that compose operations within a given DSL. We create semantic vector\nrepresentations of the execution behavior of the lambda functions and train a\nneural policy network to choose which lambdas to construct during search, and\npass them as arguments to higher-order functions to perform looping\ncomputations. Our experiments show that LambdaBeam outperforms neural,\nsymbolic, and LLM-based techniques in an integer list manipulation domain.",
        "pdf_link": "https://arxiv.org/pdf/2306.02049v2.pdf"
    },
    {
        "title": "On Optimal Caching and Model Multiplexing for Large Model Inference",
        "authors": [
            "Banghua Zhu",
            "Ying Sheng",
            "Lianmin Zheng",
            "Clark Barrett",
            "Michael I. Jordan",
            "Jiantao Jiao"
        ],
        "published": "2023-06-03T05:01:51Z",
        "summary": "Large Language Models (LLMs) and other large foundation models have achieved\nnoteworthy success, but their size exacerbates existing resource consumption\nand latency challenges. In particular, the large-scale deployment of these\nmodels is hindered by the significant resource requirements during inference.\nIn this paper, we study two approaches for mitigating these challenges:\nemploying a cache to store previous queries and learning a model multiplexer to\nchoose from an ensemble of models for query processing.\n  Theoretically, we provide an optimal algorithm for jointly optimizing both\napproaches to reduce the inference cost in both offline and online tabular\nsettings. By combining a caching algorithm, namely Greedy Dual Size with\nFrequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we\nachieve optimal rates in both offline and online settings. Empirically,\nsimulations show that the combination of our caching and model multiplexing\nalgorithms greatly improves over the baselines, with up to $50\\times$\nimprovement over the baseline when the ratio between the maximum cost and\nminimum cost is $100$. Experiments on real datasets show a $4.3\\times$\nimprovement in FLOPs over the baseline when the ratio for FLOPs is $10$, and a\n$1.8\\times$ improvement in latency when the ratio for average latency is\n$1.85$.",
        "pdf_link": "https://arxiv.org/pdf/2306.02003v2.pdf"
    },
    {
        "title": "Can Contextual Biasing Remain Effective with Whisper and GPT-2?",
        "authors": [
            "Guangzhi Sun",
            "Xianrui Zheng",
            "Chao Zhang",
            "Philip C. Woodland"
        ],
        "published": "2023-06-02T22:56:01Z",
        "summary": "End-to-end automatic speech recognition (ASR) and large language models, such\nas Whisper and GPT-2, have recently been scaled to use vast amounts of training\ndata. Despite the large amount of training data, infrequent content words that\noccur in a particular task may still exhibit poor ASR performance, with\ncontextual biasing a possible remedy. This paper investigates the effectiveness\nof neural contextual biasing for Whisper combined with GPT-2. Specifically,\nthis paper proposes integrating an adapted tree-constrained pointer generator\n(TCPGen) component for Whisper and a dedicated training scheme to dynamically\nadjust the final output without modifying any Whisper model parameters.\nExperiments across three datasets show a considerable reduction in errors on\nbiasing words with a biasing list of 1000 words. Contextual biasing was more\neffective when applied to domain-specific data and can boost the performance of\nWhisper and GPT-2 without losing their generality.",
        "pdf_link": "https://arxiv.org/pdf/2306.01942v1.pdf"
    },
    {
        "title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap",
        "authors": [
            "Q. Vera Liao",
            "Jennifer Wortman Vaughan"
        ],
        "published": "2023-06-02T22:51:26Z",
        "summary": "The rise of powerful large language models (LLMs) brings about tremendous\nopportunities for innovation but also looming risks for individuals and society\nat large. We have reached a pivotal moment for ensuring that LLMs and\nLLM-infused applications are developed and deployed responsibly. However, a\ncentral pillar of responsible AI -- transparency -- is largely missing from the\ncurrent discourse around LLMs. It is paramount to pursue new approaches to\nprovide transparency for LLMs, and years of research at the intersection of AI\nand human-computer interaction (HCI) highlight that we must do so with a\nhuman-centered perspective: Transparency is fundamentally about supporting\nappropriate human understanding, and this understanding is sought by different\nstakeholders with different goals in different contexts. In this new era of\nLLMs, we must develop and design approaches to transparency by considering the\nneeds of stakeholders in the emerging LLM ecosystem, the novel types of\nLLM-infused applications being built, and the new usage patterns and challenges\naround LLMs, all while building on lessons learned about how people process,\ninteract with, and make use of information. We reflect on the unique challenges\nthat arise in providing transparency for LLMs, along with lessons learned from\nHCI and responsible AI research that has taken a human-centered perspective on\nAI transparency. We then lay out four common approaches that the community has\ntaken to achieve transparency -- model reporting, publishing evaluation\nresults, providing explanations, and communicating uncertainty -- and call out\nopen questions around how these approaches may or may not be applied to LLMs.\nWe hope this provides a starting point for discussion and a useful roadmap for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2306.01941v2.pdf"
    },
    {
        "title": "Revisiting the Role of Language Priors in Vision-Language Models",
        "authors": [
            "Zhiqiu Lin",
            "Xinyue Chen",
            "Deepak Pathak",
            "Pengchuan Zhang",
            "Deva Ramanan"
        ],
        "published": "2023-06-02T19:19:43Z",
        "summary": "Vision-language models (VLMs) are impactful in part because they can be\napplied to a variety of visual understanding tasks in a zero-shot fashion,\nwithout any fine-tuning. We study $\\textit{generative VLMs}$ that are trained\nfor next-word generation given an image. We explore their zero-shot performance\non the illustrative task of image-text retrieval across 8 popular\nvision-language benchmarks. Our first observation is that they can be\nrepurposed for discriminative tasks (such as image-text retrieval) by simply\ncomputing the match score of generating a particular text string given an\nimage. We call this probabilistic score the $\\textit{Visual Generative\nPre-Training Score}$ (VisualGPTScore). While the VisualGPTScore produces\nnear-perfect accuracy on some retrieval benchmarks, it yields poor accuracy on\nothers. We analyze this behavior through a probabilistic lens, pointing out\nthat some benchmarks inadvertently capture unnatural language distributions by\ncreating adversarial but unlikely text captions. In fact, we demonstrate that\neven a \"blind\" language model that ignores any image evidence can sometimes\noutperform all prior art, reminiscent of similar challenges faced by the\nvisual-question answering (VQA) community many years ago. We derive a\nprobabilistic post-processing scheme that controls for the amount of linguistic\nbias in generative VLMs at test time without having to retrain or fine-tune the\nmodel. We show that the VisualGPTScore, when appropriately debiased, is a\nstrong zero-shot baseline for vision-language understanding, oftentimes\nproducing state-of-the-art accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2306.01879v3.pdf"
    },
    {
        "title": "Knowledge of cultural moral norms in large language models",
        "authors": [
            "Aida Ramezani",
            "Yang Xu"
        ],
        "published": "2023-06-02T18:23:35Z",
        "summary": "Moral norms vary across cultures. A recent line of work suggests that English\nlarge language models contain human-like moral biases, but these studies\ntypically do not examine moral variation in a diverse cultural setting. We\ninvestigate the extent to which monolingual English language models contain\nknowledge about moral norms in different countries. We consider two levels of\nanalysis: 1) whether language models capture fine-grained moral variation\nacross countries over a variety of topics such as ``homosexuality'' and\n``divorce''; 2) whether language models capture cultural diversity and shared\ntendencies in which topics people around the globe tend to diverge or agree on\nin their moral judgment. We perform our analyses with two public datasets from\nthe World Values Survey (across 55 countries) and PEW global surveys (across 40\ncountries) on morality. We find that pre-trained English language models\npredict empirical moral norms across countries worse than the English moral\nnorms reported previously. However, fine-tuning language models on the survey\ndata improves inference across countries at the expense of a less accurate\nestimate of the English moral norms. We discuss the relevance and challenges of\nincorporating cultural knowledge into the automated inference of moral norms.",
        "pdf_link": "https://arxiv.org/pdf/2306.01857v1.pdf"
    },
    {
        "title": "Evaluating Language Models for Mathematics through Interactions",
        "authors": [
            "Katherine M. Collins",
            "Albert Q. Jiang",
            "Simon Frieder",
            "Lionel Wong",
            "Miri Zilka",
            "Umang Bhatt",
            "Thomas Lukasiewicz",
            "Yuhuai Wu",
            "Joshua B. Tenenbaum",
            "William Hart",
            "Timothy Gowers",
            "Wenda Li",
            "Adrian Weller",
            "Mateja Jamnik"
        ],
        "published": "2023-06-02T17:12:25Z",
        "summary": "There is much excitement about the opportunity to harness the power of large\nlanguage models (LLMs) when building problem-solving assistants. However, the\nstandard methodology of evaluating LLMs relies on static pairs of inputs and\noutputs, and is insufficient for making an informed decision about which LLMs\nand under which assistive settings can they be sensibly used. Static assessment\nfails to account for the essential interactive element in LLM deployment, and\ntherefore limits how we understand language model capabilities. We introduce\nCheckMate, an adaptable prototype platform for humans to interact with and\nevaluate LLMs. We conduct a study with CheckMate to evaluate three language\nmodels (InstructGPT, ChatGPT, and GPT-4) as assistants in proving\nundergraduate-level mathematics, with a mixed cohort of participants from\nundergraduate students to professors of mathematics. We release the resulting\ninteraction and rating dataset, MathConverse. By analysing MathConverse, we\nderive a taxonomy of human behaviours and uncover that despite a generally\npositive correlation, there are notable instances of divergence between\ncorrectness and perceived helpfulness in LLM generations, amongst other\nfindings. Further, we garner a more granular understanding of GPT-4\nmathematical problem-solving through a series of case studies, contributed by\nexpert mathematicians. We conclude with actionable takeaways for ML\npractitioners and mathematicians: models that communicate uncertainty respond\nwell to user corrections, and are more interpretable and concise may constitute\nbetter assistants. Interactive evaluation is a promising way to navigate the\ncapability of these models; humans should be aware of language models'\nalgebraic fallibility and discern where they are appropriate to use.",
        "pdf_link": "https://arxiv.org/pdf/2306.01694v2.pdf"
    },
    {
        "title": "Fine-Grained Human Feedback Gives Better Rewards for Language Model Training",
        "authors": [
            "Zeqiu Wu",
            "Yushi Hu",
            "Weijia Shi",
            "Nouha Dziri",
            "Alane Suhr",
            "Prithviraj Ammanabrolu",
            "Noah A. Smith",
            "Mari Ostendorf",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-06-02T17:11:37Z",
        "summary": "Language models (LMs) often exhibit undesirable text generation behaviors,\nincluding generating false, toxic, or irrelevant outputs. Reinforcement\nlearning from human feedback (RLHF) - where human preference judgments on LM\noutputs are transformed into a learning signal - has recently shown promise in\naddressing these issues. However, such holistic feedback conveys limited\ninformation on long text outputs; it does not indicate which aspects of the\noutputs influenced user preference; e.g., which parts contain what type(s) of\nerrors. In this paper, we use fine-grained human feedback (e.g., which sentence\nis false, which sub-sentence is irrelevant) as an explicit training signal. We\nintroduce Fine-Grained RLHF, a framework that enables training and learning\nfrom reward functions that are fine-grained in two respects: (1) density,\nproviding a reward after every segment (e.g., a sentence) is generated; and (2)\nincorporating multiple reward models associated with different feedback types\n(e.g., factual incorrectness, irrelevance, and information incompleteness). We\nconduct experiments on detoxification and long-form question answering to\nillustrate how learning with such reward functions leads to improved\nperformance, supported by both automatic and human evaluation. Additionally, we\nshow that LM behaviors can be customized using different combinations of\nfine-grained reward models. We release all data, collected human feedback, and\ncodes at https://FineGrainedRLHF.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2306.01693v2.pdf"
    },
    {
        "title": "MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1 Updates",
        "authors": [
            "Mohammad Mozaffari",
            "Sikan Li",
            "Zhao Zhang",
            "Maryam Mehri Dehnavi"
        ],
        "published": "2023-06-02T17:00:19Z",
        "summary": "This work proposes a Momentum-Enabled Kronecker-Factor-Based Optimizer Using\nRank-1 updates, called MKOR, that improves the training time and convergence\nproperties of deep neural networks (DNNs). Second-order techniques, while\nenjoying higher convergence rates vs first-order counterparts, have cubic\ncomplexity with respect to either the model size and/or the training batch\nsize. Hence they exhibit poor scalability and performance in transformer\nmodels, e.g. large language models (LLMs), because the batch sizes in these\nmodels scale by the attention mechanism sequence length, leading to large model\nsize and batch sizes. MKOR's complexity is quadratic with respect to the model\nsize, alleviating the computation bottlenecks in second-order methods. Because\nof their high computation complexity, state-of-the-art implementations of\nsecond-order methods can only afford to update the second order information\ninfrequently, and thus do not fully exploit the promise of better convergence\nfrom these updates. By reducing the communication complexity of the\nsecond-order updates as well as achieving a linear communication complexity,\nMKOR increases the frequency of second order updates. We also propose a hybrid\nversion of MKOR (called MKOR-H) that mid-training falls backs to a first order\noptimizer if the second order updates no longer accelerate convergence. Our\nexperiments show that MKOR outperforms state -of-the-art first order methods,\ne.g. the LAMB optimizer, and best implementations of second-order methods, i.e.\nKAISA/KFAC, up to 2.57x and 1.85x respectively on BERT-Large-Uncased on 64\nGPUs.",
        "pdf_link": "https://arxiv.org/pdf/2306.01685v2.pdf"
    },
    {
        "title": "Harnessing large-language models to generate private synthetic text",
        "authors": [
            "Alexey Kurakin",
            "Natalia Ponomareva",
            "Umar Syed",
            "Liam MacDermed",
            "Andreas Terzis"
        ],
        "published": "2023-06-02T16:59:36Z",
        "summary": "Differentially private training algorithms like DP-SGD protect sensitive\ntraining data by ensuring that trained models do not reveal private\ninformation. An alternative approach, which this paper studies, is to use a\nsensitive dataset to generate synthetic data that is differentially private\nwith respect to the original data, and then non-privately training a model on\nthe synthetic data. Doing so has several advantages: synthetic data can be\nreused for other tasks (including for hyper parameter tuning), retained\nindefinitely, and shared with third parties without sacrificing privacy.\nHowever, generating private synthetic data is much harder than training a\nprivate model. To improve performance on text data, recent work has utilized\npublic data by starting with a pre-trained generative language model and\nprivately fine-tuning it on sensitive data. This model can be used to sample a\nDP synthetic dataset. While this strategy seems straightforward, executing it\nhas proven problematic. Previous approaches either show significant performance\nloss, or have, as we show, critical design flaws. In this paper we demonstrate\nthat a proper training objective along with tuning fewer parameters results in\nexcellent DP synthetic data quality. Our approach is competitive with direct\nDP-training of downstream classifiers in terms of performance on downstream\ntasks. Further, we demonstrate that our DP synthetic data is not only useful\nfor downstream classifier training, but also to tune those same models.",
        "pdf_link": "https://arxiv.org/pdf/2306.01684v2.pdf"
    },
    {
        "title": "Log Parsing: How Far Can ChatGPT Go?",
        "authors": [
            "Van-Hoang Le",
            "Hongyu Zhang"
        ],
        "published": "2023-06-02T14:58:43Z",
        "summary": "Software logs play an essential role in ensuring the reliability and\nmaintainability of large-scale software systems, as they are often the sole\nsource of runtime information. Log parsing, which converts raw log messages\ninto structured data, is an important initial step towards downstream log\nanalytics. In recent studies, ChatGPT, the current cutting-edge large language\nmodel (LLM), has been widely applied to a wide range of software engineering\ntasks. However, its performance in automated log parsing remains unclear. In\nthis paper, we evaluate ChatGPT's ability to undertake log parsing by\naddressing two research questions. (1) Can ChatGPT effectively parse logs? (2)\nHow does ChatGPT perform with different prompting methods? Our results show\nthat ChatGPT can achieve promising results for log parsing with appropriate\nprompts, especially with few-shot prompting. Based on our findings, we outline\nseveral challenges and opportunities for ChatGPT-based log parsing.",
        "pdf_link": "https://arxiv.org/pdf/2306.01590v2.pdf"
    },
    {
        "title": "EmoUS: Simulating User Emotions in Task-Oriented Dialogues",
        "authors": [
            "Hsien-Chin Lin",
            "Shutong Feng",
            "Christian Geishauser",
            "Nurul Lubis",
            "Carel van Niekerk",
            "Michael Heck",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Milica Gašić"
        ],
        "published": "2023-06-02T14:48:19Z",
        "summary": "Existing user simulators (USs) for task-oriented dialogue systems only model\nuser behaviour on semantic and natural language levels without considering the\nuser persona and emotions. Optimising dialogue systems with generic user\npolicies, which cannot model diverse user behaviour driven by different\nemotional states, may result in a high drop-off rate when deployed in the real\nworld. Thus, we present EmoUS, a user simulator that learns to simulate user\nemotions alongside user behaviour. EmoUS generates user emotions, semantic\nactions, and natural language responses based on the user goal, the dialogue\nhistory, and the user persona. By analysing what kind of system behaviour\nelicits what kind of user emotions, we show that EmoUS can be used as a probe\nto evaluate a variety of dialogue systems and in particular their effect on the\nuser's emotional state. Developing such methods is important in the age of\nlarge language model chat-bots and rising ethical concerns.",
        "pdf_link": "https://arxiv.org/pdf/2306.01579v1.pdf"
    },
    {
        "title": "PassGPT: Password Modeling and (Guided) Generation with Large Language Models",
        "authors": [
            "Javier Rando",
            "Fernando Perez-Cruz",
            "Briland Hitaj"
        ],
        "published": "2023-06-02T13:49:53Z",
        "summary": "Large language models (LLMs) successfully model natural language from vast\namounts of text without the need for explicit supervision. In this paper, we\ninvestigate the efficacy of LLMs in modeling passwords. We present PassGPT, a\nLLM trained on password leaks for password generation. PassGPT outperforms\nexisting methods based on generative adversarial networks (GAN) by guessing\ntwice as many previously unseen passwords. Furthermore, we introduce the\nconcept of guided password generation, where we leverage PassGPT sampling\nprocedure to generate passwords matching arbitrary constraints, a feat lacking\nin current GAN-based strategies. Lastly, we conduct an in-depth analysis of the\nentropy and probability distribution that PassGPT defines over passwords and\ndiscuss their use in enhancing existing password strength estimators.",
        "pdf_link": "https://arxiv.org/pdf/2306.01545v2.pdf"
    },
    {
        "title": "Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",
        "authors": [
            "Zhuo Wang",
            "Rongzhen Li",
            "Bowen Dong",
            "Jie Wang",
            "Xiuxing Li",
            "Ning Liu",
            "Chenhui Mao",
            "Wei Zhang",
            "Liling Dong",
            "Jing Gao",
            "Jianyong Wang"
        ],
        "published": "2023-06-02T12:47:45Z",
        "summary": "Recent investigations show that large language models (LLMs), specifically\nGPT-4, not only have remarkable capabilities in common Natural Language\nProcessing (NLP) tasks but also exhibit human-level performance on various\nprofessional and academic benchmarks. However, whether GPT-4 can be directly\nused in practical applications and replace traditional artificial intelligence\n(AI) tools in specialized domains requires further experimental validation. In\nthis paper, we explore the potential of LLMs such as GPT-4 to outperform\ntraditional AI tools in dementia diagnosis. Comprehensive comparisons between\nGPT-4 and traditional AI tools are conducted to examine their diagnostic\naccuracy in a clinical setting. Experimental results on two real clinical\ndatasets show that, although LLMs like GPT-4 demonstrate potential for future\nadvancements in dementia diagnosis, they currently do not surpass the\nperformance of traditional AI tools. The interpretability and faithfulness of\nGPT-4 are also evaluated by comparison with real doctors. We discuss the\nlimitations of GPT-4 in its current state and propose future research\ndirections to enhance GPT-4 in dementia diagnosis.",
        "pdf_link": "https://arxiv.org/pdf/2306.01499v1.pdf"
    },
    {
        "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations",
        "authors": [
            "Pan Li",
            "Yuyan Wang",
            "Ed H. Chi",
            "Minmin Chen"
        ],
        "published": "2023-06-02T12:00:03Z",
        "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth\naspect information, or using data mining or machine learning approaches to\nextract aspects from implicit user feedback such as user reviews. It however\nremains under-explored how the extracted aspects can help generate more\nmeaningful recommendations to the users. Meanwhile, existing research on\naspect-based recommendations often relies on separate aspect extraction models\nor assumes the aspects are given, without accounting for the fact the optimal\nset of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with\naspect-based recommendations in an end-to-end manner, achieving the two goals\ntogether in a single framework. For the aspect extraction component, we\nleverage the recent advances in large language models and design a new prompt\nlearning mechanism to generate aspects for the end recommendation task. For the\naspect-based recommendation component, the extracted aspects are concatenated\nwith the usual user and item features used by the recommendation model. The\nrecommendation task mediates the learning of the user embeddings and item\nembeddings, which are used as soft prompts to generate aspects. Therefore, the\nextracted aspects are personalized and contextualized by the recommendation\ntask. We showcase the effectiveness of our proposed method through extensive\nexperiments on three industrial datasets, where our proposed framework\nsignificantly outperforms state-of-the-art baselines in both the personalized\naspect extraction and aspect-based recommendation tasks. In particular, we\ndemonstrate that it is necessary and beneficial to combine the learning of\naspect extraction and aspect-based recommendation together. We also conduct\nextensive ablation studies to understand the contribution of each design\ncomponent in our framework.",
        "pdf_link": "https://arxiv.org/pdf/2306.01475v1.pdf"
    },
    {
        "title": "ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?",
        "authors": [
            "Michael Heck",
            "Nurul Lubis",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Shutong Feng",
            "Christian Geishauser",
            "Hsien-Chin Lin",
            "Carel van Niekerk",
            "Milica Gašić"
        ],
        "published": "2023-06-02T09:15:01Z",
        "summary": "Recent research on dialogue state tracking (DST) focuses on methods that\nallow few- and zero-shot transfer to new domains or schemas. However,\nperformance gains heavily depend on aggressive data augmentation and\nfine-tuning of ever larger language model based architectures. In contrast,\ngeneral purpose language models, trained on large amounts of diverse data, hold\nthe promise of solving any kind of task without task-specific training. We\npresent preliminary experimental results on the ChatGPT research preview,\nshowing that ChatGPT achieves state-of-the-art performance in zero-shot DST.\nDespite our findings, we argue that properties inherent to general purpose\nmodels limit their ability to replace specialized systems. We further theorize\nthat the in-context learning capabilities of such models will likely become\npowerful tools to support the development of dedicated and dynamic dialogue\nstate trackers.",
        "pdf_link": "https://arxiv.org/pdf/2306.01386v1.pdf"
    },
    {
        "title": "An Empirical Study on Challenging Math Problem Solving with GPT-4",
        "authors": [
            "Yiran Wu",
            "Feiran Jia",
            "Shaokun Zhang",
            "Hangyu Li",
            "Erkang Zhu",
            "Yue Wang",
            "Yin Tat Lee",
            "Richard Peng",
            "Qingyun Wu",
            "Chi Wang"
        ],
        "published": "2023-06-02T08:02:15Z",
        "summary": "Employing Large Language Models (LLMs) to address mathematical problems is an\nintriguing research endeavor, considering the abundance of math problems\nexpressed in natural language across numerous science and engineering fields.\nWhile several prior works have investigated solving elementary mathematics\nusing LLMs, this work explores the frontier of using GPT-4 for solving more\ncomplex and challenging math problems. We evaluate various ways of using GPT-4.\nSome of them are adapted from existing work, and one is MathChat, a\nconversational problem-solving framework newly proposed in this work. We\nperform the evaluation on difficult high school competition problems from the\nMATH dataset, which shows the advantage of the proposed conversational\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2306.01337v2.pdf"
    },
    {
        "title": "MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models",
        "authors": [
            "Masoud Monajatipoor",
            "Liunian Harold Li",
            "Mozhdeh Rouhsedaghat",
            "Lin F. Yang",
            "Kai-Wei Chang"
        ],
        "published": "2023-06-02T07:21:03Z",
        "summary": "Large-scale language models have shown the ability to adapt to a new task via\nconditioning on a few demonstrations (i.e., in-context learning). However, in\nthe vision-language domain, most large-scale pre-trained vision-language (VL)\nmodels do not possess the ability to conduct in-context learning. How can we\nenable in-context learning for VL models? In this paper, we study an\ninteresting hypothesis: can we transfer the in-context learning ability from\nthe language domain to VL domain? Specifically, we first meta-trains a language\nmodel to perform in-context learning on NLP tasks (as in MetaICL); then we\ntransfer this model to perform VL tasks by attaching a visual encoder. Our\nexperiments suggest that indeed in-context learning ability can be transferred\ncross modalities: our model considerably improves the in-context learning\ncapability on VL tasks and can even compensate for the size of the model\nsignificantly. On VQA, OK-VQA, and GQA, our method could outperform the\nbaseline model while having 20 times fewer parameters.",
        "pdf_link": "https://arxiv.org/pdf/2306.01311v1.pdf"
    },
    {
        "title": "Egocentric Planning for Scalable Embodied Task Achievement",
        "authors": [
            "Xiaotian Liu",
            "Hector Palacios",
            "Christian Muise"
        ],
        "published": "2023-06-02T06:41:24Z",
        "summary": "Embodied agents face significant challenges when tasked with performing\nactions in diverse environments, particularly in generalizing across object\ntypes and executing suitable actions to accomplish tasks. Furthermore, agents\nshould exhibit robustness, minimizing the execution of illegal actions. In this\nwork, we present Egocentric Planning, an innovative approach that combines\nsymbolic planning and Object-oriented POMDPs to solve tasks in complex\nenvironments, harnessing existing models for visual perception and natural\nlanguage processing. We evaluated our approach in ALFRED, a simulated\nenvironment designed for domestic tasks, and demonstrated its high scalability,\nachieving an impressive 36.07% unseen success rate in the ALFRED benchmark and\nwinning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires\nreliable perception and the specification or learning of a symbolic description\nof the preconditions and effects of the agent's actions, as well as what object\ntypes reveal information about others. It is capable of naturally scaling to\nsolve new tasks beyond ALFRED, as long as they can be solved using the\navailable skills. This work offers a solid baseline for studying end-to-end and\nhybrid methods that aim to generalize to new tasks, including recent approaches\nrelying on LLMs, but often struggle to scale to long sequences of actions or\nproduce robust plans for novel tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01295v1.pdf"
    },
    {
        "title": "ChatGPT is a Remarkable Tool -- For Experts",
        "authors": [
            "Amos Azaria",
            "Rina Azoulay",
            "Shulamit Reches"
        ],
        "published": "2023-06-02T06:28:21Z",
        "summary": "This paper investigates the capabilities of ChatGPT as an automated assistant\nin diverse domains, including scientific writing, mathematics, education,\nprogramming, and healthcare. We explore the potential of ChatGPT to enhance\nproductivity, streamline problem-solving processes, and improve writing style.\nFurthermore, we highlight the potential risks associated with excessive\nreliance on ChatGPT in these fields. These limitations encompass factors like\nincorrect and fictitious responses, inaccuracies in code, limited logical\nreasoning abilities, overconfidence, and critical ethical concerns of\ncopyrights and privacy violation. We outline areas and objectives where ChatGPT\nproves beneficial, applications where it should be used judiciously, and\nscenarios where its reliability may be limited. In light of observed\nlimitations, and given that the tool's fundamental errors may pose a special\nchallenge for non-experts, ChatGPT should be used with a strategic methodology.\nBy drawing from comprehensive experimental studies, we offer methods and flow\ncharts for effectively using ChatGPT. Our recommendations emphasize iterative\ninteraction with ChatGPT and independent verification of its outputs.\nConsidering the importance of utilizing ChatGPT judiciously and with expertise,\nwe recommend its usage for experts who are well-versed in the respective\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2306.03102v1.pdf"
    },
    {
        "title": "KL-Divergence Guided Temperature Sampling",
        "authors": [
            "Chung-Ching Chang",
            "David Reitter",
            "Renat Aksitov",
            "Yun-Hsuan Sung"
        ],
        "published": "2023-06-02T06:11:26Z",
        "summary": "Temperature sampling is a conventional approach to diversify large language\nmodel predictions. As temperature increases, the prediction becomes diverse but\nalso vulnerable to hallucinations -- generating tokens that are sensible but\nnot factual. One common approach to mitigate hallucinations is to provide\nsource/grounding documents and the model is trained to produce predictions that\nbind to and are attributable to the provided source. It appears that there is a\ntrade-off between diversity and attribution. To mitigate any such trade-off, we\npropose to relax the constraint of having a fixed temperature over decoding\nsteps, and a mechanism to guide the dynamic temperature according to its\nrelevance to the source through KL-divergence. Our experiments justifies the\ntrade-off, and shows that our sampling algorithm outperforms the conventional\ntop-k and top-p algorithms in conversational question-answering and\nsummarization tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01286v2.pdf"
    },
    {
        "title": "How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?",
        "authors": [
            "Aniket Deroy",
            "Kripabandhu Ghosh",
            "Saptarshi Ghosh"
        ],
        "published": "2023-06-02T03:16:19Z",
        "summary": "Automatic summarization of legal case judgements has traditionally been\nattempted by using extractive summarization methods. However, in recent years,\nabstractive summarization models are gaining popularity since they can generate\nmore natural and coherent summaries. Legal domain-specific pre-trained\nabstractive summarization models are now available. Moreover, general-domain\npre-trained Large Language Models (LLMs), such as ChatGPT, are known to\ngenerate high-quality text and have the capacity for text summarization. Hence\nit is natural to ask if these models are ready for off-the-shelf application to\nautomatically generate abstractive summaries for case judgements. To explore\nthis question, we apply several state-of-the-art domain-specific abstractive\nsummarization models and general-domain LLMs on Indian court case judgements,\nand check the quality of the generated summaries. In addition to standard\nmetrics for summary quality, we check for inconsistencies and hallucinations in\nthe summaries. We see that abstractive summarization models generally achieve\nslightly higher scores than extractive models in terms of standard summary\nevaluation metrics such as ROUGE and BLEU. However, we often find inconsistent\nor hallucinated information in the generated abstractive summaries. Overall,\nour investigation indicates that the pre-trained abstractive summarization\nmodels and LLMs are not yet ready for fully automatic deployment for case\njudgement summarization; rather a human-in-the-loop approach including manual\nchecks for inconsistencies is more suitable at present.",
        "pdf_link": "https://arxiv.org/pdf/2306.01248v2.pdf"
    },
    {
        "title": "Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators",
        "authors": [
            "Zhizheng Zhang",
            "Xiaoyi Zhang",
            "Wenxuan Xie",
            "Yan Lu"
        ],
        "published": "2023-06-02T02:42:58Z",
        "summary": "The recent success of Large Language Models (LLMs) signifies an impressive\nstride towards artificial general intelligence. They have shown a promising\nprospect in automatically completing tasks upon user instructions, functioning\nas brain-like coordinators. The associated risks will be revealed as we\ndelegate an increasing number of tasks to machines for automated completion. A\nbig question emerges: how can we make machines behave responsibly when helping\nhumans automate tasks as personal copilots? In this paper, we explore this\nquestion in depth from the perspectives of feasibility, completeness and\nsecurity. In specific, we present Responsible Task Automation (ResponsibleTA)\nas a fundamental framework to facilitate responsible collaboration between\nLLM-based coordinators and executors for task automation with three empowered\ncapabilities: 1) predicting the feasibility of the commands for executors; 2)\nverifying the completeness of executors; 3) enhancing the security (e.g., the\nprotection of users' privacy). We further propose and compare two paradigms for\nimplementing the first two capabilities. One is to leverage the generic\nknowledge of LLMs themselves via prompt engineering while the other is to adopt\ndomain-specific learnable models. Moreover, we introduce a local memory\nmechanism for achieving the third capability. We evaluate our proposed\nResponsibleTA on UI task automation and hope it could bring more attentions to\nensuring LLMs more responsible in diverse scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2306.01242v2.pdf"
    },
    {
        "title": "Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation",
        "authors": [
            "Bonan Kou",
            "Shengmai Chen",
            "Zhijie Wang",
            "Lei Ma",
            "Tianyi Zhang"
        ],
        "published": "2023-06-02T00:57:03Z",
        "summary": "Large Language Models (LLMs) have been demonstrated effective for code\ngeneration. Due to the complexity and opacity of LLMs, little is known about\nhow these models generate code. To deepen our understanding, we investigate\nwhether LLMs attend to the same parts of a natural language description as\nhuman programmers during code generation. An analysis of five LLMs on a popular\nbenchmark, HumanEval, revealed a consistent misalignment between LLMs' and\nprogrammers' attention. Furthermore, we found that there is no correlation\nbetween the code generation accuracy of LLMs and their alignment with human\nprogrammers. Through a quantitative experiment and a user study, we confirmed\nthat, among twelve different attention computation methods, attention computed\nby the perturbation-based method is most aligned with human attention and is\nconstantly favored by human programmers. Our findings highlight the need for\nhuman-aligned LLMs for better interpretability and programmer trust.",
        "pdf_link": "https://arxiv.org/pdf/2306.01220v1.pdf"
    },
    {
        "title": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
        "authors": [
            "Adithya V Ganesan",
            "Yash Kumar Lal",
            "August Håkan Nilsson",
            "H. Andrew Schwartz"
        ],
        "published": "2023-06-01T22:43:37Z",
        "summary": "Very large language models (LLMs) perform extremely well on a spectrum of NLP\ntasks in a zero-shot setting. However, little is known about their performance\non human-level NLP problems which rely on understanding psychological concepts,\nsuch as assessing personality traits. In this work, we investigate the\nzero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'\nsocial media posts. Through a set of systematic experiments, we find that\nzero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA\nfor broad classification upon injecting knowledge about the trait in the\nprompts. However, when prompted to provide fine-grained classification, its\nperformance drops to close to a simple most frequent class (MFC) baseline. We\nfurther analyze where GPT-3 performs better, as well as worse, than a\npretrained lexical model, illustrating systematic errors that suggest ways to\nimprove LLMs on human-level NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01183v1.pdf"
    },
    {
        "title": "Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study",
        "authors": [
            "Guang Lu",
            "Sylvia B. Larcher",
            "Tu Tran"
        ],
        "published": "2023-06-01T21:58:33Z",
        "summary": "Text summarization is a downstream natural language processing (NLP) task\nthat challenges the understanding and generation capabilities of language\nmodels. Considerable progress has been made in automatically summarizing short\ntexts, such as news articles, often leading to satisfactory results. However,\nsummarizing long documents remains a major challenge. This is due to the\ncomplex contextual information in the text and the lack of open-source\nbenchmarking datasets and evaluation frameworks that can be used to develop and\ntest model performance. In this work, we use ChatGPT, the latest breakthrough\nin the field of large language models (LLMs), together with the extractive\nsummarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a\nhybrid extraction and summarization pipeline for long documents such as\nbusiness articles and books. We work with the world-renowned company\ngetAbstract AG and leverage their expertise and experience in professional book\nsummarization. A practical study has shown that machine-generated summaries can\nperform at least as well as human-written summaries when evaluated using\ncurrent automated evaluation metrics. However, a closer examination of the\ntexts generated by ChatGPT through human evaluations has shown that there are\nstill critical issues in terms of text coherence, faithfulness, and style.\nOverall, our results show that the use of ChatGPT is a very promising but not\nyet mature approach for summarizing long documents and can at best serve as an\ninspiration for human editors. We anticipate that our work will inform NLP\nresearchers about the extent to which ChatGPT's capabilities for summarizing\nlong documents overlap with practitioners' needs. Further work is needed to\ntest the proposed hybrid summarization pipeline, in particular involving GPT-4,\nand to propose a new evaluation framework tailored to the task of summarizing\nlong documents.",
        "pdf_link": "https://arxiv.org/pdf/2306.01169v1.pdf"
    },
    {
        "title": "Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data",
        "authors": [
            "Nathan Vaska",
            "Victoria Helus"
        ],
        "published": "2023-06-01T20:56:34Z",
        "summary": "The impressive advances and applications of large language and joint\nlanguage-and-visual understanding models has led to an increased need for\nmethods of probing their potential reasoning capabilities. However, the\ndifficulty of gather naturally-occurring data for complex multi-modal reasoning\ntasks bottlenecks the evaluation of AI methods on tasks which are not already\ncovered by an academic dataset. In this work, we leverage recent advances in\nhigh resolution text-to-image generation to develop a framework for generating\nevaluation data for multi-modal reasoning tasks. We apply this framework to\ngenerate context-dependent anomaly data, creating a synthetic dataset on a\nchallenging task which is not well covered by existing datasets. We benchmark\nthe performance of a state-of-the-art visual question answering (VQA) model\nagainst data generated with this method, and demonstrate that while the task is\ntractable, the model performs significantly worse on the context-dependent\nanomaly detection task than on standard VQA tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.01144v1.pdf"
    },
    {
        "title": "LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization",
        "authors": [
            "Muhammad U. Nasir",
            "Sam Earle",
            "Julian Togelius",
            "Steven James",
            "Christopher Cleghorn"
        ],
        "published": "2023-06-01T19:33:21Z",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools capable of\naccomplishing a broad spectrum of tasks. Their abilities span numerous areas,\nand one area where they have made a significant impact is in the domain of code\ngeneration. Here, we propose using the coding abilities of LLMs to introduce\nmeaningful variations to code defining neural networks. Meanwhile,\nQuality-Diversity (QD) algorithms are known to discover diverse and robust\nsolutions. By merging the code-generating abilities of LLMs with the diversity\nand robustness of QD solutions, we introduce \\texttt{LLMatic}, a Neural\nArchitecture Search (NAS) algorithm. While LLMs struggle to conduct NAS\ndirectly through prompts, \\texttt{LLMatic} uses a procedural approach,\nleveraging QD for prompts and network architecture to create diverse and\nhigh-performing networks. We test \\texttt{LLMatic} on the CIFAR-10 and\nNAS-bench-201 benchmarks, demonstrating that it can produce competitive\nnetworks while evaluating just $2,000$ candidates, even without prior knowledge\nof the benchmark domain or exposure to any previous top-performing models for\nthe benchmark. The open-sourced code is available in\n\\url{https://github.com/umair-nasir14/LLMatic}.",
        "pdf_link": "https://arxiv.org/pdf/2306.01102v7.pdf"
    },
    {
        "title": "Cook-Gen: Robust Generative Modeling of Cooking Actions from Recipes",
        "authors": [
            "Revathy Venkataramanan",
            "Kaushik Roy",
            "Kanak Raj",
            "Renjith Prasad",
            "Yuxin Zi",
            "Vignesh Narayanan",
            "Amit Sheth"
        ],
        "published": "2023-06-01T18:49:47Z",
        "summary": "As people become more aware of their food choices, food computation models\nhave become increasingly popular in assisting people in maintaining healthy\neating habits. For example, food recommendation systems analyze recipe\ninstructions to assess nutritional contents and provide recipe recommendations.\nThe recent and remarkable successes of generative AI methods, such as\nauto-regressive large language models, can lead to robust methods for a more\ncomprehensive understanding of recipes for healthy food recommendations beyond\nsurface-level nutrition content assessments. In this study, we explore the use\nof generative AI methods to extend current food computation models, primarily\ninvolving the analysis of nutrition and ingredients, to also incorporate\ncooking actions (e.g., add salt, fry the meat, boil the vegetables, etc.).\nCooking actions are notoriously hard to model using statistical learning\nmethods due to irregular data patterns - significantly varying natural language\ndescriptions for the same action (e.g., marinate the meat vs. marinate the meat\nand leave overnight) and infrequently occurring patterns (e.g., add salt occurs\nfar more frequently than marinating the meat). The prototypical approach to\nhandling irregular data patterns is to increase the volume of data that the\nmodel ingests by orders of magnitude. Unfortunately, in the cooking domain,\nthese problems are further compounded with larger data volumes presenting a\nunique challenge that is not easily handled by simply scaling up. In this work,\nwe propose novel aggregation-based generative AI methods, Cook-Gen, that\nreliably generate cooking actions from recipes, despite difficulties with\nirregular data patterns, while also outperforming Large Language Models and\nother strong baselines.",
        "pdf_link": "https://arxiv.org/pdf/2306.01805v1.pdf"
    },
    {
        "title": "TopEx: Topic-based Explanations for Model Comparison",
        "authors": [
            "Shreya Havaldar",
            "Adam Stein",
            "Eric Wong",
            "Lyle Ungar"
        ],
        "published": "2023-06-01T17:59:10Z",
        "summary": "Meaningfully comparing language models is challenging with current\nexplanation methods. Current explanations are overwhelming for humans due to\nlarge vocabularies or incomparable across models. We present TopEx, an\nexplanation method that enables a level playing field for comparing language\nmodels via model-agnostic topics. We demonstrate how TopEx can identify\nsimilarities and differences between DistilRoBERTa and GPT-2 on a variety of\nNLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2306.00976v2.pdf"
    },
    {
        "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
        "authors": [
            "Ji Lin",
            "Jiaming Tang",
            "Haotian Tang",
            "Shang Yang",
            "Xingyu Dang",
            "Chuang Gan",
            "Song Han"
        ],
        "published": "2023-06-01T17:59:10Z",
        "summary": "Large language models (LLMs) have shown excellent performance on various\ntasks, but the astronomical model size raises the hardware barrier for serving\n(memory size) and slows down token generation (memory bandwidth). In this\npaper, we propose Activation-aware Weight Quantization (AWQ), a\nhardware-friendly approach for LLM low-bit weight-only quantization. Our method\nis based on the observation that weights are not equally important: protecting\nonly 1% of salient weights can greatly reduce quantization error. We then\npropose to search for the optimal per-channel scaling that protects the salient\nweights by observing the activation, not weights. AWQ does not rely on any\nbackpropagation or reconstruction, so it can well preserve LLMs' generalization\nability on different domains and modalities, without overfitting to the\ncalibration set. AWQ outperforms existing work on various language modeling and\ndomain-specific benchmarks. Thanks to better generalization, it achieves\nexcellent quantization performance for instruction-tuned LMs and, for the first\ntime, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible\ninference framework tailored for LLMs on the edge, offering more than 3x\nspeedup over the Huggingface FP16 implementation on both desktop and mobile\nGPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile\nGPU (NVIDIA Jetson Orin 64GB).",
        "pdf_link": "https://arxiv.org/pdf/2306.00978v2.pdf"
    },
    {
        "title": "Exposing Attention Glitches with Flip-Flop Language Modeling",
        "authors": [
            "Bingbin Liu",
            "Jordan T. Ash",
            "Surbhi Goel",
            "Akshay Krishnamurthy",
            "Cyril Zhang"
        ],
        "published": "2023-06-01T17:44:35Z",
        "summary": "Why do large language models sometimes output factual inaccuracies and\nexhibit erroneous reasoning? The brittleness of these models, particularly when\nexecuting long chains of reasoning, currently seems to be an inevitable price\nto pay for their advanced capabilities of coherently synthesizing knowledge,\npragmatics, and abstract thought. Towards making sense of this fundamentally\nunsolved problem, this work identifies and analyzes the phenomenon of attention\nglitches, in which the Transformer architecture's inductive biases\nintermittently fail to capture robust reasoning. To isolate the issue, we\nintroduce flip-flop language modeling (FFLM), a parametric family of synthetic\nbenchmarks designed to probe the extrapolative behavior of neural language\nmodels. This simple generative task requires a model to copy binary symbols\nover long-range dependencies, ignoring the tokens in between. We find that\nTransformer FFLMs suffer from a long tail of sporadic reasoning errors, some of\nwhich we can eliminate using various regularization techniques. Our preliminary\nmechanistic analyses show why the remaining errors may be very difficult to\ndiagnose and resolve. We hypothesize that attention glitches account for (some\nof) the closed-domain hallucinations in natural LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2306.00946v2.pdf"
    },
    {
        "title": "Vocabulary-free Image Classification",
        "authors": [
            "Alessandro Conti",
            "Enrico Fini",
            "Massimiliano Mancini",
            "Paolo Rota",
            "Yiming Wang",
            "Elisa Ricci"
        ],
        "published": "2023-06-01T17:19:43Z",
        "summary": "Recent advances in large vision-language models have revolutionized the image\nclassification paradigm. Despite showing impressive zero-shot capabilities, a\npre-defined set of categories, a.k.a. the vocabulary, is assumed at test time\nfor composing the textual prompts. However, such assumption can be impractical\nwhen the semantic context is unknown and evolving. We thus formalize a novel\ntask, termed as Vocabulary-free Image Classification (VIC), where we aim to\nassign to an input image a class that resides in an unconstrained\nlanguage-induced semantic space, without the prerequisite of a known\nvocabulary. VIC is a challenging task as the semantic space is extremely large,\ncontaining millions of concepts, with hard-to-discriminate fine-grained\ncategories. In this work, we first empirically verify that representing this\nsemantic space by means of an external vision-language database is the most\neffective way to obtain semantically relevant content for classifying the\nimage. We then propose Category Search from External Databases (CaSED), a\nmethod that exploits a pre-trained vision-language model and an external\nvision-language database to address VIC in a training-free manner. CaSED first\nextracts a set of candidate categories from captions retrieved from the\ndatabase based on their semantic similarity to the image, and then assigns to\nthe image the best matching candidate category according to the same\nvision-language model. Experiments on benchmark datasets validate that CaSED\noutperforms other complex vision-language frameworks, while being efficient\nwith much fewer parameters, paving the way for future research in this\ndirection.",
        "pdf_link": "https://arxiv.org/pdf/2306.00917v3.pdf"
    },
    {
        "title": "The feasibility of artificial consciousness through the lens of neuroscience",
        "authors": [
            "Jaan Aru",
            "Matthew Larkum",
            "James M. Shine"
        ],
        "published": "2023-06-01T17:18:15Z",
        "summary": "Interactions with large language models have led to the suggestion that these\nmodels may soon be conscious. From the perspective of neuroscience, this\nposition is difficult to defend. For one, the inputs to large language models\nlack the embodied, embedded information content characteristic of our sensory\ncontact with the world around us. Secondly, the architecture of large language\nmodels is missing key features of the thalamocortical system that have been\nlinked to conscious awareness in mammals. Finally, the evolutionary and\ndevelopmental trajectories that led to the emergence of living conscious\norganisms arguably have no parallels in artificial systems as envisioned today.\nThe existence of living organisms depends on their actions, and their survival\nis intricately linked to multi-level cellular, inter-cellular, and organismal\nprocesses culminating in agency and consciousness.",
        "pdf_link": "https://arxiv.org/pdf/2306.00915v3.pdf"
    },
    {
        "title": "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day",
        "authors": [
            "Chunyuan Li",
            "Cliff Wong",
            "Sheng Zhang",
            "Naoto Usuyama",
            "Haotian Liu",
            "Jianwei Yang",
            "Tristan Naumann",
            "Hoifung Poon",
            "Jianfeng Gao"
        ],
        "published": "2023-06-01T16:50:07Z",
        "summary": "Conversational generative AI has demonstrated remarkable promise for\nempowering biomedical practitioners, but current investigations focus on\nunimodal text. Multimodal conversational AI has seen rapid progress by\nleveraging billions of image-text pairs from the public web, but such\ngeneral-domain vision-language models still lack sophistication in\nunderstanding and conversing about biomedical images. In this paper, we propose\na cost-efficient approach for training a vision-language conversational\nassistant that can answer open-ended research questions of biomedical images.\nThe key idea is to leverage a large-scale, broad-coverage biomedical\nfigure-caption dataset extracted from PubMed Central, use GPT-4 to\nself-instruct open-ended instruction-following data from the captions, and then\nfine-tune a large general-domain vision-language model using a novel curriculum\nlearning method. Specifically, the model first learns to align biomedical\nvocabulary using the figure-caption pairs as is, then learns to master\nopen-ended conversational semantics using GPT-4 generated instruction-following\ndata, broadly mimicking how a layperson gradually acquires biomedical\nknowledge. This enables us to train a Large Language and Vision Assistant for\nBioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med\nexhibits excellent multimodal conversational capability and can follow\nopen-ended instruction to assist with inquiries about a biomedical image. On\nthree standard biomedical visual question answering datasets, LLaVA-Med\noutperforms previous supervised state-of-the-art on certain metrics. To\nfacilitate biomedical multimodal research, we will release our\ninstruction-following data and the LLaVA-Med model.",
        "pdf_link": "https://arxiv.org/pdf/2306.00890v1.pdf"
    },
    {
        "title": "Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers",
        "authors": [
            "Ruotong Wang",
            "Hongrui Chen",
            "Zihao Zhu",
            "Li Liu",
            "Yong Zhang",
            "Yanbo Fan",
            "Baoyuan Wu"
        ],
        "published": "2023-06-01T15:42:06Z",
        "summary": "Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors\nwhen exposed to specific trigger patterns, without affecting their performance\non benign samples, dubbed backdoor attack. Some recent research has focused on\ndesigning invisible triggers for backdoor attacks to ensure visual\nstealthiness, while showing high effectiveness, even under backdoor defense.\nHowever, we find that these carefully designed invisible triggers are often\nsensitive to visual distortion during inference, such as Gaussian blurring or\nenvironmental variations in physical scenarios. This phenomenon could\nsignificantly undermine the practical effectiveness of attacks, but has been\nrarely paid attention to and thoroughly investigated. To address this\nlimitation, we define a novel trigger called the Visible, Semantic,\nSample-Specific, and Compatible trigger (VSSC trigger), to achieve effective,\nstealthy and robust to visual distortion simultaneously. To implement it, we\ndevelop an innovative approach by utilizing the powerful capabilities of large\nlanguage models for choosing the suitable trigger and text-guided image editing\ntechniques for generating the poisoned image with the trigger. Extensive\nexperimental results and analysis validate the effectiveness, stealthiness and\nrobustness of the VSSC trigger. It demonstrates superior robustness to\ndistortions compared with most digital backdoor attacks and allows more\nefficient and flexible trigger integration compared to physical backdoor\nattacks. We hope that the proposed VSSC trigger and implementation approach\ncould inspire future studies on designing more practical triggers in backdoor\nattacks.",
        "pdf_link": "https://arxiv.org/pdf/2306.00816v2.pdf"
    },
    {
        "title": "GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?",
        "authors": [
            "Ning Ding",
            "Yehui Tang",
            "Zhongqian Fu",
            "Chao Xu",
            "Kai Han",
            "Yunhe Wang"
        ],
        "published": "2023-06-01T14:02:45Z",
        "summary": "The recent upsurge in pre-trained large models (e.g. GPT-4) has swept across\nthe entire deep learning community. Such powerful large language models (LLMs)\ndemonstrate advanced generative ability and multimodal understanding\ncapability, which quickly achieve new state-of-the-art performances on a\nvariety of benchmarks. The pre-trained LLM usually plays the role as a\nuniversal AI model that can conduct various tasks, including context reasoning,\narticle analysis and image content comprehension. However, considering the\nprohibitively high memory and computational cost for implementing such a large\nmodel, the conventional models (such as CNN and ViT), are still essential for\nmany visual perception tasks. In this paper, we propose to enhance the\nrepresentation ability of ordinary vision models for perception tasks (e.g.\nimage classification) by taking advantage of large pre-trained models. We\npresent a new learning paradigm in which the knowledge extracted from large\npre-trained models are utilized to help models like CNN and ViT learn enhanced\nrepresentations and achieve better performance. Firstly, we curate a high\nquality description set by prompting a multimodal LLM to generate descriptive\ntext for all training images. Furthermore, we feed these detailed descriptions\ninto a pre-trained encoder to extract text embeddings with rich semantic\ninformation that encodes the content of images. During training, text\nembeddings will serve as extra supervising signals and be aligned with image\nrepresentations learned by vision models. The alignment process helps vision\nmodels learn better and achieve higher accuracy with the assistance of\npre-trained LLMs. We conduct extensive experiments to verify that the proposed\nalgorithm consistently improves the performance for various vision models with\nheterogeneous architectures.",
        "pdf_link": "https://arxiv.org/pdf/2306.00693v2.pdf"
    },
    {
        "title": "Explanation Graph Generation via Generative Pre-training over Synthetic Graphs",
        "authors": [
            "Han Cui",
            "Shangzhan Li",
            "Yu Zhang",
            "Qi Shi"
        ],
        "published": "2023-06-01T13:20:22Z",
        "summary": "The generation of explanation graphs is a significant task that aims to\nproduce explanation graphs in response to user input, revealing the internal\nreasoning process. This task is challenging due to the significant discrepancy\nbetween unstructured user queries and structured explanation graphs. Current\nresearch commonly fine-tunes a text-based pre-trained language model on a small\ndownstream dataset that is annotated with labeled graphs. However, due to the\nlimited scale of available datasets, this approach may prove to be insufficient\nin bridging the gap between natural language text and structured graphs. In\nthis paper, to alleviate the above limitations, we propose a novel pre-trained\nframework EG3P(for Explanation Graph Generation via Generative Pre-training\nover synthetic graphs) for the explanation graph generation task. Specifically,\nwe first propose a text-to-graph generative task to pre-train the model with\nthe goal of bridging the text-graph gap. Additionally, we propose an automatic\ncorpus synthesis strategy for synthesizing a large scale of high-quality\ncorpus, reducing the reliance on costly manual annotation methods. Experimental\nresults on ExplaGraphs show the effectiveness of EG3P that our model surpasses\nall baseline systems with remarkable margins. Besides, further analysis\ndemonstrates that EG3P is able to generate better explanation graphs on actual\nreasoning tasks such as CommonsenseQA and OpenbookQA.",
        "pdf_link": "https://arxiv.org/pdf/2306.00652v1.pdf"
    },
    {
        "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
        "authors": [
            "Ryan Liu",
            "Nihar B. Shah"
        ],
        "published": "2023-06-01T12:45:53Z",
        "summary": "Given the rapid ascent of large language models (LLMs), we study the\nquestion: (How) can large language models help in reviewing of scientific\npapers or proposals? We first conduct some pilot studies where we find that (i)\nGPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to\nidentify errors) outperforms prompting to simply write a review. With these\ninsights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each\nwith a deliberately inserted error, and ask the LLM to check for the\ncorrectness of these papers. We observe that the LLM finds errors in 7 of them,\nspanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist\nquestions in the respective sections of 15 NeurIPS 2022 papers. We find that\nacross 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts,\ndeliberately designing each pair in such a way that one abstract was clearly\nsuperior than the other. The LLM, however, struggled to discern these\nrelatively straightforward distinctions accurately, committing errors in its\nevaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as\nreviewing assistants for specific reviewing tasks, but not (yet) for complete\nevaluations of papers or proposals.",
        "pdf_link": "https://arxiv.org/pdf/2306.00622v1.pdf"
    },
    {
        "title": "Analysis of ChatGPT on Source Code",
        "authors": [
            "Ahmed R. Sadik",
            "Antonello Ceravola",
            "Frank Joublin",
            "Jibesh Patra"
        ],
        "published": "2023-06-01T12:12:59Z",
        "summary": "This paper explores the use of Large Language Models (LLMs) and in particular\nChatGPT in programming, source code analysis, and code generation. LLMs and\nChatGPT are built using machine learning and artificial intelligence\ntechniques, and they offer several benefits to developers and programmers.\nWhile these models can save time and provide highly accurate results, they are\nnot yet advanced enough to replace human programmers entirely. The paper\ninvestigates the potential applications of LLMs and ChatGPT in various areas,\nsuch as code creation, code documentation, bug detection, refactoring, and\nmore. The paper also suggests that the usage of LLMs and ChatGPT is expected to\nincrease in the future as they offer unparalleled benefits to the programming\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2306.00597v2.pdf"
    },
    {
        "title": "Chain-Of-Thought Prompting Under Streaming Batch: A Case Study",
        "authors": [
            "Yuxin Tang"
        ],
        "published": "2023-06-01T11:11:39Z",
        "summary": "Recently, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities. Chain-of-Thought (CoT) has been proposed as a way of assisting\nLLMs in performing complex reasoning. However, developing effective prompts can\nbe a challenging and labor-intensive task. Many studies come out of some way to\nautomatically construct CoT from test data. Most of them assume that all test\ndata is visible before testing and only select a small subset to generate\nrationales, which is an unrealistic assumption. In this paper, we present a\ncase study on how to construct and optimize chain-of-thought prompting using\nbatch data in streaming settings.",
        "pdf_link": "https://arxiv.org/pdf/2306.00550v1.pdf"
    },
    {
        "title": "CapText: Large Language Model-based Caption Generation From Image Context and Description",
        "authors": [
            "Shinjini Ghosh",
            "Sagnik Anupam"
        ],
        "published": "2023-06-01T02:40:44Z",
        "summary": "While deep-learning models have been shown to perform well on image-to-text\ndatasets, it is difficult to use them in practice for captioning images. This\nis because captions traditionally tend to be context-dependent and offer\ncomplementary information about an image, while models tend to produce\ndescriptions that describe the visual features of the image. Prior research in\ncaption generation has explored the use of models that generate captions when\nprovided with the images alongside their respective descriptions or contexts.\nWe propose and evaluate a new approach, which leverages existing large language\nmodels to generate captions from textual descriptions and context alone,\nwithout ever processing the image directly. We demonstrate that after\nfine-tuning, our approach outperforms current state-of-the-art image-text\nalignment models like OSCAR-VinVL on this task on the CIDEr metric.",
        "pdf_link": "https://arxiv.org/pdf/2306.00301v2.pdf"
    },
    {
        "title": "Rethinking Model Evaluation as Narrowing the Socio-Technical Gap",
        "authors": [
            "Q. Vera Liao",
            "Ziang Xiao"
        ],
        "published": "2023-06-01T00:01:43Z",
        "summary": "The recent development of generative and large language models (LLMs) poses\nnew challenges for model evaluation that the research community and industry\nare grappling with. While the versatile capabilities of these models ignite\nexcitement, they also inevitably make a leap toward homogenization: powering a\nwide range of applications with a single, often referred to as\n``general-purpose'', model. In this position paper, we argue that model\nevaluation practices must take on a critical task to cope with the challenges\nand responsibilities brought by this homogenization: providing valid\nassessments for whether and how much human needs in downstream use cases can be\nsatisfied by the given model (socio-technical gap). By drawing on lessons from\nthe social sciences, human-computer interaction (HCI), and the\ninterdisciplinary field of explainable AI (XAI), we urge the community to\ndevelop evaluation methods based on real-world socio-requirements and embrace\ndiverse evaluation methods with an acknowledgment of trade-offs between realism\nto socio-requirements and pragmatic costs to conduct the evaluation. By mapping\nHCI and current NLG evaluation methods, we identify opportunities for\nevaluation methods for LLMs to narrow the socio-technical gap and pose open\nquestions.",
        "pdf_link": "https://arxiv.org/pdf/2306.03100v3.pdf"
    },
    {
        "title": "An Invariant Learning Characterization of Controlled Text Generation",
        "authors": [
            "Carolina Zheng",
            "Claudia Shi",
            "Keyon Vafa",
            "Amir Feder",
            "David M. Blei"
        ],
        "published": "2023-05-31T21:35:08Z",
        "summary": "Controlled generation refers to the problem of creating text that contains\nstylistic or semantic attributes of interest. Many approaches reduce this\nproblem to training a predictor of the desired attribute. For example,\nresearchers hoping to deploy a large language model to produce non-toxic\ncontent may use a toxicity classifier to filter generated text. In practice,\nthe generated text to classify, which is determined by user prompts, may come\nfrom a wide range of distributions. In this paper, we show that the performance\nof controlled generation may be poor if the distributions of text in response\nto user prompts differ from the distribution the predictor was trained on. To\naddress this problem, we cast controlled generation under distribution shift as\nan invariant learning problem: the most effective predictor should be invariant\nacross multiple text environments. We then discuss a natural solution that\narises from this characterization and propose heuristics for selecting natural\nenvironments. We study this characterization and the proposed method\nempirically using both synthetic and real data. Experiments demonstrate both\nthe challenge of distribution shift in controlled generation and the potential\nof invariance methods in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2306.00198v1.pdf"
    },
    {
        "title": "Automated Annotation with Generative AI Requires Validation",
        "authors": [
            "Nicholas Pangakis",
            "Samuel Wolken",
            "Neil Fasching"
        ],
        "published": "2023-05-31T20:50:45Z",
        "summary": "Generative large language models (LLMs) can be a powerful tool for augmenting\ntext annotation procedures, but their performance varies across annotation\ntasks due to prompt quality, text data idiosyncrasies, and conceptual\ndifficulty. Because these challenges will persist even as LLM technology\nimproves, we argue that any automated annotation process using an LLM must\nvalidate the LLM's performance against labels generated by humans. To this end,\nwe outline a workflow to harness the annotation potential of LLMs in a\nprincipled, efficient way. Using GPT-4, we validate this approach by\nreplicating 27 annotation tasks across 11 datasets from recent social science\narticles in high-impact journals. We find that LLM performance for text\nannotation is promising but highly contingent on both the dataset and the type\nof annotation task, which reinforces the necessity to validate on a\ntask-by-task basis. We make available easy-to-use software designed to\nimplement our workflow and streamline the deployment of LLMs for automated\nannotation.",
        "pdf_link": "https://arxiv.org/pdf/2306.00176v1.pdf"
    },
    {
        "title": "Measuring the Robustness of NLP Models to Domain Shifts",
        "authors": [
            "Nitay Calderon",
            "Naveh Porat",
            "Eyal Ben-David",
            "Alexander Chapanin",
            "Zorik Gekhman",
            "Nadav Oved",
            "Vitaly Shalumov",
            "Roi Reichart"
        ],
        "published": "2023-05-31T20:25:08Z",
        "summary": "Existing research on Domain Robustness (DR) suffers from disparate setups,\nlack of task variety, and scarce research on recent capabilities such as\nfew-shot learning. Furthermore, we claim that the common practice of measuring\nDR might further obscure the picture. Current research focuses on challenge\nsets and relies solely on the Source Drop (SD): Using the source in-domain\nperformance as a reference point for degradation. However, the Target Drop\n(TD), which measures degradation from the target in-domain performance, should\nbe used as a complementary point of view. In this study, we developed a\nbenchmark comprised of seven NLP tasks, including classification, QA, and\ngeneration. Our benchmark focuses on natural topical domain shifts and enables\nmeasuring both the SD and the TD. Our comprehensive study, involving over\n14,000 domain shifts across 18 fine-tuned and few-shot models, shows that both\nmodel types suffer from drops upon domain shifts. While fine-tuned models excel\nin-domain, few-shot LLMs often surpass them cross-domain, showing better\nrobustness. In addition, we found that a large SD can be explained by shifting\nto a harder domain rather than by a genuine DR challenge. Thus, the TD is a\nmore reliable metric for assessing DR.",
        "pdf_link": "https://arxiv.org/pdf/2306.00168v4.pdf"
    },
    {
        "title": "Better patching using LLM prompting, via Self-Consistency",
        "authors": [
            "Toufique Ahmed",
            "Premkumar Devanbu"
        ],
        "published": "2023-05-31T18:28:46Z",
        "summary": "Large Language models (LLMs) can be induced to solve non-trivial problems\nwith \"few-shot\" prompts including illustrative problem-solution examples. Now\nif the few-shots also include \"chain of thought\" (CoT) explanations, which are\nof the form problem-explanation-solution, LLMs will generate a \"explained\"\nsolution, and perform even better. Recently an exciting, substantially better\ntechnique, self-consistency [1] (S-C) has emerged, based on the intuition that\nthere are many plausible explanations for the right solution; when the LLM is\nsampled repeatedly to generate a pool of explanation-solution pairs, for a\ngiven problem, the most frequently occurring solutions in the pool (ignoring\nthe explanations) tend to be even more likely to be correct! Unfortunately, the\nuse of this highly-performant S-C (or even CoT) approach in software\nengineering settings is hampered by the lack of explanations; most software\ndatasets lack explanations. In this paper, we describe an application of the\nS-C approach to program repair, using the commit log on the fix as the\nexplanation, only in the illustrative few-shots. We achieve state-of-the art\nresults, beating previous approaches to prompting-based program repair, on the\nMODIT dataset; we also find evidence suggesting that the correct commit\nmessages are helping the LLM learn to produce better patches.",
        "pdf_link": "https://arxiv.org/pdf/2306.00108v2.pdf"
    },
    {
        "title": "Improving CLIP Training with Language Rewrites",
        "authors": [
            "Lijie Fan",
            "Dilip Krishnan",
            "Phillip Isola",
            "Dina Katabi",
            "Yonglong Tian"
        ],
        "published": "2023-05-31T17:59:04Z",
        "summary": "Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.",
        "pdf_link": "https://arxiv.org/pdf/2305.20088v2.pdf"
    },
    {
        "title": "Scaling Evidence-based Instructional Design Expertise through Large Language Models",
        "authors": [
            "Gautam Yadav"
        ],
        "published": "2023-05-31T17:54:07Z",
        "summary": "This paper presents a comprehensive exploration of leveraging Large Language\nModels (LLMs), specifically GPT-4, in the field of instructional design. With a\nfocus on scaling evidence-based instructional design expertise, our research\naims to bridge the gap between theoretical educational studies and practical\nimplementation. We discuss the benefits and limitations of AI-driven content\ngeneration, emphasizing the necessity of human oversight in ensuring the\nquality of educational materials. This work is elucidated through two detailed\ncase studies where we applied GPT-4 in creating complex higher-order\nassessments and active learning components for different courses. From our\nexperiences, we provide best practices for effectively using LLMs in\ninstructional design tasks, such as utilizing templates, fine-tuning, handling\nunexpected output, implementing LLM chains, citing references, evaluating\noutput, creating rubrics, grading, and generating distractors. We also share\nour vision of a future recommendation system, where a customized GPT-4 extracts\ninstructional design principles from educational studies and creates\npersonalized, evidence-supported strategies for users' unique educational\ncontexts. Our research contributes to understanding and optimally harnessing\nthe potential of AI-driven language models in enhancing educational outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2306.01006v2.pdf"
    },
    {
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
        "authors": [
            "Jessy Lin",
            "Nicholas Tomlin",
            "Jacob Andreas",
            "Jason Eisner"
        ],
        "published": "2023-05-31T17:50:02Z",
        "summary": "We describe a class of tasks called decision-oriented dialogues, in which AI\nassistants must collaborate with one or more humans via natural language to\nhelp them make complex decisions. We formalize three domains in which users\nface everyday decisions: (1) choosing an assignment of reviewers to conference\npapers, (2) planning a multi-step itinerary in a city, and (3) negotiating\ntravel plans for a group of friends. In each of these settings, AI assistants\nand users have disparate abilities that they must combine to arrive at the best\ndecision: assistants can access and process large amounts of information, while\nusers have preferences and constraints external to the system. For each task,\nwe build a dialogue environment where agents receive a reward based on the\nquality of the final decision they reach. Using these environments, we collect\nhuman-human dialogues with humans playing the role of assistant. To compare how\ncurrent AI assistants communicate in these settings, we present baselines using\nlarge language models in self-play. Finally, we highlight a number of\nchallenges models face in decision-oriented dialogues, ranging from efficient\ncommunication to reasoning and optimization, and release our environments as a\ntestbed for future modeling work.",
        "pdf_link": "https://arxiv.org/pdf/2305.20076v2.pdf"
    },
    {
        "title": "Let's Verify Step by Step",
        "authors": [
            "Hunter Lightman",
            "Vineet Kosaraju",
            "Yura Burda",
            "Harri Edwards",
            "Bowen Baker",
            "Teddy Lee",
            "Jan Leike",
            "John Schulman",
            "Ilya Sutskever",
            "Karl Cobbe"
        ],
        "published": "2023-05-31T17:24:00Z",
        "summary": "In recent years, large language models have greatly improved in their ability\nto perform complex multi-step reasoning. However, even state-of-the-art models\nstill regularly produce logical mistakes. To train more reliable models, we can\nturn either to outcome supervision, which provides feedback for a final result,\nor process supervision, which provides feedback for each intermediate reasoning\nstep. Given the importance of training reliable models, and given the high cost\nof human feedback, it is important to carefully compare the both methods.\nRecent work has already begun this comparison, but many questions still remain.\nWe conduct our own investigation, finding that process supervision\nsignificantly outperforms outcome supervision for training models to solve\nproblems from the challenging MATH dataset. Our process-supervised model solves\n78% of problems from a representative subset of the MATH test set.\nAdditionally, we show that active learning significantly improves the efficacy\nof process supervision. To support related research, we also release PRM800K,\nthe complete dataset of 800,000 step-level human feedback labels used to train\nour best reward model.",
        "pdf_link": "https://arxiv.org/pdf/2305.20050v1.pdf"
    },
    {
        "title": "Supplementary Features of BiLSTM for Enhanced Sequence Labeling",
        "authors": [
            "Conglei Xu",
            "Kun Shen",
            "Hongguang Sun"
        ],
        "published": "2023-05-31T15:05:25Z",
        "summary": "Sequence labeling tasks require the computation of sentence representations\nfor each word within a given sentence. A prevalent method incorporates a\nBi-directional Long Short-Term Memory (BiLSTM) layer to enhance the sequence\nstructure information. However, empirical evidence Li (2020) suggests that the\ncapacity of BiLSTM to produce sentence representations for sequence labeling\ntasks is inherently limited. This limitation primarily results from the\nintegration of fragments from past and future sentence representations to\nformulate a complete sentence representation. In this study, we observed that\nthe entire sentence representation, found in both the first and last cells of\nBiLSTM, can supplement each the individual sentence representation of each\ncell. Accordingly, we devised a global context mechanism to integrate entire\nfuture and past sentence representations into each cell's sentence\nrepresentation within the BiLSTM framework. By incorporating the BERT model\nwithin BiLSTM as a demonstration, and conducting exhaustive experiments on nine\ndatasets for sequence labeling tasks, including named entity recognition (NER),\npart of speech (POS) tagging, and End-to-End Aspect-Based sentiment analysis\n(E2E-ABSA). We noted significant improvements in F1 scores and accuracy across\nall examined datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.19928v4.pdf"
    },
    {
        "title": "Revisiting the Reliability of Psychological Scales on Large Language Models",
        "authors": [
            "Jen-tse Huang",
            "Wenxuan Wang",
            "Man Ho Lam",
            "Eric John Li",
            "Wenxiang Jiao",
            "Michael R. Lyu"
        ],
        "published": "2023-05-31T15:03:28Z",
        "summary": "Recent research has extended beyond assessing the performance of Large\nLanguage Models (LLMs) to examining their characteristics from a psychological\nstandpoint, acknowledging the necessity of understanding their behavioral\ncharacteristics. The administration of personality tests to LLMs has emerged as\na noteworthy area in this context. However, the suitability of employing\npsychological scales, initially devised for humans, on LLMs is a matter of\nongoing debate. Our study aims to determine the reliability of applying\npersonality assessments to LLMs, explicitly investigating whether LLMs\ndemonstrate consistent personality traits. Analyzing responses under 2,500\nsettings reveals that gpt-3.5-turbo shows consistency in responses to the Big\nFive Inventory, indicating a high degree of reliability. Furthermore, our\nresearch explores the potential of gpt-3.5-turbo to emulate diverse\npersonalities and represent various groups, which is a capability increasingly\nsought after in social sciences for substituting human participants with LLMs\nto reduce costs. Our findings reveal that LLMs have the potential to represent\ndifferent personalities with specific prompt instructions. By shedding light on\nthe personalization of LLMs, our study endeavors to pave the way for future\nexplorations in this field. We have made our experimental results and the\ncorresponding code openly accessible via\nhttps://github.com/CUHK-ARISE/LLMPersonality.",
        "pdf_link": "https://arxiv.org/pdf/2305.19926v3.pdf"
    },
    {
        "title": "Neuron to Graph: Interpreting Language Model Neurons at Scale",
        "authors": [
            "Alex Foote",
            "Neel Nanda",
            "Esben Kran",
            "Ioannis Konstas",
            "Shay Cohen",
            "Fazl Barez"
        ],
        "published": "2023-05-31T14:44:33Z",
        "summary": "Advances in Large Language Models (LLMs) have led to remarkable capabilities,\nyet their inner mechanisms remain largely unknown. To understand these models,\nwe need to unravel the functions of individual neurons and their contribution\nto the network. This paper introduces a novel automated approach designed to\nscale interpretability techniques across a vast array of neurons within LLMs,\nto make them more interpretable and ultimately safe. Conventional methods\nrequire examination of examples with strong neuron activation and manual\nidentification of patterns to decipher the concepts a neuron responds to. We\npropose Neuron to Graph (N2G), an innovative tool that automatically extracts a\nneuron's behaviour from the dataset it was trained on and translates it into an\ninterpretable graph. N2G uses truncation and saliency methods to emphasise only\nthe most pertinent tokens to a neuron while enriching dataset examples with\ndiverse samples to better encompass the full spectrum of neuron behaviour.\nThese graphs can be visualised to aid researchers' manual interpretation, and\ncan generate token activations on text for automatic validation by comparison\nwith the neuron's ground truth activations, which we use to show that the model\nis better at predicting neuron activation than two baseline methods. We also\ndemonstrate how the generated graph representations can be flexibly used to\nfacilitate further automation of interpretability research, by searching for\nneurons with particular properties, or programmatically comparing neurons to\neach other to identify similar neurons. Our method easily scales to build graph\nrepresentations for all neurons in a 6-layer Transformer model using a single\nTesla T4 GPU, allowing for wide usability. We release the code and instructions\nfor use at https://github.com/alexjfoote/Neuron2Graph.",
        "pdf_link": "https://arxiv.org/pdf/2305.19911v1.pdf"
    },
    {
        "title": "A Survey on Large Language Models for Recommendation",
        "authors": [
            "Likang Wu",
            "Zhi Zheng",
            "Zhaopeng Qiu",
            "Hao Wang",
            "Hongchao Gu",
            "Tingjia Shen",
            "Chuan Qin",
            "Chen Zhu",
            "Hengshu Zhu",
            "Qi Liu",
            "Hui Xiong",
            "Enhong Chen"
        ],
        "published": "2023-05-31T13:51:26Z",
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of\nNatural Language Processing (NLP) and have recently gained significant\nattention in the domain of Recommendation Systems (RS). These models, trained\non massive amounts of data using self-supervised learning, have demonstrated\nremarkable success in learning universal representations and have the potential\nto enhance various aspects of recommendation systems by some effective transfer\ntechniques such as fine-tuning and prompt tuning, and so on. The crucial aspect\nof harnessing the power of language models in enhancing recommendation quality\nis the utilization of their high-quality representations of textual features\nand their extensive coverage of external knowledge to establish correlations\nbetween items and users. To provide a comprehensive understanding of the\nexisting LLM-based recommendation systems, this survey presents a taxonomy that\ncategorizes these models into two major paradigms, respectively Discriminative\nLLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation\n(GLLM4Rec), with the latter being systematically sorted out for the first time.\nFurthermore, we systematically review and analyze existing LLM-based\nrecommendation systems within each paradigm, providing insights into their\nmethodologies, techniques, and performance. Additionally, we identify key\nchallenges and several valuable findings to provide researchers and\npractitioners with inspiration. We have also created a GitHub repository to\nindex relevant papers on LLMs for recommendation,\nhttps://github.com/WLiK/LLM4Rec.",
        "pdf_link": "https://arxiv.org/pdf/2305.19860v4.pdf"
    },
    {
        "title": "Red Teaming Language Model Detectors with Language Models",
        "authors": [
            "Zhouxing Shi",
            "Yihan Wang",
            "Fan Yin",
            "Xiangning Chen",
            "Kai-Wei Chang",
            "Cho-Jui Hsieh"
        ],
        "published": "2023-05-31T10:08:37Z",
        "summary": "The prevalence and strong capability of large language models (LLMs) present\nsignificant safety and ethical risks if exploited by malicious users. To\nprevent the potentially deceptive usage of LLMs, recent works have proposed\nalgorithms to detect LLM-generated text and protect LLMs. In this paper, we\ninvestigate the robustness and reliability of these LLM detectors under\nadversarial attacks. We study two types of attack strategies: 1) replacing\ncertain words in an LLM's output with their synonyms given the context; 2)\nautomatically searching for an instructional prompt to alter the writing style\nof the generation. In both strategies, we leverage an auxiliary LLM to generate\nthe word replacements or the instructional prompt. Different from previous\nworks, we consider a challenging setting where the auxiliary LLM can also be\nprotected by a detector. Experiments reveal that our attacks effectively\ncompromise the performance of all detectors in the study with plausible\ngenerations, underscoring the urgent need to improve the robustness of\nLLM-generated text detection systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.19713v2.pdf"
    },
    {
        "title": "CodeTF: One-stop Transformer Library for State-of-the-art Code LLM",
        "authors": [
            "Nghi D. Q. Bui",
            "Hung Le",
            "Yue Wang",
            "Junnan Li",
            "Akhilesh Deepak Gotmare",
            "Steven C. H. Hoi"
        ],
        "published": "2023-05-31T05:24:48Z",
        "summary": "Code intelligence plays a key role in transforming modern software\nengineering. Recently, deep learning-based models, especially Transformer-based\nlarge language models (LLMs), have demonstrated remarkable potential in\ntackling these tasks by leveraging massive open-source code data and\nprogramming language features. However, the development and deployment of such\nmodels often require expertise in both machine learning and software\nengineering, creating a barrier for the model adoption. In this paper, we\npresent CodeTF, an open-source Transformer-based library for state-of-the-art\nCode LLMs and code intelligence. Following the principles of modular design and\nextensible framework, we design CodeTF with a unified interface to enable rapid\naccess and development across different types of models, datasets and tasks.\nOur library supports a collection of pretrained Code LLM models and popular\ncode benchmarks, including a standardized interface to train and serve code\nLLMs efficiently, and data features such as language-specific parsers and\nutility functions for extracting code attributes. In this paper, we describe\nthe design principles, the architecture, key modules and components, and\ncompare with other related library tools. Finally, we hope CodeTF is able to\nbridge the gap between machine learning/generative AI and software engineering,\nproviding a comprehensive open-source solution for developers, researchers, and\npractitioners.",
        "pdf_link": "https://arxiv.org/pdf/2306.00029v1.pdf"
    },
    {
        "title": "Large Language Models Are Not Strong Abstract Reasoners",
        "authors": [
            "Gaël Gendron",
            "Qiming Bao",
            "Michael Witbrock",
            "Gillian Dobbie"
        ],
        "published": "2023-05-31T04:50:29Z",
        "summary": "Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nopaque, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally circumscribed.\nAbstract reasoning is a fundamental task for cognition, consisting of finding\nand applying a general pattern from few data. Evaluating deep neural\narchitectures on this task could give insight into their potential limitations\nregarding reasoning and their broad generalisation abilities, yet this is\ncurrently an under-explored area. In this paper, we introduce a new benchmark\nfor evaluating language models beyond memorization on abstract reasoning tasks.\nWe perform extensive evaluations of state-of-the-art LLMs, showing that they\ncurrently achieve very limited performance in contrast with other natural\nlanguage tasks, even when applying techniques that have been shown to improve\nperformance on other NLP tasks. We argue that guiding LLM generation to follow\ncausal paths could help improve the generalisation and reasoning abilities of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.19555v3.pdf"
    },
    {
        "title": "Catalysis distillation neural network for the few shot open catalyst challenge",
        "authors": [
            "Bowen Deng"
        ],
        "published": "2023-05-31T04:23:56Z",
        "summary": "The integration of artificial intelligence and science has resulted in\nsubstantial progress in computational chemistry methods for the design and\ndiscovery of novel catalysts. Nonetheless, the challenges of electrocatalytic\nreactions and developing a large-scale language model in catalysis persist, and\nthe recent success of ChatGPT's (Chat Generative Pre-trained Transformer)\nfew-shot methods surpassing BERT (Bidirectional Encoder Representation from\nTransformers) underscores the importance of addressing limited data, expensive\ncomputations, time constraints and structure-activity relationship in research.\nHence, the development of few-shot techniques for catalysis is critical and\nessential, regardless of present and future requirements. This paper introduces\nthe Few-Shot Open Catalyst Challenge 2023, a competition aimed at advancing the\napplication of machine learning technology for predicting catalytic reactions\non catalytic surfaces, with a specific focus on dual-atom catalysts in hydrogen\nperoxide electrocatalysis. To address the challenge of limited data in\ncatalysis, we propose a machine learning approach based on MLP-Like and a\nframework called Catalysis Distillation Graph Neural Network (CDGNN). Our\nresults demonstrate that CDGNN effectively learns embeddings from catalytic\nstructures, enabling the capture of structure-adsorption relationships. This\naccomplishment has resulted in the utmost advanced and efficient determination\nof the reaction pathway for hydrogen peroxide, surpassing the current graph\nneural network approach by 16.1%.. Consequently, CDGNN presents a promising\napproach for few-shot learning in catalysis.",
        "pdf_link": "https://arxiv.org/pdf/2305.19545v1.pdf"
    },
    {
        "title": "Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning",
        "authors": [
            "Xiaoxin He",
            "Xavier Bresson",
            "Thomas Laurent",
            "Adam Perold",
            "Yann LeCun",
            "Bryan Hooi"
        ],
        "published": "2023-05-31T03:18:03Z",
        "summary": "Representation learning on text-attributed graphs (TAGs) has become a\ncritical research problem in recent years. A typical example of a TAG is a\npaper citation graph, where the text of each paper serves as node attributes.\nInitial graph neural network (GNN) pipelines handled these text attributes by\ntransforming them into shallow or hand-crafted features, such as skip-gram or\nbag-of-words features. Recent efforts have focused on enhancing these pipelines\nwith language models (LMs), which typically demand intricate designs and\nsubstantial computational resources. With the advent of powerful large language\nmodels (LLMs) such as GPT or Llama2, which demonstrate an ability to reason and\nto utilize general knowledge, there is a growing need for techniques which\ncombine the textual modelling abilities of LLMs with the structural learning\ncapabilities of GNNs. Hence, in this work, we focus on leveraging LLMs to\ncapture textual information as features, which can be used to boost GNN\nperformance on downstream tasks. A key innovation is our use of explanations as\nfeatures: we prompt an LLM to perform zero-shot classification, request textual\nexplanations for its decision-making process, and design an LLM-to-LM\ninterpreter to translate these explanations into informative features for\ndownstream GNNs. Our experiments demonstrate that our method achieves\nstate-of-the-art results on well-established TAG datasets, including Cora,\nPubMed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23.\nFurthermore, our method significantly speeds up training, achieving a 2.88\ntimes improvement over the closest baseline on ogbn-arxiv. Lastly, we believe\nthe versatility of the proposed method extends beyond TAGs and holds the\npotential to enhance other tasks involving graph-text data. Our codes and\ndatasets are available at: https://github.com/XiaoxinHe/TAPE.",
        "pdf_link": "https://arxiv.org/pdf/2305.19523v5.pdf"
    },
    {
        "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning",
        "authors": [
            "Faeze Brahman",
            "Chandra Bhagavatula",
            "Valentina Pyatkin",
            "Jena D. Hwang",
            "Xiang Lorraine Li",
            "Hirona J. Arai",
            "Soumya Sanyal",
            "Keisuke Sakaguchi",
            "Xiang Ren",
            "Yejin Choi"
        ],
        "published": "2023-05-31T00:55:40Z",
        "summary": "Procedural planning, which entails decomposing a high-level goal into a\nsequence of temporally ordered steps, is an important yet intricate task for\nmachines. It involves integrating common-sense knowledge to reason about\ncomplex contextualized situations that are often counterfactual, e.g.\n\"scheduling a doctor's appointment without a phone\". While current approaches\nshow encouraging results using large language models (LLMs), they are hindered\nby drawbacks such as costly API calls and reproducibility issues. In this\npaper, we advocate planning using smaller language models. We present PlaSma, a\nnovel two-pronged approach to endow small language models with procedural\nknowledge and (counterfactual) planning capabilities. More concretely, we\ndevelop symbolic procedural knowledge distillation to enhance the implicit\nknowledge in small language models and an inference-time algorithm to\nfacilitate more structured and accurate reasoning. In addition, we introduce a\nnovel task, Counterfactual Planning, that requires a revision of a plan to cope\nwith a counterfactual situation. In both the original and counterfactual\nsetting, we show that orders-of-magnitude smaller models (770M-11B parameters)\ncan compete and often surpass their larger teacher models' capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2305.19472v2.pdf"
    },
    {
        "title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
        "authors": [
            "Zelalem Gero",
            "Chandan Singh",
            "Hao Cheng",
            "Tristan Naumann",
            "Michel Galley",
            "Jianfeng Gao",
            "Hoifung Poon"
        ],
        "published": "2023-05-30T22:05:11Z",
        "summary": "Extracting patient information from unstructured text is a critical task in\nhealth decision-support and clinical research. Large language models (LLMs)\nhave shown the potential to accelerate clinical curation via few-shot\nin-context learning, in contrast to supervised learning which requires much\nmore costly human annotations. However, despite drastic advances in modern LLMs\nsuch as GPT-4, they still struggle with issues regarding accuracy and\ninterpretability, especially in mission-critical domains such as health. Here,\nwe explore a general mitigation framework using self-verification, which\nleverages the LLM to provide provenance for its own extraction and check its\nown outputs. This is made possible by the asymmetry between verification and\ngeneration, where the latter is often much easier than the former. Experimental\nresults show that our method consistently improves accuracy for various LLMs in\nstandard clinical information extraction tasks. Additionally, self-verification\nyields interpretations in the form of a short text span corresponding to each\noutput, which makes it very efficient for human experts to audit the results,\npaving the way towards trustworthy extraction of clinical information in\nresource-constrained scenarios. To facilitate future research in this\ndirection, we release our code and prompts.",
        "pdf_link": "https://arxiv.org/pdf/2306.00024v1.pdf"
    },
    {
        "title": "Stable Anisotropic Regularization",
        "authors": [
            "William Rudman",
            "Carsten Eickhoff"
        ],
        "published": "2023-05-30T18:57:45Z",
        "summary": "Given the success of Large Language Models (LLMs), there has been\nconsiderable interest in studying the properties of model activations. The\nliterature overwhelmingly agrees that LLM representations are dominated by a\nfew \"outlier dimensions\" with exceedingly high variance and magnitude. Several\nstudies in Natural Language Processing (NLP) have sought to mitigate the impact\nof such outlier dimensions and force LLMs to be isotropic (i.e., have uniform\nvariance across all dimensions in embedding space). Isotropy is thought to be a\ndesirable property for LLMs that improves model performance and more closely\naligns textual representations with human intuition. However, many of the\nclaims regarding isotropy in NLP have been based on the average cosine\nsimilarity of embeddings, which has recently been shown to be a flawed measure\nof isotropy. In this paper, we propose I-STAR: IsoScore*-based STable\nAnisotropic Regularization, a novel regularization method that can be used to\nincrease or decrease levels of isotropy in embedding space during training.\nI-STAR uses IsoScore*, the first accurate measure of isotropy that is both\ndifferentiable and stable on mini-batch computations. In contrast to several\nprevious works, we find that decreasing isotropy in contextualized embeddings\nimproves performance on the majority of tasks and models considered in this\npaper.",
        "pdf_link": "https://arxiv.org/pdf/2305.19358v3.pdf"
    },
    {
        "title": "GPT4GEO: How a Language Model Sees the World's Geography",
        "authors": [
            "Jonathan Roberts",
            "Timo Lüddecke",
            "Sowmen Das",
            "Kai Han",
            "Samuel Albanie"
        ],
        "published": "2023-05-30T18:28:04Z",
        "summary": "Large language models (LLMs) have shown remarkable capabilities across a\nbroad range of tasks involving question answering and the generation of\ncoherent text and code. Comprehensively understanding the strengths and\nweaknesses of LLMs is beneficial for safety, downstream applications and\nimproving performance. In this work, we investigate the degree to which GPT-4\nhas acquired factual geographic knowledge and is capable of using this\nknowledge for interpretative reasoning, which is especially important for\napplications that involve geographic data, such as geospatial analysis, supply\nchain management, and disaster response. To this end, we design and conduct a\nseries of diverse experiments, starting from factual tasks such as location,\ndistance and elevation estimation to more complex questions such as generating\ncountry outlines and travel networks, route finding under constraints and\nsupply chain analysis. We provide a broad characterisation of what GPT-4\n(without plugins or Internet access) knows about the world, highlighting both\npotentially surprising capabilities but also limitations.",
        "pdf_link": "https://arxiv.org/pdf/2306.00020v1.pdf"
    },
    {
        "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models",
        "authors": [
            "Hongxin Li",
            "Jingran Su",
            "Yuntao Chen",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "published": "2023-05-30T17:59:30Z",
        "summary": "Computer end users have spent billions of hours completing daily tasks like\ntabular data processing and project timeline scheduling. Most of these tasks\nare repetitive and error-prone, yet most end users lack the skill to automate\nthese burdensome works. With the advent of large language models (LLMs),\ndirecting software with natural language user requests become a reachable goal.\nIn this work, we propose a SheetCopilot agent that takes natural language task\nand control spreadsheet to fulfill the requirements. We propose a set of atomic\nactions as an abstraction of spreadsheet software functionalities. We further\ndesign a state machine-based task planning framework for LLMs to robustly\ninteract with spreadsheets. We curate a representative dataset containing 221\nspreadsheet control tasks and establish a fully automated evaluation pipeline\nfor rigorously benchmarking the ability of LLMs in software control tasks. Our\nSheetCopilot correctly completes 44.3\\% of tasks for a single generation,\noutperforming the strong code generation baseline by a wide margin. Our project\npage:https://sheetcopilot.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2305.19308v2.pdf"
    },
    {
        "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
        "authors": [
            "Bailin Wang",
            "Zi Wang",
            "Xuezhi Wang",
            "Yuan Cao",
            "Rif A. Saurous",
            "Yoon Kim"
        ],
        "published": "2023-05-30T17:26:01Z",
        "summary": "Large language models (LLMs) can learn to perform a wide range of natural\nlanguage tasks from just a handful of in-context examples. However, for\ngenerating strings from highly structured languages (e.g., semantic parsing to\ncomplex domain-specific languages), it is challenging for the LLM to generalize\nfrom just a few exemplars. We propose \\emph{grammar prompting}, a simple\napproach to enable LLMs to use external knowledge and domain-specific\nconstraints, expressed through a grammar in Backus--Naur Form (BNF), during\nin-context learning. Grammar prompting augments each demonstration example with\na specialized grammar that is minimally sufficient for generating the\nparticular output example, where the specialized grammar is a subset of the\nfull DSL grammar. For inference, the LLM first predicts a BNF grammar given a\ntest input, and then generates the output according to the rules of the\ngrammar. Experiments demonstrate that grammar prompting can enable LLMs to\nperform competitively on a diverse set of DSL generation tasks, including\nsemantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and\nSMILES-based molecule generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.19234v3.pdf"
    },
    {
        "title": "Controlled Text Generation with Hidden Representation Transformations",
        "authors": [
            "Vaibhav Kumar",
            "Hana Koorehdavoudi",
            "Masud Moshtaghi",
            "Amita Misra",
            "Ankit Chadha",
            "Emilio Ferrara"
        ],
        "published": "2023-05-30T17:21:17Z",
        "summary": "We propose CHRT (Control Hidden Representation Transformation) - a controlled\nlanguage generation framework that steers large language models to generate\ntext pertaining to certain attributes (such as toxicity). CHRT gains attribute\ncontrol by modifying the hidden representation of the base model through\nlearned transformations. We employ a contrastive-learning framework to learn\nthese transformations that can be combined to gain multi-attribute control. The\neffectiveness of CHRT is experimentally shown by comparing it with seven\nbaselines over three attributes. CHRT outperforms all the baselines in the task\nof detoxification, positive sentiment steering, and text simplification while\nminimizing the loss in linguistic qualities. Further, our approach has the\nlowest inference latency of only 0.01 seconds more than the base model, making\nit the most suitable for high-performance production environments. We\nopen-source our code and release two novel datasets to further propel\ncontrolled language generation research.",
        "pdf_link": "https://arxiv.org/pdf/2305.19230v2.pdf"
    },
    {
        "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
        "authors": [
            "Xiao Liu",
            "Da Yin",
            "Chen Zhang",
            "Yansong Feng",
            "Dongyan Zhao"
        ],
        "published": "2023-05-30T17:02:58Z",
        "summary": "Causal reasoning, the ability to identify cause-and-effect relationship, is\ncrucial in human thinking. Although large language models (LLMs) succeed in\nmany NLP tasks, it is still challenging for them to conduct complex causal\nreasoning like abductive reasoning and counterfactual reasoning. Given the fact\nthat programming code may express causal relations more often and explicitly\nwith conditional statements like ``if``, we want to explore whether Code-LLMs\nacquire better causal reasoning abilities. Our experiments show that compared\nto text-only LLMs, Code-LLMs with code prompts are significantly better in\ncausal reasoning. We further intervene on the prompts from different aspects,\nand discover that the programming structure is crucial in code prompt design,\nwhile Code-LLMs are robust towards format perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2305.19213v1.pdf"
    },
    {
        "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
        "authors": [
            "Zhen Lin",
            "Shubhendu Trivedi",
            "Jimeng Sun"
        ],
        "published": "2023-05-30T16:31:26Z",
        "summary": "Large language models (LLMs) specializing in natural language generation\n(NLG) have recently started exhibiting promising capabilities across a variety\nof domains. However, gauging the trustworthiness of responses generated by LLMs\nremains an open challenge, with limited research on uncertainty quantification\n(UQ) for NLG. Furthermore, existing literature typically assumes white-box\naccess to language models, which is becoming unrealistic either due to the\nclosed-source nature of the latest LLMs or computational constraints. In this\nwork, we investigate UQ in NLG for black-box LLMs. We first differentiate\nuncertainty vs confidence: the former refers to the \"dispersion\" of the\npotential predictions for a fixed input, and the latter refers to the\nconfidence on a particular prediction/generation. We then propose and compare\nseveral confidence/uncertainty metrics, applying them to selective NLG where\nunreliable results could either be ignored or yielded for further assessment.\nExperiments were carried out with several popular LLMs on question-answering\ndatasets (for evaluation purposes). Results reveal that a simple metric for the\nsemantic dispersion can be a reliable predictor of the quality of LLM\nresponses, providing valuable insights for practitioners on uncertainty\nmanagement when adopting LLMs. The code to replicate our experiments is\navailable at https://github.com/zlin7/UQ-NLG.",
        "pdf_link": "https://arxiv.org/pdf/2305.19187v2.pdf"
    },
    {
        "title": "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images",
        "authors": [
            "Viraj Prabhu",
            "Sriram Yenamandra",
            "Prithvijit Chattopadhyay",
            "Judy Hoffman"
        ],
        "published": "2023-05-30T16:09:16Z",
        "summary": "We propose an automated algorithm to stress-test a trained visual model by\ngenerating language-guided counterfactual test images (LANCE). Our method\nleverages recent progress in large language modeling and text-based image\nediting to augment an IID test set with a suite of diverse, realistic, and\nchallenging test images without altering model weights. We benchmark the\nperformance of a diverse set of pre-trained models on our generated data and\nobserve significant and consistent performance drops. We further analyze model\nsensitivity across different types of edits, and demonstrate its applicability\nat surfacing previously unknown class-level model biases in ImageNet. Code is\navailable at https://github.com/virajprabhu/lance.",
        "pdf_link": "https://arxiv.org/pdf/2305.19164v2.pdf"
    },
    {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi"
        ],
        "published": "2023-05-30T15:25:45Z",
        "summary": "Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n\"tit for tat\" and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of \"tit for tat\" state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Codes:\nhttps://github.com/Skytliang/Multi-Agents-Debate",
        "pdf_link": "https://arxiv.org/pdf/2305.19118v1.pdf"
    },
    {
        "title": "Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale",
        "authors": [
            "Walid S. Saba"
        ],
        "published": "2023-05-30T15:15:40Z",
        "summary": "Large language models (LLMs) have achieved a milestone that undenia-bly\nchanged many held beliefs in artificial intelligence (AI). However, there\nremains many limitations of these LLMs when it comes to true language\nunderstanding, limitations that are a byproduct of the under-lying architecture\nof deep neural networks. Moreover, and due to their subsymbolic nature,\nwhatever knowledge these models acquire about how language works will always be\nburied in billions of microfeatures (weights), none of which is meaningful on\nits own, making such models hopelessly unexplainable. To address these\nlimitations, we suggest com-bining the strength of symbolic representations\nwith what we believe to be the key to the success of LLMs, namely a successful\nbottom-up re-verse engineering of language at scale. As such we argue for a\nbottom-up reverse engineering of language in a symbolic setting. Hints on what\nthis project amounts to have been suggested by several authors, and we discuss\nin some detail here how this project could be accomplished.",
        "pdf_link": "https://arxiv.org/pdf/2306.00017v4.pdf"
    },
    {
        "title": "Does Conceptual Representation Require Embodiment? Insights From Large Language Models",
        "authors": [
            "Qihui Xu",
            "Yingying Peng",
            "Samuel A. Nastase",
            "Martin Chodorow",
            "Minghua Wu",
            "Ping Li"
        ],
        "published": "2023-05-30T15:06:28Z",
        "summary": "To what extent can language alone give rise to complex concepts, or is\nembodied experience essential? Recent advancements in large language models\n(LLMs) offer fresh perspectives on this question. Although LLMs are trained on\nrestricted modalities, they exhibit human-like performance in diverse\npsychological tasks. Our study compared representations of 4,442 lexical\nconcepts between humans and ChatGPTs (GPT-3.5 and GPT-4) across multiple\ndimensions, including five key domains: emotion, salience, mental\nvisualization, sensory, and motor experience. We identify two main findings: 1)\nBoth models strongly align with human representations in non-sensorimotor\ndomains but lag in sensory and motor areas, with GPT-4 outperforming GPT-3.5;\n2) GPT-4's gains are associated with its additional visual learning, which also\nappears to benefit related dimensions like haptics and imageability. These\nresults highlight the limitations of language in isolation, and that the\nintegration of diverse modalities of inputs leads to a more human-like\nconceptual representation.",
        "pdf_link": "https://arxiv.org/pdf/2305.19103v3.pdf"
    },
    {
        "title": "GPT Models in Construction Industry: Opportunities, Limitations, and a Use Case Validation",
        "authors": [
            "Abdullahi Saka",
            "Ridwan Taiwo",
            "Nurudeen Saka",
            "Babatunde Salami",
            "Saheed Ajayi",
            "Kabiru Akande",
            "Hadi Kazemi"
        ],
        "published": "2023-05-30T12:50:51Z",
        "summary": "Large Language Models(LLMs) trained on large data sets came into prominence\nin 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT\nmodels from OpenAI have been released. These models perform well on diverse\ntasks and have been gaining widespread applications in fields such as business\nand education. However, little is known about the opportunities and challenges\nof using LLMs in the construction industry. Thus, this study aims to assess GPT\nmodels in the construction industry. A critical review, expert discussion and\ncase study validation are employed to achieve the study objectives. The\nfindings revealed opportunities for GPT models throughout the project\nlifecycle. The challenges of leveraging GPT models are highlighted and a use\ncase prototype is developed for materials selection and optimization. The\nfindings of the study would be of benefit to researchers, practitioners and\nstakeholders, as it presents research vistas for LLMs in the construction\nindustry.",
        "pdf_link": "https://arxiv.org/pdf/2305.18997v1.pdf"
    },
    {
        "title": "Chatbots put to the test in math and logic problems: A preliminary comparison and assessment of ChatGPT-3.5, ChatGPT-4, and Google Bard",
        "authors": [
            "Vagelis Plevris",
            "George Papazafeiropoulos",
            "Alejandro Jiménez Rios"
        ],
        "published": "2023-05-30T11:18:05Z",
        "summary": "A comparison between three chatbots which are based on large language models,\nnamely ChatGPT-3.5, ChatGPT-4 and Google Bard is presented, focusing on their\nability to give correct answers to mathematics and logic problems. In\nparticular, we check their ability to Understand the problem at hand; Apply\nappropriate algorithms or methods for its solution; and Generate a coherent\nresponse and a correct answer. We use 30 questions that are clear, without any\nambiguities, fully described with plain text only, and have a unique, well\ndefined correct answer. The questions are divided into two sets of 15 each. The\nquestions of Set A are 15 \"Original\" problems that cannot be found online,\nwhile Set B contains 15 \"Published\" problems that one can find online, usually\nwith their solution. Each question is posed three times to each chatbot. The\nanswers are recorded and discussed, highlighting their strengths and\nweaknesses. It has been found that for straightforward arithmetic, algebraic\nexpressions, or basic logic puzzles, chatbots may provide accurate solutions,\nalthough not in every attempt. However, for more complex mathematical problems\nor advanced logic tasks, their answers, although written in a usually\n\"convincing\" way, may not be reliable. Consistency is also an issue, as many\ntimes a chatbot will provide conflicting answers when given the same question\nmore than once. A comparative quantitative evaluation of the three chatbots is\nmade through scoring their final answers based on correctness. It was found\nthat ChatGPT-4 outperforms ChatGPT-3.5 in both sets of questions. Bard comes\nthird in the original questions of Set A, behind the other two chatbots, while\nit has the best performance (first place) in the published questions of Set B.\nThis is probably because Bard has direct access to the internet, in contrast to\nChatGPT chatbots which do not have any communication with the outside world.",
        "pdf_link": "https://arxiv.org/pdf/2305.18618v1.pdf"
    },
    {
        "title": "Multitask learning for recognizing stress and depression in social media",
        "authors": [
            "Loukas Ilias",
            "Dimitris Askounis"
        ],
        "published": "2023-05-30T10:04:01Z",
        "summary": "Stress and depression are prevalent nowadays across people of all ages due to\nthe quick paces of life. People use social media to express their feelings.\nThus, social media constitute a valuable form of information for the early\ndetection of stress and depression. Although many research works have been\nintroduced targeting the early recognition of stress and depression, there are\nstill limitations. There have been proposed multi-task learning settings, which\nuse depression and emotion (or figurative language) as the primary and\nauxiliary tasks respectively. However, although stress is inextricably linked\nwith depression, researchers face these two tasks as two separate tasks. To\naddress these limitations, we present the first study, which exploits two\ndifferent datasets collected under different conditions, and introduce two\nmultitask learning frameworks, which use depression and stress as the main and\nauxiliary tasks respectively. Specifically, we use a depression dataset and a\nstressful dataset including stressful posts from ten subreddits of five\ndomains. In terms of the first approach, each post passes through a shared BERT\nlayer, which is updated by both tasks. Next, two separate BERT encoder layers\nare exploited, which are updated by each task separately. Regarding the second\napproach, it consists of shared and task-specific layers weighted by attention\nfusion networks. We conduct a series of experiments and compare our approaches\nwith existing research initiatives, single-task learning, and transfer\nlearning. Experiments show multiple advantages of our approaches over\nstate-of-the-art ones.",
        "pdf_link": "https://arxiv.org/pdf/2305.18907v2.pdf"
    },
    {
        "title": "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation",
        "authors": [
            "Chuhao Jin",
            "Wenhui Tan",
            "Jiange Yang",
            "Bei Liu",
            "Ruihua Song",
            "Limin Wang",
            "Jianlong Fu"
        ],
        "published": "2023-05-30T09:54:20Z",
        "summary": "We propose a novel framework for learning high-level cognitive capabilities\nin robot manipulation tasks, such as making a smiley face using building\nblocks. These tasks often involve complex multi-step reasoning, presenting\nsignificant challenges due to the limited paired data connecting human\ninstructions (e.g., making a smiley face) and robot actions (e.g., end-effector\nmovement). Existing approaches relieve this challenge by adopting an open-loop\nparadigm decomposing high-level instructions into simple sub-task plans, and\nexecuting them step-by-step using low-level control models. However, these\napproaches are short of instant observations in multi-step reasoning, leading\nto sub-optimal results. To address this issue, we propose to automatically\ncollect a cognitive robot dataset by Large Language Models (LLMs). The\nresulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of\nmulti-step text plans and paired observation sequences. To enable efficient\ndata acquisition, we employ elaborated multi-round prompt designs that\neffectively reduce the burden of extensive human involvement. We further\npropose a closed-loop multi-modal embodied planning model that autoregressively\ngenerates plans by taking image observations as input. To facilitate effective\nlearning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and\nfinetune additional vision adapter and Q-former to enable fine-grained spatial\nperception for manipulation tasks. We conduct experiments to verify the\nsuperiority over existing open and closed-loop methods, and achieve a\nsignificant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4\nbased robot tasks. Real-world demos are shown in\nhttps://www.youtube.com/watch?v=ayAzID1_qQk .",
        "pdf_link": "https://arxiv.org/pdf/2305.18898v1.pdf"
    },
    {
        "title": "Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge",
        "authors": [
            "Xingyu Fu",
            "Sheng Zhang",
            "Gukyeong Kwon",
            "Pramuditha Perera",
            "Henghui Zhu",
            "Yuhao Zhang",
            "Alexander Hanbo Li",
            "William Yang Wang",
            "Zhiguo Wang",
            "Vittorio Castelli",
            "Patrick Ng",
            "Dan Roth",
            "Bing Xiang"
        ],
        "published": "2023-05-30T08:34:13Z",
        "summary": "The open-ended Visual Question Answering (VQA) task requires AI models to\njointly reason over visual and natural language inputs using world knowledge.\nRecently, pre-trained Language Models (PLM) such as GPT-3 have been applied to\nthe task and shown to be powerful world knowledge sources. However, these\nmethods suffer from low knowledge coverage caused by PLM bias -- the tendency\nto generate certain tokens over other tokens regardless of prompt changes, and\nhigh dependency on the PLM quality -- only models using GPT-3 can achieve the\nbest result.\n  To address the aforementioned challenges, we propose RASO: a new VQA pipeline\nthat deploys a generate-then-select strategy guided by world knowledge for the\nfirst time. Rather than following the de facto standard to train a multi-modal\nmodel that directly generates the VQA answer, RASO first adopts PLM to generate\nall the possible answers, and then trains a lightweight answer selection model\nfor the correct answer. As proved in our analysis, RASO expands the knowledge\ncoverage from in-domain training data by a large margin. We provide extensive\nexperimentation and show the effectiveness of our pipeline by advancing the\nstate-of-the-art by 4.1% on OK-VQA, without additional computation cost. Code\nand models are released at http://cogcomp.org/page/publication_view/1010",
        "pdf_link": "https://arxiv.org/pdf/2305.18842v1.pdf"
    },
    {
        "title": "Universality and Limitations of Prompt Tuning",
        "authors": [
            "Yihan Wang",
            "Jatin Chauhan",
            "Wei Wang",
            "Cho-Jui Hsieh"
        ],
        "published": "2023-05-30T06:47:07Z",
        "summary": "Despite the demonstrated empirical efficacy of prompt tuning to adapt a\npretrained language model for a new task, the theoretical underpinnings of the\ndifference between \"tuning parameters before the input\" against \"the tuning of\nmodel weights\" are limited. We thus take one of the first steps to understand\nthe role of soft-prompt tuning for transformer-based architectures. By\nconsidering a general purpose architecture, we analyze prompt tuning from the\nlens of both: universal approximation and limitations with finite-depth\nfixed-weight pretrained transformers for continuous-valued functions. Our\nuniversality result guarantees the existence of a strong transformer with a\nprompt to approximate any sequence-to-sequence function in the set of Lipschitz\nfunctions. The limitations of prompt tuning for limited-depth transformers are\nfirst proved by constructing a set of datasets, that cannot be memorized by a\nprompt of any length for a given single encoder layer. We also provide a lower\nbound on the required number of tunable prompt parameters and compare the\nresult with the number of parameters required for a low-rank update (based on\nLoRA) for a single-layer setting. We finally extend our analysis to multi-layer\nsettings by providing sufficient conditions under which the transformer can at\nbest learn datasets from invertible functions only. Our theoretical claims are\nalso corroborated by empirical results.",
        "pdf_link": "https://arxiv.org/pdf/2305.18787v2.pdf"
    },
    {
        "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
        "authors": [
            "Rui Yang",
            "Lin Song",
            "Yanwei Li",
            "Sijie Zhao",
            "Yixiao Ge",
            "Xiu Li",
            "Ying Shan"
        ],
        "published": "2023-05-30T05:27:21Z",
        "summary": "This paper aims to efficiently enable Large Language Models (LLMs) to use\nmultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have\nshown great potential for tool usage through sophisticated prompt engineering.\nNevertheless, these models typically rely on prohibitive computational costs\nand publicly inaccessible data. To address these challenges, we propose the\nGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and\nOPT, to use tools. It generates an instruction-following dataset by prompting\nan advanced teacher with various multi-modal contexts. By using the Low-Rank\nAdaptation (LoRA) optimization, our approach facilitates the open-source LLMs\nto solve a range of visual problems, including visual comprehension and image\ngeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs to\nuse tools, which is performed in both zero-shot and fine-tuning ways. Extensive\nexperiments demonstrate the effectiveness of our method on various language\nmodels, which not only significantly improves the accuracy of invoking seen\ntools, but also enables the zero-shot capacity for unseen tools. The code and\ndemo are available at https://github.com/StevenGrove/GPT4Tools.",
        "pdf_link": "https://arxiv.org/pdf/2305.18752v1.pdf"
    },
    {
        "title": "AdapterEM: Pre-trained Language Model Adaptation for Generalized Entity Matching using Adapter-tuning",
        "authors": [
            "John Bosco Mugeni",
            "Steven Lynden",
            "Toshiyuki Amagasa",
            "Akiyoshi Matono"
        ],
        "published": "2023-05-30T04:03:23Z",
        "summary": "Entity Matching (EM) involves identifying different data representations\nreferring to the same entity from multiple data sources and is typically\nformulated as a binary classification problem. It is a challenging problem in\ndata integration due to the heterogeneity of data representations.\nState-of-the-art solutions have adopted NLP techniques based on pre-trained\nlanguage models (PrLMs) via the fine-tuning paradigm, however, sequential\nfine-tuning of overparameterized PrLMs can lead to catastrophic forgetting,\nespecially in low-resource scenarios. In this study, we propose a\nparameter-efficient paradigm for fine-tuning PrLMs based on adapters, small\nneural networks encapsulated between layers of a PrLM, by optimizing only the\nadapter and classifier weights while the PrLMs parameters are frozen.\nAdapter-based methods have been successfully applied to multilingual speech\nproblems achieving promising results, however, the effectiveness of these\nmethods when applied to EM is not yet well understood, particularly for\ngeneralized EM with heterogeneous data. Furthermore, we explore using (i)\npre-trained adapters and (ii) invertible adapters to capture token-level\nlanguage representations and demonstrate their benefits for transfer learning\non the generalized EM benchmark. Our results show that our solution achieves\ncomparable or superior performance to full-scale PrLM fine-tuning and\nprompt-tuning baselines while utilizing a significantly smaller computational\nfootprint $\\approx 13\\%$ of the PrLM parameters.",
        "pdf_link": "https://arxiv.org/pdf/2305.18725v1.pdf"
    },
    {
        "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
        "authors": [
            "Chen Ling",
            "Xujiang Zhao",
            "Jiaying Lu",
            "Chengyuan Deng",
            "Can Zheng",
            "Junxiang Wang",
            "Tanmoy Chowdhury",
            "Yun Li",
            "Hejie Cui",
            "Xuchao Zhang",
            "Tianjiao Zhao",
            "Amit Panalkar",
            "Dhagash Mehta",
            "Stefano Pasquali",
            "Wei Cheng",
            "Haoyu Wang",
            "Yanchi Liu",
            "Zhengzhang Chen",
            "Haifeng Chen",
            "Chris White",
            "Quanquan Gu",
            "Jian Pei",
            "Carl Yang",
            "Liang Zhao"
        ],
        "published": "2023-05-30T03:00:30Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.",
        "pdf_link": "https://arxiv.org/pdf/2305.18703v7.pdf"
    },
    {
        "title": "Faith and Fate: Limits of Transformers on Compositionality",
        "authors": [
            "Nouha Dziri",
            "Ximing Lu",
            "Melanie Sclar",
            "Xiang Lorraine Li",
            "Liwei Jiang",
            "Bill Yuchen Lin",
            "Peter West",
            "Chandra Bhagavatula",
            "Ronan Le Bras",
            "Jena D. Hwang",
            "Soumya Sanyal",
            "Sean Welleck",
            "Xiang Ren",
            "Allyson Ettinger",
            "Zaid Harchaoui",
            "Yejin Choi"
        ],
        "published": "2023-05-29T23:24:14Z",
        "summary": "Transformer large language models (LLMs) have sparked admiration for their\nexceptional performance on tasks that demand intricate multi-step reasoning.\nYet, these models simultaneously show failures on surprisingly trivial\nproblems. This begs the question: Are these errors incidental, or do they\nsignal more substantial limitations? In an attempt to demystify transformer\nLLMs, we investigate the limits of these models across three representative\ncompositional tasks -- multi-digit multiplication, logic grid puzzles, and a\nclassic dynamic programming problem. These tasks require breaking problems down\ninto sub-steps and synthesizing these steps into a precise answer. We formulate\ncompositional tasks as computation graphs to systematically quantify the level\nof complexity, and break down reasoning steps into intermediate sub-procedures.\nOur empirical findings suggest that transformer LLMs solve compositional tasks\nby reducing multi-step compositional reasoning into linearized subgraph\nmatching, without necessarily developing systematic problem-solving skills. To\nround off our empirical study, we provide theoretical arguments on abstract\nmulti-step reasoning problems that highlight how autoregressive generations'\nperformance can rapidly decay with\\,increased\\,task\\,complexity.",
        "pdf_link": "https://arxiv.org/pdf/2305.18654v3.pdf"
    },
    {
        "title": "How Effective Are Neural Networks for Fixing Security Vulnerabilities",
        "authors": [
            "Yi Wu",
            "Nan Jiang",
            "Hung Viet Pham",
            "Thibaud Lutellier",
            "Jordan Davis",
            "Lin Tan",
            "Petr Babkin",
            "Sameena Shah"
        ],
        "published": "2023-05-29T20:50:27Z",
        "summary": "Security vulnerability repair is a difficult task that is in dire need of\nautomation. Two groups of techniques have shown promise: (1) large code\nlanguage models (LLMs) that have been pre-trained on source code for tasks such\nas code completion, and (2) automated program repair (APR) techniques that use\ndeep learning (DL) models to automatically fix software bugs.\n  This paper is the first to study and compare Java vulnerability repair\ncapabilities of LLMs and DL-based APR models. The contributions include that we\n(1) apply and evaluate five LLMs (Codex, CodeGen, CodeT5, PLBART and InCoder),\nfour fine-tuned LLMs, and four DL-based APR techniques on two real-world Java\nvulnerability benchmarks (Vul4J and VJBench), (2) design code transformations\nto address the training and test data overlapping threat to Codex, (3) create a\nnew Java vulnerability repair benchmark VJBench, and its transformed version\nVJBench-trans and (4) evaluate LLMs and APR techniques on the transformed\nvulnerabilities in VJBench-trans.\n  Our findings include that (1) existing LLMs and APR models fix very few Java\nvulnerabilities. Codex fixes 10.2 (20.4%), the most number of vulnerabilities.\n(2) Fine-tuning with general APR data improves LLMs' vulnerability-fixing\ncapabilities. (3) Our new VJBench reveals that LLMs and APR models fail to fix\nmany Common Weakness Enumeration (CWE) types, such as CWE-325 Missing\ncryptographic step and CWE-444 HTTP request smuggling. (4) Codex still fixes\n8.3 transformed vulnerabilities, outperforming all the other LLMs and APR\nmodels on transformed vulnerabilities. The results call for innovations to\nenhance automated Java vulnerability repair such as creating larger\nvulnerability repair training data, tuning LLMs with such data, and applying\ncode simplification transformation to facilitate vulnerability repair.",
        "pdf_link": "https://arxiv.org/pdf/2305.18607v2.pdf"
    },
    {
        "title": "Controllable Text-to-Image Generation with GPT-4",
        "authors": [
            "Tianjun Zhang",
            "Yi Zhang",
            "Vibhav Vineet",
            "Neel Joshi",
            "Xin Wang"
        ],
        "published": "2023-05-29T19:56:47Z",
        "summary": "Current text-to-image generation models often struggle to follow textual\ninstructions, especially the ones requiring spatial reasoning. On the other\nhand, Large Language Models (LLMs), such as GPT-4, have shown remarkable\nprecision in generating code snippets for sketching out text inputs\ngraphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide\nthe diffusion-based text-to-image pipelines with programmatic sketches\ngenerated by GPT-4, enhancing their abilities for instruction following.\nControl-GPT works by querying GPT-4 to write TikZ code, and the generated\nsketches are used as references alongside the text instructions for diffusion\nmodels (e.g., ControlNet) to generate photo-realistic images. One major\nchallenge to training our pipeline is the lack of a dataset containing aligned\ntext, images, and sketches. We address the issue by converting instance masks\nin existing datasets into polygons to mimic the sketches used at test time. As\na result, Control-GPT greatly boosts the controllability of image generation.\nIt establishes a new state-of-art on the spatial arrangement and object\npositioning generation and enhances users' control of object positions, sizes,\netc., nearly doubling the accuracy of prior models. Our work, as a first\nattempt, shows the potential for employing LLMs to enhance the performance in\ncomputer vision tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.18583v1.pdf"
    },
    {
        "title": "Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy",
        "authors": [
            "Pengfei Yu",
            "Heng Ji"
        ],
        "published": "2023-05-29T19:48:37Z",
        "summary": "Large Language Models~(LLMs) struggle with providing current information due\nto the outdated pre-training data. Existing methods for updating LLMs, such as\nknowledge editing and continual fine-tuning, have significant drawbacks in\ngeneralizability of new information and the requirements on structured updating\ncorpus. We identify the core challenge behind these drawbacks: the LM-logical\ndiscrepancy featuring the difference between language modeling probabilities\nand logical probabilities. To evaluate and address the core challenge, we\npropose a new task formulation of the information updating task that only\nrequires the provision of an unstructured updating corpus and evaluates the\nperformance of information updating on the generalizability to question-answer\npairs pertaining to the updating information. We further propose a novel and\neffective pipeline approach for the task, highlighting a self-prompting-based\nquestion-answer generation process and a associative distillation methods to\nbridge the LM-logical discrepancy. We develop two datasets for evaluation, one\nsourced from news articles published in March and April 2023, and the other\nfrom the Natural Questions benchmark. Experimental results demonstrate the\nsuperiority of our approach, significantly increasing the factual consistency\nscore (on a scale from 0 to 1) by up to 0.16. Furthermore, our method\neffectively mitigates forgetting utilizing a compact replay buffer with only\n2.3% of the training tokens.",
        "pdf_link": "https://arxiv.org/pdf/2305.18582v2.pdf"
    },
    {
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
        "authors": [
            "Rafael Rafailov",
            "Archit Sharma",
            "Eric Mitchell",
            "Stefano Ermon",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2023-05-29T17:57:46Z",
        "summary": "While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.",
        "pdf_link": "https://arxiv.org/pdf/2305.18290v2.pdf"
    },
    {
        "title": "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections",
        "authors": [
            "M. Jehanzeb Mirza",
            "Leonid Karlinsky",
            "Wei Lin",
            "Mateusz Kozinski",
            "Horst Possegger",
            "Rogerio Feris",
            "Horst Bischof"
        ],
        "published": "2023-05-29T17:56:35Z",
        "summary": "Recently, large-scale pre-trained Vision and Language (VL) models have set a\nnew state-of-the-art (SOTA) in zero-shot visual classification enabling\nopen-vocabulary recognition of potentially unlimited set of categories defined\nas simple language prompts. However, despite these great advances, the\nperformance of these zeroshot classifiers still falls short of the results of\ndedicated (closed category set) classifiers trained with supervised fine\ntuning. In this paper we show, for the first time, how to reduce this gap\nwithout any labels and without any paired VL data, using an unlabeled image\ncollection and a set of texts auto-generated using a Large Language Model (LLM)\ndescribing the categories of interest and effectively substituting labeled\nvisual instances of those categories. Using our label-free approach, we are\nable to attain significant performance improvements over the zero-shot\nperformance of the base VL model and other contemporary methods and baselines\non a wide variety of datasets, demonstrating absolute improvement of up to\n11.7% (3.8% on average) in the label-free setting. Moreover, despite our\napproach being label-free, we observe 1.3% average gains over leading few-shot\nprompting baselines that do use 5-shot supervision.",
        "pdf_link": "https://arxiv.org/pdf/2305.18287v2.pdf"
    },
    {
        "title": "Contextual Object Detection with Multimodal Large Language Models",
        "authors": [
            "Yuhang Zang",
            "Wei Li",
            "Jun Han",
            "Kaiyang Zhou",
            "Chen Change Loy"
        ],
        "published": "2023-05-29T17:50:33Z",
        "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
        "pdf_link": "https://arxiv.org/pdf/2305.18279v1.pdf"
    },
    {
        "title": "Beyond Confidence: Reliable Models Should Also Consider Atypicality",
        "authors": [
            "Mert Yuksekgonul",
            "Linjun Zhang",
            "James Zou",
            "Carlos Guestrin"
        ],
        "published": "2023-05-29T17:37:09Z",
        "summary": "While most machine learning models can provide confidence in their\npredictions, confidence is insufficient to understand a prediction's\nreliability. For instance, the model may have a low confidence prediction if\nthe input is not well-represented in the training dataset or if the input is\ninherently ambiguous. In this work, we investigate the relationship between how\natypical(rare) a sample or a class is and the reliability of a model's\npredictions. We first demonstrate that atypicality is strongly related to\nmiscalibration and accuracy. In particular, we empirically show that\npredictions for atypical inputs or atypical classes are more overconfident and\nhave lower accuracy. Using these insights, we show incorporating atypicality\nimproves uncertainty quantification and model performance for discriminative\nneural networks and large language models. In a case study, we show that using\natypicality improves the performance of a skin lesion classifier across\ndifferent skin tone groups without having access to the group attributes.\nOverall, we propose that models should use not only confidence but also\natypicality to improve uncertainty quantification and performance. Our results\ndemonstrate that simple post-hoc atypicality estimators can provide significant\nvalue.",
        "pdf_link": "https://arxiv.org/pdf/2305.18262v2.pdf"
    },
    {
        "title": "Do Language Models Know When They're Hallucinating References?",
        "authors": [
            "Ayush Agrawal",
            "Mirac Suzgun",
            "Lester Mackey",
            "Adam Tauman Kalai"
        ],
        "published": "2023-05-29T17:12:03Z",
        "summary": "State-of-the-art language models (LMs) are notoriously susceptible to\ngenerating hallucinated information. Such inaccurate outputs not only undermine\nthe reliability of these models but also limit their use and raise serious\nconcerns about misinformation and propaganda. In this work, we focus on\nhallucinated book and article references and present them as the \"model\norganism\" of language model hallucination research, due to their frequent and\neasy-to-discern nature. We posit that if a language model cites a particular\nreference in its output, then it should ideally possess sufficient information\nabout its authors and content, among other relevant details. Using this basic\ninsight, we illustrate that one can identify hallucinated references without\never consulting any external resources, by asking a set of direct or indirect\nqueries to the language model about the references. These queries can be\nconsidered as \"consistency checks.\" Our findings highlight that while LMs,\nincluding GPT-4, often produce inconsistent author lists for hallucinated\nreferences, they also often accurately recall the authors of real references.\nIn this sense, the LM can be said to \"know\" when it is hallucinating\nreferences. Furthermore, these findings show how hallucinated references can be\ndissected to shed light on their nature. Replication code and results can be\nfound at https://github.com/microsoft/hallucinated-references.",
        "pdf_link": "https://arxiv.org/pdf/2305.18248v3.pdf"
    },
    {
        "title": "Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models",
        "authors": [
            "Myra Cheng",
            "Esin Durmus",
            "Dan Jurafsky"
        ],
        "published": "2023-05-29T16:29:22Z",
        "summary": "To recognize and mitigate harms from large language models (LLMs), we need to\nunderstand the prevalence and nuances of stereotypes in LLM outputs. Toward\nthis end, we present Marked Personas, a prompt-based method to measure\nstereotypes in LLMs for intersectional demographic groups without any lexicon\nor data labeling. Grounded in the sociolinguistic concept of markedness (which\ncharacterizes explicitly linguistically marked categories versus unmarked\ndefaults), our proposed method is twofold: 1) prompting an LLM to generate\npersonas, i.e., natural language descriptions, of the target demographic group\nalongside personas of unmarked, default groups; 2) identifying the words that\nsignificantly distinguish personas of the target group from corresponding\nunmarked ones. We find that the portrayals generated by GPT-3.5 and GPT-4\ncontain higher rates of racial stereotypes than human-written portrayals using\nthe same prompts. The words distinguishing personas of marked (non-white,\nnon-male) groups reflect patterns of othering and exoticizing these\ndemographics. An intersectional lens further reveals tropes that dominate\nportrayals of marginalized groups, such as tropicalism and the\nhypersexualization of minoritized women. These representational harms have\nconcerning implications for downstream applications like story generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.18189v1.pdf"
    },
    {
        "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning",
        "authors": [
            "Zhanming Jie",
            "Wei Lu"
        ],
        "published": "2023-05-29T16:01:40Z",
        "summary": "Chain-of-thought (CoT) prompting with large language models has proven\neffective in numerous natural language processing tasks, but designing prompts\nthat generalize well to diverse problem types can be challenging, especially in\nthe context of math word problem (MWP) solving. Additionally, it is common to\nhave a large amount of training data that have a better diversity coverage but\nCoT annotations are not available, which limits the use of supervised learning\ntechniques. To address these issues, we investigate two approaches to leverage\nthe training data in a few-shot prompting scenario: dynamic program prompting\nand program distillation. Our approach is largely inspired by Gao et al.,\n(2022), where they proposed to replace the CoT with the programs as the\nintermediate reasoning step. Such a prompting strategy allows us to accurately\nverify the answer correctness through program execution in MWP solving. Our\ndynamic program prompting involves annotating the training data by sampling\ncorrect programs from a large language model, while program distillation\ninvolves adapting a smaller model to the program-annotated training data. Our\nexperiments on three standard MWP datasets demonstrate the effectiveness of\nthese approaches, yielding significant improvements over previous baselines for\nprompting and fine-tuning. Our results suggest that leveraging a large amount\nof training data can improve the generalization ability of prompts and boost\nthe performance of fine-tuned small models in MWP solving.",
        "pdf_link": "https://arxiv.org/pdf/2305.18170v2.pdf"
    },
    {
        "title": "LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning",
        "authors": [
            "Amirhossein Abaskohi",
            "Sascha Rothe",
            "Yadollah Yaghoobzadeh"
        ],
        "published": "2023-05-29T15:59:51Z",
        "summary": "In recent years, there has been significant progress in developing\npre-trained language models for NLP. However, these models often struggle when\nfine-tuned on small datasets. To address this issue, researchers have proposed\nvarious adaptation approaches. Prompt-based tuning is arguably the most common\nway, especially for larger models. Previous research shows that adding\ncontrastive learning to prompt-based fine-tuning is effective as it helps the\nmodel generate embeddings that are more distinguishable between classes, and it\ncan also be more sample-efficient as the model learns from positive and\nnegative examples simultaneously. One of the most important components of\ncontrastive learning is data augmentation, but unlike computer vision,\neffective data augmentation for NLP is still challenging. This paper proposes\nLM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language\nModels, which leverages prompt-based few-shot paraphrasing using generative\nlanguage models, especially large language models such as GPT-3 and OPT-175B,\nfor data augmentation. Our experiments on multiple text classification\nbenchmarks show that this augmentation method outperforms other methods, such\nas easy data augmentation, back translation, and multiple templates.",
        "pdf_link": "https://arxiv.org/pdf/2305.18169v3.pdf"
    },
    {
        "title": "Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods",
        "authors": [
            "Mengsay Loem",
            "Masahiro Kaneko",
            "Sho Takase",
            "Naoaki Okazaki"
        ],
        "published": "2023-05-29T15:31:29Z",
        "summary": "Large-scale pre-trained language models such as GPT-3 have shown remarkable\nperformance across various natural language processing tasks. However, applying\nprompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks\nand their controllability remains underexplored. Controllability in GEC is\ncrucial for real-world applications, particularly in educational settings,\nwhere the ability to tailor feedback according to learner levels and specific\nerror types can significantly enhance the learning process. This paper\ninvestigates the performance and controllability of prompt-based methods with\nGPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impact\nof task instructions and examples on GPT-3's output, focusing on controlling\naspects such as minimal edits, fluency edits, and learner levels. Our findings\ndemonstrate that GPT-3 could effectively perform GEC tasks, outperforming\nexisting supervised and unsupervised approaches. We also showed that GPT-3\ncould achieve controllability when appropriate task instructions and examples\nare given.",
        "pdf_link": "https://arxiv.org/pdf/2305.18156v1.pdf"
    },
    {
        "title": "Do Large Language Models Know What They Don't Know?",
        "authors": [
            "Zhangyue Yin",
            "Qiushi Sun",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Xuanjing Huang"
        ],
        "published": "2023-05-29T15:30:13Z",
        "summary": "Large language models (LLMs) have a wealth of knowledge that allows them to\nexcel in various Natural Language Processing (NLP) tasks. Current research\nfocuses on enhancing their performance within their existing knowledge. Despite\ntheir vast knowledge, LLMs are still limited by the amount of information they\ncan accommodate and comprehend. Therefore, the ability to understand their own\nlimitations on the unknows, referred to as self-knowledge, is of paramount\nimportance. This study aims to evaluate LLMs' self-knowledge by assessing their\nability to identify unanswerable or unknowable questions. We introduce an\nautomated methodology to detect uncertainty in the responses of these models,\nproviding a novel measure of their self-knowledge. We further introduce a\nunique dataset, SelfAware, consisting of unanswerable questions from five\ndiverse categories and their answerable counterparts. Our extensive analysis,\ninvolving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an\nintrinsic capacity for self-knowledge within these models. Moreover, we\ndemonstrate that in-context learning and instruction tuning can further enhance\nthis self-knowledge. Despite this promising insight, our findings also\nhighlight a considerable gap between the capabilities of these models and human\nproficiency in recognizing the limits of their knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2305.18153v2.pdf"
    },
    {
        "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
        "authors": [
            "Yuchuan Tian",
            "Hanting Chen",
            "Xutao Wang",
            "Zheyuan Bai",
            "Qinghua Zhang",
            "Ruifeng Li",
            "Chao Xu",
            "Yunhe Wang"
        ],
        "published": "2023-05-29T15:25:00Z",
        "summary": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are\nastonishing at generating human-like texts, but they may impact the\nauthenticity of texts. Previous works proposed methods to detect these\nAI-generated texts, including simple ML classifiers, pretrained-model-based\nzero-shot methods, and finetuned language classification models. However,\nmainstream detectors always fail on short texts, like SMSes, Tweets, and\nreviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training\nframework is proposed to address the difficulty of short-text detection without\nsacrificing long-texts. Firstly, we acknowledge the human-resemblance property\nof short machine texts, and rephrase AI text detection as a partial\nPositive-Unlabeled (PU) problem by regarding these short machine texts as\npartially ``unlabeled\". Then in this PU context, we propose the\nlength-sensitive Multiscale PU Loss, where a recurrent model in abstraction is\nused to estimate positive priors of scale-variant corpora. Additionally, we\nintroduce a Text Multiscaling module to enrich training corpora. Experiments\nshow that our MPU method augments detection performance on long AI-generated\ntexts, and significantly improves short-text detection of language model\ndetectors. Language Models trained with MPU could outcompete existing detectors\non various short-text and long-text detection benchmarks. The codes are\navailable at\nhttps://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt\nand https://github.com/YuchuanTian/AIGC_text_detector.",
        "pdf_link": "https://arxiv.org/pdf/2305.18149v4.pdf"
    },
    {
        "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
        "authors": [
            "Yi Hu",
            "Haotong Yang",
            "Zhouchen Lin",
            "Muhan Zhang"
        ],
        "published": "2023-05-29T15:14:09Z",
        "summary": "Large language models (LLMs) have scaled up to unlock a wide range of complex\nreasoning tasks with the aid of various prompting methods. However, current\nprompting methods generate natural language intermediate steps to help\nreasoning, which can cause imperfect task reduction and confusion. To mitigate\nsuch limitations, we explore code prompting, a neural symbolic prompting method\nwith both zero-shot and few-shot versions which triggers code as intermediate\nsteps. We conduct experiments on 7 widely-used benchmarks involving symbolic\nreasoning and arithmetic reasoning. Code prompting generally outperforms\nchain-of-thought (CoT) prompting. To further understand the performance and\nlimitations of code prompting, we perform extensive ablation studies and error\nanalyses, and identify several exclusive advantages of using symbolic\npromptings compared to natural language. We also consider the ensemble of code\nprompting and CoT prompting to combine the strengths of both. Finally, we show\nthrough experiments how code annotations and their locations affect code\nprompting.",
        "pdf_link": "https://arxiv.org/pdf/2305.18507v2.pdf"
    },
    {
        "title": "ChatGPT-powered Conversational Drug Editing Using Retrieval and Domain Feedback",
        "authors": [
            "Shengchao Liu",
            "Jiongxiao Wang",
            "Yijin Yang",
            "Chengpeng Wang",
            "Ling Liu",
            "Hongyu Guo",
            "Chaowei Xiao"
        ],
        "published": "2023-05-29T14:43:24Z",
        "summary": "Recent advancements in conversational large language models (LLMs), such as\nChatGPT, have demonstrated remarkable promise in various domains, including\ndrug discovery. However, existing works mainly focus on investigating the\ncapabilities of conversational LLMs on chemical reaction and retrosynthesis.\nWhile drug editing, a critical task in the drug discovery pipeline, remains\nlargely unexplored. To bridge this gap, we propose ChatDrug, a framework to\nfacilitate the systematic investigation of drug editing using LLMs. ChatDrug\njointly leverages a prompt module, a retrieval and domain feedback (ReDF)\nmodule, and a conversation module to streamline effective drug editing. We\nempirically show that ChatDrug reaches the best performance on 33 out of 39\ndrug editing tasks, encompassing small molecules, peptides, and proteins. We\nfurther demonstrate, through 10 case studies, that ChatDrug can successfully\nidentify the key substructures (e.g., the molecule functional groups, peptide\nmotifs, and protein structures) for manipulation, generating diverse and valid\nsuggestions for drug editing. Promisingly, we also show that ChatDrug can offer\ninsightful explanations from a domain-specific perspective, enhancing\ninterpretability and enabling informed decision-making. This research sheds\nlight on the potential of ChatGPT and conversational LLMs for drug editing. It\npaves the way for a more efficient and collaborative drug discovery pipeline,\ncontributing to the advancement of pharmaceutical research and development.",
        "pdf_link": "https://arxiv.org/pdf/2305.18090v1.pdf"
    },
    {
        "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
        "authors": [
            "Sihan Chen",
            "Handong Li",
            "Qunbo Wang",
            "Zijia Zhao",
            "Mingzhen Sun",
            "Xinxin Zhu",
            "Jing Liu"
        ],
        "published": "2023-05-29T14:34:50Z",
        "summary": "Vision and text have been fully explored in contemporary video-text\nfoundational models, while other modalities such as audio and subtitles in\nvideos have not received sufficient attention. In this paper, we resort to\nestablish connections between multi-modality video tracks, including Vision,\nAudio, and Subtitle, and Text by exploring an automatically generated\nlarge-scale omni-modality video caption dataset called VAST-27M. Specifically,\nwe first collect 27 million open-domain video clips and separately train a\nvision and an audio captioner to generate vision and audio captions. Then, we\nemploy an off-the-shelf Large Language Model (LLM) to integrate the generated\ncaptions, together with subtitles and instructional prompts into omni-modality\ncaptions. Based on the proposed VAST-27M dataset, we train an omni-modality\nvideo-text foundational model named VAST, which can perceive and process\nvision, audio, and subtitle modalities from video, and better support various\ntasks including vision-text, audio-text, and multi-modal video-text tasks\n(retrieval, captioning and QA). Extensive experiments have been conducted to\ndemonstrate the effectiveness of our proposed VAST-27M corpus and VAST\nfoundation model. VAST achieves 22 new state-of-the-art results on various\ncross-modality benchmarks. Code, model and dataset will be released at\nhttps://github.com/TXH-mercury/VAST.",
        "pdf_link": "https://arxiv.org/pdf/2305.18500v2.pdf"
    },
    {
        "title": "ANPL: Towards Natural Programming with Interactive Decomposition",
        "authors": [
            "Di Huang",
            "Ziyuan Nan",
            "Xing Hu",
            "Pengwei Jin",
            "Shaohui Peng",
            "Yuanbo Wen",
            "Rui Zhang",
            "Zidong Du",
            "Qi Guo",
            "Yewen Pu",
            "Yunji Chen"
        ],
        "published": "2023-05-29T14:19:40Z",
        "summary": "Though LLMs are capable of generating plausible programs, it's challenging to\ninteract with the LLMs further to revise the program, especially if the user's\nspecific requirements are different from the initial proposal. In this paper,\nwe introduce ANPL, an interactive programming system that ensures users can\nalways refine the generated code towards their specific programmatic intents\nvia structured decompositions. Borrowing the paradigm of sketching from program\nsynthesis, an ANPL program consists of a set of input-outputs that it must\nsatisfy, a ``sketch'' -- control/data flow expressed in precise code (e.g.\nPython), and ``holes'' -- sub-modules to be implemented by the LLM specified\nwith natural language. The user revises an ANPL program by either modifying the\nsketch, changing the language used to describe the holes, or providing\nadditional input-outputs to a particular hole, turning it into a sub-ANPL\nprogram that can be solved recursively. This workflow allows the users to\noffload programming burdens to the LLM as much as possible while retaining the\nability to pinpoint and resolve bugs locally, without exposing the rest of the\nprogram to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus\n(ARC), a set of unique tasks that are challenging for state-of-the-art AI\nsystems, showing it outperforms baseline programming systems that (a) without\nthe ability to decompose tasks interactively and (b) without the guarantee that\nthe modules can be correctly composed together. Additional evaluations on APPS,\nHumanEval, and real-world programming tasks have validated that the ANPL\nframework is applicable to multiple programming domains. We release the ANPL\nsolutions to the ARC tasks as a dataset, providing insights into how humans\ndecompose novel tasks programmatically. See our code at\nhttps://iprc-dip.github.io/ANPL/.",
        "pdf_link": "https://arxiv.org/pdf/2305.18498v2.pdf"
    },
    {
        "title": "BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages",
        "authors": [
            "Wen Yang",
            "Chong Li",
            "Jiajun Zhang",
            "Chengqing Zong"
        ],
        "published": "2023-05-29T14:07:52Z",
        "summary": "Large language models (LLMs) demonstrate promising translation performance\namong various natural languages. However, many LLMs especially the open-sourced\nones, such as BLOOM and LLaMA, are English-dominant and support only dozens of\nnatural languages, making the potential of LLMs on language translation less\nexplored. In this work, we present BigTranslate which adapts LLaMA that covers\nonly 20 languages and enhances it with multilingual translation capability on\nmore than 100 languages. BigTranslate is built upon LLaMA-13B and it is\noptimized in three steps. First, we continue training LLaMA with massive\nChinese monolingual data. Second, we continue training the model with a\nlarge-scale parallel dataset that covers 102 natural languages. Third, we\ninstruct-tune the foundation model with multilingual translation instructions,\nleading to our BigTranslate model. The preliminary experiments on multilingual\ntranslation show that BigTranslate performs comparably with ChatGPT and Google\nTranslate in many languages and even outperforms ChatGPT in 8 language pairs.\nWe release the BigTranslate model and hope it can advance the research\nprogress.",
        "pdf_link": "https://arxiv.org/pdf/2305.18098v3.pdf"
    },
    {
        "title": "Game of Tones: Faculty detection of GPT-4 generated content in university assessments",
        "authors": [
            "Mike Perkins",
            "Jasper Roe",
            "Darius Postma",
            "James McGaughran",
            "Don Hickerson"
        ],
        "published": "2023-05-29T13:31:58Z",
        "summary": "This study explores the robustness of university assessments against the use\nof Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and\nevaluates the ability of academic staff to detect its use when supported by the\nTurnitin Artificial Intelligence (AI) detection tool. The research involved\ntwenty-two GPT-4 generated submissions being created and included in the\nassessment process to be marked by fifteen different faculty members. The study\nreveals that although the detection tool identified 91% of the experimental\nsubmissions as containing some AI-generated content, the total detected content\nwas only 54.8%. This suggests that the use of adversarial techniques regarding\nprompt engineering is an effective method in evading AI detection tools and\nhighlights that improvements to AI detection software are needed. Using the\nTurnitin AI detect tool, faculty reported 54.5% of the experimental submissions\nto the academic misconduct process, suggesting the need for increased awareness\nand training into these tools. Genuine submissions received a mean score of\n54.4, whereas AI-generated content scored 52.3, indicating the comparable\nperformance of GPT-4 in real-life situations. Recommendations include adjusting\nassessment strategies to make them more resistant to the use of AI tools, using\nAI-inclusive assessment where possible, and providing comprehensive training\nprograms for faculty and students. This research contributes to understanding\nthe relationship between AI-generated content and academic assessment, urging\nfurther investigation to preserve academic integrity.",
        "pdf_link": "https://arxiv.org/pdf/2305.18081v1.pdf"
    },
    {
        "title": "Image Captioning with Multi-Context Synthetic Data",
        "authors": [
            "Feipeng Ma",
            "Yizhou Zhou",
            "Fengyun Rao",
            "Yueyi Zhang",
            "Xiaoyan Sun"
        ],
        "published": "2023-05-29T13:18:59Z",
        "summary": "Image captioning requires numerous annotated image-text pairs, resulting in\nsubstantial annotation costs. Recently, large models (e.g. diffusion models and\nlarge language models) have excelled in producing high-quality images and text.\nThis potential can be harnessed to create synthetic image-text pairs for\ntraining captioning models. Synthetic data can improve cost and time efficiency\nin data collection, allow for customization to specific domains, bootstrap\ngeneralization capability for zero-shot performance, and circumvent privacy\nconcerns associated with real-world data. However, existing methods struggle to\nattain satisfactory performance solely through synthetic data. We identify the\nissue as generated images from simple descriptions mostly capture a solitary\nperspective with limited context, failing to align with the intricate scenes\nprevalent in real-world imagery. To tackle this, we present an innovative\npipeline that introduces multi-context data generation. Beginning with an\ninitial text corpus, our approach employs a large language model to extract\nmultiple sentences portraying the same scene from diverse viewpoints. These\nsentences are then condensed into a single sentence with multiple contexts.\nSubsequently, we generate intricate images using the condensed captions through\ndiffusion models. Our model is exclusively trained on synthetic image-text\npairs crafted through this process. The effectiveness of our pipeline is\nvalidated through experimental results in both the in-domain and cross-domain\nsettings, where it achieves state-of-the-art performance on well-known datasets\nsuch as MSCOCO, Flickr30k, and NoCaps.",
        "pdf_link": "https://arxiv.org/pdf/2305.18072v2.pdf"
    },
    {
        "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "M Saiful Bari",
            "Mizanur Rahman",
            "Md Amran Hossen Bhuiyan",
            "Shafiq Joty",
            "Jimmy Xiangji Huang"
        ],
        "published": "2023-05-29T12:37:21Z",
        "summary": "The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.18486v4.pdf"
    },
    {
        "title": "Chatbots to ChatGPT in a Cybersecurity Space: Evolution, Vulnerabilities, Attacks, Challenges, and Future Recommendations",
        "authors": [
            "Attia Qammar",
            "Hongmei Wang",
            "Jianguo Ding",
            "Abdenacer Naouri",
            "Mahmoud Daneshmand",
            "Huansheng Ning"
        ],
        "published": "2023-05-29T12:26:44Z",
        "summary": "Chatbots shifted from rule-based to artificial intelligence techniques and\ngained traction in medicine, shopping, customer services, food delivery,\neducation, and research. OpenAI developed ChatGPT blizzard on the Internet as\nit crossed one million users within five days of its launch. However, with the\nenhanced popularity, chatbots experienced cybersecurity threats and\nvulnerabilities. This paper discussed the relevant literature, reports, and\nexplanatory incident attacks generated against chatbots. Our initial point is\nto explore the timeline of chatbots from ELIZA (an early natural language\nprocessing computer program) to GPT-4 and provide the working mechanism of\nChatGPT. Subsequently, we explored the cybersecurity attacks and\nvulnerabilities in chatbots. Besides, we investigated the ChatGPT, specifically\nin the context of creating the malware code, phishing emails, undetectable\nzero-day attacks, and generation of macros and LOLBINs. Furthermore, the\nhistory of cyberattacks and vulnerabilities exploited by cybercriminals are\ndiscussed, particularly considering the risk and vulnerabilities in ChatGPT.\nAddressing these threats and vulnerabilities requires specific strategies and\nmeasures to reduce the harmful consequences. Therefore, the future directions\nto address the challenges were presented.",
        "pdf_link": "https://arxiv.org/pdf/2306.09255v1.pdf"
    },
    {
        "title": "Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation",
        "authors": [
            "Jiawei Huang",
            "Yi Ren",
            "Rongjie Huang",
            "Dongchao Yang",
            "Zhenhui Ye",
            "Chen Zhang",
            "Jinglin Liu",
            "Xiang Yin",
            "Zejun Ma",
            "Zhou Zhao"
        ],
        "published": "2023-05-29T10:41:28Z",
        "summary": "Large diffusion models have been successful in text-to-audio (T2A) synthesis\ntasks, but they often suffer from common issues such as semantic misalignment\nand poor temporal consistency due to limited natural language understanding and\ndata scarcity. Additionally, 2D spatial structures widely used in T2A works\nlead to unsatisfactory audio quality when generating variable-length audio\nsamples since they do not adequately prioritize temporal information. To\naddress these challenges, we propose Make-an-Audio 2, a latent diffusion-based\nT2A method that builds on the success of Make-an-Audio. Our approach includes\nseveral techniques to improve semantic alignment and temporal consistency:\nFirstly, we use pre-trained large language models (LLMs) to parse the text into\nstructured <event & order> pairs for better temporal information capture. We\nalso introduce another structured-text encoder to aid in learning semantic\nalignment during the diffusion denoising process. To improve the performance of\nvariable length generation and enhance the temporal information extraction, we\ndesign a feed-forward Transformer-based diffusion denoiser. Finally, we use\nLLMs to augment and transform a large amount of audio-label data into\naudio-text datasets to alleviate the problem of scarcity of temporal data.\nExtensive experiments show that our method outperforms baseline models in both\nobjective and subjective metrics, and achieves significant gains in temporal\ninformation understanding, semantic consistency, and sound quality.",
        "pdf_link": "https://arxiv.org/pdf/2305.18474v1.pdf"
    },
    {
        "title": "Large Language Models are not Fair Evaluators",
        "authors": [
            "Peiyi Wang",
            "Lei Li",
            "Liang Chen",
            "Zefan Cai",
            "Dawei Zhu",
            "Binghuai Lin",
            "Yunbo Cao",
            "Qi Liu",
            "Tianyu Liu",
            "Zhifang Sui"
        ],
        "published": "2023-05-29T07:41:03Z",
        "summary": "In this paper, we uncover a systematic bias in the evaluation paradigm of\nadopting large language models~(LLMs), e.g., GPT-4, as a referee to score and\ncompare the quality of responses generated by candidate models. We find that\nthe quality ranking of candidate responses can be easily hacked by simply\naltering their order of appearance in the context. This manipulation allows us\nto skew the evaluation result, making one model appear considerably superior to\nthe other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries\nwith ChatGPT as an evaluator. To address this issue, we propose a calibration\nframework with three simple yet effective strategies: 1) Multiple Evidence\nCalibration, which requires the evaluator model to generate multiple evaluation\nevidence before assigning ratings; 2) Balanced Position Calibration, which\naggregates results across various orders to determine the final score; 3)\nHuman-in-the-Loop Calibration, which introduces a balanced position diversity\nentropy to measure the difficulty of each example and seeks human assistance\nwhen needed. We also manually annotate the \"win/tie/lose\" outcomes of responses\nfrom ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and\nextensive experiments demonstrate that our approach successfully mitigates\nevaluation bias, resulting in closer alignment with human judgments. We release\nour code and human annotation at \\url{https://github.com/i-Eval/FairEval} to\nfacilitate future research.",
        "pdf_link": "https://arxiv.org/pdf/2305.17926v2.pdf"
    },
    {
        "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models",
        "authors": [
            "Zechun Liu",
            "Barlas Oguz",
            "Changsheng Zhao",
            "Ernie Chang",
            "Pierre Stock",
            "Yashar Mehdad",
            "Yangyang Shi",
            "Raghuraman Krishnamoorthi",
            "Vikas Chandra"
        ],
        "published": "2023-05-29T05:22:11Z",
        "summary": "Several post-training quantization methods have been applied to large\nlanguage models (LLMs), and have been shown to perform well down to 8-bits. We\nfind that these methods break down at lower bit precision, and investigate\nquantization aware training for LLMs (LLM-QAT) to push quantization levels even\nfurther. We propose a data-free distillation method that leverages generations\nproduced by the pre-trained model, which better preserves the original output\ndistribution and allows quantizing any generative model independent of its\ntraining data, similar to post-training quantization methods. In addition to\nquantizing weights and activations, we also quantize the KV cache, which is\ncritical for increasing throughput and support long sequence dependencies at\ncurrent model sizes. We experiment with LLaMA models of sizes 7B, 13B, and 30B,\nat quantization levels down to 4-bits. We observe large improvements over\ntraining-free methods, especially in the low-bit settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.17888v1.pdf"
    },
    {
        "title": "Baselines for Identifying Watermarked Large Language Models",
        "authors": [
            "Leonard Tang",
            "Gavin Uberti",
            "Tom Shlomi"
        ],
        "published": "2023-05-29T04:26:16Z",
        "summary": "We consider the emerging problem of identifying the presence and use of\nwatermarking schemes in widely used, publicly hosted, closed source large\nlanguage models (LLMs). We introduce a suite of baseline algorithms for\nidentifying watermarks in LLMs that rely on analyzing distributions of output\ntokens and logits generated by watermarked and unmarked LLMs. Notably,\nwatermarked LLMs tend to produce distributions that diverge qualitatively and\nidentifiably from standard models. Furthermore, we investigate the\nidentifiability of watermarks at varying strengths and consider the tradeoffs\nof each of our identification mechanisms with respect to watermarking scenario.\nAlong the way, we formalize the specific problem of identifying watermarks in\nLLMs, as well as LLM watermarks and watermark detection in general, providing a\nframework and foundations for studying them.",
        "pdf_link": "https://arxiv.org/pdf/2305.18456v1.pdf"
    },
    {
        "title": "Ask an Expert: Leveraging Language Models to Improve Strategic Reasoning in Goal-Oriented Dialogue Models",
        "authors": [
            "Qiang Zhang",
            "Jason Naradowsky",
            "Yusuke Miyao"
        ],
        "published": "2023-05-29T04:19:35Z",
        "summary": "Existing dialogue models may encounter scenarios which are not\nwell-represented in the training data, and as a result generate responses that\nare unnatural, inappropriate, or unhelpful. We propose the \"Ask an Expert\"\nframework in which the model is trained with access to an \"expert\" which it can\nconsult at each turn. Advice is solicited via a structured dialogue with the\nexpert, and the model is optimized to selectively utilize (or ignore) it given\nthe context and dialogue history. In this work the expert takes the form of an\nLLM. We evaluate this framework in a mental health support domain, where the\nstructure of the expert conversation is outlined by pre-specified prompts which\nreflect a reasoning strategy taught to practitioners in the field. Blenderbot\nmodels utilizing \"Ask an Expert\" show quality improvements across all expert\nsizes, including those with fewer parameters than the dialogue model itself.\nOur best model provides a $\\sim 10\\%$ improvement over baselines, approaching\nhuman-level scores on \"engingingness\" and \"helpfulness\" metrics.",
        "pdf_link": "https://arxiv.org/pdf/2305.17878v1.pdf"
    },
    {
        "title": "Taming AI Bots: Controllability of Neural States in Large Language Models",
        "authors": [
            "Stefano Soatto",
            "Paulo Tabuada",
            "Pratik Chaudhari",
            "Tian Yu Liu"
        ],
        "published": "2023-05-29T03:58:33Z",
        "summary": "We tackle the question of whether an agent can, by suitable choice of\nprompts, control an AI bot to any state. To that end, we first introduce a\nformal definition of ``meaning'' that is amenable to analysis. Then, we\ncharacterize ``meaningful data'' on which large language models (LLMs) are\nostensibly trained, and ``well-trained LLMs'' through conditions that are\nlargely met by today's LLMs. While a well-trained LLM constructs an embedding\nspace of meanings that is Euclidean, meanings themselves do not form a vector\n(linear) subspace, but rather a quotient space within. We then characterize the\nsubset of meanings that can be reached by the state of the LLMs for some input\nprompt, and show that a well-trained bot can reach any meaning albeit with\nsmall probability. We then introduce a stronger notion of controllability as\n{\\em almost certain reachability}, and show that, when restricted to the space\nof meanings, an AI bot is controllable. We do so after introducing a functional\ncharacterization of attentive AI bots, and finally derive necessary and\nsufficient conditions for controllability. The fact that AI bots are\ncontrollable means that an adversary could steer them towards any state.\nHowever, the sampling process can be designed to counteract adverse actions and\navoid reaching undesirable regions of state space before their boundary is\ncrossed.",
        "pdf_link": "https://arxiv.org/pdf/2305.18449v1.pdf"
    },
    {
        "title": "Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery",
        "authors": [
            "Magdalena Wysocka",
            "Oskar Wysocki",
            "Maxime Delmas",
            "Vincent Mutel",
            "Andre Freitas"
        ],
        "published": "2023-05-28T22:46:21Z",
        "summary": "Inferring over and extracting information from Large Language Models (LLMs)\ntrained on a large corpus of scientific literature can potentially drive a new\nera in biomedical research, reducing the barriers for accessing existing\nmedical evidence. This work examines the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nsystematic analysis is applied to ten state-of-the-art models, from models\nspecialised on biomedical scientific corpora to general models such as ChatGPT,\nGPT-4 and Llama 2 in two prompting-based tasks: chemical compound definition\ngeneration and chemical compound-fungus relation determination. The work\nprovides a systematic assessment on the ability of LLMs to encode and express\nthese relations, verifying for fluency, prompt-alignment, semantic coherence,\nfactual knowledge and specificity of generated responses. Results show that\nwhile recent models have improved in fluency, factual accuracy is still low and\nmodels are biased towards over-represented entities. The ability of LLMs to\nserve as biomedical knowledge bases is questioned, and the need for additional\nsystematic evaluation frameworks is highlighted. The best performing GPT-4\nproduced a factual definition for 70% of chemical compounds and 43.6% factual\nrelations to fungi, whereas the best open source model BioGPT-large 30% of the\ncompounds and 30% of the relations for the best-performing prompt. The results\nshow that while LLMs are currently not fit for purpose to be used as biomedical\nfactual knowledge bases, there is a promising emerging property in the\ndirection of factuality as the models become domain specialised, scale-up in\nsize and level of human feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.17819v2.pdf"
    },
    {
        "title": "Transfer Learning for Power Outage Detection Task with Limited Training Data",
        "authors": [
            "Olukunle Owolabi"
        ],
        "published": "2023-05-28T22:36:35Z",
        "summary": "Early detection of power outages is crucial for maintaining a reliable power\ndistribution system. This research investigates the use of transfer learning\nand language models in detecting outages with limited labeled data. By\nleveraging pretraining and transfer learning, models can generalize to unseen\nclasses.\n  Using a curated balanced dataset of social media tweets related to power\noutages, we conducted experiments using zero-shot and few-shot learning. Our\nhypothesis is that Language Models pretrained with limited data could achieve\nhigh performance in outage detection tasks over baseline models. Results show\nthat while classical models outperform zero-shot Language Models, few-shot\nfine-tuning significantly improves their performance. For example, with 10%\nfine-tuning, BERT achieves 81.3% accuracy (+15.3%), and GPT achieves 74.5%\naccuracy (+8.5%). This has practical implications for analyzing and localizing\noutages in scenarios with limited data availability.\n  Our evaluation provides insights into the potential of few-shot fine-tuning\nwith Language Models for power outage detection, highlighting their strengths\nand limitations. This research contributes to the knowledge base of leveraging\nadvanced natural language processing techniques for managing critical\ninfrastructure.",
        "pdf_link": "https://arxiv.org/pdf/2305.17817v1.pdf"
    },
    {
        "title": "Targeted Data Generation: Finding and Fixing Model Weaknesses",
        "authors": [
            "Zexue He",
            "Marco Tulio Ribeiro",
            "Fereshte Khani"
        ],
        "published": "2023-05-28T19:36:50Z",
        "summary": "Even when aggregate accuracy is high, state-of-the-art NLP models often fail\nsystematically on specific subgroups of data, resulting in unfair outcomes and\neroding user trust. Additional data collection may not help in addressing these\nweaknesses, as such challenging subgroups may be unknown to users, and\nunderrepresented in the existing and new data. We propose Targeted Data\nGeneration (TDG), a framework that automatically identifies challenging\nsubgroups, and generates new data for those subgroups using large language\nmodels (LLMs) with a human in the loop. TDG estimates the expected benefit and\npotential harm of data augmentation for each subgroup, and selects the ones\nmost likely to improve within group performance without hurting overall\nperformance. In our experiments, TDG significantly improves the accuracy on\nchallenging subgroups for state-of-the-art sentiment analysis and natural\nlanguage inference models, while also improving overall test accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2305.17804v1.pdf"
    },
    {
        "title": "Semantic Segmentation with Bidirectional Language Models Improves Long-form ASR",
        "authors": [
            "W. Ronny Huang",
            "Hao Zhang",
            "Shankar Kumar",
            "Shuo-yiin Chang",
            "Tara N. Sainath"
        ],
        "published": "2023-05-28T19:31:45Z",
        "summary": "We propose a method of segmenting long-form speech by separating semantically\ncomplete sentences within the utterance. This prevents the ASR decoder from\nneedlessly processing faraway context while also preventing it from missing\nrelevant context within the current sentence. Semantically complete sentence\nboundaries are typically demarcated by punctuation in written text; but\nunfortunately, spoken real-world utterances rarely contain punctuation. We\naddress this limitation by distilling punctuation knowledge from a\nbidirectional teacher language model (LM) trained on written, punctuated text.\nWe compare our segmenter, which is distilled from the LM teacher, against a\nsegmenter distilled from a acoustic-pause-based teacher used in other works, on\na streaming ASR pipeline. The pipeline with our segmenter achieves a 3.2%\nrelative WER gain along with a 60 ms median end-of-segment latency reduction on\na YouTube captioning task.",
        "pdf_link": "https://arxiv.org/pdf/2305.18419v1.pdf"
    },
    {
        "title": "Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data",
        "authors": [
            "Mugariya Farooq",
            "Shahad Hardan",
            "Aigerim Zhumbhayeva",
            "Yujia Zheng",
            "Preslav Nakov",
            "Kun Zhang"
        ],
        "published": "2023-05-28T17:07:46Z",
        "summary": "The need for more usable and explainable machine learning models in\nhealthcare increases the importance of developing and utilizing causal\ndiscovery algorithms, which aim to discover causal relations by analyzing\nobservational data. Explainable approaches aid clinicians and biologists in\npredicting the prognosis of diseases and suggesting proper treatments. However,\nvery little research has been conducted at the crossroads between causal\ndiscovery, genomics, and breast cancer, and we aim to bridge this gap.\nMoreover, evaluation of causal discovery methods on real data is in general\nnotoriously difficult because ground-truth causal relations are usually\nunknown, and accordingly, in this paper, we also propose to address the\nevaluation problem with large language models. In particular, we exploit\nsuitable causal discovery algorithms to investigate how various perturbations\nin the genome can affect the survival of patients diagnosed with breast cancer.\nWe used three main causal discovery algorithms: PC, Greedy Equivalence Search\n(GES), and a Generalized Precision Matrix-based one. We experiment with a\nsubset of The Cancer Genome Atlas, which contains information about mutations,\ncopy number variations, protein levels, and gene expressions for 705 breast\ncancer patients. Our findings reveal important factors related to the vital\nstatus of patients using causal discovery algorithms. However, the reliability\nof these results remains a concern in the medical domain. Accordingly, as\nanother contribution of the work, the results are validated through language\nmodels trained on biomedical literature, such as BlueBERT and other large\nlanguage models trained on medical corpora. Our results profess proper\nutilization of causal discovery algorithms and language models for revealing\nreliable causal relations for clinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.18410v1.pdf"
    },
    {
        "title": "Language Models are Bounded Pragmatic Speakers: Understanding RLHF from a Bayesian Cognitive Modeling Perspective",
        "authors": [
            "Khanh Nguyen"
        ],
        "published": "2023-05-28T16:04:48Z",
        "summary": "How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.17760v6.pdf"
    },
    {
        "title": "Mitigating Label Biases for In-context Learning",
        "authors": [
            "Yu Fei",
            "Yifan Hou",
            "Zeming Chen",
            "Antoine Bosselut"
        ],
        "published": "2023-05-28T15:37:39Z",
        "summary": "Various design settings for in-context learning (ICL), such as the choice and\norder of the in-context examples, can bias a model toward a particular\nprediction without being reflective of an understanding of the task. While many\nstudies discuss these design choices, there have been few systematic\ninvestigations into categorizing them and mitigating their impact. In this\nwork, we define a typology for three types of label biases in ICL for text\nclassification: vanilla-label bias, context-label bias, and domain-label bias\n(which we conceptualize and detect for the first time).\n  Our analysis demonstrates that prior label bias calibration methods fall\nshort of addressing all three types of biases. Specifically, domain-label bias\nrestricts LLMs to random-level performance on many tasks regardless of the\nchoice of in-context examples. To mitigate the effect of these biases, we\npropose a simple bias calibration method that estimates a language model's\nlabel bias using random in-domain words from the task corpus. After controlling\nfor this estimated bias when making predictions, our novel domain-context\ncalibration significantly improves the ICL performance of GPT-J and GPT-3 on a\nwide range of tasks. The gain is substantial on tasks with large domain-label\nbias (up to 37% in Macro-F1). Furthermore, our results generalize to models\nwith different scales, pretraining methods, and manually-designed task\ninstructions, showing the prevalence of label biases in ICL.",
        "pdf_link": "https://arxiv.org/pdf/2305.19148v3.pdf"
    },
    {
        "title": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering",
        "authors": [
            "Bhawesh Kumar",
            "Charlie Lu",
            "Gauri Gupta",
            "Anil Palepu",
            "David Bellamy",
            "Ramesh Raskar",
            "Andrew Beam"
        ],
        "published": "2023-05-28T15:26:10Z",
        "summary": "As large language models continue to be widely developed, robust uncertainty\nquantification techniques will become crucial for their safe deployment in\nhigh-stakes scenarios. In this work, we explore how conformal prediction can be\nused to provide uncertainty quantification in language models for the specific\ntask of multiple-choice question-answering. We find that the uncertainty\nestimates from conformal prediction are tightly correlated with prediction\naccuracy. This observation can be useful for downstream applications such as\nselective classification and filtering out low-quality predictions. We also\ninvestigate the exchangeability assumption required by conformal prediction to\nout-of-subject questions, which may be a more realistic scenario for many\npractical applications. Our work contributes towards more trustworthy and\nreliable usage of large language models in safety-critical situations, where\nrobust guarantees of error rate are required.",
        "pdf_link": "https://arxiv.org/pdf/2305.18404v3.pdf"
    },
    {
        "title": "LoRAPrune: Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning",
        "authors": [
            "Mingyang Zhang",
            "Hao Chen",
            "Chunhua Shen",
            "Zhen Yang",
            "Linlin Ou",
            "Xinyi Yu",
            "Bohan Zhuang"
        ],
        "published": "2023-05-28T15:15:48Z",
        "summary": "Large pre-trained models (LPMs), such as LLaMA and GLM, have shown\nexceptional performance across various tasks through fine-tuning. Although\nlow-rank adaption (LoRA) has emerged to cheaply fine-tune these LPMs on\ndownstream tasks, their deployment is still hindered by the vast model scale\nand computational costs. Neural network pruning offers a way to compress LPMs.\nHowever, the current pruning methods designed for LPMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LPMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate, compact model for efficient inference in a highly memory-effective\nmanner. Specifically, we first design a LoRA-guided pruning criterion, which\nuses the weights and gradients of LoRA, rather than the gradients of\npre-trained weights for importance estimation. We then propose a structured\niterative pruning procedure, to remove redundant channels and heads. Extensive\nexperimental results demonstrate the superior performance of our LoRAPrune over\nexisting approaches on the LLaMA series models. For instance, at a 50\\%\ncompression rate, LoRAPrune outperforms LLM-Pruner by a perplexity reduction of\n8.0 on WikiText2 and 16.05 on PTB datasets, while concurrently reducing memory\nusage by 52.6\\%. The code will be released after review",
        "pdf_link": "https://arxiv.org/pdf/2305.18403v3.pdf"
    },
    {
        "title": "Breaking Language Barriers with a LEAP: Learning Strategies for Polyglot LLMs",
        "authors": [
            "Akshay Nambi",
            "Vaibhav Balloli",
            "Mercy Ranjit",
            "Tanuja Ganu",
            "Kabir Ahuja",
            "Sunayana Sitaram",
            "Kalika Bali"
        ],
        "published": "2023-05-28T14:48:38Z",
        "summary": "Large language models (LLMs) are at the forefront of transforming numerous\ndomains globally. However, their inclusivity and effectiveness remain limited\nfor non-Latin scripts and low-resource languages. This paper tackles the\nimperative challenge of enhancing the multilingual performance of LLMs,\nspecifically focusing on Generative models. Through systematic investigation\nand evaluation of diverse languages using popular question-answering (QA)\ndatasets, we present novel techniques that unlock the true potential of LLMs in\na polyglot landscape. Our approach encompasses three key strategies that yield\nremarkable improvements in multilingual proficiency. First, by meticulously\noptimizing prompts tailored for polyglot LLMs, we unlock their latent\ncapabilities, resulting in substantial performance boosts across languages.\nSecond, we introduce a new hybrid approach that synergizes GPT generation with\nmultilingual embeddings and achieves significant multilingual performance\nimprovement on critical tasks like QA and retrieval. Finally, to further propel\nthe performance of polyglot LLMs, we introduce a novel learning algorithm that\ndynamically selects the optimal prompt strategy, LLM model, and embeddings per\nquery. This dynamic adaptation maximizes the efficacy of LLMs across languages,\noutperforming best static and random strategies. Our results show substantial\nadvancements in multilingual understanding and generation across a diverse\nrange of languages.",
        "pdf_link": "https://arxiv.org/pdf/2305.17740v1.pdf"
    },
    {
        "title": "FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions",
        "authors": [
            "Noam Rotstein",
            "David Bensaid",
            "Shaked Brody",
            "Roy Ganz",
            "Ron Kimmel"
        ],
        "published": "2023-05-28T13:16:03Z",
        "summary": "The advent of vision-language pre-training techniques enhanced substantial\nprogress in the development of models for image captioning. However, these\nmodels frequently produce generic captions and may omit semantically important\nimage details. This limitation can be traced back to the image-text datasets;\nwhile their captions typically offer a general description of image content,\nthey frequently omit salient details. Considering the magnitude of these\ndatasets, manual reannotation is impractical, emphasizing the need for an\nautomated approach. To address this challenge, we leverage existing captions\nand explore augmenting them with visual details using \"frozen\" vision experts\nincluding an object detector, an attribute recognizer, and an Optical Character\nRecognizer (OCR). Our proposed method, FuseCap, fuses the outputs of such\nvision experts with the original captions using a large language model (LLM),\nyielding comprehensive image descriptions. We automatically curate a training\nset of 12M image-enriched caption pairs. These pairs undergo extensive\nevaluation through both quantitative and qualitative analyses. Subsequently,\nthis data is utilized to train a captioning generation BLIP-based model. This\nmodel outperforms current state-of-the-art approaches, producing more precise\nand detailed descriptions, demonstrating the effectiveness of the proposed\ndata-centric approach. We release this large-scale dataset of enriched\nimage-caption pairs for the community.",
        "pdf_link": "https://arxiv.org/pdf/2305.17718v2.pdf"
    },
    {
        "title": "LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers",
        "authors": [
            "Xuanqi Liu",
            "Zhuotao Liu"
        ],
        "published": "2023-05-28T13:08:13Z",
        "summary": "The community explored to build private inference frameworks for\ntransformer-based large language models (LLMs) in a server-client setting,\nwhere the server holds the model parameters and the client inputs its private\ndata (or prompt) for inference. However, these frameworks impose significant\noverhead when the private inputs are forward propagated through the original\nLLMs. In this paper, we show that substituting the computation- and\ncommunication-heavy operators in the transformer architecture with\nprivacy-computing friendly approximations can greatly reduce the private\ninference costs while incurring very minor impact on model performance.\nCompared to state-of-the-art Iron (NeurIPS 2022), our privacy-computing\nfriendly model inference pipeline achieves a $5\\times$ acceleration in\ncomputation and an 80% reduction in communication overhead, while retaining\nnearly identical accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2305.18396v3.pdf"
    },
    {
        "title": "Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks",
        "authors": [
            "Minki Kang",
            "Seanie Lee",
            "Jinheon Baek",
            "Kenji Kawaguchi",
            "Sung Ju Hwang"
        ],
        "published": "2023-05-28T13:00:00Z",
        "summary": "Large Language Models (LLMs) have shown promising performance in\nknowledge-intensive reasoning tasks that require a compound understanding of\nknowledge. However, deployment of the LLMs in real-world applications can be\nchallenging due to their high computational requirements and concerns on data\nprivacy. Previous studies have focused on building task-specific small Language\nModels (LMs) by fine-tuning them with labeled data or distilling LLMs. However,\nthese approaches are ill-suited for knowledge-intensive reasoning tasks due to\nthe limited capacity of small LMs in memorizing the knowledge required.\nMotivated by our theoretical analysis on memorization, we propose\nKnowledge-Augmented Reasoning Distillation (KARD), a novel method that\nfine-tunes small LMs to generate rationales obtained from LLMs with augmented\nknowledge retrieved from an external knowledge base. Moreover, we further\npropose a neural reranker to obtain documents relevant to rationale generation.\nWe empirically show that KARD significantly improves the performance of small\nT5 and GPT models on the challenging knowledge-intensive reasoning datasets,\nnamely MedQA-USMLE, StrategyQA, and OpenbookQA. Notably, our method makes the\n250M T5 models achieve superior performance against the fine-tuned 3B models,\nhaving 12 times larger parameters, on both MedQA-USMLE and StrategyQA\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2305.18395v2.pdf"
    },
    {
        "title": "KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Application",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Gunhee Kim",
            "Jung-Woo Ha"
        ],
        "published": "2023-05-28T12:07:16Z",
        "summary": "Large language models (LLMs) learn not only natural text generation abilities\nbut also social biases against different demographic groups from real-world\ndata. This poses a critical risk when deploying LLM-based applications.\nExisting research and resources are not readily applicable in South Korea due\nto the differences in language and culture, both of which significantly affect\nthe biases and targeted demographic groups. This limitation requires localized\nsocial bias datasets to ensure the safe and effective deployment of LLMs. To\nthis end, we present KO SB I, a new social bias dataset of 34k pairs of\ncontexts and sentences in Korean covering 72 demographic groups in 15\ncategories. We find that through filtering-based moderation, social biases in\ngenerated content can be reduced by 16.47%p on average for HyperCLOVA (30B and\n82B), and GPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2305.17701v2.pdf"
    },
    {
        "title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration",
        "authors": [
            "Hwaran Lee",
            "Seokhee Hong",
            "Joonsuk Park",
            "Takyoung Kim",
            "Meeyoung Cha",
            "Yejin Choi",
            "Byoung Pil Kim",
            "Gunhee Kim",
            "Eun-Ju Lee",
            "Yong Lim",
            "Alice Oh",
            "Sangchul Park",
            "Jung-Woo Ha"
        ],
        "published": "2023-05-28T11:51:20Z",
        "summary": "The potential social harms that large language models pose, such as\ngenerating offensive content and reinforcing biases, are steeply rising.\nExisting works focus on coping with this concern while interacting with\nill-intentioned users, such as those who explicitly make hate speech or elicit\nharmful responses. However, discussions on sensitive issues can become toxic\neven if the users are well-intentioned. For safer models in such scenarios, we\npresent the Sensitive Questions and Acceptable Response (SQuARe) dataset, a\nlarge-scale Korean dataset of 49k sensitive questions with 42k acceptable and\n46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA\nin a human-in-the-loop manner based on real news headlines. Experiments show\nthat acceptable response generation significantly improves for HyperCLOVA and\nGPT-3, demonstrating the efficacy of this dataset.",
        "pdf_link": "https://arxiv.org/pdf/2305.17696v1.pdf"
    },
    {
        "title": "Evaluating GPT-3 Generated Explanations for Hateful Content Moderation",
        "authors": [
            "Han Wang",
            "Ming Shan Hee",
            "Md Rabiul Awal",
            "Kenny Tsu Wei Choo",
            "Roy Ka-Wei Lee"
        ],
        "published": "2023-05-28T10:05:13Z",
        "summary": "Recent research has focused on using large language models (LLMs) to generate\nexplanations for hate speech through fine-tuning or prompting. Despite the\ngrowing interest in this area, these generated explanations' effectiveness and\npotential limitations remain poorly understood. A key concern is that these\nexplanations, generated by LLMs, may lead to erroneous judgments about the\nnature of flagged content by both users and content moderators. For instance,\nan LLM-generated explanation might inaccurately convince a content moderator\nthat a benign piece of content is hateful. In light of this, we propose an\nanalytical framework for examining hate speech explanations and conducted an\nextensive survey on evaluating such explanations. Specifically, we prompted\nGPT-3 to generate explanations for both hateful and non-hateful content, and a\nsurvey was conducted with 2,400 unique respondents to evaluate the generated\nexplanations. Our findings reveal that (1) human evaluators rated the\nGPT-generated explanations as high quality in terms of linguistic fluency,\ninformativeness, persuasiveness, and logical soundness, (2) the persuasive\nnature of these explanations, however, varied depending on the prompting\nstrategy employed, and (3) this persuasiveness may result in incorrect\njudgments about the hatefulness of the content. Our study underscores the need\nfor caution in applying LLM-generated explanations for content moderation. Code\nand results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.",
        "pdf_link": "https://arxiv.org/pdf/2305.17680v4.pdf"
    },
    {
        "title": "Reward Collapse in Aligning Large Language Models",
        "authors": [
            "Ziang Song",
            "Tianle Cai",
            "Jason D. Lee",
            "Weijie J. Su"
        ],
        "published": "2023-05-28T02:12:00Z",
        "summary": "The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.",
        "pdf_link": "https://arxiv.org/pdf/2305.17608v1.pdf"
    },
    {
        "title": "Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds",
        "authors": [
            "Yan Ding",
            "Xiaohan Zhang",
            "Saeid Amiri",
            "Nieqing Cao",
            "Hao Yang",
            "Andy Kaminski",
            "Chad Esselink",
            "Shiqi Zhang"
        ],
        "published": "2023-05-27T22:30:15Z",
        "summary": "Task planning systems have been developed to help robots use human knowledge\n(about actions) to complete long-horizon tasks. Most of them have been\ndeveloped for \"closed worlds\" while assuming the robot is provided with\ncomplete world knowledge. However, the real world is generally open, and the\nrobots frequently encounter unforeseen situations that can potentially break\nthe planner's completeness. Could we leverage the recent advances on\npre-trained Large Language Models (LLMs) to enable classical planning systems\nto deal with novel situations?\n  This paper introduces a novel framework, called COWP, for open-world task\nplanning and situation handling. COWP dynamically augments the robot's action\nknowledge, including the preconditions and effects of actions, with\ntask-oriented commonsense knowledge. COWP embraces the openness from LLMs, and\nis grounded to specific domains via action knowledge. For systematic\nevaluations, we collected a dataset that includes 1,085 execution-time\nsituations. Each situation corresponds to a state instance wherein a robot is\npotentially unable to complete a task using a solution that normally works.\nExperimental results show that our approach outperforms competitive baselines\nfrom the literature in the success rate of service tasks. Additionally, we have\ndemonstrated COWP using a mobile manipulator. Supplementary materials are\navailable at: https://cowplanning.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2305.17590v2.pdf"
    },
    {
        "title": "Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark",
        "authors": [
            "Jason Hoelscher-Obermaier",
            "Julia Persson",
            "Esben Kran",
            "Ioannis Konstas",
            "Fazl Barez"
        ],
        "published": "2023-05-27T19:08:04Z",
        "summary": "Recent model editing techniques promise to mitigate the problem of memorizing\nfalse or outdated associations during LLM training. However, we show that these\ntechniques can introduce large unwanted side effects which are not detected by\nexisting specificity benchmarks. We extend the existing CounterFact benchmark\nto include a dynamic component and dub our benchmark CounterFact+.\nAdditionally, we extend the metrics used for measuring specificity by a\nprincipled KL divergence-based metric. We use this improved benchmark to\nevaluate recent model editing techniques and find that they suffer from low\nspecificity. Our findings highlight the need for improved specificity\nbenchmarks that identify and prevent unwanted side effects.",
        "pdf_link": "https://arxiv.org/pdf/2305.17553v2.pdf"
    },
    {
        "title": "The Curse of Recursion: Training on Generated Data Makes Models Forget",
        "authors": [
            "Ilia Shumailov",
            "Zakhar Shumaylov",
            "Yiren Zhao",
            "Yarin Gal",
            "Nicolas Papernot",
            "Ross Anderson"
        ],
        "published": "2023-05-27T15:10:41Z",
        "summary": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.",
        "pdf_link": "https://arxiv.org/pdf/2305.17493v2.pdf"
    },
    {
        "title": "FERMAT: An Alternative to Accuracy for Numerical Reasoning",
        "authors": [
            "Jasivan Alex Sivakumar",
            "Nafise Sadat Moosavi"
        ],
        "published": "2023-05-27T15:00:45Z",
        "summary": "While pre-trained language models achieve impressive performance on various\nNLP benchmarks, they still struggle with tasks that require numerical\nreasoning. Recent advances in improving numerical reasoning are mostly achieved\nusing very large language models that contain billions of parameters and are\nnot accessible to everyone. In addition, numerical reasoning is measured using\na single score on existing datasets. As a result, we do not have a clear\nunderstanding of the strengths and shortcomings of existing models on different\nnumerical reasoning aspects and therefore, potential ways to improve them apart\nfrom scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we\nintroduce a multi-view evaluation set for numerical reasoning in English,\ncalled FERMAT. Instead of reporting a single score on a whole dataset, FERMAT\nevaluates models on various key numerical reasoning aspects such as number\nunderstanding, mathematical operations, and training dependency. Apart from\nproviding a comprehensive evaluation of models on different numerical reasoning\naspects, FERMAT enables a systematic and automated generation of an arbitrarily\nlarge training or evaluation set for each aspect.The datasets and codes are\npublicly available to generate further multi-view data for ulterior tasks and\nlanguages.",
        "pdf_link": "https://arxiv.org/pdf/2305.17491v1.pdf"
    },
    {
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks",
        "authors": [
            "Taicheng Guo",
            "Kehan Guo",
            "Bozhao Nan",
            "Zhenwen Liang",
            "Zhichun Guo",
            "Nitesh V. Chawla",
            "Olaf Wiest",
            "Xiangliang Zhang"
        ],
        "published": "2023-05-27T14:17:33Z",
        "summary": "Large Language Models (LLMs) with strong abilities in natural language\nprocessing tasks have emerged and have been applied in various kinds of areas\nsuch as science, finance and software engineering. However, the capability of\nLLMs to advance the field of chemistry remains unclear. In this paper, rather\nthan pursuing state-of-the-art performance, we aim to evaluate capabilities of\nLLMs in a wide range of tasks across the chemistry domain. We identify three\nkey chemistry-related capabilities including understanding, reasoning and\nexplaining to explore in LLMs and establish a benchmark containing eight\nchemistry tasks. Our analysis draws on widely recognized datasets facilitating\na broad exploration of the capacities of LLMs within the context of practical\nchemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are\nevaluated for each chemistry task in zero-shot and few-shot in-context learning\nsettings with carefully selected demonstration examples and specially crafted\nprompts. Our investigation found that GPT-4 outperformed other models and LLMs\nexhibit different competitive levels in eight chemistry tasks. In addition to\nthe key findings from the comprehensive benchmark analysis, our work provides\ninsights into the limitation of current LLMs and the impact of in-context\nlearning settings on LLMs' performance across various chemistry tasks. The code\nand datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.",
        "pdf_link": "https://arxiv.org/pdf/2305.18365v3.pdf"
    },
    {
        "title": "Query-Efficient Black-Box Red Teaming via Bayesian Optimization",
        "authors": [
            "Deokjae Lee",
            "JunYeong Lee",
            "Jung-Woo Ha",
            "Jin-Hwa Kim",
            "Sang-Woo Lee",
            "Hwaran Lee",
            "Hyun Oh Song"
        ],
        "published": "2023-05-27T11:00:15Z",
        "summary": "The deployment of large-scale generative models is often restricted by their\npotential risk of causing harm to users in unpredictable ways. We focus on the\nproblem of black-box red teaming, where a red team generates test cases and\ninteracts with the victim model to discover a diverse set of failures with\nlimited query access. Existing red teaming methods construct test cases based\non human supervision or language model (LM) and query all test cases in a\nbrute-force manner without incorporating any information from past evaluations,\nresulting in a prohibitively large number of queries. To this end, we propose\nBayesian red teaming (BRT), novel query-efficient black-box red teaming methods\nbased on Bayesian optimization, which iteratively identify diverse positive\ntest cases leading to model failures by utilizing the pre-defined user input\npool and the past evaluations. Experimental results on various user input pools\ndemonstrate that our method consistently finds a significantly larger number of\ndiverse positive test cases under the limited query budget than the baseline\nmethods. The source code is available at\nhttps://github.com/snu-mllab/Bayesian-Red-Teaming.",
        "pdf_link": "https://arxiv.org/pdf/2305.17444v1.pdf"
    },
    {
        "title": "Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making",
        "authors": [
            "Xuanjie Fang",
            "Sijie Cheng",
            "Yang Liu",
            "Wei Wang"
        ],
        "published": "2023-05-27T10:33:53Z",
        "summary": "Pre-trained language models (PLMs) have been widely used to underpin various\ndownstream tasks. However, the adversarial attack task has found that PLMs are\nvulnerable to small perturbations. Mainstream methods adopt a detached\ntwo-stage framework to attack without considering the subsequent influence of\nsubstitution at each step. In this paper, we formally model the adversarial\nattack task on PLMs as a sequential decision-making problem, where the whole\nattack process is sequential with two decision-making problems, i.e., word\nfinder and word substitution. Considering the attack process can only receive\nthe final state without any direct intermediate signals, we propose to use\nreinforcement learning to find an appropriate sequential attack path to\ngenerate adversaries, named SDM-Attack. Extensive experimental results show\nthat SDM-Attack achieves the highest attack success rate with a comparable\nmodification rate and semantic similarity to attack fine-tuned BERT.\nFurthermore, our analyses demonstrate the generalization and transferability of\nSDM-Attack. The code is available at https://github.com/fduxuan/SDM-Attack.",
        "pdf_link": "https://arxiv.org/pdf/2305.17440v1.pdf"
    },
    {
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "authors": [
            "Bill Yuchen Lin",
            "Yicheng Fu",
            "Karina Yang",
            "Faeze Brahman",
            "Shiyu Huang",
            "Chandra Bhagavatula",
            "Prithviraj Ammanabrolu",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-05-27T07:04:15Z",
        "summary": "We introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent's action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\ninteractive tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.17390v2.pdf"
    },
    {
        "title": "Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques",
        "authors": [
            "Daking Rai",
            "Bailin Wang",
            "Yilun Zhou",
            "Ziyu Yao"
        ],
        "published": "2023-05-27T06:09:03Z",
        "summary": "Compositional and domain generalization present significant challenges in\nsemantic parsing, even for state-of-the-art semantic parsers based on\npre-trained language models (LMs). In this study, we empirically investigate\nimproving an LM's generalization in semantic parsing with two simple\ntechniques: at the token level, we introduce a token preprocessing method to\npreserve the semantic boundaries of tokens produced by LM tokenizers; at the\nsequence level, we propose to use special tokens to mark the boundaries of\ncomponents aligned between input and output. Our experimental results on two\ntext-to-SQL semantic parsing datasets show that our token preprocessing,\nalthough simple, can substantially improve the LM performance on both types of\ngeneralization, and our component boundary marking method is particularly\nhelpful for compositional generalization.",
        "pdf_link": "https://arxiv.org/pdf/2305.17378v1.pdf"
    },
    {
        "title": "Augmenting Large Language Model Translators via Translation Memories",
        "authors": [
            "Yongyu Mu",
            "Abudurexiti Reheman",
            "Zhiquan Cao",
            "Yuchun Fan",
            "Bei Li",
            "Yinqiao Li",
            "Tong Xiao",
            "Chunliang Zhang",
            "Jingbo Zhu"
        ],
        "published": "2023-05-27T04:47:09Z",
        "summary": "Using translation memories (TMs) as prompts is a promising approach to\nin-context learning of machine translation models. In this work, we take a step\ntowards prompting large language models (LLMs) with TMs and making them better\ntranslators. We find that the ability of LLMs to ``understand'' prompts is\nindeed helpful for making better use of TMs. Experiments show that the results\nof a pre-trained LLM translator can be greatly improved by using high-quality\nTM-based prompts. These results are even comparable to those of the\nstate-of-the-art NMT systems which have access to large-scale in-domain\nbilingual data and are well tuned on the downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.17367v1.pdf"
    },
    {
        "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
        "authors": [
            "Xianjun Yang",
            "Wei Cheng",
            "Yue Wu",
            "Linda Petzold",
            "William Yang Wang",
            "Haifeng Chen"
        ],
        "published": "2023-05-27T03:58:29Z",
        "summary": "Large language models (LLMs) have notably enhanced the fluency and diversity\nof machine-generated text. However, this progress also presents a significant\nchallenge in detecting the origin of a given text, and current research on\ndetection methods lags behind the rapid evolution of LLMs. Conventional\ntraining-based methods have limitations in flexibility, particularly when\nadapting to new domains, and they often lack explanatory power. To address this\ngap, we propose a novel training-free detection strategy called Divergent\nN-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and\nthen use only the preceding portion as input to the LLMs to regenerate the new\nremaining parts. By analyzing the differences between the original and new\nremaining parts through N-gram analysis in black-box or probability divergence\nin white-box, we unveil significant discrepancies between the distribution of\nmachine-generated text and the distribution of human-written text. We conducted\nextensive experiments on the most advanced LLMs from OpenAI, including\ntext-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such\nas GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach\nexhibits state-of-the-art performance in distinguishing between human and\nGPT-generated text on four English and one German dataset, outperforming\nOpenAI's own classifier, which is trained on millions of text. Additionally,\nour methods provide reasonable explanations and evidence to support our claim,\nwhich is a unique feature of explainable detection. Our method is also robust\nunder the revised text attack and can additionally solve model sourcing. Codes\nare available at https://github.com/Xianjun-Yang/DNA-GPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.17359v2.pdf"
    },
    {
        "title": "Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance",
        "authors": [
            "Yao Fu",
            "Litu Ou",
            "Mingyu Chen",
            "Yuhao Wan",
            "Hao Peng",
            "Tushar Khot"
        ],
        "published": "2023-05-26T23:46:42Z",
        "summary": "As large language models (LLMs) are continuously being developed, their\nevaluation becomes increasingly important yet challenging. This work proposes\nChain-of-Thought Hub, an open-source evaluation suite on the multi-step\nreasoning capabilities of large language models. We are interested in this\nsetting for two reasons: (1) from the behavior of GPT and PaLM model family, we\nobserve that complex reasoning is likely to be a key differentiator between\nweaker and stronger LLMs; (2) we envisage large language models to become the\nnext-generation computational platform and foster an ecosystem of LLM-based new\napplications, this naturally requires the foundation models to perform complex\ntasks that often involve the composition of linguistic and logical operations.\nOur approach is to compile a suite of challenging reasoning benchmarks to track\nthe progress of LLMs. Our current results show that: (1) model scale clearly\ncorrelates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and\nPaLM-2 are the only two models that are comparable with GPT-4, while\nopen-sourced models still lag behind; (3) LLaMA-65B performs closely to\ncode-davinci-002, indicating that with successful further development such as\nreinforcement learning from human feedback (RLHF), it has great potential to be\nclose to GPT-3.5-Turbo. Our results also suggest that for the open-source\nefforts to catch up, the community may focus more on building better base\nmodels and exploring RLHF.",
        "pdf_link": "https://arxiv.org/pdf/2305.17306v1.pdf"
    },
    {
        "title": "Improved Instruction Ordering in Recipe-Grounded Conversation",
        "authors": [
            "Duong Minh Le",
            "Ruohao Guo",
            "Wei Xu",
            "Alan Ritter"
        ],
        "published": "2023-05-26T21:57:11Z",
        "summary": "In this paper, we study the task of instructional dialogue and focus on the\ncooking domain. Analyzing the generated output of the GPT-J model, we reveal\nthat the primary challenge for a recipe-grounded dialog system is how to\nprovide the instructions in the correct order. We hypothesize that this is due\nto the model's lack of understanding of user intent and inability to track the\ninstruction state (i.e., which step was last instructed). Therefore, we propose\nto explore two auxiliary subtasks, namely User Intent Detection and Instruction\nState Tracking, to support Response Generation with improved instruction\ngrounding. Experimenting with our newly collected dataset, ChattyChef, shows\nthat incorporating user intent and instruction state information helps the\nresponse generation model mitigate the incorrect order issue. Furthermore, to\ninvestigate whether ChatGPT has completely solved this task, we analyze its\noutputs and find that it also makes mistakes (10.7% of the responses), about\nhalf of which are out-of-order instructions. We will release ChattyChef to\nfacilitate further research in this area at:\nhttps://github.com/octaviaguo/ChattyChef.",
        "pdf_link": "https://arxiv.org/pdf/2305.17280v1.pdf"
    },
    {
        "title": "SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended)",
        "authors": [
            "Ruoxi Sun",
            "Sercan Ö. Arik",
            "Alex Muzio",
            "Lesly Miculicich",
            "Satya Gundabathula",
            "Pengcheng Yin",
            "Hanjun Dai",
            "Hootan Nakhost",
            "Rajarishi Sinha",
            "Zifeng Wang",
            "Tomas Pfister"
        ],
        "published": "2023-05-26T21:39:05Z",
        "summary": "Text-to-SQL, the process of translating natural language into Structured\nQuery Language (SQL), represents a transformative application of large language\nmodels (LLMs), potentially revolutionizing how humans interact with data. This\npaper introduces the SQL-PaLM framework, a comprehensive solution for\nunderstanding and enhancing Text-to-SQL using LLMs, using in the learning\nregimes of few-shot prompting and instruction fine-tuning. With few-shot\nprompting, we explore the effectiveness of consistency decoding with\nexecution-based error filtering. With instruction fine-tuning, we delve deep in\nunderstanding the critical paradigms that influence the performance of tuned\nLLMs. In particular, we investigate how performance can be improved through\nexpanded training data coverage and diversity, synthetic data augmentation, and\nintegrating query-specific database content. We propose a test-time selection\nmethod to further refine accuracy by integrating SQL outputs from multiple\nparadigms with execution feedback as guidance. Additionally, we tackle the\npractical challenge of navigating intricate databases with a significant number\nof tables and columns, proposing efficient techniques for accurately selecting\nrelevant database elements to enhance Text-to-SQL performance. Our holistic\napproach yields substantial advancements in Text-to-SQL, as demonstrated on two\nkey public benchmarks, Spider and BIRD. Through comprehensive ablations and\nerror analyses, we shed light on the strengths and weaknesses of our framework,\noffering valuable insights into Text-to-SQL's future work.",
        "pdf_link": "https://arxiv.org/pdf/2306.00739v4.pdf"
    },
    {
        "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
        "authors": [
            "Ruixiang Tang",
            "Dehan Kong",
            "Longtao Huang",
            "Hui Xue"
        ],
        "published": "2023-05-26T20:56:30Z",
        "summary": "Large language models (LLMs) have recently shown great potential for\nin-context learning, where LLMs learn a new task simply by conditioning on a\nfew input-label pairs (prompts). Despite their potential, our understanding of\nthe factors influencing end-task performance and the robustness of in-context\nlearning remains limited. This paper aims to bridge this knowledge gap by\ninvestigating the reliance of LLMs on shortcuts or spurious correlations within\nprompts. Through comprehensive experiments on classification and extraction\ntasks, we reveal that LLMs are \"lazy learners\" that tend to exploit shortcuts\nin prompts for downstream tasks. Additionally, we uncover a surprising finding\nthat larger models are more likely to utilize shortcuts in prompts during\ninference. Our findings provide a new perspective on evaluating robustness in\nin-context learning and pose new challenges for detecting and mitigating the\nuse of shortcuts in prompts.",
        "pdf_link": "https://arxiv.org/pdf/2305.17256v2.pdf"
    },
    {
        "title": "Large language models improve Alzheimer's disease diagnosis using multi-modality data",
        "authors": [
            "Yingjie Feng",
            "Jun Wang",
            "Xianfeng Gu",
            "Xiaoyin Xu",
            "Min Zhang"
        ],
        "published": "2023-05-26T18:42:19Z",
        "summary": "In diagnosing challenging conditions such as Alzheimer's disease (AD),\nimaging is an important reference. Non-imaging patient data such as patient\ninformation, genetic data, medication information, cognitive and memory tests\nalso play a very important role in diagnosis. Effect. However, limited by the\nability of artificial intelligence models to mine such information, most of the\nexisting models only use multi-modal image data, and cannot make full use of\nnon-image data. We use a currently very popular pre-trained large language\nmodel (LLM) to enhance the model's ability to utilize non-image data, and\nachieved SOTA results on the ADNI dataset.",
        "pdf_link": "https://arxiv.org/pdf/2305.19280v1.pdf"
    },
    {
        "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models",
        "authors": [
            "Julia Mendelsohn",
            "Ronan Le Bras",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "published": "2023-05-26T18:00:57Z",
        "summary": "Dogwhistles are coded expressions that simultaneously convey one meaning to a\nbroad audience and a second one, often hateful or provocative, to a narrow\nin-group; they are deployed to evade both political repercussions and\nalgorithmic content moderation. For example, in the sentence 'we need to end\nthe cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to\nmany, but secretly means 'Jewish' to a select few. We present the first\nlarge-scale computational investigation of dogwhistles. We develop a typology\nof dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles\nwith rich contextual information and examples, and analyze their usage in\nhistorical U.S. politicians' speeches. We then assess whether a large language\nmodel (GPT-3) can identify dogwhistles and their meanings, and find that\nGPT-3's performance varies widely across types of dogwhistles and targeted\ngroups. Finally, we show that harmful content containing dogwhistles avoids\ntoxicity detection, highlighting online risks of such coded language. This work\nsheds light on the theoretical and applied importance of dogwhistles in both\nNLP and computational social science, and provides resources for future\nresearch in modeling dogwhistles and mitigating their online harms.",
        "pdf_link": "https://arxiv.org/pdf/2305.17174v1.pdf"
    },
    {
        "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time",
        "authors": [
            "Zichang Liu",
            "Aditya Desai",
            "Fangshuo Liao",
            "Weitao Wang",
            "Victor Xie",
            "Zhaozhuo Xu",
            "Anastasios Kyrillidis",
            "Anshumali Shrivastava"
        ],
        "published": "2023-05-26T17:39:58Z",
        "summary": "Large language models(LLMs) have sparked a new wave of exciting AI\napplications. Hosting these models at scale requires significant memory\nresources. One crucial memory bottleneck for the deployment stems from the\ncontext window. It is commonly recognized that model weights are memory hungry;\nhowever, the size of key-value embedding stored during the generation process\n(KV cache) can easily surpass the model size. The enormous size of the KV cache\nputs constraints on the inference batch size, which is crucial for high\nthroughput inference workload. Inspired by an interesting observation of the\nattention scores, we hypothesize the persistence of importance: only pivotal\ntokens, which had a substantial influence at one step, will significantly\ninfluence future generations. Based on our empirical verification and\ntheoretical analysis around this hypothesis, we propose Scissorhands, a system\nthat maintains the memory usage of the KV cache at a fixed budget without\nfinetuning the model. In essence, Scissorhands manages the KV cache by storing\nthe pivotal tokens with a higher probability. We validate that Scissorhands\nreduces the inference memory usage of the KV cache by up to 5X without\ncompromising model quality. We further demonstrate that Scissorhands can be\ncombined with 4-bit quantization, traditionally used to compress model weights,\nto achieve up to 20X compression.",
        "pdf_link": "https://arxiv.org/pdf/2305.17118v2.pdf"
    },
    {
        "title": "Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model",
        "authors": [
            "David Soong",
            "Sriram Sridhar",
            "Han Si",
            "Jan-Samuel Wagner",
            "Ana Caroline Costa Sá",
            "Christina Y Yu",
            "Kubra Karagoz",
            "Meijian Guan",
            "Hisham Hamadeh",
            "Brandon W Higgs"
        ],
        "published": "2023-05-26T17:33:05Z",
        "summary": "Large language models (LLMs) have made significant advancements in natural\nlanguage processing (NLP). Broad corpora capture diverse patterns but can\nintroduce irrelevance, while focused corpora enhance reliability by reducing\nmisleading information. Training LLMs on focused corpora poses computational\nchallenges. An alternative approach is to use a retrieval-augmentation (RetA)\nmethod tested in a specific domain.\n  To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a\ncustom RetA model were compared using 19 questions on diffuse large B-cell\nlymphoma (DLBCL) disease. Eight independent reviewers assessed responses based\non accuracy, relevance, and readability (rated 1-3).\n  The RetA model performed best in accuracy (12/19 3-point scores, total=47)\nand relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4\nreceived the highest readability scores (17/19, 55), followed by GPT-3 (15/19,\n53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34),\nrelevance (32), and readability (38).\n  Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared\nto the RetA model and Prometheus. Hallucinations were mostly associated with\nnon-existent references or fabricated efficacy data.\n  These findings suggest that RetA models, supplemented with domain-specific\ncorpora, may outperform general-purpose LLMs in accuracy and relevance within\nspecific domains. However, this evaluation was limited to specific questions\nand metrics and may not capture challenges in semantic search and other NLP\ntasks. Further research will explore different LLM architectures, RetA\nmethodologies, and evaluation methods to assess strengths and limitations more\ncomprehensively.",
        "pdf_link": "https://arxiv.org/pdf/2305.17116v2.pdf"
    },
    {
        "title": "Learning and Leveraging Verifiers to Improve Planning Capabilities of Pre-trained Language Models",
        "authors": [
            "Daman Arora",
            "Subbarao Kambhampati"
        ],
        "published": "2023-05-26T16:36:55Z",
        "summary": "There have been wide spread claims in the literature about the emergent\nreasoning capabilities of Pretrained Large Language Models. However, recent\nstudies, have found that their ability to plan remains questionable. Through\nour experiments using GPT-2, we empirically demonstrate that the performance of\na finetuned baseline remains poor because it violates pre-conditions of actions\nin the plans that it generates. To improve the planning capabilities of a\nfinetuned LLM, we train a verifier, which can classify actions as being valid\nor invalid in a particular state. By randomly sampling actions from the same\ndataset, we generate examples of invalid actions which are then used to train a\nverifier which can check for action applicability. In the presence of diverse\nsampling from a generator and a verifier which can prune invalid trajectories,\nwe show significant gains in the success rate on the Blocksworld domain.\nAdditionally, we show that finetuning the GPT-2 generator itself to create the\nverifier generalizes better than finetuning the base GPT-2. Lastly, we\ninvestigate the role of the sampling temperature which can be used to control\nthe exploration-exploitation tradeoff.",
        "pdf_link": "https://arxiv.org/pdf/2305.17077v1.pdf"
    },
    {
        "title": "LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations",
        "authors": [
            "Yudong Xu",
            "Wenhao Li",
            "Pashootan Vaezipoor",
            "Scott Sanner",
            "Elias B. Khalil"
        ],
        "published": "2023-05-26T16:32:17Z",
        "summary": "Can a Large Language Model (LLM) solve simple abstract reasoning problems? We\nexplore this broad question through a systematic analysis of GPT on the\nAbstraction and Reasoning Corpus (ARC), a representative benchmark of abstract\nreasoning ability from limited examples in which solutions require some \"core\nknowledge\" of concepts such as objects, goal states, counting, and basic\ngeometry. GPT-4 solves only 13/50 of the most straightforward ARC tasks when\nusing textual encodings for their two-dimensional input-output grids. Our\nfailure analysis reveals that GPT-4's capacity to identify objects and reason\nabout them is significantly influenced by the sequential nature of the text\nthat represents an object within a text encoding of a task. To test this\nhypothesis, we design a new benchmark, the 1D-ARC, which consists of\none-dimensional (array-like) tasks that are more conducive to GPT-based\nreasoning, and where it indeed performs better than on the (2D) ARC. To\nalleviate this issue, we propose an object-based representation that is\nobtained through an external tool, resulting in nearly doubling the performance\non solved ARC tasks and near-perfect scores on the easier 1D-ARC. Although the\nstate-of-the-art GPT-4 is unable to \"reason\" perfectly within non-language\ndomains such as the 1D-ARC or a simple ARC subset, our study reveals that the\nuse of object-based representations can significantly improve its reasoning\nability. Visualizations, GPT logs, and data are available at\nhttps://khalil-research.github.io/LLM4ARC.",
        "pdf_link": "https://arxiv.org/pdf/2305.18354v2.pdf"
    },
    {
        "title": "Mindstorms in Natural Language-Based Societies of Mind",
        "authors": [
            "Mingchen Zhuge",
            "Haozhe Liu",
            "Francesco Faccio",
            "Dylan R. Ashley",
            "Róbert Csordás",
            "Anand Gopalakrishnan",
            "Abdullah Hamdi",
            "Hasan Abed Al Kader Hammoud",
            "Vincent Herrmann",
            "Kazuki Irie",
            "Louis Kirsch",
            "Bing Li",
            "Guohao Li",
            "Shuming Liu",
            "Jinjie Mai",
            "Piotr Piękos",
            "Aditya Ramesh",
            "Imanol Schlag",
            "Weimin Shi",
            "Aleksandar Stanić",
            "Wenyi Wang",
            "Yuhui Wang",
            "Mengmeng Xu",
            "Deng-Ping Fan",
            "Bernard Ghanem",
            "Jürgen Schmidhuber"
        ],
        "published": "2023-05-26T16:21:25Z",
        "summary": "Both Minsky's \"society of mind\" and Schmidhuber's \"learning to think\" inspire\ndiverse societies of large multimodal neural networks (NNs) that solve problems\nby interviewing each other in a \"mindstorm.\" Recent implementations of NN-based\nsocieties of minds consist of large language models (LLMs) and other NN-based\nexperts communicating through a natural language interface. In doing so, they\novercome the limitations of single LLMs, improving multimodal zero-shot\nreasoning. In these natural language-based societies of mind (NLSOMs), new\nagents -- all communicating through the same universal symbolic language -- are\neasily added in a modular fashion. To demonstrate the power of NLSOMs, we\nassemble and experiment with several of them (having up to 129 members),\nleveraging mindstorms in them to solve some practical AI tasks: visual question\nanswering, image captioning, text-to-image synthesis, 3D generation, egocentric\nretrieval, embodied AI, and general language-based task solving. We view this\nas a starting point towards much larger NLSOMs with billions of agents-some of\nwhich may be humans. And with this emergence of great societies of\nheterogeneous minds, many new research questions have suddenly become paramount\nto the future of artificial intelligence. What should be the social structure\nof an NLSOM? What would be the (dis)advantages of having a monarchical rather\nthan a democratic structure? How can principles of NN economies be used to\nmaximize the total reward of a reinforcement learning NLSOM? In this work, we\nidentify, discuss, and try to answer some of these questions.",
        "pdf_link": "https://arxiv.org/pdf/2305.17066v1.pdf"
    },
    {
        "title": "A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks",
        "authors": [
            "Jacob Abernethy",
            "Alekh Agarwal",
            "Teodor V. Marinov",
            "Manfred K. Warmuth"
        ],
        "published": "2023-05-26T15:49:43Z",
        "summary": "We study the phenomenon of \\textit{in-context learning} (ICL) exhibited by\nlarge language models, where they can adapt to a new learning task, given a\nhandful of labeled examples, without any explicit parameter optimization. Our\ngoal is to explain how a pre-trained transformer model is able to perform ICL\nunder reasonable assumptions on the pre-training process and the downstream\ntasks. We posit a mechanism whereby a transformer can achieve the following:\n(a) receive an i.i.d. sequence of examples which have been converted into a\nprompt using potentially-ambiguous delimiters, (b) correctly segment the prompt\ninto examples and labels, (c) infer from the data a \\textit{sparse linear\nregressor} hypothesis, and finally (d) apply this hypothesis on the given test\nexample and return a predicted label. We establish that this entire procedure\nis implementable using the transformer mechanism, and we give sample complexity\nguarantees for this learning framework. Our empirical findings validate the\nchallenge of segmentation, and we show a correspondence between our posited\nmechanisms and observed attention maps for step (c).",
        "pdf_link": "https://arxiv.org/pdf/2305.17040v1.pdf"
    },
    {
        "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
        "authors": [
            "Gengze Zhou",
            "Yicong Hong",
            "Qi Wu"
        ],
        "published": "2023-05-26T14:41:06Z",
        "summary": "Trained with an unprecedented scale of data, large language models (LLMs)\nlike ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities\nfrom model scaling. Such a trend underscored the potential of training LLMs\nwith unlimited language data, advancing the development of a universal embodied\nagent. In this work, we introduce the NavGPT, a purely LLM-based\ninstruction-following navigation agent, to reveal the reasoning capability of\nGPT models in complex embodied scenes by performing zero-shot sequential action\nprediction for vision-and-language navigation (VLN). At each step, NavGPT takes\nthe textual descriptions of visual observations, navigation history, and future\nexplorable directions as inputs to reason the agent's current status, and makes\nthe decision to approach the target. Through comprehensive experiments, we\ndemonstrate NavGPT can explicitly perform high-level planning for navigation,\nincluding decomposing instruction into sub-goal, integrating commonsense\nknowledge relevant to navigation task resolution, identifying landmarks from\nobserved scenes, tracking navigation progress, and adapting to exceptions with\nplan adjustment. Furthermore, we show that LLMs is capable of generating\nhigh-quality navigational instructions from observations and actions along a\npath, as well as drawing accurate top-down metric trajectory given the agent's\nnavigation history. Despite the performance of using NavGPT to zero-shot R2R\ntasks still falling short of trained models, we suggest adapting multi-modality\ninputs for LLMs to use as visual navigation agents and applying the explicit\nreasoning of LLMs to benefit learning-based models.",
        "pdf_link": "https://arxiv.org/pdf/2305.16986v3.pdf"
    },
    {
        "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
        "authors": [
            "Yunqing Zhao",
            "Tianyu Pang",
            "Chao Du",
            "Xiao Yang",
            "Chongxuan Li",
            "Ngai-Man Cheung",
            "Min Lin"
        ],
        "published": "2023-05-26T13:49:44Z",
        "summary": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented\nperformance in response generation, especially with visual inputs, enabling\nmore creative and adaptable interaction than large language models such as\nChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable modality (e.g., vision). To this end, we propose evaluating the\nrobustness of open-source large VLMs in the most realistic and high-risk\nsetting, where adversaries have only black-box system access and seek to\ndeceive the model into returning the targeted responses. In particular, we\nfirst craft targeted adversarial examples against pretrained models such as\nCLIP and BLIP, and then transfer these adversarial examples to other VLMs such\nas MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we\nobserve that black-box queries on these VLMs can further improve the\neffectiveness of targeted evasion, resulting in a surprisingly high success\nrate for generating targeted responses. Our findings provide a quantitative\nunderstanding regarding the adversarial vulnerability of large VLMs and call\nfor a more thorough examination of their potential security flaws before\ndeployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.16934v2.pdf"
    },
    {
        "title": "Large Language Models Are Partially Primed in Pronoun Interpretation",
        "authors": [
            "Suet-Ying Lam",
            "Qingcheng Zeng",
            "Kexun Zhang",
            "Chenyu You",
            "Rob Voigt"
        ],
        "published": "2023-05-26T13:30:48Z",
        "summary": "While a large body of literature suggests that large language models (LLMs)\nacquire rich linguistic representations, little is known about whether they\nadapt to linguistic biases in a human-like way. The present study probes this\nquestion by asking whether LLMs display human-like referential biases using\nstimuli and procedures from real psycholinguistic experiments. Recent\npsycholinguistic studies suggest that humans adapt their referential biases\nwith recent exposure to referential patterns; closely replicating three\nrelevant psycholinguistic experiments from Johnson & Arnold (2022) in an\nin-context learning (ICL) framework, we found that InstructGPT adapts its\npronominal interpretations in response to the frequency of referential patterns\nin the local discourse, though in a limited fashion: adaptation was only\nobserved relative to syntactic but not semantic biases. By contrast, FLAN-UL2\nfails to generate meaningful patterns. Our results provide further evidence\nthat contemporary LLMs discourse representations are sensitive to syntactic\npatterns in the local context but less so to semantic patterns. Our data and\ncode are available at \\url{https://github.com/zkx06111/llm_priming}.",
        "pdf_link": "https://arxiv.org/pdf/2305.16917v1.pdf"
    },
    {
        "title": "Playing repeated games with Large Language Models",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "published": "2023-05-26T12:17:59Z",
        "summary": "Large Language Models (LLMs) are transforming society and permeating into\ndiverse applications. As a result, LLMs will frequently interact with us and\nother agents. It is, therefore, of great societal value to understand how LLMs\nbehave in interactive social settings. Here, we propose to use behavioral game\ntheory to study LLM's cooperation and coordination behavior. To do so, we let\ndifferent LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with\neach other and with other, human-like strategies. Our results show that LLMs\ngenerally perform well in such tasks and also uncover persistent behavioral\nsignatures. In a large set of two players-two strategies games, we find that\nLLMs are particularly good at games where valuing their own self-interest pays\noff, like the iterated Prisoner's Dilemma family. However, they behave\nsub-optimally in games that require coordination. We, therefore, further focus\non two games from these distinct families. In the canonical iterated Prisoner's\nDilemma, we find that GPT-4 acts particularly unforgivingly, always defecting\nafter another agent has defected only once. In the Battle of the Sexes, we find\nthat GPT-4 cannot match the behavior of the simple convention to alternate\nbetween options. We verify that these behavioral signatures are stable across\nrobustness checks. Finally, we show how GPT-4's behavior can be modified by\nproviding further information about the other player as well as by asking it to\npredict the other player's actions before making a choice. These results enrich\nour understanding of LLM's social behavior and pave the way for a behavioral\ngame theory for machines.",
        "pdf_link": "https://arxiv.org/pdf/2305.16867v1.pdf"
    },
    {
        "title": "HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis",
        "authors": [
            "Christoforos Vasilatos",
            "Manaar Alam",
            "Talal Rahwan",
            "Yasir Zaki",
            "Michail Maniatakos"
        ],
        "published": "2023-05-26T11:07:25Z",
        "summary": "As the use of Large Language Models (LLMs) in text generation tasks\nproliferates, concerns arise over their potential to compromise academic\nintegrity. The education sector currently tussles with distinguishing\nstudent-authored homework assignments from AI-generated ones. This paper\naddresses the challenge by introducing HowkGPT, designed to identify homework\nassignments generated by AI. HowkGPT is built upon a dataset of academic\nassignments and accompanying metadata [17] and employs a pretrained LLM to\ncompute perplexity scores for student-authored and ChatGPT-generated responses.\nThese scores then assist in establishing a threshold for discerning the origin\nof a submitted assignment. Given the specificity and contextual nature of\nacademic work, HowkGPT further refines its analysis by defining\ncategory-specific thresholds derived from the metadata, enhancing the precision\nof the detection. This study emphasizes the critical need for effective\nstrategies to uphold academic integrity amidst the growing influence of LLMs\nand provides an approach to ensuring fair and accurate grading in educational\ninstitutions.",
        "pdf_link": "https://arxiv.org/pdf/2305.18226v2.pdf"
    },
    {
        "title": "Do GPTs Produce Less Literal Translations?",
        "authors": [
            "Vikas Raunak",
            "Arul Menezes",
            "Matt Post",
            "Hany Hassan Awadalla"
        ],
        "published": "2023-05-26T10:38:31Z",
        "summary": "Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose\nlanguage models capable of addressing many natural language generation or\nunderstanding tasks. On the task of Machine Translation (MT), multiple works\nhave investigated few-shot prompting mechanisms to elicit better translations\nfrom LLMs. However, there has been relatively little investigation on how such\ntranslations differ qualitatively from the translations generated by standard\nNeural Machine Translation (NMT) models. In this work, we investigate these\ndifferences in terms of the literalness of translations produced by the two\nsystems. Using literalness measures involving word alignment and monotonicity,\nwe find that translations out of English (E-X) from GPTs tend to be less\nliteral, while exhibiting similar or better scores on MT quality metrics. We\ndemonstrate that this finding is borne out in human evaluations as well. We\nthen show that these differences are especially pronounced when translating\nsentences that contain idiomatic expressions.",
        "pdf_link": "https://arxiv.org/pdf/2305.16806v4.pdf"
    },
    {
        "title": "Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media",
        "authors": [
            "Loukas Ilias",
            "Spiros Mouzakitis",
            "Dimitris Askounis"
        ],
        "published": "2023-05-26T10:19:04Z",
        "summary": "In today's fast-paced world, the rates of stress and depression present a\nsurge. Social media provide assistance for the early detection of mental health\nconditions. Existing methods mainly introduce feature extraction approaches and\ntrain shallow machine learning classifiers. Other researches use deep neural\nnetworks or transformers. Despite the fact that transformer-based models\nachieve noticeable improvements, they cannot often capture rich factual\nknowledge. Although there have been proposed a number of studies aiming to\nenhance the pretrained transformer-based models with extra information or\nadditional modalities, no prior work has exploited these modifications for\ndetecting stress and depression through social media. In addition, although the\nreliability of a machine learning model's confidence in its predictions is\ncritical for high-risk applications, there is no prior work taken into\nconsideration the model calibration. To resolve the above issues, we present\nthe first study in the task of depression and stress detection in social media,\nwhich injects extra linguistic information in transformer-based models, namely\nBERT and MentalBERT. Specifically, the proposed approach employs a Multimodal\nAdaptation Gate for creating the combined embeddings, which are given as input\nto a BERT (or MentalBERT) model. For taking into account the model calibration,\nwe apply label smoothing. We test our proposed approaches in three publicly\navailable datasets and demonstrate that the integration of linguistic features\ninto transformer-based models presents a surge in the performance. Also, the\nusage of label smoothing contributes to both the improvement of the model's\nperformance and the calibration of the model. We finally perform a linguistic\nanalysis of the posts and show differences in language between stressful and\nnon-stressful texts, as well as depressive and non-depressive posts.",
        "pdf_link": "https://arxiv.org/pdf/2305.16797v2.pdf"
    },
    {
        "title": "Distinguishing Human Generated Text From ChatGPT Generated Text Using Machine Learning",
        "authors": [
            "Niful Islam",
            "Debopom Sutradhar",
            "Humaira Noor",
            "Jarin Tasnim Raya",
            "Monowara Tabassum Maisha",
            "Dewan Md Farid"
        ],
        "published": "2023-05-26T09:27:43Z",
        "summary": "ChatGPT is a conversational artificial intelligence that is a member of the\ngenerative pre-trained transformer of the large language model family. This\ntext generative model was fine-tuned by both supervised learning and\nreinforcement learning so that it can produce text documents that seem to be\nwritten by natural intelligence. Although there are numerous advantages of this\ngenerative model, it comes with some reasonable concerns as well. This paper\npresents a machine learning-based solution that can identify the ChatGPT\ndelivered text from the human written text along with the comparative analysis\nof a total of 11 machine learning and deep learning algorithms in the\nclassification process. We have tested the proposed model on a Kaggle dataset\nconsisting of 10,000 texts out of which 5,204 texts were written by humans and\ncollected from news and social media. On the corpus generated by GPT-3.5, the\nproposed algorithm presents an accuracy of 77%.",
        "pdf_link": "https://arxiv.org/pdf/2306.01761v1.pdf"
    },
    {
        "title": "Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification",
        "authors": [
            "Nicolò Tamagnone",
            "Selim Fekih",
            "Ximena Contla",
            "Nayid Orozco",
            "Navid Rekabsaz"
        ],
        "published": "2023-05-26T09:15:05Z",
        "summary": "Accurate and rapid situation analysis during humanitarian crises is critical\nto delivering humanitarian aid efficiently and is fundamental to humanitarian\nimperatives and the Leave No One Behind (LNOB) principle. This data analysis\ncan highly benefit from language processing systems, e.g., by classifying the\ntext data according to a humanitarian ontology. However, approaching this by\nsimply fine-tuning a generic large language model (LLM) involves considerable\npractical and ethical issues, particularly the lack of effectiveness on\ndata-sparse and complex subdomains, and the encoding of societal biases and\nunwanted associations. In this work, we aim to provide an effective and\nethically-aware system for humanitarian data analysis. We approach this by (1)\nintroducing a novel architecture adjusted to the humanitarian analysis\nframework, (2) creating and releasing a novel humanitarian-specific LLM called\nHumBert, and (3) proposing a systematic way to measure and mitigate biases. Our\nexperiments' results show the better performance of our approach on zero-shot\nand full-training settings in comparison with strong baseline models, while\nalso revealing the existence of biases in the resulting LLMs. Utilizing a\ntargeted counterfactual data augmentation approach, we significantly reduce\nthese biases without compromising performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.16756v2.pdf"
    },
    {
        "title": "Can large language models generate salient negative statements?",
        "authors": [
            "Hiba Arnaout",
            "Simon Razniewski"
        ],
        "published": "2023-05-26T09:13:59Z",
        "summary": "We examine the ability of large language models (LLMs) to generate salient\n(interesting) negative statements about real-world entities; an emerging\nresearch topic of the last few years. We probe the LLMs using zero- and k-shot\nunconstrained probes, and compare with traditional methods for negation\ngeneration, i.e., pattern-based textual extractions and knowledge-graph-based\ninferences, as well as crowdsourced gold statements. We measure the correctness\nand salience of the generated lists about subjects from different domains. Our\nevaluation shows that guided probes do in fact improve the quality of generated\nnegatives, compared to the zero-shot variant. Nevertheless, using both prompts,\nLLMs still struggle with the notion of factuality of negatives, frequently\ngenerating many ambiguous statements, or statements with negative keywords but\na positive meaning.",
        "pdf_link": "https://arxiv.org/pdf/2305.16755v2.pdf"
    },
    {
        "title": "A Closer Look at In-Context Learning under Distribution Shifts",
        "authors": [
            "Kartik Ahuja",
            "David Lopez-Paz"
        ],
        "published": "2023-05-26T07:47:21Z",
        "summary": "In-context learning, a capability that enables a model to learn from input\nexamples on the fly without necessitating weight updates, is a defining\ncharacteristic of large language models. In this work, we follow the setting\nproposed in (Garg et al., 2022) to better understand the generality and\nlimitations of in-context learning from the lens of the simple yet fundamental\ntask of linear regression. The key question we aim to address is: Are\ntransformers more adept than some natural and simpler architectures at\nperforming in-context learning under varying distribution shifts? To compare\ntransformers, we propose to use a simple architecture based on set-based\nMulti-Layer Perceptrons (MLPs). We find that both transformers and set-based\nMLPs exhibit in-context learning under in-distribution evaluations, but\ntransformers more closely emulate the performance of ordinary least squares\n(OLS). Transformers also display better resilience to mild distribution shifts,\nwhere set-based MLPs falter. However, under severe distribution shifts, both\nmodels' in-context learning abilities diminish.",
        "pdf_link": "https://arxiv.org/pdf/2305.16704v1.pdf"
    },
    {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Lingkai Kong",
            "Bo Dai",
            "Chao Zhang"
        ],
        "published": "2023-05-26T05:52:27Z",
        "summary": "Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.16653v1.pdf"
    },
    {
        "title": "TADA: Task-Agnostic Dialect Adapters for English",
        "authors": [
            "Will Held",
            "Caleb Ziems",
            "Diyi Yang"
        ],
        "published": "2023-05-26T05:45:03Z",
        "summary": "Large Language Models, the dominant starting point for Natural Language\nProcessing (NLP) applications, fail at a higher rate for speakers of English\ndialects other than Standard American English (SAE). Prior work addresses this\nusing task-specific data or synthetic data augmentation, both of which require\nintervention for each dialect and task pair. This poses a scalability issue\nthat prevents the broad adoption of robust dialectal English NLP. We introduce\na simple yet effective method for task-agnostic dialect adaptation by aligning\nnon-SAE dialects using adapters and composing them with task-specific adapters\nfrom SAE. Task-Agnostic Dialect Adapters (TADA) improve dialectal robustness on\n4 dialectal variants of the GLUE benchmark without task-specific supervision.",
        "pdf_link": "https://arxiv.org/pdf/2305.16651v1.pdf"
    },
    {
        "title": "Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing",
        "authors": [
            "Jaehun Jung",
            "Peter West",
            "Liwei Jiang",
            "Faeze Brahman",
            "Ximing Lu",
            "Jillian Fisher",
            "Taylor Sorensen",
            "Yejin Choi"
        ],
        "published": "2023-05-26T05:19:24Z",
        "summary": "We present Impossible Distillation, a novel framework for paraphrasing and\nsentence summarization, that distills a high-quality dataset and model from a\nlow-quality teacher that itself cannot perform these tasks. Unlike prior works\nthat rely on an extreme-scale teacher model (e.g., GPT3) or task-specific\narchitecture, we hypothesize and verify the paraphrastic proximity intrinsic to\npre-trained LMs (e.g., GPT2), where paraphrases occupy a proximal subspace in\nthe LM distribution. By identifying and distilling generations from these\nsubspaces, Impossible Distillation produces a high-quality dataset and model\neven from GPT2-scale LMs. We evaluate our method on multiple benchmarks\nspanning unconstrained / syntax-controlled paraphrase generation and sentence\nsummarization. Our model with 770M parameters consistently outperforms strong\nbaselines, including models distilled from ChatGPT, and sometimes, even ChatGPT\nitself. Also, we find that our distilled dataset from 1.5B LMs exhibits higher\ndiversity and fidelity than up to 13 times larger datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.16635v3.pdf"
    },
    {
        "title": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks",
        "authors": [
            "Agam Shah",
            "Sudheer Chava"
        ],
        "published": "2023-05-26T05:13:01Z",
        "summary": "Recently large language models (LLMs) like ChatGPT have shown impressive\nperformance on many natural language processing tasks with zero-shot. In this\npaper, we investigate the effectiveness of zero-shot LLMs in the financial\ndomain. We compare the performance of ChatGPT along with some open-source\ngenerative LLMs in zero-shot mode with RoBERTa fine-tuned on annotated data. We\naddress three inter-related research questions on data annotation, performance\ngaps, and the feasibility of employing generative models in the finance domain.\nOur findings demonstrate that ChatGPT performs well even without labeled data\nbut fine-tuned models generally outperform it. Our research also highlights how\nannotating with generative models can be time-intensive. Our codebase is\npublicly available on GitHub under CC BY-NC 4.0 license.",
        "pdf_link": "https://arxiv.org/pdf/2305.16633v1.pdf"
    },
    {
        "title": "Evaluation of Question Generation Needs More References",
        "authors": [
            "Shinhyeok Oh",
            "Hyojun Go",
            "Hyeongdon Moon",
            "Yunsung Lee",
            "Myeongho Jeong",
            "Hyun Seung Lee",
            "Seungtaek Choi"
        ],
        "published": "2023-05-26T04:40:56Z",
        "summary": "Question generation (QG) is the task of generating a valid and fluent\nquestion based on a given context and the target answer. According to various\npurposes, even given the same context, instructors can ask questions about\ndifferent concepts, and even the same concept can be written in different ways.\nHowever, the evaluation for QG usually depends on single reference-based\nsimilarity metrics, such as n-gram-based metric or learned metric, which is not\nsufficient to fully evaluate the potential of QG methods. To this end, we\npropose to paraphrase the reference question for a more robust QG evaluation.\nUsing large language models such as GPT-3, we created semantically and\nsyntactically diverse questions, then adopt the simple aggregation of the\npopular evaluation metrics as the final scores. Through our experiments, we\nfound that using multiple (pseudo) references is more effective for QG\nevaluation while showing a higher correlation with human evaluations than\nevaluation with a single reference.",
        "pdf_link": "https://arxiv.org/pdf/2305.16626v1.pdf"
    },
    {
        "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model",
        "authors": [
            "Zhijie Deng",
            "Hongcheng Gao",
            "Yibo Miao",
            "Hao Zhang"
        ],
        "published": "2023-05-26T04:23:10Z",
        "summary": "The detection of machine-generated text, especially from large language\nmodels (LLMs), is crucial in preventing serious social problems resulting from\ntheir misuse. Some methods train dedicated detectors on specific datasets but\nfall short in generalizing to unseen test data, while other zero-shot ones\noften yield suboptimal performance. Although the recent DetectGPT has shown\npromising detection performance, it suffers from significant inefficiency\nissues, as detecting a single candidate requires scoring hundreds of its\nperturbations with the source LLM. This paper aims to bridge this gap.\nTechnically, we propose to incorporate a Bayesian surrogate model, which allows\nus to select typical samples based on Bayesian uncertainty and interpolate\nscores from typical samples to other ones, to improve query efficiency. Our\nempirical results demonstrate that our method significantly outperforms\nexisting approaches under a low query budget. Notably, our method achieves\nsimilar performance with up to 2 times fewer queries than DetectGPT and 3.7%\nhigher AUROC at a query number of 5.",
        "pdf_link": "https://arxiv.org/pdf/2305.16617v1.pdf"
    },
    {
        "title": "AaKOS: Aspect-adaptive Knowledge-based Opinion Summarization",
        "authors": [
            "Guan Wang",
            "Weihua Li",
            "Edmund M-K. Lai",
            "Quan Bai"
        ],
        "published": "2023-05-26T03:44:35Z",
        "summary": "The rapid growth of information on the Internet has led to an overwhelming\namount of opinions and comments on various activities, products, and services.\nThis makes it difficult and time-consuming for users to process all the\navailable information when making decisions. Text summarization, a Natural\nLanguage Processing (NLP) task, has been widely explored to help users quickly\nretrieve relevant information by generating short and salient content from long\nor multiple documents. Recent advances in pre-trained language models, such as\nChatGPT, have demonstrated the potential of Large Language Models (LLMs) in\ntext generation. However, LLMs require massive amounts of data and resources\nand are challenging to implement as offline applications. Furthermore, existing\ntext summarization approaches often lack the ``adaptive\" nature required to\ncapture diverse aspects in opinion summarization, which is particularly\ndetrimental to users with specific requirements or preferences. In this paper,\nwe propose an Aspect-adaptive Knowledge-based Opinion Summarization model for\nproduct reviews, which effectively captures the adaptive nature required for\nopinion summarization. The model generates aspect-oriented summaries given a\nset of reviews for a particular product, efficiently providing users with\nuseful information on specific aspects they are interested in, ensuring the\ngenerated summaries are more personalized and informative. Extensive\nexperiments have been conducted using real-world datasets to evaluate the\nproposed model. The results demonstrate that our model outperforms\nstate-of-the-art approaches and is adaptive and efficient in generating\nsummaries that focus on particular aspects, enabling users to make\nwell-informed decisions and catering to their diverse interests and\npreferences.",
        "pdf_link": "https://arxiv.org/pdf/2306.05537v1.pdf"
    },
    {
        "title": "Heterogeneous Value Alignment Evaluation for Large Language Models",
        "authors": [
            "Zhaowei Zhang",
            "Ceyao Zhang",
            "Nian Liu",
            "Siyuan Qi",
            "Ziqi Rong",
            "Song-Chun Zhu",
            "Shuguang Cui",
            "Yaodong Yang"
        ],
        "published": "2023-05-26T02:34:20Z",
        "summary": "The emergent capabilities of Large Language Models (LLMs) have made it\ncrucial to align their values with those of humans. However, current\nmethodologies typically attempt to assign value as an attribute to LLMs, yet\nlack attention to the ability to pursue value and the importance of\ntransferring heterogeneous values in specific practical applications. In this\npaper, we propose a Heterogeneous Value Alignment Evaluation (HVAE) system,\ndesigned to assess the success of aligning LLMs with heterogeneous values.\nSpecifically, our approach first brings the Social Value Orientation (SVO)\nframework from social psychology, which corresponds to how much weight a person\nattaches to the welfare of others in relation to their own. We then assign the\nLLMs with different social values and measure whether their behaviors align\nwith the inducing values. We conduct evaluations with new auto-metric\n\\textit{value rationality} to represent the ability of LLMs to align with\nspecific values. Evaluating the value rationality of five mainstream LLMs, we\ndiscern a propensity in LLMs towards neutral values over pronounced personal\nvalues. By examining the behavior of these LLMs, we contribute to a deeper\ninsight into the value alignment of LLMs within a heterogeneous value system.",
        "pdf_link": "https://arxiv.org/pdf/2305.17147v3.pdf"
    },
    {
        "title": "Neural Task Synthesis for Visual Programming",
        "authors": [
            "Victor-Alexandru Pădurean",
            "Georgios Tzannetos",
            "Adish Singla"
        ],
        "published": "2023-05-26T01:08:18Z",
        "summary": "Generative neural models hold great promise in enhancing programming\neducation by synthesizing new content. We seek to design neural models that can\nautomatically generate programming tasks for a given specification in the\ncontext of visual programming domains. Despite the recent successes of large\ngenerative models like GPT-4, our initial results show that these models are\nineffective in synthesizing visual programming tasks and struggle with logical\nand spatial reasoning. We propose a novel neuro-symbolic technique,\nNeurTaskSyn, that can synthesize programming tasks for a specification given in\nthe form of desired programming concepts exercised by its solution code and\nconstraints on the visual task. NeurTaskSyn has two components: the first\ncomponent is trained via imitation learning procedure to generate possible\nsolution codes, and the second component is trained via reinforcement learning\nprocedure to guide an underlying symbolic execution engine that generates\nvisual tasks for these codes. We demonstrate the effectiveness of NeurTaskSyn\nthrough an extensive empirical evaluation and a qualitative study on reference\ntasks taken from the Hour of Code: Classic Maze challenge by Code-dot-org and\nthe Intro to Programming with Karel course by CodeHS-dot-com.",
        "pdf_link": "https://arxiv.org/pdf/2305.18342v3.pdf"
    },
    {
        "title": "CONA: A novel CONtext-Aware instruction paradigm for communication using large language model",
        "authors": [
            "Nan Zhou",
            "Xinghui Tao",
            "Xi Chen"
        ],
        "published": "2023-05-26T00:53:18Z",
        "summary": "We introduce CONA, a novel context-aware instruction paradigm for effective\nknowledge dissemination using generative pre-trained transformer (GPT) models.\nCONA is a flexible framework designed to leverage the capabilities of Large\nLanguage Models (LLMs) and incorporate DIKW (Data, Information, Knowledge,\nWisdom) hierarchy to automatically instruct and optimise presentation content,\nanticipate potential audience inquiries, and provide context-aware answers that\nadaptive to the knowledge level of the audience group. The unique aspect of the\nCONA paradigm lies in its combination of an independent advisory mechanism and\na recursive feedback loop rooted on the DIKW hierarchy. This synergy\nsignificantly enhances context-aware contents, ensuring they are accessible and\neasily comprehended by the audience. This paradigm is an early pioneer to\nexplore new methods for knowledge dissemination and communication in the LLM\nera, offering effective support for everyday knowledge sharing scenarios. We\nconduct experiments on a range of audience roles, along with materials from\nvarious disciplines using GPT4. Both quantitative and qualitative results\ndemonstrated that the proposed CONA paradigm achieved remarkable performance\ncompared to the outputs guided by conventional prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2305.18620v1.pdf"
    },
    {
        "title": "The Dangers of trusting Stochastic Parrots: Faithfulness and Trust in Open-domain Conversational Question Answering",
        "authors": [
            "Sabrina Chiesurin",
            "Dimitris Dimakopoulos",
            "Marco Antonio Sobrevilla Cabezudo",
            "Arash Eshghi",
            "Ioannis Papaioannou",
            "Verena Rieser",
            "Ioannis Konstas"
        ],
        "published": "2023-05-25T22:54:13Z",
        "summary": "Large language models are known to produce output which sounds fluent and\nconvincing, but is also often wrong, e.g. \"unfaithful\" with respect to a\nrationale as retrieved from a knowledge base. In this paper, we show that\ntask-based systems which exhibit certain advanced linguistic dialog behaviors,\nsuch as lexical alignment (repeating what the user said), are in fact preferred\nand trusted more, whereas other phenomena, such as pronouns and ellipsis are\ndis-preferred. We use open-domain question answering systems as our test-bed\nfor task based dialog generation and compare several open- and closed-book\nmodels. Our results highlight the danger of systems that appear to be\ntrustworthy by parroting user input while providing an unfaithful response.",
        "pdf_link": "https://arxiv.org/pdf/2305.16519v1.pdf"
    },
    {
        "title": "On the Tool Manipulation Capability of Open-source Large Language Models",
        "authors": [
            "Qiantong Xu",
            "Fenglu Hong",
            "Bo Li",
            "Changran Hu",
            "Zhengyu Chen",
            "Jian Zhang"
        ],
        "published": "2023-05-25T22:10:20Z",
        "summary": "Recent studies on software tool manipulation with large language models\n(LLMs) mostly rely on closed model APIs. The industrial adoption of these\nmodels is substantially constrained due to the security and robustness risks in\nexposing information to closed LLM API services. In this paper, we ask can we\nenhance open-source LLMs to be competitive to leading closed LLM APIs in tool\nmanipulation, with practical amount of human supervision. By analyzing common\ntool manipulation failures, we first demonstrate that open-source LLMs may\nrequire training with usage examples, in-context demonstration and generation\nstyle regulation to resolve failures. These insights motivate us to revisit\nclassical methods in LLM literature, and demonstrate that we can adapt them as\nmodel alignment with programmatic data generation, system prompts and\nin-context demonstration retrievers to enhance open-source LLMs for tool\nmanipulation. To evaluate these techniques, we create the ToolBench, a tool\nmanipulation benchmark consisting of diverse software tools for real-world\ntasks. We demonstrate that our techniques can boost leading open-source LLMs by\nup to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4\nout of 8 ToolBench tasks. We show that such enhancement typically requires\nabout one developer day to curate data for each tool, rendering a recipe with\npractical amount of human supervision.",
        "pdf_link": "https://arxiv.org/pdf/2305.16504v1.pdf"
    },
    {
        "title": "Coarse-Tuning Models of Code with Reinforcement Learning Feedback",
        "authors": [
            "Abhinav Jain",
            "Chima Adiole",
            "Swarat Chaudhuri",
            "Thomas Reps",
            "Chris Jermaine"
        ],
        "published": "2023-05-25T22:09:08Z",
        "summary": "Large Language Models (LLMs) pre-trained on code have recently emerged as the\ndominant approach to program synthesis. However, these models are trained using\nnext-token prediction, which ignores the syntax and semantics of code. We\npropose RLCF, that further trains a pre-trained LLM via reinforcement learning,\nusing feedback from a grounding function that scores the quality of the code.\nThe grounding function uses (i) compiler-derived feedback on whether the code\nit generates passes a set of correctness checks; and (ii) feedback from a\ndifferent LLM that compares the generated code to a reference code. RLCF is\nmodel- and language-agnostic. We empirically evaluate it on the MBJP and MathQA\ntasks for Java. Our experiments show that RLCF raises the odds that an\nLLM-generated program compiles, is executable, and produces the right output on\ntests, often allowing LLMs to match the performance of 2x-8x larger LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.18341v2.pdf"
    },
    {
        "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training",
        "authors": [
            "Federico Cassano",
            "Ming-Ho Yee",
            "Noah Shinn",
            "Arjun Guha",
            "Steven Holtzen"
        ],
        "published": "2023-05-25T21:16:09Z",
        "summary": "TypeScript and Python are two programming languages that support optional\ntype annotations, which are useful but tedious to introduce and maintain. This\nhas motivated automated type prediction: given an untyped program, produce a\nwell-typed output program. Large language models (LLMs) are promising for type\nprediction, but there are challenges: fill-in-the-middle performs poorly,\nprograms may not fit into the context window, generated types may not type\ncheck, and it is difficult to measure how well-typed the output program is. We\naddress these challenges by building OpenTau, a search-based approach for type\nprediction that leverages large language models. We propose a new metric for\ntype prediction quality, give a tree-based program decomposition that searches\na space of generated types, and present fill-in-the-type fine-tuning for LLMs.\nWe evaluate our work with a new dataset for TypeScript type prediction, and\nshow that 47.4% of files type check (14.5% absolute improvement) with an\noverall rate of 3.3 type errors per file. All code, data, and models are\navailable at: https://github.com/GammaTauAI/opentau.",
        "pdf_link": "https://arxiv.org/pdf/2305.17145v1.pdf"
    },
    {
        "title": "Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models",
        "authors": [
            "Isabelle Lorge",
            "Janet Pierrehumbert"
        ],
        "published": "2023-05-25T18:56:26Z",
        "summary": "Vector space models of word meaning all share the assumption that words\noccurring in similar contexts have similar meanings. In such models, words that\nare similar in their topical associations but differ in their logical force\ntend to emerge as semantically close, creating well-known challenges for NLP\napplications that involve logical reasoning. Modern pretrained language models,\nsuch as BERT, RoBERTa and GPT-3 hold the promise of performing better on\nlogical tasks than classic static word embeddings. However, reports are mixed\nabout their success. In the current paper, we advance this discussion through a\nsystematic study of scalar adverbs, an under-explored class of words with\nstrong logical force. Using three different tasks, involving both naturalistic\nsocial media data and constructed examples, we investigate the extent to which\nBERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these\ncommon words. We ask: 1) Do the models distinguish amongst the three semantic\ncategories of MODALITY, FREQUENCY and DEGREE? 2) Do they have implicit\nrepresentations of full scales from maximally negative to maximally positive?\n3) How do word frequency and contextual factors impact model performance? We\nfind that despite capturing some aspects of logical meaning, the models fall\nfar short of human performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.16426v2.pdf"
    },
    {
        "title": "Context-aware attention layers coupled with optimal transport domain adaptation and multimodal fusion methods for recognizing dementia from spontaneous speech",
        "authors": [
            "Loukas Ilias",
            "Dimitris Askounis"
        ],
        "published": "2023-05-25T18:18:09Z",
        "summary": "Alzheimer's disease (AD) constitutes a complex neurocognitive disease and is\nthe main cause of dementia. Although many studies have been proposed targeting\nat diagnosing dementia through spontaneous speech, there are still limitations.\nExisting state-of-the-art approaches, which propose multimodal methods, train\nseparately language and acoustic models, employ majority-vote approaches, and\nconcatenate the representations of the different modalities either at the input\nlevel, i.e., early fusion, or during training. Also, some of them employ\nself-attention layers, which calculate the dependencies between representations\nwithout considering the contextual information. In addition, no prior work has\ntaken into consideration the model calibration. To address these limitations,\nwe propose some new methods for detecting AD patients, which capture the intra-\nand cross-modal interactions. First, we convert the audio files into log-Mel\nspectrograms, their delta, and delta-delta and create in this way an image per\naudio file consisting of three channels. Next, we pass each transcript and\nimage through BERT and DeiT models respectively. After that, context-based\nself-attention layers, self-attention layers with a gate model, and optimal\ntransport domain adaptation methods are employed for capturing the intra- and\ninter-modal interactions. Finally, we exploit two methods for fusing the self\nand cross-attention features. For taking into account the model calibration, we\napply label smoothing. We use both performance and calibration metrics.\nExperiments conducted on the ADReSS and ADReSSo Challenge datasets indicate the\nefficacy of our introduced approaches over existing research initiatives with\nour best performing model reaching Accuracy and F1-score up to 91.25% and\n91.06% respectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.16406v2.pdf"
    },
    {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "published": "2023-05-25T17:59:49Z",
        "summary": "The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular \"ObtainDiamond\" task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe \"ObtainDiamond\" task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the \"ObtainDiamond\" task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.",
        "pdf_link": "https://arxiv.org/pdf/2305.17144v2.pdf"
    },
    {
        "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
        "authors": [
            "Amirkeivan Mohtashami",
            "Martin Jaggi"
        ],
        "published": "2023-05-25T17:53:42Z",
        "summary": "While Transformers have shown remarkable success in natural language\nprocessing, their attention mechanism's large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model's attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system's memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity to over 32k tokens, allowing\nfor inference at the context lengths of GPT-4. We release the implementation of\nlandmark attention and the code to reproduce our experiments at\nhttps://github.com/epfml/landmark-attention/.",
        "pdf_link": "https://arxiv.org/pdf/2305.16300v2.pdf"
    },
    {
        "title": "Transformative Effects of ChatGPT on Modern Education: Emerging Era of AI Chatbots",
        "authors": [
            "Sukhpal Singh Gill",
            "Minxian Xu",
            "Panos Patros",
            "Huaming Wu",
            "Rupinder Kaur",
            "Kamalpreet Kaur",
            "Stephanie Fuller",
            "Manmeet Singh",
            "Priyansh Arora",
            "Ajith Kumar Parlikad",
            "Vlado Stankovski",
            "Ajith Abraham",
            "Soumya K. Ghosh",
            "Hanan Lutfiyya",
            "Salil S. Kanhere",
            "Rami Bahsoon",
            "Omer Rana",
            "Schahram Dustdar",
            "Rizos Sakellariou",
            "Steve Uhlig",
            "Rajkumar Buyya"
        ],
        "published": "2023-05-25T17:35:57Z",
        "summary": "ChatGPT, an AI-based chatbot, was released to provide coherent and useful\nreplies based on analysis of large volumes of data. In this article, leading\nscientists, researchers and engineers discuss the transformative effects of\nChatGPT on modern education. This research seeks to improve our knowledge of\nChatGPT capabilities and its use in the education sector, identifying potential\nconcerns and challenges. Our preliminary evaluation concludes that ChatGPT\nperformed differently in each subject area including finance, coding and maths.\nWhile ChatGPT has the ability to help educators by creating instructional\ncontent, offering suggestions and acting as an online educator to learners by\nanswering questions and promoting group work, there are clear drawbacks in its\nuse, such as the possibility of producing inaccurate or false data and\ncircumventing duplicate content (plagiarism) detectors where originality is\nessential. The often reported hallucinations within Generative AI in general,\nand also relevant for ChatGPT, can render its use of limited benefit where\naccuracy is essential. What ChatGPT lacks is a stochastic measure to help\nprovide sincere and sensitive communication with its users. Academic\nregulations and evaluation practices used in educational institutions need to\nbe updated, should ChatGPT be used as a tool in education. To address the\ntransformative effects of ChatGPT on the learning environment, educating\nteachers and students alike about its capabilities and limitations will be\ncrucial.",
        "pdf_link": "https://arxiv.org/pdf/2306.03823v1.pdf"
    },
    {
        "title": "UFO: Unified Fact Obtaining for Commonsense Question Answering",
        "authors": [
            "Zhifeng Li",
            "Yifan Fan",
            "Bowei Zou",
            "Yu Hong"
        ],
        "published": "2023-05-25T13:25:49Z",
        "summary": "Leveraging external knowledge to enhance the reasoning ability is crucial for\ncommonsense question answering. However, the existing knowledge bases heavily\nrely on manual annotation which unavoidably causes deficiency in coverage of\nworld-wide commonsense knowledge. Accordingly, the knowledge bases fail to be\nflexible enough to support the reasoning over diverse questions. Recently,\nlarge-scale language models (LLMs) have dramatically improved the intelligence\nin capturing and leveraging knowledge, which opens up a new way to address the\nissue of eliciting knowledge from language models. We propose a Unified Facts\nObtaining (UFO) approach. UFO turns LLMs into knowledge sources and produces\nrelevant facts (knowledge statements) for the given question. We first develop\na unified prompt consisting of demonstrations that cover different aspects of\ncommonsense and different question styles. On this basis, we instruct the LLMs\nto generate question-related supporting facts for various commonsense questions\nvia prompting. After facts generation, we apply a dense retrieval-based fact\nselection strategy to choose the best-matched fact. This kind of facts will be\nfed into the answer inference model along with the question. Notably, due to\nthe design of unified prompts, UFO can support reasoning in various commonsense\naspects (including general commonsense, scientific commonsense, and social\ncommonsense). Extensive experiments on CommonsenseQA 2.0, OpenBookQA, QASC, and\nSocial IQA benchmarks show that UFO significantly improves the performance of\nthe inference model and outperforms manually constructed knowledge sources.",
        "pdf_link": "https://arxiv.org/pdf/2305.16048v1.pdf"
    },
    {
        "title": "ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs",
        "authors": [
            "Zihao Zhao",
            "Sheng Wang",
            "Jinchen Gu",
            "Yitao Zhu",
            "Lanzhuju Mei",
            "Zixu Zhuang",
            "Zhiming Cui",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "published": "2023-05-25T12:03:31Z",
        "summary": "The integration of Computer-Assisted Diagnosis (CAD) with Large Language\nModels (LLMs) holds great potential in clinical applications, specifically in\nthe roles of virtual family doctors and clinic assistants. However, current\nworks in this field are plagued by limitations, specifically a restricted scope\nof applicable image domains and the provision of unreliable medical advice.\nThis restricts their overall processing capabilities. Furthermore, the mismatch\nin writing style between LLMs and radiologists undermines their practical\nusefulness. To tackle these challenges, we introduce ChatCAD+, which is\ndesigned to be universal and reliable. It is capable of handling medical images\nfrom diverse domains and leveraging up-to-date information from reputable\nmedical websites to provide reliable medical advice. Additionally, it\nincorporates a template retrieval system that improves report generation\nperformance via exemplar reports. This approach ensures greater consistency\nwith the expertise of human professionals. The source code is available at\nhttps://github.com/zhaozh10/ChatCAD.",
        "pdf_link": "https://arxiv.org/pdf/2305.15964v4.pdf"
    },
    {
        "title": "Linguistic Properties of Truthful Response",
        "authors": [
            "Bruce W. Lee",
            "Benedict Florance Arockiaraj",
            "Helen Jin"
        ],
        "published": "2023-05-25T09:17:39Z",
        "summary": "We investigate the phenomenon of an LLM's untruthful response using a large\nset of 220 handcrafted linguistic features. We focus on GPT-3 models and find\nthat the linguistic profiles of responses are similar across model sizes. That\nis, how varying-sized LLMs respond to given prompts stays similar on the\nlinguistic properties level. We expand upon this finding by training support\nvector machines that rely only upon the stylistic components of model responses\nto classify the truthfulness of statements. Though the dataset size limits our\ncurrent findings, we show the possibility that truthfulness detection is\npossible without evaluating the content itself. But at the same time, the\nlimited scope of our experiments must be taken into account in interpreting the\nresults.",
        "pdf_link": "https://arxiv.org/pdf/2305.15875v2.pdf"
    },
    {
        "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
        "authors": [
            "Niels Mündler",
            "Jingxuan He",
            "Slobodan Jenko",
            "Martin Vechev"
        ],
        "published": "2023-05-25T08:43:46Z",
        "summary": "Large language models (large LMs) are susceptible to producing text that\ncontains hallucinated content. An important instance of this problem is\nself-contradiction, where the LM generates two contradictory sentences within\nthe same context. In this work, we present a comprehensive investigation into\nself-contradiction for various instruction-tuned LMs, covering evaluation,\ndetection, and mitigation. Our primary evaluation task is open-domain text\ngeneration, but we also demonstrate the applicability of our approach to\nshorter question answering. Our analysis reveals the prevalence of\nself-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We\nthen propose a novel prompting-based framework designed to effectively detect\nand mitigate self-contradictions. Our detector achieves high accuracy, e.g.,\naround 80% F1 score when prompting ChatGPT. The mitigation algorithm\niteratively refines the generated text to remove contradictory information\nwhile preserving text fluency and informativeness. Importantly, our entire\nframework is applicable to black-box LMs and does not require retrieval of\nexternal knowledge. Rather, our method complements retrieval-based methods, as\na large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be\nverified using online text. Our approach is practically effective and has been\nreleased as a push-button tool to benefit the public at\nhttps://chatprotect.ai/.",
        "pdf_link": "https://arxiv.org/pdf/2305.15852v3.pdf"
    },
    {
        "title": "ChatGPT for PLC/DCS Control Logic Generation",
        "authors": [
            "Heiko Koziolek",
            "Sten Gruener",
            "Virendra Ashiwal"
        ],
        "published": "2023-05-25T07:46:53Z",
        "summary": "Large language models (LLMs) providing generative AI have become popular to\nsupport software engineers in creating, summarizing, optimizing, and\ndocumenting source code. It is still unknown how LLMs can support control\nengineers using typical control programming languages in programming tasks.\nResearchers have explored GitHub CoPilot or DeepMind AlphaCode for source code\ngeneration but did not yet tackle control logic programming. The contribution\nof this paper is an exploratory study, for which we created 100 LLM prompts in\n10 representative categories to analyze control logic generation for of PLCs\nand DCS from natural language. We tested the prompts by generating answers with\nChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3\nStructured Text code in many cases and demonstrated useful reasoning skills\nthat could boost control engineer productivity. Our prompt collection is the\nbasis for a more formal LLM benchmark to test and compare such models for\ncontrol logic generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.15809v1.pdf"
    },
    {
        "title": "Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback",
        "authors": [
            "Yiqi Lin",
            "Hao Wu",
            "Ruichen Wang",
            "Haonan Lu",
            "Xiaodong Lin",
            "Hui Xiong",
            "Lin Wang"
        ],
        "published": "2023-05-25T07:43:39Z",
        "summary": "Generating and editing a 3D scene guided by natural language poses a\nchallenge, primarily due to the complexity of specifying the positional\nrelations and volumetric changes within the 3D space. Recent advancements in\nLarge Language Models (LLMs) have demonstrated impressive reasoning,\nconversational, and zero-shot generation abilities across various domains.\nSurprisingly, these models also show great potential in realizing and\ninterpreting the 3D space. In light of this, we propose a novel language-guided\ninteractive 3D generation system, dubbed LI3D, that integrates LLMs as a 3D\nlayout interpreter into the off-the-shelf layout-to-3D generative models,\nallowing users to flexibly and interactively generate visual content.\nSpecifically, we design a versatile layout structure base on the bounding boxes\nand semantics to prompt the LLMs to model the spatial generation and reasoning\nfrom language. Our system also incorporates LLaVA, a large language and vision\nassistant, to provide generative feedback from the visual aspect for improving\nthe visual quality of generated content. We validate the effectiveness of LI3D,\nprimarily in 3D generation and editing through multi-round interactions, which\ncan be flexibly extended to 2D generation and editing. Various experiments\ndemonstrate the potential benefits of incorporating LLMs in generative AI for\napplications, e.g., metaverse. Moreover, we benchmark the layout reasoning\nperformance of LLMs with neural visual artist tasks, revealing their emergent\nability in the spatial layout domain.",
        "pdf_link": "https://arxiv.org/pdf/2305.15808v1.pdf"
    },
    {
        "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
        "authors": [
            "Sotiris Anagnostidis",
            "Dario Pavllo",
            "Luca Biggio",
            "Lorenzo Noci",
            "Aurelien Lucchi",
            "Thomas Hofmann"
        ],
        "published": "2023-05-25T07:39:41Z",
        "summary": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hard\nto scale to long sequences. Despite several works trying to reduce their\ncomputational cost, most of LLMs still adopt attention layers between all pairs\nof tokens in the sequence, thus incurring a quadratic cost. In this study, we\npresent a novel approach that dynamically prunes contextual information while\npreserving the model's expressiveness, resulting in reduced memory and\ncomputational requirements during inference. Our method employs a learnable\nmechanism that determines which uninformative tokens can be dropped from the\ncontext at any point across the generation process. By doing so, our approach\nnot only addresses performance concerns but also enhances interpretability,\nproviding valuable insight into the model's decision-making process. Our\ntechnique can be applied to existing pre-trained models through a\nstraightforward fine-tuning process, and the pruning strength can be specified\nby a sparsity parameter. Notably, our empirical findings demonstrate that we\ncan effectively prune up to 80\\% of the context without significant performance\ndegradation on downstream tasks, offering a valuable tool for mitigating\ninference costs. Our reference implementation achieves up to $2\\times$ increase\nin inference throughput and even greater memory savings.",
        "pdf_link": "https://arxiv.org/pdf/2305.15805v2.pdf"
    },
    {
        "title": "On the Planning Abilities of Large Language Models : A Critical Investigation",
        "authors": [
            "Karthik Valmeekam",
            "Matthew Marquez",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "published": "2023-05-25T06:32:23Z",
        "summary": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) the effectiveness of LLMs in generating\nplans autonomously in commonsense planning tasks and (2) the potential of LLMs\nin LLM-Modulo settings where they act as a source of heuristic guidance for\nexternal planners and verifiers. We conduct a systematic study by generating a\nsuite of instances on domains similar to the ones employed in the International\nPlanning Competition and evaluate LLMs in two distinct modes: autonomous and\nheuristic. Our findings reveal that LLMs' ability to generate executable plans\nautonomously is rather limited, with the best model (GPT-4) having an average\nsuccess rate of ~12% across the domains. However, the results in the LLM-Modulo\nsetting show more promise. In the LLM-Modulo setting, we demonstrate that\nLLM-generated plans can improve the search process for underlying sound\nplanners and additionally show that external verifiers can help provide\nfeedback on the generated plans and back-prompt the LLM for better plan\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2305.15771v2.pdf"
    },
    {
        "title": "The False Promise of Imitating Proprietary LLMs",
        "authors": [
            "Arnav Gudibande",
            "Eric Wallace",
            "Charlie Snell",
            "Xinyang Geng",
            "Hao Liu",
            "Pieter Abbeel",
            "Sergey Levine",
            "Dawn Song"
        ],
        "published": "2023-05-25T05:00:12Z",
        "summary": "An emerging method to cheaply improve a weaker language model is to finetune\nit on outputs from a stronger model, such as a proprietary system like ChatGPT\n(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply\nimitate the proprietary model's capabilities using a weaker open-source model.\nIn this work, we critically analyze this approach. We first finetune a series\nof LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data\nsources, and imitation data amounts (0.3M--150M tokens). We then evaluate the\nmodels using crowd raters and canonical NLP benchmarks. Initially, we were\nsurprised by the output quality of our imitation models -- they appear far\nbetter at following instructions, and crowd workers rate their outputs as\ncompetitive with ChatGPT. However, when conducting more targeted automatic\nevaluations, we find that imitation models close little to none of the gap from\nthe base LM to ChatGPT on tasks that are not heavily supported in the imitation\ndata. We show that these performance discrepancies may slip past human raters\nbecause imitation models are adept at mimicking ChatGPT's style but not its\nfactuality. Overall, we conclude that model imitation is a false promise: there\nexists a substantial capabilities gap between open and closed LMs that, with\ncurrent methods, can only be bridged using an unwieldy amount of imitation data\nor by using more capable base LMs. In turn, we argue that the highest leverage\naction for improving open-source models is to tackle the difficult challenge of\ndeveloping better base LMs, rather than taking the shortcut of imitating\nproprietary systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.15717v1.pdf"
    },
    {
        "title": "Asking Before Action: Gather Information in Embodied Decision Making with Language Models",
        "authors": [
            "Xiaoyu Chen",
            "Shenao Zhang",
            "Pushi Zhang",
            "Li Zhao",
            "Jianyu Chen"
        ],
        "published": "2023-05-25T04:05:08Z",
        "summary": "With strong capabilities of reasoning and a generic understanding of the\nworld, Large Language Models (LLMs) have shown great potential in building\nversatile embodied decision making agents capable of performing diverse tasks.\nHowever, when deployed to unfamiliar environments, we show that LLM agents face\nchallenges in efficiently gathering necessary information, leading to\nsuboptimal performance. On the other hand, in unfamiliar scenarios, human\nindividuals often seek additional information from their peers before taking\naction, leveraging external knowledge to avoid unnecessary trial and error.\nBuilding upon this intuition, we propose \\textit{Asking Before Action} (ABA), a\nmethod that empowers the agent to proactively query external sources for\npertinent information using natural language during their interactions in the\nenvironment. In this way, the agent is able to enhance its efficiency and\nperformance by mitigating wasteful steps and circumventing the difficulties\nassociated with exploration in unfamiliar environments. We empirically evaluate\nour method on an embodied decision making benchmark, ALFWorld, and demonstrate\nthat despite modest modifications in prompts, our method exceeds baseline LLM\nagents by more than $40$%. Further experiments on two variants of ALFWorld\nillustrate that by imitation learning, ABA effectively retains and reuses\nqueried and known information in subsequent tasks, mitigating the need for\nrepetitive inquiries. Both qualitative and quantitative results exhibit\nremarkable performance on tasks that previous methods struggle to solve.",
        "pdf_link": "https://arxiv.org/pdf/2305.15695v1.pdf"
    },
    {
        "title": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting",
        "authors": [
            "Lei Shu",
            "Liangchen Luo",
            "Jayakumar Hoskere",
            "Yun Zhu",
            "Yinxiao Liu",
            "Simon Tong",
            "Jindong Chen",
            "Lei Meng"
        ],
        "published": "2023-05-25T03:26:26Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncreative tasks such as storytelling and E-mail generation. However, as LLMs are\nprimarily trained on final text results rather than intermediate revisions, it\nmight be challenging for them to perform text rewriting tasks. Most studies in\nthe rewriting tasks focus on a particular transformation type within the\nboundaries of single sentences. In this work, we develop new strategies for\ninstruction tuning and reinforcement learning to better align LLMs for\ncross-sentence rewriting tasks using diverse wording and structures expressed\nthrough natural languages including 1) generating rewriting instruction data\nfrom Wiki edits and public corpus through instruction generation and\nchain-of-thought prompting; 2) collecting comparison data for reward model\ntraining through a new ranking function. To facilitate this research, we\nintroduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting\ntypes expressed through natural language instructions. Our results show\nsignificant improvements over a variety of baselines. The public repository is\navailable on GitHub under Google Research\n(https://github.com/google-research/google-research/tree/master/rewritelm).",
        "pdf_link": "https://arxiv.org/pdf/2305.15685v2.pdf"
    },
    {
        "title": "Undetectable Watermarks for Language Models",
        "authors": [
            "Miranda Christ",
            "Sam Gunn",
            "Or Zamir"
        ],
        "published": "2023-05-25T02:57:16Z",
        "summary": "Recent advances in the capabilities of large language models such as GPT-4\nhave spurred increasing concern about our ability to detect AI-generated text.\nPrior works have suggested methods of embedding watermarks in model outputs, by\nnoticeably altering the output distribution. We ask: Is it possible to\nintroduce a watermark without incurring any detectable change to the output\ndistribution?\n  To this end we introduce a cryptographically-inspired notion of undetectable\nwatermarks for language models. That is, watermarks can be detected only with\nthe knowledge of a secret key; without the secret key, it is computationally\nintractable to distinguish watermarked outputs from those of the original\nmodel. In particular, it is impossible for a user to observe any degradation in\nthe quality of the text. Crucially, watermarks should remain undetectable even\nwhen the user is allowed to adaptively query the model with arbitrarily chosen\nprompts. We construct undetectable watermarks based on the existence of one-way\nfunctions, a standard assumption in cryptography.",
        "pdf_link": "https://arxiv.org/pdf/2306.09194v1.pdf"
    },
    {
        "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
        "authors": [
            "Haonan Duan",
            "Adam Dziedzic",
            "Nicolas Papernot",
            "Franziska Boenisch"
        ],
        "published": "2023-05-24T22:06:08Z",
        "summary": "Large language models (LLMs) are excellent in-context learners. However, the\nsensitivity of data contained in prompts raises privacy concerns. Our work\nfirst shows that these concerns are valid: we instantiate a simple but highly\neffective membership inference attack against the data used to prompt LLMs. To\naddress this vulnerability, one could forego prompting and resort to\nfine-tuning LLMs with known algorithms for private gradient descent. However,\nthis comes at the expense of the practicality and efficiency offered by\nprompting. Therefore, we propose to privately learn to prompt. We first show\nthat soft prompts can be obtained privately through gradient descent on\ndownstream data. However, this is not the case for discrete prompts. Thus, we\norchestrate a noisy vote among an ensemble of LLMs presented with different\nprompts, i.e., a flock of stochastic parrots. The vote privately transfers the\nflock's knowledge into a single public prompt. We show that LLMs prompted with\nour private algorithms closely match the non-private baselines. For example,\nusing GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the\nsst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs.\n95.2% for the non-private baseline. Through our experiments, we also show that\nour prompt-based approach is easily deployed with existing commercial APIs.",
        "pdf_link": "https://arxiv.org/pdf/2305.15594v1.pdf"
    },
    {
        "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
        "authors": [
            "Yuan Yang",
            "Siheng Xiong",
            "Ali Payani",
            "Ehsan Shareghi",
            "Faramarz Fekri"
        ],
        "published": "2023-05-24T19:59:51Z",
        "summary": "Translating natural language sentences to first-order logic (NL-FOL\ntranslation) is a longstanding challenge in the NLP and formal logic\nliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for\nNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of\ndirectly translating natural language into FOL rules, which outperforms\nGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,\nand can achieve similar performance as GPT-4 with a fraction of the cost. This\ncorrection ability was achieved by a novel supervised fine-tuning (SFT) +\nreinforcement learning with human feedback (RLHF) framework, which initially\ntrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought\nreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier\nas the reward model.\n  To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel\ngener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset\nof 34K high-quality and diverse sentence-level NL-FOL pairs collected from\nGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4\nfor pairs, and dynamically adjusts the prompts to ensure the collection of\npairs with rich and diverse contexts at different levels of complexity, and\nverifies the validity of the generated FOL rules. Codes, weights, and data are\navailable at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small\n\\text{https://github.com/gblackout/LogicLLaMA}}}$.",
        "pdf_link": "https://arxiv.org/pdf/2305.15541v1.pdf"
    },
    {
        "title": "Large Language Models are Few-Shot Health Learners",
        "authors": [
            "Xin Liu",
            "Daniel McDuff",
            "Geza Kovacs",
            "Isaac Galatzer-Levy",
            "Jacob Sunshine",
            "Jiening Zhan",
            "Ming-Zher Poh",
            "Shun Liao",
            "Paolo Di Achille",
            "Shwetak Patel"
        ],
        "published": "2023-05-24T19:25:16Z",
        "summary": "Large language models (LLMs) can capture rich representations of concepts\nthat are useful for real-world tasks. However, language alone is limited. While\nexisting LLMs excel at text-based inferences, health applications require that\nmodels be grounded in numerical data (e.g., vital signs, laboratory values in\nclinical domains; steps, movement in the wellness domain) that is not easily or\nreadily expressed as text in existing training corpus. We demonstrate that with\nonly few-shot tuning, a large language model is capable of grounding various\nphysiological and behavioral time-series data and making meaningful inferences\non numerous health tasks for both clinical and wellness contexts. Using data\nfrom wearable and medical sensor recordings, we evaluate these capabilities on\nthe tasks of cardiac signal analysis, physical activity recognition, metabolic\ncalculation (e.g., calories burned), and estimation of stress reports and\nmental health screeners.",
        "pdf_link": "https://arxiv.org/pdf/2305.15525v1.pdf"
    },
    {
        "title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",
        "authors": [
            "Antonio Valerio Miceli-Barone",
            "Fazl Barez",
            "Ioannis Konstas",
            "Shay B. Cohen"
        ],
        "published": "2023-05-24T18:54:39Z",
        "summary": "Large Language Models (LLMs) have successfully been applied to code\ngeneration tasks, raising the question of how well these models understand\nprogramming. Typical programming languages have invariances and equivariances\nin their semantics that human programmers intuitively understand and exploit,\nsuch as the (near) invariance to the renaming of identifiers. We show that LLMs\nnot only fail to properly generate correct Python code when default function\nnames are swapped, but some of them even become more confident in their\nincorrect predictions as the model size increases, an instance of the recently\ndiscovered phenomenon of Inverse Scaling, which runs contrary to the commonly\nobserved trend of increasing prediction quality with increasing model size. Our\nfindings indicate that, despite their astonishing typical-case performance,\nLLMs still lack a deep, abstract understanding of the content they manipulate,\nmaking them unsuitable for tasks that statistically deviate from their training\ndata, and that mere scaling is not enough to achieve such capability.",
        "pdf_link": "https://arxiv.org/pdf/2305.15507v1.pdf"
    },
    {
        "title": "Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective",
        "authors": [
            "Guhao Feng",
            "Bohang Zhang",
            "Yuntian Gu",
            "Haotian Ye",
            "Di He",
            "Liwei Wang"
        ],
        "published": "2023-05-24T17:59:21Z",
        "summary": "Recent studies have discovered that Chain-of-Thought prompting (CoT) can\ndramatically improve the performance of Large Language Models (LLMs),\nparticularly when dealing with complex tasks involving mathematics or\nreasoning. Despite the enormous empirical success, the underlying mechanisms\nbehind CoT and how it unlocks the potential of LLMs remain elusive. In this\npaper, we take a first step towards theoretically answering these questions.\nSpecifically, we examine the expressivity of LLMs with CoT in solving\nfundamental mathematical and decision-making problems. By using circuit\ncomplexity theory, we first give impossibility results showing that\nbounded-depth Transformers are unable to directly produce correct answers for\nbasic arithmetic/equation tasks unless the model size grows super-polynomially\nwith respect to the input length. In contrast, we then prove by construction\nthat autoregressive Transformers of constant size suffice to solve both tasks\nby generating CoT derivations using a commonly used math language format.\nMoreover, we show LLMs with CoT can handle a general class of decision-making\nproblems known as Dynamic Programming, thus justifying its power in tackling\ncomplex real-world tasks. Finally, an extensive set of experiments show that,\nwhile Transformers always fail to directly predict the answers, they can\nconsistently learn to generate correct solutions step-by-step given sufficient\nCoT demonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2305.15408v5.pdf"
    },
    {
        "title": "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models",
        "authors": [
            "Weixi Feng",
            "Wanrong Zhu",
            "Tsu-jui Fu",
            "Varun Jampani",
            "Arjun Akula",
            "Xuehai He",
            "Sugato Basu",
            "Xin Eric Wang",
            "William Yang Wang"
        ],
        "published": "2023-05-24T17:56:16Z",
        "summary": "Attaining a high degree of user controllability in visual generation often\nrequires intricate, fine-grained inputs like layouts. However, such inputs\nimpose a substantial burden on users when compared to simple text inputs. To\naddress the issue, we study how Large Language Models (LLMs) can serve as\nvisual planners by generating layouts from text conditions, and thus\ncollaborate with visual generative models. We propose LayoutGPT, a method to\ncompose in-context visual demonstrations in style sheet language to enhance the\nvisual planning skills of LLMs. LayoutGPT can generate plausible layouts in\nmultiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also\nshows superior performance in converting challenging language concepts like\nnumerical and spatial relations to layout arrangements for faithful\ntext-to-image generation. When combined with a downstream image generation\nmodel, LayoutGPT outperforms text-to-image models/systems by 20-40% and\nachieves comparable performance as human users in designing visual layouts for\nnumerical and spatial correctness. Lastly, LayoutGPT achieves comparable\nperformance to supervised methods in 3D indoor scene synthesis, demonstrating\nits effectiveness and potential in multiple visual domains.",
        "pdf_link": "https://arxiv.org/pdf/2305.15393v2.pdf"
    },
    {
        "title": "Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing",
        "authors": [
            "Shufan Wang",
            "Sebastien Jean",
            "Sailik Sengupta",
            "James Gung",
            "Nikolaos Pappas",
            "Yi Zhang"
        ],
        "published": "2023-05-24T16:50:36Z",
        "summary": "In executable task-oriented semantic parsing, the system aims to translate\nusers' utterances in natural language to machine-interpretable programs (API\ncalls) that can be executed according to pre-defined API specifications. With\nthe popularity of Large Language Models (LLMs), in-context learning offers a\nstrong baseline for such scenarios, especially in data-limited regimes.\nHowever, LLMs are known to hallucinate and therefore pose a formidable\nchallenge in constraining generated content. Thus, it remains uncertain if LLMs\ncan effectively perform task-oriented utterance-to-API generation where\nrespecting API's structural and task-specific constraints is crucial.\n  In this work, we seek to measure, analyze and mitigate such constraints\nviolations. First, we identify the categories of various constraints in\nobtaining API-semantics from task-oriented utterances, and define fine-grained\nmetrics that complement traditional ones. Second, we leverage these metrics to\nconduct a detailed error analysis of constraints violations seen in\nstate-of-the-art LLMs, which motivates us to investigate two mitigation\nstrategies: Semantic-Retrieval of Demonstrations (SRD) and API-aware\nConstrained Decoding (API-CD). Our experiments show that these strategies are\neffective at reducing constraints violations and improving the quality of the\ngenerated API calls, but require careful consideration given their\nimplementation complexity and latency.",
        "pdf_link": "https://arxiv.org/pdf/2305.15338v1.pdf"
    },
    {
        "title": "Gorilla: Large Language Model Connected with Massive APIs",
        "authors": [
            "Shishir G. Patil",
            "Tianjun Zhang",
            "Xin Wang",
            "Joseph E. Gonzalez"
        ],
        "published": "2023-05-24T16:48:11Z",
        "summary": "Large Language Models (LLMs) have seen an impressive wave of advances\nrecently, with models now excelling in a variety of tasks, such as mathematical\nreasoning and program synthesis. However, their potential to effectively use\ntools via API calls remains unfulfilled. This is a challenging task even for\ntoday's state-of-the-art LLMs such as GPT-4, largely due to their inability to\ngenerate accurate input arguments and their tendency to hallucinate the wrong\nusage of an API call. We release Gorilla, a finetuned LLaMA-based model that\nsurpasses the performance of GPT-4 on writing API calls. When combined with a\ndocument retriever, Gorilla demonstrates a strong capability to adapt to\ntest-time document changes, enabling flexible user updates or version changes.\nIt also substantially mitigates the issue of hallucination, commonly\nencountered when prompting LLMs directly. To evaluate the model's ability, we\nintroduce APIBench, a comprehensive dataset consisting of HuggingFace,\nTorchHub, and TensorHub APIs. The successful integration of the retrieval\nsystem with Gorilla demonstrates the potential for LLMs to use tools more\naccurately, keep up with frequently updated documentation, and consequently\nincrease the reliability and applicability of their outputs. Gorilla's code,\nmodel, data, and demo are available at https://gorilla.cs.berkeley.edu",
        "pdf_link": "https://arxiv.org/pdf/2305.15334v1.pdf"
    },
    {
        "title": "Visual Programming for Text-to-Image Generation and Evaluation",
        "authors": [
            "Jaemin Cho",
            "Abhay Zala",
            "Mohit Bansal"
        ],
        "published": "2023-05-24T16:42:17Z",
        "summary": "As large language models have demonstrated impressive performance in many\ndomains, recent works have adopted language models (LMs) as controllers of\nvisual modules for vision-and-language tasks. While existing work focuses on\nequipping LMs with visual understanding, we propose two novel\ninterpretable/explainable visual programming frameworks for text-to-image (T2I)\ngeneration and evaluation. First, we introduce VPGen, an interpretable\nstep-by-step T2I generation framework that decomposes T2I generation into three\nsteps: object/count generation, layout generation, and image generation. We\nemploy an LM to handle the first two steps (object/count generation and layout\ngeneration), by finetuning it on text-layout pairs. Our step-by-step T2I\ngeneration framework provides stronger spatial control than end-to-end models,\nthe dominant approach for this task. Furthermore, we leverage the world\nknowledge of pretrained LMs, overcoming the limitation of previous\nlayout-guided T2I works that can only handle predefined object classes. We\ndemonstrate that our VPGen has improved control in counts/spatial\nrelations/scales of objects than state-of-the-art T2I generation models.\nSecond, we introduce VPEval, an interpretable and explainable evaluation\nframework for T2I generation based on visual programming. Unlike previous T2I\nevaluations with a single scoring model that is accurate in some skills but\nunreliable in others, VPEval produces evaluation programs that invoke a set of\nvisual modules that are experts in different skills, and also provides\nvisual+textual explanations of the evaluation results. Our analysis shows that\nVPEval provides a more human-correlated evaluation for skill-specific and\nopen-ended prompts than widely used single model-based evaluation. We hope that\nour work encourages future progress on interpretable/explainable generation and\nevaluation for T2I models.",
        "pdf_link": "https://arxiv.org/pdf/2305.15328v2.pdf"
    },
    {
        "title": "Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",
        "authors": [
            "Evangelos Pournaras"
        ],
        "published": "2023-05-24T16:23:46Z",
        "summary": "Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
        "pdf_link": "https://arxiv.org/pdf/2305.15299v4.pdf"
    },
    {
        "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
        "authors": [
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Minlie Huang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-05-24T16:17:36Z",
        "summary": "Large language models are powerful text processors and reasoners, but are\nstill subject to limitations including outdated knowledge and hallucinations,\nwhich necessitates connecting them to the world. Retrieval-augmented large\nlanguage models have raised extensive attention for grounding model generation\non external knowledge. However, retrievers struggle to capture relevance,\nespecially for queries with complex information needs. Recent work has proposed\nto improve relevance modeling by having large language models actively involved\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\nshow that strong performance can be achieved by a method we call Iter-RetGen,\nwhich synergizes retrieval and generation in an iterative manner. A model\noutput shows what might be needed to finish a task, and thus provides an\ninformative context for retrieving more relevant knowledge which in turn helps\ngenerate a better output in the next iteration. Compared with recent work which\ninterleaves retrieval with generation when producing an output, Iter-RetGen\nprocesses all retrieved knowledge as a whole and largely preserves the\nflexibility in generation without structural constraints. We evaluate\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\nreasoning, and show that it can flexibly leverage parametric knowledge and\nnon-parametric knowledge, and is superior to or competitive with\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\nretrieval and generation. We can further improve performance via\ngeneration-augmented retrieval adaptation.",
        "pdf_link": "https://arxiv.org/pdf/2305.15294v2.pdf"
    },
    {
        "title": "A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification",
        "authors": [
            "Rohan Bhambhoria",
            "Lei Chen",
            "Xiaodan Zhu"
        ],
        "published": "2023-05-24T16:04:26Z",
        "summary": "In recent years, large language models (LLMs) have achieved strong\nperformance on benchmark tasks, especially in zero or few-shot settings.\nHowever, these benchmarks often do not adequately address the challenges posed\nin the real-world, such as that of hierarchical classification. In order to\naddress this challenge, we propose refactoring conventional tasks on\nhierarchical datasets into a more indicative long-tail prediction task. We\nobserve LLMs are more prone to failure in these cases. To address these\nlimitations, we propose the use of entailment-contradiction prediction in\nconjunction with LLMs, which allows for strong performance in a strict\nzero-shot setting. Importantly, our method does not require any parameter\nupdates, a resource-intensive process and achieves strong performance across\nmultiple datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.15282v2.pdf"
    },
    {
        "title": "Revisiting Token Dropping Strategy in Efficient BERT Pretraining",
        "authors": [
            "Qihuang Zhong",
            "Liang Ding",
            "Juhua Liu",
            "Xuebo Liu",
            "Min Zhang",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2023-05-24T15:59:44Z",
        "summary": "Token dropping is a recently-proposed strategy to speed up the pretraining of\nmasked language models, such as BERT, by skipping the computation of a subset\nof the input tokens at several middle layers. It can effectively reduce the\ntraining time without degrading much performance on downstream tasks. However,\nwe empirically find that token dropping is prone to a semantic loss problem and\nfalls short in handling semantic-intense tasks. Motivated by this, we propose a\nsimple yet effective semantic-consistent learning method (ScTD) to improve the\ntoken dropping. ScTD aims to encourage the model to learn how to preserve the\nsemantic information in the representation space. Extensive experiments on 12\ntasks show that, with the help of our ScTD, token dropping can achieve\nconsistent and significant performance gains across all task types and model\nsizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings\nup to +1.56% average improvement over the vanilla token dropping.",
        "pdf_link": "https://arxiv.org/pdf/2305.15273v1.pdf"
    },
    {
        "title": "Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples",
        "authors": [
            "Abulhair Saparov",
            "Richard Yuanzhe Pang",
            "Vishakh Padmakumar",
            "Nitish Joshi",
            "Seyed Mehran Kazemi",
            "Najoung Kim",
            "He He"
        ],
        "published": "2023-05-24T15:55:51Z",
        "summary": "Given the intractably large size of the space of proofs, any model that is\ncapable of general deductive reasoning must generalize to proofs of greater\ncomplexity. Recent studies have shown that large language models (LLMs) possess\nsome abstract deductive reasoning ability given chain-of-thought prompts.\nHowever, they have primarily been tested on proofs using modus ponens or of a\nspecific size, and from the same distribution as the in-context examples. To\nmeasure the general deductive reasoning ability of LLMs, we test on a broad set\nof deduction rules and measure their ability to generalize to more complex\nproofs from simpler demonstrations from multiple angles: depth-, width-, and\ncompositional generalization. To facilitate systematic exploration, we\nconstruct a new synthetic and programmable reasoning dataset that enables\ncontrol over deduction rules and proof complexity. Our experiments on four LLMs\nof various sizes and training objectives show that they are able to generalize\nto compositional proofs. However, they have difficulty generalizing to longer\nproofs, and they require explicit demonstrations to produce hypothetical\nsubproofs, specifically in proof by cases and proof by contradiction.",
        "pdf_link": "https://arxiv.org/pdf/2305.15269v3.pdf"
    },
    {
        "title": "EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models",
        "authors": [
            "Zhengwei Tao",
            "Zhi Jin",
            "Xiaoying Bai",
            "Haiyan Zhao",
            "Yanlin Feng",
            "Jia Li",
            "Wenpeng Hu"
        ],
        "published": "2023-05-24T15:55:40Z",
        "summary": "Events serve as fundamental units of occurrence within various contexts. The\nprocessing of event semantics in textual information forms the basis of\nnumerous natural language processing (NLP) applications. Recent studies have\nbegun leveraging large language models (LLMs) to address event semantic\nprocessing. However, the extent that LLMs can effectively tackle these\nchallenges remains uncertain. Furthermore, the lack of a comprehensive\nevaluation framework for event semantic processing poses a significant\nchallenge in evaluating these capabilities. In this paper, we propose an\noverarching framework for event semantic processing, encompassing\nunderstanding, reasoning, and prediction, along with their fine-grained\naspects. To comprehensively evaluate the event semantic processing abilities of\nmodels, we introduce a novel benchmark called EVEVAL. We collect 8 datasets\nthat cover all aspects of event semantic processing. Extensive experiments are\nconducted on EVEVAL, leading to several noteworthy findings based on the\nobtained results.",
        "pdf_link": "https://arxiv.org/pdf/2305.15268v1.pdf"
    },
    {
        "title": "Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model",
        "authors": [
            "Zirui Liu",
            "Guanchu Wang",
            "Shaochen Zhong",
            "Zhaozhuo Xu",
            "Daochen Zha",
            "Ruixiang Tang",
            "Zhimeng Jiang",
            "Kaixiong Zhou",
            "Vipin Chaudhary",
            "Shuai Xu",
            "Xia Hu"
        ],
        "published": "2023-05-24T15:52:08Z",
        "summary": "With the rapid growth in model size, fine-tuning the large pre-trained\nlanguage model has become increasingly difficult due to its extensive memory\nusage. Previous works usually focus on reducing the number of trainable\nparameters in the network. While the model parameters do contribute to memory\nusage, the primary memory bottleneck during training arises from storing\nfeature maps, also known as activations, as they are crucial for gradient\ncalculation. Notably, neural networks are usually trained using stochastic\ngradient descent. We argue that in stochastic optimization, models can handle\nnoisy gradients as long as the gradient estimator is unbiased with reasonable\nvariance. Following this motivation, we propose a new family of unbiased\nestimators called WTA-CRS, for matrix production with reduced variance, which\nonly requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the\ncontext of tuning transformers, our proposed estimators exhibit lower variance\ncompared to existing ones. By replacing the linear operation with our\napproximated one in transformers, we can achieve up to 2.7$\\times$ peak memory\nreduction with almost no accuracy drop and enables up to $6.4\\times$ larger\nbatch size. Under the same hardware, WTA-CRS enables better down-streaming task\nperformance by applying larger models and/or faster training speed with larger\nbatch sizes.",
        "pdf_link": "https://arxiv.org/pdf/2305.15265v2.pdf"
    },
    {
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration",
        "authors": [
            "Kejuan Yang",
            "Xiao Liu",
            "Kaiwen Men",
            "Aohan Zeng",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2023-05-24T15:48:29Z",
        "summary": "We identify two crucial limitations in the evaluation of recent\nparallel-integrated method Parallel Context Windows (PCW), which extends the\nmaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing\nwindow-wise attention and positional embedding techniques. We first show that a\nsimple yet strong baseline, weighted sum ensemble, is missing for the\nin-context few-shot classification. Moreover, on more challenging\nChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected\ndeterioration regarding question miscomprehension and false inference. Based on\nour findings, we suggest that the existing PCW design may not guarantee\nsufficient improvement and practicality in handling lengthy documents in\nreal-world applications. More community efforts on enabling language models'\nlong context understanding ability should be paid.",
        "pdf_link": "https://arxiv.org/pdf/2305.15262v1.pdf"
    },
    {
        "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
        "authors": [
            "Eliya Nachmani",
            "Alon Levkovitch",
            "Roy Hirsch",
            "Julian Salazar",
            "Chulayuth Asawaroengchai",
            "Soroosh Mariooryad",
            "Ehud Rivlin",
            "RJ Skerry-Ryan",
            "Michelle Tadmor Ramanovich"
        ],
        "published": "2023-05-24T15:39:43Z",
        "summary": "We present a novel approach to adapting pre-trained large language models\n(LLMs) to perform question answering (QA) and speech continuation. By endowing\nthe LLM with a pre-trained speech encoder, our model becomes able to take\nspeech inputs and generate speech outputs. The entire system is trained\nend-to-end and operates directly on spectrograms, simplifying our architecture.\nKey to our approach is a training objective that jointly supervises speech\nrecognition, text continuation, and speech synthesis using only paired\nspeech-text pairs, enabling a `cross-modal' chain-of-thought within a single\ndecoding pass. Our method surpasses existing spoken language models in speaker\npreservation and semantic coherence. Furthermore, the proposed model improves\nupon direct initialization in retaining the knowledge of the original LLM as\ndemonstrated through spoken QA datasets. Audio samples can be found at\nhttps://michelleramanovich.github.io/spectron/spectron",
        "pdf_link": "https://arxiv.org/pdf/2305.15255v3.pdf"
    },
    {
        "title": "Machine Unlearning: its nature, scope, and importance for a \"delete culture\"",
        "authors": [
            "Luciano Floridi"
        ],
        "published": "2023-05-24T15:27:04Z",
        "summary": "The article explores the cultural shift from recording to deleting\ninformation in the digital age and its implications on privacy, intellectual\nproperty (IP), and Large Language Models like ChatGPT. It begins by defining a\ndelete culture where information, in principle legal, is made unavailable or\ninaccessible because unacceptable or undesirable, especially but not only due\nto its potential to infringe on privacy or IP. Then it focuses on two\nstrategies in this context: deleting, to make information unavailable; and\nblocking, to make it inaccessible. The article argues that both strategies have\nsignificant implications, particularly for machine learning (ML) models where\ninformation is not easily made unavailable. However, the emerging research area\nof Machine Unlearning (MU) is highlighted as a potential solution. MU, still in\nits infancy, seeks to remove specific data points from ML models, effectively\nmaking them 'forget' completely specific information. If successful, MU could\nprovide a feasible means to manage the overabundance of information and ensure\na better protection of privacy and IP. However, potential ethical risks, such\nas misuse, overuse, and underuse of MU, should be systematically studied to\ndevise appropriate policies.",
        "pdf_link": "https://arxiv.org/pdf/2305.15242v1.pdf"
    },
    {
        "title": "SAIL: Search-Augmented Instruction Learning",
        "authors": [
            "Hongyin Luo",
            "Yung-Sung Chuang",
            "Yuan Gong",
            "Tianhua Zhang",
            "Yoon Kim",
            "Xixin Wu",
            "Danny Fox",
            "Helen Meng",
            "James Glass"
        ],
        "published": "2023-05-24T15:07:30Z",
        "summary": "Large language models (LLMs) have been significantly improved by instruction\nfine-tuning, but still lack transparency and the ability to utilize up-to-date\nknowledge and information. In this work, we propose search-augmented\ninstruction learning (SAIL), which grounds the language generation and\ninstruction following abilities on complex search results generated by in-house\nand external search engines. With an instruction tuning corpus, we collect\nsearch results for each training case from different search APIs and domains,\nand construct a new search-grounded training set containing\n\\textit{(instruction, grounding information, response)} triplets. We then\nfine-tune the LLaMA-7B model on the constructed training set. Since the\ncollected results contain unrelated and disputing languages, the model needs to\nlearn to ground on trustworthy search results, filter out distracting passages,\nand generate the target response. The search result-denoising process entails\nexplicit trustworthy information selection and multi-hop reasoning, since the\nretrieved passages might be informative but not contain the\ninstruction-following answer. Experiments show that the fine-tuned SAIL-7B\nmodel has a strong instruction-following ability, and it performs significantly\nbetter on transparency-sensitive tasks, including open-ended question answering\nand fact checking.",
        "pdf_link": "https://arxiv.org/pdf/2305.15225v2.pdf"
    },
    {
        "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
        "authors": [
            "Tetsu Kasanishi",
            "Masaru Isonuma",
            "Junichiro Mori",
            "Ichiro Sakata"
        ],
        "published": "2023-05-24T14:26:30Z",
        "summary": "Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.",
        "pdf_link": "https://arxiv.org/pdf/2305.15186v1.pdf"
    },
    {
        "title": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models",
        "authors": [
            "Mingyu Derek Ma",
            "Xiaoxuan Wang",
            "Po-Nien Kung",
            "P. Jeffrey Brantingham",
            "Nanyun Peng",
            "Wei Wang"
        ],
        "published": "2023-05-24T12:15:19Z",
        "summary": "Information extraction tasks such as event extraction require an in-depth\nunderstanding of the output structure and sub-task dependencies. They heavily\nrely on task-specific training data in the form of (passage, target structure)\npairs to obtain reasonable performance. However, obtaining such data through\nhuman annotation is costly, leading to a pressing need for low-resource\ninformation extraction approaches that require minimal human labeling for\nreal-world applications. Fine-tuning supervised models with synthesized\ntraining data would be a generalizable method, but the existing data generation\nmethods either still rely on large-scale ground-truth data or cannot be applied\nto complicated IE tasks due to their poor performance. To address these\nchallenges, we propose STAR, a data generation method that leverages Large\nLanguage Models (LLMs) to synthesize data instances given limited seed\ndemonstrations, thereby boosting low-resource information extraction\nperformance. Our approach involves generating target structures (Y) followed by\ngenerating passages (X), all accomplished with the aid of LLMs. We design\nfine-grained step-by-step instructions to obtain the initial data instances. We\nfurther reduce errors and improve data quality through self-reflection error\nidentification and self-refinement with iterative revision. Our experiments\nshow that the data generated by STAR significantly improve the performance of\nlow-resource event extraction and relation extraction tasks, even surpassing\nthe effectiveness of human-curated data. Human assessment of the data quality\nshows STAR-generated data exhibits higher passage quality and better align with\nthe task definitions compared with the human-curated data.",
        "pdf_link": "https://arxiv.org/pdf/2305.15090v3.pdf"
    },
    {
        "title": "Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models",
        "authors": [
            "Geewook Kim",
            "Hodong Lee",
            "Daehee Kim",
            "Haeji Jung",
            "Sanghee Park",
            "Yoonsik Kim",
            "Sangdoo Yun",
            "Taeho Kil",
            "Bado Lee",
            "Seunghyun Park"
        ],
        "published": "2023-05-24T11:59:13Z",
        "summary": "Recent advances in Large Language Models (LLMs) have stimulated a surge of\nresearch aimed at extending their applications to the visual domain. While\nthese models exhibit promise in generating abstract image captions and\nfacilitating natural conversations, their performance on text-rich images still\nrequires improvement. In this paper, we introduce Contrastive Reading Model\n(Cream), a novel neural architecture designed to enhance the language-image\nunderstanding capability of LLMs by capturing intricate details that are often\noverlooked in existing methods. Cream combines vision and auxiliary encoders,\nfortified by a contrastive feature alignment technique, to achieve a more\neffective comprehension of language information in visually situated contexts\nwithin the images. Our approach bridges the gap between vision and language\nunderstanding, paving the way for the development of more sophisticated\nDocument Intelligence Assistants. Through rigorous evaluations across diverse\nvisually-situated language understanding tasks that demand reasoning\ncapabilities, we demonstrate the compelling performance of Cream, positioning\nit as a prominent model in the field of visual document understanding. We\nprovide our codebase and newly-generated datasets at\nhttps://github.com/naver-ai/cream .",
        "pdf_link": "https://arxiv.org/pdf/2305.15080v2.pdf"
    },
    {
        "title": "Contrastive Learning of Sentence Embeddings from Scratch",
        "authors": [
            "Junlei Zhang",
            "Zhenzhong Lan",
            "Junxian He"
        ],
        "published": "2023-05-24T11:56:21Z",
        "summary": "Contrastive learning has been the dominant approach to train state-of-the-art\nsentence embeddings. Previous studies have typically learned sentence\nembeddings either through the use of human-annotated natural language inference\n(NLI) data or via large-scale unlabeled sentences in an unsupervised manner.\nHowever, even in the case of unlabeled data, their acquisition presents\nchallenges in certain domains due to various reasons. To address these issues,\nwe present SynCSE, a contrastive learning framework that trains sentence\nembeddings with synthesized data. Specifically, we explore utilizing large\nlanguage models to synthesize the required data samples for contrastive\nlearning, including (1) producing positive and negative annotations given\nunlabeled sentences (SynCSE-partial), and (2) generating sentences along with\ntheir corresponding annotations from scratch (SynCSE-scratch). Experimental\nresults on sentence similarity and reranking tasks indicate that both\nSynCSE-partial and SynCSE-scratch greatly outperform unsupervised baselines,\nand SynCSE-partial even achieves comparable performance to the supervised\nmodels in most settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.15077v2.pdf"
    },
    {
        "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor",
        "authors": [
            "Hongbo Zhang",
            "Junying Chen",
            "Feng Jiang",
            "Fei Yu",
            "Zhihong Chen",
            "Jianquan Li",
            "Guiming Chen",
            "Xiangbo Wu",
            "Zhiyi Zhang",
            "Qingying Xiao",
            "Xiang Wan",
            "Benyou Wang",
            "Haizhou Li"
        ],
        "published": "2023-05-24T11:56:01Z",
        "summary": "In this paper, we present HuatuoGPT, a large language model (LLM) for medical\nconsultation. The core recipe of HuatuoGPT is to leverage both\n\\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors}\nin the supervised fine-tuned stage. The responses of ChatGPT are usually\ndetailed, well-presented and informative while it cannot perform like a doctor\nin many aspects, e.g. for integrative diagnosis. We argue that real-world data\nfrom doctors would be complementary to distilled data in the sense the former\ncould tame a distilled language model to perform like doctors. To better\nleverage the strengths of both data, we train a reward model to align the\nlanguage model with the merits that both data bring, following an RLAIF\n(reinforced learning from AI feedback) fashion. To evaluate and benchmark the\nmodels, we propose a comprehensive evaluation scheme (including automatic and\nmanual metrics). Experimental results demonstrate that HuatuoGPT achieves\nstate-of-the-art results in performing medical consultation among open-source\nLLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It\nis worth noting that by using additional real-world data and RLAIF, the\ndistilled language model (i.e., HuatuoGPT) outperforms its teacher model\nChatGPT in most cases. Our code, data, and models are publicly available at\n\\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is\navailable at \\url{https://www.HuatuoGPT.cn/}.",
        "pdf_link": "https://arxiv.org/pdf/2305.15075v1.pdf"
    },
    {
        "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
        "authors": [
            "Daman Arora",
            "Himanshu Gaurav Singh",
            "Mausam"
        ],
        "published": "2023-05-24T11:55:59Z",
        "summary": "The performance of large language models (LLMs) on existing reasoning\nbenchmarks has significantly improved over the past years. In response, we\npresent JEEBench, a considerably more challenging benchmark dataset for\nevaluating the problem solving abilities of LLMs. We curate 515 challenging\npre-engineering mathematics, physics and chemistry problems from the highly\ncompetitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep\nin-domain knowledge is essential for solving problems in this benchmark. Our\nevaluation on various open-source and proprietary models reveals that the\nhighest performance, even after using techniques like self-consistency,\nself-refinement and chain-of-thought prompting, is less than 40%. The typical\nfailure modes of GPT-4, the best model, are errors in algebraic manipulation,\ndifficulty in grounding abstract concepts into mathematical equations\naccurately and failure in retrieving relevant domain-specific concepts. We also\nobserve that by mere prompting, GPT-4 is unable to assess risk introduced by\nnegative marking for incorrect answers. For this, we develop a post-hoc\nconfidence-thresholding method over self-consistency, which enables effective\nresponse selection. We hope that our challenging benchmark will guide future\nre-search in problem-solving using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.15074v3.pdf"
    },
    {
        "title": "ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind",
        "authors": [
            "Xiaomeng Ma",
            "Lingyu Gao",
            "Qihui Xu"
        ],
        "published": "2023-05-24T11:54:07Z",
        "summary": "Theory of Mind (ToM), the capacity to comprehend the mental states of\ndistinct individuals, is essential for numerous practical applications. With\nthe development of large language models (LLMs), there is a heated debate about\nwhether they are able to perform ToM tasks. Previous studies have used\ndifferent tasks and prompts to test the ToM on LLMs and the results are\ninconsistent: some studies asserted these models are capable of exhibiting ToM,\nwhile others suggest the opposite. In this study, We present ToMChallenges, a\ndataset for comprehensively evaluating the Theory of Mind based on the\nSally-Anne and Smarties tests with a diverse set of tasks. In addition, we also\npropose an auto-grader to streamline the answer evaluation process. We tested\nthree models: davinci, turbo, and gpt-4. Our evaluation results and error\nanalyses show that LLMs have inconsistent behaviors across prompts and tasks.\nPerforming the ToM tasks robustly remains a challenge for the LLMs. In\naddition, our paper wants to raise awareness in evaluating the ToM in LLMs and\nwe want to invite more discussion on how to design the prompts and tasks for\nToM tasks that can better assess the LLMs' ability.",
        "pdf_link": "https://arxiv.org/pdf/2305.15068v2.pdf"
    },
    {
        "title": "Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying References",
        "authors": [
            "Tianyi Tang",
            "Hongyuan Lu",
            "Yuchen Eleanor Jiang",
            "Haoyang Huang",
            "Dongdong Zhang",
            "Wayne Xin Zhao",
            "Tom Kocmi",
            "Furu Wei"
        ],
        "published": "2023-05-24T11:53:29Z",
        "summary": "Most research about natural language generation (NLG) relies on evaluation\nbenchmarks with limited references for a sample, which may result in poor\ncorrelations with human judgements. The underlying reason is that one semantic\nmeaning can actually be expressed in different forms, and the evaluation with a\nsingle or few references may not accurately reflect the quality of the model's\nhypotheses. To address this issue, this paper presents a simple and effective\nmethod, named Div-Ref, to enhance existing evaluation benchmarks by enriching\nthe number of references. We leverage large language models (LLMs) to diversify\nthe expression of a single reference into multiple high-quality ones to cover\nthe semantic space of the reference sentence as much as possible. We conduct\ncomprehensive experiments to empirically demonstrate that diversifying the\nexpression of reference can significantly enhance the correlation between\nautomatic evaluation and human evaluation. This idea is compatible with recent\nLLM-based evaluation which can similarly derive advantages from incorporating\nmultiple references. We strongly encourage future generation benchmarks to\ninclude more references, even if they are generated by LLMs, which is once for\nall. We release all the code and data at https://github.com/RUCAIBox/Div-Ref to\nfacilitate research.",
        "pdf_link": "https://arxiv.org/pdf/2305.15067v2.pdf"
    },
    {
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking",
        "authors": [
            "Jiayan Guo",
            "Lun Du",
            "Hengyu Liu",
            "Mengyu Zhou",
            "Xinyi He",
            "Shi Han"
        ],
        "published": "2023-05-24T11:53:19Z",
        "summary": "Large language models~(LLM) like ChatGPT have become indispensable to\nartificial general intelligence~(AGI), demonstrating excellent performance in\nvarious natural language processing tasks. In the real world, graph data is\nubiquitous and an essential part of AGI and prevails in domains like social\nnetwork analysis, bioinformatics and recommender systems. The training corpus\nof large language models often includes some algorithmic components, which\nallows them to achieve certain effects on some graph data-related problems.\nHowever, there is still little research on their performance on a broader range\nof graph-structured data. In this study, we conduct an extensive investigation\nto assess the proficiency of LLMs in comprehending graph data, employing a\ndiverse range of structural and semantic-related tasks. Our analysis\nencompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph\nunderstanding. Through our study, we not only uncover the current limitations\nof language models in comprehending graph structures and performing associated\nreasoning tasks but also emphasize the necessity for further advancements and\nnovel approaches to enhance their graph processing capabilities. Our findings\ncontribute valuable insights towards bridging the gap between language models\nand graph understanding, paving the way for more effective graph mining and\nknowledge extraction.",
        "pdf_link": "https://arxiv.org/pdf/2305.15066v2.pdf"
    },
    {
        "title": "Lawyer LLaMA Technical Report",
        "authors": [
            "Quzhe Huang",
            "Mingxu Tao",
            "Chen Zhang",
            "Zhenwei An",
            "Cong Jiang",
            "Zhibin Chen",
            "Zirui Wu",
            "Yansong Feng"
        ],
        "published": "2023-05-24T11:52:07Z",
        "summary": "Large Language Models (LLMs), like LLaMA, have exhibited remarkable\nperformance across various tasks. Nevertheless, when deployed to specific\ndomains such as law or medicine, the models still confront the challenge of a\ndeficiency in domain-specific knowledge and an inadequate capability to\nleverage that knowledge to resolve domain-related problems. In this paper, we\npropose a new framework to adapt LLMs to specific domains and build Lawyer\nLLaMA, a legal domain LLM, based on this framework. Specifically, we inject\ndomain knowledge during the continual training stage and teach the model to\nlearn professional skills using properly designed supervised fine-tuning tasks.\nMoreover, to alleviate the hallucination problem during the model's generation,\nwe add a retrieval module and extract relevant legal articles before the model\nanswers any queries. When learning domain-specific skills, we find that\nexperts' experience is much more useful than experiences distilled from\nChatGPT, where hundreds of expert-written data outperform tens of thousands of\nChatGPT-generated ones. We will release our model and data.",
        "pdf_link": "https://arxiv.org/pdf/2305.15062v2.pdf"
    },
    {
        "title": "Who Wrote this Code? Watermarking for Code Generation",
        "authors": [
            "Taehyun Lee",
            "Seokhee Hong",
            "Jaewoo Ahn",
            "Ilgee Hong",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Jamin Shin",
            "Gunhee Kim"
        ],
        "published": "2023-05-24T11:49:52Z",
        "summary": "With the remarkable generation performance of large language models, ethical\nand legal concerns about using them have been raised, such as plagiarism and\ncopyright issues. For such concerns, several approaches to watermark and detect\nLLM-generated text have been proposed very recently. However, we discover that\nthe previous methods fail to function appropriately with code generation tasks\nbecause of the syntactic and semantic characteristics of code. Based on\n\\citet{Kirchenbauer2023watermark}, we propose a new watermarking method,\nSelective WatErmarking via Entropy Thresholding (SWEET), that promotes \"green\"\ntokens only at the position with high entropy of the token distribution during\ngeneration, thereby preserving the correctness of the generated code. The\nwatermarked code is detected by the statistical test and Z-score based on the\nentropy information. Our experiments on HumanEval and MBPP show that SWEET\nsignificantly improves the Pareto Frontier between the code correctness and\nwatermark detection performance. We also show that notable post-hoc detection\nmethods (e.g. DetectGPT) fail to work well in this task. Finally, we show that\nsetting a reasonable entropy threshold is not much of a challenge. Code is\navailable at https://github.com/hongcheki/sweet-watermark.",
        "pdf_link": "https://arxiv.org/pdf/2305.15060v3.pdf"
    },
    {
        "title": "A Monte Carlo Language Model Pipeline for Zero-Shot Sociopolitical Event Extraction",
        "authors": [
            "Erica Cai",
            "Brendan O'Connor"
        ],
        "published": "2023-05-24T11:41:33Z",
        "summary": "We consider dyadic zero-shot event extraction (EE) to identify actions\nbetween pairs of actors. The \\emph{zero-shot} setting allows social scientists\nor other non-computational researchers to extract any customized,\nuser-specified set of events without training, resulting in a \\emph{dyadic}\nevent database, allowing insight into sociopolitical relational dynamics among\nactors and the higher level organizations or countries they represent.\nUnfortunately, we find that current zero-shot EE methods perform poorly for the\ntask, with issues including word sense ambiguity, modality mismatch, and\nefficiency. Straightforward application of large language model prompting\ntypically performs even worse. We address these challenges with a new\nfine-grained, multi-stage generative question-answer method, using a Monte\nCarlo approach to exploit and overcome the randomness of generative outputs. It\nperforms 90\\% fewer queries than a previous approach, with strong performance\non the widely-used Automatic Content Extraction dataset. Finally, we extend our\nmethod to extract affiliations of actor arguments and demonstrate our method\nand findings on a dyadic international relations case study.",
        "pdf_link": "https://arxiv.org/pdf/2305.15051v1.pdf"
    },
    {
        "title": "Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science",
        "authors": [
            "Veniamin Veselovsky",
            "Manoel Horta Ribeiro",
            "Akhil Arora",
            "Martin Josifoski",
            "Ashton Anderson",
            "Robert West"
        ],
        "published": "2023-05-24T11:27:59Z",
        "summary": "Large Language Models (LLMs) have democratized synthetic data generation,\nwhich in turn has the potential to simplify and broaden a wide gamut of NLP\ntasks. Here, we tackle a pervasive problem in synthetic data generation: its\ngenerative distribution often differs from the distribution of real-world data\nresearchers care about (in other words, it is unfaithful). In a case study on\nsarcasm detection, we study three strategies to increase the faithfulness of\nsynthetic data: grounding, filtering, and taxonomy-based generation. We\nevaluate these strategies using the performance of classifiers trained with\ngenerated synthetic data on real-world data. While all three strategies improve\nthe performance of classifiers, we find that grounding works best for the task\nat hand. As synthetic data generation plays an ever-increasing role in NLP\nresearch, we expect this work to be a stepping stone in improving its utility.\nWe conclude this paper with some recommendations on how to generate\nhigh(er)-fidelity synthetic data for specific tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.15041v1.pdf"
    },
    {
        "title": "Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations",
        "authors": [
            "Wei-Lin Chen",
            "Cheng-Kuang Wu",
            "Yun-Nung Chen",
            "Hsin-Hsi Chen"
        ],
        "published": "2023-05-24T11:22:34Z",
        "summary": "Large language models (LLMs) have exhibited striking in-context learning\n(ICL) ability to adapt to target tasks with a few input-output demonstrations.\nFor better ICL, different methods are proposed to select representative\ndemonstrations from existing training corpora. However, such settings are not\naligned with real-world practices, as end-users usually query LMs without\naccess to demonstration pools. In this work, we introduce Self-ICL -- a simple\nframework which bootstraps LMs' intrinsic capabilities to perform zero-shot\nICL. Given a test input, Self-ICL first prompts the model to generate\npseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via\nzero-shot prompting. Finally, we perform ICL for the test input with the\npseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard\ntasks shows Self-ICL outperforms zero-shot baselines on both average accuracy\nand head-to-head comparison. Moreover, with zero-shot chain-of-thought,\nSelf-ICL achieves results comparable to using real demonstrations.\nAdditionally, we conduct a range of analyses to validate Self-ICL's\neffectiveness and provide insights for its behaviors under different settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.15035v2.pdf"
    },
    {
        "title": "ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories",
        "authors": [
            "Heming Xia",
            "Qingxiu Dong",
            "Lei Li",
            "Jingjing Xu",
            "Tianyu Liu",
            "Ziwei Qin",
            "Zhifang Sui"
        ],
        "published": "2023-05-24T11:14:31Z",
        "summary": "Recently, Large Language Models (LLMs) have been serving as general-purpose\ninterfaces, posing a significant demand for comprehensive visual knowledge.\nHowever, it remains unclear how well current LLMs and their visually augmented\ncounterparts (VaLMs) can master visual commonsense knowledge. To investigate\nthis, we propose ImageNetVC, a human-annotated dataset specifically designed\nfor zero- and few-shot visual commonsense evaluation across 1,000 ImageNet\ncategories. Utilizing ImageNetVC, we benchmark the fundamental visual\ncommonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze\nthe factors affecting the visual commonsense knowledge of large-scale models,\nproviding insights into the development of language models enriched with visual\ncommonsense knowledge. Our code and dataset are available at\nhttps://github.com/hemingkx/ImageNetVC.",
        "pdf_link": "https://arxiv.org/pdf/2305.15028v2.pdf"
    },
    {
        "title": "ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification",
        "authors": [
            "Biao Zhao",
            "Weiqiang Jin",
            "Javier Del Ser",
            "Guang Yang"
        ],
        "published": "2023-05-24T11:06:23Z",
        "summary": "In the era of sustainable smart agriculture, a massive amount of agricultural\nnews text is being posted on the Internet, in which massive agricultural\nknowledge has been accumulated. In this context, it is urgent to explore\neffective text classification techniques for users to access the required\nagricultural knowledge with high efficiency. Mainstream deep learning\napproaches employing fine-tuning strategies on pre-trained language models\n(PLMs), have demonstrated remarkable performance gains over the past few years.\nNonetheless, these methods still face many drawbacks that are complex to solve,\nincluding: 1. Limited agricultural training data due to the expensive-cost and\nlabour-intensive annotation; 2. Poor domain transferability, especially of\ncross-linguistic ability; 3. Complex and expensive large models\ndeployment.Inspired by the extraordinary success brought by the recent ChatGPT\n(e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore\nthe capability and utilization of ChatGPT applying to the agricultural\ninformatization field. ....(shown in article).... Code has been released on\nGithub\nhttps://github.com/albert-jin/agricultural_textual_classification_ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.15024v1.pdf"
    },
    {
        "title": "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models",
        "authors": [
            "Gen Luo",
            "Yiyi Zhou",
            "Tianhe Ren",
            "Shengxin Chen",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published": "2023-05-24T11:06:15Z",
        "summary": "Recently, growing interest has been aroused in extending the multimodal\ncapability of large language models (LLMs), e.g., vision-language (VL)\nlearning, which is regarded as the next milestone of artificial general\nintelligence. However, existing solutions are prohibitively expensive, which\nnot only need to optimize excessive parameters, but also require another\nlarge-scale pre-training before VL instruction tuning. In this paper, we\npropose a novel and affordable solution for the effective VL adaption of LLMs,\ncalled Mixture-of-Modality Adaptation (MMA). Instead of using large neural\nnetworks to connect the image encoder and LLM, MMA adopts lightweight modules,\ni.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables\nthe joint optimization of the image and language models. Meanwhile, MMA is also\nequipped with a routing algorithm to help LLMs achieve an automatic shift\nbetween single- and multi-modal instructions without compromising their ability\nof natural language understanding. To validate MMA, we apply it to a recent LLM\ncalled LLaMA and term this formed large vision-language instructed model as\nLaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two\nsetups, namely multimodal science question answering and multimodal dialogue.\nThe experimental results not only demonstrate the competitive performance and\nthe superior training efficiency of LaVIN than existing multimodal LLMs, but\nalso confirm its great potential as a general-purpose chatbot. More\nimportantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4\ntraining hours with 3.8M trainable parameters, greatly confirming the\neffectiveness of MMA. Our project is released at\nhttps://luogen1996.github.io/lavin.",
        "pdf_link": "https://arxiv.org/pdf/2305.15023v3.pdf"
    },
    {
        "title": "Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems",
        "authors": [
            "Marek Kadlčík",
            "Michal Štefánik",
            "Ondřej Sotolář",
            "Vlastimil Martinek"
        ],
        "published": "2023-05-24T10:58:20Z",
        "summary": "Despite outstanding performance in many tasks, language models are\nnotoriously inclined to make factual errors in tasks requiring arithmetic\ncomputation. We address this deficiency by creating Calc-X, a collection of\ndatasets that demonstrates the appropriate use of a calculator in reasoning\nchains. Calc-X is suitable for teaching language models to offload computations\nto a symbolic system. We survey and unify several existing chain-of-thought\ndatasets into a proposed format, resulting in a standard collection of over\n300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X\ncollection to train open-source calculator-using models we call Calcformers and\nshow that these models approximately double the accuracy of generating correct\nresults compared to vanilla language model baselines. We make all Calc-X\ndatasets, source code and Calcformers models publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.15017v2.pdf"
    },
    {
        "title": "Estimating Class Separability of Datasets Using Persistent Homology with Application to LLM Fine-Tuning",
        "authors": [
            "Najah Ghalyan",
            "Kostis Gourgoulias",
            "Yash Satsangi",
            "Sean Moran",
            "Maxime Labonne",
            "Joseph Sabelja"
        ],
        "published": "2023-05-24T10:58:09Z",
        "summary": "This paper proposes a method to estimate the class separability of an\nunlabeled text dataset by inspecting the topological characteristics of\nsentence-transformer embeddings of the text. Experiments conducted involve both\nbinary and multi-class cases, with balanced and imbalanced scenarios. The\nresults demonstrate a clear correlation and a better consistency between the\nproposed method and other separability and classification metrics, such as\nThornton's method and the AUC score of a logistic regression classifier, as\nwell as unsupervised methods. Finally, we empirically show that the proposed\nmethod can be part of a stopping criterion for fine-tuning language-model\nclassifiers. By monitoring the class separability of the embedding space after\neach training iteration, we can detect when the training process stops\nimproving the separability of the embeddings without using additional labels.",
        "pdf_link": "https://arxiv.org/pdf/2305.15016v3.pdf"
    },
    {
        "title": "Unlocking Temporal Question Answering for Large Language Models Using Code Execution",
        "authors": [
            "Xingxuan Li",
            "Liying Cheng",
            "Qingyu Tan",
            "Hwee Tou Ng",
            "Shafiq Joty",
            "Lidong Bing"
        ],
        "published": "2023-05-24T10:57:53Z",
        "summary": "Large language models (LLMs) have made significant progress in natural\nlanguage processing (NLP), and are utilized extensively in various\napplications. Recent works, such as chain-of-thought (CoT), have shown that\nintermediate reasoning steps can improve the performance of LLMs for complex\nreasoning tasks, such as math problems and symbolic question-answering tasks.\nHowever, we notice the challenge that LLMs face when it comes to temporal\nreasoning. Our preliminary experiments show that generating intermediate\nreasoning steps does not always boost the performance of complex temporal\nquestion-answering tasks. Therefore, we propose a novel framework that combines\nthe extraction capability of LLMs and the logical reasoning capability of a\nPython solver to tackle this issue. Extensive experiments and analysis\ndemonstrate the effectiveness of our framework in handling intricate time-bound\nreasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.15014v1.pdf"
    },
    {
        "title": "Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization",
        "authors": [
            "Aman Priyanshu",
            "Supriti Vijay",
            "Ayush Kumar",
            "Rakshit Naidu",
            "Fatemehsadat Mireshghallah"
        ],
        "published": "2023-05-24T10:48:05Z",
        "summary": "LLM-powered chatbots are becoming widely adopted in applications such as\nhealthcare, personal assistants, industry hiring decisions, etc. In many of\nthese cases, chatbots are fed sensitive, personal information in their prompts,\nas samples for in-context learning, retrieved records from a database, or as\npart of the conversation. The information provided in the prompt could directly\nappear in the output, which might have privacy ramifications if there is\nsensitive information there. As such, in this paper, we aim to understand the\ninput copying and regurgitation capabilities of these models during inference\nand how they can be directly instructed to limit this copying by complying with\nregulations such as HIPAA and GDPR, based on their internal knowledge of them.\nMore specifically, we find that when ChatGPT is prompted to summarize cover\nletters of a 100 candidates, it would retain personally identifiable\ninformation (PII) verbatim in 57.4% of cases, and we find this retention to be\nnon-uniform between different subgroups of people, based on attributes such as\ngender identity. We then probe ChatGPT's perception of privacy-related policies\nand privatization mechanisms by directly instructing it to provide compliant\noutputs and observe a significant omission of PII from output.",
        "pdf_link": "https://arxiv.org/pdf/2305.15008v1.pdf"
    },
    {
        "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
        "authors": [
            "Wenxuan Zhang",
            "Yue Deng",
            "Bing Liu",
            "Sinno Jialin Pan",
            "Lidong Bing"
        ],
        "published": "2023-05-24T10:45:25Z",
        "summary": "Sentiment analysis (SA) has been a long-standing research area in natural\nlanguage processing. It can offer rich insights into human sentiments and\nopinions and has thus seen considerable interest from both academia and\nindustry. With the advent of large language models (LLMs) such as ChatGPT,\nthere is a great potential for their employment on SA problems. However, the\nextent to which existing LLMs can be leveraged for different sentiment analysis\ntasks remains unclear. This paper aims to provide a comprehensive investigation\ninto the capabilities of LLMs in performing various sentiment analysis tasks,\nfrom conventional sentiment classification to aspect-based sentiment analysis\nand multifaceted analysis of subjective texts. We evaluate performance across\n13 tasks on 26 datasets and compare the results against small language models\n(SLMs) trained on domain-specific datasets. Our study reveals that while LLMs\ndemonstrate satisfactory performance in simpler tasks, they lag behind in more\ncomplex tasks requiring deeper understanding or structured sentiment\ninformation. However, LLMs significantly outperform SLMs in few-shot learning\nsettings, suggesting their potential when annotation resources are limited. We\nalso highlight the limitations of current evaluation practices in assessing\nLLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a\nmore comprehensive and realistic evaluation. Data and code during our\ninvestigations are available at\n\\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.",
        "pdf_link": "https://arxiv.org/pdf/2305.15005v1.pdf"
    },
    {
        "title": "LLMDet: A Third Party Large Language Models Generated Text Detection Tool",
        "authors": [
            "Kangxi Wu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-24T10:45:16Z",
        "summary": "Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x5.0 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.",
        "pdf_link": "https://arxiv.org/pdf/2305.15004v3.pdf"
    },
    {
        "title": "A RelEntLess Benchmark for Modelling Graded Relations between Named Entities",
        "authors": [
            "Asahi Ushio",
            "Jose Camacho Collados",
            "Steven Schockaert"
        ],
        "published": "2023-05-24T10:41:24Z",
        "summary": "Relations such as \"is influenced by\", \"is known for\" or \"is a competitor of\"\nare inherently graded: we can rank entity pairs based on how well they satisfy\nthese relations, but it is hard to draw a line between those pairs that satisfy\nthem and those that do not. Such graded relations play a central role in many\napplications, yet they are typically not covered by existing Knowledge Graphs.\nIn this paper, we consider the possibility of using Large Language Models\n(LLMs) to fill this gap. To this end, we introduce a new benchmark, in which\nentity pairs have to be ranked according to how much they satisfy a given\ngraded relation. The task is formulated as a few-shot ranking problem, where\nmodels only have access to a description of the relation and five prototypical\ninstances. We use the proposed benchmark to evaluate state-of-the-art relation\nembedding strategies as well as several recent LLMs, covering both publicly\navailable LLMs and closed models such as GPT-4. Overall, we find a strong\ncorrelation between model size and performance, with smaller Language Models\nstruggling to outperform a naive baseline. The results of the largest Flan-T5\nand OPT models are remarkably strong, although a clear gap with human\nperformance remains.",
        "pdf_link": "https://arxiv.org/pdf/2305.15002v2.pdf"
    },
    {
        "title": "Enabling and Analyzing How to Efficiently Extract Information from Hybrid Long Documents with LLMs",
        "authors": [
            "Chongjian Yue",
            "Xinrun Xu",
            "Xiaojun Ma",
            "Lun Du",
            "Hengyu Liu",
            "Zhiming Ding",
            "Yanbing Jiang",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023-05-24T10:35:58Z",
        "summary": "Large Language Models (LLMs) demonstrate exceptional performance in textual\nunderstanding and tabular reasoning tasks. However, their ability to comprehend\nand analyze hybrid text, containing textual and tabular data, remains\nunderexplored. In this research, we specialize in harnessing the potential of\nLLMs to comprehend critical information from financial reports, which are\nhybrid long-documents. We propose an Automated Financial Information Extraction\n(AFIE) framework that enhances LLMs' ability to comprehend and extract\ninformation from financial reports. To evaluate AFIE, we develop a Financial\nReports Numerical Extraction (FINE) dataset and conduct an extensive\nexperimental analysis. Our framework is effectively validated on GPT-3.5 and\nGPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively,\ncompared to a naive method. These results suggest that the AFIE framework\noffers accuracy for automated numerical extraction from complex, hybrid\ndocuments.",
        "pdf_link": "https://arxiv.org/pdf/2305.16344v2.pdf"
    },
    {
        "title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",
        "authors": [
            "Dongjie Yang",
            "Ruifeng Yuan",
            "Yuantao Fan",
            "Yifei Yang",
            "Zili Wang",
            "Shusen Wang",
            "Hai Zhao"
        ],
        "published": "2023-05-24T10:30:42Z",
        "summary": "Large Language Models (LLMs) have attained the impressive capability to\nresolve a wide range of NLP tasks by fine-tuning high-quality instruction data.\nHowever, collecting human-written data of high quality, especially multi-turn\ndialogues, is expensive and unattainable for most people. Though previous\nstudies have used powerful LLMs to generate the dialogues automatically, they\nall suffer from generating untruthful dialogues because of the model\nhallucination. Therefore, we propose a method called RefGPT to generate\nenormous truthful and customized dialogues without worrying about factual\nerrors caused by the model hallucination. RefGPT solves the model hallucination\nin dialogue generation by restricting the LLMs to leverage the given reference\ninstead of reciting their own knowledge to generate dialogues. Additionally,\nRefGPT adds detailed controls on every utterance to enable high customization\ncapability, which previous studies have ignored. On the basis of RefGPT, we\nalso propose two high-quality dialogue datasets generated by GPT-4, namely\nRefGPT-Fact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn\ndialogues based on factual knowledge and RefGPT-Code has 76k multi-turn\ndialogues covering a wide range of coding scenarios. Our code and datasets are\nreleased in https://github.com/mutonix/RefGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.14994v3.pdf"
    },
    {
        "title": "Controlling Pre-trained Language Models for Grade-Specific Text Simplification",
        "authors": [
            "Sweta Agrawal",
            "Marine Carpuat"
        ],
        "published": "2023-05-24T10:29:45Z",
        "summary": "Text simplification (TS) systems rewrite text to make it more readable while\npreserving its content. However, what makes a text easy to read depends on the\nintended readers. Recent work has shown that pre-trained language models can\nsimplify text using a wealth of techniques to control output simplicity,\nranging from specifying only the desired reading grade level, to directly\nspecifying low-level edit operations. Yet it remains unclear how to set these\ncontrol parameters in practice. Existing approaches set them at the corpus\nlevel, disregarding the complexity of individual inputs and considering only\none level of output complexity. In this work, we conduct an empirical study to\nunderstand how different control mechanisms impact the adequacy and simplicity\nof text simplification systems. Based on these insights, we introduce a simple\nmethod that predicts the edit operations required for simplifying a text for a\nspecific grade level on an instance-per-instance basis. This approach improves\nthe quality of the simplified outputs over corpus-level search-based\nheuristics.",
        "pdf_link": "https://arxiv.org/pdf/2305.14993v2.pdf"
    },
    {
        "title": "Reasoning with Language Model is Planning with World Model",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Jiahua Hong",
            "Zhen Wang",
            "Daisy Zhe Wang",
            "Zhiting Hu"
        ],
        "published": "2023-05-24T10:28:28Z",
        "summary": "Large language models (LLMs) have shown remarkable reasoning capabilities,\nespecially when prompted to generate intermediate reasoning steps (e.g.,\nChain-of-Thought, CoT). However, LLMs can still struggle with problems that are\neasy for humans, such as generating action plans for executing tasks in a given\nenvironment, or performing complex math, logical, and commonsense reasoning.\nThe deficiency stems from the key fact that LLMs lack an internal\n$\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment\nstatus, intermediate variable values) and simulate long-term outcomes of\nactions. This prevents LLMs from performing deliberate planning akin to human\nbrains, which involves exploring alternative reasoning paths, anticipating\nfuture states and rewards, and iteratively refining existing reasoning steps.\nTo overcome the limitations, we propose a new LLM reasoning framework,\n$\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning\n$\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning\nagent, and incorporates a principled planning algorithm (based on Monto Carlo\nTree Search) for strategic exploration in the vast reasoning space. During\nreasoning, the LLM (as agent) incrementally builds a reasoning tree under the\nguidance of the LLM (as world model) and task-specific rewards, and obtains a\nhigh-reward reasoning path efficiently with a proper balance between\nexploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of\nchallenging reasoning problems including plan generation, math reasoning, and\nlogical inference. Empirical results on these tasks demonstrate the superiority\nof RAP over various strong baselines, including CoT and least-to-most prompting\nwith self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33%\nrelative improvement in a plan generation setting.",
        "pdf_link": "https://arxiv.org/pdf/2305.14992v2.pdf"
    },
    {
        "title": "Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios",
        "authors": [
            "Yilun Zhao",
            "Haowei Zhang",
            "Shengyun Si",
            "Linyong Nan",
            "Xiangru Tang",
            "Arman Cohan"
        ],
        "published": "2023-05-24T10:22:30Z",
        "summary": "Tabular data is prevalent across various industries, necessitating\nsignificant time and effort for users to understand and manipulate for their\ninformation-seeking purposes. The advancements in large language models (LLMs)\nhave shown enormous potential to improve user efficiency. However, the adoption\nof LLMs in real-world applications for table information seeking remains\nunderexplored. In this paper, we investigate the table-to-text capabilities of\ndifferent LLMs using four datasets within two real-world information seeking\nscenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets\nfor data insight generation, along with the FeTaQA and our newly-constructed\nF2WTQ datasets for query-based generation. We structure our investigation\naround three research questions, evaluating the performance of LLMs in\ntable-to-text generation, automated evaluation, and feedback generation,\nrespectively. Experimental results indicate that the current high-performing\nLLM, specifically GPT-4, can effectively serve as a table-to-text generator,\nevaluator, and feedback generator, facilitating users' information seeking\npurposes in real-world scenarios. However, a significant performance gap still\nexists between other open-sourced LLMs (e.g., Tulu and LLaMA-2) and GPT-4\nmodels. Our data and code are publicly available at\nhttps://github.com/yale-nlp/LLM-T2T.",
        "pdf_link": "https://arxiv.org/pdf/2305.14987v2.pdf"
    },
    {
        "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models",
        "authors": [
            "Haoxuan You",
            "Rui Sun",
            "Zhecan Wang",
            "Long Chen",
            "Gengyu Wang",
            "Hammad A. Ayyubi",
            "Kai-Wei Chang",
            "Shih-Fu Chang"
        ],
        "published": "2023-05-24T10:19:57Z",
        "summary": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT",
        "pdf_link": "https://arxiv.org/pdf/2305.14985v1.pdf"
    },
    {
        "title": "GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Abdul Waheed",
            "El Moatez Billah Nagoudi",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2023-05-24T10:12:39Z",
        "summary": "ChatGPT's emergence heralds a transformative phase in NLP, particularly\ndemonstrated through its excellent performance on many English benchmarks.\nHowever, the model's efficacy across diverse linguistic contexts remains\nlargely uncharted territory. This work aims to bridge this knowledge gap, with\na primary focus on assessing ChatGPT's capabilities on Arabic languages and\ndialectal varieties. Our comprehensive study conducts a large-scale automated\nand human evaluation of ChatGPT, encompassing 44 distinct language\nunderstanding and generation tasks on over 60 different datasets. To our\nknowledge, this marks the first extensive performance analysis of ChatGPT's\ndeployment in Arabic NLP. Our findings indicate that, despite its remarkable\nperformance in English, ChatGPT is consistently surpassed by smaller models\nthat have undergone finetuning on Arabic. We further undertake a meticulous\ncomparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), unveiling the relative shortcomings of both models in handling\nArabic dialects compared to MSA. Although we further explore and confirm the\nutility of employing GPT-4 as a potential alternative for human evaluation, our\nwork adds to a growing body of research underscoring the limitations of\nChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.14976v2.pdf"
    },
    {
        "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
        "authors": [
            "Katherine Tian",
            "Eric Mitchell",
            "Allan Zhou",
            "Archit Sharma",
            "Rafael Rafailov",
            "Huaxiu Yao",
            "Chelsea Finn",
            "Christopher D. Manning"
        ],
        "published": "2023-05-24T10:12:33Z",
        "summary": "A trustworthy real-world prediction system should produce well-calibrated\nconfidence scores; that is, its confidence in an answer should be indicative of\nthe likelihood that the answer is correct, enabling deferral to an expert in\ncases of low-confidence predictions. Recent studies have shown that\nunsupervised pre-training produces large language models (LMs) whose\nconditional probabilities are remarkably well-calibrated. However, the most\nwidely-used LMs are fine-tuned with reinforcement learning from human feedback\n(RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional\nprobabilities that are very poorly calibrated. In light of this perceived\nweakness, we conduct a broad evaluation of methods for extracting confidence\nscores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find\nthat verbalized confidences emitted as output tokens are typically\nbetter-calibrated than the model's conditional probabilities on the TriviaQA,\nSciQ, and TruthfulQA benchmarks, often reducing the expected calibration error\nby a relative 50%.",
        "pdf_link": "https://arxiv.org/pdf/2305.14975v2.pdf"
    },
    {
        "title": "OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning",
        "authors": [
            "Jiazheng Li",
            "Runcong Zhao",
            "Yongxin Yang",
            "Yulan He",
            "Lin Gui"
        ],
        "published": "2023-05-24T10:08:04Z",
        "summary": "The remarkable performance of pre-trained large language models has\nrevolutionised various natural language processing applications. Due to huge\nparametersizes and extensive running costs, companies or organisations tend to\ntransfer the models to the target task by zero-shot prompting techniques.\nHowever, the prohibitive costs of tokens and time have hindered their adoption\nin applications. We propose OverPrompt, leveraging the in-context learning\ncapability of LLMs to handle multiple task inputs, thereby reducing token and\ntime costs. This approach could potentially improve task performance during API\nqueries due to better conditional distribution mapping. Evaluated across\ndiverse classification datasets, our experiments show that OverPrompt can\nachieve cost-efficient zero-shot classification without causing significant\ndetriment to task performance, and in some cases, even improving it. An\nablation study conducted on various LLMs, along with an investigation into the\nrobustness of our prompting strategy to different input ordering, offers\nvaluable insights into the broader applicability of our method across diverse\ntasks. These findings also suggest a more seamless integration of our method\nwith LLMs through an API.",
        "pdf_link": "https://arxiv.org/pdf/2305.14973v2.pdf"
    },
    {
        "title": "Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks",
        "authors": [
            "Abhinav Rao",
            "Sachin Vashistha",
            "Atharva Naik",
            "Somak Aditya",
            "Monojit Choudhury"
        ],
        "published": "2023-05-24T09:57:37Z",
        "summary": "Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating their prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited\nstudies have been conducted to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We survey existing jailbreak methods and their\neffectiveness on open-source and commercial LLMs (such as GPT-based models,\nOPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak\ndetection in terms of their effectiveness against known attacks. For further\nanalysis, we release a dataset of model outputs across 3700 jailbreak prompts\nover 4 tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14965v4.pdf"
    },
    {
        "title": "Editing Common Sense in Transformers",
        "authors": [
            "Anshita Gupta",
            "Debanjan Mondal",
            "Akshay Krishna Sheshadri",
            "Wenlong Zhao",
            "Xiang Lorraine Li",
            "Sarah Wiegreffe",
            "Niket Tandon"
        ],
        "published": "2023-05-24T09:50:54Z",
        "summary": "Editing model parameters directly in Transformers makes updating open-source\ntransformer-based models possible without re-training (Meng et al., 2023).\nHowever, these editing methods have only been evaluated on statements about\nencyclopedic knowledge with a single correct answer. Commonsense knowledge with\nmultiple correct answers, e.g., an apple can be green or red but not\ntransparent, has not been studied but is as essential for enhancing\ntransformers' reliability and usefulness. In this paper, we investigate whether\ncommonsense judgments are causally associated with localized, editable\nparameters in Transformers, and we provide an affirmative answer. We find that\ndirectly applying the MEMIT editing algorithm results in sub-par performance\nand improve it for the commonsense domain by varying edit tokens and improving\nthe layer selection strategy, i.e., $MEMIT_{CSK}$. GPT-2 Large and XL models\nedited using $MEMIT_{CSK}$ outperform best-fine-tuned baselines by 10.97% and\n10.73% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel\nevaluation dataset, PROBE SET, that contains unaffected and affected\nneighborhoods, affected paraphrases, and affected reasoning challenges.\n$MEMIT_{CSK}$ performs well across the metrics while fine-tuning baselines show\nsignificant trade-offs between unaffected and affected metrics. These results\nsuggest a compelling future direction for incorporating feedback about common\nsense into Transformers through direct model editing.",
        "pdf_link": "https://arxiv.org/pdf/2305.14956v3.pdf"
    },
    {
        "title": "Adversarial Demonstration Attacks on Large Language Models",
        "authors": [
            "Jiongxiao Wang",
            "Zichen Liu",
            "Keun Hee Park",
            "Zhuojun Jiang",
            "Zhaoheng Zheng",
            "Zhuofeng Wu",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "published": "2023-05-24T09:40:56Z",
        "summary": "With the emergence of more powerful large language models (LLMs), such as\nChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence\nin leveraging these models for specific tasks by utilizing data-label pairs as\nprecondition prompts. While incorporating demonstrations can greatly enhance\nthe performance of LLMs across various tasks, it may introduce a new security\nconcern: attackers can manipulate only the demonstrations without changing the\ninput to perform an attack. In this paper, we investigate the security concern\nof ICL from an adversarial perspective, focusing on the impact of\ndemonstrations. We propose a novel attack method named advICL, which aims to\nmanipulate only the demonstration without changing the input to mislead the\nmodels. Our results demonstrate that as the number of demonstrations increases,\nthe robustness of in-context learning would decrease. Additionally, we also\nidentify the intrinsic property of the demonstrations is that they can be used\n(prepended) with different inputs. As a result, it introduces a more practical\nthreat model in which an attacker can attack the test input example even\nwithout knowing and manipulating it. To achieve it, we propose the transferable\nversion of advICL, named Transferable-advICL. Our experiment shows that the\nadversarial demonstration generated by Transferable-advICL can successfully\nattack the unseen test input examples. We hope that our study reveals the\ncritical security risks associated with ICL and underscores the need for\nextensive research on the robustness of ICL, particularly given its increasing\nsignificance in the advancement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.14950v2.pdf"
    },
    {
        "title": "How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench",
        "authors": [
            "Qinyuan Ye",
            "Harvey Yiyun Fu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023-05-24T09:35:34Z",
        "summary": "We investigate the predictability of large language model (LLM) capabilities:\ngiven records of past experiments using different model families, numbers of\nparameters, tasks, and numbers of in-context examples, can we accurately\npredict LLM performance on new experiment configurations? Answering this\nquestion has practical implications for LLM users (e.g., deciding which models\nto try), developers (e.g., prioritizing evaluation on representative tasks),\nand the research community (e.g., identifying hard-to-predict capabilities that\nwarrant further investigation).\n  We study the performance prediction problem on experiment records from\nBIG-bench. On a random train-test split, an MLP-based predictor achieves an\n$R^2$ score greater than 95%, indicating the presence of learnable patterns\nwithin the experiment records. We then formulate the problem of searching for\n\"small-bench,\" an informative subset of BIG-bench tasks from which the\nperformance on the full set can be maximally recovered. We find a subset as\ninformative as BIG-bench Hard for evaluating new model families, while being\n$3\\times$ smaller. Additionally, we find competitive subsets by clustering task\nrepresentations learned by our MLP-based predictor and selecting tasks close to\ncluster centroids, highlighting the importance of task diversity in\nconstructing \"small-bench.\"",
        "pdf_link": "https://arxiv.org/pdf/2305.14947v2.pdf"
    },
    {
        "title": "Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark",
        "authors": [
            "Minje Choi",
            "Jiaxin Pei",
            "Sagar Kumar",
            "Chang Shu",
            "David Jurgens"
        ],
        "published": "2023-05-24T09:21:06Z",
        "summary": "Large language models (LLMs) have been shown to perform well at a variety of\nsyntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed\nin many forms including conversational agents that interact with humans, we\nlack a grounded benchmark to measure how well LLMs understand \\textit{social}\nlanguage. Here, we introduce a new theory-driven benchmark, SocKET, that\ncontains 58 NLP tasks testing social knowledge which we group into five\ncategories: humor & sarcasm, offensiveness, sentiment & emotion, and\ntrustworthiness. In tests on the benchmark, we demonstrate that current models\nattain only moderate performance but reveal significant potential for task\ntransfer among different types and categories of tasks, which were predicted\nfrom theory. Through zero-shot evaluations, we show that pretrained models\nalready possess some innate but limited capabilities of social language\nunderstanding and training on one category of tasks can improve zero-shot\ntesting on others. Our benchmark provides a systematic way to analyze model\nperformance on an important dimension of language and points to clear room for\nimprovement to build more socially-aware LLMs. The associated resources are\nreleased at https://github.com/minjechoi/SOCKET.",
        "pdf_link": "https://arxiv.org/pdf/2305.14938v2.pdf"
    },
    {
        "title": "GRACE: Discriminator-Guided Chain-of-Thought Reasoning",
        "authors": [
            "Muhammad Khalifa",
            "Lajanugen Logeswaran",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "published": "2023-05-24T09:16:51Z",
        "summary": "In the context of multi-step reasoning, e.g., with chain-of-thought, language\nmodels (LMs) can easily assign a high likelihood to incorrect steps. As a\nresult, decoding strategies that optimize for solution likelihood often yield\nincorrect solutions. To address this issue, we propose Guiding chain-of-thought\nReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding\napproach that steers the decoding process towards producing correct reasoning\nsteps. GRACE employs a discriminator trained with a contrastive loss over\ncorrect and incorrect steps, which is used during decoding to score next-step\ncandidates based on their correctness. Importantly, GRACE only requires\nsampling from the LM, without the need for LM training or fine-tuning. Using\nmodels from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and\ntwo symbolic reasoning tasks, where it exhibits substantial performance gains\ncompared to greedy decoding, verifiers, and self-consistency in most settings.\nWhen further combined with self-consistency, GRACE outperforms all the\nbaselines by sizeable margins. Human and LLM evaluations over GSM8K show that\nGRACE not only improves the final answer accuracy but also the correctness of\nthe intermediate reasoning. Our implementation can be accessed at\n\\url{https://github.com/mukhal/grace}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14934v2.pdf"
    },
    {
        "title": "In-Context Impersonation Reveals Large Language Models' Strengths and Biases",
        "authors": [
            "Leonard Salewski",
            "Stephan Alaniz",
            "Isabel Rio-Torto",
            "Eric Schulz",
            "Zeynep Akata"
        ],
        "published": "2023-05-24T09:13:15Z",
        "summary": "In everyday conversations, humans can take on different roles and adapt their\nvocabulary to their chosen roles. We explore whether LLMs can take on, that is\nimpersonate, different roles when they generate text in-context. We ask LLMs to\nassume different personas before solving vision and language tasks. We do this\nby prefixing the prompt with a persona that is associated either with a social\nidentity or domain expertise. In a multi-armed bandit task, we find that LLMs\npretending to be children of different ages recover human-like developmental\nstages of exploration. In a language-based reasoning task, we find that LLMs\nimpersonating domain experts perform better than LLMs impersonating non-domain\nexperts. Finally, we test whether LLMs' impersonations are complementary to\nvisual information when describing different categories. We find that\nimpersonation can improve performance: an LLM prompted to be a bird expert\ndescribes birds better than one prompted to be a car expert. However,\nimpersonation can also uncover LLMs' biases: an LLM prompted to be a man\ndescribes cars better than one prompted to be a woman. These findings\ndemonstrate that LLMs are capable of taking on diverse roles and that this\nin-context impersonation can be used to uncover their hidden strengths and\nbiases.",
        "pdf_link": "https://arxiv.org/pdf/2305.14930v2.pdf"
    },
    {
        "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
        "authors": [
            "Kellin Pelrine",
            "Anne Imouza",
            "Camille Thibault",
            "Meilina Reksoprodjo",
            "Caleb Gupta",
            "Joel Christoph",
            "Jean-François Godbout",
            "Reihaneh Rabbany"
        ],
        "published": "2023-05-24T09:10:20Z",
        "summary": "Misinformation poses a critical societal challenge, and current approaches\nhave yet to produce an effective solution. We propose focusing on\ngeneralization, uncertainty, and how to leverage recent large language models,\nin order to create more practical tools to evaluate information veracity in\ncontexts where perfect classification is impossible. We first demonstrate that\nGPT-4 can outperform prior methods in multiple settings and languages. Next, we\nexplore generalization, revealing that GPT-4 and RoBERTa-large exhibit\ndifferences in failure modes. Third, we propose techniques to handle\nuncertainty that can detect impossible examples and strongly improve outcomes.\nWe also discuss results on other language models, temperature, prompting,\nversioning, explainability, and web retrieval, each one providing practical\ninsights and directions for future research. Finally, we publish the LIAR-New\ndataset with novel paired English and French misinformation data and\nPossibility labels that indicate if there is sufficient context for veracity\nevaluation. Overall, this research lays the groundwork for future tools that\ncan drive real-world progress to combat misinformation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14928v3.pdf"
    },
    {
        "title": "Universal Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hootan Nakhost",
            "Hanjun Dai",
            "Julian Martin Eisenschlos",
            "Sercan O. Arik",
            "Tomas Pfister"
        ],
        "published": "2023-05-24T09:09:48Z",
        "summary": "A hallmark of modern large language models (LLMs) is their impressive general\nzero-shot and few-shot abilities, often elicited through in-context learning\n(ICL) via prompting. However, while highly coveted and being the most general,\nzero-shot performances in LLMs are still typically weaker due to the lack of\nguidance and the difficulty of applying existing automatic prompt design\nmethods in general tasks when ground-truth labels are unavailable. In this\nstudy, we address this by presenting Universal Self-Adaptive Prompting (USP),\nan automatic prompt design approach specifically tailored for zero-shot\nlearning (while compatible with few-shot). Requiring only a small amount of\nunlabeled data and an inference-only LLM, USP is highly versatile: to achieve\nuniversal prompting, USP categorizes a possible NLP task into one of the three\npossible task types and then uses a corresponding selector to select the most\nsuitable queries and zero-shot model-generated responses as\npseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a\nfully automated way. We evaluate USP with PaLM and PaLM 2 models and\ndemonstrate performances that are considerably stronger than standard zero-shot\nbaselines and often comparable to or even superior to few-shot baselines across\nmore than 40 natural language understanding, natural language generation, and\nreasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14926v2.pdf"
    },
    {
        "title": "Frugal Prompting for Dialog Models",
        "authors": [
            "Bishal Santra",
            "Sakya Basak",
            "Abhinandan De",
            "Manish Gupta",
            "Pawan Goyal"
        ],
        "published": "2023-05-24T09:06:49Z",
        "summary": "The use of large language models (LLMs) in natural language processing (NLP)\ntasks is rapidly increasing, leading to changes in how researchers approach\nproblems in the field. To fully utilize these models' abilities, a better\nunderstanding of their behavior for different input protocols is required. With\nLLMs, users can directly interact with the models through a text-based\ninterface to define and solve various tasks. Hence, understanding the\nconversational abilities of these LLMs, which may not have been specifically\ntrained for dialog modeling, is also important. This study examines different\napproaches for building dialog systems using LLMs by considering various\naspects of the prompt. As part of prompt tuning, we experiment with various\nways of providing instructions, exemplars, current query and additional\ncontext. The research also analyzes the representations of dialog history that\nhave the optimal usable-information density. Based on the findings, the paper\nsuggests more compact ways of providing dialog history information while\nensuring good performance and reducing model's inference-API costs. The\nresearch contributes to a better understanding of how LLMs can be effectively\nused for building interactive systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.14919v2.pdf"
    },
    {
        "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
        "authors": [
            "Lin Guan",
            "Karthik Valmeekam",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "published": "2023-05-24T08:59:15Z",
        "summary": "There is a growing interest in applying pre-trained large language models\n(LLMs) to planning problems. However, methods that use LLMs directly as\nplanners are currently impractical due to several factors, including limited\ncorrectness of plans, strong reliance on feedback from interactions with\nsimulators or even the actual environment, and the inefficiency in utilizing\nhuman feedback. In this work, we introduce a novel alternative paradigm that\nconstructs an explicit world (domain) model in planning domain definition\nlanguage (PDDL) and then uses it to plan with sound domain-independent\nplanners. To address the fact that LLMs may not generate a fully functional\nPDDL model initially, we employ LLMs as an interface between PDDL and sources\nof corrective feedback, such as PDDL validators and humans. For users who lack\na background in PDDL, we show that LLMs can translate PDDL into natural\nlanguage and effectively encode corrective feedback back to the underlying\ndomain model. Our framework not only enjoys the correctness guarantee offered\nby the external planners but also reduces human involvement by allowing users\nto correct domain models at the beginning, rather than inspecting and\ncorrecting (through interactive prompting) every generated plan as in previous\nwork. On two IPC domains and a Household domain that is more complicated than\ncommonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be\nleveraged to produce high-quality PDDL models for over 40 actions, and the\ncorrected PDDL models are then used to successfully solve 48 challenging\nplanning tasks. Resources, including the source code, are released at:\nhttps://guansuns.github.io/pages/llm-dm.",
        "pdf_link": "https://arxiv.org/pdf/2305.14909v2.pdf"
    },
    {
        "title": "PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions",
        "authors": [
            "Anthony Chen",
            "Panupong Pasupat",
            "Sameer Singh",
            "Hongrae Lee",
            "Kelvin Guu"
        ],
        "published": "2023-05-24T08:59:00Z",
        "summary": "The remarkable capabilities of large language models have been accompanied by\na persistent drawback: the generation of false and unsubstantiated claims\ncommonly known as \"hallucinations\". To combat this issue, recent research has\nintroduced approaches that involve editing and attributing the outputs of\nlanguage models, particularly through prompt-based editing. However, the\ninference cost and speed of using large language models for editing currently\nbottleneck prompt-based methods. These bottlenecks motivate the training of\ncompact editors, which is challenging due to the scarcity of training data for\nthis purpose. To overcome these challenges, we exploit the power of large\nlanguage models to introduce corruptions (i.e., noise) into text and\nsubsequently fine-tune compact editors to denoise the corruptions by\nincorporating relevant evidence. Our methodology is entirely unsupervised and\nprovides us with faux hallucinations for training in any domain. Our Petite\nUnsupervised Research and Revision model, PURR, not only improves attribution\nover existing editing methods based on fine-tuning and prompting, but also\nachieves faster execution times by orders of magnitude.",
        "pdf_link": "https://arxiv.org/pdf/2305.14908v1.pdf"
    },
    {
        "title": "Coverage-based Example Selection for In-Context Learning",
        "authors": [
            "Shivanshu Gupta",
            "Matt Gardner",
            "Sameer Singh"
        ],
        "published": "2023-05-24T08:58:28Z",
        "summary": "In-context learning (ICL), the ability of large language models to perform\nnovel tasks by conditioning on a prompt with a few task examples, requires\nthese examples to be informative about the test instance. The standard approach\nof independently ranking and selecting the most similar examples selects\nredundant examples while omitting important information. In this work, we show\nthat BERTScore-Recall (BSR) selects better examples that demonstrate more of\nthe salient aspects, e.g. reasoning patterns, of the test input. We further\nextend BSR and many standard metrics to easily optimizable set-level metrics,\ngiving still better coverage of those salient aspects. On 15 datasets spanning\n6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric\nfor in-context example selection across the board, and (2) for compositional\ntasks, set selection using Set-BSR outperforms independent ranking by up to 17\npoints on average and, despite being training-free, surpasses methods that\nleverage task or LLM-specific training.",
        "pdf_link": "https://arxiv.org/pdf/2305.14907v3.pdf"
    },
    {
        "title": "M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection",
        "authors": [
            "Yuxia Wang",
            "Jonibek Mansurov",
            "Petar Ivanov",
            "Jinyan Su",
            "Artem Shelmanov",
            "Akim Tsvigun",
            "Chenxi Whitehouse",
            "Osama Mohammed Afzal",
            "Tarek Mahmoud",
            "Toru Sasaki",
            "Thomas Arnold",
            "Alham Fikri Aji",
            "Nizar Habash",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2023-05-24T08:55:11Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capability to\ngenerate fluent responses to a wide variety of user queries. However, this has\nalso raised concerns about the potential misuse of such texts in journalism,\neducation, and academia. In this study, we strive to create automated systems\nthat can detect machine-generated texts and pinpoint potential misuse. We first\nintroduce a large-scale benchmark \\textbf{M4}, which is a multi-generator,\nmulti-domain, and multi-lingual corpus for machine-generated text detection.\nThrough an extensive empirical study of this dataset, we show that it is\nchallenging for detectors to generalize well on instances from unseen domains\nor LLMs. In such cases, detectors tend to misclassify machine-generated text as\nhuman-written. These results show that the problem is far from solved and that\nthere is a lot of room for improvement. We believe that our dataset will enable\nfuture research towards more robust approaches to this pressing societal\nproblem. The dataset is available at https://github.com/mbzuai-nlp/M4.",
        "pdf_link": "https://arxiv.org/pdf/2305.14902v2.pdf"
    },
    {
        "title": "PIVOINE: Instruction Tuning for Open-world Information Extraction",
        "authors": [
            "Keming Lu",
            "Xiaoman Pan",
            "Kaiqiang Song",
            "Hongming Zhang",
            "Dong Yu",
            "Jianshu Chen"
        ],
        "published": "2023-05-24T08:52:08Z",
        "summary": "We consider the problem of Open-world Information Extraction (Open-world IE),\nwhich extracts comprehensive entity profiles from unstructured texts. Different\nfrom the conventional closed-world setting of Information Extraction (IE),\nOpen-world IE considers a more general situation where entities and relations\ncould be beyond a predefined ontology. More importantly, we seek to develop a\nlarge language model (LLM) that is able to perform Open-world IE to extract\ndesirable entity profiles characterized by (possibly fine-grained) natural\nlanguage instructions. We achieve this by finetuning LLMs using instruction\ntuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction\ntuning dataset for Open-world IE enriched with a comprehensive corpus,\nextensive annotations, and diverse instructions. We finetune the pretrained\nBLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE\nwith strong instruction-following capabilities. Our experiments demonstrate\nthat PIVOINE significantly outperforms traditional closed-world methods and\nother LLM baselines, displaying impressive generalization capabilities on both\nunseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as\na promising solution to tackle the open-world challenge in IE effectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.14898v1.pdf"
    },
    {
        "title": "Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory",
        "authors": [
            "Ziang Xiao",
            "Susu Zhang",
            "Vivian Lai",
            "Q. Vera Liao"
        ],
        "published": "2023-05-24T08:38:23Z",
        "summary": "We address a fundamental challenge in Natural Language Generation (NLG) model\nevaluation -- the design and evaluation of evaluation metrics. Recognizing the\nlimitations of existing automatic metrics and noises from how current human\nevaluation was conducted, we propose MetricEval, a framework informed by\nmeasurement theory, the foundation of educational test design, for\nconceptualizing and evaluating the reliability and validity of NLG evaluation\nmetrics. The framework formalizes the source of measurement error and offers\nstatistical tools for evaluating evaluation metrics based on empirical data.\nWith our framework, one can quantify the uncertainty of the metrics to better\ninterpret the result. To exemplify the use of our framework in practice, we\nanalyzed a set of evaluation metrics for summarization and identified issues\nrelated to conflated validity structure in human-eval and reliability in\nLLM-based metrics. Through MetricEval, we aim to promote the design,\nevaluation, and interpretation of valid and reliable metrics to advance robust\nand effective NLG models.",
        "pdf_link": "https://arxiv.org/pdf/2305.14889v2.pdf"
    },
    {
        "title": "Leveraging GPT-4 for Automatic Translation Post-Editing",
        "authors": [
            "Vikas Raunak",
            "Amr Sharaf",
            "Yiren Wang",
            "Hany Hassan Awadallah",
            "Arul Menezes"
        ],
        "published": "2023-05-24T08:30:05Z",
        "summary": "While Neural Machine Translation (NMT) represents the leading approach to\nMachine Translation (MT), the outputs of NMT models still require translation\npost-editing to rectify errors and enhance quality under critical settings. In\nthis work, we formalize the task of direct translation post-editing with Large\nLanguage Models (LLMs) and explore the use of GPT-4 to automatically post-edit\nNMT outputs across several language pairs. Our results demonstrate that GPT-4\nis adept at translation post-editing, producing meaningful and trustworthy\nedits to translations that help improve its general quality as well as remove\ndifferent classes of major errors in translations. In particular, human\nevaluations on assessing edit trustworthiness show that GPT-4 exhibits a large\nimprovement over the prior state-of-the-art LLM. Notably, we improve upon\nstate-of-the-art performance on WMT-22 English-Chinese, English-German,\nChinese-English and German-English language pairs using GPT-4 based\npost-editing, as evaluated by state-of-the-art MT quality metrics. However, we\nalso show that GPT-4 could produce hallucinated edits, thereby urging caution\nin its use as an expert translation post-editor.",
        "pdf_link": "https://arxiv.org/pdf/2305.14878v2.pdf"
    },
    {
        "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
        "authors": [
            "Yuwei Zhang",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2023-05-24T08:24:25Z",
        "summary": "We introduce ClusterLLM, a novel text clustering framework that leverages\nfeedback from an instruction-tuned large language model, such as ChatGPT.\nCompared with traditional unsupervised methods that builds upon \"small\"\nembedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the\nemergent capability of LLM even if its embeddings are inaccessible; and (2) it\nunderstands the user's preference on clustering through textual instruction\nand/or a few annotated data. First, we prompt ChatGPT for insights on\nclustering perspective by constructing hard triplet questions <does A better\ncorrespond to B than C>, where A, B and C are similar data points that belong\nto different clusters according to small embedder. We empirically show that\nthis strategy is both effective for fine-tuning small embedder and\ncost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on\nclustering granularity by carefully designed pairwise questions <do A and B\nbelong to the same category>, and tune the granularity from cluster hierarchies\nthat is the most consistent with the ChatGPT answers. Extensive experiments on\n14 datasets show that ClusterLLM consistently improves clustering quality, at\nan average cost of ~$0.6 per dataset. The code will be available at\nhttps://github.com/zhang-yu-wei/ClusterLLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.14871v2.pdf"
    },
    {
        "title": "CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering",
        "authors": [
            "Weiqi Wang",
            "Tianqing Fang",
            "Wenxuan Ding",
            "Baixuan Xu",
            "Xin Liu",
            "Yangqiu Song",
            "Antoine Bosselut"
        ],
        "published": "2023-05-24T08:21:31Z",
        "summary": "The task of zero-shot commonsense question answering evaluates models on\ntheir capacity to reason about general scenarios beyond those presented in\nspecific datasets. Existing approaches for tackling this task leverage external\nknowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on\nsynthetic QA pairs constructed from CSKBs. In these approaches, negative\nexamples (distractors) are formulated by randomly sampling from CSKBs using\nfairly primitive keyword constraints. However, two bottlenecks limit these\napproaches: the inherent incompleteness of CSKBs limits the semantic coverage\nof synthetic QA pairs, and the lack of human annotations makes the sampled\nnegative examples potentially uninformative and contradictory. To tackle these\nlimitations above, we propose Conceptualization-Augmented Reasoner (CAR), a\nzero-shot commonsense question-answering framework that fully leverages the\npower of conceptualization. Specifically, CAR abstracts a commonsense knowledge\ntriple to many higher-level instances, which increases the coverage of CSKB and\nexpands the ground-truth answer space, reducing the likelihood of selecting\nfalse-negative distractors. Extensive experiments demonstrate that CAR more\nrobustly generalizes to answering questions about zero-shot commonsense\nscenarios than existing methods, including large language models, such as\nGPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at\nhttps://github.com/HKUST-KnowComp/CAR.",
        "pdf_link": "https://arxiv.org/pdf/2305.14869v2.pdf"
    },
    {
        "title": "How To Train Your (Compressed) Large Language Model",
        "authors": [
            "Ananya Harsh Jha",
            "Tom Sherborne",
            "Evan Pete Walsh",
            "Dirk Groeneveld",
            "Emma Strubell",
            "Iz Beltagy"
        ],
        "published": "2023-05-24T08:18:35Z",
        "summary": "With the increase in the size of large language models (LLMs), we need\ncompression methods that can reduce the model size while preserving the\ngenerality and zero-shot promptability of the model. This goal is more\nambitious than the typical compression setup, which reduces the model's size at\nthe expense of specializing it to a specific end-task. To study this, we\ndevelop a task-agnostic compression pipeline with a large-scale evaluation\ncomprising language modeling perplexity and 12 zero-shot end-tasks. Our results\nshow that a simple layer-wise pruning followed by continued language model\npretraining matches or outperforms three existing state-of-the-art baselines\nwhile being 1.5x more computationally efficient. However, unlike typical\ntask-specialized compression, our best-compressed model significantly\nunderperforms a similar-sized model trained from scratch. We posit the\nhalf-sized pretrained model as an upper bound for task-agnostic compression and\ncall for future work to bridge this gap under a reasonable token budget. Our\nfindings highlight the inadequacy of existing compression methods for LLMs and\nestablish a requirement for new methods that preserve a model's generality and\nzero-shot promptability under compression. We release our code and evaluation\nsetup to facilitate reproducibility and help iterate on method design.",
        "pdf_link": "https://arxiv.org/pdf/2305.14864v2.pdf"
    },
    {
        "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
        "authors": [
            "Akari Asai",
            "Sneha Kudugunta",
            "Xinyan Velocity Yu",
            "Terra Blevins",
            "Hila Gonen",
            "Machel Reid",
            "Yulia Tsvetkov",
            "Sebastian Ruder",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-05-24T08:06:33Z",
        "summary": "Despite remarkable advancements in few-shot generalization in natural\nlanguage processing, most models are developed and evaluated primarily in\nEnglish. To facilitate research on few-shot cross-lingual transfer, we\nintroduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across\n54 languages in a sequence-to-sequence format and provides a fixed set of\nfew-shot examples and instructions. BUFFET is designed to establish a rigorous\nand equitable evaluation framework for few-shot cross-lingual transfer across a\nbroad range of tasks and languages. Using BUFFET, we perform thorough\nevaluations of state-of-the-art multilingual large language models with\ndifferent transfer methods, namely in-context learning and fine-tuning. Our\nfindings reveal significant room for improvement in few-shot in-context\ncross-lingual transfer. In particular, ChatGPT with in-context learning often\nperforms worse than much smaller mT5-base models fine-tuned on English task\ndata and few-shot in-language examples. Our analysis suggests various avenues\nfor future research in few-shot cross-lingual transfer, such as improved\npretraining, understanding, and future evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2305.14857v1.pdf"
    },
    {
        "title": "SummIt: Iterative Text Summarization via ChatGPT",
        "authors": [
            "Haopeng Zhang",
            "Xiao Liu",
            "Jiawei Zhang"
        ],
        "published": "2023-05-24T07:40:06Z",
        "summary": "Text summarization systems have made significant progress in recent years,\nbut typically generate summaries in one single step. However, the one-shot\nsummarization setting is sometimes inadequate, as the generated summary may\ncontain hallucinations or overlook essential details related to the reader's\ninterests. This paper addresses this limitation by proposing SummIt, an\niterative text summarization framework based on large language models like\nChatGPT. Our framework enables the model to refine the generated summary\niteratively through self-evaluation and feedback, resembling humans' iterative\nprocess when drafting and revising summaries. Furthermore, we explore the\npotential benefits of integrating knowledge and topic extractors into the\nframework to enhance summary faithfulness and controllability. We automatically\nevaluate the performance of our framework on three benchmark summarization\ndatasets. We also conduct a human evaluation to validate the effectiveness of\nthe iterative refinements and identify a potential issue of over-correction.",
        "pdf_link": "https://arxiv.org/pdf/2305.14835v2.pdf"
    },
    {
        "title": "PromptNER: Prompting For Named Entity Recognition",
        "authors": [
            "Dhananjay Ashok",
            "Zachary C. Lipton"
        ],
        "published": "2023-05-24T07:38:24Z",
        "summary": "In a surprising turn, Large Language Models (LLMs) together with a growing\narsenal of prompt-based heuristics now offer powerful off-the-shelf approaches\nproviding few-shot solutions to myriad classic NLP problems. However, despite\npromising early results, these LLM-based few-shot methods remain far from the\nstate of the art in Named Entity Recognition (NER), where prevailing methods\ninclude learning representations via end-to-end structural understanding and\nfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,\na new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to\nany new NER task PromptNER requires a set of entity definitions in addition to\nthe standard few-shot examples. Given a sentence, PromptNER prompts an LLM to\nproduce a list of potential entities along with corresponding explanations\njustifying their compatibility with the provided entity type definitions.\nRemarkably, PromptNER achieves state-of-the-art performance on few-shot NER,\nachieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9%\n(absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on\nthe FewNERD dataset. PromptNER also moves the state of the art on Cross Domain\nNER, outperforming prior methods (including those not limited to the few-shot\nsetting), setting a new mark on 3/5 CrossNER target domains, with an average F1\ngain of 3%, despite using less than 2% of the available data.",
        "pdf_link": "https://arxiv.org/pdf/2305.15444v2.pdf"
    },
    {
        "title": "Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation",
        "authors": [
            "Prashant Krishnan",
            "Zilong Wang",
            "Yangkun Wang",
            "Jingbo Shang"
        ],
        "published": "2023-05-24T07:34:33Z",
        "summary": "Recent advances of incorporating layout information, typically bounding box\ncoordinates, into pre-trained language models have achieved significant\nperformance in entity recognition from document images. Using coordinates can\neasily model the absolute position of each token, but they might be sensitive\nto manipulations in document images (e.g., shifting, rotation or scaling),\nespecially when the training data is limited in few-shot settings. In this\npaper, we propose to further introduce the topological adjacency relationship\namong the tokens, emphasizing their relative position information.\nSpecifically, we consider the tokens in the documents as nodes and formulate\nthe edges based on the topological heuristics from the k-nearest bounding\nboxes. Such adjacency graphs are invariant to affine transformations including\nshifting, rotations and scaling. We incorporate these graphs into the\npre-trained language model by adding graph neural network layers on top of the\nlanguage model embeddings, leading to a novel model LAGER. Extensive\nexperiments on two benchmark datasets show that LAGER significantly outperforms\nstrong baselines under different few-shot settings and also demonstrate better\nrobustness to manipulations.",
        "pdf_link": "https://arxiv.org/pdf/2305.14828v2.pdf"
    },
    {
        "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners",
        "authors": [
            "Xiaojuan Tang",
            "Zilong Zheng",
            "Jiaqi Li",
            "Fanxu Meng",
            "Song-Chun Zhu",
            "Yitao Liang",
            "Muhan Zhang"
        ],
        "published": "2023-05-24T07:33:34Z",
        "summary": "The emergent few-shot reasoning capabilities of Large Language Models (LLMs)\nhave excited the natural language and machine learning community over recent\nyears. Despite of numerous successful applications, the underlying mechanism of\nsuch in-context capabilities still remains unclear. In this work, we\nhypothesize that the learned \\textit{semantics} of language tokens do the most\nheavy lifting during the reasoning process. Different from human's symbolic\nreasoning process, the semantic representations of LLMs could create strong\nconnections among tokens, thus composing a superficial logical chain. To test\nour hypothesis, we decouple semantics from the language reasoning process and\nevaluate three kinds of reasoning abilities, i.e., deduction, induction and\nabduction. Our findings reveal that semantics play a vital role in LLMs'\nin-context reasoning -- LLMs perform significantly better when semantics are\nconsistent with commonsense but struggle to solve symbolic or\ncounter-commonsense reasoning tasks by leveraging in-context new knowledge. The\nsurprising observations question whether modern LLMs have mastered the\ninductive, deductive and abductive reasoning abilities as in human\nintelligence, and motivate research on unveiling the magic existing within the\nblack-box LLMs. On the whole, our analysis provides a novel perspective on the\nrole of semantics in developing and evaluating language models' reasoning\nabilities. Code is available at {\\url{https://github.com/XiaojuanTang/ICSR}}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14825v2.pdf"
    },
    {
        "title": "Mitigating Temporal Misalignment by Discarding Outdated Facts",
        "authors": [
            "Michael J. Q. Zhang",
            "Eunsol Choi"
        ],
        "published": "2023-05-24T07:30:08Z",
        "summary": "While large language models are able to retain vast amounts of world\nknowledge seen during pretraining, such knowledge is prone to going out of date\nand is nontrivial to update. Furthermore, these models are often used under\ntemporal misalignment, tasked with answering questions about the present,\ndespite having only been trained on data collected in the past. To mitigate the\neffects of temporal misalignment, we propose fact duration prediction: the task\nof predicting how long a given fact will remain true. In our experiments, we\ndemonstrate that identifying which facts are prone to rapid change can help\nmodels avoid reciting outdated information and determine which predictions\nrequire seeking out up-to-date knowledge sources. We also show how modeling\nfact duration improves calibration for knowledge-intensive tasks, such as\nopen-retrieval question answering, under temporal misalignment, by discarding\nvolatile facts. Our data and code are released publicly at\nhttps://github.com/mikejqzhang/mitigating_misalignment.",
        "pdf_link": "https://arxiv.org/pdf/2305.14824v3.pdf"
    },
    {
        "title": "Estimating Large Language Model Capabilities without Labeled Test Data",
        "authors": [
            "Harvey Yiyun Fu",
            "Qinyuan Ye",
            "Albert Xu",
            "Xiang Ren",
            "Robin Jia"
        ],
        "published": "2023-05-24T06:55:09Z",
        "summary": "Large Language Models (LLMs) have the impressive ability to perform\nin-context learning (ICL) from only a few examples, but the success of ICL\nvaries widely from task to task. Thus, it is important to quickly determine\nwhether ICL is applicable to a new task, but directly evaluating ICL accuracy\ncan be expensive in situations where test data is expensive to annotate -- the\nexact situations where ICL is most appealing. In this paper, we propose the\ntask of ICL accuracy estimation, in which we predict the accuracy of an LLM\nwhen doing in-context learning on a new task given only unlabeled test data for\nthat task. To perform ICL accuracy estimation, we propose a method that trains\na meta-model using LLM confidence scores as features. We compare our method to\nseveral strong accuracy estimation baselines on a new benchmark that covers 4\nLLMs and 3 task collections. The meta-model improves over all baselines across\n8 out of 12 settings and achieves the same estimation performance as directly\nevaluating on 40 collected labeled test examples per task. At the same time, no\nexisting approach provides an accurate and reliable ICL accuracy estimation in\nevery setting, highlighting the need for better ways to measure the uncertainty\nof LLM predictions.",
        "pdf_link": "https://arxiv.org/pdf/2305.14802v2.pdf"
    },
    {
        "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
        "authors": [
            "Zexuan Zhong",
            "Zhengxuan Wu",
            "Christopher D. Manning",
            "Christopher Potts",
            "Danqi Chen"
        ],
        "published": "2023-05-24T06:48:41Z",
        "summary": "The information stored in large language models (LLMs) falls out of date\nquickly, and retraining from scratch is often not an option. This has recently\ngiven rise to a range of techniques for injecting new facts through updating\nmodel weights. Current evaluation paradigms are extremely limited, mainly\nvalidating the recall of edited facts, but changing one fact should cause\nrippling changes to the model's related beliefs. If we edit the UK Prime\nMinister to now be Rishi Sunak, then we should get a different answer to Who is\nmarried to the British Prime Minister? In this work, we present a benchmark,\nMQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising\nmulti-hop questions that assess whether edited models correctly answer\nquestions where the answer should change as an entailed consequence of edited\nfacts. While we find that current knowledge-editing approaches can recall\nedited facts accurately, they fail catastrophically on the constructed\nmulti-hop questions. We thus propose a simple memory-based approach, MeLLo,\nwhich stores all edited facts externally while prompting the language model\niteratively to generate answers that are consistent with the edited facts.\nWhile MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up\nto 175B) and outperforms previous model editors by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2305.14795v2.pdf"
    },
    {
        "title": "Prompting Large Language Models for Counterfactual Generation: An Empirical Study",
        "authors": [
            "Yongqi Li",
            "Mayi Xu",
            "Xin Miao",
            "Shen Zhou",
            "Tieyun Qian"
        ],
        "published": "2023-05-24T06:44:32Z",
        "summary": "Large language models (LLMs) have made remarkable progress in a wide range of\nnatural language understanding and generation tasks. However, their ability to\ngenerate counterfactuals has not been examined systematically. To bridge this\ngap, we present a comprehensive evaluation framework on various types of NLU\ntasks, which covers all key factors in determining LLMs' capability of\ngenerating counterfactuals. Based on this framework, we 1) investigate the\nstrengths and weaknesses of LLMs as the counterfactual generator, and 2)\ndisclose the factors that affect LLMs when generating counterfactuals,\nincluding both the intrinsic properties of LLMs and prompt designing. The\nresults show that, though LLMs are promising in most cases, they face\nchallenges in complex tasks like RE since they are bounded by task-specific\nperformance, entity constraints, and inherent selection bias. We also find that\nalignment techniques, e.g., instruction-tuning and reinforcement learning from\nhuman feedback, may potentially enhance the counterfactual generation ability\nof LLMs. On the contrary, simply increasing the parameter size does not yield\nthe desired improvements. Besides, from the perspective of prompt designing,\ntask guidelines unsurprisingly play an important role. However, the\nchain-of-thought approach does not always help due to inconsistency issues.",
        "pdf_link": "https://arxiv.org/pdf/2305.14791v2.pdf"
    },
    {
        "title": "Adapting Language Models to Compress Contexts",
        "authors": [
            "Alexis Chevalier",
            "Alexander Wettig",
            "Anirudh Ajith",
            "Danqi Chen"
        ],
        "published": "2023-05-24T06:42:44Z",
        "summary": "Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These language models are capable\nof compressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments, and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show\nthat AutoCompressors can utilize long contexts to improve perplexity. We\nevaluate AutoCompressors on in-context learning by compressing task\ndemonstrations and find that summary vectors are good substitutes for\nplain-text demonstrations, increasing accuracy while reducing inference costs.\nFinally, we explore the benefits of pre-computing summary vectors for large\ncorpora by applying summary vectors to retrievalaugmented language modeling and\na passage re-ranking task. Overall, AutoCompressors emerge as a simple and\ninexpensive solution to extend the context window of LMs while speeding up\ninference over long contexts.",
        "pdf_link": "https://arxiv.org/pdf/2305.14788v2.pdf"
    },
    {
        "title": "ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds",
        "authors": [
            "Victoria Basmov",
            "Yoav Goldberg",
            "Reut Tsarfaty"
        ],
        "published": "2023-05-24T06:41:09Z",
        "summary": "This paper sheds light on the limitations of ChatGPT's understanding\ncapabilities, focusing on simple inference tasks that are typically easy for\nhumans but appear to be challenging for the model. Specifically, we target (i)\ngrammatically-specified entailments, (ii) premises with evidential adverbs of\nuncertainty, and (iii) monotonicity entailments. We present expert-designed\nevaluation sets for these inference types and conduct experiments in a\nzero-shot setup. Our results show that the model struggles with these types of\ninferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT\ndemonstrates knowledge of the underlying linguistic concepts when prompted\ndirectly, it often fails to incorporate this knowledge to make correct\ninferences. Even more strikingly, further experiments show that embedding the\npremise under presupposition triggers or non-factive verbs causes the model to\npredict entailment more frequently {regardless} of the correct semantic label.\nOverall these results suggest that, despite GPT's celebrated language\nunderstanding capacity, ChatGPT has blindspots with respect to certain types of\nentailment, and that certain entailment-cancelling features act as ``blinds''\novershadowing the semantics of the embedded premise. Our analyses emphasize the\nneed for further research into the linguistic comprehension and reasoning\ncapabilities of LLMs, in order to improve their reliability, and establish\ntheir trustworthiness for real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.14785v1.pdf"
    },
    {
        "title": "Anthropomorphization of AI: Opportunities and Risks",
        "authors": [
            "Ameet Deshpande",
            "Tanmay Rajpurohit",
            "Karthik Narasimhan",
            "Ashwin Kalyan"
        ],
        "published": "2023-05-24T06:39:45Z",
        "summary": "Anthropomorphization is the tendency to attribute human-like traits to\nnon-human entities. It is prevalent in many social contexts -- children\nanthropomorphize toys, adults do so with brands, and it is a literary device.\nIt is also a versatile tool in science, with behavioral psychology and\nevolutionary biology meticulously documenting its consequences. With widespread\nadoption of AI systems, and the push from stakeholders to make it human-like\nthrough alignment techniques, human voice, and pictorial avatars, the tendency\nfor users to anthropomorphize it increases significantly. We take a dyadic\napproach to understanding this phenomenon with large language models (LLMs) by\nstudying (1) the objective legal implications, as analyzed through the lens of\nthe recent blueprint of AI bill of rights and the (2) subtle psychological\naspects customization and anthropomorphization. We find that anthropomorphized\nLLMs customized for different user bases violate multiple provisions in the\nlegislative blueprint. In addition, we point out that anthropomorphization of\nLLMs affects the influence they can have on their users, thus having the\npotential to fundamentally change the nature of human-AI interaction, with\npotential for manipulation and negative influence. With LLMs being\nhyper-personalized for vulnerable groups like children and patients among\nothers, our work is a timely and important contribution. We propose a\nconservative strategy for the cautious use of anthropomorphization to improve\ntrustworthiness of AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.14784v1.pdf"
    },
    {
        "title": "Using Natural Language Explanations to Rescale Human Judgments",
        "authors": [
            "Manya Wadhwa",
            "Jifan Chen",
            "Junyi Jessy Li",
            "Greg Durrett"
        ],
        "published": "2023-05-24T06:19:14Z",
        "summary": "The rise of large language models (LLMs) has brought a critical need for\nhigh-quality human-labeled data, particularly for processes like human feedback\nand evaluation. A common practice is to label data via consensus annotation\nover crowdworker judgments. However, annotators' judgments for subjective tasks\ncan differ in many ways: they may have different qualitative judgments about an\nexample, and they may map those to a labeling scheme in different ways. We show\nthat these nuances can be captured by natural language explanations, and\npropose a method to rescale ordinal annotations and explanations using LLMs.\nSpecifically, we feed annotators' Likert ratings and corresponding explanations\ninto an LLM and prompt it to produce a numeric score anchored in a scoring\nrubric. These scores should reflect the annotators' underlying assessments of\nthe example. The rubric can be designed or modified after annotation, and\ninclude distinctions that may not have been known when the original error\ntaxonomy was devised. We explore our technique in the context of rating system\noutputs for a document-grounded question answering task, where LLMs achieve\nnear-human performance. Our method rescales the raw judgments without impacting\nagreement and brings the scores closer to human judgments grounded in the same\nscoring rubric.",
        "pdf_link": "https://arxiv.org/pdf/2305.14770v2.pdf"
    },
    {
        "title": "Allies: Prompting Large Language Model with Beam Search",
        "authors": [
            "Hao Sun",
            "Xiao Liu",
            "Yeyun Gong",
            "Yan Zhang",
            "Daxin Jiang",
            "Linjun Yang",
            "Nan Duan"
        ],
        "published": "2023-05-24T06:16:44Z",
        "summary": "With the advance of large language models (LLMs), the research field of LLM\napplications becomes more and more popular and the idea of constructing\npipelines to accomplish complex tasks by stacking LLM API calls come true.\nHowever, this kind of methods face two limitations: narrow information coverage\nand low fault tolerance. In this work, we propose a novel method called ALLIES.\nGiven an input query, ALLIES leverages LLMs to iteratively generate new queries\nrelated to the original query, enabling an iterative reasoning process. By\niteratively refining and expanding the scope of the original query, ALLIES\ncaptures and utilizes hidden knowledge that may not be directly obtainable\nthrough retrieval. We take zero-shot open-domain question answering (ODQA) as\nan application scene and evaluate ALLIES on the widely-used benchmarks, such as\nNQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES\nsignificantly outperforms other zero-shot baselines, indicating its\neffectiveness in tackling those challenges. Our code is available in\nhttps://github.com/microsoft/SimXNS/tree/main/ALLIES.",
        "pdf_link": "https://arxiv.org/pdf/2305.14766v3.pdf"
    },
    {
        "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models",
        "authors": [
            "Natalie Shapira",
            "Mosh Levy",
            "Seyed Hossein Alavi",
            "Xuhui Zhou",
            "Yejin Choi",
            "Yoav Goldberg",
            "Maarten Sap",
            "Vered Shwartz"
        ],
        "published": "2023-05-24T06:14:31Z",
        "summary": "The escalating debate on AI's capabilities warrants developing reliable\nmetrics to assess machine \"intelligence\". Recently, many anecdotal examples\nwere used to suggest that newer large language models (LLMs) like ChatGPT and\nGPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached\nconflicting conclusions regarding those abilities. We investigate the extent of\nLLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs\nexhibit certain N-ToM abilities, this behavior is far from being robust. We\nfurther examine the factors impacting performance on N-ToM tasks and discover\nthat LLMs struggle with adversarial examples, indicating reliance on shallow\nheuristics rather than robust ToM abilities. We caution against drawing\nconclusions from anecdotal examples, limited benchmark testing, and using\nhuman-designed psychological tests to evaluate models.",
        "pdf_link": "https://arxiv.org/pdf/2305.14763v1.pdf"
    },
    {
        "title": "A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification",
        "authors": [
            "Yiannis Charalambous",
            "Norbert Tihanyi",
            "Ridhi Jain",
            "Youcheng Sun",
            "Mohamed Amine Ferrag",
            "Lucas C. Cordeiro"
        ],
        "published": "2023-05-24T05:54:10Z",
        "summary": "In this paper we present a novel solution that combines the capabilities of\nLarge Language Models (LLMs) with Formal Verification strategies to verify and\nautomatically repair software vulnerabilities. Initially, we employ Bounded\nModel Checking (BMC) to locate the software vulnerability and derive a\ncounterexample. The counterexample provides evidence that the system behaves\nincorrectly or contains a vulnerability. The counterexample that has been\ndetected, along with the source code, are provided to the LLM engine. Our\napproach involves establishing a specialized prompt language for conducting\ncode debugging and generation to understand the vulnerability's root cause and\nrepair the code. Finally, we use BMC to verify the corrected version of the\ncode generated by the LLM. As a proof of concept, we create ESBMC-AI based on\nthe Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained\nTransformer model, specifically gpt-3.5-turbo, to detect and fix errors in C\nprograms. Our experimentation involved generating a dataset comprising 1000 C\ncode samples, each consisting of 20 to 50 lines of code. Notably, our proposed\nmethod achieved an impressive success rate of up to 80% in repairing vulnerable\ncode encompassing buffer overflow and pointer dereference failures. We assert\nthat this automated approach can effectively incorporate into the software\ndevelopment lifecycle's continuous integration and deployment (CI/CD) process.",
        "pdf_link": "https://arxiv.org/pdf/2305.14752v1.pdf"
    },
    {
        "title": "Mastering the ABCDs of Complex Questions: Answer-Based Claim Decomposition for Fine-grained Self-Evaluation",
        "authors": [
            "Nishant Balepur",
            "Jie Huang",
            "Samraj Moorjani",
            "Hari Sundaram",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-05-24T05:53:11Z",
        "summary": "When answering complex questions, large language models (LLMs) may produce\nanswers that do not satisfy all criteria of the question. While existing\nself-evaluation techniques aim to detect if such answers are correct, these\ntechniques are unable to determine which criteria of the question are satisfied\nby the generated answers. To address this issue, we propose answer-based claim\ndecomposition (ABCD), a prompting strategy that decomposes questions into a\nseries of true/false claims that can be used to verify which criteria of the\ninput question an answer satisfies. Using the decomposed ABCD claims, we\nperform fine-grained self-evaluation. Through preliminary experiments on three\ndatasets, including a newly-collected challenge dataset ObscureQA, we find that\nGPT-3.5 has some ability to determine to what extent its answer satisfies the\ncriteria of the input question, and can give insights into the errors and\nknowledge gaps of the model.",
        "pdf_link": "https://arxiv.org/pdf/2305.14750v1.pdf"
    },
    {
        "title": "ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation",
        "authors": [
            "Dongxu Yue",
            "Qin Guo",
            "Munan Ning",
            "Jiaxi Cui",
            "Yuesheng Zhu",
            "Li Yuan"
        ],
        "published": "2023-05-24T05:28:37Z",
        "summary": "Editing real facial images is a crucial task in computer vision with\nsignificant demand in various real-world applications. While GAN-based methods\nhave showed potential in manipulating images especially when combined with\nCLIP, these methods are limited in their ability to reconstruct real images due\nto challenging GAN inversion capability. Despite the successful image\nreconstruction achieved by diffusion-based methods, there are still challenges\nin effectively manipulating fine-gained facial attributes with textual\ninstructions.To address these issues and facilitate convenient manipulation of\nreal facial images, we propose a novel approach that conduct text-driven image\nediting in the semantic latent space of diffusion model. By aligning the\ntemporal feature of the diffusion model with the semantic condition at\ngenerative process, we introduce a stable manipulation strategy, which perform\nprecise zero-shot manipulation effectively. Furthermore, we develop an\ninteractive system named ChatFace, which combines the zero-shot reasoning\nability of large language models to perform efficient manipulations in\ndiffusion semantic latent space. This system enables users to perform complex\nmulti-attribute manipulations through dialogue, opening up new possibilities\nfor interactive image editing. Extensive experiments confirmed that our\napproach outperforms previous methods and enables precise editing of real\nfacial images, making it a promising candidate for real-world applications.\nProject page: https://dongxuyue.github.io/chatface/",
        "pdf_link": "https://arxiv.org/pdf/2305.14742v2.pdf"
    },
    {
        "title": "In-Context Demonstration Selection with Cross Entropy Difference",
        "authors": [
            "Dan Iter",
            "Reid Pryzant",
            "Ruochen Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Yichong Xu",
            "Chenguang Zhu"
        ],
        "published": "2023-05-24T05:04:00Z",
        "summary": "Large language models (LLMs) can use in-context demonstrations to improve\nperformance on zero-shot tasks. However, selecting the best in-context examples\nis challenging because model performance can vary widely depending on the\nselected examples. We present a cross-entropy difference (CED) method for\nselecting in-context demonstrations. Our method is based on the observation\nthat the effectiveness of in-context demonstrations negatively correlates with\nthe perplexity of the test example by a language model that was finetuned on\nthat demonstration. We utilize parameter efficient finetuning to train small\nmodels on training data that are used for computing the cross-entropy\ndifference between a test example and every candidate in-context demonstration.\nThis metric is used to rank and select in-context demonstrations independently\nfor each test input. We evaluate our method on a mix-domain dataset that\ncombines 8 benchmarks, representing 4 text generation tasks, showing that CED\nfor in-context demonstration selection can improve performance for a variety of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.14726v2.pdf"
    },
    {
        "title": "I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors",
        "authors": [
            "Tuhin Chakrabarty",
            "Arkadiy Saakyan",
            "Olivia Winn",
            "Artemis Panagopoulou",
            "Yue Yang",
            "Marianna Apidianaki",
            "Smaranda Muresan"
        ],
        "published": "2023-05-24T05:01:10Z",
        "summary": "Visual metaphors are powerful rhetorical devices used to persuade or\ncommunicate creative ideas through images. Similar to linguistic metaphors,\nthey convey meaning implicitly through symbolism and juxtaposition of the\nsymbols. We propose a new task of generating visual metaphors from linguistic\nmetaphors. This is a challenging task for diffusion-based text-to-image models,\nsuch as DALL$\\cdot$E 2, since it requires the ability to model implicit meaning\nand compositionality. We propose to solve the task through the collaboration\nbetween Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3\n(davinci-002) with Chain-of-Thought prompting generates text that represents a\nvisual elaboration of the linguistic metaphor containing the implicit meaning\nand relevant objects, which is then used as input to the diffusion-based\ntext-to-image models.Using a human-AI collaboration framework, where humans\ninteract both with the LLM and the top-performing diffusion model, we create a\nhigh-quality dataset containing 6,476 visual metaphors for 1,540 linguistic\nmetaphors and their associated visual elaborations. Evaluation by professional\nillustrators shows the promise of LLM-Diffusion Model collaboration for this\ntask . To evaluate the utility of our Human-AI collaboration framework and the\nquality of our dataset, we perform both an intrinsic human-based evaluation and\nan extrinsic evaluation using visual entailment as a downstream task.",
        "pdf_link": "https://arxiv.org/pdf/2305.14724v2.pdf"
    },
    {
        "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
        "authors": [
            "Jiashu Xu",
            "Mingyu Derek Ma",
            "Fei Wang",
            "Chaowei Xiao",
            "Muhao Chen"
        ],
        "published": "2023-05-24T04:27:21Z",
        "summary": "We investigate security concerns of the emergent instruction tuning paradigm,\nthat models are trained on crowdsourced datasets with task instructions to\nachieve superior performance. Our studies demonstrate that an attacker can\ninject backdoors by issuing very few malicious instructions (~1000 tokens) and\ncontrol model behavior through data poisoning, without even the need to modify\ndata instances or labels themselves. Through such instruction attacks, the\nattacker can achieve over 90% attack success rate across four commonly used NLP\ndatasets. As an empirical study on instruction attacks, we systematically\nevaluated unique perspectives of instruction attacks, such as poison transfer\nwhere poisoned models can transfer to 15 diverse generative datasets in a\nzero-shot manner; instruction transfer where attackers can directly apply\npoisoned instruction on many other datasets; and poison resistance to continual\nfinetuning. Lastly, we show that RLHF and clean demonstrations might mitigate\nsuch backdoors to some degree. These findings highlight the need for more\nrobust defenses against poisoning attacks in instruction-tuning models and\nunderscore the importance of ensuring data quality in instruction\ncrowdsourcing.",
        "pdf_link": "https://arxiv.org/pdf/2305.14710v2.pdf"
    },
    {
        "title": "SciFix: Outperforming GPT3 on Scientific Factual Error Correction",
        "authors": [
            "Dhananjay Ashok",
            "Atharva Kulkarni",
            "Hai Pham",
            "Barnabás Póczos"
        ],
        "published": "2023-05-24T04:24:16Z",
        "summary": "Due to the prohibitively high cost of creating error correction datasets,\nmost Factual Claim Correction methods rely on a powerful verification model to\nguide the correction process. This leads to a significant drop in performance\nin domains like scientific claims, where good verification models do not always\nexist. In this work, we introduce SciFix, a scientific claim correction system\nthat does not require a verifier but can outperform existing methods by a\nconsiderable margin -- achieving correction accuracy of 84% on the SciFact\ndataset, 77% on SciFact-Open and 72% on the CovidFact dataset, compared to next\nbest accuracies of 7%, 5%, and 15% on the same datasets respectively. Our\nmethod leverages the power of prompting with LLMs during training to create a\nrichly annotated dataset that can be used for fully supervised training and\nregularization. We additionally use a claim-aware decoding procedure to improve\nthe quality of corrected claims. Our method outperforms the very LLM that was\nused to generate the annotated dataset -- with Few-Shot Prompting on GPT3.5\nachieving 58%, 61%, and 64% on the respective datasets, a consistently lower\ncorrection accuracy, despite using nearly 800 times as many parameters as our\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2305.14707v2.pdf"
    },
    {
        "title": "Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models",
        "authors": [
            "Sheng Shen",
            "Le Hou",
            "Yanqi Zhou",
            "Nan Du",
            "Shayne Longpre",
            "Jason Wei",
            "Hyung Won Chung",
            "Barret Zoph",
            "William Fedus",
            "Xinyun Chen",
            "Tu Vu",
            "Yuexin Wu",
            "Wuyang Chen",
            "Albert Webson",
            "Yunxuan Li",
            "Vincent Zhao",
            "Hongkun Yu",
            "Kurt Keutzer",
            "Trevor Darrell",
            "Denny Zhou"
        ],
        "published": "2023-05-24T04:22:26Z",
        "summary": "Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be\nutilized to add learnable parameters to Large Language Models (LLMs) without\nincreasing inference cost. Instruction tuning is a technique for training LLMs\nto follow instructions. We advocate combining these two approaches, as we find\nthat MoE models benefit more from instruction tuning than dense models. In\nparticular, we conduct empirical studies across three experimental setups: (i)\nDirect finetuning on individual downstream tasks devoid of instruction tuning;\n(ii) Instructiontuning followed by in-context few-shot or zero-shot\ngeneralization on downstream tasks; and (iii) Instruction tuning supplemented\nby further finetuning on individual downstream tasks. In the first scenario,\nMoE models overall underperform dense models of identical computational\ncapacity. This narrative, however, dramatically changes with the introduction\nof instruction tuning (second and third scenario), used independently or in\nconjunction with task-specific finetuning. Our most powerful model,\nFLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark\ntasks, while using only a third of the FLOPs. The advancements embodied\nbyFLAN-MOE inspire a reevaluation of the design principles of large-scale,\nhigh-performance language models in the framework of task-agnostic learning.",
        "pdf_link": "https://arxiv.org/pdf/2305.14705v2.pdf"
    },
    {
        "title": "DecipherPref: Analyzing Influential Factors in Human Preference Judgments via GPT-4",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Fei Liu"
        ],
        "published": "2023-05-24T04:13:15Z",
        "summary": "Human preference judgments are pivotal in guiding large language models\n(LLMs) to produce outputs that align with human values. Human evaluations are\nalso used in summarization tasks to compare outputs from various systems,\ncomplementing existing automatic metrics. Despite their significance, however,\nthere has been limited research probing these pairwise or $k$-wise comparisons.\nThe collective impact and relative importance of factors such as output length,\ninformativeness, fluency, and factual consistency are still not well\nunderstood. It is also unclear if there are other hidden factors influencing\nhuman judgments. In this paper, we conduct an in-depth examination of a\ncollection of pairwise human judgments released by OpenAI. Utilizing the\nBradley-Terry-Luce (BTL) model, we reveal the inherent preferences embedded in\nthese human judgments. We find that the most favored factors vary across tasks\nand genres, whereas the least favored factors tend to be consistent, e.g.,\noutputs are too brief, contain excessive off-focus content or hallucinated\nfacts. Our findings have implications on the construction of balanced datasets\nin human preference evaluations, which is a crucial step in shaping the\nbehaviors of future LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.14702v3.pdf"
    },
    {
        "title": "A Causal View of Entity Bias in (Large) Language Models",
        "authors": [
            "Fei Wang",
            "Wenjie Mo",
            "Yiwei Wang",
            "Wenxuan Zhou",
            "Muhao Chen"
        ],
        "published": "2023-05-24T03:59:18Z",
        "summary": "Entity bias widely affects pretrained (large) language models, causing them\nto rely on (biased) parametric knowledge to make unfaithful predictions.\nAlthough causality-inspired methods have shown great potential to mitigate\nentity bias, it is hard to precisely estimate the parameters of underlying\ncausal models in practice. The rise of black-box LLMs also makes the situation\neven worse, because of their inaccessible parameters and uncalibrated logits.\nTo address these problems, we propose a specific structured causal model (SCM)\nwhose parameters are comparatively easier to estimate. Building upon this SCM,\nwe propose causal intervention techniques to mitigate entity bias for both\nwhite-box and black-box settings. The proposed causal intervention perturbs the\noriginal entity with neighboring entities. This intervention reduces specific\nbiasing information pertaining to the original entity while still preserving\nsufficient semantic information from similar entities. Under the white-box\nsetting, our training-time intervention improves OOD performance of PLMs on\nrelation extraction (RE) and machine reading comprehension (MRC) by 5.7 points\nand by 9.1 points, respectively. Under the black-box setting, our in-context\nintervention effectively reduces the entity-based knowledge conflicts of\nGPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on\nMRC and up to 17.6 points of reduction in memorization ratio on RE. Our code is\navailable at https://github.com/luka-group/Causal-View-of-Entity-Bias.",
        "pdf_link": "https://arxiv.org/pdf/2305.14695v2.pdf"
    },
    {
        "title": "Have Large Language Models Developed a Personality?: Applicability of Self-Assessment Tests in Measuring Personality in LLMs",
        "authors": [
            "Xiaoyang Song",
            "Akshat Gupta",
            "Kiyan Mohebbizadeh",
            "Shujie Hu",
            "Anant Singh"
        ],
        "published": "2023-05-24T03:53:43Z",
        "summary": "Have Large Language Models (LLMs) developed a personality? The short answer\nis a resounding \"We Don't Know!\". In this paper, we show that we do not yet\nhave the right tools to measure personality in language models. Personality is\nan important characteristic that influences behavior. As LLMs emulate\nhuman-like intelligence and performance in various tasks, a natural question to\nask is whether these models have developed a personality. Previous works have\nevaluated machine personality through self-assessment personality tests, which\nare a set of multiple-choice questions created to evaluate personality in\nhumans. A fundamental assumption here is that human personality tests can\naccurately measure personality in machines. In this paper, we investigate the\nemergence of personality in five LLMs of different sizes ranging from 1.5B to\n30B. We propose the Option-Order Symmetry property as a necessary condition for\nthe reliability of these self-assessment tests. Under this condition, the\nanswer to self-assessment questions is invariant to the order in which the\noptions are presented. We find that many LLMs personality test responses do not\npreserve option-order symmetry. We take a deeper look at LLMs test responses\nwhere option-order symmetry is preserved to find that in these cases, LLMs do\nnot take into account the situational statement being tested and produce the\nexact same answer irrespective of the situation being tested. We also identify\nthe existence of inherent biases in these LLMs which is the root cause of the\naforementioned phenomenon and makes self-assessment tests unreliable. These\nobservations indicate that self-assessment tests are not the correct tools to\nmeasure personality in LLMs. Through this paper, we hope to draw attention to\nthe shortcomings of current literature in measuring personality in LLMs and\ncall for developing tools for machine personality measurement.",
        "pdf_link": "https://arxiv.org/pdf/2305.14693v1.pdf"
    },
    {
        "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
        "authors": [
            "Benfeng Xu",
            "An Yang",
            "Junyang Lin",
            "Quan Wang",
            "Chang Zhou",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "published": "2023-05-24T03:51:31Z",
        "summary": "The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\n\\url{https://github.com/OFA-Sys/ExpertLLaMA}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14688v1.pdf"
    },
    {
        "title": "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response",
        "authors": [
            "Yongkang Liu",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang",
            "Hinrich Schütze"
        ],
        "published": "2023-05-24T02:52:48Z",
        "summary": "LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.",
        "pdf_link": "https://arxiv.org/pdf/2305.14658v2.pdf"
    },
    {
        "title": "Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering",
        "authors": [
            "Davit Soselia",
            "Khalid Saifullah",
            "Tianyi Zhou"
        ],
        "published": "2023-05-24T02:17:32Z",
        "summary": "Automated reverse engineering of HTML/CSS code from UI screenshots is an\nimportant yet challenging problem with broad applications in website\ndevelopment and design. In this paper, we propose a novel vision-code\ntransformer (ViCT) composed of a vision encoder processing the screenshots and\na language decoder to generate the code. They are initialized by pre-trained\nmodels such as ViT/DiT and GPT-2/LLaMA but aligning the two modalities requires\nend-to-end finetuning, which aims to minimize the visual discrepancy between\nthe code-rendered webpage and the original screenshot. However, the rendering\nis non-differentiable and causes costly overhead. We address this problem by\nactor-critic fine-tuning where a visual critic without rendering (ViCR) is\ndeveloped to predict visual discrepancy given the original and generated code.\nTo train and evaluate our models, we created two synthetic datasets of varying\ncomplexity, with over 75,000 unique (code, screenshot) pairs. We evaluate the\nUI-to-Code performance using a combination of automated metrics such as MSE,\nBLEU, IoU, and a novel htmlBLEU score. ViCT outperforms a strong baseline model\nDiT-GPT2, improving IoU from 0.64 to 0.79 and lowering MSE from 12.25 to 9.02.\nWith much lower computational cost, it can achieve comparable performance as\nwhen using a larger decoder such as LLaMA.",
        "pdf_link": "https://arxiv.org/pdf/2305.14637v2.pdf"
    },
    {
        "title": "Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
        "authors": [
            "Xiang Zhang",
            "Senyu Li",
            "Bradley Hauer",
            "Ning Shi",
            "Grzegorz Kondrak"
        ],
        "published": "2023-05-24T02:05:03Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional natural language\nunderstanding abilities and have excelled in a variety of natural language\nprocessing (NLP)tasks in recent years. Despite the fact that most LLMs are\ntrained predominantly in English, multiple studies have demonstrated their\ncomparative performance in many other languages. However, fundamental questions\npersist regarding how LLMs acquire their multi-lingual abilities and how\nperformance varies across different languages. These inquiries are crucial for\nthe study of LLMs since users and researchers often come from diverse language\nbackgrounds, potentially influencing their utilization and interpretation of\nLLMs' results. In this work, we propose a systematic way of qualifying the\nperformance disparities of LLMs under multilingual settings. We investigate the\nphenomenon of across-language generalizations in LLMs, wherein insufficient\nmulti-lingual training data leads to advanced multi-lingual capabilities. To\naccomplish this, we employ a novel back-translation-based prompting method. The\nresults show that GPT exhibits highly translating-like behaviour in\nmultilingual settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.16339v2.pdf"
    },
    {
        "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
        "authors": [
            "Chenglei Si",
            "Weijia Shi",
            "Chen Zhao",
            "Luke Zettlemoyer",
            "Jordan Boyd-Graber"
        ],
        "published": "2023-05-24T02:00:51Z",
        "summary": "While recent large language models (LLMs) improve on various question\nanswering (QA) datasets, it remains difficult for a single model to generalize\nacross question types that require distinct reasoning abilities. We provide\nempirical evidence that state-of-the-art LLMs suffer from poor generalizability\non reasoning types beyond those seen in the prompt. To remedy this, we propose\na Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse\nspecialized language models. We specialize the backbone language model with\nprompts optimized for different reasoning categories, including factual,\nmultihop, mathematical, and commonsense reasoning. Our key insight is to\nleverage agreement among the specialized experts to select the best answer for\neach question, or to abstain from answering. This gives MoRE higher accuracy\nthan any single specialized model on a collection of 12 QA datasets from four\nreasoning types. Beyond generalizability, the interpretable design of MoRE\nimproves selective question answering results compared to baselines without\nincorporating inter-expert agreement. This framework is also more interpretable\nand useful to human consumers of QA outputs. Our human study confirms that\npresenting expert predictions and the answer selection process helps annotators\nmore accurately calibrate when to trust the system's output. We release all\ncode and data to facilitate future work.",
        "pdf_link": "https://arxiv.org/pdf/2305.14628v2.pdf"
    },
    {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
            "Tianyu Gao",
            "Howard Yen",
            "Jiatong Yu",
            "Danqi Chen"
        ],
        "published": "2023-05-24T01:53:49Z",
        "summary": "Large language models (LLMs) have emerged as a widely-used tool for\ninformation seeking, but their generated outputs are prone to hallucination. In\nthis work, our aim is to allow LLMs to generate text with citations, improving\ntheir factual correctness and verifiability. Existing work mainly relies on\ncommercial search engines and human evaluation, making it challenging to\nreproduce and compare different modeling approaches. We propose ALCE, the first\nbenchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set\nof questions and retrieval corpora and requires building end-to-end systems to\nretrieve supporting evidence and generate answers with citations. We develop\nautomatic metrics along three dimensions -- fluency, correctness, and citation\nquality -- and demonstrate their strong correlation with human judgements. Our\nexperiments with state-of-the-art LLMs and novel prompting strategies show that\ncurrent systems have considerable room for improvement -- For example, on the\nELI5 dataset, even the best models lack complete citation support 50% of the\ntime. Our analyses further highlight promising future directions, including\ndeveloping better retrievers, advancing long-context LLMs, and improving the\nability to synthesize information from multiple sources.",
        "pdf_link": "https://arxiv.org/pdf/2305.14627v2.pdf"
    },
    {
        "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
        "authors": [
            "Miaoran Li",
            "Baolin Peng",
            "Michel Galley",
            "Jianfeng Gao",
            "Zhu Zhang"
        ],
        "published": "2023-05-24T01:46:07Z",
        "summary": "Fact-checking is an essential task in NLP that is commonly utilized for\nvalidating the factual accuracy of claims. Prior work has mainly focused on\nfine-tuning pre-trained languages models on specific datasets, which can be\ncomputationally intensive and time-consuming. With the rapid development of\nlarge language models (LLMs), such as ChatGPT and GPT-3, researchers are now\nexploring their in-context learning capabilities for a wide range of tasks. In\nthis paper, we aim to assess the capacity of LLMs for fact-checking by\nintroducing Self-Checker, a framework comprising a set of plug-and-play modules\nthat facilitate fact-checking by purely prompting LLMs in an almost zero-shot\nsetting. This framework provides a fast and efficient way to construct\nfact-checking systems in low-resource environments. Empirical results\ndemonstrate the potential of Self-Checker in utilizing LLMs for fact-checking.\nHowever, there is still significant room for improvement compared to SOTA\nfine-tuned models, which suggests that LLM adoption could be a promising\napproach for future fact-checking research.",
        "pdf_link": "https://arxiv.org/pdf/2305.14623v2.pdf"
    },
    {
        "title": "Think Before You Act: Decision Transformers with Internal Working Memory",
        "authors": [
            "Jikun Kang",
            "Romain Laroche",
            "Xindi Yuan",
            "Adam Trischler",
            "Xue Liu",
            "Jie Fu"
        ],
        "published": "2023-05-24T01:20:22Z",
        "summary": "Large language model (LLM)-based decision-making agents have shown the\nability to generalize across multiple tasks. However, their performance relies\non massive data and compute. We argue that this inefficiency stems from the\nforgetting phenomenon, in which a model memorizes its behaviors in parameters\nthroughout training. As a result, training on a new task may deteriorate the\nmodel's performance on previous tasks. In contrast to LLMs' implicit memory\nmechanism, the human brain utilizes distributed memory storage, which helps\nmanage and organize multiple skills efficiently, mitigating the forgetting\nphenomenon. Thus inspired, we propose an internal working memory module to\nstore, blend, and retrieve information for different downstream tasks.\nEvaluation results show that the proposed method improves training efficiency\nand generalization in both Atari games and meta-world object manipulation\ntasks. Moreover, we demonstrate that memory fine-tuning further enhances the\nadaptability of the proposed architecture.",
        "pdf_link": "https://arxiv.org/pdf/2305.16338v1.pdf"
    },
    {
        "title": "This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language Models",
        "authors": [
            "Bryan Li",
            "Samar Haider",
            "Chris Callison-Burch"
        ],
        "published": "2023-05-24T01:16:17Z",
        "summary": "Do the Spratly Islands belong to China, the Philippines, or Vietnam? A\npretrained large language model (LLM) may answer differently if asked in the\nlanguages of each claimant country: Chinese, Tagalog, or Vietnamese. This\ncontrasts with a multilingual human, who would likely answer consistently. In\nthis paper, we show that LLMs recall certain geographical knowledge\ninconsistently when queried in different languages -- a phenomenon we term\ngeopolitical bias. As a targeted case study, we consider territorial disputes,\nan inherently controversial and multilingual task. We introduce BorderLines, a\ndataset of territorial disputes which covers 251 territories, each associated\nwith a set of multiple-choice questions in the languages of each claimant\ncountry (49 languages in total). We also propose a suite of evaluation metrics\nto precisely quantify bias and consistency in responses across different\nlanguages. We then evaluate various multilingual LLMs on our dataset and\nmetrics to probe their internal knowledge and use the proposed metrics to\ndiscover numerous inconsistencies in how these models respond in different\nlanguages. Finally, we explore several prompt modification strategies, aiming\nto either amplify or mitigate geopolitical bias, which highlights how brittle\nLLMs are and how they tailor their responses depending on cues from the\ninteraction context. Our code and data are available at\nhttps://github.com/manestay/borderlines",
        "pdf_link": "https://arxiv.org/pdf/2305.14610v4.pdf"
    },
    {
        "title": "ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers",
        "authors": [
            "Kexun Zhang",
            "Danqing Wang",
            "Jingtao Xia",
            "William Yang Wang",
            "Lei Li"
        ],
        "published": "2023-05-24T00:10:15Z",
        "summary": "Large language models (LLMs) excel at implementing code from functionality\ndescriptions but struggle with algorithmic problems that require not only\nimplementation but also identification of the suitable algorithm. Moreover,\nLLM-generated programs lack guaranteed correctness and require human\nverification. To address these challenges, we propose ALGO, a framework that\nsynthesizes Algorithmic programs with LLM-Generated Oracles to guide the\ngeneration and verify their correctness. ALGO first generates a reference\noracle by prompting an LLM to exhaustively enumerate all the combinations of\nrelevant variables. This oracle is then utilized to guide an arbitrary search\nstrategy in exploring the algorithm space and to verify the synthesized\nalgorithms. Our study shows that the LLM-generated oracles are correct for 88%\nof the cases. With the oracles as verifiers, ALGO can be integrated with any\nexisting code generation model in a model-agnostic manner to enhance its\nperformance. Experiments show that when equipped with ALGO, we achieve an 8x\nbetter one-submission pass rate over the Codex model and a 2.6x better\none-submission pass rate over CodeT, the current state-of-the-art model on\nCodeContests. We can also get 1.3x better pass rate over the ChatGPT Code\nInterpreter on unseen problems. The problem set we used for testing, the\nprompts we used, the verifier and solution programs, and the test cases\ngenerated by ALGO are available at https://github.com/zkx06111/ALGO.",
        "pdf_link": "https://arxiv.org/pdf/2305.14591v3.pdf"
    },
    {
        "title": "Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person",
        "authors": [
            "Lucas Rafael Stefanel Gris",
            "Ricardo Marcacini",
            "Arnaldo Candido Junior",
            "Edresson Casanova",
            "Anderson Soares",
            "Sandra Maria Aluísio"
        ],
        "published": "2023-05-23T23:37:29Z",
        "summary": "Automatic speech recognition (ASR) systems play a key role in applications\ninvolving human-machine interactions. Despite their importance, ASR models for\nthe Portuguese language proposed in the last decade have limitations in\nrelation to the correct identification of punctuation marks in automatic\ntranscriptions, which hinder the use of transcriptions by other systems,\nmodels, and even by humans. However, recently Whisper ASR was proposed by\nOpenAI, a general-purpose speech recognition model that has generated great\nexpectations in dealing with such limitations. This chapter presents the first\nstudy on the performance of Whisper for punctuation prediction in the\nPortuguese language. We present an experimental evaluation considering both\ntheoretical aspects involving pausing points (comma) and complete ideas\n(exclamation, question, and fullstop), as well as practical aspects involving\ntranscript-based topic modeling - an application dependent on punctuation marks\nfor promising performance. We analyzed experimental results from videos of\nMuseum of the Person, a virtual museum that aims to tell and preserve people's\nlife histories, thus discussing the pros and cons of Whisper in a real-world\nscenario. Although our experiments indicate that Whisper achieves\nstate-of-the-art results, we conclude that some punctuation marks require\nimprovements, such as exclamation, semicolon and colon.",
        "pdf_link": "https://arxiv.org/pdf/2305.14580v2.pdf"
    },
    {
        "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
        "authors": [
            "Simeng Sun",
            "Yang Liu",
            "Shuohang Wang",
            "Chenguang Zhu",
            "Mohit Iyyer"
        ],
        "published": "2023-05-23T23:06:04Z",
        "summary": "Strategies such as chain-of-thought prompting improve the performance of\nlarge language models (LLMs) on complex reasoning tasks by decomposing input\nexamples into intermediate steps. However, it remains unclear how to apply such\nmethods to reason over long input documents, in which both the decomposition\nand the output of each intermediate step are non-trivial to obtain. In this\nwork, we propose PEARL, a prompting framework to improve reasoning over long\ndocuments, which consists of three stages: action mining, plan formulation, and\nplan execution. More specifically, given a question about a long document,\nPEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE,\nFIND_EVENT, FIND_RELATION) and then executes them over the document to obtain\nthe answer. Each stage of PEARL is implemented via zero-shot or few-shot\nprompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate\nPEARL on a challenging subset of the QuALITY dataset, which contains questions\nthat require complex reasoning over long narrative texts. PEARL outperforms\nzero-shot and chain-of-thought prompting on this dataset, and ablation\nexperiments show that each stage of PEARL is critical to its performance.\nOverall, PEARL is a first step towards leveraging LLMs to reason over long\ndocuments.",
        "pdf_link": "https://arxiv.org/pdf/2305.14564v1.pdf"
    },
    {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "authors": [
            "Nick McKenna",
            "Tianyi Li",
            "Liang Cheng",
            "Mohammad Javad Hosseini",
            "Mark Johnson",
            "Mark Steedman"
        ],
        "published": "2023-05-23T22:24:44Z",
        "summary": "Large Language Models (LLMs) are claimed to be capable of Natural Language\nInference (NLI), necessary for applied tasks like question answering and\nsummarization. We present a series of behavioral studies on several LLM\nfamilies (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled\nexperiments. We establish two biases originating from pretraining which predict\nmuch of their behavior, and show that these are major sources of hallucination\nin generative LLMs. First, memorization at the level of sentences: we show\nthat, regardless of the premise, models falsely label NLI test samples as\nentailing when the hypothesis is attested in training data, and that entities\nare used as ``indices'' to access the memorized data. Second, statistical\npatterns of usage learned at the level of corpora: we further show a similar\neffect when the premise predicate is less frequent than that of the hypothesis\nin the training data, a bias following from previous studies. We demonstrate\nthat LLMs perform significantly worse on NLI test samples which do not conform\nto these biases than those which do, and we offer these as valuable controls\nfor future LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14552v2.pdf"
    },
    {
        "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond",
        "authors": [
            "Philippe Laban",
            "Wojciech Kryściński",
            "Divyansh Agarwal",
            "Alexander R. Fabbri",
            "Caiming Xiong",
            "Shafiq Joty",
            "Chien-Sheng Wu"
        ],
        "published": "2023-05-23T21:50:06Z",
        "summary": "With the recent appearance of LLMs in practical settings, having methods that\ncan effectively detect factual inconsistencies is crucial to reduce the\npropagation of misinformation and improve trust in model outputs. When testing\non existing factual consistency benchmarks, we find that a few large language\nmodels (LLMs) perform competitively on classification benchmarks for factual\ninconsistency detection compared to traditional non-LLM methods. However, a\ncloser analysis reveals that most LLMs fail on more complex formulations of the\ntask and exposes issues with existing evaluation benchmarks, affecting\nevaluation precision. To address this, we propose a new protocol for\ninconsistency detection benchmark creation and implement it in a 10-domain\nbenchmark called SummEdits. This new benchmark is 20 times more cost-effective\nper sample than previous benchmarks and highly reproducible, as we estimate\ninter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with\nperformance close to random chance. The best-performing model, GPT-4, is still\n8\\% below estimated human performance, highlighting the gaps in LLMs' ability\nto reason about facts and detect inconsistencies when they occur.",
        "pdf_link": "https://arxiv.org/pdf/2305.14540v1.pdf"
    },
    {
        "title": "MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems",
        "authors": [
            "Jakub Macina",
            "Nico Daheim",
            "Sankalan Pal Chowdhury",
            "Tanmay Sinha",
            "Manu Kapur",
            "Iryna Gurevych",
            "Mrinmaya Sachan"
        ],
        "published": "2023-05-23T21:44:56Z",
        "summary": "While automatic dialogue tutors hold great potential in making education\npersonalized and more accessible, research on such systems has been hampered by\na lack of sufficiently large and high-quality datasets. Collecting such\ndatasets remains challenging, as recording tutoring sessions raises privacy\nconcerns and crowdsourcing leads to insufficient data quality. To address this,\nwe propose a framework to generate such dialogues by pairing human teachers\nwith a Large Language Model (LLM) prompted to represent common student errors.\nWe describe how we use this framework to collect MathDial, a dataset of 3k\none-to-one teacher-student tutoring dialogues grounded in multi-step math\nreasoning problems. While models like GPT-3 are good problem solvers, they fail\nat tutoring because they generate factually incorrect feedback or are prone to\nrevealing solutions to students too early. To overcome this, we let teachers\nprovide learning opportunities to students by guiding them using various\nscaffolding questions according to a taxonomy of teacher moves. We demonstrate\nMathDial and its extensive annotations can be used to finetune models to be\nmore effective tutors (and not just solvers). We confirm this by automatic and\nhuman evaluation, notably in an interactive setting that measures the trade-off\nbetween student solving success and telling solutions. The dataset is released\npublicly.",
        "pdf_link": "https://arxiv.org/pdf/2305.14536v2.pdf"
    },
    {
        "title": "Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models",
        "authors": [
            "Shashank Sonkar",
            "Richard G. Baraniuk"
        ],
        "published": "2023-05-23T20:26:03Z",
        "summary": "We explore whether Large Language Models (LLMs) are capable of logical\nreasoning with distorted facts, which we call Deduction under Perturbed\nEvidence (DUPE). DUPE presents a unique challenge to LLMs since they typically\nrely on their parameters, which encode mostly accurate information, to reason\nand make inferences. However, in DUPE, LLMs must reason over manipulated or\nfalsified evidence present in their prompts, which can result in false\nconclusions that are valid only under the manipulated evidence. Our goal with\nDUPE is to determine whether LLMs can arrive at these false conclusions and\nidentify whether the dominant factor influencing the deduction process is the\nencoded data in the parameters or the manipulated evidence in the prompts. To\nevaluate the DUPE capabilities of LLMs, we create a DUPEd version of the\nStrategyQA dataset, where facts are manipulated to reverse the answer to the\nquestion. Our findings show that even the most advanced GPT models struggle to\nreason on manipulated facts - showcasing poor DUPE skills - with accuracy\ndropping by 45% compared to the original dataset. We also investigate prompt\nsettings inspired from student simulation models, which mitigate the accuracy\ndrop to some extent. Our findings have practical implications for understanding\nthe performance of LLMs in real-world applications such as student simulation\nmodels that involve reasoning over inaccurate information.",
        "pdf_link": "https://arxiv.org/pdf/2305.14507v1.pdf"
    },
    {
        "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
        "authors": [
            "Zhiheng Xi",
            "Senjie Jin",
            "Yuhao Zhou",
            "Rui Zheng",
            "Songyang Gao",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-05-23T19:58:30Z",
        "summary": "Prompting methods such as Chain-of-Thought (CoT) have shed new light on\nenhancing the reasoning capabilities of large language models, and researchers\nhave extensively explored the generation process of rationales and answers.\nHowever, they have overlooked the potential challenges posed by the poor\nquality of reasoning problems, which may influence the reasoning performance\nsignificantly. In this work, we propose Self-Polish (SP), a novel method that\nfacilitates the model's problem-solving process by prompting them to\nprogressively refine the given problems to be more comprehensible and solvable.\nSpecifically, the method teaches models to eliminate irrelevant information,\nrearrange the logic structure and organize local conditions into new ones\nparallelly. SP is orthogonal to all other prompting methods, making it\nconvenient to integrate with state-of-the-art techniques for further\nimprovement. We conduct thorough experiments on five benchmarks to illustrate\nthe effectiveness of the proposed method. For example, with Text-davinci-003,\nour method boosts the performance of standard few-shot prompting by $8.0\\%$ on\nGSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by\n$6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method\nalso showcases impressive performance on robustness evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14497v1.pdf"
    },
    {
        "title": "Language Model Self-improvement by Reinforcement Learning Contemplation",
        "authors": [
            "Jing-Cheng Pang",
            "Pengyuan Wang",
            "Kaiyuan Li",
            "Xiong-Hui Chen",
            "Jiacheng Xu",
            "Zongzhang Zhang",
            "Yang Yu"
        ],
        "published": "2023-05-23T19:25:52Z",
        "summary": "Large Language Models (LLMs) have exhibited remarkable performance across\nvarious natural language processing (NLP) tasks. However, fine-tuning these\nmodels often necessitates substantial supervision, which can be expensive and\ntime-consuming to obtain. This paper introduces a novel unsupervised method\ncalled LanguageModel Self-Improvement by Reinforcement Learning Contemplation\n(SIRLC) that improves LLMs without reliance on external labels. Our approach is\ngrounded in the observation that it is simpler for language models to assess\ntext quality than to generate text. Building on this insight, SIRLC assigns\nLLMs dual roles as both student and teacher. As a student, the LLM generates\nanswers to unlabeled questions, while as a teacher, it evaluates the generated\ntext and assigns scores accordingly. The model parameters are updated using\nreinforcement learning to maximize the evaluation score. We demonstrate that\nSIRLC can be applied to various NLP tasks, such as reasoning problems, text\ngeneration, and machine translation. Our experiments show that SIRLC\neffectively improves LLM performance without external supervision, resulting in\na 5.6% increase in answering accuracy for reasoning tasks and a rise in\nBERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be\napplied to models of different sizes, showcasing its broad applicability.",
        "pdf_link": "https://arxiv.org/pdf/2305.14483v1.pdf"
    },
    {
        "title": "Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA",
        "authors": [
            "David Heineman",
            "Yao Dou",
            "Mounica Maddela",
            "Wei Xu"
        ],
        "published": "2023-05-23T18:30:49Z",
        "summary": "Large language models (e.g., GPT-4) are uniquely capable of producing highly\nrated text simplification, yet current human evaluation methods fail to provide\na clear understanding of systems' specific strengths and weaknesses. To address\nthis limitation, we introduce SALSA, an edit-based human annotation framework\nthat enables holistic and fine-grained text simplification evaluation. We\ndevelop twenty one linguistically grounded edit types, covering the full\nspectrum of success and failure across dimensions of conceptual, syntactic and\nlexical simplicity. Using SALSA, we collect 19K edit annotations on 840\nsimplifications, revealing discrepancies in the distribution of simplification\nstrategies performed by fine-tuned models, prompted LLMs and humans, and find\nGPT-3.5 performs more quality edits than humans, but still exhibits frequent\nerrors. Using our fine-grained annotations, we develop LENS-SALSA, a\nreference-free automatic simplification metric, trained to predict sentence-\nand word-level quality simultaneously. Additionally, we introduce word-level\nquality estimation for simplification and report promising baseline results.\nOur data, new metric, and annotation toolkit are available at\nhttps://salsa-eval.com.",
        "pdf_link": "https://arxiv.org/pdf/2305.14458v2.pdf"
    },
    {
        "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models",
        "authors": [
            "Tarek Naous",
            "Michael J. Ryan",
            "Alan Ritter",
            "Wei Xu"
        ],
        "published": "2023-05-23T18:27:51Z",
        "summary": "As the reach of large language models (LMs) expands globally, their ability\nto cater to diverse cultural contexts becomes crucial. Despite advancements in\nmultilingual capabilities, models are not designed with appropriate cultural\nnuances. In this paper, we show that multilingual and Arabic monolingual LMs\nexhibit bias towards entities associated with Western culture. We introduce\nCAMeL, a novel resource of 628 naturally-occurring prompts and 20,368 entities\nspanning eight types that contrast Arab and Western cultures. CAMeL provides a\nfoundation for measuring cultural biases in LMs through both extrinsic and\nintrinsic evaluations. Using CAMeL, we examine the cross-cultural performance\nin Arabic of 16 different LMs on tasks such as story generation, NER, and\nsentiment analysis, where we find concerning cases of stereotyping and cultural\nunfairness. We further test their text-infilling performance, revealing the\nincapability of appropriate adaptation to Arab cultural contexts. Finally, we\nanalyze 6 Arabic pre-training corpora and find that commonly used sources such\nas Wikipedia may not be best suited to build culturally aware LMs, if used as\nthey are without adjustment. We will make CAMeL publicly available at:\nhttps://github.com/tareknaous/camel",
        "pdf_link": "https://arxiv.org/pdf/2305.14456v4.pdf"
    },
    {
        "title": "On Robustness of Finetuned Transformer-based NLP Models",
        "authors": [
            "Pavan Kalyan Reddy Neerudu",
            "Subba Reddy Oota",
            "Mounika Marreddy",
            "Venkateswara Rao Kagita",
            "Manish Gupta"
        ],
        "published": "2023-05-23T18:25:18Z",
        "summary": "Transformer-based pretrained models like BERT, GPT-2 and T5 have been\nfinetuned for a large number of natural language processing (NLP) tasks, and\nhave been shown to be very effective. However, while finetuning, what changes\nacross layers in these models with respect to pretrained checkpoints is\nunder-studied. Further, how robust are these models to perturbations in input\ntext? Does the robustness vary depending on the NLP task for which the models\nhave been finetuned? While there exists some work on studying the robustness of\nBERT finetuned for a few NLP tasks, there is no rigorous study that compares\nthis robustness across encoder only, decoder only and encoder-decoder models.\nIn this paper, we characterize changes between pretrained and finetuned\nlanguage model representations across layers using two metrics: CKA and STIR.\nFurther, we study the robustness of three language models (BERT, GPT-2 and T5)\nwith eight different text perturbations on classification tasks from the\nGeneral Language Understanding Evaluation (GLUE) benchmark, and generation\ntasks like summarization, free-form generation and question generation. GPT-2\nrepresentations are more robust than BERT and T5 across multiple types of input\nperturbation. Although models exhibit good robustness broadly, dropping nouns,\nverbs or changing characters are the most impactful. Overall, this study\nprovides valuable insights into perturbation-specific weaknesses of popular\nTransformer-based models, which should be kept in mind when passing inputs. We\nmake the code and models publicly available\n[https://github.com/PavanNeerudu/Robustness-of-Transformers-models].",
        "pdf_link": "https://arxiv.org/pdf/2305.14453v2.pdf"
    },
    {
        "title": "Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding",
        "authors": [
            "Zheng Chen",
            "Ziyan Jiang",
            "Fan Yang",
            "Eunah Cho",
            "Xing Fan",
            "Xiaojiang Huang",
            "Yanbin Lu",
            "Aram Galstyan"
        ],
        "published": "2023-05-23T18:15:29Z",
        "summary": "Conversational AI systems such as Alexa need to understand defective queries\nto ensure robust conversational understanding and reduce user friction. These\ndefective queries often arise from user ambiguities, mistakes, or errors in\nautomatic speech recognition (ASR) and natural language understanding (NLU).\n  Personalized query rewriting is an approach that focuses on reducing defects\nin queries by taking into account the user's individual behavior and\npreferences. It typically relies on an index of past successful user\ninteractions with the conversational AI. However, unseen interactions within\nthe user's history present additional challenges for personalized query\nrewriting. This paper presents our \"Collaborative Query Rewriting\" approach,\nwhich specifically addresses the task of rewriting new user interactions that\nhave not been previously observed in the user's history. This approach builds a\n\"User Feedback Interaction Graph\" (FIG) of historical user-entity interactions\nand leverages multi-hop graph traversal to enrich each user's index to cover\nfuture unseen defective queries. The enriched user index is called a\nCollaborative User Index and contains hundreds of additional entries. To\ncounteract precision degradation from the enlarged index, we add additional\ntransformer layers to the L1 retrieval model and incorporate graph-based and\nguardrail features into the L2 ranking model.\n  Since the user index can be pre-computed, we further investigate the\nutilization of a Large Language Model (LLM) to enhance the FIG for user-entity\nlink prediction in the Video/Music domains. Specifically, this paper\ninvestigates the Dolly-V2 7B model. We found that the user index augmented by\nthe fine-tuned Dolly-V2 generation significantly enhanced the coverage of\nfuture unseen user interactions, thereby boosting QR performance on unseen\nqueries compared with the graph traversal only approach.",
        "pdf_link": "https://arxiv.org/pdf/2305.14449v3.pdf"
    },
    {
        "title": "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning",
        "authors": [
            "Wentao Bao",
            "Lichang Chen",
            "Heng Huang",
            "Yu Kong"
        ],
        "published": "2023-05-23T18:00:22Z",
        "summary": "Compositional zero-shot learning (CZSL) task aims to recognize unseen\ncompositional visual concepts, e.g., sliced tomatoes, where the model is\nlearned only from the seen compositions, e.g., sliced potatoes and red\ntomatoes. Thanks to the prompt tuning on large pre-trained visual language\nmodels such as CLIP, recent literature shows impressively better CZSL\nperformance than traditional vision-based methods. However, the key aspects\nthat impact the generalization to unseen compositions, including the diversity\nand informativeness of class context, and the entanglement between visual\nprimitives, i.e., state and object, are not properly addressed in existing\nCLIP-based CZSL literature. In this paper, we propose a model by prompting the\nlanguage-informed distribution, aka., PLID, for the CZSL task. Specifically,\nthe PLID leverages pre-trained large language models (LLM) to 1) formulate the\nlanguage-informed class distributions which are diverse and informative, and 2)\nenhance the compositionality of the class embedding. Moreover, a\nvisual-language primitive decomposition (VLPD) module and a stochastic logit\nmixup (SLM) strategy are proposed to dynamically fuse the decisions from the\ncompositional and the primitive logit space. Orthogonal to the existing\nliterature of soft, hard, or distributional prompts, our method advocates\nprompting the LLM-supported class distribution that leads to a better zero-shot\ngeneralization. Experimental results on MIT-States, UT-Zappos, and C-GQA\ndatasets show the superior performance of the PLID to the prior arts.",
        "pdf_link": "https://arxiv.org/pdf/2305.14428v2.pdf"
    },
    {
        "title": "Schema-Driven Information Extraction from Heterogeneous Tables",
        "authors": [
            "Fan Bai",
            "Junmo Kang",
            "Gabriel Stanovsky",
            "Dayne Freitag",
            "Alan Ritter"
        ],
        "published": "2023-05-23T17:58:10Z",
        "summary": "In this paper, we explore the question of whether large language models can\nsupport cost-efficient information extraction from tables. We introduce\nschema-driven information extraction, a new task that transforms tabular data\ninto structured records following a human-authored schema. To assess various\nLLM's capabilities on this task, we present a benchmark comprised of tables\nfrom four diverse domains: machine learning papers, chemistry literature,\nmaterial science journals, and webpages. We use this collection of annotated\ntables to evaluate the ability of open-source and API-based language models to\nextract information from tables covering diverse domains and data formats. Our\nexperiments demonstrate that surprisingly competitive performance can be\nachieved without requiring task-specific pipelines or labels, achieving F1\nscores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover,\nthrough detailed ablation studies and analyses, we investigate the factors\ncontributing to model success and validate the practicality of distilling\ncompact models to reduce API reliance.",
        "pdf_link": "https://arxiv.org/pdf/2305.14336v3.pdf"
    },
    {
        "title": "DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation",
        "authors": [
            "Susung Hong",
            "Junyoung Seo",
            "Heeseong Shin",
            "Sunghwan Hong",
            "Seungryong Kim"
        ],
        "published": "2023-05-23T17:57:09Z",
        "summary": "In the paradigm of AI-generated content (AIGC), there has been increasing\nattention to transferring knowledge from pre-trained text-to-image (T2I) models\nto text-to-video (T2V) generation. Despite their effectiveness, these\nframeworks face challenges in maintaining consistent narratives and handling\nshifts in scene composition or object placement from a single abstract user\nprompt. Exploring the ability of large language models (LLMs) to generate\ntime-dependent, frame-by-frame prompts, this paper introduces a new framework,\ndubbed DirecT2V. DirecT2V leverages instruction-tuned LLMs as directors,\nenabling the inclusion of time-varying content and facilitating consistent\nvideo generation. To maintain temporal consistency and prevent mapping the\nvalue to a different object, we equip a diffusion model with a novel value\nmapping method and dual-softmax filtering, which do not require any additional\ntraining. The experimental results validate the effectiveness of our framework\nin producing visually coherent and storyful videos from abstract user prompts,\nsuccessfully addressing the challenges of zero-shot video generation.",
        "pdf_link": "https://arxiv.org/pdf/2305.14330v3.pdf"
    },
    {
        "title": "Benchmarking LLM-based Machine Translation on Cultural Awareness",
        "authors": [
            "Binwei Yao",
            "Ming Jiang",
            "Diyi Yang",
            "Junjie Hu"
        ],
        "published": "2023-05-23T17:56:33Z",
        "summary": "Translating cultural-specific content is crucial for effective cross-cultural\ncommunication. However, many MT systems still struggle to translate sentences\ncontaining cultural-specific entities accurately and understandably. Recent\nadvancements in in-context learning utilize lightweight prompts to guide large\nlanguage models (LLMs) in machine translation tasks. Nevertheless, the\neffectiveness of this approach in enhancing machine translation with cultural\nawareness remains uncertain. To address this gap, we introduce a new data\ncuration pipeline to construct a culturally relevant parallel corpus, enriched\nwith annotations of cultural-specific items. Furthermore, we devise a novel\nevaluation metric to assess the understandability of translations in a\nreference-free manner by GPT-4. We evaluate a variety of neural machine\ntranslation (NMT) and LLM-based MT systems using our dataset. Additionally, we\npropose several prompting strategies for LLMs to incorporate external and\ninternal cultural knowledge into the translation process. Our results\ndemonstrate that eliciting explanations can significantly enhance the\nunderstandability of cultural-specific entities, especially those without\nwell-known translations.",
        "pdf_link": "https://arxiv.org/pdf/2305.14328v2.pdf"
    },
    {
        "title": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
        "authors": [
            "Da Yin",
            "Xiao Liu",
            "Fan Yin",
            "Ming Zhong",
            "Hritik Bansal",
            "Jiawei Han",
            "Kai-Wei Chang"
        ],
        "published": "2023-05-23T17:56:26Z",
        "summary": "Instruction tuning has emerged to enhance the capabilities of large language\nmodels (LLMs) to comprehend instructions and generate appropriate responses.\nExisting methods either manually annotate or employ LLM (e.g., GPT-series) to\ngenerate data for instruction tuning. However, they often overlook associating\ninstructions with existing annotated datasets. In this paper, we propose\nDynosaur, a dynamic growth paradigm for the automatic curation of\ninstruction-tuning data. Based on the metadata of existing datasets, we use\nLLMs to automatically construct instruction-tuning data by identifying relevant\ndata fields and generating appropriate instructions.\n  By leveraging the existing annotated datasets, Dynosaur offers several\nadvantages: 1) it reduces the API cost for generating instructions (e.g., it\ncosts less than $12 USD by calling GPT-3.5-turbo for generating 800K\ninstruction tuning samples; 2) it provides high-quality data for instruction\ntuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform\nwith comparable data sizes); and 3) it supports the continuous improvement of\nmodels by generating instruction-tuning data when a new annotated dataset\nbecomes available. We further investigate a continual learning scheme for\nlearning with the ever-growing instruction-tuning dataset, and demonstrate that\nreplaying tasks with diverse instruction embeddings not only helps mitigate\nforgetting issues but generalizes to unseen tasks better.\n  Code and data are available at https://github.com/WadeYin9712/Dynosaur.",
        "pdf_link": "https://arxiv.org/pdf/2305.14327v2.pdf"
    },
    {
        "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
        "authors": [
            "Yilun Du",
            "Shuang Li",
            "Antonio Torralba",
            "Joshua B. Tenenbaum",
            "Igor Mordatch"
        ],
        "published": "2023-05-23T17:55:11Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nlanguage generation, understanding, and few-shot learning in recent years. An\nextensive body of work has explored how their performance may be further\nimproved through the tools of prompting, ranging from verification,\nself-consistency, or intermediate scratchpads. In this paper, we present a\ncomplementary approach to improve language responses where multiple language\nmodel instances propose and debate their individual responses and reasoning\nprocesses over multiple rounds to arrive at a common final answer. Our findings\nindicate that this approach significantly enhances mathematical and strategic\nreasoning across a number of tasks. We also demonstrate that our approach\nimproves the factual validity of generated content, reducing fallacious answers\nand hallucinations that contemporary models are prone to. Our approach may be\ndirectly applied to existing black-box models and uses identical procedure and\nprompts for all tasks we investigate. Overall, our findings suggest that such\n\"society of minds\" approach has the potential to significantly advance the\ncapabilities of LLMs and pave the way for further breakthroughs in language\ngeneration and understanding.",
        "pdf_link": "https://arxiv.org/pdf/2305.14325v1.pdf"
    },
    {
        "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
        "authors": [
            "Zhipeng Chen",
            "Kun Zhou",
            "Beichen Zhang",
            "Zheng Gong",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-23T17:54:33Z",
        "summary": "Although large language models (LLMs) have achieved excellent performance in\na variety of evaluation benchmarks, they still struggle in complex reasoning\ntasks which require specific knowledge and multi-hop reasoning. To improve the\nreasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought\nreasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model\nthe chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize\ntools in a more natural way through chatting. At each turn, LLMs can either\ninteract with tools or perform the reasoning. Our approach can effectively\nleverage the multi-turn conversation ability of chat-based LLMs, and integrate\nthe thought chain following and tools manipulation in a unified way. Specially,\nwe initialize the early turns of the conversation by the knowledge about tools,\ntasks, and reasoning format, and propose an iterative tool-augmented reasoning\nstep to perform step-by-step tool-augmented reasoning. The experiment results\non two complex reasoning datasets (MATH and HotpotQA) have shown the\neffectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative\nimprovement over the state-of-the-art baseline. Our code and data are available\nat: \\url{https://github.com/RUCAIBOX/ChatCoT}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14323v3.pdf"
    },
    {
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "authors": [
            "Ali Modarressi",
            "Ayyoob Imani",
            "Mohsen Fayyaz",
            "Hinrich Schütze"
        ],
        "published": "2023-05-23T17:53:38Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.",
        "pdf_link": "https://arxiv.org/pdf/2305.14322v1.pdf"
    },
    {
        "title": "CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models",
        "authors": [
            "Cheng Qian",
            "Chi Han",
            "Yi R. Fung",
            "Yujia Qin",
            "Zhiyuan Liu",
            "Heng Ji"
        ],
        "published": "2023-05-23T17:51:52Z",
        "summary": "Large Language Models (LLMs) have made significant progress in utilizing\ntools, but their ability is limited by API availability and the instability of\nimplicit reasoning, particularly when both planning and execution are involved.\nTo overcome these limitations, we propose CREATOR, a novel framework that\nenables LLMs to create their own tools using documentation and code\nrealization. CREATOR disentangles abstract tool creation and concrete decision\nexecution, resulting in improved performance. We evaluate CREATOR on MATH and\nTabMWP benchmarks, respectively consisting of challenging math competition\nproblems and diverse tabular contents. Remarkably, CREATOR outperforms existing\nchain-of-thought, program-of-thought, and tool-using baselines. Additionally,\nwe introduce the Creation Challenge dataset, featuring 2K diverse questions, to\nemphasize the necessity and benefits of LLMs' tool creation ability. Further\nresearch demonstrates that leveraging LLMs as tool creators facilitates\nknowledge transfer, and LLMs exhibit varying levels of tool creation abilities,\nenabling them to adapt to diverse situations. The tool creation ability\nrevolutionizes the LLM's problem-solving paradigm, driving us closer to the\nnext frontier of artificial intelligence. All the codes and data are released.",
        "pdf_link": "https://arxiv.org/pdf/2305.14318v2.pdf"
    },
    {
        "title": "Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science",
        "authors": [
            "Yida Mu",
            "Ben P. Wu",
            "William Thorne",
            "Ambrose Robinson",
            "Nikolaos Aletras",
            "Carolina Scarton",
            "Kalina Bontcheva",
            "Xingyi Song"
        ],
        "published": "2023-05-23T17:48:21Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) have exhibited impressive\nlanguage understanding and the capacity to generate responses that follow\nspecific prompts. However, due to the computational demands associated with\ntraining these models, their applications often adopt a zero-shot setting. In\nthis paper, we evaluate the zero-shot performance of two publicly accessible\nLLMs, ChatGPT and OpenAssistant, in the context of six Computational Social\nScience classification tasks, while also investigating the effects of various\nprompting strategies. Our experiments investigate the impact of prompt\ncomplexity, including the effect of incorporating label definitions into the\nprompt; use of synonyms for label names; and the influence of integrating past\nmemories during foundation model training. The findings indicate that in a\nzero-shot setting, current LLMs are unable to match the performance of smaller,\nfine-tuned baseline transformer models (such as BERT-large). Additionally, we\nfind that different prompting strategies can significantly affect\nclassification accuracy, with variations in accuracy and F1 scores exceeding\n10\\%.",
        "pdf_link": "https://arxiv.org/pdf/2305.14310v3.pdf"
    },
    {
        "title": "QTSumm: Query-Focused Summarization over Tabular Data",
        "authors": [
            "Yilun Zhao",
            "Zhenting Qi",
            "Linyong Nan",
            "Boyu Mi",
            "Yixin Liu",
            "Weijin Zou",
            "Simeng Han",
            "Ruizhe Chen",
            "Xiangru Tang",
            "Yumo Xu",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023-05-23T17:43:51Z",
        "summary": "People primarily consult tables to conduct data analysis or answer specific\nquestions. Text generation systems that can provide accurate table summaries\ntailored to users' information needs can facilitate more efficient access to\nrelevant data insights. Motivated by this, we define a new query-focused table\nsummarization task, where text generation models have to perform human-like\nreasoning and analysis over the given table to generate a tailored summary. We\nintroduce a new benchmark named QTSumm for this task, which contains 7,111\nhuman-annotated query-summary pairs over 2,934 tables covering diverse topics.\nWe investigate a set of strong baselines on QTSumm, including text generation,\ntable-to-text generation, and large language models. Experimental results and\nmanual analysis reveal that the new task presents significant challenges in\ntable-to-text generation for future research. Moreover, we propose a new\napproach named ReFactor, to retrieve and reason over query-relevant information\nfrom tabular data to generate several natural language facts. Experimental\nresults demonstrate that ReFactor can bring improvements to baselines by\nconcatenating the generated facts to the model input. Our data and code are\npublicly available at https://github.com/yale-nlp/QTSumm.",
        "pdf_link": "https://arxiv.org/pdf/2305.14303v2.pdf"
    },
    {
        "title": "Evaluation of African American Language Bias in Natural Language Generation",
        "authors": [
            "Nicholas Deas",
            "Jessi Grieser",
            "Shana Kleiner",
            "Desmond Patton",
            "Elsbeth Turcan",
            "Kathleen McKeown"
        ],
        "published": "2023-05-23T17:34:37Z",
        "summary": "We evaluate how well LLMs understand African American Language (AAL) in\ncomparison to their performance on White Mainstream English (WME), the\nencouraged \"standard\" form of English taught in American classrooms. We measure\nLLM performance using automatic metrics and human judgments for two tasks: a\ncounterpart generation task, where a model generates AAL (or WME) given WME (or\nAAL), and a masked span prediction (MSP) task, where models predict a phrase\nthat was removed from their input. Our contributions include: (1) evaluation of\nsix pre-trained, large language models on the two language generation tasks;\n(2) a novel dataset of AAL text from multiple contexts (social media, hip-hop\nlyrics, focus groups, and linguistic interviews) with human-annotated\ncounterparts in WME; and (3) documentation of model performance gaps that\nsuggest bias and identification of trends in lack of understanding of AAL\nfeatures.",
        "pdf_link": "https://arxiv.org/pdf/2305.14291v2.pdf"
    },
    {
        "title": "LLM-powered Data Augmentation for Enhanced Cross-lingual Performance",
        "authors": [
            "Chenxi Whitehouse",
            "Monojit Choudhury",
            "Alham Fikri Aji"
        ],
        "published": "2023-05-23T17:33:27Z",
        "summary": "This paper explores the potential of leveraging Large Language Models (LLMs)\nfor data augmentation in multilingual commonsense reasoning datasets where the\navailable training data is extremely limited. To achieve this, we utilise\nseveral LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment\nthree datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate\nthe effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR,\nusing the synthesised data. We compare the performance of training with data\ngenerated in English and target languages, as well as translated\nEnglish-generated data, revealing the overall advantages of incorporating data\ngenerated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best\ncase. Furthermore, we conduct a human evaluation by asking native speakers to\nassess the naturalness and logical coherence of the generated examples across\ndifferent languages. The results of the evaluation indicate that LLMs such as\nChatGPT and GPT-4 excel at producing natural and coherent text in most\nlanguages, however, they struggle to generate meaningful text in certain\nlanguages like Tamil. We also observe that ChatGPT falls short in generating\nplausible alternatives compared to the original dataset, whereas examples from\nGPT-4 exhibit competitive logical consistency.",
        "pdf_link": "https://arxiv.org/pdf/2305.14288v2.pdf"
    },
    {
        "title": "INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback",
        "authors": [
            "Wenda Xu",
            "Danqing Wang",
            "Liangming Pan",
            "Zhenqiao Song",
            "Markus Freitag",
            "William Yang Wang",
            "Lei Li"
        ],
        "published": "2023-05-23T17:27:22Z",
        "summary": "Automatically evaluating the quality of language generation is critical.\nAlthough recent learned metrics show high correlation with human judgement,\nthese metrics can not explain their verdict or associate the scores with\ndefects in generated text. To address this limitation, we present\nInstructScore, an explainable evaluation metric for text generation. By\nharnessing both explicit human instruction and the implicit knowledge of GPT-4,\nwe fine-tune a text evaluation metric based on LLaMA, producing both a score\nfor generated text and a human readable diagnostic report. We evaluate\nInstructScore on a variety of generation tasks, including translation,\ncaptioning, data-to-text and commonsense generation. Experiments show that our\n7B model surpasses all other unsupervised metrics, including those based on\n175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct\nsupervision from human-rated data, achieves performance levels on par with\nstate-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",
        "pdf_link": "https://arxiv.org/pdf/2305.14282v3.pdf"
    },
    {
        "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
        "authors": [
            "Angelica Chen",
            "Jason Phang",
            "Alicia Parrish",
            "Vishakh Padmakumar",
            "Chen Zhao",
            "Samuel R. Bowman",
            "Kyunghyun Cho"
        ],
        "published": "2023-05-23T17:25:59Z",
        "summary": "Large language models (LLMs) have achieved widespread success on a variety of\nin-context few-shot tasks, but this success is typically evaluated via\ncorrectness rather than consistency. We argue that self-consistency is an\nimportant criteria for valid multi-step reasoning in tasks where the solution\nis composed of the answers to multiple sub-steps. We propose two types of\nself-consistency that are particularly important for multi-step reasoning --\nhypothetical consistency (a model's ability to predict what its output would be\nin a hypothetical other context) and compositional consistency (consistency of\na model's final outputs when intermediate sub-steps are replaced with the\nmodel's outputs for those steps). We demonstrate that multiple variants of the\nGPT-3/-4 models exhibit poor consistency rates across both types of consistency\non a variety of tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14279v4.pdf"
    },
    {
        "title": "SciMON: Scientific Inspiration Machines Optimized for Novelty",
        "authors": [
            "Qingyun Wang",
            "Doug Downey",
            "Heng Ji",
            "Tom Hope"
        ],
        "published": "2023-05-23T17:12:08Z",
        "summary": "We explore and enhance the ability of neural language models to generate\nnovel scientific directions grounded in literature. Work on literature-based\nhypothesis generation has traditionally focused on binary link prediction --\nseverely limiting the expressivity of hypotheses. This line of work also does\nnot focus on optimizing novelty. We take a dramatic departure with a novel\nsetting in which models use as input background contexts (e.g., problems,\nexperimental settings, goals), and output natural language ideas grounded in\nliterature. We present SciMON, a modeling framework that uses retrieval of\n\"inspirations\" from past scientific papers, and explicitly optimizes for\nnovelty by iteratively comparing to prior papers and updating idea suggestions\nuntil sufficient novelty is achieved. Comprehensive evaluations reveal that\nGPT-4 tends to generate ideas with overall low technical depth and novelty,\nwhile our methods partially mitigate this issue. Our work represents a first\nstep toward evaluating and developing language models that generate new ideas\nderived from the scientific literature.",
        "pdf_link": "https://arxiv.org/pdf/2305.14259v5.pdf"
    },
    {
        "title": "Hierarchical Prompting Assists Large Language Model on Web Navigation",
        "authors": [
            "Abishek Sridhar",
            "Robert Lo",
            "Frank F. Xu",
            "Hao Zhu",
            "Shuyan Zhou"
        ],
        "published": "2023-05-23T17:10:39Z",
        "summary": "Large language models (LLMs) struggle on processing complicated observations\nin interactive decision making tasks. To alleviate this issue, we propose a\nsimple hierarchical prompting approach. Diverging from previous prompting\napproaches that always put the full observation (e.g. a web page) to the\nprompt, we propose to first construct an action-aware observation which is more\ncondensed and relevant with a dedicated SUMMARIZER prompt. The ACTOR prompt\nthen predicts the next action based on the summarized observation. While our\nmethod has broad applicability, we particularly demonstrate its efficacy in the\ncomplex domain of web navigation where a full observation often contains\nredundant and irrelevant information. Our approach outperforms the previous\nstate-of-the-art prompting mechanics by 6.2% on task success rate,\ndemonstrating its potential on interactive decision making tasks with long\nobservation traces.",
        "pdf_link": "https://arxiv.org/pdf/2305.14257v3.pdf"
    },
    {
        "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
        "authors": [
            "Sewon Min",
            "Kalpesh Krishna",
            "Xinxi Lyu",
            "Mike Lewis",
            "Wen-tau Yih",
            "Pang Wei Koh",
            "Mohit Iyyer",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-05-23T17:06:00Z",
        "summary": "Evaluating the factuality of long-form text generated by large language\nmodels (LMs) is non-trivial because (1) generations often contain a mixture of\nsupported and unsupported pieces of information, making binary judgments of\nquality inadequate, and (2) human evaluation is time-consuming and costly. In\nthis paper, we introduce FACTSCORE, a new evaluation that breaks a generation\ninto a series of atomic facts and computes the percentage of atomic facts\nsupported by a reliable knowledge source. We conduct an extensive human\nevaluation to obtain FACTSCOREs of people biographies generated by several\nstate-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the\nretrieval-augmented PerplexityAI -- and report new analysis demonstrating the\nneed for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since\nhuman evaluation is costly, we also introduce an automated model that estimates\nFACTSCORE using retrieval and a strong language model, with less than a 2%\nerror rate. Finally, we use this automated metric to evaluate 6,500 generations\nfrom a new set of 13 recent LMs that would have cost $26K if evaluated by\nhumans, with various findings: GPT-4 and ChatGPT are more factual than public\nmodels, and Vicuna and Alpaca are some of the best public models. FACTSCORE is\navailable for public use via `pip install factscore`.",
        "pdf_link": "https://arxiv.org/pdf/2305.14251v2.pdf"
    },
    {
        "title": "Language Models with Rationality",
        "authors": [
            "Nora Kassner",
            "Oyvind Tafjord",
            "Ashish Sabharwal",
            "Kyle Richardson",
            "Hinrich Schuetze",
            "Peter Clark"
        ],
        "published": "2023-05-23T17:04:25Z",
        "summary": "While large language models (LLMs) are proficient at question-answering (QA),\nit is not always clear how (or even if) an answer follows from their latent\n\"beliefs\". This lack of interpretability is a growing impediment to widespread\nuse of LLMs. To address this, our goals are to make model beliefs and their\ninferential relationships explicit, and to resolve inconsistencies that may\nexist, so that answers are supported by interpretable chains of reasoning drawn\nfrom a consistent network of beliefs. Our approach, which we call REFLEX, is to\nadd a rational, self-reflecting layer on top of the LLM. First, given a\nquestion, we construct a belief graph using a backward-chaining process to\nmaterialize relevant model beliefs (including beliefs about answer candidates)\nand their inferential relationships. Second, we identify and minimize\ncontradictions in that graph using a formal constraint reasoner. We find that\nREFLEX significantly improves consistency (by 8%-11% absolute) without harming\noverall answer accuracy, resulting in answers supported by faithful chains of\nreasoning drawn from a more consistent belief system. This suggests a new style\nof system architecture in which an LLM extended with a rational layer can\nprovide an interpretable window into system beliefs, add a systematic reasoning\ncapability, and repair latent inconsistencies present in the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.14250v2.pdf"
    },
    {
        "title": "On Learning to Summarize with Large Language Models as References",
        "authors": [
            "Yixin Liu",
            "Kejian Shi",
            "Katherine S He",
            "Longtian Ye",
            "Alexander R. Fabbri",
            "Pengfei Liu",
            "Dragomir Radev",
            "Arman Cohan"
        ],
        "published": "2023-05-23T16:56:04Z",
        "summary": "Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we investigate a new learning\nsetting of text summarization models that considers the LLMs as the reference\nor the gold-standard oracle on these datasets. To examine the standard\npractices that are aligned with this new learning setting, we investigate two\nLLM-based summary quality evaluation methods for model training and adopt a\ncontrastive learning training method to leverage the LLM-guided learning\nsignals. Our experiments on the CNN/DailyMail and XSum datasets demonstrate\nthat smaller summarization models can achieve similar performance as LLMs under\nLLM-based evaluation. However, we found that the smaller models can not yet\nreach LLM-level performance under human evaluation despite promising\nimprovements brought by our proposed training methods. Meanwhile, we perform a\nmeta-analysis on this new learning setting that reveals a discrepancy between\nhuman and LLM-based evaluation, highlighting the benefits and risks of this\nLLM-as-reference setting we investigated.",
        "pdf_link": "https://arxiv.org/pdf/2305.14239v2.pdf"
    },
    {
        "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
        "authors": [
            "Ruochen Zhang",
            "Samuel Cahyawijaya",
            "Jan Christian Blaise Cruz",
            "Genta Indra Winata",
            "Alham Fikri Aji"
        ],
        "published": "2023-05-23T16:50:48Z",
        "summary": "Multilingual Large Language Models (LLMs) have recently shown great\ncapabilities in a wide range of tasks, exhibiting state-of-the-art performance\nthrough zero-shot or few-shot prompting methods. While there have been\nextensive studies on their abilities in monolingual tasks, the investigation of\ntheir potential in the context of code-switching (CSW), the practice of\nalternating languages within an utterance, remains relatively uncharted. In\nthis paper, we provide a comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their performance across four tasks: sentiment\nanalysis, machine translation, summarization and word-level language\nidentification. Our results indicate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero or few-shot prompting, they\nstill underperform in comparison to fine-tuned models of much smaller scales.\nWe argue that current \"multilingualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling for future research to bridge\nthis discrepancy.",
        "pdf_link": "https://arxiv.org/pdf/2305.14235v2.pdf"
    },
    {
        "title": "ManiTweet: A New Benchmark for Identifying Manipulation of News on Social Media",
        "authors": [
            "Kung-Hsiang Huang",
            "Hou Pong Chan",
            "Kathleen McKeown",
            "Heng Ji"
        ],
        "published": "2023-05-23T16:40:07Z",
        "summary": "Considerable advancements have been made to tackle the misrepresentation of\ninformation derived from reference articles in the domains of fact-checking and\nfaithful summarization. However, an unaddressed aspect remains - the\nidentification of social media posts that manipulate information within\nassociated news articles. This task presents a significant challenge, primarily\ndue to the prevalence of personal opinions in such posts. We present a novel\ntask, identifying manipulation of news on social media, which aims to detect\nmanipulation in social media posts and identify manipulated or inserted\ninformation. To study this task, we have proposed a data collection schema and\ncurated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and\ncorresponding articles. Our analysis demonstrates that this task is highly\nchallenging, with large language models (LLMs) yielding unsatisfactory\nperformance. Additionally, we have developed a simple yet effective basic model\nthat outperforms LLMs significantly on the ManiTweet dataset. Finally, we have\nconducted an exploratory analysis of human-written tweets, unveiling intriguing\nconnections between manipulation and the domain and factuality of news\narticles, as well as revealing that manipulated sentences are more likely to\nencapsulate the main story or consequences of a news outlet.",
        "pdf_link": "https://arxiv.org/pdf/2305.14225v1.pdf"
    },
    {
        "title": "Question Answering as Programming for Solving Time-Sensitive Questions",
        "authors": [
            "Xinyu Zhu",
            "Cheng Yang",
            "Bei Chen",
            "Siheng Li",
            "Jian-Guang Lou",
            "Yujiu Yang"
        ],
        "published": "2023-05-23T16:35:16Z",
        "summary": "Question answering plays a pivotal role in human daily life because it\ninvolves our acquisition of knowledge about the world. However, due to the\ndynamic and ever-changing nature of real-world facts, the answer can be\ncompletely different when the time constraint in the question changes.\nRecently, Large Language Models (LLMs) have shown remarkable intelligence in\nquestion answering, while our experiments reveal that the aforementioned\nproblems still pose a significant challenge to existing LLMs. This can be\nattributed to the LLMs' inability to perform rigorous reasoning based on\nsurface-level text semantics. To overcome this limitation, rather than\nrequiring LLMs to directly answer the question, we propose a novel approach\nwhere we reframe the $\\textbf{Q}$uestion $\\textbf{A}$nswering task\n$\\textbf{a}$s $\\textbf{P}$rogramming ($\\textbf{QAaP}$). Concretely, by\nleveraging modern LLMs' superior capability in understanding both natural\nlanguage and programming language, we endeavor to harness LLMs to represent\ndiversely expressed text as well-structured code and select the best matching\nanswer from multiple candidates through programming. We evaluate our QAaP\nframework on several time-sensitive question answering datasets and achieve\ndecent improvement, up to $14.5$% over strong baselines. Our codes and data are\navailable at https://github.com/TianHongZXY/qaap",
        "pdf_link": "https://arxiv.org/pdf/2305.14221v3.pdf"
    },
    {
        "title": "Exploring Chain-of-Thought Style Prompting for Text-to-SQL",
        "authors": [
            "Chang-You Tai",
            "Ziru Chen",
            "Tianshu Zhang",
            "Xiang Deng",
            "Huan Sun"
        ],
        "published": "2023-05-23T16:32:36Z",
        "summary": "In-context learning with large language models (LLMs) has recently caught\nincreasing attention due to its superior few-shot performance on various tasks.\nHowever, its performance on text-to-SQL parsing still has much room for\nimprovement. In this paper, we hypothesize that a crucial aspect of LLMs to\nimprove for text-to-SQL parsing is their multi-step reasoning ability. Thus, we\nsystematically study how to enhance LLMs' reasoning ability through chain of\nthought (CoT) style prompting, including the original chain-of-thought\nprompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023).\nOur experiments demonstrate that iterative prompting as in Zhou et al. (2023)\nmay be unnecessary for text-to-SQL parsing, and using detailed reasoning steps\ntends to have more error propagation issues. Based on these findings, we\npropose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2\nand 6.5 point absolute gains on the Spider development set and the Spider\nRealistic set, respectively, compared to the standard prompting method without\nreasoning steps; 2.4 and 1.5 point absolute gains, compared to the\nleast-to-most prompting method.",
        "pdf_link": "https://arxiv.org/pdf/2305.14215v2.pdf"
    },
    {
        "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
        "authors": [
            "Benjamin Minixhofer",
            "Jonas Pfeiffer",
            "Ivan Vulić"
        ],
        "published": "2023-05-23T16:32:27Z",
        "summary": "While many languages possess processes of joining two or more words to create\ncompound words, previous studies have been typically limited only to languages\nwith excessively productive compound formation (e.g., German, Dutch) and there\nis no public dataset containing compound and non-compound words across a large\nnumber of languages. In this work, we systematically study decompounding, the\ntask of splitting compound words into their constituents, at a wide scale. We\nfirst address the data gap by introducing a dataset of 255k compound and\nnon-compound words across 56 diverse languages obtained from Wiktionary. We\nthen use this dataset to evaluate an array of Large Language Models (LLMs) on\nthe decompounding task. We find that LLMs perform poorly, especially on words\nwhich are tokenized unfavorably by subword tokenization. We thus introduce a\nnovel methodology to train dedicated models for decompounding. The proposed\ntwo-stage procedure relies on a fully self-supervised objective in the first\nstage, while the second, supervised learning stage optionally fine-tunes the\nmodel on the annotated Wiktionary data. Our self-supervised models outperform\nthe prior best unsupervised decompounding models by 13.9% accuracy on average.\nOur fine-tuned models outperform all prior (language-specific) decompounding\ntools. Furthermore, we use our models to leverage decompounding during the\ncreation of a subword tokenizer, which we refer to as CompoundPiece.\nCompoundPiece tokenizes compound words more favorably on average, leading to\nimproved performance on decompounding over an otherwise equivalent model using\nSentencePiece tokenization.",
        "pdf_link": "https://arxiv.org/pdf/2305.14214v2.pdf"
    },
    {
        "title": "Domain Private Transformers for Multi-Domain Dialog Systems",
        "authors": [
            "Anmol Kabra",
            "Ethan R. Elenberg"
        ],
        "published": "2023-05-23T16:27:12Z",
        "summary": "Large, general purpose language models have demonstrated impressive\nperformance across many different conversational domains. While multi-domain\nlanguage models achieve low overall perplexity, their outputs are not\nguaranteed to stay within the domain of a given input prompt. This paper\nproposes domain privacy as a novel way to quantify how likely a conditional\nlanguage model will leak across domains. We also develop policy functions based\non token-level domain classification, and propose an efficient fine-tuning\nmethod to improve the trained model's domain privacy. Experiments on membership\ninference attacks show that our proposed method has comparable resiliency to\nmethods adapted from recent literature on differentially private language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.14208v2.pdf"
    },
    {
        "title": "Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",
        "authors": [
            "Silei Xu",
            "Shicheng Liu",
            "Theo Culhane",
            "Elizaveta Pertseva",
            "Meng-Hsi Wu",
            "Sina J. Semnani",
            "Monica S. Lam"
        ],
        "published": "2023-05-23T16:20:43Z",
        "summary": "While large language models (LLMs) can answer many questions correctly, they\ncan also hallucinate and give wrong answers. Wikidata, with its over 12 billion\nfacts, can be used to ground LLMs to improve their factuality. This paper\npresents WikiWebQuestions, a high-quality question answering benchmark for\nWikidata. Ported over from WebQuestions for Freebase, it consists of real-world\ndata with SPARQL annotation. This paper presents a few-shot\nsequence-to-sequence semantic parser for Wikidata. We modify SPARQL to use the\nunique domain and property names instead of their IDs. We train the parser to\nuse either the results from an entity linker or mentions in the query. We\nfine-tune LLaMA by adding the few-shot training data to that used to fine-tune\nAlpaca. Our experimental results demonstrate the effectiveness of this\nmethodology, establishing a strong baseline of 76% and 65% answer accuracy in\nthe dev and test sets of WikiWebQuestions, respectively. By pairing our\nsemantic parser with GPT-3, we combine verifiable results with qualified GPT-3\nguesses to provide useful answers to 96% of the questions in dev. We also show\nthat our method outperforms the state-of-the-art for the QALD-7 Wikidata\ndataset by 3.6% in F1 score.",
        "pdf_link": "https://arxiv.org/pdf/2305.14202v2.pdf"
    },
    {
        "title": "HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors of Language Models in Human-Machine Conversations",
        "authors": [
            "Anthony Sicilia",
            "Jennifer C. Gates",
            "Malihe Alikhani"
        ],
        "published": "2023-05-23T16:15:24Z",
        "summary": "While demographic factors like age and gender change the way people talk, and\nin particular, the way people talk to machines, there is little investigation\ninto how large pre-trained language models (LMs) can adapt to these changes. To\nremedy this gap, we consider how demographic factors in LM language skills can\nbe measured to determine compatibility with a target demographic. We suggest\nclinical techniques from Speech Language Pathology, which has norms for\nacquisition of language skills in humans. We conduct evaluation with a domain\nexpert (i.e., a clinically licensed speech language pathologist), and also\npropose automated techniques to complement clinical evaluation at scale.\nEmpirically, we focus on age, finding LM capability varies widely depending on\ntask: GPT-3.5 mimics the ability of humans ranging from age 6-15 at tasks\nrequiring inference, and simultaneously, outperforms a typical 21 year old at\nmemorization. GPT-3.5 also has trouble with social language use, exhibiting\nless than 50% of the tested pragmatic skills. Findings affirm the importance of\nconsidering demographic alignment and conversational goals when using LMs as\npublic-facing tools. Code, data, and a package will be available.",
        "pdf_link": "https://arxiv.org/pdf/2305.14195v3.pdf"
    },
    {
        "title": "In-Context Probing: Toward Building Robust Classifiers via Probing Large Language Models",
        "authors": [
            "Afra Amini",
            "Massimiliano Ciaramita"
        ],
        "published": "2023-05-23T15:43:04Z",
        "summary": "Large language models are able to learn new tasks in context, where they are\nprovided with instructions and a few annotated examples. However, the\neffectiveness of in-context learning is dependent on the provided context, and\nthe performance on a downstream task can vary considerably, depending on the\ninstruction. Importantly, such dependency on the context can surface in\nunpredictable ways, e.g., a seemingly more informative instruction might lead\nto a worse performance. In this paper, we propose an alternative approach,\nwhich we term In-Context Probing (ICP). Similar to in-context learning, we\ncontextualize the representation of the input with an instruction, but instead\nof decoding the output prediction, we probe the contextualized representation\nto predict the label. Through a series of experiments on a diverse set of\nclassification tasks, we show that in-context probing is significantly more\nrobust to changes in instructions. We further show that ICP performs\ncompetitive or superior to finetuning and can be particularly helpful to build\nclassifiers on top of smaller models, with less than a hundred training\nexamples.",
        "pdf_link": "https://arxiv.org/pdf/2305.14171v3.pdf"
    },
    {
        "title": "DetGPT: Detect What You Need via Reasoning",
        "authors": [
            "Renjie Pi",
            "Jiahui Gao",
            "Shizhe Diao",
            "Rui Pan",
            "Hanze Dong",
            "Jipeng Zhang",
            "Lewei Yao",
            "Jianhua Han",
            "Hang Xu",
            "Lingpeng Kong",
            "Tong Zhang"
        ],
        "published": "2023-05-23T15:37:28Z",
        "summary": "In recent years, the field of computer vision has seen significant\nadvancements thanks to the development of large language models (LLMs). These\nmodels have enabled more effective and sophisticated interactions between\nhumans and machines, paving the way for novel techniques that blur the lines\nbetween human and machine intelligence. In this paper, we introduce a new\nparadigm for object detection that we call reasoning-based object detection.\nUnlike conventional object detection methods that rely on specific object\nnames, our approach enables users to interact with the system using natural\nlanguage instructions, allowing for a higher level of interactivity. Our\nproposed method, called DetGPT, leverages state-of-the-art multi-modal models\nand open-vocabulary object detectors to perform reasoning within the context of\nthe user's instructions and the visual scene. This enables DetGPT to\nautomatically locate the object of interest based on the user's expressed\ndesires, even if the object is not explicitly mentioned. For instance, if a\nuser expresses a desire for a cold beverage, DetGPT can analyze the image,\nidentify a fridge, and use its knowledge of typical fridge contents to locate\nthe beverage. This flexibility makes our system applicable across a wide range\nof fields, from robotics and automation to autonomous driving. Overall, our\nproposed paradigm and DetGPT demonstrate the potential for more sophisticated\nand intuitive interactions between humans and machines. We hope that our\nproposed paradigm and approach will provide inspiration to the community and\nopen the door to more interative and versatile object detection systems. Our\nproject page is launched at detgpt.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2305.14167v2.pdf"
    },
    {
        "title": "Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization",
        "authors": [
            "Jeonghoon Kim",
            "Jung Hyun Lee",
            "Sungdong Kim",
            "Joonsuk Park",
            "Kang Min Yoo",
            "Se Jung Kwon",
            "Dongsoo Lee"
        ],
        "published": "2023-05-23T15:20:01Z",
        "summary": "Large language models (LLMs) face the challenges in fine-tuning and\ndeployment due to their high memory demands and computational costs. While\nparameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage\nof the optimizer state during fine-tuning, the inherent size of pre-trained LLM\nweights continues to be a pressing concern. Even though quantization techniques\nare widely proposed to ease memory demands and accelerate LLM inference, most\nof these techniques are geared towards the deployment phase. To bridge this\ngap, this paper presents Parameter-Efficient and Quantization-aware Adaptation\n(PEQA) - a simple yet effective method that combines the advantages of PEFT\nwith quantized LLMs. By updating solely the quantization scales, PEQA can be\ndirectly applied to quantized LLMs, ensuring seamless task transitions.\nParallel to existing PEFT methods, PEQA significantly reduces the memory\noverhead associated with the optimizer state. Furthermore, it leverages the\nadvantages of quantization to substantially reduce model sizes. Even after\nfine-tuning, the quantization structure of a PEQA-tuned LLM remains intact,\nallowing for accelerated inference on the deployment stage. We employ\nPEQA-tuning for task-specific adaptation on LLMs with up to 65 billion\nparameters. To assess the logical reasoning and language comprehension of\nPEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction\ndataset. Our results show that even when LLMs are quantized to below 4-bit\nprecision, their capabilities in language modeling, few-shot in-context\nlearning, and comprehension can be resiliently restored to (or even improved\nover) their full-precision original performances with PEQA.",
        "pdf_link": "https://arxiv.org/pdf/2305.14152v2.pdf"
    },
    {
        "title": "GrACE: Generation using Associated Code Edits",
        "authors": [
            "Priyanshu Gupta",
            "Avishree Khare",
            "Yasharth Bajpai",
            "Saikat Chakraborty",
            "Sumit Gulwani",
            "Aditya Kanade",
            "Arjun Radhakrishna",
            "Gustavo Soares",
            "Ashish Tiwari"
        ],
        "published": "2023-05-23T14:55:44Z",
        "summary": "Developers expend a significant amount of time in editing code for a variety\nof reasons such as bug fixing or adding new features. Designing effective\nmethods to predict code edits has been an active yet challenging area of\nresearch due to the diversity of code edits and the difficulty of capturing the\ndeveloper intent. In this work, we address these challenges by endowing\npre-trained large language models (LLMs) of code with the knowledge of prior,\nrelevant edits. The generative capability of the LLMs helps address the\ndiversity in code changes and conditioning code generation on prior edits helps\ncapture the latent developer intent. We evaluate two well-known LLMs, Codex and\nCodeT5, in zero-shot and fine-tuning settings respectively. In our experiments\nwith two datasets, the knowledge of prior edits boosts the performance of the\nLLMs significantly and enables them to generate 29% and 54% more correctly\nedited code in top-1 suggestions relative to the current state-of-the-art\nsymbolic and neural approaches, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2305.14129v3.pdf"
    },
    {
        "title": "Dr.ICL: Demonstration-Retrieved In-context Learning",
        "authors": [
            "Man Luo",
            "Xin Xu",
            "Zhuyun Dai",
            "Panupong Pasupat",
            "Mehran Kazemi",
            "Chitta Baral",
            "Vaiva Imbrasaite",
            "Vincent Y Zhao"
        ],
        "published": "2023-05-23T14:55:25Z",
        "summary": "In-context learning (ICL), teaching a large language model (LLM) to perform a\ntask with few-shot demonstrations rather than adjusting the model parameters,\nhas emerged as a strong paradigm for using LLMs. While early studies primarily\nused a fixed or random set of demonstrations for all test queries, recent\nresearch suggests that retrieving semantically similar demonstrations to the\ninput from a pool of available demonstrations results in better performance.\nThis work expands the applicability of retrieval-based ICL approaches by\ndemonstrating that even simple word-overlap similarity measures such as BM25\noutperform randomly selected demonstrations. Furthermore, we extend the success\nof retrieval-based ICL to instruction-finetuned LLMs as well as\nChain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that\nalthough a model has already seen the training data at training time,\nretrieving demonstrations from the training data at test time yields better\nresults compared to using no demonstrations or random demonstrations. Last but\nnot least, we train a task-specific demonstration retriever that outperforms\noff-the-shelf retrievers.",
        "pdf_link": "https://arxiv.org/pdf/2305.14128v1.pdf"
    },
    {
        "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
        "authors": [
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hanjun Dai",
            "Sercan O. Arik",
            "Tomas Pfister"
        ],
        "published": "2023-05-23T14:27:16Z",
        "summary": "Modern large language models (LLMs) have demonstrated impressive capabilities\nat sophisticated tasks, often through step-by-step reasoning similar to humans.\nThis is made possible by their strong few and zero-shot abilities -- they can\neffectively learn from a handful of handcrafted, completed responses\n(\"in-context examples\"), or are prompted to reason spontaneously through\nspecially designed triggers. Nonetheless, some limitations have been observed.\nFirst, performance in the few-shot setting is sensitive to the choice of\nexamples, whose design requires significant human effort. Moreover, given the\ndiverse downstream tasks of LLMs, it may be difficult or laborious to handcraft\nper-task labels. Second, while the zero-shot setting does not require\nhandcrafting, its performance is limited due to the lack of guidance to the\nLLMs. To address these limitations, we propose Consistency-based Self-adaptive\nPrompting (COSP), a novel prompt design method for LLMs. Requiring neither\nhandcrafted responses nor ground-truth labels, COSP selects and builds the set\nof examples from the LLM zero-shot outputs via carefully designed criteria that\ncombine consistency, diversity and repetition. In the zero-shot setting for\nthree different LLMs, we show that using only LLM predictions, COSP improves\nperformance up to 15% compared to zero-shot baselines and matches or exceeds\nfew-shot baselines for a range of reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.14106v1.pdf"
    },
    {
        "title": "Revisiting Acceptability Judgements",
        "authors": [
            "Hai Hu",
            "Ziyin Zhang",
            "Weifang Huang",
            "Jackie Yan-Ki Lai",
            "Aini Li",
            "Yina Patterson",
            "Jiahui Huang",
            "Peng Zhang",
            "Chien-Jer Charles Lin",
            "Rui Wang"
        ],
        "published": "2023-05-23T14:16:22Z",
        "summary": "In this work, we revisit linguistic acceptability in the context of large\nlanguage models. We introduce CoLAC - Corpus of Linguistic Acceptability in\nChinese, the first large-scale acceptability dataset for a non-Indo-European\nlanguage. It is verified by native speakers and is the first acceptability\ndataset that comes with two sets of labels: a linguist label and a crowd label.\nOur experiments show that even the largest InstructGPT model performs only at\nchance level on CoLAC, while ChatGPT's performance (48.30 MCC) is also much\nbelow supervised models (59.03 MCC) and human (65.11 MCC). Through\ncross-lingual transfer experiments and fine-grained linguistic analysis, we\nprovide detailed analysis of the model predictions and demonstrate for the\nfirst time that knowledge of linguistic acceptability can be transferred across\ntypologically distinct languages, as well as be traced back to pre-training.\nOur dataset is publicly available at\n\\url{https://github.com/huhailinguist/CoLAC}.",
        "pdf_link": "https://arxiv.org/pdf/2305.14091v3.pdf"
    },
    {
        "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
        "authors": [
            "Shiqi Chen",
            "Siyang Gao",
            "Junxian He"
        ],
        "published": "2023-05-23T13:48:32Z",
        "summary": "Detecting factual errors in summaries has been an important and challenging\nsubject in summarization research. Inspired by the emergent ability of large\nlanguage models (LLMs), we explore evaluating factual consistency of summaries\nby directly prompting LLMs. We present a comprehensive empirical study to\nassess the ability of LLMs as factual consistency evaluators, which consists of\n(1) analyzing different LLMs such as the GPT model series and Flan-T5; (2)\ninvestigating a variety of prompting methods including vanilla prompting,\nchain-of-thought prompting, and a sentence-by-sentence prompting method to\ntackle long summaries; and (3) evaluating on diverse summaries generated by\nmultiple summarization systems, ranging from pre-transformer methods to SOTA\npretrained models. Our experiments demonstrate that prompting LLMs is able to\noutperform the previous best factuality systems in all settings, by up to 12.2\nabsolute points in terms of the binary classification accuracy on inconsistency\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2305.14069v2.pdf"
    },
    {
        "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
        "authors": [
            "Seungone Kim",
            "Se June Joo",
            "Doyoung Kim",
            "Joel Jang",
            "Seonghyeon Ye",
            "Jamin Shin",
            "Minjoon Seo"
        ],
        "published": "2023-05-23T13:14:59Z",
        "summary": "Language models (LMs) with less than 100B parameters are known to perform\npoorly on chain-of-thought (CoT) reasoning in contrast to large LMs when\nsolving unseen tasks. In this work, we aim to equip smaller LMs with the\nstep-by-step reasoning capability by instruction tuning with CoT rationales. In\norder to achieve this goal, we first introduce a new instruction-tuning dataset\ncalled the CoT Collection, which augments the existing Flan Collection\n(including only 9 CoT tasks) with additional 1.84 million rationales across\n1,060 tasks. We show that CoT fine-tuning Flan-T5 (3B & 11B) with CoT\nCollection enables smaller LMs to have better CoT capabilities on unseen tasks.\nOn the BIG-Bench-Hard (BBH) benchmark, we report an average improvement of\n+4.34% (Flan-T5 3B) and +2.60% (Flan-T5 11B), in terms of zero-shot task\naccuracy. Furthermore, we show that instruction tuning with CoT Collection\nallows LMs to possess stronger few-shot learning capabilities on 4\ndomain-specific tasks, resulting in an improvement of +2.24% (Flan-T5 3B) and\n+2.37% (Flan-T5 11B), even outperforming ChatGPT utilizing demonstrations until\nthe max length by a +13.98% margin. Our code, the CoT Collection data, and\nmodel checkpoints are publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.14045v2.pdf"
    },
    {
        "title": "ChipGPT: How far are we from natural language hardware design",
        "authors": [
            "Kaiyan Chang",
            "Ying Wang",
            "Haimeng Ren",
            "Mengdi Wang",
            "Shengwen Liang",
            "Yinhe Han",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "published": "2023-05-23T12:54:02Z",
        "summary": "As large language models (LLMs) like ChatGPT exhibited unprecedented machine\nintelligence, it also shows great performance in assisting hardware engineers\nto realize higher-efficiency logic design via natural language interaction. To\nestimate the potential of the hardware design process assisted by LLMs, this\nwork attempts to demonstrate an automated design environment that explores LLMs\nto generate hardware logic designs from natural language specifications. To\nrealize a more accessible and efficient chip development flow, we present a\nscalable four-stage zero-code logic design framework based on LLMs without\nretraining or finetuning. At first, the demo, ChipGPT, begins by generating\nprompts for the LLM, which then produces initial Verilog programs. Second, an\noutput manager corrects and optimizes these programs before collecting them\ninto the final design space. Eventually, ChipGPT will search through this space\nto select the optimal design under the target metrics. The evaluation sheds\nsome light on whether LLMs can generate correct and complete hardware logic\ndesigns described by natural language for some specifications. It is shown that\nChipGPT improves programmability, and controllability, and shows broader design\noptimization space compared to prior work and native LLMs alone.",
        "pdf_link": "https://arxiv.org/pdf/2305.14019v3.pdf"
    },
    {
        "title": "IfQA: A Dataset for Open-domain Question Answering under Counterfactual Presuppositions",
        "authors": [
            "Wenhao Yu",
            "Meng Jiang",
            "Peter Clark",
            "Ashish Sabharwal"
        ],
        "published": "2023-05-23T12:43:19Z",
        "summary": "Although counterfactual reasoning is a fundamental aspect of intelligence,\nthe lack of large-scale counterfactual open-domain question-answering (QA)\nbenchmarks makes it difficult to evaluate and improve models on this ability.\nTo address this void, we introduce the first such dataset, named IfQA, where\neach question is based on a counterfactual presupposition via an \"if\" clause.\nFor example, if Los Angeles was on the east coast of the U.S., what would be\nthe time difference between Los Angeles and Paris? Such questions require\nmodels to go beyond retrieving direct factual knowledge from the Web: they must\nidentify the right information to retrieve and reason about an imagined\nsituation that may even go against the facts built into their parameters. The\nIfQA dataset contains over 3,800 questions that were annotated annotated by\ncrowdworkers on relevant Wikipedia passages. Empirical analysis reveals that\nthe IfQA dataset is highly challenging for existing open-domain QA methods,\nincluding supervised retrieve-then-read pipeline methods (EM score 36.2), as\nwell as recent few-shot approaches such as chain-of-thought prompting with\nGPT-3 (EM score 27.4). The unique challenges posed by the IfQA benchmark will\npush open-domain QA research on both retrieval and counterfactual reasoning\nfronts.",
        "pdf_link": "https://arxiv.org/pdf/2305.14010v1.pdf"
    },
    {
        "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
        "authors": [
            "Wenhao Yu",
            "Zhihan Zhang",
            "Zhenwen Liang",
            "Meng Jiang",
            "Ashish Sabharwal"
        ],
        "published": "2023-05-23T12:29:44Z",
        "summary": "Large language models (LLMs) exhibit remarkable performance across various\nNLP tasks. However, they often generate incorrect or hallucinated information,\nwhich hinders their practical applicability in real-world scenarios. Human\nfeedback has been shown to effectively enhance the factuality and quality of\ngenerated content, addressing some of these limitations. However, this approach\nis resource-intensive, involving manual input and supervision, which can be\ntime-consuming and expensive. Moreover, it cannot be provided during inference,\nfurther limiting its practical utility in dynamic and interactive applications.\nIn this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs\nby providing automatic retrieval feedback in a plug-and-play framework without\nthe need for expensive fine-tuning. ReFeed first generates initial outputs,\nthen utilizes a retrieval model to acquire relevant information from large\ndocument collections, and finally incorporates the retrieved information into\nthe in-context demonstration for output refinement, thereby addressing the\nlimitations of LLMs in a more efficient and cost-effective manner. Experiments\non four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed\ncould improve over +6.0% under zero-shot setting and +2.5% under few-shot\nsetting, compared to baselines without using retrieval feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.14002v1.pdf"
    },
    {
        "title": "Make a Choice! Knowledge Base Question Answering with In-Context Learning",
        "authors": [
            "Chuanyuan Tan",
            "Yuehe Chen",
            "Wenbiao Shao",
            "Wenliang Chen"
        ],
        "published": "2023-05-23T11:56:03Z",
        "summary": "Question answering over knowledge bases (KBQA) aims to answer factoid\nquestions with a given knowledge base (KB). Due to the large scale of KB,\nannotated data is impossible to cover all fact schemas in KB, which poses a\nchallenge to the generalization ability of methods that require a sufficient\namount of annotated data. Recently, LLMs have shown strong few-shot performance\nin many NLP tasks. We expect LLM can help existing methods improve their\ngeneralization ability, especially in low-resource situations. In this paper,\nwe present McL-KBQA, a framework that incorporates the few-shot ability of LLM\ninto the KBQA method via ICL-based multiple choice and then improves the\neffectiveness of the QA tasks. Experimental results on two KBQA datasets\ndemonstrate the competitive performance of McL-KBQA with strong improvements in\ngeneralization. We expect to explore a new way to QA tasks from KBQA in\nconjunction with LLM, how to generate answers normatively and correctly with\nstrong generalization.",
        "pdf_link": "https://arxiv.org/pdf/2305.13972v1.pdf"
    },
    {
        "title": "Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning",
        "authors": [
            "Saibo Geng",
            "Martin Josifoski",
            "Maxime Peyrard",
            "Robert West"
        ],
        "published": "2023-05-23T11:54:37Z",
        "summary": "Despite their impressive performance, large language models (LMs) still\nstruggle with reliably generating complex output structures when not finetuned\nto follow the required output format exactly. To address this issue,\ngrammar-constrained decoding (GCD) can be used to control the generation of\nLMs, guaranteeing that the output follows a given structure. Most existing GCD\nmethods are, however, limited to specific tasks, such as parsing or code\ngeneration. In this work, we demonstrate that formal grammars can describe the\noutput space for a much wider range of tasks and argue that GCD can serve as a\nunified framework for structured NLP tasks in general. For increased\nflexibility, we introduce input-dependent grammars, which allow the grammar to\ndepend on the input and thus enable the generation of different output\nstructures for different inputs. We then empirically demonstrate the power and\nflexibility of GCD-enhanced LMs on (1) information extraction, (2) entity\ndisambiguation, and (3) constituency parsing. Our results indicate that\ngrammar-constrained LMs substantially outperform unconstrained LMs or even beat\ntask-specific finetuned models. Grammar constraints thus hold great promise for\nharnessing off-the-shelf LMs for a wide range of structured NLP tasks,\nespecially where training data is scarce or finetuning is expensive. Code and\ndata: https://github.com/epfl-dlab/GCD.",
        "pdf_link": "https://arxiv.org/pdf/2305.13971v6.pdf"
    },
    {
        "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
        "authors": [
            "Moxin Li",
            "Wenjie Wang",
            "Fuli Feng",
            "Yixin Cao",
            "Jizhi Zhang",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-23T11:30:43Z",
        "summary": "Large Language Model (LLM) has demonstrated significant ability in various\nNatural Language Processing tasks. However, their effectiveness is highly\ndependent on the phrasing of the task prompt, leading to research on automatic\nprompt optimization using labeled task data. We reveal that these prompt\noptimization techniques are vulnerable to distribution shifts such as\nsubpopulation shifts, which are common for LLMs in real-world scenarios such as\ncustomer reviews analysis. In this light, we propose a new problem of robust\nprompt optimization for LLMs against distribution shifts, which requires the\nprompt optimized over the labeled source group can simultaneously generalize to\nan unlabeled target group. To solve this problem, we propose Generalized Prompt\nOptimization framework, which incorporates the unlabeled data from the target\ngroup into prompt optimization. Extensive experimental results demonstrate the\neffectiveness of the proposed framework with significant performance\nimprovement on the target group and comparable performance on the source group.",
        "pdf_link": "https://arxiv.org/pdf/2305.13954v3.pdf"
    },
    {
        "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
        "authors": [
            "Jinyan Su",
            "Terry Yue Zhuo",
            "Di Wang",
            "Preslav Nakov"
        ],
        "published": "2023-05-23T11:18:30Z",
        "summary": "With the rapid progress of large language models (LLMs) and the huge amount\nof text they generated, it becomes more and more impractical to manually\ndistinguish whether a text is machine-generated. Given the growing use of LLMs\nin social media and education, it prompts us to develop methods to detect\nmachine-generated text, preventing malicious usage such as plagiarism,\nmisinformation, and propaganda. Previous work has studied several zero-shot\nmethods, which require no training data. These methods achieve good\nperformance, but there is still a lot of room for improvement. In this paper,\nwe introduce two novel zero-shot methods for detecting machine-generated text\nby leveraging the log rank information. One is called DetectLLM-LRR, which is\nfast and efficient, and the other is called DetectLLM-NPR, which is more\naccurate, but slower due to the need for perturbations. Our experiments on\nthree datasets and seven language models show that our proposed methods improve\nover the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover,\nDetectLLM-NPR needs fewer perturbations than previous work to achieve the same\nlevel of performance, which makes it more practical for real-world use. We also\ninvestigate the efficiency--performance trade-off based on users preference on\nthese two measures and we provide intuition for using them in practice\neffectively. We release the data and the code of both methods in\nhttps://github.com/mbzuai-nlp/DetectLLM",
        "pdf_link": "https://arxiv.org/pdf/2306.05540v1.pdf"
    },
    {
        "title": "Generating Data for Symbolic Language with Large Language Models",
        "authors": [
            "Jiacheng Ye",
            "Chengzu Li",
            "Lingpeng Kong",
            "Tao Yu"
        ],
        "published": "2023-05-23T10:44:00Z",
        "summary": "While large language models (LLMs) bring not only performance but also\ncomplexity, recent work has started to turn LLMs into data generators rather\nthan task inferencers, where another affordable task model is trained for\nefficient deployment and inference. However, such an approach has primarily\nbeen applied to natural language tasks and has not yet been explored for\nsymbolic language tasks with complex structured outputs (e.g., semantic parsing\nand code generation). In this paper, we propose SymGen which utilizes LLMs for\ngenerating various annotation-expensive symbolic language data. SymGen consists\nof an informative prompt to steer generation and an agreement-based verifier to\nimprove data correctness. We conduct extensive experiments on six symbolic\nlanguage tasks across various settings. Compared with the LLMs, we demonstrate\nthe 1\\%-sized task model can achieve comparable or better performance, largely\ncutting inference and deployment costs. We also show that generated data with\nonly a few human demonstrations can be as effective as over 10 times the amount\nof human-annotated data when training the task model, saving a considerable\namount of annotation effort. SymGen sheds new light on data generation for\ncomplex tasks, and we release the code at\n\\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.",
        "pdf_link": "https://arxiv.org/pdf/2305.13917v1.pdf"
    },
    {
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning",
        "authors": [
            "Xuekai Zhu",
            "Biqing Qi",
            "Kaiyan Zhang",
            "Xinwei Long",
            "Zhouhan Lin",
            "Bowen Zhou"
        ],
        "published": "2023-05-23T10:11:56Z",
        "summary": "While large language models (LLMs) excel in various natural language\nprocessing tasks, their huge size and the inaccessibility of parameters present\nchallenges for practical deployment. Previous studies try to distill\ntask-specific ability from LLMs to smaller models, using data synthesis and\nchain-of-thought (CoT) fine-tuning. However, synthetic CoT data often contains\nfaulty reasoning, which deteriorates the quality of distillation, especially in\nreasoning capabilities. In this work, we propose Program-aided Distillation\n(PaD), which introduces reasoning programs to suppress the errors in distilled\ndata, and thus achieves better distillation quality for reasoning tasks. In\nPaD, we utilize the reasoning program to substitute the CoT, allowing automated\nerror checking of synthetic data. Further, through error injecting and further\ntraining, the small distilling model could iteratively self-refine the\nreasoning. Moreover, we conduct a step-wise beam search by step-by-step\nverifying to acquire more exact reasoning chains. We evaluate PaD on arithmetic\nreasoning, symbolic reasoning, and general ability. Experimental results\ndemonstrate that smaller models using PaD can not only outperform certain\nLLMs~(e.g., LLaMA-1 13B) but also achieve strong improvement over baselines\nwith a significantly smaller scale of parameters and data. The source code is\npublicly available at https://github.com/Xuekai-Zhu/pad.",
        "pdf_link": "https://arxiv.org/pdf/2305.13888v2.pdf"
    },
    {
        "title": "A Trip Towards Fairness: Bias and De-Biasing in Large Language Models",
        "authors": [
            "Leonardo Ranaldi",
            "Elena Sofia Ruzzetti",
            "Davide Venditti",
            "Dario Onorati",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2023-05-23T09:35:37Z",
        "summary": "Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training\nare emerging as the next big revolution in natural language processing and\nunderstanding. These CtB-LLMs are democratizing access to trainable Very\nLarge-Language Models (VLLMs) and, thus, may represent the building blocks of\nmany NLP systems solving downstream tasks. Hence, a little or a large bias in\nCtB-LLMs may cause huge harm. In this paper, we performed a large investigation\nof the bias of three families of CtB-LLMs, and we showed that debiasing\ntechniques are effective and usable. Indeed, according to current tests, the\nLLaMA and the OPT families have an important bias in gender, race, religion,\nand profession. In contrast to the analysis for other LLMs, we discovered that\nbias depends not on the number of parameters but on the perplexity. Finally,\nthe debiasing of OPT using LoRA reduces bias up to 4.12 points in the\nnormalized stereotype score.",
        "pdf_link": "https://arxiv.org/pdf/2305.13862v2.pdf"
    },
    {
        "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
        "authors": [
            "Yi Liu",
            "Gelei Deng",
            "Zhengzi Xu",
            "Yuekang Li",
            "Yaowen Zheng",
            "Ying Zhang",
            "Lida Zhao",
            "Tianwei Zhang",
            "Kailong Wang",
            "Yang Liu"
        ],
        "published": "2023-05-23T09:33:38Z",
        "summary": "Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.",
        "pdf_link": "https://arxiv.org/pdf/2305.13860v2.pdf"
    },
    {
        "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
        "authors": [
            "Danqing Wang",
            "Lei Li"
        ],
        "published": "2023-05-23T08:51:08Z",
        "summary": "Large language models (LLMs) have demonstrated their potential to refine\ntheir generation based on their own feedback. However, the feedback from LLM\nitself is often inaccurate, thereby limiting its benefits. In this paper, we\npropose Study Assistant for Large LAnguage Model (SALAM), a novel framework\nwith an auxiliary agent to assist the main LLM in learning from mistakes\nthrough interactive cooperation. In the gathering phase, the student assistant\nagent probes the main LLM, analyzes its errors, and collects the interaction in\na mistake memory. During the examination phase, the study assistant provides\nguidelines by retrieving relevant cases to help the main LLM anticipate and\navoid similar errors. We first investigate the effectiveness of a general study\nassistant and then customize it to provide LLM-specific guidance through\nimitation learning from successful guidance experiences. Our experiments on\nthree LLMs using two challenging frameworks demonstrate that SALAM can\nsignificantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on\nBBQ.",
        "pdf_link": "https://arxiv.org/pdf/2305.13829v3.pdf"
    },
    {
        "title": "\"Is the Pope Catholic?\" Applying Chain-of-Thought Reasoning to Understanding Conversational Implicatures",
        "authors": [
            "Zae Myung Kim",
            "David E. Taylor",
            "Dongyeop Kang"
        ],
        "published": "2023-05-23T08:49:50Z",
        "summary": "Conversational implicatures are pragmatic inferences that require listeners\nto deduce the intended meaning conveyed by a speaker from their explicit\nutterances. Although such inferential reasoning is fundamental to human\ncommunication, recent research indicates that large language models struggle to\ncomprehend these implicatures as effectively as the average human. This paper\ndemonstrates that by incorporating Grice's Four Maxims into the model through\nchain-of-thought prompting, we can significantly enhance its performance,\nsurpassing even the average human performance on this task.",
        "pdf_link": "https://arxiv.org/pdf/2305.13826v1.pdf"
    },
    {
        "title": "Can Large Language Models Capture Dissenting Human Voices?",
        "authors": [
            "Noah Lee",
            "Na Min An",
            "James Thorne"
        ],
        "published": "2023-05-23T07:55:34Z",
        "summary": "Large language models (LLMs) have shown impressive achievements in solving a\nbroad range of tasks. Augmented by instruction fine-tuning, LLMs have also been\nshown to generalize in zero-shot settings as well. However, whether LLMs\nclosely align with the human disagreement distribution has not been\nwell-studied, especially within the scope of natural language inference (NLI).\nIn this paper, we evaluate the performance and alignment of LLM distribution\nwith humans using two different techniques to estimate the multinomial\ndistribution: Monte Carlo Estimation (MCE) and Log Probability Estimation\n(LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks\nand simultaneously fail to capture human disagreement distribution. The\ninference and human alignment performances plunge even further on data samples\nwith high human disagreement levels, raising concerns about their natural\nlanguage understanding (NLU) ability and their representativeness to a larger\nhuman population. The source code for the experiments is available at\nhttps://github.com/xfactlab/emnlp2023-LLM-Disagreement",
        "pdf_link": "https://arxiv.org/pdf/2305.13788v2.pdf"
    },
    {
        "title": "Enhancing Black-Box Few-Shot Text Classification with Prompt-Based Data Augmentation",
        "authors": [
            "Danqing Luo",
            "Chen Zhang",
            "Jiahui Xu",
            "Bin Wang",
            "Yiming Chen",
            "Yan Zhang",
            "Haizhou Li"
        ],
        "published": "2023-05-23T07:54:34Z",
        "summary": "Training or finetuning large-scale language models (LLMs) such as GPT-3\nrequires substantial computation resources, motivating recent efforts to\nexplore parameter-efficient adaptation to downstream tasks. One practical area\nof research is to treat these models as black boxes and interact with them\nthrough their inference APIs. In this paper, we investigate how to optimize\nfew-shot text classification without accessing the gradients of the LLMs. To\nachieve this, we treat the black-box model as a feature extractor and train a\nclassifier with the augmented text data. Data augmentation is performed using\nprompt-based finetuning on an auxiliary language model with a much smaller\nparameter size than the black-box model. Through extensive experiments on eight\ntext classification datasets, we show that our approach, dubbed BT-Classifier,\nsignificantly outperforms state-of-the-art black-box few-shot learners and\nperforms on par with methods that rely on full-model tuning.",
        "pdf_link": "https://arxiv.org/pdf/2305.13785v2.pdf"
    },
    {
        "title": "Aligning Large Language Models through Synthetic Feedback",
        "authors": [
            "Sungdong Kim",
            "Sanghwan Bae",
            "Jamin Shin",
            "Soyoung Kang",
            "Donghyun Kwak",
            "Kang Min Yoo",
            "Minjoon Seo"
        ],
        "published": "2023-05-23T06:41:16Z",
        "summary": "Aligning large language models (LLMs) to human values has become increasingly\nimportant as it enables sophisticated steering of LLMs. However, it requires\nsignificant human demonstrations and feedback or distillation from proprietary\nLLMs such as ChatGPT. In this work, we propose a novel alignment learning\nframework with synthetic feedback not dependent on extensive human annotations\nand proprietary LLMs. First, we perform reward modeling (RM) with synthetic\nfeedback by contrasting responses from vanilla LLMs with various sizes and\nprompts. Then, we use the RM to simulate high-quality demonstrations to train a\nsupervised policy and further optimize the model with reinforcement learning.\nOur resulting model, Aligned Language Model with Synthetic Training dataset\n(ALMoST), outperforms recent open-sourced models, which are trained on the\noutputs of InstructGPT or human-annotated demonstrations, in alignment\nbenchmarks. In human evaluation, our model is preferred to Alpaca and Dolly-v2,\n55.0% and 58.5% of the time, respectively. Further analyses demonstrate the\nefficacy and importance of synthetic feedback in our framework. The code is\navailable at https://github.com/naver-ai/almost",
        "pdf_link": "https://arxiv.org/pdf/2305.13735v2.pdf"
    },
    {
        "title": "Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting",
        "authors": [
            "Rui Wang",
            "Hongru Wang",
            "Fei Mi",
            "Yi Chen",
            "Boyang Xue",
            "Kam-Fai Wong",
            "Ruifeng Xu"
        ],
        "published": "2023-05-23T06:38:20Z",
        "summary": "Numerous works are proposed to align large language models (LLMs) with human\nintents to better fulfill instructions, ensuring they are trustful and helpful.\nNevertheless, some human instructions are often malicious or misleading and\nfollowing them will lead to untruthful and unsafe responses. Previous work\nrarely focused on understanding how LLMs manage instructions based on\ncounterfactual premises, referred to here as \\textit{inductive instructions},\nwhich may stem from users' false beliefs or malicious intents. In this paper,\nwe aim to reveal the behaviors of LLMs towards \\textit{inductive instructions}\nand enhance their truthfulness and helpfulness accordingly. Specifically, we\nfirst introduce a benchmark of \\underline{\\textbf{Indu}}ctive\n{In\\underline{\\textbf{st}}ruct}ions (\\textsc{\\textbf{INDust}}), where the false\nknowledge is incorporated into instructions in multiple different styles. After\nextensive human and automatic evaluations, we uncovered a universal\nvulnerability among LLMs in processing inductive instructions. Additionally, we\nidentified that different inductive styles affect the models' ability to\nidentify the same underlying errors, and the complexity of the underlying\nassumptions also influences the model's performance. Motivated by these\nresults, we propose \\textsc{Dual-critique} prompting to improve LLM robustness\nagainst inductive instructions. Our experiments demonstrate that\n\\textsc{Dual-critique} prompting significantly bolsters the robustness of a\ndiverse array of LLMs, even when confronted with varying degrees of inductive\ninstruction complexity and differing inductive styles.",
        "pdf_link": "https://arxiv.org/pdf/2305.13733v2.pdf"
    },
    {
        "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
        "authors": [
            "Sukmin Cho",
            "Soyeong Jeong",
            "Jeongyeon Seo",
            "Jong C. Park"
        ],
        "published": "2023-05-23T06:35:33Z",
        "summary": "Re-rankers, which order retrieved documents with respect to the relevance\nscore on the given query, have gained attention for the information retrieval\n(IR) task. Rather than fine-tuning the pre-trained language model (PLM), the\nlarge-scale language model (LLM) is utilized as a zero-shot re-ranker with\nexcellent results. While LLM is highly dependent on the prompts, the impact and\nthe optimization of the prompts for the zero-shot re-ranker are not explored\nyet. Along with highlighting the impact of optimization on the zero-shot\nre-ranker, we propose a novel discrete prompt optimization method, Constrained\nPrompt generation (Co-Prompt), with the metric estimating the optimum for\nre-ranking. Co-Prompt guides the generated texts from PLM toward optimal\nprompts based on the metric without parameter update. The experimental results\ndemonstrate that Co-Prompt leads to outstanding re-ranking performance against\nthe baselines. Also, Co-Prompt generates more interpretable prompts for humans\nagainst other prompt optimization methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.13729v1.pdf"
    },
    {
        "title": "Exploring Self-supervised Logic-enhanced Training for Large Language Models",
        "authors": [
            "Fangkai Jiao",
            "Zhiyang Teng",
            "Bosheng Ding",
            "Zhengyuan Liu",
            "Nancy F. Chen",
            "Shafiq Joty"
        ],
        "published": "2023-05-23T06:13:10Z",
        "summary": "Existing efforts to improve logical reasoning ability of language models have\npredominantly relied on supervised fine-tuning, hindering generalization to new\ndomains and/or tasks. The development of Large Langauge Models (LLMs) has\ndemonstrated the capacity of compressing abundant knowledge into a single\nproxy, enabling them to tackle multiple tasks effectively. Our preliminary\nexperiments, nevertheless, show that LLMs do not show capability on logical\nreasoning. The performance of LLMs on logical reasoning benchmarks is far\nbehind the existing state-of-the-art baselines. In this paper, we make the\nfirst attempt to investigate the feasibility of incorporating logical knowledge\nthrough self-supervised post-training, and activating it via in-context\nlearning, which we termed as LogicLLM. Specifically, we devise an\nauto-regressive objective variant of MERIt and integrate it with two LLM\nseries, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to\n13 billion. The results on two challenging logical reasoning benchmarks\ndemonstrate the effectiveness of LogicLLM. Besides, we conduct extensive\nablation studies to analyze the key factors in designing logic-oriented proxy\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.13718v5.pdf"
    },
    {
        "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
        "authors": [
            "Alfonso Amayuelas",
            "Liangming Pan",
            "Wenhu Chen",
            "William Wang"
        ],
        "published": "2023-05-23T05:59:21Z",
        "summary": "This paper investigates the capabilities of Large Language Models (LLMs) in\nthe context of understanding their own knowledge and measuring their\nuncertainty. We argue this is an important feature for mitigating\nhallucinations. Specifically, we focus on addressing \\textit{known-unknown}\nquestions, characterized by high uncertainty due to the absence of definitive\nanswers. To facilitate our study, we collect a dataset with new Known-Unknown\nQuestions (KUQ) and propose a novel categorization scheme to elucidate the\nsources of uncertainty. Subsequently, we assess the LLMs' ability to\ndifferentiate between known and unknown questions and classify them\naccordingly. Moreover, we evaluate the quality of their answers in an\nOpen-Ended QA setting. To quantify the uncertainty expressed in the answers, we\ncreate a semantic evaluation method that measures the model's accuracy in\nexpressing uncertainty between known vs unknown questions.",
        "pdf_link": "https://arxiv.org/pdf/2305.13712v1.pdf"
    },
    {
        "title": "Few-Shot Data Synthesis for Open Domain Multi-Hop Question Answering",
        "authors": [
            "Mingda Chen",
            "Xilun Chen",
            "Wen-tau Yih"
        ],
        "published": "2023-05-23T04:57:31Z",
        "summary": "Few-shot learning for open domain multi-hop question answering typically\nrelies on the incontext learning capability of large language models (LLMs).\nWhile powerful, these LLMs usually contain tens or hundreds of billions of\nparameters, making them rather inefficient at inference time. To improve\nperformance of smaller language models, we propose a data synthesis framework\nfor multi-hop question answering that requires less than 10 human annotated\nquestion answer pairs. Our framework depends only on rich, naturally-occurring\nrelationships among documents and is built upon the data generation functions\nparameterized by LLMs and prompts. We synthesize millions of multi-hop\nquestions and claims to finetune language models, evaluated on popular\nbenchmarks for multi-hop question answering and fact verification. Empirically,\nour approach improves model performance significantly, allowing the finetuned\nmodels to be competitive with GPT-3.5 based approaches while being almost\none-third the size in parameter count.",
        "pdf_link": "https://arxiv.org/pdf/2305.13691v2.pdf"
    },
    {
        "title": "Error Detection for Text-to-SQL Semantic Parsing",
        "authors": [
            "Shijie Chen",
            "Ziru Chen",
            "Huan Sun",
            "Yu Su"
        ],
        "published": "2023-05-23T04:44:22Z",
        "summary": "Despite remarkable progress in text-to-SQL semantic parsing in recent years,\nthe performance of existing parsers is still far from perfect. Specifically,\nmodern text-to-SQL parsers based on deep learning are often over-confident,\nthus casting doubt on their trustworthiness when deployed for real use. In this\npaper, we propose a parser-independent error detection model for text-to-SQL\nsemantic parsing. Using a language model of code as its bedrock, we enhance our\nerror detection model with graph neural networks that learn structural features\nof both natural language questions and SQL queries. We train our model on\nrealistic parsing errors collected from a cross-domain setting, which leads to\nstronger generalization ability. Experiments with three strong text-to-SQL\nparsers featuring different decoding mechanisms show that our approach\noutperforms parser-dependent uncertainty metrics. Our model could also\neffectively improve the performance and usability of text-to-SQL semantic\nparsers regardless of their architectures. (Our implementation is available at\nhttps://github.com/OSU-NLP-Group/Text2SQL-Error-Detection)",
        "pdf_link": "https://arxiv.org/pdf/2305.13683v2.pdf"
    },
    {
        "title": "Polyglot or Not? Measuring Multilingual Encyclopedic Knowledge in Foundation Models",
        "authors": [
            "Tim Schott",
            "Daniel Furman",
            "Shreshta Bhat"
        ],
        "published": "2023-05-23T04:31:39Z",
        "summary": "In this work, we assess the ability of foundation models to recall\nencyclopedic knowledge across a wide range of linguistic contexts. To support\nthis, we: 1) produce a 20-language dataset that contains 303k factual\nassociations paired with counterfactuals, 2) evaluate 5 models in a\nmultilingual test, and 3) benchmark a diverse set of 24 models in an\nEnglish-only test. Meta's LLaMA achieves the highest scores in both\nmultilingual and English-only evaluations. Yet, an analysis of LLaMA's errors\nreveals significant limitations in its ability to recall facts in languages\nother than English, plus difficulties related to the location and gender of\nfact subjects. Overall, our findings suggest that today's foundation models are\nfar from polyglots.",
        "pdf_link": "https://arxiv.org/pdf/2305.13675v2.pdf"
    },
    {
        "title": "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models",
        "authors": [
            "Shuo Zhang",
            "Liangming Pan",
            "Junzhou Zhao",
            "William Yang Wang"
        ],
        "published": "2023-05-23T04:22:50Z",
        "summary": "Large language models often necessitate grounding on external knowledge to\ngenerate faithful and reliable answers. Yet even with the correct groundings in\nthe reference, they can ignore them and rely on wrong groundings or their\ninherent biases to hallucinate when users, being largely unaware of the\nspecifics of the stored information, pose questions that might not directly\ncorrelate with the retrieved groundings. In this work, we formulate this\nknowledge alignment problem and introduce MixAlign, a framework that interacts\nwith both the human user and the knowledge base to obtain and integrate\nclarifications on how the user question relates to the stored information.\nMixAlign employs a language model to achieve automatic knowledge alignment and,\nif necessary, further enhances this alignment through human user\nclarifications. Experimental results highlight the crucial role of knowledge\nalignment in boosting model performance and mitigating hallucination, with\nimprovements noted up to 22.2% and 27.1% respectively. We also demonstrate the\neffectiveness of MixAlign in improving knowledge alignment by producing\nhigh-quality, user-centered clarifications.",
        "pdf_link": "https://arxiv.org/pdf/2305.13669v2.pdf"
    },
    {
        "title": "On the Risk of Misinformation Pollution with Large Language Models",
        "authors": [
            "Yikang Pan",
            "Liangming Pan",
            "Wenhu Chen",
            "Preslav Nakov",
            "Min-Yen Kan",
            "William Yang Wang"
        ],
        "published": "2023-05-23T04:10:26Z",
        "summary": "In this paper, we comprehensively investigate the potential misuse of modern\nLarge Language Models (LLMs) for generating credible-sounding misinformation\nand its subsequent impact on information-intensive applications, particularly\nOpen-Domain Question Answering (ODQA) systems. We establish a threat model and\nsimulate potential misuse scenarios, both unintentional and intentional, to\nassess the extent to which LLMs can be utilized to produce misinformation. Our\nstudy reveals that LLMs can act as effective misinformation generators, leading\nto a significant degradation in the performance of ODQA systems. To mitigate\nthe harm caused by LLM-generated misinformation, we explore three defense\nstrategies: prompting, misinformation detection, and majority voting. While\ninitial results show promising trends for these defensive strategies, much more\nwork needs to be done to address the challenge of misinformation pollution. Our\nwork highlights the need for further research and interdisciplinary\ncollaboration to address LLM-generated misinformation and to promote\nresponsible use of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.13661v2.pdf"
    },
    {
        "title": "Prompt-Based Monte-Carlo Tree Search for Goal-Oriented Dialogue Policy Planning",
        "authors": [
            "Xiao Yu",
            "Maximillian Chen",
            "Zhou Yu"
        ],
        "published": "2023-05-23T04:07:03Z",
        "summary": "Planning for goal-oriented dialogue often requires simulating future dialogue\ninteractions and estimating task progress. Many approaches thus consider\ntraining neural networks to perform look-ahead search algorithms such as A*\nsearch and Monte Carlo Tree Search (MCTS). However, this training often\nrequires abundant annotated data, which creates challenges when faced with\nnoisy annotations or low-resource settings. We introduce GDP-Zero, an approach\nusing Open-Loop MCTS to perform goal-oriented dialogue policy planning without\nany model training. GDP-Zero prompts a large language model to act as a policy\nprior, value function, user simulator, and system model during the tree search.\nWe evaluate GDP-Zero on the goal-oriented task PersuasionForGood, and find that\nits responses are preferred over ChatGPT up to 59.32% of the time, and are\nrated more persuasive than ChatGPT during interactive evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2305.13660v2.pdf"
    },
    {
        "title": "ChatGPT as your Personal Data Scientist",
        "authors": [
            "Md Mahadi Hassan",
            "Alex Knipper",
            "Shubhra Kanti Karmaker Santu"
        ],
        "published": "2023-05-23T04:00:16Z",
        "summary": "The rise of big data has amplified the need for efficient, user-friendly\nautomated machine learning (AutoML) tools. However, the intricacy of\nunderstanding domain-specific data and defining prediction tasks necessitates\nhuman intervention making the process time-consuming while preventing full\nautomation. Instead, envision an intelligent agent capable of assisting users\nin conducting AutoML tasks through intuitive, natural conversations without\nrequiring in-depth knowledge of the underlying machine learning (ML) processes.\nThis agent's key challenge is to accurately comprehend the user's prediction\ngoals and, consequently, formulate precise ML tasks, adjust data sets and model\nparameters accordingly, and articulate results effectively. In this paper, we\ntake a pioneering step towards this ambitious goal by introducing a\nChatGPT-based conversational data-science framework to act as a \"personal data\nscientist\". Precisely, we utilize Large Language Models (ChatGPT) to build a\nnatural interface between the users and the ML models (Scikit-Learn), which in\nturn, allows us to approach this ambitious problem with a realistic solution.\n  Our model pivots around four dialogue states: Data Visualization, Task\nFormulation, Prediction Engineering, and Result Summary and Recommendation.\nEach state marks a unique conversation phase, impacting the overall user-system\ninteraction. Multiple LLM instances, serving as \"micro-agents\", ensure a\ncohesive conversation flow, granting us granular control over the\nconversation's progression. In summary, we developed an end-to-end system that\nnot only proves the viability of the novel concept of conversational data\nscience but also underscores the potency of LLMs in solving complex tasks.\nInterestingly, its development spotlighted several critical weaknesses in the\ncurrent LLMs (ChatGPT) and highlighted substantial opportunities for\nimprovement.",
        "pdf_link": "https://arxiv.org/pdf/2305.13657v1.pdf"
    },
    {
        "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
        "authors": [
            "Long Lian",
            "Boyi Li",
            "Adam Yala",
            "Trevor Darrell"
        ],
        "published": "2023-05-23T03:59:06Z",
        "summary": "Recent advancements in text-to-image diffusion models have yielded impressive\nresults in generating realistic and diverse images. However, these models still\nstruggle with complex prompts, such as those that involve numeracy and spatial\nreasoning. This work proposes to enhance prompt understanding capabilities in\ndiffusion models. Our method leverages a pretrained large language model (LLM)\nfor grounded generation in a novel two-stage process. In the first stage, the\nLLM generates a scene layout that comprises captioned bounding boxes from a\ngiven prompt describing the desired image. In the second stage, a novel\ncontroller guides an off-the-shelf diffusion model for layout-grounded image\ngeneration. Both stages utilize existing pretrained models without additional\nmodel parameter optimization. Our method significantly outperforms the base\ndiffusion model and several strong baselines in accurately generating images\naccording to prompts that require various capabilities, doubling the generation\naccuracy across four tasks on average. Furthermore, our method enables\ninstruction-based multi-round scene specification and can handle prompts in\nlanguages not supported by the underlying diffusion model. We anticipate that\nour method will unleash users' creativity by accurately following more complex\nprompts. Our code, demo, and benchmark are available at:\nhttps://llm-grounded-diffusion.github.io",
        "pdf_link": "https://arxiv.org/pdf/2305.13655v3.pdf"
    },
    {
        "title": "InstructAlign: High-and-Low Resource Language Alignment via Continual Crosslingual Instruction Tuning",
        "authors": [
            "Samuel Cahyawijaya",
            "Holy Lovenia",
            "Tiezheng Yu",
            "Willy Chung",
            "Pascale Fung"
        ],
        "published": "2023-05-23T02:51:34Z",
        "summary": "Large language models (LLMs) that are tuned with instructions have\ndemonstrated remarkable capabilities in various tasks and languages. However,\ntheir ability to generalize to underrepresented languages is limited due to the\nscarcity of available data. Additionally, directly adapting new languages to\ninstruction-tuned LLMs can result in catastrophic forgetting, which leads to\nthe loss of multitasking ability. To address this issue, we propose\nInstructAlign which uses continual crosslingual instruction tuning to enable\nLLMs to align new unseen languages with previously learned high-resource\nlanguages. Our results demonstrate the effectiveness of InstructAlign in\nenabling the model to understand low-resource languages with limited parallel\ndata while preventing catastrophic forgetting. Our work contributes to the\nadvancement of language adaptation methods, particularly for adapting\ninstruction-tuned LLMs to underrepresented languages. Our code is released on\nhttps://github.com/HLTCHKUST/InstructAlign",
        "pdf_link": "https://arxiv.org/pdf/2305.13627v2.pdf"
    },
    {
        "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
        "authors": [
            "Yang Deng",
            "Lizi Liao",
            "Liang Chen",
            "Hongru Wang",
            "Wenqiang Lei",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-23T02:49:35Z",
        "summary": "Conversational systems based on Large Language Models (LLMs), such as\nChatGPT, show exceptional proficiency in context understanding and response\ngeneration. However, despite their impressive capabilities, they still possess\nlimitations, such as providing randomly-guessed answers to ambiguous queries or\nfailing to refuse users' requests, both of which are considered aspects of a\nconversational agent's proactivity. This raises the question of whether\nLLM-based conversational systems are equipped to handle proactive dialogue\nproblems. In this work, we conduct a comprehensive analysis of LLM-based\nconversational systems, specifically focusing on three aspects of proactive\ndialogue systems: clarification, target-guided, and non-collaborative\ndialogues. To trigger the proactivity of LLMs, we propose the Proactive\nChain-of-Thought prompting scheme, which augments LLMs with the goal planning\ncapability over descriptive reasoning chains. Empirical findings are discussed\nto promote future studies on LLM-based proactive dialogue systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.13626v2.pdf"
    },
    {
        "title": "Understanding Programs by Exploiting (Fuzzing) Test Cases",
        "authors": [
            "Jianyu Zhao",
            "Yuyang Rong",
            "Yiwen Guo",
            "Yifeng He",
            "Hao Chen"
        ],
        "published": "2023-05-23T01:51:46Z",
        "summary": "Semantic understanding of programs has attracted great attention in the\ncommunity. Inspired by recent successes of large language models (LLMs) in\nnatural language understanding, tremendous progress has been made by treating\nprogramming language as another sort of natural language and training LLMs on\ncorpora of program code. However, programs are essentially different from texts\nafter all, in a sense that they are normally heavily structured and\nsyntax-strict. In particular, programs and their basic units (i.e., functions\nand subroutines) are designed to demonstrate a variety of behaviors and/or\nprovide possible outputs, given different inputs. The relationship between\ninputs and possible outputs/behaviors represents the functions/subroutines and\nprofiles the program as a whole. Therefore, we propose to incorporate such a\nrelationship into learning, for achieving a deeper semantic understanding of\nprograms. To obtain inputs that are representative enough to trigger the\nexecution of most part of the code, we resort to fuzz testing and propose fuzz\ntuning to boost the performance of program understanding and code\nrepresentation learning, given a pre-trained LLM. The effectiveness of the\nproposed method is verified on two program understanding tasks including code\nclone detection and code classification, and it outperforms current\nstate-of-the-arts by large margins. Code is available at\nhttps://github.com/rabbitjy/FuzzTuning.",
        "pdf_link": "https://arxiv.org/pdf/2305.13592v2.pdf"
    },
    {
        "title": "Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?",
        "authors": [
            "Aaron Chan",
            "Anant Kharkar",
            "Roshanak Zilouchian Moghaddam",
            "Yevhen Mohylevskyy",
            "Alec Helyar",
            "Eslam Kamal",
            "Mohamed Elkamhawy",
            "Neel Sundaresan"
        ],
        "published": "2023-05-23T01:21:55Z",
        "summary": "Software vulnerabilities bear enterprises significant costs. Despite\nextensive efforts in research and development of software vulnerability\ndetection methods, uncaught vulnerabilities continue to put software owners and\nusers at risk. Many current vulnerability detection methods require that code\nsnippets can compile and build before attempting detection. This,\nunfortunately, introduces a long latency between the time a vulnerability is\ninjected to the time it is removed, which can substantially increases the cost\nof fixing a vulnerability. We recognize that the current advances in machine\nlearning can be used to detect vulnerable code patterns on syntactically\nincomplete code snippets as the developer is writing the code at EditTime. In\nthis paper we present a practical system that leverages deep learning on a\nlarge-scale data set of vulnerable code patterns to learn complex\nmanifestations of more than 250 vulnerability types and detect vulnerable code\npatterns at EditTime. We discuss zero-shot, few-shot, and fine-tuning\napproaches on state of the art pre-trained Large Language Models (LLMs). We\nshow that in comparison with state of the art vulnerability detection models\nour approach improves the state of the art by 10%. We also evaluate our\napproach to detect vulnerability in auto-generated code by code LLMs.\nEvaluation on a benchmark of high-risk code scenarios shows a reduction of up\nto 90% vulnerability reduction.",
        "pdf_link": "https://arxiv.org/pdf/2306.01754v1.pdf"
    },
    {
        "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
        "authors": [
            "Binfeng Xu",
            "Zhiyuan Peng",
            "Bowen Lei",
            "Subhabrata Mukherjee",
            "Yuchen Liu",
            "Dongkuan Xu"
        ],
        "published": "2023-05-23T00:16:48Z",
        "summary": "Augmented Language Models (ALMs) blend the reasoning capabilities of Large\nLanguage Models (LLMs) with tools that allow for knowledge retrieval and action\nexecution. Existing ALM systems trigger LLM thought processes while pulling\nobservations from these tools in an interleaved fashion. Specifically, an LLM\nreasons to call an external tool, gets halted to fetch the tool's response, and\nthen decides the next action based on all preceding response tokens. Such a\nparadigm, though straightforward and easy to implement, often leads to huge\ncomputation complexity from redundant prompts and repeated execution. This\nstudy addresses such challenges for the first time, proposing a modular\nparadigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning\nprocess from external observations, thus significantly reducing token\nconsumption. Comprehensive evaluations across six public NLP benchmarks and a\ncurated dataset reveal consistent performance enhancements with our proposed\nmethodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy\nimprovement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO\ndemonstrates robustness under tool-failure scenarios. Beyond prompt efficiency,\ndecoupling parametric modules from non-parametric tool calls enables\ninstruction fine-tuning to offload LLMs into smaller language models, thus\nsubstantially reducing model parameters. Our illustrative work offloads\nreasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant\npotential for truly efficient and scalable ALM systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.18323v1.pdf"
    },
    {
        "title": "How Language Model Hallucinations Can Snowball",
        "authors": [
            "Muru Zhang",
            "Ofir Press",
            "William Merrill",
            "Alisa Liu",
            "Noah A. Smith"
        ],
        "published": "2023-05-22T23:14:28Z",
        "summary": "A major risk of using language models in practical applications is their\ntendency to hallucinate incorrect statements. Hallucinations are often\nattributed to knowledge gaps in LMs, but we hypothesize that in some cases,\nwhen justifying previously generated hallucinations, LMs output false claims\nthat they can separately recognize as incorrect. We construct three\nquestion-answering datasets where ChatGPT and GPT-4 often state an incorrect\nanswer and offer an explanation with at least one incorrect claim. Crucially,\nwe find that ChatGPT and GPT-4 can identify 67% and 87% of their own mistakes,\nrespectively. We refer to this phenomenon as hallucination snowballing: an LM\nover-commits to early mistakes, leading to more mistakes that it otherwise\nwould not make.",
        "pdf_link": "https://arxiv.org/pdf/2305.13534v1.pdf"
    },
    {
        "title": "A Study of Generative Large Language Model for Medical Research and Healthcare",
        "authors": [
            "Cheng Peng",
            "Xi Yang",
            "Aokun Chen",
            "Kaleb E Smith",
            "Nima PourNejatian",
            "Anthony B Costa",
            "Cheryl Martin",
            "Mona G Flores",
            "Ying Zhang",
            "Tanja Magoc",
            "Gloria Lipori",
            "Duane A Mitchell",
            "Naykky S Ospina",
            "Mustafa M Ahmed",
            "William R Hogan",
            "Elizabeth A Shenkman",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2023-05-22T22:37:24Z",
        "summary": "There is enormous enthusiasm and concerns in using large language models\n(LLMs) in healthcare, yet current assumptions are all based on general-purpose\nLLMs such as ChatGPT. This study develops a clinical generative LLM,\nGatorTronGPT, using 277 billion words of mixed clinical and English text with a\nGPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical\nnatural language processing for medical research. Synthetic NLP models trained\nusing GatorTronGPT generated text outperform NLP models trained using\nreal-world clinical text. Physicians Turing test using 1 (worst) to 9 (best)\nscale shows that there is no significant difference in linguistic readability\n(p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical\nrelevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that\nphysicians cannot differentiate them (p < 0.001). This study provides insights\non the opportunities and challenges of LLMs for medical research and\nhealthcare.",
        "pdf_link": "https://arxiv.org/pdf/2305.13523v1.pdf"
    },
    {
        "title": "Small Language Models Improve Giants by Rewriting Their Outputs",
        "authors": [
            "Giorgos Vernikos",
            "Arthur Bražinskas",
            "Jakub Adamek",
            "Jonathan Mallinson",
            "Aliaksei Severyn",
            "Eric Malmi"
        ],
        "published": "2023-05-22T22:07:50Z",
        "summary": "Despite the impressive performance of large language models (LLMs), they\noften lag behind specialized models in various tasks. LLMs only use a fraction\nof the existing training data for in-context learning, while task-specific\nmodels harness the full dataset for fine-tuning. In this work, we tackle the\nproblem of leveraging training data to improve the performance of LLMs without\nfine-tuning. Our approach directly targets LLM predictions without requiring\naccess to their weights. We create a pool of candidates from the LLM through\nfew-shot prompting and we employ a compact model, the LM-corrector (LMCor),\nspecifically trained to merge these candidates to produce an enhanced output.\nOur experiments on four natural language generation tasks demonstrate that even\na small LMCor model (250M) substantially improves the few-shot performance of\nLLMs (62B), matching and even outperforming standard fine-tuning. Furthermore,\nwe illustrate the robustness of LMCor against different prompts, thereby\nminimizing the need for extensive prompt engineering. Finally, we show that\nLMCor can be seamlessly integrated with different LLMs at inference, serving as\na plug-and-play module to improve their performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.13514v2.pdf"
    },
    {
        "title": "Can ChatGPT Detect Intent? Evaluating Large Language Models for Spoken Language Understanding",
        "authors": [
            "Mutian He",
            "Philip N. Garner"
        ],
        "published": "2023-05-22T21:59:26Z",
        "summary": "Recently, large pretrained language models have demonstrated strong language\nunderstanding capabilities. This is particularly reflected in their zero-shot\nand in-context learning abilities on downstream tasks through prompting. To\nassess their impact on spoken language understanding (SLU), we evaluate several\nsuch models like ChatGPT and OPT of different sizes on multiple benchmarks. We\nverify the emergent ability unique to the largest models as they can reach\nintent classification accuracy close to that of supervised models with zero or\nfew shots on various languages given oracle transcripts. By contrast, the\nresults for smaller models fitting a single GPU fall far behind. We note that\nthe error cases often arise from the annotation scheme of the dataset;\nresponses from ChatGPT are still reasonable. We show, however, that the model\nis worse at slot filling, and its performance is sensitive to ASR errors,\nsuggesting serious challenges for the application of those textual models on\nSLU.",
        "pdf_link": "https://arxiv.org/pdf/2305.13512v2.pdf"
    },
    {
        "title": "Clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
        "authors": [
            "Kranti Chalamalasetti",
            "Jana Götze",
            "Sherzod Hakimov",
            "Brielen Madureira",
            "Philipp Sadler",
            "David Schlangen"
        ],
        "published": "2023-05-22T19:56:10Z",
        "summary": "Recent work has proposed a methodology for the systematic evaluation of\n\"Situated Language Understanding Agents\"-agents that operate in rich linguistic\nand non-linguistic contexts-through testing them in carefully constructed\ninteractive settings. Other recent work has argued that Large Language Models\n(LLMs), if suitably set up, can be understood as (simulators of) such agents. A\nconnection suggests itself, which this paper explores: Can LLMs be evaluated\nmeaningfully by exposing them to constrained game-like settings that are built\nto challenge specific capabilities? As a proof of concept, this paper\ninvestigates five interaction settings, showing that current chat-optimised\nLLMs are, to an extent, capable to follow game-play instructions. Both this\ncapability and the quality of the game play, measured by how well the\nobjectives of the different games are met, follows the development cycle, with\nnewer models performing better. The metrics even for the comparatively simple\nexample games are far from being saturated, suggesting that the proposed\ninstrument will remain to have diagnostic value. Our general framework for\nimplementing and evaluating games with LLMs is available at\nhttps://github.com/clembench .",
        "pdf_link": "https://arxiv.org/pdf/2305.13455v3.pdf"
    },
    {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "authors": [
            "Yiming Wang",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published": "2023-05-22T18:54:35Z",
        "summary": "Automatic summarization generates concise summaries that contain key ideas of\nsource documents. As the most mainstream datasets for the news sub-domain,\nCNN/DailyMail and BBC XSum have been widely used for performance benchmarking.\nHowever, the reference summaries of those datasets turn out to be noisy, mainly\nin terms of factual hallucination and information redundancy. To address this\nchallenge, we first annotate new expert-writing Element-aware test sets\nfollowing the \"Lasswell Communication Model\" proposed by Lasswell (1948),\nallowing reference summaries to focus on more fine-grained news elements\nobjectively and comprehensively. Utilizing the new test sets, we observe the\nsurprising zero-shot summary ability of LLMs, which addresses the issue of the\ninconsistent results between human preference and automatic evaluation metrics\nof LLMs' zero-shot summaries in prior work. Further, we propose a Summary\nChain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step\nby step, which helps them integrate more fine-grained details of source\ndocuments into the final summaries that correlate with the human writing\nmindset. Experimental results show our method outperforms state-of-the-art\nfine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two\ndatasets, respectively. Dataset and code are publicly available at\nhttps://github.com/Alsace08/SumCoT.",
        "pdf_link": "https://arxiv.org/pdf/2305.13412v1.pdf"
    },
    {
        "title": "DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules",
        "authors": [
            "Yanchen Liu",
            "William Held",
            "Diyi Yang"
        ],
        "published": "2023-05-22T18:43:31Z",
        "summary": "Existing large language models (LLMs) that mainly focus on Standard American\nEnglish (SAE) often lead to significantly worse performance when being applied\nto other English dialects. While existing mitigations tackle discrepancies for\nindividual target dialects, they assume access to high-accuracy dialect\nidentification systems. The boundaries between dialects are inherently\nflexible, making it difficult to categorize language into discrete predefined\ncategories. In this paper, we propose DADA (Dialect Adaptation via Dynamic\nAggregation), a modular approach to imbue SAE-trained models with\nmulti-dialectal robustness by composing adapters which handle specific\nlinguistic features. The compositional architecture of DADA allows for both\ntargeted adaptation to specific dialect variants and simultaneous adaptation to\nvarious dialects. We show that DADA is effective for both single task and\ninstruction finetuned language models, offering an extensible and interpretable\nframework for adapting existing LLMs to different English dialects.",
        "pdf_link": "https://arxiv.org/pdf/2305.13406v3.pdf"
    },
    {
        "title": "Can LLMs facilitate interpretation of pre-trained language models?",
        "authors": [
            "Basel Mousi",
            "Nadir Durrani",
            "Fahim Dalvi"
        ],
        "published": "2023-05-22T18:03:13Z",
        "summary": "Work done to uncover the knowledge encoded within pre-trained language models\nrely on annotated corpora or human-in-the-loop methods. However, these\napproaches are limited in terms of scalability and the scope of interpretation.\nWe propose using a large language model, ChatGPT, as an annotator to enable\nfine-grained interpretation analysis of pre-trained language models. We\ndiscover latent concepts within pre-trained language models by applying\nagglomerative hierarchical clustering over contextualized representations and\nthen annotate these concepts using ChatGPT. Our findings demonstrate that\nChatGPT produces accurate and semantically richer annotations compared to\nhuman-annotated concepts. Additionally, we showcase how GPT-based annotations\nempower interpretation analysis methodologies of which we demonstrate two:\nprobing frameworks and neuron interpretation. To facilitate further exploration\nand experimentation in the field, we make available a substantial ConceptNet\ndataset (TCN) comprising 39,000 annotated concepts.",
        "pdf_link": "https://arxiv.org/pdf/2305.13386v2.pdf"
    },
    {
        "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Peng Cui",
            "Tiannan Wang",
            "Zhenxin Xiao",
            "Yifan Hou",
            "Ryan Cotterell",
            "Mrinmaya Sachan"
        ],
        "published": "2023-05-22T17:58:10Z",
        "summary": "The fixed-size context of Transformer makes GPT models incapable of\ngenerating arbitrarily long text. In this paper, we introduce RecurrentGPT, a\nlanguage-based simulacrum of the recurrence mechanism in RNNs. RecurrentGPT is\nbuilt upon a large language model (LLM) such as ChatGPT and uses natural\nlanguage to simulate the Long Short-Term Memory mechanism in an LSTM. At each\ntimestep, RecurrentGPT generates a paragraph of text and updates its\nlanguage-based long-short term memory stored on the hard drive and the prompt,\nrespectively. This recurrence mechanism enables RecurrentGPT to generate texts\nof arbitrary length without forgetting. Since human users can easily observe\nand edit the natural language memories, RecurrentGPT is interpretable and\nenables interactive generation of long text. RecurrentGPT is an initial step\ntowards next-generation computer-assisted writing systems beyond local editing\nsuggestions. In addition to producing AI-generated content (AIGC), we also\ndemonstrate the possibility of using RecurrentGPT as an interactive fiction\nthat directly interacts with consumers. We call this usage of generative models\nby ``AI As Contents'' (AIAC), which we believe is the next form of conventional\nAIGC. We further demonstrate the possibility of using RecurrentGPT to create\npersonalized interactive fiction that directly interacts with readers instead\nof interacting with writers. More broadly, RecurrentGPT demonstrates the\nutility of borrowing ideas from popular model designs in cognitive science and\ndeep learning for prompting LLMs. Our code is available at\nhttps://github.com/aiwaves-cn/RecurrentGPT and an online demo is available at\nhttps://www.aiwaves.org/recurrentgpt.",
        "pdf_link": "https://arxiv.org/pdf/2305.13304v1.pdf"
    },
    {
        "title": "Language-Agnostic Bias Detection in Language Models with Bias Probing",
        "authors": [
            "Abdullatif Köksal",
            "Omer Faruk Yalcin",
            "Ahmet Akbiyik",
            "M. Tahir Kilavuz",
            "Anna Korhonen",
            "Hinrich Schütze"
        ],
        "published": "2023-05-22T17:58:01Z",
        "summary": "Pretrained language models (PLMs) are key components in NLP, but they contain\nstrong social biases. Quantifying these biases is challenging because current\nmethods focusing on fill-the-mask objectives are sensitive to slight changes in\ninput. To address this, we propose a bias probing technique called LABDet, for\nevaluating social bias in PLMs with a robust and language-agnostic method. For\nnationality as a case study, we show that LABDet `surfaces' nationality bias by\ntraining a classifier on top of a frozen PLM on non-nationality sentiment\ndetection. We find consistent patterns of nationality bias across monolingual\nPLMs in six languages that align with historical and political context. We also\nshow for English BERT that bias surfaced by LABDet correlates well with bias in\nthe pretraining data; thus, our work is one of the few studies that directly\nlinks pretraining data to PLM behavior. Finally, we verify LABDet's reliability\nand applicability to different templates and languages through an extensive set\nof robustness checks. We publicly share our code and dataset in\nhttps://github.com/akoksal/LABDet.",
        "pdf_link": "https://arxiv.org/pdf/2305.13302v2.pdf"
    },
    {
        "title": "Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Renze Lou",
            "Yu Su"
        ],
        "published": "2023-05-22T17:57:41Z",
        "summary": "By providing external information to large language models (LLMs), tool\naugmentation (including retrieval augmentation) has emerged as a promising\nsolution for addressing the limitations of LLMs' static parametric memory.\nHowever, how receptive are LLMs to such external evidence, especially when the\nevidence conflicts with their parametric memory? We present the first\ncomprehensive and controlled investigation into the behavior of LLMs when\nencountering knowledge conflicts. We propose a systematic framework to elicit\nhigh-quality parametric memory from LLMs and construct the corresponding\ncounter-memory, which enables us to conduct a series of controlled experiments.\nOur investigation reveals seemingly contradicting behaviors of LLMs. On the one\nhand, different from prior wisdom, we find that LLMs can be highly receptive to\nexternal evidence even when that conflicts with their parametric memory, given\nthat the external evidence is coherent and convincing. On the other hand, LLMs\nalso demonstrate a strong confirmation bias when the external evidence contains\nsome information that is consistent with their parametric memory, despite being\npresented with conflicting evidence at the same time. These results pose\nimportant implications that are worth careful consideration for the further\ndevelopment and deployment of tool- and retrieval-augmented LLMs. Resources are\navailable at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.",
        "pdf_link": "https://arxiv.org/pdf/2305.13300v4.pdf"
    },
    {
        "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
        "authors": [
            "Chenglei Si",
            "Dan Friedman",
            "Nitish Joshi",
            "Shi Feng",
            "Danqi Chen",
            "He He"
        ],
        "published": "2023-05-22T17:56:31Z",
        "summary": "In-context learning (ICL) is an important paradigm for adapting large\nlanguage models (LLMs) to new tasks, but the generalization behavior of ICL\nremains poorly understood. We investigate the inductive biases of ICL from the\nperspective of feature bias: which feature ICL is more likely to use given a\nset of underspecified demonstrations in which two features are equally\npredictive of the labels. First, we characterize the feature biases of GPT-3\nmodels by constructing underspecified demonstrations from a range of NLP\ndatasets and feature combinations. We find that LLMs exhibit clear feature\nbiases - for example, demonstrating a strong bias to predict labels according\nto sentiment rather than shallow lexical features, like punctuation. Second, we\nevaluate the effect of different interventions that are designed to impose an\ninductive bias in favor of a particular feature, such as adding a natural\nlanguage instruction or using semantically relevant label words. We find that,\nwhile many interventions can influence the learner to prefer a particular\nfeature, it can be difficult to overcome strong prior biases. Overall, our\nresults provide a broader picture of the types of features that ICL may be more\nlikely to exploit and how to impose inductive biases that are better aligned\nwith the intended task.",
        "pdf_link": "https://arxiv.org/pdf/2305.13299v1.pdf"
    },
    {
        "title": "AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback",
        "authors": [
            "Yann Dubois",
            "Xuechen Li",
            "Rohan Taori",
            "Tianyi Zhang",
            "Ishaan Gulrajani",
            "Jimmy Ba",
            "Carlos Guestrin",
            "Percy Liang",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2023-05-22T17:55:50Z",
        "summary": "Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their strong instruction-following abilities. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following requires\ntackling three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 50x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO, DPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.",
        "pdf_link": "https://arxiv.org/pdf/2305.14387v4.pdf"
    },
    {
        "title": "Fairness of ChatGPT",
        "authors": [
            "Yunqi Li",
            "Yongfeng Zhang"
        ],
        "published": "2023-05-22T17:51:56Z",
        "summary": "Understanding and addressing unfairness in LLMs are crucial for responsible\nAI deployment. However, there is a limited availability of quantitative\nanalyses and in-depth studies regarding fairness evaluations in LLMs,\nespecially when applying LLMs to high-stakes fields. This work aims to fill\nthis gap by providing a systematic evaluation of the effectiveness and fairness\nof LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's\nperformance in high-takes fields including education, criminology, finance and\nhealthcare. To make thorough evaluation, we consider both group fairness and\nindividual fairness and we also observe the disparities in ChatGPT's outputs\nunder a set of biased or unbiased prompts. This work contributes to a deeper\nunderstanding of LLMs' fairness performance, facilitates bias mitigation and\nfosters the development of responsible artificial intelligence systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.18569v1.pdf"
    },
    {
        "title": "Evaluating ChatGPT's Performance for Multilingual and Emoji-based Hate Speech Detection",
        "authors": [
            "Mithun Das",
            "Saurabh Kumar Pandey",
            "Animesh Mukherjee"
        ],
        "published": "2023-05-22T17:36:58Z",
        "summary": "Hate speech is a severe issue that affects many online platforms. So far,\nseveral studies have been performed to develop robust hate speech detection\nsystems. Large language models like ChatGPT have recently shown a great promise\nin performing several tasks, including hate speech detection. However, it is\ncrucial to comprehend the limitations of these models to build robust hate\nspeech detection systems. To bridge this gap, our study aims to evaluate the\nstrengths and weaknesses of the ChatGPT model in detecting hate speech at a\ngranular level across 11 languages. Our evaluation employs a series of\nfunctionality tests that reveals various intricate failures of the model which\nthe aggregate metrics like macro F1 or accuracy are not able to unfold. In\naddition, we investigate the influence of complex emotions, such as the use of\nemojis in hate speech, on the performance of the ChatGPT model. Our analysis\nhighlights the shortcomings of the generative models in detecting certain types\nof hate speech and highlighting the need for further research and improvements\nin the workings of these models.",
        "pdf_link": "https://arxiv.org/pdf/2305.13276v2.pdf"
    },
    {
        "title": "CLASS: A Design Framework for building Intelligent Tutoring Systems based on Learning Science principles",
        "authors": [
            "Shashank Sonkar",
            "Naiming Liu",
            "Debshila Basu Mallick",
            "Richard G. Baraniuk"
        ],
        "published": "2023-05-22T17:35:05Z",
        "summary": "We present a design framework called Conversational Learning with Analytical\nStep-by-Step Strategies (CLASS) for building advanced Intelligent Tutoring\nSystems (ITS) powered by high-performance Large Language Models (LLMs). The\nCLASS framework empowers ITS with two key capabilities. First, through a\ncarefully curated scaffolding dataset, CLASS equips ITS with essential\nproblem-solving strategies, enabling it to provide tutor-like, step-by-step\nguidance to students. Second, by using a dynamic conversational dataset, CLASS\nassists ITS in facilitating natural language interactions, fostering engaging\nstudent-tutor conversations. The CLASS framework also provides valuable\ninsights into ITS' internal decision-making process which allows seamless\nintegration of user feedback, thus enabling continuous refinement and\nimprovement. We also present a proof-of-concept ITS, referred to as SPOCK,\nwhich is trained using the CLASS framework with a focus on introductory\ncollege-level biology content. A carefully constructed protocol was developed\nfor SPOCK's preliminary evaluation, examining aspects such as the factual\naccuracy and relevance of its responses. Experts in the field of biology\noffered favorable remarks, particularly highlighting SPOCK's capability to\nbreak down questions into manageable subproblems and provide encouraging\nresponses to students. Code and models are available at\nhttps://github.com/luffycodes/Tutorbot-Spock.",
        "pdf_link": "https://arxiv.org/pdf/2305.13272v2.pdf"
    },
    {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "authors": [
            "Xingxuan Li",
            "Ruochen Zhao",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Shafiq Joty",
            "Soujanya Poria",
            "Lidong Bing"
        ],
        "published": "2023-05-22T17:34:23Z",
        "summary": "We present chain-of-knowledge (CoK), a novel framework that augments large\nlanguage models (LLMs) by dynamically incorporating grounding information from\nheterogeneous sources. It results in more factual rationales and reduced\nhallucination in generation. Specifically, CoK consists of three stages:\nreasoning preparation, dynamic knowledge adapting, and answer consolidation.\nGiven a knowledge-intensive question, CoK first prepares several preliminary\nrationales and answers while identifying the relevant knowledge domains. If\nthere is no majority consensus among the answers from samples, CoK corrects the\nrationales step by step by adapting knowledge from the identified domains.\nThese corrected rationales can plausibly serve as a better foundation for the\nfinal answer consolidation. Unlike prior studies that primarily use\nunstructured data, CoK also leverages structured knowledge sources such as\nWikidata and tables that provide more reliable factual information. To access\nboth unstructured and structured knowledge sources in the dynamic knowledge\nadapting stage, we propose an adaptive query generator that allows the\ngeneration of queries for various types of query languages, including SPARQL,\nSQL, and natural sentences. Moreover, to minimize error propagation between\nrationales, CoK corrects the rationales progressively using preceding corrected\nrationales to generate and correct subsequent rationales. Extensive experiments\nshow that CoK consistently improves the performance of LLMs on\nknowledge-intensive tasks across different domains.",
        "pdf_link": "https://arxiv.org/pdf/2305.13269v4.pdf"
    },
    {
        "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
        "authors": [
            "Yueting Yang",
            "Xintong Zhang",
            "Wenjuan Han"
        ],
        "published": "2023-05-22T17:33:44Z",
        "summary": "Pre-trained visual language models (VLM) have shown excellent performance in\nimage caption tasks. However, it sometimes shows insufficient reasoning\nability. In contrast, large language models (LLMs) emerge with powerful\nreasoning capabilities. Therefore, we propose a method called TReE, which\ntransfers the reasoning ability of a large language model to a visual language\nmodel in zero-shot scenarios. TReE contains three stages: observation,\nthinking, and re-thinking. Observation stage indicates that VLM obtains the\noverall information of the relative image. Thinking stage combines the image\ninformation and task description as the prompt of the LLM, inference with the\nrationals. Re-Thinking stage learns from rationale and then inference the final\nresult through VLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.13267v1.pdf"
    },
    {
        "title": "Prompting is not a substitute for probability measurements in large language models",
        "authors": [
            "Jennifer Hu",
            "Roger Levy"
        ],
        "published": "2023-05-22T17:33:17Z",
        "summary": "Prompting is now a dominant method for evaluating the linguistic knowledge of\nlarge language models (LLMs). While other methods directly read out models'\nprobability distributions over strings, prompting requires models to access\nthis internal information by processing linguistic input, thereby implicitly\ntesting a new type of emergent ability: metalinguistic judgment. In this study,\nwe compare metalinguistic prompting and direct probability measurements as ways\nof measuring models' linguistic knowledge. Broadly, we find that LLMs'\nmetalinguistic judgments are inferior to quantities directly derived from\nrepresentations. Furthermore, consistency gets worse as the prompt query\ndiverges from direct measurements of next-word probabilities. Our findings\nsuggest that negative results relying on metalinguistic prompts cannot be taken\nas conclusive evidence that an LLM lacks a particular linguistic\ngeneralization. Our results also highlight the value that is lost with the move\nto closed APIs where access to probability distributions is limited.",
        "pdf_link": "https://arxiv.org/pdf/2305.13264v2.pdf"
    },
    {
        "title": "\"According to ...\": Prompting Language Models Improves Quoting from Pre-Training Data",
        "authors": [
            "Orion Weller",
            "Marc Marone",
            "Nathaniel Weir",
            "Dawn Lawrie",
            "Daniel Khashabi",
            "Benjamin Van Durme"
        ],
        "published": "2023-05-22T17:25:24Z",
        "summary": "Large Language Models (LLMs) may hallucinate and generate fake information,\ndespite pre-training on factual data. Inspired by the journalistic device of\n\"according to sources\", we propose according-to prompting: directing LLMs to\nground responses against previously observed text. To quantify this grounding,\nwe propose a novel evaluation metric (QUIP-Score) that measures the extent to\nwhich model-produced answers are directly found in underlying text corpora. We\nillustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S.\nlegal tax code) that these prompts improve grounding under our metrics, with\nthe additional benefit of often improving end-task performance. Furthermore,\nprompts that ask the model to decrease grounding (or to ground to other\ncorpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase\nor decrease grounded generations on request.",
        "pdf_link": "https://arxiv.org/pdf/2305.13252v2.pdf"
    },
    {
        "title": "Chip-Chat: Challenges and Opportunities in Conversational Hardware Design",
        "authors": [
            "Jason Blocklove",
            "Siddharth Garg",
            "Ramesh Karri",
            "Hammond Pearce"
        ],
        "published": "2023-05-22T17:13:33Z",
        "summary": "Modern hardware design starts with specifications provided in natural\nlanguage. These are then translated by hardware engineers into appropriate\nHardware Description Languages (HDLs) such as Verilog before synthesizing\ncircuit elements. Automating this translation could reduce sources of human\nerror from the engineering process. But, it is only recently that artificial\nintelligence (AI) has demonstrated capabilities for machine-based end-to-end\ndesign translations. Commercially-available instruction-tuned Large Language\nModels (LLMs) such as OpenAI's ChatGPT and Google's Bard claim to be able to\nproduce code in a variety of programming languages; but studies examining them\nfor hardware are still lacking. In this work, we thus explore the challenges\nfaced and opportunities presented when leveraging these recent advances in LLMs\nfor hardware design. Given that these `conversational' LLMs perform best when\nused interactively, we perform a case study where a hardware engineer\nco-architects a novel 8-bit accumulator-based microprocessor architecture with\nthe LLM according to real-world hardware constraints. We then sent the\nprocessor to tapeout in a Skywater 130nm shuttle, meaning that this `Chip-Chat'\nresulted in what we believe to be the world's first wholly-AI-written HDL for\ntapeout.",
        "pdf_link": "https://arxiv.org/pdf/2305.13243v2.pdf"
    },
    {
        "title": "Deepfake Text Detection in the Wild",
        "authors": [
            "Yafu Li",
            "Qintong Li",
            "Leyang Cui",
            "Wei Bi",
            "Longyue Wang",
            "Linyi Yang",
            "Shuming Shi",
            "Yue Zhang"
        ],
        "published": "2023-05-22T17:13:29Z",
        "summary": "Recent advances in large language models have enabled them to reach a level\nof text generation comparable to that of humans. These models show powerful\ncapabilities across a wide range of content, including news article writing,\nstory generation, and scientific writing. Such capability further narrows the\ngap between human-authored and machine-generated texts, highlighting the\nimportance of deepfake text detection to avoid potential risks such as fake\nnews propagation and plagiarism. However, previous work has been limited in\nthat they testify methods on testbed of specific domains or certain language\nmodels. In practical scenarios, the detector faces texts from various domains\nor LLMs without knowing their sources. To this end, we build a wild testbed by\ngathering texts from various human writings and deepfake texts generated by\ndifferent LLMs. Human annotators are only slightly better than random guessing\nat identifying machine-generated texts. Empirical results on automatic\ndetection methods further showcase the challenges of deepfake text detection in\na wild testbed. In addition, out-of-distribution poses a greater challenge for\na detector to be employed in realistic application scenarios. We release our\nresources at https://github.com/yafuly/DeepfakeTextDetect.",
        "pdf_link": "https://arxiv.org/pdf/2305.13242v1.pdf"
    },
    {
        "title": "To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis",
        "authors": [
            "Fuzhao Xue",
            "Yao Fu",
            "Wangchunshu Zhou",
            "Zangwei Zheng",
            "Yang You"
        ],
        "published": "2023-05-22T17:02:15Z",
        "summary": "Recent research has highlighted the importance of dataset size in scaling\nlanguage models. However, large language models (LLMs) are notoriously\ntoken-hungry during pre-training, and high-quality text data on the web is\napproaching its scaling limit for LLMs. To further enhance LLMs, a\nstraightforward approach is to repeat the pre-training data for additional\nepochs. In this study, we empirically investigate three key aspects under this\napproach. First, we explore the consequences of repeating pre-training data,\nrevealing that the model is susceptible to overfitting, leading to multi-epoch\ndegradation. Second, we examine the key factors contributing to multi-epoch\ndegradation, finding that significant factors include dataset size, model\nparameters, and training objectives, while less influential factors consist of\ndataset quality and model FLOPs. Finally, we explore whether widely used\nregularization can alleviate multi-epoch degradation. Most regularization\ntechniques do not yield significant improvements, except for dropout, which\ndemonstrates remarkable effectiveness but requires careful tuning when scaling\nup the model size. Additionally, we discover that leveraging mixture-of-experts\n(MoE) enables cost-effective and efficient hyper-parameter tuning for\ncomputationally intensive dense LLMs with comparable trainable parameters,\npotentially impacting efficient LLM development on a broader scale.",
        "pdf_link": "https://arxiv.org/pdf/2305.13230v2.pdf"
    },
    {
        "title": "Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A Preliminary Study on Writing Assistance",
        "authors": [
            "Yue Zhang",
            "Leyang Cui",
            "Deng Cai",
            "Xinting Huang",
            "Tao Fang",
            "Wei Bi"
        ],
        "published": "2023-05-22T16:56:44Z",
        "summary": "Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered\nsignificant attention due to their exceptional capabilities in handling a\ndiverse range of tasks. Recent studies demonstrate that open-sourced smaller\nfoundational models, such as 7B-size LLaMA, can also display remarkable\nproficiency in tackling diverse tasks when fine-tuned using instruction-driven\ndata. In this work, we investigate a practical problem setting where the\nprimary focus is on one or a few particular tasks rather than general-purpose\ninstruction following, and explore whether LLMs can be beneficial and further\nimproved for such targeted scenarios. We choose the writing-assistant scenario\nas the testbed, which includes seven writing tasks. We collect training data\nfor these tasks, reframe them in an instruction-following format, and\nsubsequently refine the LLM, specifically LLaMA, via instruction tuning.\nExperimental results show that fine-tuning LLaMA on writing instruction data\nsignificantly improves its ability on writing tasks. We also conduct more\nexperiments and analyses to offer insights for future work on effectively\nfine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion\nregarding the necessity of employing LLMs for only one targeted task, taking\ninto account the efforts required for tuning and the resources consumed during\ndeployment.",
        "pdf_link": "https://arxiv.org/pdf/2305.13225v2.pdf"
    },
    {
        "title": "SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
        "authors": [
            "Xinyuan Lu",
            "Liangming Pan",
            "Qian Liu",
            "Preslav Nakov",
            "Min-Yen Kan"
        ],
        "published": "2023-05-22T16:13:50Z",
        "summary": "Current scientific fact-checking benchmarks exhibit several shortcomings,\nsuch as biases arising from crowd-sourced claims and an over-reliance on\ntext-based evidence. We present SCITAB, a challenging evaluation dataset\nconsisting of 1.2K expert-verified scientific claims that 1) originate from\nauthentic scientific publications and 2) require compositional reasoning for\nverification. The claims are paired with evidence-containing scientific tables\nannotated with labels. Through extensive evaluations, we demonstrate that\nSCITAB poses a significant challenge to state-of-the-art models, including\ntable-based pretraining models and large language models. All models except\nGPT-4 achieved performance barely above random guessing. Popular prompting\ntechniques, such as Chain-of-Thought, do not achieve much performance gains on\nSCITAB. Our analysis uncovers several unique challenges posed by SCITAB,\nincluding table grounding, claim ambiguity, and compositional reasoning. Our\ncodes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
        "pdf_link": "https://arxiv.org/pdf/2305.13186v3.pdf"
    },
    {
        "title": "Teaching Probabilistic Logical Reasoning to Transformers",
        "authors": [
            "Aliakbar Nafar",
            "Kristen Brent Venable",
            "Parisa Kordjamshidi"
        ],
        "published": "2023-05-22T16:08:20Z",
        "summary": "In this paper, we evaluate the capability of transformer-based language\nmodels in making inferences over uncertain text that includes uncertain rules\nof reasoning. We cover both Pre-trained Language Models (PLMs) and generative\nLarge Language Models (LLMs). Our evaluation results show that both generations\nof language models struggle with reasoning over uncertain text. We propose a\nnovel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT),\nthat utilizes probabilistic logical rules as constraints in the fine-tuning\nphase without relying on these rules in the inference stage. To assess the\neffectiveness of PCT, we utilize the related corpora and, additionally, create\na new and more challenging benchmark that, unlike the previous ones, uses\ninstance-specific rules. Our study demonstrates that PCT improves the\ntransformer-based language model's intrinsic reasoning and makes their\nprobabilistic logical reasoning process more explicit and explainable.\nFurthermore, PCT equips these models to effectively handle novel situations,\nincluding higher reasoning depth, new domains, and complex probabilistic\nstructures.",
        "pdf_link": "https://arxiv.org/pdf/2305.13179v2.pdf"
    },
    {
        "title": "Editing Large Language Models: Problems, Methods, and Opportunities",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Bozhong Tian",
            "Siyuan Cheng",
            "Zhoubo Li",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-22T16:00:00Z",
        "summary": "Despite the ability to train capable LLMs, the methodology for maintaining\ntheir relevancy and rectifying errors remains elusive. To this end, the past\nfew years have witnessed a surge in techniques for editing LLMs, the objective\nof which is to efficiently alter the behavior of LLMs within a specific domain\nwithout negatively impacting performance across other inputs. This paper\nembarks on a deep exploration of the problems, methods, and opportunities\nrelated to model editing for LLMs. In particular, we provide an exhaustive\noverview of the task definition and challenges associated with model editing,\nalong with an in-depth empirical analysis of the most progressive methods\ncurrently at our disposal. We also build a new benchmark dataset to facilitate\na more robust evaluation and pinpoint enduring issues intrinsic to existing\ntechniques. Our objective is to provide valuable insights into the\neffectiveness and feasibility of each editing technique, thereby assisting the\ncommunity in making informed decisions on the selection of the most appropriate\nmethod for a specific task or context. Code and datasets are available at\nhttps://github.com/zjunlp/EasyEdit.",
        "pdf_link": "https://arxiv.org/pdf/2305.13172v3.pdf"
    },
    {
        "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities",
        "authors": [
            "Yuqi Zhu",
            "Xiaohan Wang",
            "Jing Chen",
            "Shuofei Qiao",
            "Yixin Ou",
            "Yunzhi Yao",
            "Shumin Deng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-22T15:56:44Z",
        "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG.",
        "pdf_link": "https://arxiv.org/pdf/2305.13168v2.pdf"
    },
    {
        "title": "Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate",
        "authors": [
            "Boshi Wang",
            "Xiang Yue",
            "Huan Sun"
        ],
        "published": "2023-05-22T15:47:31Z",
        "summary": "Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive\nperformance in complex reasoning tasks. However, it is difficult to know\nwhether the models are reasoning based on deep understandings of truth and\nlogic, or leveraging their memorized patterns in a relatively superficial way.\nIn this work, we explore testing LLMs' reasoning by engaging with them in a\ndebate-like conversation, where given a question, the LLM and the user need to\ndiscuss to make the correct decision starting from opposing arguments. Upon\nmitigating the Clever Hans effect, our task requires the LLM to not only\nachieve the correct answer on its own, but also be able to hold and defend its\nbelief instead of blindly believing or getting misled by the user's (invalid)\narguments and critiques, thus testing in greater depth whether the LLM grasps\nthe essence of the reasoning required to solve the problem. Across a range of\ncomplex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench\ntasks, we find that despite their impressive performance as reported in\nexisting work on generating correct step-by-step solutions in the beginning,\nLLMs like ChatGPT cannot maintain their beliefs in truth for a significant\nportion of examples when challenged by oftentimes absurdly invalid arguments.\nOur work points to danger zones of model alignment, and also suggests more\ncareful treatments and interpretations of the recent findings that LLMs can\nimprove their responses based on feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.13160v2.pdf"
    },
    {
        "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
        "authors": [
            "Xiaolei Wang",
            "Xinyu Tang",
            "Wayne Xin Zhao",
            "Jingyuan Wang",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-22T15:12:43Z",
        "summary": "The recent success of large language models (LLMs) has shown great potential\nto develop more powerful conversational recommender systems (CRSs), which rely\non natural language conversations to satisfy user needs. In this paper, we\nembark on an investigation into the utilization of ChatGPT for conversational\nrecommendation, revealing the inadequacy of the existing evaluation protocol.\nIt might over-emphasize the matching with the ground-truth items or utterances\ngenerated by human annotators, while neglecting the interactive nature of being\na capable CRS. To overcome the limitation, we further propose an interactive\nEvaluation approach based on LLMs named iEvaLM that harnesses LLM-based user\nsimulators. Our evaluation approach can simulate various interaction scenarios\nbetween users and systems. Through the experiments on two publicly available\nCRS datasets, we demonstrate notable improvements compared to the prevailing\nevaluation protocol. Furthermore, we emphasize the evaluation of\nexplainability, and ChatGPT showcases persuasive explanation generation for its\nrecommendations. Our study contributes to a deeper comprehension of the\nuntapped potential of LLMs for CRSs and provides a more flexible and\neasy-to-use evaluation framework for future research endeavors. The codes and\ndata are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
        "pdf_link": "https://arxiv.org/pdf/2305.13112v2.pdf"
    },
    {
        "title": "Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",
        "authors": [
            "Katherine Abramski",
            "Salvatore Citraro",
            "Luigi Lombardi",
            "Giulio Rossetti",
            "Massimo Stella"
        ],
        "published": "2023-05-22T15:06:51Z",
        "summary": "Large language models are becoming increasingly integrated into our lives.\nHence, it is important to understand the biases present in their outputs in\norder to avoid perpetuating harmful stereotypes, which originate in our own\nflawed ways of thinking. This challenge requires developing new benchmarks and\nmethods for quantifying affective and semantic bias, keeping in mind that LLMs\nact as psycho-social mirrors that reflect the views and tendencies that are\nprevalent in society. One such tendency that has harmful negative effects is\nthe global phenomenon of anxiety toward math and STEM subjects. Here, we\ninvestigate perceptions of math and STEM fields provided by cutting-edge\nlanguage models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach\nfrom network science and cognitive psychology. Specifically, we use behavioral\nforma mentis networks (BFMNs) to understand how these LLMs frame math and STEM\ndisciplines in relation to other concepts. We use data obtained by probing the\nthree LLMs in a language generation task that has previously been applied to\nhumans. Our findings indicate that LLMs have an overall negative perception of\nmath and STEM fields, with math being perceived most negatively. We observe\nsignificant differences across the three LLMs. We observe that newer versions\n(i.e. GPT-4) produce richer, more complex perceptions as well as less negative\nperceptions compared to older versions and N=159 high-school students. These\nfindings suggest that advances in the architecture of LLMs may lead to\nincreasingly less biased models that could even perhaps someday aid in reducing\nharmful stereotypes in society rather than perpetuating them.",
        "pdf_link": "https://arxiv.org/pdf/2305.18320v1.pdf"
    },
    {
        "title": "Observations on LLMs for Telecom Domain: Capabilities and Limitations",
        "authors": [
            "Sumit Soman",
            "Ranjani H G"
        ],
        "published": "2023-05-22T15:04:16Z",
        "summary": "The landscape for building conversational interfaces (chatbots) has witnessed\na paradigm shift with recent developments in generative Artificial Intelligence\n(AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and\nGPT4), Google's Bard, Large Language Model Meta AI (LLaMA), among others. In\nthis paper, we analyze capabilities and limitations of incorporating such\nmodels in conversational interfaces for the telecommunication domain,\nspecifically for enterprise wireless products and services. Using Cradlepoint's\npublicly available data for our experiments, we present a comparative analysis\nof the responses from such models for multiple use-cases including domain\nadaptation for terminology and product taxonomy, context continuity, robustness\nto input perturbations and errors. We believe this evaluation would provide\nuseful insights to data scientists engaged in building customized\nconversational interfaces for domain-specific requirements.",
        "pdf_link": "https://arxiv.org/pdf/2305.13102v1.pdf"
    },
    {
        "title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
        "authors": [
            "Chenhui Shen",
            "Liying Cheng",
            "Xuan-Phi Nguyen",
            "Yang You",
            "Lidong Bing"
        ],
        "published": "2023-05-22T14:58:13Z",
        "summary": "With the recent undeniable advancement in reasoning abilities in large\nlanguage models (LLMs) like ChatGPT and GPT-4, there is a growing trend for\nusing LLMs on various tasks. One area where LLMs can be employed is as an\nalternative evaluation metric for complex generative tasks, which generally\ndemands expensive human judges to complement the traditional automatic metrics\nfor various evaluation dimensions such as fluency and consistency. In this\nwork, we conduct extensive analysis to investigate the stability and\nreliability of LLMs as automatic evaluators for abstractive summarization. We\nfound that while ChatGPT and GPT-4 outperform the commonly used automatic\nmetrics, they are not ready as human replacements due to significant\nlimitations. That is, LLM evaluators rate each candidate system inconsistently\nand are dimension-dependent. They also struggle to compare candidates with\nclose performance and become more unreliable with higher-quality summaries by\nobtaining a lower correlation with humans. In other words, with better\nabstractive summarization systems being introduced at a fast pace, LLMs may\nresult in misleading and unreliable evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2305.13091v2.pdf"
    },
    {
        "title": "InheritSumm: A General, Versatile and Compact Summarizer by Distilling from GPT",
        "authors": [
            "Yichong Xu",
            "Ruochen Xu",
            "Dan Iter",
            "Yang Liu",
            "Shuohang Wang",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023-05-22T14:52:32Z",
        "summary": "While large models such as GPT-3 demonstrate exceptional performance in\nzeroshot and fewshot summarization tasks, their extensive serving and\nfine-tuning costs hinder their utilization in various applications. Conversely,\nprevious studies have found that although automatic metrics tend to favor\nsmaller fine-tuned models, the quality of the summaries they generate is\ninferior to that of larger models like GPT-3 when assessed by human evaluators.\nTo address this issue, we propose InheritSumm, a versatile and compact\nsummarization model derived from GPT-3.5 through distillation. InheritSumm not\nonly exhibits comparable zeroshot and fewshot summarization capabilities to\nGPT-3.5 but is also sufficiently compact for fine-tuning purposes. Experimental\nresults demonstrate that InheritSumm achieves similar or superior performance\nto GPT-3.5 in zeroshot and fewshot settings. Furthermore, it outperforms the\npreviously established best small models in both prefix-tuning and full-data\nfine-tuning scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.13083v1.pdf"
    },
    {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "authors": [
            "Shuofei Qiao",
            "Honghao Gui",
            "Chengfei Lv",
            "Qianghuai Jia",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-22T14:37:05Z",
        "summary": "Tools serve as pivotal interfaces that enable humans to understand and\nreshape the environment. With the advent of foundation models, AI systems can\nutilize tools to expand their capabilities and interact with the real world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce large language models to utilize\ntools indiscriminately, as complex tasks often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the large language model\nselectively use tools by improving the accuracy of tool usage while enhancing\ninsufficient tool learning and mitigating excessive reliance on tools. Code is\navailable at https://github.com/zjunlp/TRICE.",
        "pdf_link": "https://arxiv.org/pdf/2305.13068v3.pdf"
    },
    {
        "title": "Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study",
        "authors": [
            "Yuan Sui",
            "Mengyu Zhou",
            "Mingjie Zhou",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023-05-22T14:23:46Z",
        "summary": "Large language models (LLMs) are becoming attractive as few-shot reasoners to\nsolve Natural Language (NL)-related tasks. However, there is still much to\nlearn about how well LLMs understand structured data, such as tables. Although\ntables can be used as input to LLMs with serialization, there is a lack of\ncomprehensive studies that examine whether LLMs can truly comprehend such data.\nIn this paper, we try to understand this by designing a benchmark to evaluate\nthe structural understanding capabilities (SUC) of LLMs. The benchmark we\ncreate includes seven tasks, each with its own unique challenges, e.g., cell\nlookup, row retrieval, and size detection. We perform a series of evaluations\non GPT-3.5 and GPT-4. We find that performance varied depending on several\ninput choices, including table input format, content order, role prompting, and\npartition marks. Drawing from the insights gained through the benchmark\nevaluations, we propose \\textit{self-augmentation} for effective structural\nprompting, such as critical value / range identification using internal\nknowledge of LLMs. When combined with carefully chosen input choices, these\nstructural prompting methods lead to promising improvements in LLM performance\non a variety of tabular tasks, e.g., TabFact($\\uparrow2.31\\%$),\nHybridQA($\\uparrow2.13\\%$), SQA($\\uparrow2.72\\%$), Feverous($\\uparrow0.84\\%$),\nand ToTTo($\\uparrow5.68\\%$). We believe that our open source benchmark and\nproposed prompting methods can serve as a simple yet generic selection for\nfuture research.",
        "pdf_link": "https://arxiv.org/pdf/2305.13062v4.pdf"
    },
    {
        "title": "Automated stance detection in complex topics and small languages: the challenging case of immigration in polarizing news media",
        "authors": [
            "Mark Mets",
            "Andres Karjus",
            "Indrek Ibrus",
            "Maximilian Schich"
        ],
        "published": "2023-05-22T13:56:35Z",
        "summary": "Automated stance detection and related machine learning methods can provide\nuseful insights for media monitoring and academic research. Many of these\napproaches require annotated training datasets, which limits their\napplicability for languages where these may not be readily available. This\npaper explores the applicability of large language models for automated stance\ndetection in a challenging scenario, involving a morphologically complex,\nlower-resource language, and a socio-culturally complex topic, immigration. If\nthe approach works in this case, it can be expected to perform as well or\nbetter in less demanding scenarios. We annotate a large set of pro and\nanti-immigration examples, and compare the performance of multiple language\nmodels as supervised learners. We also probe the usability of ChatGPT as an\ninstructable zero-shot classifier for the same task. Supervised achieves\nacceptable performance, and ChatGPT yields similar accuracy. This is promising\nas a potentially simpler and cheaper alternative for text classification tasks,\nincluding in lower-resource languages. We further use the best-performing model\nto investigate diachronic trends over seven years in two corpora of Estonian\nmainstream and right-wing populist news sources, demonstrating the\napplicability of the approach for news analytics and media monitoring settings,\nand discuss correspondences between stance changes and real-world events.",
        "pdf_link": "https://arxiv.org/pdf/2305.13047v1.pdf"
    },
    {
        "title": "Iterative Forward Tuning Boosts In-context Learning in Language Models",
        "authors": [
            "Jiaxi Yang",
            "Binyuan Hui",
            "Min Yang",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-05-22T13:18:17Z",
        "summary": "Large language models (LLMs) have exhibited an emergent in-context learning\n(ICL) ability. However, the ICL models that can solve ordinary cases are hardly\nextended to solve more complex tasks by processing the demonstration examples\nonce. This single-turn ICL is incoordinate with the decision making process of\nhumans by learning from analogy. In this paper, we propose an effective and\nefficient two-stage framework to boost ICL in LLMs by exploiting a dual form\nbetween Transformer attention and gradient descent-based optimization.\nConcretely, we divide the ICL process into \"Deep-Thinking\" and inference\nstages. The \"Deep-Thinking\" stage performs iterative forward optimization of\ndemonstrations, which is expected to boost the reasoning abilities of LLMs at\ntest time by \"thinking\" demonstrations multiple times. It produces accumulated\nmeta-gradients by manipulating the Key-Value matrices in the self-attention\nmodules of the Transformer. Then, the inference stage only takes the test query\nas input without concatenating demonstrations and applies the learned\nmeta-gradients through attention for output prediction. In this way,\ndemonstrations are not required during the inference stage since they are\nalready learned and stored in the definitive meta-gradients. LLMs can be\neffectively and efficiently adapted to downstream tasks. Extensive experiments\non ten classification and multiple-choice datasets show that our method\nachieves substantially better performance than standard ICL in terms of both\naccuracy and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2305.13016v2.pdf"
    },
    {
        "title": "Can Large Language Models emulate an inductive Thematic Analysis of semi-structured interviews? An exploration and provocation on the limits of the approach and the model",
        "authors": [
            "Stefano De Paoli"
        ],
        "published": "2023-05-22T13:16:07Z",
        "summary": "Large Language Models (LLMs) have emerged as powerful generative Artificial\nIntelligence solutions which can be applied to several fields and areas of\nwork. This paper presents results and reflection of an experiment done to use\nthe model GPT 3.5-Turbo to emulate some aspects of an inductive Thematic\nAnalysis. Previous research on this subject has largely worked on conducting\ndeductive analysis. Thematic Analysis is a qualitative method for analysis\ncommonly used in social sciences and it is based on interpretations made by the\nhuman analyst(s) and the identification of explicit and latent meanings in\nqualitative data. Attempting an analysis based on human interpretation with an\nLLM clearly is a provocation but also a way to learn something about how these\nsystems can or cannot be used in qualitative research. The paper presents the\nmotivations for attempting this emulation, it reflects on how the six steps to\na Thematic Analysis proposed by Braun and Clarke can at least partially be\nreproduced with the LLM and it also reflects on what are the outputs produced\nby the model. The paper used two existing datasets of open access\nsemi-structured interviews, previously analysed with Thematic Analysis by other\nresearchers. It used the previously produced analysis (and the related themes)\nto compare with the results produced by the LLM. The results show that the\nmodel can infer at least partially some of the main Themes. The objective of\nthe paper is not to replace human analysts in qualitative analysis but to learn\nif some elements of LLM data manipulation can to an extent be of support for\nqualitative research.",
        "pdf_link": "https://arxiv.org/pdf/2305.13014v4.pdf"
    },
    {
        "title": "Text-based Person Search without Parallel Image-Text Data",
        "authors": [
            "Yang Bai",
            "Jingyao Wang",
            "Min Cao",
            "Chen Chen",
            "Ziqiang Cao",
            "Liqiang Nie",
            "Min Zhang"
        ],
        "published": "2023-05-22T12:13:08Z",
        "summary": "Text-based person search (TBPS) aims to retrieve the images of the target\nperson from a large image gallery based on a given natural language\ndescription. Existing methods are dominated by training models with parallel\nimage-text pairs, which are very costly to collect. In this paper, we make the\nfirst attempt to explore TBPS without parallel image-text data ($\\mu$-TBPS), in\nwhich only non-parallel images and texts, or even image-only data, can be\nadopted. Towards this end, we propose a two-stage framework,\ngeneration-then-retrieval (GTR), to first generate the corresponding pseudo\ntext for each image and then perform the retrieval in a supervised manner. In\nthe generation stage, we propose a fine-grained image captioning strategy to\nobtain an enriched description of the person image, which firstly utilizes a\nset of instruction prompts to activate the off-the-shelf pretrained\nvision-language model to capture and generate fine-grained person attributes,\nand then converts the extracted attributes into a textual description via the\nfinetuned large language model or the hand-crafted template. In the retrieval\nstage, considering the noise interference of the generated texts for training\nmodel, we develop a confidence score-based training scheme by enabling more\nreliable texts to contribute more during the training. Experimental results on\nmultiple TBPS benchmarks (i.e., CUHK-PEDES, ICFG-PEDES and RSTPReid) show that\nthe proposed GTR can achieve a promising performance without relying on\nparallel image-text data.",
        "pdf_link": "https://arxiv.org/pdf/2305.12964v2.pdf"
    },
    {
        "title": "ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness",
        "authors": [
            "Jan Cegin",
            "Jakub Simko",
            "Peter Brusilovsky"
        ],
        "published": "2023-05-22T11:46:32Z",
        "summary": "The emergence of generative large language models (LLMs) raises the question:\nwhat will be its impact on crowdsourcing? Traditionally, crowdsourcing has been\nused for acquiring solutions to a wide variety of human-intelligence tasks,\nincluding ones involving text generation, modification or evaluation. For some\nof these tasks, models like ChatGPT can potentially substitute human workers.\nIn this study, we investigate whether this is the case for the task of\nparaphrase generation for intent classification. We apply data collection\nmethodology of an existing crowdsourcing study (similar scale, prompts and seed\ndata) using ChatGPT and Falcon-40B. We show that ChatGPT-created paraphrases\nare more diverse and lead to at least as robust models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12947v2.pdf"
    },
    {
        "title": "ExplainCPE: A Free-text Explanation Benchmark of Chinese Pharmacist Examination",
        "authors": [
            "Dongfang Li",
            "Jindi Yu",
            "Baotian Hu",
            "Zhenran Xu",
            "Min Zhang"
        ],
        "published": "2023-05-22T11:45:42Z",
        "summary": "As ChatGPT and GPT-4 spearhead the development of Large Language Models\n(LLMs), more researchers are investigating their performance across various\ntasks. But more research needs to be done on the interpretability capabilities\nof LLMs, that is, the ability to generate reasons after an answer has been\ngiven. Existing explanation datasets are mostly English-language general\nknowledge questions, which leads to insufficient thematic and linguistic\ndiversity. To address the language bias and lack of medical resources in\ngenerating rationales QA datasets, we present ExplainCPE (over 7k instances), a\nchallenging medical benchmark in Simplified Chinese. We analyzed the errors of\nChatGPT and GPT-4, pointing out the limitations of current LLMs in\nunderstanding text and computational reasoning. During the experiment, we also\nfound that different LLMs have different preferences for in-context learning.\nExplainCPE presents a significant challenge, but its potential for further\ninvestigation is promising, and it can be used to evaluate the ability of a\nmodel to generate explanations. AI safety and trustworthiness need more\nattention, and this work makes the first step to explore the medical\ninterpretability of LLMs.The dataset is available at\nhttps://github.com/HITsz-TMG/ExplainCPE.",
        "pdf_link": "https://arxiv.org/pdf/2305.12945v2.pdf"
    },
    {
        "title": "Album Storytelling with Iterative Story-aware Captioning and Large Language Models",
        "authors": [
            "Munan Ning",
            "Yujia Xie",
            "Dongdong Chen",
            "Zeyin Song",
            "Lu Yuan",
            "Yonghong Tian",
            "Qixiang Ye",
            "Li Yuan"
        ],
        "published": "2023-05-22T11:45:10Z",
        "summary": "This work studies how to transform an album to vivid and coherent stories, a\ntask we refer to as \"album storytelling\". While this task can help preserve\nmemories and facilitate experience sharing, it remains an underexplored area in\ncurrent literature. With recent advances in Large Language Models (LLMs), it is\nnow possible to generate lengthy, coherent text, opening up the opportunity to\ndevelop an AI assistant for album storytelling. One natural approach is to use\ncaption models to describe each photo in the album, and then use LLMs to\nsummarize and rewrite the generated captions into an engaging story. However,\nwe find this often results in stories containing hallucinated information that\ncontradicts the images, as each generated caption (\"story-agnostic\") is not\nalways about the description related to the whole story or miss some necessary\ninformation. To address these limitations, we propose a new iterative album\nstorytelling pipeline. Specifically, we start with an initial story and build a\nstory-aware caption model to refine the captions using the whole story as\nguidance. The polished captions are then fed into the LLMs to generate a new\nrefined story. This process is repeated iteratively until the story contains\nminimal factual errors while maintaining coherence. To evaluate our proposed\npipeline, we introduce a new dataset of image collections from vlogs and a set\nof systematic evaluation metrics. Our results demonstrate that our method\neffectively generates more accurate and engaging stories for albums, with\nenhanced coherence and vividness.",
        "pdf_link": "https://arxiv.org/pdf/2305.12943v2.pdf"
    },
    {
        "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
        "authors": [
            "Yuxin Jiang",
            "Chunkit Chan",
            "Mingyang Chen",
            "Wei Wang"
        ],
        "published": "2023-05-22T09:49:16Z",
        "summary": "The practice of transferring knowledge from a sophisticated, proprietary\nlarge language model (LLM) to a compact, open-source LLM has garnered\nconsiderable attention. Previous works have focused on a unidirectional\nknowledge distillation way by aligning the responses of the student model with\nthose of the teacher model to a set of instructions. Nevertheless, they\noverlooked the possibility of incorporating any reciprocal\n\"feedback\"--identifying challenging instructions where the student model's\nperformance falls short--to boost the student model's proficiency iteratively.\nTo this end, we propose a novel adversarial distillation framework for a more\nefficient knowledge transfer. Leveraging the versatile role adaptability of\nLLMs, we prompt the teacher model to identify \"hard\" instructions and generate\nnew \"hard\" instructions for the student model, creating a three-stage\nadversarial loop of imitation, discrimination, and generation. By applying this\nadversarial framework, we successfully transfer knowledge from ChatGPT to a\nstudent model (named Lion), using a mere 70k training data. Our results show\nthat Lion-13B not only achieves comparable open-ended generation capabilities\nto ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned\nmodels like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks\nsuch as BIG-Bench Hard (BBH) and 16.7% on AGIEval. Code and model can be found\nat https://github.com/YJiangcm/Lion.",
        "pdf_link": "https://arxiv.org/pdf/2305.12870v2.pdf"
    },
    {
        "title": "Automatic Code Summarization via ChatGPT: How Far Are We?",
        "authors": [
            "Weisong Sun",
            "Chunrong Fang",
            "Yudu You",
            "Yun Miao",
            "Yi Liu",
            "Yuekang Li",
            "Gelei Deng",
            "Shenghan Huang",
            "Yuchen Chen",
            "Quanjun Zhang",
            "Hanwei Qian",
            "Yang Liu",
            "Zhenyu Chen"
        ],
        "published": "2023-05-22T09:43:40Z",
        "summary": "To support software developers in understanding and maintaining programs,\nvarious automatic code summarization techniques have been proposed to generate\na concise natural language comment for a given code snippet. Recently, the\nemergence of large language models (LLMs) has led to a great boost in the\nperformance of natural language processing tasks. Among them, ChatGPT is the\nmost popular one which has attracted wide attention from the software\nengineering community. However, it still remains unclear how ChatGPT performs\nin (automatic) code summarization. Therefore, in this paper, we focus on\nevaluating ChatGPT on a widely-used Python dataset called CSN-Python and\ncomparing it with several state-of-the-art (SOTA) code summarization models.\nSpecifically, we first explore an appropriate prompt to guide ChatGPT to\ngenerate in-distribution comments. Then, we use such a prompt to ask ChatGPT to\ngenerate comments for all code snippets in the CSN-Python test set. We adopt\nthree widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the\nquality of the comments generated by ChatGPT and SOTA models (including NCS,\nCodeBERT, and CodeT5). The experimental results show that in terms of BLEU and\nROUGE-L, ChatGPT's code summarization performance is significantly worse than\nall three SOTA models. We also present some cases and discuss the advantages\nand disadvantages of ChatGPT in code summarization. Based on the findings, we\noutline several open challenges and opportunities in ChatGPT-based code\nsummarization.",
        "pdf_link": "https://arxiv.org/pdf/2305.12865v1.pdf"
    },
    {
        "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering",
        "authors": [
            "Vaishali Pal",
            "Andrew Yates",
            "Evangelos Kanoulas",
            "Maarten de Rijke"
        ],
        "published": "2023-05-22T08:25:15Z",
        "summary": "Recent advances in tabular question answering (QA) with large language models\nare constrained in their coverage and only answer questions over a single\ntable. However, real-world queries are complex in nature, often over multiple\ntables in a relational database or web page. Single table questions do not\ninvolve common table operations such as set operations, Cartesian products\n(joins), or nested queries. Furthermore, multi-table operations often result in\na tabular output, which necessitates table generation capabilities of tabular\nQA models. To fill this gap, we propose a new task of answering questions over\nmultiple tables. Our model, MultiTabQA, not only answers questions over\nmultiple tables, but also generalizes to generate tabular answers. To enable\neffective training, we build a pre-training dataset comprising of 132,645 SQL\nqueries and tabular answers. Further, we evaluate the generated tables by\nintroducing table-specific metrics of varying strictness assessing various\nlevels of granularity of the table structure. MultiTabQA outperforms\nstate-of-the-art single table QA models adapted to a multi-table QA setting by\nfinetuning on three datasets: Spider, Atis and GeoQuery.",
        "pdf_link": "https://arxiv.org/pdf/2305.12820v2.pdf"
    },
    {
        "title": "LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space",
        "authors": [
            "Chi Han",
            "Jialiang Xu",
            "Manling Li",
            "Yi Fung",
            "Chenkai Sun",
            "Nan Jiang",
            "Tarek Abdelzaher",
            "Heng Ji"
        ],
        "published": "2023-05-22T07:52:04Z",
        "summary": "In recent years, large language models (LMs) have achieved remarkable\nprogress across various natural language processing tasks. As pre-training and\nfine-tuning are costly and might negatively impact model performance, it is\ndesired to efficiently adapt an existing model to different conditions such as\nstyles, sentiments or narratives, when facing different audiences or scenarios.\nHowever, efficient adaptation of a language model to diverse conditions remains\nan open challenge. This work is inspired by the observation that text\nconditions are often associated with selection of certain words in a context.\nTherefore we introduce LM-Switch, a theoretically grounded, lightweight and\nsimple method for generative language model conditioning. We begin by\ninvestigating the effect of conditions in Hidden Markov Models (HMMs), and\nestablish a theoretical connection with language model. Our finding suggests\nthat condition shifts in HMMs are associated with linear transformations in\nword embeddings. LM-Switch is then designed to deploy a learnable linear factor\nin the word embedding space for language model conditioning. We show that\nLM-Switch can model diverse tasks, and achieves comparable or better\nperformance compared with state-of-the-art baselines in LM detoxification and\ngeneration control, despite requiring no more than 1% of parameters compared\nwith baselines and little extra time overhead compared with base LMs. It is\nalso able to learn from as few as a few sentences or one document. Moreover, a\nlearned LM-Switch can be transferred to other LMs of different sizes, achieving\na detoxification performance similar to the best baseline. We will make our\ncode available to the research community following publication.",
        "pdf_link": "https://arxiv.org/pdf/2305.12798v1.pdf"
    },
    {
        "title": "GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs",
        "authors": [
            "Pengcheng Jiang",
            "Cao Xiao",
            "Adam Cross",
            "Jimeng Sun"
        ],
        "published": "2023-05-22T07:35:43Z",
        "summary": "Clinical predictive models often rely on patients' electronic health records\n(EHR), but integrating medical knowledge to enhance predictions and\ndecision-making is challenging. This is because personalized predictions\nrequire personalized knowledge graphs (KGs), which are difficult to generate\nfrom patient EHR data. To address this, we propose \\textsc{GraphCare}, an\nopen-world framework that uses external KGs to improve EHR-based predictions.\nOur method extracts knowledge from large language models (LLMs) and external\nbiomedical KGs to build patient-specific KGs, which are then used to train our\nproposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare\npredictions. On two public datasets, MIMIC-III and MIMIC-IV, \\textsc{GraphCare}\nsurpasses baselines in four vital healthcare prediction tasks: mortality,\nreadmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it\nboosts AUROC by 17.6\\% and 6.6\\% for mortality and readmission, and F1-score by\n7.9\\% and 10.8\\% for LOS and drug recommendation, respectively. Notably,\n\\textsc{GraphCare} demonstrates a substantial edge in scenarios with limited\ndata availability. Our findings highlight the potential of using external KGs\nin healthcare prediction tasks and demonstrate the promise of\n\\textsc{GraphCare} in generating personalized KGs for promoting personalized\nmedicine.",
        "pdf_link": "https://arxiv.org/pdf/2305.12788v3.pdf"
    },
    {
        "title": "Explaining Emergent In-Context Learning as Kernel Regression",
        "authors": [
            "Chi Han",
            "Ziqi Wang",
            "Han Zhao",
            "Heng Ji"
        ],
        "published": "2023-05-22T06:45:02Z",
        "summary": "Large language models (LLMs) have initiated a paradigm shift in transfer\nlearning. In contrast to the classic pretraining-then-finetuning procedure, in\norder to use LLMs for downstream prediction tasks, one only needs to provide a\nfew demonstrations, known as in-context examples, without adding more or\nupdating existing model parameters. This in-context learning (ICL) capability\nof LLMs is intriguing, and it is not yet fully understood how pretrained LLMs\nacquire such capabilities. In this paper, we investigate the reason why a\ntransformer-based language model can accomplish in-context learning after\npre-training on a general language corpus by proposing one hypothesis that LLMs\ncan simulate kernel regression with internal representations when faced with\nin-context examples. More concretely, we first prove that Bayesian inference on\nin-context prompts can be asymptotically understood as kernel regression $\\hat\ny = \\sum_i y_i K(x, x_i)/\\sum_i K(x, x_i)$ as the number of in-context\ndemonstrations grows. Then, we empirically investigate the in-context behaviors\nof language models. We find that during ICL, the attention and hidden features\nin LLMs match the behaviors of a kernel regression. Finally, our theory\nprovides insights into multiple phenomena observed in the ICL field: why\nretrieving demonstrative samples similar to test samples can help, why ICL\nperformance is sensitive to the output formats, and why ICL accuracy benefits\nfrom selecting in-distribution and representative samples.",
        "pdf_link": "https://arxiv.org/pdf/2305.12766v2.pdf"
    },
    {
        "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
        "authors": [
            "Liangming Pan",
            "Xiaobao Wu",
            "Xinyuan Lu",
            "Anh Tuan Luu",
            "William Yang Wang",
            "Min-Yen Kan",
            "Preslav Nakov"
        ],
        "published": "2023-05-22T06:11:15Z",
        "summary": "Fact-checking real-world claims often requires collecting multiple pieces of\nevidence and applying complex multi-step reasoning. In this paper, we present\nProgram-Guided Fact-Checking (ProgramFC), a novel fact-checking model that\ndecomposes complex claims into simpler sub-tasks that can be solved using a\nshared library of specialized functions. We first leverage the in-context\nlearning ability of large language models to generate reasoning programs to\nguide the verification process. Afterward, we execute the program by delegating\neach sub-task to the corresponding sub-task handler. This process makes our\nmodel both explanatory and data-efficient, providing clear explanations of its\nreasoning process and requiring minimal training data. We evaluate ProgramFC on\ntwo challenging fact-checking datasets and show that it outperforms seven\nfact-checking baselines across different settings of evidence availability,\nwith explicit output programs that benefit human debugging. Our codes and data\nare publicly available at https://github.com/mbzuai-nlp/ProgramFC.",
        "pdf_link": "https://arxiv.org/pdf/2305.12744v1.pdf"
    },
    {
        "title": "Can We Edit Factual Knowledge by In-Context Learning?",
        "authors": [
            "Ce Zheng",
            "Lei Li",
            "Qingxiu Dong",
            "Yuxuan Fan",
            "Zhiyong Wu",
            "Jingjing Xu",
            "Baobao Chang"
        ],
        "published": "2023-05-22T06:07:58Z",
        "summary": "Previous studies have shown that large language models (LLMs) like GPTs store\nmassive factual knowledge in their parameters. However, the stored knowledge\ncould be false or out-dated. Traditional knowledge editing methods refine LLMs\nvia fine-tuning on texts containing specific knowledge. However, with the\nincreasing scales of LLMs, these gradient-based approaches bring large\ncomputation costs. The trend of model-as-a-service also makes it impossible to\nmodify knowledge in black-box LMs. Inspired by in-context learning (ICL), a new\nparadigm based on demonstration contexts without parameter updating, we explore\nwhether ICL can edit factual knowledge. To answer this question, we give a\ncomprehensive empirical study of ICL strategies. Experiments show that\nin-context knowledge editing (IKE), without any gradient and parameter\nupdating, achieves a competitive success rate compared to gradient-based\nmethods on GPT-J (6B) but with much fewer side effects, including less\nover-editing on similar but unrelated facts and less knowledge forgetting on\npreviously stored knowledge. We also apply the method to larger LMs with tens\nor hundreds of parameters like OPT-175B, which shows the scalability of our\nmethod. The code is available at https://github.com/Zce1112zslx/IKE.",
        "pdf_link": "https://arxiv.org/pdf/2305.12740v1.pdf"
    },
    {
        "title": "llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
        "authors": [
            "Masanori Hirano",
            "Masahiro Suzuki",
            "Hiroki Sakaji"
        ],
        "published": "2023-05-22T04:59:33Z",
        "summary": "This study constructed a Japanese chat dataset for tuning large language\nmodels (LLMs), which consist of about 8.4 million records. Recently, LLMs have\nbeen developed and gaining popularity. However, high-performing LLMs are\nusually mainly for English. There are two ways to support languages other than\nEnglish by those LLMs: constructing LLMs from scratch or tuning existing\nmodels. However, in both ways, datasets are necessary parts. In this study, we\nfocused on supporting Japanese in those LLMs and making a dataset for training\nor tuning LLMs in Japanese. The dataset we constructed consisted of various\ntasks, such as translation and knowledge tasks. In our experiment, we tuned an\nexisting LLM using our dataset and evaluated the performance qualitatively. The\nresults suggest that our dataset is possibly beneficial for LLMs. However, we\nalso revealed some difficulties in constructing LLMs in languages other than\nEnglish.",
        "pdf_link": "https://arxiv.org/pdf/2305.12720v1.pdf"
    },
    {
        "title": "Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage",
        "authors": [
            "Hanyin Shao",
            "Jie Huang",
            "Shen Zheng",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-05-22T04:30:35Z",
        "summary": "The advancement of large language models (LLMs) brings notable improvements\nacross various applications, while simultaneously raising concerns about\npotential private data exposure. One notable capability of LLMs is their\nability to form associations between different pieces of information, but this\nraises concerns when it comes to personally identifiable information (PII).\nThis paper delves into the association capabilities of language models, aiming\nto uncover the factors that influence their proficiency in associating\ninformation. Our study reveals that as models scale up, their capacity to\nassociate entities/information intensifies, particularly when target pairs\ndemonstrate shorter co-occurrence distances or higher co-occurrence\nfrequencies. However, there is a distinct performance gap when associating\ncommonsense knowledge versus PII, with the latter showing lower accuracy.\nDespite the proportion of accurately predicted PII being relatively small, LLMs\nstill demonstrate the capability to predict specific instances of email\naddresses and phone numbers when provided with appropriate prompts. These\nfindings underscore the potential risk to PII confidentiality posed by the\nevolving capabilities of LLMs, especially as they continue to expand in scale\nand power.",
        "pdf_link": "https://arxiv.org/pdf/2305.12707v2.pdf"
    },
    {
        "title": "G3Detector: General GPT-Generated Text Detector",
        "authors": [
            "Haolan Zhan",
            "Xuanli He",
            "Qiongkai Xu",
            "Yuxiang Wu",
            "Pontus Stenetorp"
        ],
        "published": "2023-05-22T03:35:00Z",
        "summary": "The burgeoning progress in the field of Large Language Models (LLMs) heralds\nsignificant benefits due to their unparalleled capacities. However, it is\ncritical to acknowledge the potential misuse of these models, which could give\nrise to a spectrum of social and ethical dilemmas. Despite numerous preceding\nefforts centered around distinguishing synthetic text, most existing detection\nsystems fail to identify data synthesized by the latest LLMs, such as ChatGPT\nand GPT-4. In response to this challenge, we introduce an unpretentious yet\npotent detection approach proficient in identifying synthetic text across a\nwide array of fields. Moreover, our detector demonstrates outstanding\nperformance uniformly across various model architectures and decoding\nstrategies. It also possesses the capability to identify text generated\nutilizing a potent detection-evasion technique. Our comprehensive research\nunderlines our commitment to boosting the robustness and efficiency of\nmachine-generated text detection mechanisms, particularly in the context of\nswiftly progressing and increasingly adaptive AI technologies.",
        "pdf_link": "https://arxiv.org/pdf/2305.12680v2.pdf"
    },
    {
        "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
        "authors": [
            "Siyu Yuan",
            "Jiangjie Chen",
            "Xuyang Ge",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "published": "2023-05-22T03:04:06Z",
        "summary": "The vital role of analogical reasoning in human cognition allows us to grasp\nnovel concepts by linking them with familiar ones through shared relational\nstructures. Despite the attention previous research has given to word\nanalogies, this work suggests that Large Language Models (LLMs) often overlook\nthe structures that underpin these analogies, raising questions about the\nefficacy of word analogies as a measure of analogical reasoning skills akin to\nhuman cognition. In response to this, our paper introduces a task of analogical\nstructure abduction, grounded in cognitive psychology, designed to abduce\nstructures that form an analogy between two systems. In support of this task,\nwe establish a benchmark called SCAR, containing 400 scientific analogies from\n13 distinct fields, tailored for evaluating analogical reasoning with structure\nabduction. The empirical evidence underlines the continued challenges faced by\nLLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need\nfor future exploration to enhance their abilities.",
        "pdf_link": "https://arxiv.org/pdf/2305.12660v2.pdf"
    },
    {
        "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning",
        "authors": [
            "Qiming Bao",
            "Alex Yuxuan Peng",
            "Zhenyun Deng",
            "Wanjun Zhong",
            "Gael Gendron",
            "Timothy Pistotti",
            "Neset Tan",
            "Nathan Young",
            "Yang Chen",
            "Yonghua Zhu",
            "Paul Denny",
            "Michael Witbrock",
            "Jiamou Liu"
        ],
        "published": "2023-05-21T23:16:26Z",
        "summary": "Combining large language models with logical reasoning enhances their\ncapacity to address problems in a robust and reliable manner. Nevertheless, the\nintricate nature of logical reasoning poses challenges to gathering reliable\ndata from the web for building comprehensive training datasets, subsequently\naffecting the performance on downstream tasks. To address this, we introduce a\nnovel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the\noriginal text into an Abstract Meaning Representation (AMR) graph, a structured\nsemantic representation that encapsulates the logic structure of the sentence,\nupon which operations are performed to generate logically modified AMR graphs.\nThe modified AMR graphs are subsequently converted back into text to create\naugmented data. Notably, our methodology is architecture-agnostic and enhances\nboth generative large language models, such as GPT-3.5 and GPT-4, through\nprompt augmentation, and discriminative large language models through\ncontrastive learning with logic-driven data augmentation. Empirical evidence\nunderscores the efficacy of our proposed method with improvement in performance\nacross seven downstream tasks, such as reading comprehension requiring logical\nreasoning, textual entailment, and natural language inference. Furthermore, our\nmethod leads on the ReClor leaderboard\n(https://eval.ai/web/challenges/challenge-page/503/leaderboard/1347). The\nsource code and data are publicly available https://bit.ly/3OWKe8r.",
        "pdf_link": "https://arxiv.org/pdf/2305.12599v4.pdf"
    },
    {
        "title": "On the Limitations of Simulating Active Learning",
        "authors": [
            "Katerina Margatina",
            "Nikolaos Aletras"
        ],
        "published": "2023-05-21T22:52:13Z",
        "summary": "Active learning (AL) is a human-and-model-in-the-loop paradigm that\niteratively selects informative unlabeled data for human annotation, aiming to\nimprove over random sampling. However, performing AL experiments with human\nannotations on-the-fly is a laborious and expensive process, thus unrealistic\nfor academic research. An easy fix to this impediment is to simulate AL, by\ntreating an already labeled and publicly available dataset as the pool of\nunlabeled data. In this position paper, we first survey recent literature and\nhighlight the challenges across all different steps within the AL loop. We\nfurther unveil neglected caveats in the experimental setup that can\nsignificantly affect the quality of AL research. We continue with an\nexploration of how the simulation setting can govern empirical findings,\narguing that it might be one of the answers behind the ever posed question\n``why do active learning algorithms sometimes fail to outperform random\nsampling?''. We argue that evaluating AL algorithms on available labeled\ndatasets might provide a lower bound as to their effectiveness in real data. We\nbelieve it is essential to collectively shape the best practices for AL\nresearch, particularly as engineering advancements in LLMs push the research\nfocus towards data-driven approaches (e.g., data efficiency, alignment,\nfairness). In light of this, we have developed guidelines for future work. Our\naim is to draw attention to these limitations within the community, in the hope\nof finding ways to address them.",
        "pdf_link": "https://arxiv.org/pdf/2305.13342v1.pdf"
    },
    {
        "title": "Has It All Been Solved? Open NLP Research Questions Not Solved by Large Language Models",
        "authors": [
            "Oana Ignat",
            "Zhijing Jin",
            "Artem Abzaliev",
            "Laura Biester",
            "Santiago Castro",
            "Naihao Deng",
            "Xinyi Gao",
            "Aylin Gunal",
            "Jacky He",
            "Ashkan Kazemi",
            "Muhammad Khalifa",
            "Namho Koh",
            "Andrew Lee",
            "Siyang Liu",
            "Do June Min",
            "Shinka Mori",
            "Joan Nwatu",
            "Veronica Perez-Rosas",
            "Siqi Shen",
            "Zekun Wang",
            "Winston Wu",
            "Rada Mihalcea"
        ],
        "published": "2023-05-21T19:06:30Z",
        "summary": "Recent progress in large language models (LLMs) has enabled the deployment of\nmany generative NLP applications. At the same time, it has also led to a\nmisleading public discourse that ``it's all been solved.'' Not surprisingly,\nthis has, in turn, made many NLP researchers -- especially those at the\nbeginning of their careers -- worry about what NLP research area they should\nfocus on. Has it all been solved, or what remaining questions can we work on\nregardless of LLMs? To address this question, this paper compiles NLP research\ndirections rich for exploration. We identify fourteen different research areas\nencompassing 45 research directions that require new research and are not\ndirectly solvable by LLMs. While we identify many research areas, many others\nexist; we do not cover areas currently addressed by LLMs, but where LLMs lag\nbehind in performance or those focused on LLM development. We welcome\nsuggestions for other research directions to include:\nhttps://bit.ly/nlp-era-llm",
        "pdf_link": "https://arxiv.org/pdf/2305.12544v2.pdf"
    },
    {
        "title": "TheoremQA: A Theorem-driven Question Answering dataset",
        "authors": [
            "Wenhu Chen",
            "Ming Yin",
            "Max Ku",
            "Pan Lu",
            "Yixin Wan",
            "Xueguang Ma",
            "Jianyu Xu",
            "Xinyi Wang",
            "Tony Xia"
        ],
        "published": "2023-05-21T17:51:35Z",
        "summary": "The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in\nsolving fundamental math problems like GSM8K by achieving over 90% accuracy.\nHowever, their capabilities to solve more challenging math problems which\nrequire domain-specific knowledge (i.e. theorem) have yet to be investigated.\nIn this paper, we introduce TheoremQA, the first theorem-driven\nquestion-answering dataset designed to evaluate AI models' capabilities to\napply theorems to solve challenging science problems. TheoremQA is curated by\ndomain experts containing 800 high-quality questions covering 350 theorems\n(e.g. Taylor's theorem, Lagrange's theorem, Huffman coding, Quantum Theorem,\nElasticity Theorem, etc) from Math, Physics, EE&CS, and Finance. We evaluate a\nwide spectrum of 16 large language and code models with different prompting\nstrategies like Chain-of-Thoughts and Program-of-Thoughts. We found that\nGPT-4's capabilities to solve these problems are unparalleled, achieving an\naccuracy of 51% with Program-of-Thoughts Prompting. All the existing\nopen-sourced models are below 15%, barely surpassing the random-guess baseline.\nGiven the diversity and broad coverage of TheoremQA, we believe it can be used\nas a better benchmark to evaluate LLMs' capabilities to solve challenging\nscience problems. The data and code are released in\nhttps://github.com/wenhuchen/TheoremQA.",
        "pdf_link": "https://arxiv.org/pdf/2305.12524v3.pdf"
    },
    {
        "title": "LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance",
        "authors": [
            "Xiao Yu",
            "Yuang Qi",
            "Kejiang Chen",
            "Guoqiang Chen",
            "Xi Yang",
            "Pengyuan Zhu",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "published": "2023-05-21T17:26:16Z",
        "summary": "Large language models (LLMs) can generate texts that carry the risk of\nvarious misuses, including plagiarism, planting fake reviews on e-commerce\nplatforms, or creating inflammatory false tweets. Detecting whether a text is\nmachine-generated has thus become increasingly important. While existing\ndetection methods exhibit superior performance, they often lack\ngeneralizability due to their heavy dependence on training data. To alleviate\nthis problem, we propose a model-related generated text detection method, the\nLLM Paternity Test (LLM-Pat). Specifically, given any candidate text\n(\\textit{child}), LLM-Pat employs an intermediary LLM (\\textit{parent}) to\nreconstruct a \\textit{sibling} text corresponding to the given text and then\nmeasures the similarity between candidate texts and their sibling texts. High\nsimilarity indicates that the candidate text is machine-generated, akin to\ngenetic traits. We have constructed datasets encompassing four scenarios:\nstudent responses in educational settings, news creation, academic paper\nwriting, and social media bots to assess the performance of LLM-Pat. The\nexperiments show that LLM-Pat outperforms the existing detection methods and is\nmore robust against paraphrasing attacks and re-translating attacks. Besides,\nLLM-Pat can also be used to trace which large language model the text was\ngenerated by. The constructed dataset and code will be released to benefit the\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2305.12519v2.pdf"
    },
    {
        "title": "Retrieving Texts based on Abstract Descriptions",
        "authors": [
            "Shauli Ravfogel",
            "Valentina Pyatkin",
            "Amir DN Cohen",
            "Avshalom Manevich",
            "Yoav Goldberg"
        ],
        "published": "2023-05-21T17:14:31Z",
        "summary": "While instruction-tuned Large Language Models (LLMs) excel at extracting\ninformation from text, they are not suitable for locating texts conforming to a\ngiven description in a large document collection (semantic retrieval).\nSimilarity search over embedding vectors does allow to perform retrieval by\nquery, but the similarity reflected in the embedding is ill-defined and\nnon-consistent, and is sub-optimal for many use cases. What, then, is a good\nquery representation for effective retrieval?\n  We identify the well defined and consistent task of retrieving sentences\nbased on abstract descriptions of their content. We demonstrate the inadequacy\nof current text embeddings and propose an alternative model that significantly\nimproves when used in standard nearest neighbor search. The model is trained\nusing positive and negative pairs sourced through prompting a LLM. While it is\neasy to source the training material from an LLM, the retrieval task cannot be\nperformed by the LLM directly. This demonstrates that data from LLMs can be\nused not only for distilling more efficient specialized models than the\noriginal LLM, but also for creating new capabilities not immediately possible\nusing the original model.",
        "pdf_link": "https://arxiv.org/pdf/2305.12517v2.pdf"
    },
    {
        "title": "GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",
        "authors": [
            "Jessica López Espejel",
            "El Hassane Ettifouri",
            "Mahaman Sanoussi Yahaya Alassan",
            "El Mehdi Chouham",
            "Walid Dahhane"
        ],
        "published": "2023-05-21T14:45:17Z",
        "summary": "Large Language Models (LLMs) have exhibited remarkable performance on various\nNatural Language Processing (NLP) tasks. However, there is a current hot debate\nregarding their reasoning capacity. In this paper, we examine the performance\nof GPT-3.5, GPT-4, and BARD models, by performing a thorough technical\nevaluation on different reasoning tasks across eleven distinct datasets. Our\npaper provides empirical evidence showcasing the superior performance of\nChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting\nthroughout almost all evaluated tasks. While the superiority of GPT-4 compared\nto GPT-3.5 might be explained by its larger size and NLP efficiency, this was\nnot evident for BARD. We also demonstrate that the three models show limited\nproficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To\nbolster our findings, we present a detailed and comprehensive analysis of the\nresults from these three models. Furthermore, we propose a set of engineered\nprompts that enhances the zero-shot setting performance of all three models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12477v2.pdf"
    },
    {
        "title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
        "authors": [
            "Xiaotian Zhang",
            "Chunyang Li",
            "Yi Zong",
            "Zhengyu Ying",
            "Liang He",
            "Xipeng Qiu"
        ],
        "published": "2023-05-21T14:39:28Z",
        "summary": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious natural language processing tasks; however, how to comprehensively and\naccurately assess their performance becomes an urgent issue to be addressed.\nThis paper introduces GAOKAO-Bench, an intuitive benchmark that employs\nquestions from the Chinese GAOKAO examination as test samples, including both\nsubjective and objective questions. To align with human examination methods, we\ndesign a method based on zero-shot settings to evaluate the performance of\nLLMs. With human evaluation, we obtain the converted total score of LLMs,\nincluding GPT-4, ChatGPT and ERNIE-Bot.Our findings reveal that LLMs have\nachieved competitive scores in Chinese GAOKAO examination, while they exhibit\nsignificant performance disparities across various subjects. We also use LLMs\nto grade the subjective questions, and find that model scores achieve a\nmoderate level of consistency with human scores. In conclusion, this research\ncontributes a robust evaluation benchmark for future large language models and\noffers valuable insights into the advantages and limitations of such models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12474v3.pdf"
    },
    {
        "title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification",
        "authors": [
            "Renliang Sun",
            "Wei Xu",
            "Xiaojun Wan"
        ],
        "published": "2023-05-21T14:03:49Z",
        "summary": "Randomly masking text spans in ordinary texts in the pre-training stage\nhardly allows models to acquire the ability to generate simple texts. It can\nhurt the performance of pre-trained models on text simplification tasks. In\nthis paper, we propose a new continued pre-training strategy to teach the\npre-trained model to generate simple texts. We continue pre-training BART, a\nrepresentative model, to obtain SimpleBART. It consistently and significantly\nimproves the results on lexical simplification, sentence simplification, and\ndocument-level simplification tasks over BART. At the end, we compare\nSimpleBART with several representative large language models (LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2305.12463v1.pdf"
    },
    {
        "title": "Evaluating Open-QA Evaluation",
        "authors": [
            "Cunxiang Wang",
            "Sirui Cheng",
            "Qipeng Guo",
            "Yuanhao Yue",
            "Bowen Ding",
            "Zhikun Xu",
            "Yidong Wang",
            "Xiangkun Hu",
            "Zheng Zhang",
            "Yue Zhang"
        ],
        "published": "2023-05-21T10:40:55Z",
        "summary": "This study focuses on the evaluation of the Open Question Answering (Open-QA)\ntask, which can directly estimate the factuality of large language models\n(LLMs). Current automatic evaluation methods have shown limitations, indicating\nthat human evaluation still remains the most reliable approach. We introduce a\nnew task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset\nEVOUNA, designed to assess the accuracy of AI-generated answers in relation to\nstandard answers within Open-QA. Our evaluation of these methods utilizes\nhuman-annotated results to measure their performance. Specifically, the work\ninvestigates methods that show high correlation with human evaluations, deeming\nthem more reliable. We also discuss the pitfalls of current methods and methods\nto improve LLM-based evaluators. We believe this new QA-Eval task and\ncorresponding dataset EVOUNA will facilitate the development of more effective\nautomatic evaluation tools and prove valuable for future research in this area.\nAll resources are available at \\url{https://github.com/wangcunxiang/QA-Eval}\nand it is under the Apache-2.0 License.",
        "pdf_link": "https://arxiv.org/pdf/2305.12421v4.pdf"
    },
    {
        "title": "Language Knowledge-Assisted Representation Learning for Skeleton-Based Action Recognition",
        "authors": [
            "Haojun Xu",
            "Yan Gao",
            "Zheng Hui",
            "Jie Li",
            "Xinbo Gao"
        ],
        "published": "2023-05-21T08:29:16Z",
        "summary": "How humans understand and recognize the actions of others is a complex\nneuroscientific problem that involves a combination of cognitive mechanisms and\nneural networks. Research has shown that humans have brain areas that recognize\nactions that process top-down attentional information, such as the\ntemporoparietal association area. Also, humans have brain regions dedicated to\nunderstanding the minds of others and analyzing their intentions, such as the\nmedial prefrontal cortex of the temporal lobe. Skeleton-based action\nrecognition creates mappings for the complex connections between the human\nskeleton movement patterns and behaviors. Although existing studies encoded\nmeaningful node relationships and synthesized action representations for\nclassification with good results, few of them considered incorporating a priori\nknowledge to aid potential representation learning for better performance.\nLA-GCN proposes a graph convolution network using large-scale language models\n(LLM) knowledge assistance. First, the LLM knowledge is mapped into a priori\nglobal relationship (GPR) topology and a priori category relationship (CPR)\ntopology between nodes. The GPR guides the generation of new \"bone\"\nrepresentations, aiming to emphasize essential node information from the data\nlevel. The CPR mapping simulates category prior knowledge in human brain\nregions, encoded by the PC-AC module and used to add additional\nsupervision-forcing the model to learn class-distinguishable features. In\naddition, to improve information transfer efficiency in topology modeling, we\npropose multi-hop attention graph convolution. It aggregates each node's\nk-order neighbor simultaneously to speed up model convergence. LA-GCN reaches\nstate-of-the-art on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.12398v1.pdf"
    },
    {
        "title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
        "authors": [
            "Jiuzhou Han",
            "Nigel Collier",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "published": "2023-05-21T08:11:24Z",
        "summary": "Large language models (LLMs) have shown great abilities of solving various\nnatural language tasks in different domains. Due to the training objective of\nLLMs and their pre-training data, LLMs are not very well equipped for tasks\ninvolving structured data generation. We propose a framework, Prompting with\nIterative Verification (PiVe), to improve graph-based generative capability of\nLLMs. We show how a small language model could be trained to act as a verifier\nmodule for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively\nimprove its performance via fine-grained corrective instructions. We also show\nhow the verifier module could apply iterative corrections offline for a more\ncost-effective solution to the text-to-graph generation task. Experiments on\nthree graph-based datasets show consistent improvement gained via PiVe.\nAdditionally, we create GenWiki-HIQ and highlight that the verifier module can\nbe used as a data augmentation tool to help improve the quality of\nautomatically generated parallel text-graph datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.12392v2.pdf"
    },
    {
        "title": "Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models",
        "authors": [
            "Yijia Zhang",
            "Lingran Zhao",
            "Shijie Cao",
            "Wenqiang Wang",
            "Ting Cao",
            "Fan Yang",
            "Mao Yang",
            "Shanghang Zhang",
            "Ningyi Xu"
        ],
        "published": "2023-05-21T05:28:37Z",
        "summary": "Efficient deployment of large language models (LLMs) necessitates low-bit\nquantization to minimize model size and inference cost. While low-bit integer\nformats (e.g., INT8/INT4) have been the conventional choice, emerging low-bit\nfloating-point formats (e.g., FP8/FP4) offer a compelling alternative and are\ngaining support from cutting-edge hardware, such as NVIDIA's H100 GPU. However,\nthe superiority of low-bit INT versus FP formats for quantization on LLMs\nremains unclear. In this study, we conduct a comparative analysis of INT and FP\nquantization with the same bit-width, revealing that the optimal quantization\nformat varies across different layers due to the complexity and diversity of\ntensor distribution. Consequently, we advocate the Mixture of Formats\nQuantization (MoFQ), which selects the optimal format on a layer-wise basis.\nThis simple yet effective approach achieves state-of-the-art results in both\nweight-only (W-only) and weight-activation (WA) post-training quantization\nscenarios when tested on LLaMA across various tasks. In 4-bit W-only\nquantization, MoFQ surpasses GPTQ without complex hyperparameter tuning and\nwith an order of magnitude faster quantization speed. While in 8-bit WA\nquantization, MoFQ significantly outperforms INT/FP-only methods, achieving\nperformance close to the full precision model. Notably, MoFQ incurs no hardware\noverhead compared to INT/FP-only quantization, as the bit-width remains\nunchanged.",
        "pdf_link": "https://arxiv.org/pdf/2305.12356v1.pdf"
    },
    {
        "title": "Task-agnostic Distillation of Encoder-Decoder Language Models",
        "authors": [
            "Chen Zhang",
            "Yang Yang",
            "Jingang Wang",
            "Dawei Song"
        ],
        "published": "2023-05-21T03:35:45Z",
        "summary": "Finetuning pretrained language models (LMs) have enabled appealing\nperformance on a diverse array of tasks. The intriguing task-agnostic property\nhas driven a shifted focus from task-specific to task-agnostic distillation of\nLMs. While task-agnostic, compute-efficient, performance-preserved LMs can be\nyielded by task-agnostic distillation, previous studies mainly sit in\ndistillation of either encoder-only LMs (e.g., BERT) or decoder-only ones\n(e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g.,\nT5) can posit very distinguished behaviors. Frustratingly, we discover that\nexisting task-agnostic distillation methods can fail to handle the distillation\nof encoder-decoder LMs. To the demand, we explore a few paths and uncover a\npath named as MiniEnD that successfully tackles the distillation of\nencoder-decoder LMs in a task-agnostic fashion. We examine MiniEnD on language\nunderstanding and abstractive summarization. The results showcase that MiniEnD\nis generally effective and is competitive compared to other alternatives. We\nfurther scale MiniEnD up to distillation of 3B encoder-decoder language models\nwith interpolated distillation. The results imply the opportunities and\nchallenges in distilling large language models (e.g., LLaMA).",
        "pdf_link": "https://arxiv.org/pdf/2305.12330v1.pdf"
    },
    {
        "title": "Gene Set Summarization using Large Language Models",
        "authors": [
            "Marcin P. Joachimiak",
            "J. Harry Caufield",
            "Nomi L. Harris",
            "Hyeongsik Kim",
            "Christopher J. Mungall"
        ],
        "published": "2023-05-21T02:06:33Z",
        "summary": "Molecular biologists frequently interpret gene lists derived from\nhigh-throughput experiments and computational analysis. This is typically done\nas a statistical enrichment analysis that measures the over- or\nunder-representation of biological function terms associated with genes or\ntheir properties, based on curated assertions from a knowledge base (KB) such\nas the Gene Ontology (GO). Interpreting gene lists can also be framed as a\ntextual summarization task, enabling the use of Large Language Models (LLMs),\npotentially utilizing scientific texts directly and avoiding reliance on a KB.\n  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language\nDescriptions of Controlled Terms for Ontology Reporting), a method that uses\nGPT models to perform gene set function summarization as a complement to\nstandard enrichment analysis. This method can use different sources of gene\nfunctional information: (1) structured text derived from curated ontological KB\nannotations, (2) ontology-free narrative gene summaries, or (3) direct model\nretrieval.\n  We demonstrate that these methods are able to generate plausible and\nbiologically valid summary GO term lists for gene sets. However, GPT-based\napproaches are unable to deliver reliable scores or p-values and often return\nterms that are not statistically significant. Crucially, these methods were\nrarely able to recapitulate the most precise and informative term from standard\nenrichment, likely due to an inability to generalize and reason using an\nontology. Results are highly nondeterministic, with minor variations in prompt\nresulting in radically different term lists. Our results show that at this\npoint, LLM-based methods are unsuitable as a replacement for standard term\nenrichment analysis and that manual curation of ontological assertions remains\nnecessary.",
        "pdf_link": "https://arxiv.org/pdf/2305.13338v2.pdf"
    },
    {
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
        "authors": [
            "Liangming Pan",
            "Alon Albalak",
            "Xinyi Wang",
            "William Yang Wang"
        ],
        "published": "2023-05-20T22:25:38Z",
        "summary": "Large Language Models (LLMs) have shown human-like reasoning abilities but\nstill struggle with complex logical problems. This paper introduces a novel\nframework, Logic-LM, which integrates LLMs with symbolic solvers to improve\nlogical problem-solving. Our method first utilizes LLMs to translate a natural\nlanguage problem into a symbolic formulation. Afterward, a deterministic\nsymbolic solver performs inference on the formulated problem. We also introduce\na self-refinement module, which utilizes the symbolic solver's error messages\nto revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on\nfive logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO,\nLogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant\nperformance boost of 39.2% over using LLM alone with standard prompting and\n18.4% over LLM with chain-of-thought prompting. Our findings suggest that\nLogic-LM, by combining LLMs with symbolic logic, offers a promising avenue for\nfaithful logical reasoning. Code and data are publicly available at\nhttps://github.com/teacherpeterpan/Logic-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.12295v2.pdf"
    },
    {
        "title": "Tweetorial Hooks: Generative AI Tools to Motivate Science on Social Media",
        "authors": [
            "Tao Long",
            "Dorothy Zhang",
            "Grace Li",
            "Batool Taraif",
            "Samia Menon",
            "Kynnedy Simone Smith",
            "Sitong Wang",
            "Katy Ilonka Gero",
            "Lydia B. Chilton"
        ],
        "published": "2023-05-20T18:47:40Z",
        "summary": "Communicating science and technology is essential for the public to\nunderstand and engage in a rapidly changing world. Tweetorials are an emerging\nphenomenon where experts explain STEM topics on social media in creative and\nengaging ways. However, STEM experts struggle to write an engaging \"hook\" in\nthe first tweet that captures the reader's attention. We propose methods to use\nlarge language models (LLMs) to help users scaffold their process of writing a\nrelatable hook for complex scientific topics. We demonstrate that LLMs can help\nwriters find everyday experiences that are relatable and interesting to the\npublic, avoid jargon, and spark curiosity. Our evaluation shows that the system\nreduces cognitive load and helps people write better hooks. Lastly, we discuss\nthe importance of interactivity with LLMs to preserve the correctness,\neffectiveness, and authenticity of the writing.",
        "pdf_link": "https://arxiv.org/pdf/2305.12265v2.pdf"
    },
    {
        "title": "Collaborative Development of NLP models",
        "authors": [
            "Fereshte Khani",
            "Marco Tulio Ribeiro"
        ],
        "published": "2023-05-20T15:55:39Z",
        "summary": "Despite substantial advancements, Natural Language Processing (NLP) models\noften require post-training adjustments to enforce business rules, rectify\nundesired behavior, and align with user values. These adjustments involve\noperationalizing \"concepts\"--dictating desired model responses to certain\ninputs. However, it's difficult for a single entity to enumerate and define all\npossible concepts, indicating a need for a multi-user, collaborative model\nalignment framework. Moreover, the exhaustive delineation of a concept is\nchallenging, and an improper approach can create shortcuts or interfere with\noriginal data or other concepts.\n  To address these challenges, we introduce CoDev, a framework that enables\nmulti-user interaction with the model, thereby mitigating individual\nlimitations. CoDev aids users in operationalizing their concepts using Large\nLanguage Models, and relying on the principle that NLP models exhibit simpler\nbehaviors in local regions. Our main insight is learning a \\emph{local} model\nfor each concept, and a \\emph{global} model to integrate the original data with\nall concepts. We then steer a large language model to generate instances within\nconcept boundaries where local and global disagree. Our experiments show CoDev\nis effective at helping multiple users operationalize concepts and avoid\ninterference for a variety of scenarios, tasks, and models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12219v2.pdf"
    },
    {
        "title": "VNHSGE: VietNamese High School Graduation Examination Dataset for Large Language Models",
        "authors": [
            "Xuan-Quy Dao",
            "Ngoc-Bich Le",
            "The-Duy Vo",
            "Xuan-Dung Phan",
            "Bac-Bien Ngo",
            "Van-Tien Nguyen",
            "Thi-My-Thanh Nguyen",
            "Hong-Phuoc Nguyen"
        ],
        "published": "2023-05-20T14:13:08Z",
        "summary": "The VNHSGE (VietNamese High School Graduation Examination) dataset, developed\nexclusively for evaluating large language models (LLMs), is introduced in this\narticle. The dataset, which covers nine subjects, was generated from the\nVietnamese National High School Graduation Examination and comparable tests.\n300 literary essays have been included, and there are over 19,000\nmultiple-choice questions on a range of topics. The dataset assesses LLMs in\nmultitasking situations such as question answering, text generation, reading\ncomprehension, visual question answering, and more by including both textual\ndata and accompanying images. Using ChatGPT and BingChat, we evaluated LLMs on\nthe VNHSGE dataset and contrasted their performance with that of Vietnamese\nstudents to see how well they performed. The results show that ChatGPT and\nBingChat both perform at a human level in a number of areas, including\nliterature, English, history, geography, and civics education. They still have\nspace to grow, though, especially in the areas of mathematics, physics,\nchemistry, and biology. The VNHSGE dataset seeks to provide an adequate\nbenchmark for assessing the abilities of LLMs with its wide-ranging coverage\nand variety of activities. We intend to promote future developments in the\ncreation of LLMs by making this dataset available to the scientific community,\nespecially in resolving LLMs' limits in disciplines involving mathematics and\nthe natural sciences.",
        "pdf_link": "https://arxiv.org/pdf/2305.12199v1.pdf"
    },
    {
        "title": "The Case Against Explainability",
        "authors": [
            "Hofit Wasserman Rozen",
            "Niva Elkin-Koren",
            "Ran Gilad-Bachrach"
        ],
        "published": "2023-05-20T10:56:19Z",
        "summary": "As artificial intelligence (AI) becomes more prevalent there is a growing\ndemand from regulators to accompany decisions made by such systems with\nexplanations. However, a persistent gap exists between the need to execute a\nmeaningful right to explanation vs. the ability of Machine Learning systems to\ndeliver on such a legal requirement. The regulatory appeal towards \"a right to\nexplanation\" of AI systems can be attributed to the significant role of\nexplanations, part of the notion called reason-giving, in law. Therefore, in\nthis work we examine reason-giving's purposes in law to analyze whether reasons\nprovided by end-user Explainability can adequately fulfill them.\n  We find that reason-giving's legal purposes include: (a) making a better and\nmore just decision, (b) facilitating due-process, (c) authenticating human\nagency, and (d) enhancing the decision makers' authority. Using this\nmethodology, we demonstrate end-user Explainabilty's inadequacy to fulfil\nreason-giving's role in law, given reason-giving's functions rely on its impact\nover a human decision maker. Thus, end-user Explainability fails, or is\nunsuitable, to fulfil the first, second and third legal function. In contrast\nwe find that end-user Explainability excels in the fourth function, a quality\nwhich raises serious risks considering recent end-user Explainability research\ntrends, Large Language Models' capabilities, and the ability to manipulate\nend-users by both humans and machines. Hence, we suggest that in some cases the\nright to explanation of AI systems could bring more harm than good to end\nusers. Accordingly, this study carries some important policy ramifications, as\nit calls upon regulators and Machine Learning practitioners to reconsider the\nwidespread pursuit of end-user Explainability and a right to explanation of AI\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2305.12167v1.pdf"
    },
    {
        "title": "LogiCoT: Logical Chain-of-Thought Instruction-Tuning",
        "authors": [
            "Hanmeng Liu",
            "Zhiyang Teng",
            "Leyang Cui",
            "Chaoli Zhang",
            "Qiji Zhou",
            "Yue Zhang"
        ],
        "published": "2023-05-20T09:23:09Z",
        "summary": "Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive\nchain-of-thought reasoning ability. Recent work on self-instruction tuning,\nsuch as Alpaca, has focused on enhancing the general proficiency of models.\nThese instructions enable the model to achieve performance comparable to\nGPT-3.5 on general tasks like open-domain text generation and paraphrasing.\nHowever, they fall short of helping the model handle complex reasoning tasks.\nTo bridge the gap, this paper presents LogiCoT, a new instruction-tuning\ndataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the\nprocess of harvesting instructions for prompting GPT-4 to generate\nchain-of-thought rationales. LogiCoT serves as an instruction set for teaching\nmodels of logical reasoning and elicits general reasoning skills.",
        "pdf_link": "https://arxiv.org/pdf/2305.12147v2.pdf"
    },
    {
        "title": "LMs: Understanding Code Syntax and Semantics for Code Analysis",
        "authors": [
            "Wei Ma",
            "Shangqing Liu",
            "Zhihao Lin",
            "Wenhan Wang",
            "Qiang Hu",
            "Ye Liu",
            "Cen Zhang",
            "Liming Nie",
            "Li Li",
            "Yang Liu"
        ],
        "published": "2023-05-20T08:43:49Z",
        "summary": "Large language models~(LLMs) demonstrate significant potential to\nrevolutionize software engineering (SE) by exhibiting outstanding performance\nin SE tasks such as code and document generation. However, the high reliability\nand risk control requirements in software engineering raise concerns about the\nlack of interpretability of LLMs. To address this concern, we conducted a study\nto evaluate the capabilities of LLMs and their limitations for code analysis in\nSE. We break down the abilities needed for artificial intelligence~(AI) models\nto address SE tasks related to code analysis into three categories: 1) syntax\nunderstanding, 2) static behavior understanding, and 3) dynamic behavior\nunderstanding. Our investigation focused on the ability of LLMs to comprehend\ncode syntax and semantic structures, which include abstract syntax trees (AST),\ncontrol flow graphs (CFG), and call graphs (CG). We employed four\nstate-of-the-art foundational models, GPT4, GPT3.5, StarCoder and\nCodeLlama-13b-instruct. We assessed the performance of LLMs on cross-language\ntasks involving C, Java, Python, and Solidity.\n  Our findings revealed that while LLMs have a talent for understanding code\nsyntax, they struggle with comprehending code semantics, particularly dynamic\nsemantics. We conclude that LLMs possess capabilities similar to an Abstract\nSyntax Tree (AST) parser, demonstrating initial competencies in static code\nanalysis. Furthermore, our study highlights that LLMs are susceptible to\nhallucinations when interpreting code semantic structures and fabricating\nnonexistent facts. These results indicate the need to explore methods to verify\nthe correctness of LLM output to ensure its dependability in SE. More\nimportantly, our study provides an initial answer to why the codes generated by\nLLM are usually syntax-correct but vulnerable.",
        "pdf_link": "https://arxiv.org/pdf/2305.12138v4.pdf"
    },
    {
        "title": "Can Public Large Language Models Help Private Cross-device Federated Learning?",
        "authors": [
            "Boxin Wang",
            "Yibo Jacky Zhang",
            "Yuan Cao",
            "Bo Li",
            "H. Brendan McMahan",
            "Sewoong Oh",
            "Zheng Xu",
            "Manzil Zaheer"
        ],
        "published": "2023-05-20T07:55:58Z",
        "summary": "We study (differentially) private federated learning (FL) of language models.\nThe language models in cross-device FL are relatively small, which can be\ntrained with meaningful formal user-level differential privacy (DP) guarantees\nwhen massive parallelism in training is enabled by the participation of a\nmoderate size of users. Recently, public data has been used to improve\nprivacy-utility trade-offs for both large and small language models. In this\nwork, we provide a systematic study of using large-scale public data and LLMs\nto help differentially private training of on-device FL models, and further\nimprove the privacy-utility tradeoff by techniques of distillation. Moreover,\nwe propose a novel distribution matching algorithm with theoretical grounding\nto sample public data close to private data distribution, which significantly\nimproves the sample efficiency of (pre-)training on public data. The proposed\nmethod is efficient and effective for training private model by taking\nadvantage of public data, especially for customized on-device architectures\nthat do not have ready-to-use pre-trained models.",
        "pdf_link": "https://arxiv.org/pdf/2305.12132v1.pdf"
    },
    {
        "title": "Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?",
        "authors": [
            "Neeraj Varshney",
            "Mihir Parmar",
            "Nisarg Patel",
            "Divij Handa",
            "Sayantan Sarkar",
            "Man Luo",
            "Chitta Baral"
        ],
        "published": "2023-05-20T05:20:37Z",
        "summary": "Pre-training on large corpora of text enables the language models to acquire\na vast amount of factual and commonsense knowledge which allows them to achieve\nremarkable performance on a variety of language understanding tasks. They\ntypically acquire this knowledge by learning from the pre-training text and\ncapturing certain patterns from it. However, real-world settings often present\nscenarios that do not abide by these patterns i.e. scenarios that break the\ncommon assumptions. Can state-of-the-art NLP models correctly reason over the\ncontexts of such scenarios?\n  Addressing the above question, in this paper, we investigate the ability of\nmodels to correctly reason over contexts that break the common assumptions. To\nthis end, we first systematically create evaluation data in which each data\ninstance consists of (a) a common assumption, (b) a context that follows the\nassumption, (c) a context that breaks the assumption, and (d) questions based\non the contexts. Then, through evaluations on multiple models including GPT-3\nand Flan T5, we show that while doing fairly well on contexts that follow the\ncommon assumptions, the models struggle to correctly reason over contexts that\nbreak those assumptions. Specifically, the performance gap is as high as 20%\nabsolute points. Furthermore, we thoroughly analyze these results revealing\nseveral interesting findings. We believe our work and findings will encourage\nand facilitate further research in developing more robust models that can also\nreliably reason over contexts that break the common assumptions. Data is\navailable at \\url{https://github.com/nrjvarshney/break_the_common_assumptions}.",
        "pdf_link": "https://arxiv.org/pdf/2305.12096v1.pdf"
    },
    {
        "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
        "authors": [
            "Wenyue Hua",
            "Yingqiang Ge",
            "Shuyuan Xu",
            "Jianchao Ji",
            "Yongfeng Zhang"
        ],
        "published": "2023-05-20T04:32:59Z",
        "summary": "Recent advancements in foundation models such as large language models (LLM)\nhave propelled them to the forefront of recommender systems (RS). Moreover,\nfairness in RS is critical since many users apply it for decision-making and\ndemand fulfillment. However, at present, there is a lack of understanding\nregarding the level of fairness exhibited by recommendation foundation models\nand the appropriate methods for equitably treating different groups of users in\nfoundation models. In this paper, we focus on user-side unfairness problem and\nshow through a thorough examination that there is unfairness involved in LLMs\nthat lead to unfair recommendation results. To eliminate bias from LLM for\nfairness-aware recommendation, we introduce a novel Unbiased P5 (UP5)\nfoundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP\nincludes two sub-modules: a personalized prefix prompt that enhances fairness\nwith respect to individual sensitive attributes, and a Prompt Mixture that\nintegrates multiple counterfactually-fair prompts for a set of sensitive\nattributes. Experiments are conducted on two real-world datasets, MovieLens-1M\nand Insurance, and results are compared with both matching-based and\nsequential-based fairness-aware recommendation models. The results show that\nUP5 achieves better recommendation performance and meanwhile exhibits a high\nlevel of fairness.",
        "pdf_link": "https://arxiv.org/pdf/2305.12090v1.pdf"
    },
    {
        "title": "MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement",
        "authors": [
            "Zifeng Wang",
            "Chufan Gao",
            "Cao Xiao",
            "Jimeng Sun"
        ],
        "published": "2023-05-20T03:37:09Z",
        "summary": "Tabular data prediction has been employed in medical applications such as\npatient health risk prediction. However, existing methods usually revolve\naround the algorithm design while overlooking the significance of data\nengineering. Medical tabular datasets frequently exhibit significant\nheterogeneity across different sources, with limited sample sizes per source.\nAs such, previous predictors are often trained on manually curated small\ndatasets that struggle to generalize across different tabular datasets during\ninference. This paper proposes to scale medical tabular data predictors\n(MediTab) to various tabular inputs with varying features. The method uses a\ndata engine that leverages large language models (LLMs) to consolidate tabular\nsamples to overcome the barrier across tables with distinct schema. It also\naligns out-domain data with the target task using a \"learn, annotate, and\nrefinement\" pipeline. The expanded training data then enables the pre-trained\nMediTab to infer for arbitrary tabular input in the domain without fine-tuning,\nresulting in significant improvements over supervised baselines: it reaches an\naverage ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3\ntrial outcome prediction datasets, respectively. In addition, MediTab exhibits\nimpressive zero-shot performances: it outperforms supervised XGBoost models by\n8.9% and 17.2% on average in two prediction tasks, respectively. The code is\navailable at https://github.com/RyanWangZf/MediTab.",
        "pdf_link": "https://arxiv.org/pdf/2305.12081v2.pdf"
    },
    {
        "title": "DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining",
        "authors": [
            "Weifeng Jiang",
            "Qianren Mao",
            "Chenghua Lin",
            "Jianxin Li",
            "Ting Deng",
            "Weiyi Yang",
            "Zheng Wang"
        ],
        "published": "2023-05-20T03:23:16Z",
        "summary": "Many text mining models are constructed by fine-tuning a large deep\npre-trained language model (PLM) in downstream tasks. However, a significant\nchallenge nowadays is maintaining performance when we use a lightweight model\nwith limited labelled samples. We present DisCo, a semi-supervised learning\n(SSL) framework for fine-tuning a cohort of small student models generated from\na large PLM using knowledge distillation. Our key insight is to share\ncomplementary knowledge among distilled student cohorts to promote their SSL\neffectiveness. DisCo employs a novel co-training technique to optimize a cohort\nof multiple small student models by promoting knowledge sharing among students\nunder diversified views: model views produced by different distillation\nstrategies and data views produced by various input augmentations. We evaluate\nDisCo on both semi-supervised text classification and extractive summarization\ntasks. Experimental results show that DisCo can produce student models that are\n7.6 times smaller and 4.8 times faster in inference than the baseline PLMs\nwhile maintaining comparable performance. We also show that DisCo-generated\nstudent models outperform the similar-sized models elaborately tuned in\ndistinct tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.12074v3.pdf"
    },
    {
        "title": "AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation",
        "authors": [
            "Vijayaraghavan Murali",
            "Chandra Maddila",
            "Imad Ahmad",
            "Michael Bolin",
            "Daniel Cheng",
            "Negar Ghorbani",
            "Renuka Fernandez",
            "Nachiappan Nagappan",
            "Peter C. Rigby"
        ],
        "published": "2023-05-20T00:45:15Z",
        "summary": "Generative LLMs have been shown to effectively power AI-based code authoring\ntools that can suggest entire statements or blocks of code during code\nauthoring. In this paper we present CodeCompose, an AI-assisted code authoring\ntool developed and deployed at Meta internally. CodeCompose is based on the\nInCoder LLM that merges generative capabilities with bi-directionality. We have\nscaled up CodeCompose to serve tens of thousands of developers at Meta, across\n9 programming languages and several coding surfaces. We present our experience\nin making design decisions about the model and system architecture for\nCodeCompose that addresses these challenges.\n  To release a LLM model at this scale, we needed to first ensure that it is\nsufficiently accurate. In a random sample of 20K source code files, depending\non the language, we are able to reproduce hidden lines between 40% and 58% of\nthe time, an improvement of 1.4x and 4.1x over a model trained only on public\ndata.\n  We gradually rolled CodeCompose out to developers. At the time of this\nwriting, 16K developers have used it with 8% of their code coming directly from\nCodeCompose.\n  To triangulate our numerical findings, we conduct a thematic analysis on the\nfeedback from 70 developers. We find that 91.5% of the feedback is positive,\nwith the most common themes being discovering APIs, dealing with boilerplate\ncode, and accelerating coding. Meta continues to integrate this feedback into\nCodeCompose.",
        "pdf_link": "https://arxiv.org/pdf/2305.12050v2.pdf"
    },
    {
        "title": "Clinical Camel: An Open Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding",
        "authors": [
            "Augustin Toma",
            "Patrick R. Lawler",
            "Jimmy Ba",
            "Rahul G. Krishnan",
            "Barry B. Rubin",
            "Bo Wang"
        ],
        "published": "2023-05-19T23:07:09Z",
        "summary": "We present Clinical Camel, an open large language model (LLM) explicitly\ntailored for clinical research. Fine-tuned from LLaMA-2 using QLoRA, Clinical\nCamel achieves state-of-the-art performance across medical benchmarks among\nopenly available medical LLMs. Leveraging efficient single-GPU training,\nClinical Camel surpasses GPT-3.5 in five-shot evaluations on all assessed\nbenchmarks, including 64.3% on the USMLE Sample Exam (compared to 58.5% for\nGPT-3.5), 77.9% on PubMedQA (compared to 60.2%), 60.7% on MedQA (compared to\n53.6%), and 54.2% on MedMCQA (compared to 51.0%). In addition to these\nbenchmarks, Clinical Camel demonstrates its broader capabilities, such as\nsynthesizing plausible clinical notes. This work introduces dialogue-based\nknowledge encoding, a novel method to synthesize conversational data from dense\nmedical texts. While benchmark results are encouraging, extensive and rigorous\nhuman evaluation across diverse clinical scenarios is imperative to ascertain\nsafety before implementation. By openly sharing Clinical Camel, we hope to\nfoster transparent and collaborative research, working towards the safe\nintegration of LLMs within the healthcare domain. Significant challenges\nconcerning reliability, bias, and the potential for outdated knowledge persist.\nNonetheless, the transparency provided by an open approach reinforces the\nscientific rigor essential for future clinical applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.12031v2.pdf"
    },
    {
        "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
        "authors": [
            "Badr AlKhamissi",
            "Siddharth Verma",
            "Ping Yu",
            "Zhijing Jin",
            "Asli Celikyilmaz",
            "Mona Diab"
        ],
        "published": "2023-05-19T20:58:22Z",
        "summary": "In this paper, we conduct a thorough investigation into the reasoning\ncapabilities of Large Language Models (LLMs), focusing specifically on the Open\nPretrained Transformers (OPT) models as a representative of such models. Our\nstudy entails finetuning three different sizes of OPT on a carefully curated\nreasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned\nwithout explanations, and OPT-RE, finetuned with explanations. We then evaluate\nall models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS\nbenchmark, covering 26 distinct reasoning skills, utilizing three prompting\ntechniques. Through a comprehensive grid of 27 configurations and 6,156 test\nevaluations, we investigate the dimensions of finetuning, prompting, and scale\nto understand the role of explanations on different reasoning skills. Our\nfindings reveal that having explanations in the fewshot exemplar has no\nsignificant impact on the model's performance when the model is finetuned,\nwhile positively affecting the non-finetuned counterpart. Moreover, we observe\na slight yet consistent increase in classification accuracy as we incorporate\nexplanations during prompting and finetuning, respectively. Finally, we offer\ninsights on which skills benefit the most from incorporating explanations\nduring finetuning and prompting, such as Numerical (+20.4%) and Analogical\n(+13.9%) reasoning, as well as skills that exhibit negligible or negative\neffects.",
        "pdf_link": "https://arxiv.org/pdf/2305.12001v2.pdf"
    },
    {
        "title": "Deep Learning Approaches to Lexical Simplification: A Survey",
        "authors": [
            "Kai North",
            "Tharindu Ranasinghe",
            "Matthew Shardlow",
            "Marcos Zampieri"
        ],
        "published": "2023-05-19T20:56:22Z",
        "summary": "Lexical Simplification (LS) is the task of replacing complex for simpler\nwords in a sentence whilst preserving the sentence's original meaning. LS is\nthe lexical component of Text Simplification (TS) with the aim of making texts\nmore accessible to various target populations. A past survey (Paetzold and\nSpecia, 2017) has provided a detailed overview of LS. Since this survey,\nhowever, the AI/NLP community has been taken by storm by recent advances in\ndeep learning, particularly with the introduction of large language models\n(LLM) and prompt learning. The high performance of these models sparked renewed\ninterest in LS. To reflect these recent advances, we present a comprehensive\nsurvey of papers published between 2017 and 2023 on LS and its sub-tasks with a\nspecial focus on deep learning. We also present benchmark datasets for the\nfuture development of LS systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.12000v1.pdf"
    },
    {
        "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
        "authors": [
            "Masahiro Kaneko",
            "Naoaki Okazaki"
        ],
        "published": "2023-05-19T17:51:05Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious tasks and gained significant attention. LLMs are also used for local\nsequence transduction tasks, including grammatical error correction (GEC) and\nformality style transfer, where most tokens in a source text are kept\nunchanged. However, the models that generate all target tokens in such tasks\nhave a tendency to simply copy the input text as is, without making needed\nchanges, because the difference between input and output texts is minimal in\nthe training data. This is also inefficient because the computational cost\ngrows quadratically with the target sequence length with Transformer. This\npaper proposes predicting edit spans for the source text for local sequence\ntransduction tasks. Representing an edit span with a position of the source\ntext and corrected tokens, we can reduce the length of the target sequence and\nthe computational cost for inference. We apply instruction tuning for LLMs on\nthe supervision data of edit spans. Experiments show that the proposed method\nachieves comparable performance to the baseline in four tasks, paraphrasing,\nformality style transfer, GEC, and text simplification, despite reducing the\nlength of the target text by as small as 21%. Furthermore, we report that the\ntask-specific fine-tuning with the proposed method achieved state-of-the-art\nperformance in the four tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.11862v2.pdf"
    },
    {
        "title": "How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings",
        "authors": [
            "Shuaichen Chang",
            "Eric Fosler-Lussier"
        ],
        "published": "2023-05-19T17:43:58Z",
        "summary": "Large language models (LLMs) with in-context learning have demonstrated\nremarkable capability in the text-to-SQL task. Previous research has prompted\nLLMs with various demonstration-retrieval strategies and intermediate reasoning\nsteps to enhance the performance of LLMs. However, those works often employ\nvaried strategies when constructing the prompt text for text-to-SQL inputs,\nsuch as databases and demonstration examples. This leads to a lack of\ncomparability in both the prompt constructions and their primary contributions.\nFurthermore, selecting an effective prompt construction has emerged as a\npersistent problem for future research. To address this limitation, we\ncomprehensively investigate the impact of prompt constructions across various\nsettings and provide insights into prompt constructions for future text-to-SQL\nstudies.",
        "pdf_link": "https://arxiv.org/pdf/2305.11853v3.pdf"
    },
    {
        "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
        "authors": [
            "Hye Sun Yun",
            "Iain J. Marshall",
            "Thomas A. Trikalinos",
            "Byron C. Wallace"
        ],
        "published": "2023-05-19T17:09:19Z",
        "summary": "Medical systematic reviews play a vital role in healthcare decision making\nand policy. However, their production is time-consuming, limiting the\navailability of high-quality and up-to-date evidence summaries. Recent\nadvancements in large language models (LLMs) offer the potential to\nautomatically generate literature reviews on demand, addressing this issue.\nHowever, LLMs sometimes generate inaccurate (and potentially misleading) texts\nby hallucination or omission. In healthcare, this can make LLMs unusable at\nbest and dangerous at worst. We conducted 16 interviews with international\nsystematic review experts to characterize the perceived utility and risks of\nLLMs in the specific context of medical evidence reviews. Experts indicated\nthat LLMs can assist in the writing process by drafting summaries, generating\ntemplates, distilling information, and crosschecking information. They also\nraised concerns regarding confidently composed but inaccurate LLM outputs and\nother potential downstream harms, including decreased accountability and\nproliferation of low-quality reviews. Informed by this qualitative analysis, we\nidentify criteria for rigorous evaluation of biomedical LLMs aligned with\ndomain expert views.",
        "pdf_link": "https://arxiv.org/pdf/2305.11828v3.pdf"
    },
    {
        "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
        "authors": [
            "Hongru Wang",
            "Rui Wang",
            "Fei Mi",
            "Yang Deng",
            "Zezhong Wang",
            "Bin Liang",
            "Ruifeng Xu",
            "Kam-Fai Wong"
        ],
        "published": "2023-05-19T16:27:43Z",
        "summary": "Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower\ndialogue systems with strong language understanding and generation\ncapabilities. However, most of the previous works prompt the LLMs to directly\ngenerate a response based on the dialogue context, overlooking the underlying\nlinguistic cues about the user status exhibited in the context. Such in-depth\ndialogue scenarios are challenging for existing LLMs to figure out the user's\nhidden needs and respond satisfactorily through a single-step inference. To\nthis end, we propose a novel linguistic cue-based chain-of-thoughts\n(\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate\nreasoning step to find cues exhibited in the dialogue, aiming to provide a more\npersonalized and engaging response. To evaluate the approach, we build a\nbenchmark with in-depth dialogue questions, consisting of 6 datasets in both\nChinese and English, targeting 3 major linguistic cues during the conversation:\n\\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct\nextensive experiments on the proposed benchmark with 5 LLMs under both\nzero-shot and one-shot settings. Empirical results demonstrate our proposed\n\\textit{Cue}-CoT method outperforms standard prompting methods in terms of both\n\\textit{helpfulness} and \\textit{acceptability} on all datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.11792v2.pdf"
    },
    {
        "title": "Prompting with Pseudo-Code Instructions",
        "authors": [
            "Mayank Mishra",
            "Prince Kumar",
            "Riyaz Bhat",
            "Rudra Murthy V",
            "Danish Contractor",
            "Srikanth Tamilselvam"
        ],
        "published": "2023-05-19T16:25:01Z",
        "summary": "Prompting with natural language instructions has recently emerged as a\npopular method of harnessing the capabilities of large language models. Given\nthe inherent ambiguity present in natural language, it is intuitive to consider\nthe possible advantages of prompting with less ambiguous prompt styles, such as\nthe use of pseudo-code.\n  In this paper we explore if prompting via pseudo-code instructions helps\nimprove the performance of pre-trained language models. We manually create a\ndataset of pseudo-code prompts for 132 different tasks spanning classification,\nQA and generative language tasks, sourced from the Super-NaturalInstructions\ndataset. Using these prompts along with their counterparts in natural language,\nwe study their performance on two LLM families - BLOOM and CodeGen. Our\nexperiments show that using pseudo-code instructions leads to better results,\nwith an average increase (absolute) of 7-16 points in F1 scores for\nclassification tasks and an improvement (relative) of 12-38% in aggregate\nROUGE-L scores across all tasks. We include detailed ablation studies which\nindicate that code comments, docstrings, and the structural clues encoded in\npseudo-code all contribute towards the improvement in performance.\n  To the best of our knowledge our work is the first to demonstrate how\npseudo-code prompts can be helpful in improving the performance of pre-trained\nLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.11790v3.pdf"
    },
    {
        "title": "Cross-Lingual Supervision improves Large Language Models Pre-training",
        "authors": [
            "Andrea Schioppa",
            "Xavier Garcia",
            "Orhan Firat"
        ],
        "published": "2023-05-19T16:14:07Z",
        "summary": "The recent rapid progress in pre-training Large Language Models has relied on\nusing self-supervised language modeling objectives like next token prediction\nor span corruption. On the other hand, Machine Translation Systems are mostly\ntrained using cross-lingual supervision that requires aligned data between\nsource and target languages. We demonstrate that pre-training Large Language\nModels on a mixture of a self-supervised Language Modeling objective and the\nsupervised Machine Translation objective, therefore including cross-lingual\nparallel data during pre-training, yields models with better in-context\nlearning abilities. As pre-training is a very resource-intensive process and a\ngrid search on the best mixing ratio between the two objectives is\nprohibitively expensive, we propose a simple yet effective strategy to learn it\nduring pre-training.",
        "pdf_link": "https://arxiv.org/pdf/2305.11778v1.pdf"
    },
    {
        "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
        "authors": [
            "Mustafa Safa Ozdayi",
            "Charith Peris",
            "Jack FitzGerald",
            "Christophe Dupuy",
            "Jimit Majmudar",
            "Haidar Khan",
            "Rahil Parikh",
            "Rahul Gupta"
        ],
        "published": "2023-05-19T15:45:29Z",
        "summary": "Large Language Models (LLMs) are known to memorize significant portions of\ntheir training data. Parts of this memorized content have been shown to be\nextractable by simply querying the model, which poses a privacy risk. We\npresent a novel approach which uses prompt-tuning to control the extraction\nrates of memorized content in LLMs. We present two prompt training strategies\nto increase and decrease extraction rates, which correspond to an attack and a\ndefense, respectively. We demonstrate the effectiveness of our techniques by\nusing models from the GPT-Neo family on a public benchmark. For the 1.3B\nparameter GPT-Neo model, our attack yields a 9.3 percentage point increase in\nextraction rate compared to our baseline. Our defense can be tuned to achieve\ndifferent privacy-utility trade-offs by a user-specified hyperparameter. We\nachieve an extraction rate reduction of up to 97.7% relative to our baseline,\nwith a perplexity increase of 16.9%.",
        "pdf_link": "https://arxiv.org/pdf/2305.11759v1.pdf"
    },
    {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Junyi Li",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-19T15:36:27Z",
        "summary": "Large language models (LLMs), such as ChatGPT, are prone to generate\nhallucinations, i.e., content that conflicts with the source or cannot be\nverified by the factual knowledge. To understand what types of content and to\nwhich extent LLMs are apt to hallucinate, we introduce the Hallucination\nEvaluation benchmark for Large Language Models (HaluEval), a large collection\nof generated and human-annotated hallucinated samples for evaluating the\nperformance of LLMs in recognizing hallucination. To generate these samples, we\npropose a ChatGPT-based two-step framework, i.e., sampling-then-filtering.\nBesides, we also hire some human labelers to annotate the hallucinations in\nChatGPT responses. The empirical results suggest that ChatGPT is likely to\ngenerate hallucinated content in specific topics by fabricating unverifiable\ninformation (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face\ngreat challenges in recognizing the hallucinations in texts. However, our\nexperiments also prove that providing external knowledge or adding reasoning\nsteps can help LLMs recognize hallucinations. Our benchmark can be accessed at\nhttps://github.com/RUCAIBox/HaluEval.",
        "pdf_link": "https://arxiv.org/pdf/2305.11747v3.pdf"
    },
    {
        "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
        "authors": [
            "Zhibin Gou",
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Yujiu Yang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-05-19T15:19:44Z",
        "summary": "Recent developments in large language models (LLMs) have been impressive.\nHowever, these models sometimes show inconsistencies and problematic behavior,\nsuch as hallucinating facts, generating flawed code, or creating offensive and\ntoxic content. Unlike these models, humans typically utilize external tools to\ncross-check and refine their initial content, like using a search engine for\nfact-checking, or a code interpreter for debugging. Inspired by this\nobservation, we introduce a framework called CRITIC that allows LLMs, which are\nessentially \"black boxes\" to validate and progressively amend their own outputs\nin a manner similar to human interaction with tools. More specifically,\nstarting with an initial output, CRITIC interacts with appropriate tools to\nevaluate certain aspects of the text, and then revises the output based on the\nfeedback obtained during this validation process. Comprehensive evaluations\ninvolving free-form question answering, mathematical program synthesis, and\ntoxicity reduction demonstrate that CRITIC consistently enhances the\nperformance of LLMs. Meanwhile, our research highlights the crucial importance\nof external feedback in promoting the ongoing self-improvement of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.11738v4.pdf"
    },
    {
        "title": "S$^3$HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering",
        "authors": [
            "Fangyu Lei",
            "Xiang Li",
            "Yifan Wei",
            "Shizhu He",
            "Yiming Huang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2023-05-19T15:01:48Z",
        "summary": "Answering multi-hop questions over hybrid factual knowledge from the given\ntext and table (TextTableQA) is a challenging task. Existing models mainly\nadopt a retriever-reader framework, which have several deficiencies, such as\nnoisy labeling in training retriever, insufficient utilization of heterogeneous\ninformation over text and table, and deficient ability for different reasoning\noperations. In this paper, we propose a three-stage TextTableQA framework\nS3HQA, which comprises of retriever, selector, and reasoner. We use a retriever\nwith refinement training to solve the noisy labeling problem. Then, a hybrid\nselector considers the linked relationships between heterogeneous data to\nselect the most relevant factual knowledge. For the final stage, instead of\nadapting a reading comprehension module like in previous methods, we employ a\ngeneration-based reasoner to obtain answers. This includes two approaches: a\nrow-wise generator and an LLM prompting generator~(first time used in this\ntask). The experimental results demonstrate that our method achieves\ncompetitive results in the few-shot setting. When trained on the full dataset,\nour approach outperforms all baseline methods, ranking first on the HybridQA\nleaderboard.",
        "pdf_link": "https://arxiv.org/pdf/2305.11725v1.pdf"
    },
    {
        "title": "Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses",
        "authors": [
            "Xenia Ohmer",
            "Elia Bruni",
            "Dieuwke Hupkes"
        ],
        "published": "2023-05-19T13:23:51Z",
        "summary": "At the staggering pace with which the capabilities of large language models\n(LLMs) are increasing, creating future-proof evaluation sets to assess their\nunderstanding becomes more and more challenging. In this paper, we propose a\nnovel paradigm for evaluating LLMs which leverages the idea that correct world\nunderstanding should be consistent across different (Fregean) senses of the\nsame meaning. Accordingly, we measure understanding not in terms of correctness\nbut by evaluating consistency across multiple senses that are generated by the\nmodel itself. We showcase our approach by instantiating a test where the\ndifferent senses are different languages, hence using multilingual\nself-consistency as a litmus test for the model's understanding and\nsimultaneously addressing the important topic of multilinguality. Taking one of\nthe latest versions of ChatGPT as our object of study, we evaluate multilingual\nconsistency for two different tasks across three different languages. We show\nthat its multilingual consistency is still lacking, and that its task and world\nunderstanding are thus not language-independent. As our approach does not\nrequire any static evaluation corpora in languages other than English, it can\neasily and cheaply be extended to different languages and tasks and could\nbecome an integral part of future benchmarking efforts.",
        "pdf_link": "https://arxiv.org/pdf/2305.11662v3.pdf"
    },
    {
        "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
        "authors": [
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "published": "2023-05-19T12:10:53Z",
        "summary": "Large language models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. However, such impressive capability typically\ncomes with a substantial model size, which presents significant challenges in\nboth the deployment, inference, and training stages. With LLM being a\ngeneral-purpose task solver, we explore its compression in a task-agnostic\nmanner, which aims to preserve the multi-task solving and language generation\nability of the original LLM. One challenge to achieving this is the enormous\nsize of the training corpus of LLM, which makes both data transfer and model\npost-training over-burdensome. Thus, we tackle the compression of LLMs within\nthe bound of two constraints: being task-agnostic and minimizing the reliance\non the original training dataset. Our method, named LLM-Pruner, adopts\nstructural pruning that selectively removes non-critical coupled structures\nbased on gradient information, maximally preserving the majority of the LLM's\nfunctionality. To this end, the performance of pruned models can be efficiently\nrecovered through tuning techniques, LoRA, in merely 3 hours, requiring only\n50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna,\nand ChatGLM, and demonstrate that the compressed models still exhibit\nsatisfactory capabilities in zero-shot classification and generation. The code\nis available at: https://github.com/horseee/LLM-Pruner",
        "pdf_link": "https://arxiv.org/pdf/2305.11627v3.pdf"
    },
    {
        "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "published": "2023-05-19T11:15:33Z",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities in various\napplications, but they still face various inconsistency issues. Existing works\nprimarily focus on the inconsistency issues within a single LLM, while we\ncomplementarily explore the inter-consistency among multiple LLMs for\ncollaboration. To examine whether LLMs can collaborate effectively to achieve a\nconsensus for a shared goal, we focus on commonsense reasoning, and introduce a\nformal debate framework (FORD) to conduct a three-stage debate among LLMs with\nreal-world scenarios alignment: fair debate, mismatched debate, and roundtable\ndebate. Through extensive experiments on various datasets, LLMs can effectively\ncollaborate to reach a consensus despite noticeable inter-inconsistencies, but\nimbalances in their abilities can lead to domination by superior LLMs.\nLeveraging a more advanced LLM like GPT-4 as an authoritative judge can boost\ncollaboration performance. Our work contributes to understanding the\ninter-consistency among LLMs and lays the foundation for developing future\ncollaboration methods. Codes and data are available at\nhttps://github.com/Waste-Wood/FORD",
        "pdf_link": "https://arxiv.org/pdf/2305.11595v3.pdf"
    },
    {
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "authors": [
            "Shibo Hao",
            "Tianyang Liu",
            "Zhen Wang",
            "Zhiting Hu"
        ],
        "published": "2023-05-19T09:54:21Z",
        "summary": "Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.11554v4.pdf"
    },
    {
        "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
        "authors": [
            "Fangkai Yang",
            "Pu Zhao",
            "Zezhong Wang",
            "Lu Wang",
            "Jue Zhang",
            "Mohit Garg",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "published": "2023-05-19T09:23:25Z",
        "summary": "Large Language Model (LLM) has gained popularity and achieved remarkable\nresults in open-domain tasks, but its performance in real industrial\ndomain-specific scenarios is average due to its lack of specific domain\nknowledge. This issue has attracted widespread attention, but there are few\nrelevant benchmarks available. In this paper, we provide a benchmark Question\nAnswering (QA) dataset named MSQA, centered around Microsoft products and IT\ntechnical problems encountered by customers. This dataset contains industry\ncloud-specific QA knowledge, an area not extensively covered in general LLMs,\nmaking it well-suited for evaluating methods aiming to enhance LLMs'\ndomain-specific capabilities. In addition, we propose a new model interaction\nparadigm that can empower LLM to achieve better performance on domain-specific\ntasks where it is not proficient. Extensive experiments demonstrate that the\napproach following our method outperforms the commonly used LLM with retrieval\nmethods. We make our source code and sample data available at:\nhttps://aka.ms/Microsoft_QA.",
        "pdf_link": "https://arxiv.org/pdf/2305.11541v3.pdf"
    },
    {
        "title": "InstructIE: A Bilingual Instruction-based Information Extraction Dataset",
        "authors": [
            "Honghao Gui",
            "Shuofei Qiao",
            "Jintian Zhang",
            "Hongbin Ye",
            "Mengshu Sun",
            "Lei Liang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-19T08:51:11Z",
        "summary": "Traditional information extraction (IE) methodologies, constrained by\npre-defined classes and static training paradigms, often falter in\nadaptability, especially in the dynamic world. To bridge this gap, we explore\nan instruction-based IE paradigm in this paper, leveraging the substantial\ncross-task generalization capabilities of Large Language Models (LLMs). We\nobserve that most existing IE datasets tend to be overly redundant in their\nlabel sets, which leads to the inclusion of numerous labels not directly\nrelevant to the extraction content when constructing instructions. To tackle\nthis issue, we introduce a bilingual theme-centric IE instruction dataset\n(Chinese and English), InstructIE, and for the first time, incorporate a theme\nscheme design that effectively simplifies the label structure. Furthermore, we\ndevelop an innovative framework named KG2Instruction, which is specifically\ndesigned for the automatic generation of such datasets. Experimental\nevaluations based on InstructIE reveal that while current models show promise\nin Instruction-based IE tasks, opportunities for their potential optimization\nalso emerge. The dataset is available at\nhttps://huggingface.co/datasets/zjunlp/InstructIE.",
        "pdf_link": "https://arxiv.org/pdf/2305.11527v2.pdf"
    },
    {
        "title": "PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning",
        "authors": [
            "Chengfeng Dou",
            "Zhi Jin",
            "Wenping Jiao",
            "Haiyan Zhao",
            "Zhenwei Tao",
            "Yongqiang Zhao"
        ],
        "published": "2023-05-19T08:18:24Z",
        "summary": "The patient-centered medical dialogue systems strive to offer diagnostic\ninterpretation services to users who are less knowledgeable about medical\nknowledge, through emphasizing the importance of providing responses specific\nto the patients. It is difficult for the large language models (LLMs) to\nguarantee the specificity of responses in spite of its promising performance\neven in some tasks in medical field. Inspired by in-context learning, we\npropose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing this\nchallenge. PlugMed is equipped with two modules, the prompt generation (PG)\nmodule and the response ranking (RR) module, to enhances LLMs' dialogue\nstrategies for improving the specificity of the dialogue. The PG module is\ndesigned to stimulate the imitative ability of LLMs by providing them with real\ndialogues from similar patients as prompts. The RR module incorporates\nfine-tuned small model as response filter to enable the selection of\nappropriate responses generated by LLMs. Furthermore, we introduce a new\nevaluation method based on matching both user's intent and high-frequency\nmedical term to effectively assess the specificity of the responses. We conduct\nexperimental evaluations on three medical dialogue datasets, and the results,\nincluding both automatic and human evaluation, demonstrate the effectiveness of\nour approach.",
        "pdf_link": "https://arxiv.org/pdf/2305.11508v2.pdf"
    },
    {
        "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
        "authors": [
            "Tianci Xue",
            "Ziqi Wang",
            "Zhenhailong Wang",
            "Chi Han",
            "Pengfei Yu",
            "Heng Ji"
        ],
        "published": "2023-05-19T08:02:52Z",
        "summary": "Large language Models (LLMs) have achieved promising performance on\narithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting. However, LLMs face challenges in maintaining factual consistency\nduring reasoning, exhibiting tendencies to condition overlooking, question\nmisinterpretation, and condition hallucination over given problems. Existing\nmethods use coarse-grained feedback (e.g., whether the answer is correct) to\nimprove factual consistency. In this work, we propose RCoT (Reversing\nChain-of-Thought), a novel method to improve LLMs' reasoning abilities by\nautomatically detecting and rectifying factual inconsistency in LLMs, generated\nsolutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct\nthe problem based on generated solutions. Then fine-grained comparisons between\nthe original problem and the reconstructed problem expose the factual\ninconsistency in the original solutions. To rectify the solution, RCoT\nformulates detected factual inconsistency into fine-grained feedback to guide\nLLMs in revising solutions. Experimental results demonstrate improvements of\nRCoT over standard CoT, Self-Consistency and Self-Refine across seven\narithmetic datasets. Moreover, we find that manually written fine-grained\nfeedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT\nreaches 94.6% accuracy on GSM8K), encouraging the community to further explore\nthe fine-grained feedback generation methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.11499v2.pdf"
    },
    {
        "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
        "authors": [
            "Suhyeon Lee",
            "Won Jun Kim",
            "Jinho Chang",
            "Jong Chul Ye"
        ],
        "published": "2023-05-19T07:44:39Z",
        "summary": "Following the impressive development of LLMs, vision-language alignment in\nLLMs is actively being researched to enable multimodal reasoning and visual IO.\nThis direction of research is particularly relevant to medical imaging because\nmedical image analysis and generation consist of reasoning based on a\ncombination of visual features and prior knowledge. Many recent works have\nfocused on training adapter networks that serve as an information bridge\nbetween image processing networks and LLMs; but presumably, in order to achieve\nmaximum reasoning potential of LLMs on visual information as well, visual and\nlanguage features should be allowed to interact more freely. This is especially\nimportant in the medical domain because understanding and generating medical\nimages such as chest X-rays (CXR) require not only accurate visual and\nlanguage-based reasoning but also a more intimate mapping between the two\nmodalities. Thus, taking inspiration from previous work on the transformer and\nVQ-GAN combination for bidirectional image and text generation, we build upon\nthis approach and develop a method for instruction-tuning an LLM pre-trained\nonly on text to gain vision-language capabilities for medical images.\nSpecifically, we leverage a pretrained LLM's existing question-answering and\ninstruction-following abilities to teach it to understand visual inputs by\ninstructing it to answer questions about image inputs and, symmetrically,\noutput both text and image responses appropriate to a given query by tuning the\nLLM with diverse tasks that encompass image-based text-generation and\ntext-based image-generation. We show that our model, LLM-CXR, trained in this\napproach shows better image-text alignment in both CXR understanding and\ngeneration tasks while being smaller in size compared to previously developed\nmodels that perform a narrower range of tasks. The code is at\nhttps://github.com/hyn2028/llm-cxr.",
        "pdf_link": "https://arxiv.org/pdf/2305.11490v5.pdf"
    },
    {
        "title": "Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language Models",
        "authors": [
            "Sangho Suh",
            "Bryan Min",
            "Srishti Palani",
            "Haijun Xia"
        ],
        "published": "2023-05-19T07:31:59Z",
        "summary": "People are increasingly turning to large language models (LLMs) for complex\ninformation tasks like academic research or planning a move to another city.\nHowever, while they often require working in a nonlinear manner -- e.g., to\narrange information spatially to organize and make sense of it, current\ninterfaces for interacting with LLMs are generally linear to support\nconversational interaction. To address this limitation and explore how we can\nsupport LLM-powered exploration and sensemaking, we developed Sensecape, an\ninteractive system designed to support complex information tasks with an LLM by\nenabling users to (1) manage the complexity of information through multilevel\nabstraction and (2) seamlessly switch between foraging and sensemaking. Our\nwithin-subject user study reveals that Sensecape empowers users to explore more\ntopics and structure their knowledge hierarchically, thanks to the\nexternalization of levels of abstraction. We contribute implications for\nLLM-based workflows and interfaces for information tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.11483v2.pdf"
    },
    {
        "title": "Graphologue: Exploring Large Language Model Responses with Interactive Diagrams",
        "authors": [
            "Peiling Jiang",
            "Jude Rayan",
            "Steven P. Dow",
            "Haijun Xia"
        ],
        "published": "2023-05-19T06:53:25Z",
        "summary": "Large language models (LLMs) have recently soared in popularity due to their\nease of access and the unprecedented ability to synthesize text responses to\ndiverse user questions. However, LLMs like ChatGPT present significant\nlimitations in supporting complex information tasks due to the insufficient\naffordances of the text-based medium and linear conversational structure.\nThrough a formative study with ten participants, we found that LLM interfaces\noften present long-winded responses, making it difficult for people to quickly\ncomprehend and interact flexibly with various pieces of information,\nparticularly during more complex tasks. We present Graphologue, an interactive\nsystem that converts text-based responses from LLMs into graphical diagrams to\nfacilitate information-seeking and question-answering tasks. Graphologue\nemploys novel prompting strategies and interface designs to extract entities\nand relationships from LLM responses and constructs node-link diagrams in\nreal-time. Further, users can interact with the diagrams to flexibly adjust the\ngraphical presentation and to submit context-specific prompts to obtain more\ninformation. Utilizing diagrams, Graphologue enables graphical, non-linear\ndialogues between humans and LLMs, facilitating information exploration,\norganization, and comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2305.11473v2.pdf"
    },
    {
        "title": "Hint of Thought prompting: an explainable and zero-shot approach to reasoning tasks with LLMs",
        "authors": [
            "Ioktong Lei",
            "Zhidong Deng"
        ],
        "published": "2023-05-19T06:30:17Z",
        "summary": "As a way of communicating with users and any LLMs like GPT or PaLM2,\nprompting becomes an increasingly important research topic for better\nutilization of LLMs. Although simple prompting performs well on single-step\nquestions, it cannot permanently activate the correct knowledge path for\nmulti-step reasoning tasks. The chain of thought (CoT), which often contains\nzero-shot CoT and few-shot CoT, is a recently developed prompting method that\ncan explain the reasoning process to the LLM and outperforms simple prompting\nin three challenging reasoning tasks, including arithmetic, symbolic, and\ncommonsense reasoning. In this paper, we propose a novel hint of thought (HoT)\nprompting with explainability and zero-shot generalization. First, it is\ndecomposed into the following three steps: explainable sub-questions, logical\nreasoning, and answer extraction. Second, such three steps are sequentially\nordered in the format of step-by-step hints, which can be easily adjusted and\nexplained to different tasks. Finally, experimental results demonstrate that\nour HoT prompting has a significant advantage on the zero-shot reasoning task\ncompared to existing zero-shot CoT. We did zero-shot experiments on math tasks\nlike GSM8K, ADDSUB, AQUA, SVAMP and commonsense tasks such as StrategyQA. In\nparticular, the accuracy of the proposed HoT prompting is improved with GSM8K\nfrom 40.50% to 67.80%, with AQUA from 31.9% to 46.4%, with SVAMP from 63.7% to\n76.9%, and with ADDSUB from 74.7% to 87.34%, respectively, which even defeats\nthe competitive PoT approach on GSM8k, AQUA, and SVAMP.",
        "pdf_link": "https://arxiv.org/pdf/2305.11461v5.pdf"
    },
    {
        "title": "Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions",
        "authors": [
            "Shiyao Ding",
            "Takayuki Ito"
        ],
        "published": "2023-05-19T06:27:16Z",
        "summary": "Finding an agreement among diverse opinions is a challenging topic in\nmultiagent systems. Recently, large language models (LLMs) have shown great\npotential in addressing this challenge due to their remarkable capabilities in\ncomprehending human opinions and generating human-like text. However, they\ntypically rely on extensive human-annotated data. In this paper, we propose\nSelf-Agreement, a novel framework for fine-tuning LLMs to autonomously find\nagreement using data generated by LLM itself. Specifically, our approach\nemploys the generative pre-trained transformer-3 (GPT-3) to generate multiple\nopinions for each question in a question dataset and create several agreement\ncandidates among these opinions. Then, a bidirectional encoder representations\nfrom transformers (BERT)-based model evaluates the agreement score of each\nagreement candidate and selects the one with the highest agreement score. This\nprocess yields a dataset of question-opinion-agreements, which we use to\nfine-tune a pre-trained LLM for discovering agreements among diverse opinions.\nRemarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework\nachieves comparable performance to GPT-3 with only 1/25 of its parameters,\nshowcasing its ability to identify agreement among various opinions without the\nneed for human-annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2305.11460v1.pdf"
    },
    {
        "title": "TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",
        "authors": [
            "Shubhra Kanti Karmaker Santu",
            "Dongji Feng"
        ],
        "published": "2023-05-19T04:59:34Z",
        "summary": "While LLMs have shown great success in understanding and generating text in\ntraditional conversational settings, their potential for performing ill-defined\ncomplex tasks is largely under-studied. Indeed, we are yet to conduct\ncomprehensive benchmarking studies with multiple LLMs that are exclusively\nfocused on a complex task. However, conducting such benchmarking studies is\nchallenging because of the large variations in LLMs' performance when different\nprompt types/styles are used and different degrees of detail are provided in\nthe prompts. To address this issue, the paper proposes a general taxonomy that\ncan be used to design prompts with specific properties in order to perform a\nwide range of complex tasks. This taxonomy will allow future benchmarking\nstudies to report the specific categories of prompts used as part of the study,\nenabling meaningful comparisons across different studies. Also, by establishing\na common standard through this taxonomy, researchers will be able to draw more\naccurate conclusions about LLMs' performance on a specific complex task.",
        "pdf_link": "https://arxiv.org/pdf/2305.11430v2.pdf"
    },
    {
        "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
        "authors": [
            "Satyapriya Krishna",
            "Jiaqi Ma",
            "Dylan Slack",
            "Asma Ghandeharioun",
            "Sameer Singh",
            "Himabindu Lakkaraju"
        ],
        "published": "2023-05-19T04:46:04Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex tasks. Moreover, recent research has shown that\nincorporating human-annotated rationales (e.g., Chain-of-Thought prompting)\nduring in-context learning can significantly enhance the performance of these\nmodels, particularly on tasks that require reasoning capabilities. However,\nincorporating such rationales poses challenges in terms of scalability as this\nrequires a high degree of human involvement. In this work, we present a novel\nframework, Amplifying Model Performance by Leveraging In-Context Learning with\nPost Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges\nby automating the process of rationale generation. To this end, we leverage\npost hoc explanation methods which output attribution scores (explanations)\ncapturing the influence of each of the input features on model predictions.\nMore specifically, we construct automated natural language rationales that\nembed insights from post hoc explanations to provide corrective signals to\nLLMs. Extensive experimentation with real-world datasets demonstrates that our\nframework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%\nover a wide range of tasks, including those where prior approaches which rely\non human-annotated rationales such as Chain-of-Thought prompting fall short.\nOur work makes one of the first attempts at highlighting the potential of post\nhoc explanations as valuable tools for enhancing the effectiveness of LLMs.\nFurthermore, we conduct additional empirical analyses and ablation studies to\ndemonstrate the impact of each of the components of AMPLIFY, which, in turn,\nleads to critical insights for refining in-context learning.",
        "pdf_link": "https://arxiv.org/pdf/2305.11426v3.pdf"
    },
    {
        "title": "Federated Foundation Models: Privacy-Preserving and Collaborative Learning for Large Models",
        "authors": [
            "Sixing Yu",
            "J. Pablo Muñoz",
            "Ali Jannesari"
        ],
        "published": "2023-05-19T03:51:59Z",
        "summary": "Foundation Models (FMs), such as LLaMA, BERT, GPT, ViT, and CLIP, have\ndemonstrated remarkable success in a wide range of applications, driven by\ntheir ability to leverage vast amounts of data for pre-training. However,\noptimizing FMs often requires access to sensitive data, raising privacy\nconcerns and limiting their applicability in many domains. In this paper, we\npropose the Federated Foundation Models (FFMs) paradigm, which combines the\nbenefits of FMs and Federated Learning (FL) to enable privacy-preserving and\ncollaborative learning across multiple end-users. We discuss the potential\nbenefits and challenges of integrating FL into the lifespan of FMs, covering\npre-training, fine-tuning, and application. We further outline potential future\nresearch avenues in FFM, including FFM pre-training, FFM fine-tuning, and\nfederated prompt tuning, which allow the development of more personalized and\ncontext-aware models while ensuring data privacy. Moreover, we explore the\npossibility of continual/lifelong learning in FFMs, as increased computational\npower at the edge may unlock the potential for optimizing FMs using newly\ngenerated private data close to the data source. The proposed FFM concepts\noffer a flexible and scalable framework for training large language models in a\nprivacy-preserving manner, setting the stage for subsequent advancements in\nboth FM training and federated learning.",
        "pdf_link": "https://arxiv.org/pdf/2305.11414v3.pdf"
    },
    {
        "title": "A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation",
        "authors": [
            "Xiaowei Huang",
            "Wenjie Ruan",
            "Wei Huang",
            "Gaojie Jin",
            "Yi Dong",
            "Changshun Wu",
            "Saddek Bensalem",
            "Ronghui Mu",
            "Yi Qi",
            "Xingyu Zhao",
            "Kaiwen Cai",
            "Yanghao Zhang",
            "Sihao Wu",
            "Peipei Xu",
            "Dengyu Wu",
            "Andre Freitas",
            "Mustafa A. Mustafa"
        ],
        "published": "2023-05-19T02:41:12Z",
        "summary": "Large Language Models (LLMs) have exploded a new heatwave of AI for their\nability to engage end-users in human-level conversations with detailed and\narticulate answers across many knowledge domains. In response to their fast\nadoption in many industrial applications, this survey concerns their safety and\ntrustworthiness. First, we review known vulnerabilities and limitations of the\nLLMs, categorising them into inherent issues, attacks, and unintended bugs.\nThen, we consider if and how the Verification and Validation (V&V) techniques,\nwhich have been widely developed for traditional software and deep learning\nmodels such as convolutional neural networks as independent processes to check\nthe alignment of their implementations against the specifications, can be\nintegrated and further extended throughout the lifecycle of the LLMs to provide\nrigorous analysis to the safety and trustworthiness of LLMs and their\napplications. Specifically, we consider four complementary techniques:\nfalsification and evaluation, verification, runtime monitoring, and regulations\nand ethical use. In total, 370+ references are considered to support the quick\nunderstanding of the safety and trustworthiness issues from the perspective of\nV&V. While intensive research has been conducted to identify the safety and\ntrustworthiness issues, rigorous yet practical methods are called for to ensure\nthe alignment of LLMs with safety and trustworthiness requirements.",
        "pdf_link": "https://arxiv.org/pdf/2305.11391v2.pdf"
    },
    {
        "title": "ChatGPT for Us: Preserving Data Privacy in ChatGPT via Dialogue Text Ambiguation to Expand Mental Health Care Delivery",
        "authors": [
            "Anaelia Ovalle",
            "Mehrab Beikzadeh",
            "Parshan Teimouri",
            "Kai-Wei Chang",
            "Majid Sarrafzadeh"
        ],
        "published": "2023-05-19T02:09:52Z",
        "summary": "Large language models have been useful in expanding mental health care\ndelivery. ChatGPT, in particular, has gained popularity for its ability to\ngenerate human-like dialogue. However, data-sensitive domains -- including but\nnot limited to healthcare -- face challenges in using ChatGPT due to privacy\nand data-ownership concerns. To enable its utilization, we propose a text\nambiguation framework that preserves user privacy. We ground this in the task\nof addressing stress prompted by user-provided texts to demonstrate the\nviability and helpfulness of privacy-preserved generations. Our results suggest\nthat chatGPT recommendations are still able to be moderately helpful and\nrelevant, even when the original user text is not provided.",
        "pdf_link": "https://arxiv.org/pdf/2306.05552v1.pdf"
    },
    {
        "title": "Visualizing Linguistic Diversity of Text Datasets Synthesized by Large Language Models",
        "authors": [
            "Emily Reif",
            "Minsuk Kahng",
            "Savvas Petridis"
        ],
        "published": "2023-05-19T00:53:45Z",
        "summary": "Large language models (LLMs) can be used to generate smaller, more refined\ndatasets via few-shot prompting for benchmarking, fine-tuning or other use\ncases. However, understanding and evaluating these datasets is difficult, and\nthe failure modes of LLM-generated data are still not well understood.\nSpecifically, the data can be repetitive in surprising ways, not only\nsemantically but also syntactically and lexically. We present LinguisticLens, a\nnovel inter-active visualization tool for making sense of and analyzing\nsyntactic diversity of LLM-generated datasets. LinguisticLens clusters text\nalong syntactic, lexical, and semantic axes. It supports hierarchical\nvisualization of a text dataset, allowing users to quickly scan for an overview\nand inspect individual examples. The live demo is available at\nshorturl.at/zHOUV.",
        "pdf_link": "https://arxiv.org/pdf/2305.11364v2.pdf"
    },
    {
        "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
        "authors": [
            "Giorgi Kokaia",
            "Pratyush Sinha",
            "Yutong Jiang",
            "Nozha Boujemaa"
        ],
        "published": "2023-05-18T22:47:06Z",
        "summary": "We introduce two novel methods, Tree-Search and Self-contextualizing QA,\ndesigned to enhance the performance of large language models (LLMs) in\nquestion-answering tasks. Tree-Search is a sampling technique specifically\ncreated to extract diverse information from an LLM for a given prompt.\nSelf-contextualizing QA leverages Tree-Search to enable the model to create its\nown context using a wide range of information relevant to the prompt, evaluate\nit explicitly and return a open book answer to the initial prompt . We\ndemonstrate that the quality of generated answers improves according to various\nmetrics, including accuracy, informativeness, coherence, and consistency, as\nevaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods\nresult in increased robustness and that performance is positively correlated\nwith tree size, benefiting both answer quality and robustness. Finally, we\ndiscuss other promising applications of Tree-Search, highlighting its potential\nto enhance a broad range of tasks beyond question-answering.\n  \\noindent We also discuss several areas for future work, including refining\nthe Tree-Search and Self-Contextualizing QA methods, improving the coherence of\nthe generated context, and investigating the impact of bootstrapping on model\nrobustness",
        "pdf_link": "https://arxiv.org/pdf/2305.11334v1.pdf"
    },
    {
        "title": "Collaborative Generative AI: Integrating GPT-k for Efficient Editing in Text-to-Image Generation",
        "authors": [
            "Wanrong Zhu",
            "Xinyi Wang",
            "Yujie Lu",
            "Tsu-Jui Fu",
            "Xin Eric Wang",
            "Miguel Eckstein",
            "William Yang Wang"
        ],
        "published": "2023-05-18T21:53:58Z",
        "summary": "The field of text-to-image (T2I) generation has garnered significant\nattention both within the research community and among everyday users. Despite\nthe advancements of T2I models, a common issue encountered by users is the need\nfor repetitive editing of input prompts in order to receive a satisfactory\nimage, which is time-consuming and labor-intensive. Given the demonstrated text\ngeneration power of large-scale language models, such as GPT-k, we investigate\nthe potential of utilizing such models to improve the prompt editing process\nfor T2I generation. We conduct a series of experiments to compare the common\nedits made by humans and GPT-k, evaluate the performance of GPT-k in prompting\nT2I, and examine factors that may influence this process. We found that GPT-k\nmodels focus more on inserting modifiers while humans tend to replace words and\nphrases, which includes changes to the subject matter. Experimental results\nshow that GPT-k are more effective in adjusting modifiers rather than\npredicting spontaneous changes in the primary subject matters. Adopting the\nedit suggested by GPT-k models may reduce the percentage of remaining edits by\n20-30%.",
        "pdf_link": "https://arxiv.org/pdf/2305.11317v2.pdf"
    },
    {
        "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
        "authors": [
            "Jiaxu Zhao",
            "Meng Fang",
            "Zijing Shi",
            "Yitong Li",
            "Ling Chen",
            "Mykola Pechenizkiy"
        ],
        "published": "2023-05-18T18:58:30Z",
        "summary": "\\textit{\\textbf{\\textcolor{red}{Warning}:} This paper contains content that\nmay be offensive or upsetting.} Pretrained conversational agents have been\nexposed to safety issues, exhibiting a range of stereotypical human biases such\nas gender bias. However, there are still limited bias categories in current\nresearch, and most of them only focus on English. In this paper, we introduce a\nnew Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese\nconversational language models. Apart from those previous well-explored bias\ncategories, CHBias includes under-explored bias categories, such as ageism and\nappearance biases, which received less attention. We evaluate two popular\npretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias.\nFurthermore, to mitigate different biases, we apply several debiasing methods\nto the Chinese pretrained models. Experimental results show that these Chinese\npretrained models are potentially risky for generating texts that contain\nsocial biases, and debiasing methods using the proposed dataset can make\nresponse generation less biased while preserving the models' conversational\ncapabilities.",
        "pdf_link": "https://arxiv.org/pdf/2305.11262v1.pdf"
    },
    {
        "title": "Comparing Machines and Children: Using Developmental Psychology Experiments to Assess the Strengths and Weaknesses of LaMDA Responses",
        "authors": [
            "Eliza Kosoy",
            "Emily Rose Reagan",
            "Leslie Lai",
            "Alison Gopnik",
            "Danielle Krettek Cobb"
        ],
        "published": "2023-05-18T18:15:43Z",
        "summary": "Developmental psychologists have spent decades devising experiments to test\nthe intelligence and knowledge of infants and children, tracing the origin of\ncrucial concepts and capacities. Moreover, experimental techniques in\ndevelopmental psychology have been carefully designed to discriminate the\ncognitive capacities that underlie particular behaviors. We propose that using\nclassical experiments from child development is a particularly effective way to\nprobe the computational abilities of AI models, in general, and LLMs in\nparticular. First, the methodological techniques of developmental psychology,\nsuch as the use of novel stimuli to control for past experience or control\nconditions to determine whether children are using simple associations, can be\nequally helpful for assessing the capacities of LLMs. In parallel, testing LLMs\nin this way can tell us whether the information that is encoded in text is\nsufficient to enable particular responses, or whether those responses depend on\nother kinds of information, such as information from exploration of the\nphysical world. In this work we adapt classical developmental experiments to\nevaluate the capabilities of LaMDA, a large language model from Google. We\npropose a novel LLM Response Score (LRS) metric which can be used to evaluate\nother language models, such as GPT. We find that LaMDA generates appropriate\nresponses that are similar to those of children in experiments involving social\nunderstanding, perhaps providing evidence that knowledge of these domains is\ndiscovered through language. On the other hand, LaMDA's responses in early\nobject and action understanding, theory of mind, and especially causal\nreasoning tasks are very different from those of young children, perhaps\nshowing that these domains require more real-world, self-initiated exploration\nand cannot simply be learned from patterns in language input.",
        "pdf_link": "https://arxiv.org/pdf/2305.11243v2.pdf"
    },
    {
        "title": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks",
        "authors": [
            "Wenhai Wang",
            "Zhe Chen",
            "Xiaokang Chen",
            "Jiannan Wu",
            "Xizhou Zhu",
            "Gang Zeng",
            "Ping Luo",
            "Tong Lu",
            "Jie Zhou",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "published": "2023-05-18T17:59:42Z",
        "summary": "Large language models (LLMs) have notably accelerated progress towards\nartificial general intelligence (AGI), with their impressive zero-shot capacity\nfor user-tailored tasks, endowing them with immense potential across a range of\napplications. However, in the field of computer vision, despite the\navailability of numerous powerful vision foundation models (VFMs), they are\nstill restricted to tasks in a pre-defined form, struggling to match the\nopen-ended task capabilities of LLMs. In this work, we present an LLM-based\nframework for vision-centric tasks, termed VisionLLM. This framework provides a\nunified perspective for vision and language tasks by treating images as a\nforeign language and aligning vision-centric tasks with language tasks that can\nbe flexibly defined and managed using language instructions. An LLM-based\ndecoder can then make appropriate predictions based on these instructions for\nopen-ended tasks. Extensive experiments show that the proposed VisionLLM can\nachieve different levels of task customization through language instructions,\nfrom fine-grained object-level to coarse-grained task-level customization, all\nwith good results. It's noteworthy that, with a generalist LLM-based framework,\nour model can achieve over 60\\% mAP on COCO, on par with detection-specific\nmodels. We hope this model can set a new baseline for generalist vision and\nlanguage models. The demo shall be released based on\nhttps://github.com/OpenGVLab/InternGPT. The code shall be released at\nhttps://github.com/OpenGVLab/VisionLLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.11175v2.pdf"
    },
    {
        "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
        "authors": [
            "Zorik Gekhman",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Chen Elkind",
            "Idan Szpektor"
        ],
        "published": "2023-05-18T17:58:35Z",
        "summary": "Factual consistency evaluation is often conducted using Natural Language\nInference (NLI) models, yet these models exhibit limited success in evaluating\nsummaries. Previous work improved such models with synthetic training data.\nHowever, the data is typically based on perturbed human-written summaries,\nwhich often differ in their characteristics from real model-generated summaries\nand have limited coverage of possible factual errors. Alternatively, large\nlanguage models (LLMs) have recently shown promising results in directly\nevaluating generative tasks, but are too computationally expensive for\npractical use. Motivated by these limitations, we introduce TrueTeacher, a\nmethod for generating synthetic data by annotating diverse model-generated\nsummaries using a LLM. Unlike prior work, TrueTeacher does not rely on\nhuman-written summaries, and is multilingual by nature. Experiments on the TRUE\nbenchmark show that a student model trained using our data, substantially\noutperforms both the state-of-the-art model with similar capacity, and the LLM\nteacher. In a systematic study, we compare TrueTeacher to existing synthetic\ndata generation methods and demonstrate its superiority and robustness to\ndomain-shift. We also show that our method generalizes to multilingual\nscenarios. Lastly, we release our large scale synthetic dataset (1.4M\nexamples), generated using TrueTeacher, and a checkpoint trained on this data.",
        "pdf_link": "https://arxiv.org/pdf/2305.11171v3.pdf"
    },
    {
        "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors",
        "authors": [
            "Kai Zhang",
            "Bernal Jiménez Gutiérrez",
            "Yu Su"
        ],
        "published": "2023-05-18T17:48:03Z",
        "summary": "Recent work has shown that fine-tuning large language models (LLMs) on\nlarge-scale instruction-following datasets substantially improves their\nperformance on a wide range of NLP tasks, especially in the zero-shot setting.\nHowever, even advanced instruction-tuned LLMs still fail to outperform small\nLMs on relation extraction (RE), a fundamental information extraction task. We\nhypothesize that instruction-tuning has been unable to elicit strong RE\ncapabilities in LLMs due to RE's low incidence in instruction-tuning datasets,\nmaking up less than 1% of all tasks (Wang et al., 2022). To address this\nlimitation, we propose QA4RE, a framework that aligns RE with question\nanswering (QA), a predominant task in instruction-tuning datasets.\nComprehensive zero-shot RE experiments over four datasets with two series of\ninstruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework\nconsistently improves LLM performance, strongly verifying our hypothesis and\nenabling LLMs to outperform strong zero-shot baselines by a large margin.\nAdditionally, we provide thorough experiments and discussions to show the\nrobustness, few-shot effectiveness, and strong transferability of our QA4RE\nframework. This work illustrates a promising way of adapting LLMs to\nchallenging and underrepresented tasks by aligning these tasks with more common\ninstruction-tuning tasks like QA.",
        "pdf_link": "https://arxiv.org/pdf/2305.11159v1.pdf"
    },
    {
        "title": "Trading Syntax Trees for Wordpieces: Target-oriented Opinion Words Extraction with Wordpieces and Aspect Enhancement",
        "authors": [
            "Samuel Mensah",
            "Kai Sun",
            "Nikolaos Aletras"
        ],
        "published": "2023-05-18T15:22:00Z",
        "summary": "State-of-the-art target-oriented opinion word extraction (TOWE) models\ntypically use BERT-based text encoders that operate on the word level, along\nwith graph convolutional networks (GCNs) that incorporate syntactic information\nextracted from syntax trees. These methods achieve limited gains with GCNs and\nhave difficulty using BERT wordpieces. Meanwhile, BERT wordpieces are known to\nbe effective at representing rare words or words with insufficient context\ninformation. To address this issue, this work trades syntax trees for BERT\nwordpieces by entirely removing the GCN component from the methods'\narchitectures. To enhance TOWE performance, we tackle the issue of aspect\nrepresentation loss during encoding. Instead of solely utilizing a sentence as\nthe input, we use a sentence-aspect pair. Our relatively simple approach\nachieves state-of-the-art results on benchmark datasets and should serve as a\nstrong baseline for further research.",
        "pdf_link": "https://arxiv.org/pdf/2305.11034v1.pdf"
    },
    {
        "title": "Generalized Planning in PDDL Domains with Pretrained Large Language Models",
        "authors": [
            "Tom Silver",
            "Soham Dan",
            "Kavitha Srinivas",
            "Joshua B. Tenenbaum",
            "Leslie Pack Kaelbling",
            "Michael Katz"
        ],
        "published": "2023-05-18T14:48:20Z",
        "summary": "Recent work has considered whether large language models (LLMs) can function\nas planners: given a task, generate a plan. We investigate whether LLMs can\nserve as generalized planners: given a domain and training tasks, generate a\nprogram that efficiently produces plans for other tasks in the domain. In\nparticular, we consider PDDL domains and use GPT-4 to synthesize Python\nprograms. We also consider (1) Chain-of-Thought (CoT) summarization, where the\nLLM is prompted to summarize the domain and propose a strategy in words before\nsynthesizing the program; and (2) automated debugging, where the program is\nvalidated with respect to the training tasks, and in case of errors, the LLM is\nre-prompted with four types of feedback. We evaluate this approach in seven\nPDDL domains and compare it to four ablations and four baselines. Overall, we\nfind that GPT-4 is a surprisingly powerful generalized planner. We also\nconclude that automated debugging is very important, that CoT summarization has\nnon-uniform impact, that GPT-4 is far superior to GPT-3.5, and that just two\ntraining tasks are often sufficient for strong generalization.",
        "pdf_link": "https://arxiv.org/pdf/2305.11014v2.pdf"
    },
    {
        "title": "The Web Can Be Your Oyster for Improving Large Language Models",
        "authors": [
            "Junyi Li",
            "Tianyi Tang",
            "Wayne Xin Zhao",
            "Jingyuan Wang",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-18T14:20:32Z",
        "summary": "Large language models (LLMs) encode a large amount of world knowledge.\nHowever, as such knowledge is frozen at the time of model training, the models\nbecome static and limited by the training data at that time. In order to\nfurther improve the capacity of LLMs for knowledge-intensive tasks, we consider\naugmenting LLMs with the large-scale web using search engine. Unlike previous\naugmentation sources (e.g., Wikipedia data dump), the web provides broader,\nmore comprehensive and constantly updated information. In this paper, we\npresent a web-augmented LLM UNIWEB, which is trained over 16\nknowledge-intensive tasks in a unified text-to-text format. Instead of simply\nusing the retrieved contents from web, our approach has made two major\nimprovements. Firstly, we propose an adaptive search engine assisted learning\nmethod that can self-evaluate the confidence level of LLM's predictions, and\nadaptively determine when to refer to the web for more data, which can avoid\nuseless or noisy augmentation from web. Secondly, we design a pretraining task,\ni.e., continual knowledge learning, based on salient spans prediction, to\nreduce the discrepancy between the encoded and retrieved knowledge. Experiments\non a wide range of knowledge-intensive tasks show that our model significantly\noutperforms previous retrieval-augmented methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.10998v2.pdf"
    },
    {
        "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
        "authors": [
            "Ning Lu",
            "Shengcai Liu",
            "Rui He",
            "Qi Wang",
            "Yew-Soon Ong",
            "Ke Tang"
        ],
        "published": "2023-05-18T10:03:25Z",
        "summary": "Large language models (LLMs) have shown remarkable performance in various\ntasks and have been extensively utilized by the public. However, the increasing\nconcerns regarding the misuse of LLMs, such as plagiarism and spamming, have\nled to the development of multiple detectors, including fine-tuned classifiers\nand statistical methods. In this study, we equip LLMs with prompts, rather than\nrelying on an external paraphraser, to evaluate the vulnerability of these\ndetectors. We propose a novel Substitution-based In-Context example\nOptimization method (SICO) to automatically construct prompts for evading the\ndetectors. SICO is cost-efficient as it requires only 40 human-written examples\nand a limited number of LLM inferences to generate a prompt. Moreover, once a\ntask-specific prompt has been constructed, it can be universally used against a\nwide range of detectors. Extensive experiments across three real-world tasks\ndemonstrate that SICO significantly outperforms the paraphraser baselines and\nenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by\n0.5 on average. Furthermore, a comprehensive human evaluation as well as a\nvalidation experiment in the wild show that the SICO-generated text achieves\nhuman-level readability and task completion rates. Finally, the strong\nperformance of SICO exhibits its potential as a reliable evaluation tool for\nfuture detectors. The codes and data are located on\nhttps://github.com/ColinLu50/Evade-GPT-Detector.",
        "pdf_link": "https://arxiv.org/pdf/2305.10847v5.pdf"
    },
    {
        "title": "X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models",
        "authors": [
            "Yixiong Chen",
            "Li Liu",
            "Chris Ding"
        ],
        "published": "2023-05-18T09:56:44Z",
        "summary": "This paper introduces a novel explainable image quality evaluation approach\ncalled X-IQE, which leverages visual large language models (LLMs) to evaluate\ntext-to-image generation methods by generating textual explanations. X-IQE\nutilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce\nself-consistent, unbiased texts that are highly correlated with human\nevaluation. It offers several advantages, including the ability to distinguish\nbetween real and generated images, evaluate text-image alignment, and assess\nimage aesthetics without requiring model training or fine-tuning. X-IQE is more\ncost-effective and efficient compared to human evaluation, while significantly\nenhancing the transparency and explainability of deep image quality evaluation\nmodels. We validate the effectiveness of our method as a benchmark using images\ngenerated by prevalent diffusion models. X-IQE demonstrates similar performance\nto state-of-the-art (SOTA) evaluation methods on COCO Caption, while overcoming\nthe limitations of previous evaluation models on DrawBench, particularly in\nhandling ambiguous generation prompts and text recognition in generated images.\nProject website:\nhttps://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models",
        "pdf_link": "https://arxiv.org/pdf/2305.10843v2.pdf"
    },
    {
        "title": "ProgSG: Cross-Modality Representation Learning for Programs in Electronic Design Automation",
        "authors": [
            "Yunsheng Bai",
            "Atefeh Sohrabizadeh",
            "Zongyue Qin",
            "Ziniu Hu",
            "Yizhou Sun",
            "Jason Cong"
        ],
        "published": "2023-05-18T09:44:18Z",
        "summary": "Recent years have witnessed the growing popularity of domain-specific\naccelerators (DSAs), such as Google's TPUs, for accelerating various\napplications such as deep learning, search, autonomous driving, etc. To\nfacilitate DSA designs, high-level synthesis (HLS) is used, which allows a\ndeveloper to compile a high-level description in the form of software code in C\nand C++ into a design in low-level hardware description languages (such as VHDL\nor Verilog) and eventually synthesized into a DSA on an ASIC\n(application-specific integrated circuit) or FPGA (field-programmable gate\narrays). However, existing HLS tools still require microarchitecture decisions,\nexpressed in terms of pragmas (such as directives for parallelization and\npipelining). To enable more people to design DSAs, it is desirable to automate\nsuch decisions with the help of deep learning for predicting the quality of HLS\ndesigns. This requires us a deeper understanding of the program, which is a\ncombination of original code and pragmas. Naturally, these programs can be\nconsidered as sequence data, for which large language models (LLM) can help. In\naddition, these programs can be compiled and converted into a control data flow\ngraph (CDFG), and the compiler also provides fine-grained alignment between the\ncode tokens and the CDFG nodes. However, existing works either fail to leverage\nboth modalities or combine the two in shallow or coarse ways. We propose ProgSG\nallowing the source code sequence modality and the graph modalities to interact\nwith each other in a deep and fine-grained way. To alleviate the scarcity of\nlabeled designs, a pre-training method is proposed based on a suite of\ncompiler's data flow analysis tasks. Experimental results on two benchmark\ndatasets show the superiority of ProgSG over baseline methods that either only\nconsider one modality or combine the two without utilizing the alignment\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2305.10838v2.pdf"
    },
    {
        "title": "Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants",
        "authors": [
            "Amal Haddad Haddad",
            "Damith Premasiri",
            "Tharindu Ranasinghe",
            "Ruslan Mitkov"
        ],
        "published": "2023-05-18T09:22:29Z",
        "summary": "The domain of Botany is rich with metaphorical terms. Those terms play an\nimportant role in the description and identification of flowers and plants.\nHowever, the identification of such terms in discourse is an arduous task. This\nleads in some cases to committing errors during translation processes and\nlexicographic tasks. The process is even more challenging when it comes to\nmachine translation, both in the cases of single-word terms and multi-word\nterms. One of the recent concerns of Natural Language Processing (NLP)\napplications and Machine Translation (MT) technologies is the automatic\nidentification of metaphor-based words in discourse through Deep Learning (DL).\nIn this study, we seek to fill this gap through the use of thirteen popular\ntransformer based models, as well as ChatGPT, and we show that discriminative\nmodels perform better than GPT-3.5 model with our best performer reporting\n92.2349% F1 score in metaphoric flower and plant names identification task.",
        "pdf_link": "https://arxiv.org/pdf/2305.10833v3.pdf"
    },
    {
        "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings",
        "authors": [
            "Qian Chen",
            "Wen Wang",
            "Qinglin Zhang",
            "Siqi Zheng",
            "Chong Deng",
            "Hai Yu",
            "Jiaqing Liu",
            "Yukun Ma",
            "Chong Zhang"
        ],
        "published": "2023-05-18T07:56:40Z",
        "summary": "Prior studies diagnose the anisotropy problem in sentence representations\nfrom pre-trained language models, e.g., BERT, without fine-tuning. Our analysis\nreveals that the sentence embeddings from BERT suffer from a bias towards\nuninformative words, limiting the performance in semantic textual similarity\n(STS) tasks. To address this bias, we propose a simple and efficient\nunsupervised approach, Diagonal Attention Pooling (Ditto), which weights words\nwith model-based importance estimations and computes the weighted average of\nword representations from pre-trained models as sentence embeddings. Ditto can\nbe easily applied to any pre-trained language model as a postprocessing\noperation. Compared to prior sentence embedding approaches, Ditto does not add\nparameters nor requires any learning. Empirical evaluations demonstrate that\nour proposed Ditto can alleviate the anisotropy problem and improve various\npre-trained models on STS tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.10786v2.pdf"
    },
    {
        "title": "Human Behavioral Benchmarking: Numeric Magnitude Comparison Effects in Large Language Models",
        "authors": [
            "Raj Sanjay Shah",
            "Vijay Marupudi",
            "Reba Koenen",
            "Khushi Bhardwaj",
            "Sashank Varma"
        ],
        "published": "2023-05-18T07:50:44Z",
        "summary": "Large Language Models (LLMs) do not differentially represent numbers, which\nare pervasive in text. In contrast, neuroscience research has identified\ndistinct neural representations for numbers and words. In this work, we\ninvestigate how well popular LLMs capture the magnitudes of numbers (e.g., that\n$4 < 5$) from a behavioral lens. Prior research on the representational\ncapabilities of LLMs evaluates whether they show human-level performance, for\ninstance, high overall accuracy on standard benchmarks. Here, we ask a\ndifferent question, one inspired by cognitive science: How closely do the\nnumber representations of LLMscorrespond to those of human language users, who\ntypically demonstrate the distance, size, and ratio effects? We depend on a\nlinking hypothesis to map the similarities among the model embeddings of number\nwords and digits to human response times. The results reveal surprisingly\nhuman-like representations across language models of different architectures,\ndespite the absence of the neural circuitry that directly supports these\nrepresentations in the human brain. This research shows the utility of\nunderstanding LLMs using behavioral benchmarks and points the way to future\nwork on the number representations of LLMs and their cognitive plausibility.",
        "pdf_link": "https://arxiv.org/pdf/2305.10782v3.pdf"
    },
    {
        "title": "Ethical ChatGPT: Concerns, Challenges, and Commandments",
        "authors": [
            "Jianlong Zhou",
            "Heimo Müller",
            "Andreas Holzinger",
            "Fang Chen"
        ],
        "published": "2023-05-18T02:04:13Z",
        "summary": "Large language models, e.g. ChatGPT are currently contributing enormously to\nmake artificial intelligence even more popular, especially among the general\npopulation. However, such chatbot models were developed as tools to support\nnatural language communication between humans. Problematically, it is very much\na ``statistical correlation machine\" (correlation instead of causality) and\nthere are indeed ethical concerns associated with the use of AI language models\nsuch as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights\nspecific ethical concerns on ChatGPT and articulates key challenges when\nChatGPT is used in various applications. Practical commandments for different\nstakeholders of ChatGPT are also proposed that can serve as checklist\nguidelines for those applying ChatGPT in their applications. These commandment\nexamples are expected to motivate the ethical use of ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.10646v1.pdf"
    },
    {
        "title": "Are Large Language Models Fit For Guided Reading?",
        "authors": [
            "Peter Ochieng"
        ],
        "published": "2023-05-18T02:03:55Z",
        "summary": "This paper looks at the ability of large language models to participate in\neducational guided reading. We specifically, evaluate their ability to generate\nmeaningful questions from the input text, generate diverse questions both in\nterms of content coverage and difficulty of the questions and evaluate their\nability to recommend part of the text that a student should re-read based on\nthe student's responses to the questions. Based on our evaluation of ChatGPT\nand Bard, we report that,\n  1) Large language models are able to generate high quality meaningful\nquestions that have high correlation with the input text, 2) They generate\ndiverse question that cover most topics in the input text even though this\nability is significantly degraded as the input text increases, 3)The large\nlanguage models are able to generate both low and high cognitive questions even\nthough they are significantly biased toward low cognitive question, 4) They are\nable to effectively summarize responses and extract a portion of text that\nshould be re-read.",
        "pdf_link": "https://arxiv.org/pdf/2305.10645v2.pdf"
    },
    {
        "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
        "authors": [
            "Jiannan Xiang",
            "Tianhua Tao",
            "Yi Gu",
            "Tianmin Shu",
            "Zirui Wang",
            "Zichao Yang",
            "Zhiting Hu"
        ],
        "published": "2023-05-18T00:35:38Z",
        "summary": "While large language models (LMs) have shown remarkable capabilities across\nnumerous tasks, they often struggle with simple reasoning and planning in\nphysical environments, such as understanding object permanence or planning\nhousehold activities. The limitation arises from the fact that LMs are trained\nonly on written text and miss essential embodied knowledge and skills. In this\npaper, we propose a new paradigm of enhancing LMs by finetuning them with world\nmodels, to gain diverse embodied knowledge while retaining their general\nlanguage capabilities. Our approach deploys an embodied agent in a world model,\nparticularly a simulator of the physical world (VirtualHome), and acquires a\ndiverse set of embodied experiences through both goal-oriented planning and\nrandom exploration. These experiences are then used to finetune LMs to teach\ndiverse abilities of reasoning and acting in the physical world, e.g., planning\nand completing goals, object permanence and tracking, etc. Moreover, it is\ndesirable to preserve the generality of LMs during finetuning, which\nfacilitates generalizing the embodied knowledge across tasks rather than being\ntied to specific simulations. We thus further introduce the classical (EWC) for\nselective weight updates, combined with low-rank adapters (LoRA) for training\nefficiency. Extensive experiments show our approach substantially improves base\nLMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs\n(1.3B, 6B, and 13B) enhanced by our approach match or even outperform much\nlarger LMs (e.g., ChatGPT).",
        "pdf_link": "https://arxiv.org/pdf/2305.10626v3.pdf"
    },
    {
        "title": "Temporal Knowledge Graph Forecasting Without Knowledge Using In-Context Learning",
        "authors": [
            "Dong-Ho Lee",
            "Kian Ahrabian",
            "Woojeong Jin",
            "Fred Morstatter",
            "Jay Pujara"
        ],
        "published": "2023-05-17T23:50:28Z",
        "summary": "Temporal knowledge graph (TKG) forecasting benchmarks challenge models to\npredict future facts using knowledge of past facts. In this paper, we apply\nlarge language models (LLMs) to these benchmarks using in-context learning\n(ICL). We investigate whether and to what extent LLMs can be used for TKG\nforecasting, especially without any fine-tuning or explicit modules for\ncapturing structural and temporal information. For our experiments, we present\na framework that converts relevant historical facts into prompts and generates\nranked predictions using token probabilities. Surprisingly, we observe that\nLLMs, out-of-the-box, perform on par with state-of-the-art TKG models carefully\ndesigned and trained for TKG forecasting. Our extensive evaluation presents\nperformances across several models and datasets with different characteristics,\ncompares alternative heuristics for preparing contextual information, and\ncontrasts to prominent TKG methods and simple frequency and recency baselines.\nWe also discover that using numerical indices instead of entity/relation names,\ni.e., hiding semantic information, does not significantly affect the\nperformance ($\\pm$0.4\\% Hit@1). This shows that prior semantic knowledge is\nunnecessary; instead, LLMs can leverage the existing patterns in the context to\nachieve such performance. Our analysis also reveals that ICL enables LLMs to\nlearn irregular patterns from the historical context, going beyond simple\npredictions based on common or recent information.",
        "pdf_link": "https://arxiv.org/pdf/2305.10613v3.pdf"
    },
    {
        "title": "Solving Cosine Similarity Underestimation between High Frequency Words by L2 Norm Discounting",
        "authors": [
            "Saeth Wannasuphoprasit",
            "Yi Zhou",
            "Danushka Bollegala"
        ],
        "published": "2023-05-17T23:41:30Z",
        "summary": "Cosine similarity between two words, computed using their contextualised\ntoken embeddings obtained from masked language models (MLMs) such as BERT has\nshown to underestimate the actual similarity between those words (Zhou et al.,\n2022). This similarity underestimation problem is particularly severe for\nhighly frequent words. Although this problem has been noted in prior work, no\nsolution has been proposed thus far. We observe that the L2 norm of\ncontextualised embeddings of a word correlates with its log-frequency in the\npretraining corpus. Consequently, the larger L2 norms associated with the\nhighly frequent words reduce the cosine similarity values measured between\nthem, thus underestimating the similarity scores. To solve this issue, we\npropose a method to discount the L2 norm of a contextualised word embedding by\nthe frequency of that word in a corpus when measuring the cosine similarities\nbetween words. We show that the so called stop words behave differently from\nthe rest of the words, which require special consideration during their\ndiscounting process. Experimental results on a contextualised word similarity\ndataset show that our proposed discounting method accurately solves the\nsimilarity underestimation problem.",
        "pdf_link": "https://arxiv.org/pdf/2305.10610v1.pdf"
    },
    {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "Thomas L. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "published": "2023-05-17T23:16:17Z",
        "summary": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.",
        "pdf_link": "https://arxiv.org/pdf/2305.10601v2.pdf"
    },
    {
        "title": "Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt",
        "authors": [
            "Zhaozhuo Xu",
            "Zirui Liu",
            "Beidi Chen",
            "Yuxin Tang",
            "Jue Wang",
            "Kaixiong Zhou",
            "Xia Hu",
            "Anshumali Shrivastava"
        ],
        "published": "2023-05-17T20:45:13Z",
        "summary": "While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.",
        "pdf_link": "https://arxiv.org/pdf/2305.11186v2.pdf"
    },
    {
        "title": "Statistical Knowledge Assessment for Large Language Models",
        "authors": [
            "Qingxiu Dong",
            "Jingjing Xu",
            "Lingpeng Kong",
            "Zhifang Sui",
            "Lei Li"
        ],
        "published": "2023-05-17T18:54:37Z",
        "summary": "Given varying prompts regarding a factoid question, can a large language\nmodel (LLM) reliably generate factually correct answers? Existing LLMs may\ngenerate distinct responses for different prompts. In this paper, we study the\nproblem of quantifying knowledge contained in an LLM regarding a given set of\nfacts. We propose KaRR, a statistical approach to assess factual knowledge for\nLLMs. The main idea is to estimate the ratio of LLM generating text\ncorresponding to the answer entity given diverse prompts of the subject and the\nquerying relation, versus it generating by random chances. Our assessment suite\ncontains a comprehensive set of 994,123 entities and 600 relations, with\n1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes,\nincluding LLaMA, Alpaca, OPT, etc. Experiments show that our results have a\nstrong correlation (0.43 Kendall's $\\tau$) with the results of human assessment\non LLMs. Our results reveal that the knowledge in LLMs with the same backbone\narchitecture adheres to the scaling law, while tuning on instruction-following\ndata sometimes compromises the model's capability to generate factually correct\ntext reliably.",
        "pdf_link": "https://arxiv.org/pdf/2305.10519v2.pdf"
    },
    {
        "title": "ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages",
        "authors": [
            "Sourojit Ghosh",
            "Aylin Caliskan"
        ],
        "published": "2023-05-17T18:30:05Z",
        "summary": "In this multicultural age, language translation is one of the most performed\ntasks, and it is becoming increasingly AI-moderated and automated. As a novel\nAI system, ChatGPT claims to be proficient in such translation tasks and in\nthis paper, we put that claim to the test. Specifically, we examine ChatGPT's\naccuracy in translating between English and languages that exclusively use\ngender-neutral pronouns. We center this study around Bengali, the 7$^{th}$ most\nspoken language globally, but also generalize our findings across five other\nlanguages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT\nperpetuates gender defaults and stereotypes assigned to certain occupations\n(e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to\nwork), as it converts gender-neutral pronouns in languages to `he' or `she'. We\nalso observe ChatGPT completely failing to translate the English gender-neutral\npronoun `they' into equivalent gender-neutral pronouns in other languages, as\nit produces translations that are incoherent and incorrect. While it does\nrespect and provide appropriately gender-marked versions of Bengali words when\nprompted with gender information in English, ChatGPT appears to confer a higher\nrespect to men than to women in the same occupation. We conclude that ChatGPT\nexhibits the same gender biases which have been demonstrated for tools like\nGoogle Translate or MS Translator, as we provide recommendations for a human\ncentered approach for future designers of AIs that perform language translation\nto better accommodate such low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2305.10510v1.pdf"
    },
    {
        "title": "Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families",
        "authors": [
            "Stefania Druga",
            "Nancy Otero"
        ],
        "published": "2023-05-17T17:52:25Z",
        "summary": "How can AI enhance creative coding experiences for families? This study\nexplores the potential of large language models (LLMs) in helping families with\ncreative coding using Scratch. Based on our previous user study involving a\nprototype AI assistant, we devised three evaluation scenarios to determine if\nLLMs could help families comprehend game code, debug programs, and generate new\nideas for future projects. We utilized 22 Scratch projects for each scenario\nand generated responses from LLMs with and without practice tasks, resulting in\n120 creative coding support scenario datasets. In addition, the authors\nindependently evaluated their precision, pedagogical value, and age-appropriate\nlanguage. Our findings show that LLMs achieved an overall success rate of more\nthan 80\\% on the different tasks and evaluation criteria. This research offers\nvaluable information on using LLMs for creative family coding and presents\ndesign guidelines for future AI-supported coding applications. Our evaluation\nframework, together with our labeled evaluation data, is publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.10417v1.pdf"
    },
    {
        "title": "BAD: BiAs Detection for Large Language Models in the context of candidate screening",
        "authors": [
            "Nam Ho Koh",
            "Joseph Plata",
            "Joyce Chai"
        ],
        "published": "2023-05-17T17:47:31Z",
        "summary": "Application Tracking Systems (ATS) have allowed talent managers, recruiters,\nand college admissions committees to process large volumes of potential\ncandidate applications efficiently. Traditionally, this screening process was\nconducted manually, creating major bottlenecks due to the quantity of\napplications and introducing many instances of human bias. The advent of large\nlanguage models (LLMs) such as ChatGPT and the potential of adopting methods to\ncurrent automated application screening raises additional bias and fairness\nissues that must be addressed. In this project, we wish to identify and\nquantify the instances of social bias in ChatGPT and other OpenAI LLMs in the\ncontext of candidate screening in order to demonstrate how the use of these\nmodels could perpetuate existing biases and inequalities in the hiring process.",
        "pdf_link": "https://arxiv.org/pdf/2305.10407v1.pdf"
    },
    {
        "title": "Predicting Side Effect of Drug Molecules using Recurrent Neural Networks",
        "authors": [
            "Collin Beaudoin",
            "Koustubh Phalak",
            "Swaroop Ghosh"
        ],
        "published": "2023-05-17T16:56:19Z",
        "summary": "Identification and verification of molecular properties such as side effects\nis one of the most important and time-consuming steps in the process of\nmolecule synthesis. For example, failure to identify side effects before\nsubmission to regulatory groups can cost millions of dollars and months of\nadditional research to the companies. Failure to identify side effects during\nthe regulatory review can also cost lives. The complexity and expense of this\ntask have made it a candidate for a machine learning-based solution. Prior\napproaches rely on complex model designs and excessive parameter counts for\nside effect predictions. We believe reliance on complex models only shifts the\ndifficulty away from chemists rather than alleviating the issue. Implementing\nlarge models is also expensive without prior access to high-performance\ncomputers. We propose a heuristic approach that allows for the utilization of\nsimple neural networks, specifically the recurrent neural network, with a 98+%\nreduction in the number of required parameters compared to available large\nlanguage models while still obtaining near identical results as top-performing\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.10473v1.pdf"
    },
    {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
            "Yifan Li",
            "Yifan Du",
            "Kun Zhou",
            "Jinpeng Wang",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-17T16:34:01Z",
        "summary": "Inspired by the superior language abilities of large language models (LLM),\nlarge vision-language models (LVLM) have been recently explored by integrating\npowerful LLMs for improving the performance on complex multimodal tasks.\nDespite the promising progress on LVLMs, we find that LVLMs suffer from the\nhallucination problem, i.e. they tend to generate objects that are inconsistent\nwith the target images in the descriptions. To investigate it, this work\npresents the first systematic study on object hallucination of LVLMs. We\nconduct the evaluation experiments on several representative LVLMs, and show\nthat they mostly suffer from severe object hallucination issue. We further\ndiscuss that the visual instructions may influence the hallucination, and find\nthat: objects that frequently occur in the visual instructions or co-occur with\nthe image objects, are obviously prone to be hallucinated by LVLMs. Besides, we\nfind that existing evaluation methods might be affected by the input\ninstructions and generation styles of LVLMs. Thus, we further design an\nimproved evaluation method for object hallucination by proposing a\npolling-based query method called POPE. Experiment results demonstrate that our\nPOPE can evaluate the object hallucination in a more stable and flexible way.\nOur codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
        "pdf_link": "https://arxiv.org/pdf/2305.10355v3.pdf"
    },
    {
        "title": "Controllable Speaking Styles Using a Large Language Model",
        "authors": [
            "Atli Thor Sigurgeirsson",
            "Simon King"
        ],
        "published": "2023-05-17T16:01:50Z",
        "summary": "Reference-based Text-to-Speech (TTS) models can generate multiple,\nprosodically-different renditions of the same target text. Such models jointly\nlearn a latent acoustic space during training, which can be sampled from during\ninference. Controlling these models during inference typically requires finding\nan appropriate reference utterance, which is non-trivial.\n  Large generative language models (LLMs) have shown excellent performance in\nvarious language-related tasks. Given only a natural language query text (the\nprompt), such models can be used to solve specific, context-dependent tasks.\nRecent work in TTS has attempted similar prompt-based control of novel speaking\nstyle generation. Those methods do not require a reference utterance and can,\nunder ideal conditions, be controlled with only a prompt. But existing methods\ntypically require a prompt-labelled speech corpus for jointly training a\nprompt-conditioned encoder.\n  In contrast, we instead employ an LLM to directly suggest prosodic\nmodifications for a controllable TTS model, using contextual information\nprovided in the prompt. The prompt can be designed for a multitude of tasks.\nHere, we give two demonstrations: control of speaking style; prosody\nappropriate for a given dialogue context. The proposed method is rated most\nappropriate in 50% of cases vs. 31% for a baseline model.",
        "pdf_link": "https://arxiv.org/pdf/2305.10321v2.pdf"
    },
    {
        "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
        "authors": [
            "Hanxu Hu",
            "Hongyuan Lu",
            "Huajian Zhang",
            "Yun-Ze Song",
            "Wai Lam",
            "Yue Zhang"
        ],
        "published": "2023-05-17T15:07:50Z",
        "summary": "In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning",
        "pdf_link": "https://arxiv.org/pdf/2305.10276v6.pdf"
    },
    {
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "authors": [
            "Wanjun Zhong",
            "Lianghong Guo",
            "Qiqi Gao",
            "He Ye",
            "Yanlin Wang"
        ],
        "published": "2023-05-17T14:40:29Z",
        "summary": "Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.",
        "pdf_link": "https://arxiv.org/pdf/2305.10250v3.pdf"
    },
    {
        "title": "Language Model Tokenizers Introduce Unfairness Between Languages",
        "authors": [
            "Aleksandar Petrov",
            "Emanuele La Malfa",
            "Philip H. S. Torr",
            "Adel Bibi"
        ],
        "published": "2023-05-17T14:17:57Z",
        "summary": "Recent language models have shown impressive multilingual performance, even\nwhen not explicitly trained for it. Despite this, there are concerns about the\nquality of their outputs across different languages. In this paper, we show how\ndisparity in the treatment of different languages arises at the tokenization\nstage, well before a model is even invoked. The same text translated into\ndifferent languages can have drastically different tokenization lengths, with\ndifferences up to 15 times in some cases. These disparities persist even for\ntokenizers that are intentionally trained for multilingual support.\nCharacter-level and byte-level models also exhibit over 4 times the difference\nin the encoding length for some language pairs. This induces unfair treatment\nfor some language communities in regard to the cost of accessing commercial\nlanguage services, the processing time and latency, as well as the amount of\ncontent that can be provided as context to the models. Therefore, we make the\ncase that we should train future language models using multilingually fair\nsubword tokenizers.",
        "pdf_link": "https://arxiv.org/pdf/2305.15425v2.pdf"
    },
    {
        "title": "Large Language Models Leverage External Knowledge to Extend Clinical Insight Beyond Language Boundaries",
        "authors": [
            "Jiageng Wu",
            "Xian Wu",
            "Zhaopeng Qiu",
            "Minghui Li",
            "Yingying Zhang",
            "Yefeng Zheng",
            "Changzheng Yuan",
            "Jie Yang"
        ],
        "published": "2023-05-17T12:31:26Z",
        "summary": "$\\textbf{Objectives}$: Large Language Models (LLMs) such as ChatGPT and\nMed-PaLM have excelled in various medical question-answering tasks. However,\nthese English-centric models encounter challenges in non-English clinical\nsettings, primarily due to limited clinical knowledge in respective languages,\na consequence of imbalanced training corpora. We systematically evaluate LLMs\nin the Chinese medical context and develop a novel in-context learning\nframework to enhance their performance.\n  $\\textbf{Materials and Methods}$: The latest China National Medical Licensing\nExamination (CNMLE-2022) served as the benchmark. We collected 53 medical books\nand 381,149 medical questions to construct the medical knowledge base and\nquestion bank. The proposed Knowledge and Few-shot Enhancement In-context\nLearning (KFE) framework leverages the in-context learning ability of LLMs to\nintegrate diverse external clinical knowledge sources. We evaluated KFE with\nChatGPT(GPT3.5), GPT4, Baichuan2(BC2)-7B, and BC2-13B in CNMLE-2022 and\ninvestigated the effectiveness of different pathways for incorporating LLMs\nwith medical knowledge from 7 perspectives.\n  $\\textbf{Results}$: Directly applying ChatGPT failed to qualify for the\nCNMLE-2022 at a score of 51. Cooperated with the KFE, the LLMs with varying\nsizes yielded consistent and significant improvements. The ChatGPT's\nperformance surged to 70.04 and GPT-4 achieved the highest score of 82.59. This\nsurpasses the qualification threshold (60) and exceeds the average human score\nof 68.70. It also enabled a smaller BC2-13B to pass the examination, showcasing\nthe great potential in low-resource settings.\n  $\\textbf{Conclusion}$: By synergizing medical knowledge through in-context\nlearning, LLM can extend clinical insight beyond language barriers,\nsignificantly reducing language-related disparities of LLM applications and\nensuring global benefit in healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2305.10163v4.pdf"
    },
    {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "published": "2023-05-17T11:55:32Z",
        "summary": "We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player's negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model's negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game's rules or cannot incorporate AI feedback for further\nimprovement. (2) Models' abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.",
        "pdf_link": "https://arxiv.org/pdf/2305.10142v1.pdf"
    },
    {
        "title": "Can Language Models Solve Graph Problems in Natural Language?",
        "authors": [
            "Heng Wang",
            "Shangbin Feng",
            "Tianxing He",
            "Zhaoxuan Tan",
            "Xiaochuang Han",
            "Yulia Tsvetkov"
        ],
        "published": "2023-05-17T08:29:21Z",
        "summary": "Large language models (LLMs) are increasingly adopted for a variety of tasks\nwith implicit graphical structures, such as planning in robotics, multi-hop\nquestion answering or knowledge probing, structured commonsense reasoning, and\nmore. While LLMs have advanced the state-of-the-art on these tasks with\nstructure implications, whether LLMs could explicitly process textual\ndescriptions of graphs and structures, map them to grounded conceptual spaces,\nand perform structured operations remains underexplored. To this end, we\npropose NLGraph (Natural Language Graph), a comprehensive benchmark of\ngraph-based problem solving designed in natural language. NLGraph contains\n29,370 problems, covering eight graph reasoning tasks with varying complexity\nfrom simple tasks such as connectivity and shortest path up to complex problems\nsuch as maximum flow and simulating graph neural networks. We evaluate LLMs\n(GPT-3/4) with various prompting approaches on the NLGraph benchmark and find\nthat 1) language models do demonstrate preliminary graph reasoning abilities,\n2) the benefit of advanced prompting and in-context learning diminishes on more\ncomplex graph problems, while 3) LLMs are also (un)surprisingly brittle in the\nface of spurious correlations in graph and problem settings. We then propose\nBuild-a-Graph Prompting and Algorithmic Prompting, two instruction-based\napproaches to enhance LLMs in solving natural language graph problems.\nBuild-a-Graph and Algorithmic prompting improve the performance of LLMs on\nNLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to\nsolve the most complicated graph reasoning tasks in our setup with language\nmodels remains an open research question. The NLGraph benchmark and evaluation\ncode are available at https://github.com/Arthur-Heng/NLGraph.",
        "pdf_link": "https://arxiv.org/pdf/2305.10037v3.pdf"
    },
    {
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
        "authors": [
            "Wenjun Peng",
            "Jingwei Yi",
            "Fangzhao Wu",
            "Shangxi Wu",
            "Bin Zhu",
            "Lingjuan Lyu",
            "Binxing Jiao",
            "Tong Xu",
            "Guangzhong Sun",
            "Xing Xie"
        ],
        "published": "2023-05-17T08:28:54Z",
        "summary": "Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.",
        "pdf_link": "https://arxiv.org/pdf/2305.10036v3.pdf"
    },
    {
        "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
        "authors": [
            "Shangbin Feng",
            "Weijia Shi",
            "Yuyang Bai",
            "Vidhisha Balachandran",
            "Tianxing He",
            "Yulia Tsvetkov"
        ],
        "published": "2023-05-17T05:25:27Z",
        "summary": "By design, large language models (LLMs) are static general-purpose models,\nexpensive to retrain or update frequently. As they are increasingly adopted for\nknowledge-intensive tasks, it becomes evident that these design choices lead to\nfailures to generate factual, relevant, and up-to-date knowledge. To this end,\nwe propose Knowledge Card, a modular framework to plug in new factual and\nrelevant knowledge into general-purpose LLMs. We first introduce knowledge\ncards -- specialized language models trained on corpora from specific domains\nand sources. Knowledge cards serve as parametric repositories that are selected\nat inference time to generate background knowledge for the base LLM. We then\npropose three content selectors to dynamically select and retain information in\ndocuments generated by knowledge cards, specifically controlling for relevance,\nbrevity, and factuality of outputs. Finally, we propose two complementary\nintegration approaches to augment the base LLM with the (relevant, factual)\nknowledge curated from the specialized LMs. Through extensive experiments, we\ndemonstrate that Knowledge Card achieves state-of-the-art performance on six\nbenchmark datasets. Ultimately, Knowledge Card framework enables dynamic\nsynthesis and updates of knowledge from diverse domains. Its modularity will\nensure that relevant knowledge can be continuously updated through the\ncollective efforts of the research community.",
        "pdf_link": "https://arxiv.org/pdf/2305.09955v3.pdf"
    },
    {
        "title": "\"I'm fully who I am\": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
        "authors": [
            "Anaelia Ovalle",
            "Palash Goyal",
            "Jwala Dhamala",
            "Zachary Jaggers",
            "Kai-Wei Chang",
            "Aram Galstyan",
            "Richard Zemel",
            "Rahul Gupta"
        ],
        "published": "2023-05-17T04:21:45Z",
        "summary": "Transgender and non-binary (TGNB) individuals disproportionately experience\ndiscrimination and exclusion from daily life. Given the recent popularity and\nadoption of language generation technologies, the potential to further\nmarginalize this population only grows. Although a multitude of NLP fairness\nliterature focuses on illuminating and addressing gender biases, assessing\ngender harms for TGNB identities requires understanding how such identities\nuniquely interact with societal gender norms and how they differ from gender\nbinary-centric perspectives. Such measurement frameworks inherently require\ncentering TGNB voices to help guide the alignment between gender-inclusive NLP\nand whom they are intended to serve. Towards this goal, we ground our work in\nthe TGNB community and existing interdisciplinary literature to assess how the\nsocial reality surrounding experienced marginalization of TGNB persons\ncontributes to and persists within Open Language Generation (OLG). This social\nknowledge serves as a guide for evaluating popular large language models (LLMs)\non two key aspects: (1) misgendering and (2) harmful responses to gender\ndisclosure. To do this, we introduce TANGO, a dataset of template-based\nreal-world text curated from a TGNB-oriented community. We discover a dominance\nof binary gender norms reflected by the models; LLMs least misgendered subjects\nin generated text when triggered by prompts whose subjects used binary\npronouns. Meanwhile, misgendering was most prevalent when triggering generation\nwith singular they and neopronouns. When prompted with gender disclosures, TGNB\ndisclosure generated the most stigmatizing language and scored most toxic, on\naverage. Our findings warrant further research on how TGNB harms manifest in\nLLMs and serve as a broader case study toward concretely grounding the design\nof gender-inclusive AI in community voices and interdisciplinary literature.",
        "pdf_link": "https://arxiv.org/pdf/2305.09941v4.pdf"
    },
    {
        "title": "Smaller Language Models are Better Black-box Machine-Generated Text Detectors",
        "authors": [
            "Niloofar Mireshghallah",
            "Justus Mattern",
            "Sicun Gao",
            "Reza Shokri",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2023-05-17T00:09:08Z",
        "summary": "With the advent of fluent generative language models that can produce\nconvincing utterances very similar to those written by humans, distinguishing\nwhether a piece of text is machine-generated or human-written becomes more\nchallenging and more important, as such models could be used to spread\nmisinformation, fake news, fake reviews and to mimic certain authors and\nfigures. To this end, there have been a slew of methods proposed to detect\nmachine-generated text. Most of these methods need access to the logits of the\ntarget model or need the ability to sample from the target. One such black-box\ndetection method relies on the observation that generated text is locally\noptimal under the likelihood function of the generator, while human-written\ntext is not. We find that overall, smaller and partially-trained models are\nbetter universal text detectors: they can more precisely detect text generated\nfrom both small and larger models. Interestingly, we find that whether the\ndetector and generator were trained on the same data is not critically\nimportant to the detection success. For instance the OPT-125M model has an AUC\nof 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT\nfamily, GPTJ-6B, has AUC of 0.45.",
        "pdf_link": "https://arxiv.org/pdf/2305.09859v4.pdf"
    },
    {
        "title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs",
        "authors": [
            "Jiao Chen",
            "Luyi Ma",
            "Xiaohan Li",
            "Nikhil Thakurdesai",
            "Jianpeng Xu",
            "Jason H. D. Cho",
            "Kaushiki Nag",
            "Evren Korpeoglu",
            "Sushant Kumar",
            "Kannan Achan"
        ],
        "published": "2023-05-17T00:08:36Z",
        "summary": "Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system\nperformance by providing structured information about entities and their\nrelationships, such as complementary or substitutable relations between\nproducts or product types, which can be utilized in recommender systems.\nHowever, relation labeling in KGs remains a challenging task due to the dynamic\nnature of e-commerce domains and the associated cost of human labor. Recently,\nbreakthroughs in Large Language Models (LLMs) have shown surprising results in\nnumerous natural language processing tasks. In this paper, we conduct an\nempirical study of LLMs for relation labeling in e-commerce KGs, investigating\ntheir powerful learning capabilities in natural language and effectiveness in\npredicting relations between product types with limited labeled data. We\nevaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets,\ndemonstrating their ability to achieve competitive performance compared to\nhumans on relation labeling tasks using just 1 to 5 labeled examples per\nrelation. Additionally, we experiment with different prompt engineering\ntechniques to examine their impact on model performance. Our results show that\nLLMs significantly outperform existing KG completion models in relation\nlabeling for e-commerce KGs and exhibit performance strong enough to replace\nhuman labeling.",
        "pdf_link": "https://arxiv.org/pdf/2305.09858v1.pdf"
    },
    {
        "title": "Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites",
        "authors": [
            "Hans W. A. Hanley",
            "Zakir Durumeric"
        ],
        "published": "2023-05-16T21:51:01Z",
        "summary": "As large language models (LLMs) like ChatGPT have gained traction, an\nincreasing number of news websites have begun utilizing them to generate\narticles. However, not only can these language models produce factually\ninaccurate articles on reputable websites but disreputable news sites can\nutilize LLMs to mass produce misinformation. To begin to understand this\nphenomenon, we present one of the first large-scale studies of the prevalence\nof synthetic articles within online news media. To do this, we train a\nDeBERTa-based synthetic news detector and classify over 15.46 million articles\nfrom 3,074 misinformation and mainstream news websites. We find that between\nJanuary 1, 2022, and May 1, 2023, the relative number of synthetic news\narticles increased by 57.3% on mainstream websites while increasing by 474% on\nmisinformation sites. We find that this increase is largely driven by smaller\nless popular websites. Analyzing the impact of the release of ChatGPT using an\ninterrupted-time-series, we show that while its release resulted in a marked\nincrease in synthetic articles on small sites as well as misinformation news\nwebsites, there was not a corresponding increase on large mainstream news\nwebsites.",
        "pdf_link": "https://arxiv.org/pdf/2305.09820v5.pdf"
    },
    {
        "title": "Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models",
        "authors": [
            "Evan King",
            "Haoxiang Yu",
            "Sangsu Lee",
            "Christine Julien"
        ],
        "published": "2023-05-16T20:52:04Z",
        "summary": "Smart home assistants function best when user commands are direct and\nwell-specified (e.g., \"turn on the kitchen light\"), or when a hard-coded\nroutine specifies the response. In more natural communication, however, human\nspeech is unconstrained, often describing goals (e.g., \"make it cozy in here\"\nor \"help me save energy\") rather than indicating specific target devices and\nactions to take on those devices. Current systems fail to understand these\nunder-specified commands since they cannot reason about devices and settings as\nthey relate to human situations. We introduce large language models (LLMs) to\nthis problem space, exploring their use for controlling devices and creating\nautomation routines in response to under-specified user commands in smart\nhomes. We empirically study the baseline quality and failure modes of\nLLM-created action plans with a survey of age-diverse users. We find that LLMs\ncan reason creatively to achieve challenging goals, but they experience\npatterns of failure that diminish their usefulness. We address these gaps with\nSasha, a smarter smart home assistant. Sasha responds to loosely-constrained\ncommands like \"make it cozy\" or \"help me sleep better\" by executing plans to\nachieve user goals, e.g., setting a mood with available devices, or devising\nautomation routines. We implement and evaluate Sasha in a hands-on user study,\nshowing the capabilities and limitations of LLM-driven smart homes when faced\nwith unconstrained user-generated scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.09802v3.pdf"
    },
    {
        "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
        "authors": [
            "Jane Pan",
            "Tianyu Gao",
            "Howard Chen",
            "Danqi Chen"
        ],
        "published": "2023-05-16T18:05:19Z",
        "summary": "Large language models (LLMs) exploit in-context learning (ICL) to solve tasks\nwith only a few demonstrations, but its mechanisms are not yet well-understood.\nSome works suggest that LLMs only recall already learned concepts from\npre-training, while others hint that ICL performs implicit learning over\ndemonstrations. We characterize two ways through which ICL leverages\ndemonstrations. Task recognition (TR) captures the extent to which LLMs can\nrecognize a task through demonstrations -- even without ground-truth labels --\nand apply their pre-trained priors, whereas task learning (TL) is the ability\nto capture new input-label mappings unseen in pre-training. Using a wide range\nof classification datasets and three LLM families (GPT-3, LLaMA and OPT), we\ndesign controlled experiments to disentangle the roles of TR and TL in ICL. We\nshow that (1) models can achieve non-trivial performance with only TR, and TR\ndoes not further improve with larger models or more demonstrations; (2) LLMs\nacquire TL as the model scales, and TL's performance consistently improves with\nmore demonstrations in context. Our findings unravel two different forces\nbehind ICL and we advocate for discriminating them in future ICL research due\nto their distinct nature.",
        "pdf_link": "https://arxiv.org/pdf/2305.09731v1.pdf"
    },
    {
        "title": "SatLM: Satisfiability-Aided Language Models Using Declarative Prompting",
        "authors": [
            "Xi Ye",
            "Qiaochu Chen",
            "Isil Dillig",
            "Greg Durrett"
        ],
        "published": "2023-05-16T17:55:51Z",
        "summary": "Prior work has combined chain-of-thought prompting in large language models\n(LLMs) with programmatic representations to perform effective and transparent\nreasoning. While such an approach works well for tasks that only require\nforward reasoning (e.g., straightforward arithmetic), it is less effective for\nconstraint solving problems that require more sophisticated planning and\nsearch. In this paper, we propose a new satisfiability-aided language modeling\n(SatLM) approach for improving the reasoning capabilities of LLMs. We use an\nLLM to generate a declarative task specification rather than an imperative\nprogram and leverage an off-the-shelf automated theorem prover to derive the\nfinal answer. This approach has two key advantages. The declarative\nspecification is closer to the problem description than the reasoning steps\nare, so the LLM can parse it out of the description more accurately.\nFurthermore, by offloading the actual reasoning task to an automated theorem\nprover, our approach can guarantee the correctness of the answer with respect\nto the parsed specification and avoid planning errors in the solving process.\nWe evaluate SATLM on 8 different datasets and show that it consistently\noutperforms program-aided LMs in the imperative paradigm. In particular, SATLM\noutperforms program-aided LMs by 23% on a challenging subset of the GSM\narithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and\nBoardgameQA, surpassing previous models that are trained on the respective\ntraining sets.",
        "pdf_link": "https://arxiv.org/pdf/2305.09656v3.pdf"
    },
    {
        "title": "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction",
        "authors": [
            "Junsol Kim",
            "Byungkyu Lee"
        ],
        "published": "2023-05-16T17:13:07Z",
        "summary": "Large language models (LLMs) that produce human-like responses have begun to\nrevolutionize research practices in the social sciences. We develop a novel\nmethodological framework that fine-tunes LLMs with repeated cross-sectional\nsurveys to incorporate the meaning of survey questions, individual beliefs, and\ntemporal contexts for opinion prediction. We introduce two new emerging\napplications of the AI-augmented survey: retrodiction (i.e., predict year-level\nmissing responses) and unasked opinion prediction (i.e., predict entirely\nmissing responses). Among 3,110 binarized opinions from 68,846 Americans in the\nGeneral Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in\nretrodiction (AUC = 0.86 for personal opinion prediction, $\\rho$ = 0.98 for\npublic opinion prediction). These remarkable prediction capabilities allow us\nto fill in missing trends with high confidence and pinpoint when public\nattitudes changed, such as the rising support for same-sex marriage. On the\nother hand, our fine-tuned Alpaca-7b models show modest success in unasked\nopinion prediction (AUC = 0.73, $\\rho$ = 0.67). We discuss practical\nconstraints and ethical concerns regarding individual autonomy and privacy when\nusing LLMs for opinion prediction. Our study demonstrates that LLMs and surveys\ncan mutually enhance each other's capabilities: LLMs can broaden survey\npotential, while surveys can improve the alignment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.09620v3.pdf"
    },
    {
        "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
        "authors": [
            "Karan Singhal",
            "Tao Tu",
            "Juraj Gottweis",
            "Rory Sayres",
            "Ellery Wulczyn",
            "Le Hou",
            "Kevin Clark",
            "Stephen Pfohl",
            "Heather Cole-Lewis",
            "Darlene Neal",
            "Mike Schaekermann",
            "Amy Wang",
            "Mohamed Amin",
            "Sami Lachgar",
            "Philip Mansfield",
            "Sushant Prakash",
            "Bradley Green",
            "Ewa Dominowska",
            "Blaise Aguera y Arcas",
            "Nenad Tomasev",
            "Yun Liu",
            "Renee Wong",
            "Christopher Semturs",
            "S. Sara Mahdavi",
            "Joelle Barral",
            "Dale Webster",
            "Greg S. Corrado",
            "Yossi Matias",
            "Shekoofeh Azizi",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2023-05-16T17:11:29Z",
        "summary": "Recent artificial intelligence (AI) systems have reached milestones in \"grand\nchallenges\" ranging from Go to protein-folding. The capability to retrieve\nmedical knowledge, reason over it, and answer medical questions comparably to\nphysicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical\nquestion answering; Med-PaLM was the first model to exceed a \"passing\" score in\nUS Medical Licensing Examination (USMLE) style questions with a score of 67.2%\non the MedQA dataset. However, this and other prior work suggested significant\nroom for improvement, especially when models' answers were compared to\nclinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by\nleveraging a combination of base LLM improvements (PaLM 2), medical domain\nfinetuning, and prompting strategies including a novel ensemble refinement\napproach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM\nby over 19% and setting a new state-of-the-art. We also observed performance\napproaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU\nclinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple\naxes relevant to clinical applications. In pairwise comparative ranking of 1066\nconsumer medical questions, physicians preferred Med-PaLM 2 answers to those\nproduced by physicians on eight of nine axes pertaining to clinical utility (p\n< 0.001). We also observed significant improvements compared to Med-PaLM on\nevery evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form\n\"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models\nin real-world settings, these results highlight rapid progress towards\nphysician-level performance in medical question answering.",
        "pdf_link": "https://arxiv.org/pdf/2305.09617v1.pdf"
    },
    {
        "title": "Large Language Models are Built-in Autoregressive Search Engines",
        "authors": [
            "Noah Ziems",
            "Wenhao Yu",
            "Zhihan Zhang",
            "Meng Jiang"
        ],
        "published": "2023-05-16T17:04:48Z",
        "summary": "Document retrieval is a key stage of standard Web search engines. Existing\ndual-encoder dense retrievers obtain representations for questions and\ndocuments independently, allowing for only shallow interactions between them.\nTo overcome this limitation, recent autoregressive search engines replace the\ndual-encoder architecture by directly generating identifiers for relevant\ndocuments in the candidate pool. However, the training cost of such\nautoregressive search engines rises sharply as the number of candidate\ndocuments increases. In this paper, we find that large language models (LLMs)\ncan follow human instructions to directly generate URLs for document retrieval.\n  Surprisingly, when providing a few {Query-URL} pairs as in-context\ndemonstrations, LLMs can generate Web URLs where nearly 90\\% of the\ncorresponding documents contain correct answers to open-domain questions. In\nthis way, LLMs can be thought of as built-in search engines, since they have\nnot been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better\nretrieval performance than existing retrieval approaches by a significant\nmargin on three open-domain question answering benchmarks, under both zero and\nfew-shot settings. The code for this work can be found at\n\\url{https://github.com/Ziems/llm-url}.",
        "pdf_link": "https://arxiv.org/pdf/2305.09612v1.pdf"
    },
    {
        "title": "Life of PII -- A PII Obfuscation Transformer",
        "authors": [
            "Ajinkya Deshmukh",
            "Saumya Banthia",
            "Anantha Sharma"
        ],
        "published": "2023-05-16T15:48:36Z",
        "summary": "Protecting sensitive information is crucial in today's world of Large\nLanguage Models (LLMs) and data-driven services. One common method used to\npreserve privacy is by using data perturbation techniques to reduce\noverreaching utility of (sensitive) Personal Identifiable Information (PII)\ndata while maintaining its statistical and semantic properties. Data\nperturbation methods often result in significant information loss, making them\nimpractical for use. In this paper, we propose 'Life of PII', a novel\nObfuscation Transformer framework for transforming PII into faux-PII while\npreserving the original information, intent, and context as much as possible.\nOur approach includes an API to interface with the given document, a\nconfiguration-based obfuscator, and a model based on the Transformer\narchitecture, which has shown high context preservation and performance in\nnatural language processing tasks and LLMs.\n  Our Transformer-based approach learns mapping between the original PII and\nits transformed faux-PII representation, which we call \"obfuscated\" data. Our\nexperiments demonstrate that our method, called Life of PII, outperforms\ntraditional data perturbation techniques in terms of both utility preservation\nand privacy protection. We show that our approach can effectively reduce\nutility loss while preserving the original information, offering greater\nflexibility in the trade-off between privacy protection and data utility. Our\nwork provides a solution for protecting PII in various real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.09550v2.pdf"
    },
    {
        "title": "CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural Topic Modeling",
        "authors": [
            "Zheng Fang",
            "Yulan He",
            "Rob Procter"
        ],
        "published": "2023-05-16T10:07:33Z",
        "summary": "Most existing topic models rely on bag-of-words (BOW) representation, which\nlimits their ability to capture word order information and leads to challenges\nwith out-of-vocabulary (OOV) words in new documents. Contextualized word\nembeddings, however, show superiority in word sense disambiguation and\neffectively address the OOV issue. In this work, we introduce a novel neural\ntopic model called the Contextlized Word Topic Model (CWTM), which integrates\ncontextualized word embeddings from BERT. The model is capable of learning the\ntopic vector of a document without BOW information. In addition, it can also\nderive the topic vectors for individual words within a document based on their\ncontextualized word embeddings. Experiments across various datasets show that\nCWTM generates more coherent and meaningful topics compared to existing topic\nmodels, while also accommodating unseen words in newly encountered documents.",
        "pdf_link": "https://arxiv.org/pdf/2305.09329v3.pdf"
    },
    {
        "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
        "authors": [
            "Hao Chen",
            "Yiming Zhang",
            "Qi Zhang",
            "Hantao Yang",
            "Xiaomeng Hu",
            "Xuetao Ma",
            "Yifan Yanggong",
            "Junbo Zhao"
        ],
        "published": "2023-05-16T07:52:57Z",
        "summary": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.",
        "pdf_link": "https://arxiv.org/pdf/2305.09246v1.pdf"
    },
    {
        "title": "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models",
        "authors": [
            "Boxi Cao",
            "Qiaoyu Tang",
            "Hongyu Lin",
            "Shanshan Jiang",
            "Bin Dong",
            "Xianpei Han",
            "Jiawei Chen",
            "Tianshu Wang",
            "Le Sun"
        ],
        "published": "2023-05-16T03:50:38Z",
        "summary": "Memory is one of the most essential cognitive functions serving as a\nrepository of world knowledge and episodes of activities. In recent years,\nlarge-scale pre-trained language models have shown remarkable memorizing\nability. On the contrary, vanilla neural networks without pre-training have\nbeen long observed suffering from the catastrophic forgetting problem. To\ninvestigate such a retentive-forgetful contradiction and understand the memory\nmechanism of language models, we conduct thorough experiments by controlling\nthe target knowledge types, the learning strategies and the learning schedules.\nWe find that: 1) Vanilla language models are forgetful; 2) Pre-training leads\nto retentive language models; 3) Knowledge relevance and diversification\nsignificantly influence the memory formation. These conclusions are useful for\nunderstanding the abilities of pre-trained language models and shed light on\ndesigning and evaluating new learning and inference algorithms of language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2305.09144v2.pdf"
    },
    {
        "title": "SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting",
        "authors": [
            "Xiaoying Zhang",
            "Baolin Peng",
            "Kun Li",
            "Jingyan Zhou",
            "Helen Meng"
        ],
        "published": "2023-05-15T23:29:56Z",
        "summary": "Building end-to-end task bots and maintaining their integration with new\nfunctionalities using minimal human efforts is a long-standing challenge in\ndialog research. Recently large language models (LLMs) have demonstrated\nexceptional proficiency in conversational engagement and adherence to\ninstructions across various downstream tasks. In this work, we introduce\nSGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems\neffortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we\ninstruct fixed LLMs to generate appropriate responses on novel tasks,\ncircumventing the need for training data. Specifically, SGP-TOD comprises three\ncomponents: a LLM for engaging with users, a DST Prompter to aid the LLM with\ndialog state tracking, which is then used to retrieve database items, and a\nPolicy Prompter to elicit proper responses adhering to the provided dialog\npolicy. Experimental results on Multiwoz, RADDLE and STAR datasets show that\nour training-free strategy SGP-TOD, without any task-specific data, yields\nstate-of-the-art (SOTA) zero-shot performance, greatly surpasses the few-shot\napproaches. In a domain-extension setting, SGP-TOD aptly adapts to new\nfunctionalities by merely adding supplementary schema rules. We make our code\nand data publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.09067v1.pdf"
    },
    {
        "title": "Small Models are Valuable Plug-ins for Large Language Models",
        "authors": [
            "Canwen Xu",
            "Yichong Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Chenguang Zhu",
            "Julian McAuley"
        ],
        "published": "2023-05-15T17:59:01Z",
        "summary": "Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their\nweights are often publicly unavailable and their immense sizes make the models\ndifficult to be tuned with common hardware. As a result, effectively tuning\nthese models with large-scale supervised data can be challenging. As an\nalternative, In-Context Learning (ICL) can only use a small number of\nsupervised examples due to context length limits. In this paper, we propose\nSuper In-Context Learning (SuperICL) which allows black-box LLMs to work with\nlocally fine-tuned smaller models, resulting in superior performance on\nsupervised tasks. Our experiments demonstrate that SuperICL can improve\nperformance beyond state-of-the-art fine-tuned models while addressing the\ninstability problem of in-context learning. Furthermore, SuperICL can enhance\nthe capabilities of smaller models, such as multilinguality and\ninterpretability.",
        "pdf_link": "https://arxiv.org/pdf/2305.08848v1.pdf"
    },
    {
        "title": "Large Language Models are Zero-Shot Rankers for Recommender Systems",
        "authors": [
            "Yupeng Hou",
            "Junjie Zhang",
            "Zihan Lin",
            "Hongyu Lu",
            "Ruobing Xie",
            "Julian McAuley",
            "Wayne Xin Zhao"
        ],
        "published": "2023-05-15T17:57:39Z",
        "summary": "Recently, large language models (LLMs) (e.g., GPT-4) have demonstrated\nimpressive general-purpose task-solving abilities, including the potential to\napproach recommendation tasks. Along this line of research, this work aims to\ninvestigate the capacity of LLMs that act as the ranking model for recommender\nsystems. We first formalize the recommendation problem as a conditional ranking\ntask, considering sequential interaction histories as conditions and the items\nretrieved by other candidate generation models as candidates. To solve the\nranking task by LLMs, we carefully design the prompting template and conduct\nextensive experiments on two widely-used datasets. We show that LLMs have\npromising zero-shot ranking abilities but (1) struggle to perceive the order of\nhistorical interactions, and (2) can be biased by popularity or item positions\nin the prompts. We demonstrate that these issues can be alleviated using\nspecially designed prompting and bootstrapping strategies. Equipped with these\ninsights, zero-shot LLMs can even challenge conventional recommendation models\nwhen ranking candidates are retrieved by multiple candidate generators. The\ncode and processed datasets are available at\nhttps://github.com/RUCAIBox/LLMRank.",
        "pdf_link": "https://arxiv.org/pdf/2305.08845v2.pdf"
    },
    {
        "title": "RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs",
        "authors": [
            "Afra Feyza Akyürek",
            "Ekin Akyürek",
            "Aman Madaan",
            "Ashwin Kalyan",
            "Peter Clark",
            "Derry Wijaya",
            "Niket Tandon"
        ],
        "published": "2023-05-15T17:57:16Z",
        "summary": "Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.",
        "pdf_link": "https://arxiv.org/pdf/2305.08844v2.pdf"
    },
    {
        "title": "Knowledge Rumination for Pre-trained Language Models",
        "authors": [
            "Yunzhi Yao",
            "Peng Wang",
            "Shengyu Mao",
            "Chuanqi Tan",
            "Fei Huang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "published": "2023-05-15T15:47:09Z",
        "summary": "Previous studies have revealed that vanilla pre-trained language models\n(PLMs) lack the capacity to handle knowledge-intensive NLP tasks alone; thus,\nseveral works have attempted to integrate external knowledge into PLMs.\nHowever, despite the promising outcome, we empirically observe that PLMs may\nhave already encoded rich knowledge in their pre-trained parameters but fail to\nfully utilize them when applying them to knowledge-intensive tasks. In this\npaper, we propose a new paradigm dubbed Knowledge Rumination to help the\npre-trained language model utilize that related latent knowledge without\nretrieving it from the external corpus. By simply adding a prompt like \"As far\nas I know\" to the PLMs, we try to review related latent knowledge and inject\nthem back into the model for knowledge consolidation. We apply the proposed\nknowledge rumination to various language models, including RoBERTa, DeBERTa,\nand GPT-3. Experimental results on six commonsense reasoning tasks and GLUE\nbenchmarks demonstrate the effectiveness of our proposed approach, which proves\nthat the knowledge stored in PLMs can be better exploited to enhance\nperformance. Code is available in\nhttps://github.com/zjunlp/knowledge-rumination.",
        "pdf_link": "https://arxiv.org/pdf/2305.08732v3.pdf"
    },
    {
        "title": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility",
        "authors": [
            "Wentao Ye",
            "Mingfeng Ou",
            "Tianyi Li",
            "Yipeng chen",
            "Xuetao Ma",
            "Yifan Yanggong",
            "Sai Wu",
            "Jie Fu",
            "Gang Chen",
            "Haobo Wang",
            "Junbo Zhao"
        ],
        "published": "2023-05-15T15:44:51Z",
        "summary": "The recent popularity of large language models (LLMs) has brought a\nsignificant impact to boundless fields, particularly through their open-ended\necosystem such as the APIs, open-sourced models, and plugins. However, with\ntheir widespread deployment, there is a general lack of research that\nthoroughly discusses and analyzes the potential risks concealed. In that case,\nwe intend to conduct a preliminary but pioneering study covering the\nrobustness, consistency, and credibility of LLMs systems. With most of the\nrelated literature in the era of LLM uncharted, we propose an automated\nworkflow that copes with an upscaled number of queries/responses. Overall, we\nconduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,\nand OPT. Core to our workflow consists of a data primitive, followed by an\nautomated interpreter that evaluates these LLMs under different adversarial\nmetrical systems. As a result, we draw several, and perhaps unfortunate,\nconclusions that are quite uncommon from this trendy community. Briefly, they\nare: (i)-the minor but inevitable error occurrence in the user-generated query\ninput may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possess\npoor consistency when processing semantically similar query input. In addition,\nas a side finding, we find that ChatGPT is still capable to yield the correct\nanswer even when the input is polluted at an extreme level. While this\nphenomenon demonstrates the powerful memorization of the LLMs, it raises\nserious concerns about using such data for LLM-involved evaluation in academic\ndevelopment. To deal with it, we propose a novel index associated with a\ndataset that roughly decides the feasibility of using such data for\nLLM-involved evaluation. Extensive empirical studies are tagged to support the\naforementioned claims.",
        "pdf_link": "https://arxiv.org/pdf/2305.10235v4.pdf"
    },
    {
        "title": "Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks",
        "authors": [
            "Chengguang Gan",
            "Tatsunori Mori"
        ],
        "published": "2023-05-15T15:19:08Z",
        "summary": "Prompt engineering relevance research has seen a notable surge in recent\nyears, primarily driven by advancements in pre-trained language models and\nlarge language models. However, a critical issue has been identified within\nthis domain: the inadequate of sensitivity and robustness of these models\ntowards Prompt Templates, particularly in lesser-studied languages such as\nJapanese. This paper explores this issue through a comprehensive evaluation of\nseveral representative Large Language Models (LLMs) and a widely-utilized\npre-trained model(PLM). These models are scrutinized using a benchmark dataset\nin Japanese, with the aim to assess and analyze the performance of the current\nmultilingual models in this context. Our experimental results reveal startling\ndiscrepancies. A simple modification in the sentence structure of the Prompt\nTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.\nThis observation underscores the fact that even the highly performance GPT-4\nmodel encounters significant stability issues when dealing with diverse\nJapanese prompt templates, rendering the consistency of the model's output\nresults questionable. In light of these findings, we conclude by proposing\npotential research trajectories to further enhance the development and\nperformance of Large Language Models in their current stage.",
        "pdf_link": "https://arxiv.org/pdf/2305.08714v2.pdf"
    },
    {
        "title": "Natural Language Decomposition and Interpretation of Complex Utterances",
        "authors": [
            "Harsh Jhamtani",
            "Hao Fang",
            "Patrick Xia",
            "Eran Levy",
            "Jacob Andreas",
            "Ben Van Durme"
        ],
        "published": "2023-05-15T14:35:00Z",
        "summary": "Designing natural language interfaces has historically required collecting\nsupervised data to translate user requests into carefully designed intent\nrepresentations. This requires enumerating and labeling a long tail of user\nrequests, which is challenging. At the same time, large language models (LLMs)\nencode knowledge about goals and plans that can help conversational assistants\ninterpret user requests requiring numerous steps to complete. We introduce an\napproach to handle complex-intent-bearing utterances from a user via a process\nof hierarchical natural language decomposition and interpretation. Our approach\nuses a pre-trained language model to decompose a complex utterance into a\nsequence of simpler natural language steps and interprets each step using the\nlanguage-to-program model designed for the interface. To test our approach, we\ncollect and release DeCU -- a new NL-to-program benchmark to evaluate\nDecomposition of Complex Utterances. Experiments show that the proposed\napproach enables the interpretation of complex utterances with almost no\ncomplex training data, while outperforming standard few-shot prompting\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2305.08677v2.pdf"
    },
    {
        "title": "Unsupervised Sentence Representation Learning with Frequency-induced Adversarial Tuning and Incomplete Sentence Filtering",
        "authors": [
            "Bing Wang",
            "Ximing Li",
            "Zhiyao Yang",
            "Yuanyuan Guan",
            "Jiayin Li",
            "Shengsheng Wang"
        ],
        "published": "2023-05-15T13:59:23Z",
        "summary": "Pre-trained Language Model (PLM) is nowadays the mainstay of Unsupervised\nSentence Representation Learning (USRL). However, PLMs are sensitive to the\nfrequency information of words from their pre-training corpora, resulting in\nanisotropic embedding space, where the embeddings of high-frequency words are\nclustered but those of low-frequency words disperse sparsely. This anisotropic\nphenomenon results in two problems of similarity bias and information bias,\nlowering the quality of sentence embeddings. To solve the problems, we\nfine-tune PLMs by leveraging the frequency information of words and propose a\nnovel USRL framework, namely Sentence Representation Learning with\nFrequency-induced Adversarial tuning and Incomplete sentence filtering\n(SLT-FAI). We calculate the word frequencies over the pre-training corpora of\nPLMs and assign words thresholding frequency labels. With them, (1) we\nincorporate a similarity discriminator used to distinguish the embeddings of\nhigh-frequency and low-frequency words, and adversarially tune the PLM with it,\nenabling to achieve uniformly frequency-invariant embedding space; and (2) we\npropose a novel incomplete sentence detection task, where we incorporate an\ninformation discriminator to distinguish the embeddings of original sentences\nand incomplete sentences by randomly masking several low-frequency words,\nenabling to emphasize the more informative low-frequency words. Our SLT-FAI is\na flexible and plug-and-play framework, and it can be integrated with existing\nUSRL techniques. We evaluate SLT-FAI with various backbones on benchmark\ndatasets. Empirical results indicate that SLT-FAI can be superior to the\nexisting USRL baselines. Our code is released in\n\\url{https://github.com/wangbing1416/SLT-FAI}.",
        "pdf_link": "https://arxiv.org/pdf/2305.08655v1.pdf"
    },
    {
        "title": "Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study",
        "authors": [
            "Yaxin Fan",
            "Feng Jiang",
            "Peifeng Li",
            "Haizhou Li"
        ],
        "published": "2023-05-15T07:14:41Z",
        "summary": "Large language models, like ChatGPT, have shown remarkable capability in many\ndownstream tasks, yet their ability to understand discourse structures of\ndialogues remains less explored, where it requires higher level capabilities of\nunderstanding and reasoning. In this paper, we aim to systematically inspect\nChatGPT's performance in two discourse analysis tasks: topic segmentation and\ndiscourse parsing, focusing on its deep semantic understanding of linear and\nhierarchical discourse structures underlying dialogue. To instruct ChatGPT to\ncomplete these tasks, we initially craft a prompt template consisting of the\ntask description, output format, and structured input. Then, we conduct\nexperiments on four popular topic segmentation datasets and two discourse\nparsing datasets. The experimental results showcase that ChatGPT demonstrates\nproficiency in identifying topic structures in general-domain conversations yet\nstruggles considerably in specific-domain conversations. We also found that\nChatGPT hardly understands rhetorical structures that are more complex than\ntopic structures. Our deeper investigation indicates that ChatGPT can give more\nreasonable topic structures than human annotations but only linearly parses the\nhierarchical rhetorical structures. In addition, we delve into the impact of\nin-context learning (e.g., chain-of-thought) on ChatGPT and conduct the\nablation study on various prompt components, which can provide a research\nfoundation for future work. The code is available at\n\\url{https://github.com/yxfanSuda/GPTforDDA}.",
        "pdf_link": "https://arxiv.org/pdf/2305.08391v2.pdf"
    },
    {
        "title": "Text Classification via Large Language Models",
        "authors": [
            "Xiaofei Sun",
            "Xiaoya Li",
            "Jiwei Li",
            "Fei Wu",
            "Shangwei Guo",
            "Tianwei Zhang",
            "Guoyin Wang"
        ],
        "published": "2023-05-15T06:24:45Z",
        "summary": "Despite the remarkable success of large-scale Language Models (LLMs) such as\nGPT-3, their performances still significantly underperform fine-tuned models in\nthe task of text classification. This is due to (1) the lack of reasoning\nability in addressing complex linguistic phenomena (e.g., intensification,\ncontrast, irony etc); (2) limited number of tokens allowed in in-context\nlearning.\n  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts\na progressive reasoning strategy tailored to addressing the complex linguistic\nphenomena involved in text classification: CARP first prompts LLMs to find\nsuperficial clues (e.g., keywords, tones, semantic relations, references, etc),\nbased on which a diagnostic reasoning process is induced for final decisions.\nTo further address the limited-token issue, CARP uses a fine-tuned model on the\nsupervised dataset for $k$NN demonstration search in the in-context learning,\nallowing the model to take the advantage of both LLM's generalization ability\nand the task-specific evidence provided by the full labeled dataset.\nRemarkably, CARP yields new SOTA performances on 4 out of 5 widely-used\ntext-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on\nAGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance\ncomparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP\ndelivers impressive abilities on low-resource and domain-adaptation setups.\nSpecifically, using 16 examples per class, CARP achieves comparable\nperformances to supervised models with 1,024 examples per class.",
        "pdf_link": "https://arxiv.org/pdf/2305.08377v3.pdf"
    },
    {
        "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
        "authors": [
            "Yuzhen Huang",
            "Yuzhuo Bai",
            "Zhihao Zhu",
            "Junlei Zhang",
            "Jinghan Zhang",
            "Tangjun Su",
            "Junteng Liu",
            "Chuancheng Lv",
            "Yikai Zhang",
            "Jiayi Lei",
            "Yao Fu",
            "Maosong Sun",
            "Junxian He"
        ],
        "published": "2023-05-15T03:20:19Z",
        "summary": "New NLP benchmarks are urgently needed to align with the rapid development of\nlarge language models (LLMs). We present C-Eval, the first comprehensive\nChinese evaluation suite designed to assess advanced knowledge and reasoning\nabilities of foundation models in a Chinese context. C-Eval comprises\nmultiple-choice questions across four difficulty levels: middle school, high\nschool, college, and professional. The questions span 52 diverse disciplines,\nranging from humanities to science and engineering. C-Eval is accompanied by\nC-Eval Hard, a subset of very challenging subjects in C-Eval that requires\nadvanced reasoning abilities to solve. We conduct a comprehensive evaluation of\nthe most advanced LLMs on C-Eval, including both English- and Chinese-oriented\nmodels. Results indicate that only GPT-4 could achieve an average accuracy of\nover 60%, suggesting that there is still significant room for improvement for\ncurrent LLMs. We anticipate C-Eval will help analyze important strengths and\nshortcomings of foundation models, and foster their development and growth for\nChinese users.",
        "pdf_link": "https://arxiv.org/pdf/2305.08322v3.pdf"
    },
    {
        "title": "Semantic Composition in Visually Grounded Language Models",
        "authors": [
            "Rohan Pandey"
        ],
        "published": "2023-05-15T03:19:42Z",
        "summary": "What is sentence meaning and its ideal representation? Much of the expressive\npower of human language derives from semantic composition, the mind's ability\nto represent meaning hierarchically & relationally over constituents. At the\nsame time, much sentential meaning is outside the text and requires grounding\nin sensory, motor, and experiential modalities to be adequately learned.\nAlthough large language models display considerable compositional ability,\nrecent work shows that visually-grounded language models drastically fail to\nrepresent compositional structure. In this thesis, we explore whether & how\nmodels compose visually grounded semantics, and how we might improve their\nability to do so.\n  Specifically, we introduce 1) WinogroundVQA, a new compositional visual\nquestion answering benchmark, 2) Syntactic Neural Module Distillation, a\nmeasure of compositional ability in sentence embedding models, 3) Causal\nTracing for Image Captioning Models to locate neural representations vital for\nvision-language composition, 4) Syntactic MeanPool to inject a compositional\ninductive bias into sentence embeddings, and 5) Cross-modal Attention\nCongruence Regularization, a self-supervised objective function for\nvision-language relation alignment. We close by discussing connections of our\nwork to neuroscience, psycholinguistics, formal semantics, and philosophy.",
        "pdf_link": "https://arxiv.org/pdf/2305.16328v1.pdf"
    },
    {
        "title": "Large Language Model Guided Tree-of-Thought",
        "authors": [
            "Jieyi Long"
        ],
        "published": "2023-05-15T01:18:23Z",
        "summary": "In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\napproach aimed at improving the problem-solving capabilities of auto-regressive\nlarge language models (LLMs). The ToT technique is inspired by the human mind's\napproach for solving complex reasoning tasks through trial and error. In this\nprocess, the human mind explores the solution space through a tree-like thought\nprocess, allowing for backtracking when necessary. To implement ToT as a\nsoftware system, we augment an LLM with additional modules including a prompter\nagent, a checker module, a memory module, and a ToT controller. In order to\nsolve a given problem, these modules engage in a multi-round conversation with\nthe LLM. The memory module records the conversation and state history of the\nproblem solving process, which allows the system to backtrack to the previous\nsteps of the thought-process and explore other directions from there. To verify\nthe effectiveness of the proposed technique, we implemented a ToT-based solver\nfor the Sudoku Puzzle. Experimental results show that the ToT framework can\nsignificantly increase the success rate of Sudoku puzzle solving. Our\nimplementation of the ToT-based Sudoku solver is available on GitHub:\n\\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.",
        "pdf_link": "https://arxiv.org/pdf/2305.08291v1.pdf"
    },
    {
        "title": "$SmartProbe$: A Virtual Moderator for Market Research Surveys",
        "authors": [
            "Josh Seltzer",
            "Jiahua Pan",
            "Kathy Cheng",
            "Yuxiao Sun",
            "Santosh Kolagati",
            "Jimmy Lin",
            "Shi Zong"
        ],
        "published": "2023-05-14T22:36:08Z",
        "summary": "Market research surveys are a powerful methodology for understanding consumer\nperspectives at scale, but are limited by depth of understanding and insights.\nA virtual moderator can introduce elements of qualitative research into\nsurveys, developing a rapport with survey participants and dynamically asking\nprobing questions, ultimately to elicit more useful information for market\nresearchers. In this work, we introduce ${\\tt SmartProbe}$, an API which\nleverages the adaptive capabilities of large language models (LLMs), and\nincorporates domain knowledge from market research, in order to generate\neffective probing questions in any market research survey. We outline the\nmodular processing flow of $\\tt SmartProbe$, and evaluate the quality and\neffectiveness of its generated probing questions. We believe our efforts will\ninspire industry practitioners to build real-world applications based on the\nlatest advances in LLMs. Our demo is publicly available at\nhttps://nexxt.in/smartprobe-demo",
        "pdf_link": "https://arxiv.org/pdf/2305.08271v1.pdf"
    },
    {
        "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
        "authors": [
            "Mandar Sharma",
            "Nikhil Muralidhar",
            "Naren Ramakrishnan"
        ],
        "published": "2023-05-14T20:57:11Z",
        "summary": "The field of Math-NLP has witnessed significant growth in recent years,\nmotivated by the desire to expand LLM performance to the learning of\nnon-linguistic notions (numerals, and subsequently, arithmetic reasoning).\nHowever, non-linguistic skill injection typically comes at a cost for LLMs: it\nleads to catastrophic forgetting of core linguistic skills, a consequence that\noften remains unaddressed in the literature. As Math-NLP has been able to\ncreate LLMs that can closely approximate the mathematical skills of a\ngrade-schooler or the arithmetic reasoning skills of a calculator, the\npracticality of these models fail if they concomitantly shed their linguistic\ncapabilities. In this work, we take a closer look into the phenomena of\ncatastrophic forgetting as it pertains to LLMs and subsequently offer a novel\nframework for non-linguistic skill injection for LLMs based on information\ntheoretic interventions and skill-specific losses that enable the learning of\nstrict arithmetic reasoning. Our model outperforms the state-of-the-art both on\ninjected non-linguistic skills and on linguistic knowledge retention, and does\nso with a fraction of the non-linguistic training data (1/4) and zero\nadditional synthetic linguistic training data.",
        "pdf_link": "https://arxiv.org/pdf/2305.08246v1.pdf"
    },
    {
        "title": "Mobile-Env: An Evaluation Platform and Benchmark for LLM-GUI Interaction",
        "authors": [
            "Danyang Zhang",
            "Hongshen Xu",
            "Zihan Zhao",
            "Lu Chen",
            "Ruisheng Cao",
            "Kai Yu"
        ],
        "published": "2023-05-14T12:31:03Z",
        "summary": "The User Interface (UI) is pivotal for human interaction with the digital\nworld, facilitating efficient control of machines, information navigation, and\ncomplex task completion. To achieve easy, efficient, and free interactions,\nresearchers have been exploring the potential of encapsulating the traditional\nProgramming Language Interfaces (PLIs) and Graphical User Interfaces (GUIs)\ninto Natural Language Interfaces (NLIs). However, due to the limited\ncapabilities of small models, traditional work mainly focuses on tasks for\nwhich only a single step is needed. This largely constrains the application of\nNLIs. Recently, Large Language Models (LLMs) have exhibited robust reasoning\nand planning abilities, yet their potential for multi-turn interactions in\ncomplex environments remains under-explored. To assess LLMs as NLIs in\nreal-world graphical environments, we introduce the GUI interaction platform,\nMobile-Env, specifically on mobile apps. Mobile-Env enhances interaction\nflexibility, task extensibility, and environment adaptability compared with\nprevious environments. A GUI task set based on WikiHow app is collected on\nMobile-Env to form a benchmark covering a range of GUI interaction\ncapabilities. We further conduct comprehensive evaluations of LLM agents,\nincluding various versions of GPT, LLaMA 2, and AgentLM, on WikiHow task set to\nacquire insights into the potentials and challenges of LLMs in GUI\ninteractions.",
        "pdf_link": "https://arxiv.org/pdf/2305.08144v3.pdf"
    },
    {
        "title": "Watermarking Text Generated by Black-Box Language Models",
        "authors": [
            "Xi Yang",
            "Kejiang Chen",
            "Weiming Zhang",
            "Chang Liu",
            "Yuang Qi",
            "Jie Zhang",
            "Han Fang",
            "Nenghai Yu"
        ],
        "published": "2023-05-14T07:37:33Z",
        "summary": "LLMs now exhibit human-like skills in various fields, leading to worries\nabout misuse. Thus, detecting generated text is crucial. However, passive\ndetection methods are stuck in domain specificity and limited adversarial\nrobustness. To achieve reliable detection, a watermark-based method was\nproposed for white-box LLMs, allowing them to embed watermarks during text\ngeneration. The method involves randomly dividing the model vocabulary to\nobtain a special list and adjusting the probability distribution to promote the\nselection of words in the list. A detection algorithm aware of the list can\nidentify the watermarked text. However, this method is not applicable in many\nreal-world scenarios where only black-box language models are available. For\ninstance, third-parties that develop API-based vertical applications cannot\nwatermark text themselves because API providers only supply generated text and\nwithhold probability distributions to shield their commercial interests. To\nallow third-parties to autonomously inject watermarks into generated text, we\ndevelop a watermarking framework for black-box language model usage scenarios.\nSpecifically, we first define a binary encoding function to compute a random\nbinary encoding corresponding to a word. The encodings computed for\nnon-watermarked text conform to a Bernoulli distribution, wherein the\nprobability of a word representing bit-1 being approximately 0.5. To inject a\nwatermark, we alter the distribution by selectively replacing words\nrepresenting bit-0 with context-based synonyms that represent bit-1. A\nstatistical test is then used to identify the watermark. Experiments\ndemonstrate the effectiveness of our method on both Chinese and English\ndatasets. Furthermore, results under re-translation, polishing, word deletion,\nand synonym substitution attacks reveal that it is arduous to remove the\nwatermark without compromising the original semantics.",
        "pdf_link": "https://arxiv.org/pdf/2305.08883v1.pdf"
    },
    {
        "title": "ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance",
        "authors": [
            "Kaushik Roy",
            "Manas Gaur",
            "Misagh Soltani",
            "Vipula Rawte",
            "Ashwin Kalyan",
            "Amit Sheth"
        ],
        "published": "2023-05-13T21:31:02Z",
        "summary": "Current Virtual Mental Health Assistants (VMHAs) provide counseling and\nsuggestive care. They refrain from patient diagnostic assistance because they\nlack training in safety-constrained and specialized clinical process knowledge.\nIn this work, we define Proknow as an ordered set of information that maps to\nevidence-based guidelines or categories of conceptual understanding to experts\nin a domain. We also introduce a new dataset of diagnostic conversations guided\nby safety constraints and Proknow that healthcare professionals use. We develop\na method for natural language question generation (NLG) that collects\ndiagnostic information from the patient interactively. We demonstrate the\nlimitations of using state-of-the-art large-scale language models (LMs) on this\ndataset. Our algorithm models the process knowledge through explicitly modeling\nsafety, knowledge capture, and explainability. LMs augmented with ProKnow\nguided method generated 89% safer questions in the depression and anxiety\ndomain. The Explainability of the generated question is assessed by computing\nsimilarity with concepts in depression and anxiety knowledge bases. Overall,\nirrespective of the type of LMs augmented with our ProKnow, we achieved an\naverage 82% improvement over simple pre-trained LMs on safety, explainability,\nand process-guided question generation. We qualitatively and quantitatively\nevaluate the efficacy of the proposed ProKnow-guided methods by introducing\nthree new evaluation metrics for safety, explainability, and process knowledge\nadherence.",
        "pdf_link": "https://arxiv.org/pdf/2305.08010v2.pdf"
    },
    {
        "title": "Beyond the Safeguards: Exploring the Security Risks of ChatGPT",
        "authors": [
            "Erik Derner",
            "Kristina Batistič"
        ],
        "published": "2023-05-13T21:01:14Z",
        "summary": "The increasing popularity of large language models (LLMs) such as ChatGPT has\nled to growing concerns about their safety, security risks, and ethical\nimplications. This paper aims to provide an overview of the different types of\nsecurity risks associated with ChatGPT, including malicious text and code\ngeneration, private data disclosure, fraudulent services, information\ngathering, and producing unethical content. We present an empirical study\nexamining the effectiveness of ChatGPT's content filters and explore potential\nways to bypass these safeguards, demonstrating the ethical implications and\nsecurity risks that persist in LLMs even when protections are in place. Based\non a qualitative analysis of the security implications, we discuss potential\nstrategies to mitigate these risks and inform researchers, policymakers, and\nindustry professionals about the complex security challenges posed by LLMs like\nChatGPT. This study contributes to the ongoing discussion on the ethical and\nsecurity implications of LLMs, underscoring the need for continued research in\nthis area.",
        "pdf_link": "https://arxiv.org/pdf/2305.08005v1.pdf"
    },
    {
        "title": "Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics",
        "authors": [
            "Steve Phelps",
            "Yvan I. Russell"
        ],
        "published": "2023-05-13T17:23:16Z",
        "summary": "In this study, we investigate the capacity of large language models (LLMs),\nspecifically GPT-3.5, to operationalise natural language descriptions of\ncooperative, competitive, altruistic, and self-interested behavior in social\ndilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of\na non-zero-sum interaction, but our broader research program encompasses a\nrange of experimental economics scenarios, including the ultimatum game,\ndictator game, and public goods game. Using a within-subject experimental\ndesign, we instantiated LLM-generated agents with various prompts that conveyed\ndifferent cooperative and competitive stances. We then assessed the agents'\nlevel of cooperation in the iterated Prisoner's Dilemma, taking into account\ntheir responsiveness to the cooperative or defection actions of their partners.\nOur results provide evidence that LLMs can translate natural language\ndescriptions of altruism and selfishness into appropriate behaviour to some\nextent, but exhibit limitations in adapting their behavior based on conditioned\nreciprocity. The observed pattern of increased cooperation with defectors and\ndecreased cooperation with cooperators highlights potential constraints in the\nLLM's ability to generalize its knowledge about human behavior in social\ndilemmas. We call upon the research community to further explore the factors\ncontributing to the emergent behavior of LLM-generated agents in a wider array\nof social dilemmas, examining the impact of model architecture, training\nparameters, and various partner strategies on agent behavior. As more advanced\nLLMs like GPT-4 become available, it is crucial to investigate whether they\nexhibit similar limitations or are capable of more nuanced cooperative\nbehaviors, ultimately fostering the development of AI systems that better align\nwith human values and social norms.",
        "pdf_link": "https://arxiv.org/pdf/2305.07970v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models in Conversational Recommender Systems",
        "authors": [
            "Luke Friedman",
            "Sameer Ahuja",
            "David Allen",
            "Zhenning Tan",
            "Hakim Sidahmed",
            "Changbo Long",
            "Jun Xie",
            "Gabriel Schubiner",
            "Ajay Patel",
            "Harsh Lara",
            "Brian Chu",
            "Zexi Chen",
            "Manoj Tiwari"
        ],
        "published": "2023-05-13T16:40:07Z",
        "summary": "A Conversational Recommender System (CRS) offers increased transparency and\ncontrol to users by enabling them to engage with the system through a real-time\nmulti-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an\nunprecedented ability to converse naturally and incorporate world knowledge and\ncommon-sense reasoning into language understanding, unlocking the potential of\nthis paradigm. However, effectively leveraging LLMs within a CRS introduces new\ntechnical challenges, including properly understanding and controlling a\ncomplex conversation and retrieving from external sources of information. These\nissues are exacerbated by a large, evolving item corpus and a lack of\nconversational data for training. In this paper, we provide a roadmap for\nbuilding an end-to-end large-scale CRS using LLMs. In particular, we propose\nnew implementations for user preference understanding, flexible dialogue\nmanagement and explainable recommendations as part of an integrated\narchitecture powered by LLMs. For improved personalization, we describe how an\nLLM can consume interpretable natural language user profiles and use them to\nmodulate session-level context. To overcome conversational data limitations in\nthe absence of an existing production CRS, we propose techniques for building a\ncontrollable LLM-based user simulator to generate synthetic conversations. As a\nproof of concept we introduce RecLLM, a large-scale CRS for YouTube videos\nbuilt on LaMDA, and demonstrate its fluency and diverse functionality through\nsome illustrative example conversations.",
        "pdf_link": "https://arxiv.org/pdf/2305.07961v2.pdf"
    },
    {
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
        "authors": [
            "Yue Wang",
            "Hung Le",
            "Akhilesh Deepak Gotmare",
            "Nghi D. Q. Bui",
            "Junnan Li",
            "Steven C. H. Hoi"
        ],
        "published": "2023-05-13T14:23:07Z",
        "summary": "Large language models (LLMs) pretrained on vast source code have achieved\nprominent progress in code intelligence. However, existing code LLMs have two\nmain limitations in terms of architecture and pretraining tasks. First, they\noften adopt a specific architecture (encoder-only or decoder-only) or rely on a\nunified encoder-decoder network for different downstream tasks. The former\nparadigm is limited by inflexibility in applications while in the latter, the\nmodel is treated as a single system for all tasks, leading to suboptimal\nperformance on a subset of tasks. Secondly, they often employ a limited set of\npretraining objectives which might not be relevant to some downstream tasks and\nhence result in substantial performance degrade. To address these limitations,\nwe propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which\ncomponent modules can be flexibly combined to suit a wide range of downstream\ncode tasks. Such flexibility is enabled by our proposed mixture of pretraining\nobjectives to mitigate the pretrain-finetune discrepancy. These objectives\ncover span denoising, contrastive learning, text-code matching, and causal LM\npretraining tasks, on both unimodal and bimodal multilingual code corpora.\nFurthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs\nwithout training from scratch to efficiently scale up our models, and explore\ninstruction-tuning to align with natural language instructions. We extensively\nevaluate CodeT5+ on over 20 code-related benchmarks in different settings,\nincluding zero-shot, finetuning, and instruction-tuning. We observe\nstate-of-the-art (SoTA) model performance on various code-related tasks, such\nas code generation and completion, math programming, and text-to-code retrieval\ntasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA\nresults on HumanEval code generation task against other open code LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.07922v2.pdf"
    },
    {
        "title": "Dual Use Concerns of Generative AI and Large Language Models",
        "authors": [
            "Alexei Grinbaum",
            "Laurynas Adomaitis"
        ],
        "published": "2023-05-13T10:08:57Z",
        "summary": "We suggest the implementation of the Dual Use Research of Concern (DURC)\nframework, originally designed for life sciences, to the domain of generative\nAI, with a specific focus on Large Language Models (LLMs). With its\ndemonstrated advantages and drawbacks in biological research, we believe the\nDURC criteria can be effectively redefined for LLMs, potentially contributing\nto improved AI governance. Acknowledging the balance that must be struck when\nemploying the DURC framework, we highlight its crucial political role in\nenhancing societal awareness of the impact of generative AI. As a final point,\nwe offer a series of specific recommendations for applying the DURC approach to\nLLM research.",
        "pdf_link": "https://arxiv.org/pdf/2305.07882v2.pdf"
    },
    {
        "title": "Improving Small Language Models on PubMedQA via Generative Data Augmentation",
        "authors": [
            "Zhen Guo",
            "Peiqi Wang",
            "Yanwei Wang",
            "Shangdi Yu"
        ],
        "published": "2023-05-12T23:49:23Z",
        "summary": "Large Language Models (LLMs) have made remarkable advancements in the field\nof natural language processing. However, their increasing size poses challenges\nin terms of computational cost. On the other hand, Small Language Models (SLMs)\nare known for their efficiency, but they often struggle with limited capacity\nand training data, especially in specific domains. In this paper, we introduce\na novel method aimed at improving SLMs in the medical domain using LLM-based\ngenerative data augmentation. The objective of our approach is to develop more\nefficient and capable models that are specifically tailored for specialized\napplications. Through experiments conducted on the PubMedQA dataset, we\ndemonstrate the effectiveness of LLMs in refining and diversifying existing\nquestion-answer pairs. This refinement process leads to improved performance in\na significantly smaller model after fine-tuning. Notably, our best SLM, with\nunder 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA\ndataset. Our code and generated data are publicly available to facilitate\nfurther explorations.",
        "pdf_link": "https://arxiv.org/pdf/2305.07804v4.pdf"
    },
    {
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models",
        "authors": [
            "Yongchao Chen",
            "Rujul Gandhi",
            "Yang Zhang",
            "Chuchu Fan"
        ],
        "published": "2023-05-12T21:22:08Z",
        "summary": "Temporal Logic (TL) can be used to rigorously specify complex high-level\nspecification for systems in many engineering applications. The translation\nbetween natural language (NL) and TL has been under-explored due to the lack of\ndataset and generalizable model across different application domains. In this\npaper, we propose an accurate and generalizable transformation framework of\nEnglish instructions from NL to TL, exploring the use of Large Language Models\n(LLMs) at multiple stages. Our contributions are twofold. First, we develop a\nframework to create a dataset of NL-TL pairs combining LLMs and human\nannotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5\nmodels on the lifted versions (i.e., the specific Atomic Propositions (AP) are\nhidden) of the NL and TL. The enhanced generalizability originates from two\naspects: 1) Usage of lifted NL-TL characterizes common logical structures,\nwithout constraints of specific domains. 2) Application of LLMs in dataset\ncreation largely enhances corpus richness. We test the generalization of\ntrained models on five varied domains. To achieve full NL-TL transformation, we\neither combine the lifted model with AP recognition task or do the further\nfinetuning on each specific domain. During the further finetuning, our model\nachieves higher accuracy (>95%) using only <10% training data, compared with\nthe baseline sequence to sequence (Seq2Seq) model.",
        "pdf_link": "https://arxiv.org/pdf/2305.07766v2.pdf"
    },
    {
        "title": "Knowledge Authoring for Rules and Actions",
        "authors": [
            "Yuheng Wang",
            "Paul Fodor",
            "Michael Kifer"
        ],
        "published": "2023-05-12T21:08:35Z",
        "summary": "Knowledge representation and reasoning (KRR) systems describe and reason with\ncomplex concepts and relations in the form of facts and rules. Unfortunately,\nwide deployment of KRR systems runs into the problem that domain experts have\ngreat difficulty constructing correct logical representations of their domain\nknowledge. Knowledge engineers can help with this construction process, but\nthere is a deficit of such specialists. The earlier Knowledge Authoring Logic\nMachine (KALM) based on Controlled Natural Language (CNL) was shown to have\nvery high accuracy for authoring facts and questions. More recently, KALMFL, a\nsuccessor of KALM, replaced CNL with factual English, which is much less\nrestrictive and requires very little training from users. However, KALMFL has\nlimitations in representing certain types of knowledge, such as authoring rules\nfor multi-step reasoning or understanding actions with timestamps. To address\nthese limitations, we propose KALMRA to enable authoring of rules and actions.\nOur evaluation using the UTI guidelines benchmark shows that KALMRA achieves a\nhigh level of correctness (100%) on rule authoring. When used for authoring and\nreasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI\nbenchmark, demonstrating its effectiveness in more sophisticated KRR jobs.\nFinally, we illustrate the logical reasoning capabilities of KALMRA by drawing\nattention to the problems faced by the recently made famous AI, ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.07763v1.pdf"
    },
    {
        "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
        "authors": [
            "Ronen Eldan",
            "Yuanzhi Li"
        ],
        "published": "2023-05-12T20:56:48Z",
        "summary": "Language models (LMs) are powerful tools for natural language processing, but\nthey often struggle to produce coherent and fluent text when they are small.\nModels with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can\nrarely generate coherent and consistent English text beyond a few words even\nafter extensive training. This raises the question of whether the emergence of\nthe ability to produce coherent English text only occurs at larger scales (with\nhundreds of millions of parameters or more) and complex architectures (with\nmany layers of global attention).\n  In this work, we introduce TinyStories, a synthetic dataset of short stories\nthat only contain words that a typical 3 to 4-year-olds usually understand,\ngenerated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train\nand evaluate LMs that are much smaller than the state-of-the-art models (below\n10 million total parameters), or have much simpler architectures (with only one\ntransformer block), yet still produce fluent and consistent stories with\nseveral paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\n  We also introduce a new paradigm for the evaluation of language models: We\nsuggest a framework which uses GPT-4 to grade the content generated by these\nmodels as if those were stories written by students and graded by a (human)\nteacher. This new paradigm overcomes the flaws of standard benchmarks which\noften requires the model's output to be very structures, and moreover provides\na multidimensional score for the model, providing scores for different\ncapabilities such as grammar, creativity and consistency.\n  We hope that TinyStories can facilitate the development, analysis and\nresearch of LMs, especially for low-resource or specialized domains, and shed\nlight on the emergence of language capabilities in LMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.07759v2.pdf"
    },
    {
        "title": "Text2Cohort: Facilitating Intuitive Access to Biomedical Data with Natural Language Cohort Discovery",
        "authors": [
            "Pranav Kulkarni",
            "Adway Kanhere",
            "Paul H. Yi",
            "Vishwa S. Parekh"
        ],
        "published": "2023-05-12T17:46:06Z",
        "summary": "The Imaging Data Commons (IDC) is a cloud-based database that provides\nresearchers with open access to cancer imaging data, with the goal of\nfacilitating collaboration. However, cohort discovery within the IDC database\nhas a significant technical learning curve. Recently, large language models\n(LLM) have demonstrated exceptional utility for natural language processing\ntasks. We developed Text2Cohort, a LLM-powered toolkit to facilitate\nuser-friendly natural language cohort discovery in the IDC. Our method\ntranslates user input into IDC queries using grounding techniques and returns\nthe query's response. We evaluate Text2Cohort on 50 natural language inputs,\nfrom information extraction to cohort discovery. Our toolkit successfully\ngenerated responses with an 88% accuracy and 0.94 F1 score. We demonstrate that\nText2Cohort can enable researchers to discover and curate cohorts on IDC with\nhigh levels of accuracy using natural language in a more intuitive and\nuser-friendly way.",
        "pdf_link": "https://arxiv.org/pdf/2305.07637v3.pdf"
    },
    {
        "title": "Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation",
        "authors": [
            "Jizhi Zhang",
            "Keqin Bao",
            "Yang Zhang",
            "Wenjie Wang",
            "Fuli Feng",
            "Xiangnan He"
        ],
        "published": "2023-05-12T16:54:36Z",
        "summary": "The remarkable achievements of Large Language Models (LLMs) have led to the\nemergence of a novel recommendation paradigm -- Recommendation via LLM\n(RecLLM). Nevertheless, it is important to note that LLMs may contain social\nprejudices, and therefore, the fairness of recommendations made by RecLLM\nrequires further investigation. To avoid the potential risks of RecLLM, it is\nimperative to evaluate the fairness of RecLLM with respect to various sensitive\nattributes on the user side. Due to the differences between the RecLLM paradigm\nand the traditional recommendation paradigm, it is problematic to directly use\nthe fairness benchmark of traditional recommendation. To address the dilemma,\nwe propose a novel benchmark called Fairness of Recommendation via LLM\n(FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset\nthat accounts for eight sensitive attributes1 in two recommendation scenarios:\nmusic and movies. By utilizing our FaiRLLM benchmark, we conducted an\nevaluation of ChatGPT and discovered that it still exhibits unfairness to some\nsensitive attributes when generating recommendations. Our code and dataset can\nbe found at https://github.com/jizhi-zhang/FaiRLLM.",
        "pdf_link": "https://arxiv.org/pdf/2305.07609v3.pdf"
    },
    {
        "title": "Generative AI: Implications and Applications for Education",
        "authors": [
            "Anastasia Olga",
            "Tzirides",
            "Akash Saini",
            "Gabriela Zapata",
            "Duane Searsmith",
            "Bill Cope",
            "Mary Kalantzis",
            "Vania Castro",
            "Theodora Kourkoulou",
            "John Jones",
            "Rodrigo Abrantes da Silva",
            "Jen Whiting",
            "Nikoleta Polyxeni Kastania"
        ],
        "published": "2023-05-12T16:52:38Z",
        "summary": "The launch of ChatGPT in November 2022 precipitated a panic among some\neducators while prompting qualified enthusiasm from others. Under the umbrella\nterm Generative AI, ChatGPT is an example of a range of technologies for the\ndelivery of computer-generated text, image, and other digitized media. This\npaper examines the implications for education of one generative AI technology,\nchatbots responding from large language models, or C-LLM. It reports on an\napplication of a C-LLM to AI review and assessment of complex student work. In\na concluding discussion, the paper explores the intrinsic limits of generative\nAI, bound as it is to language corpora and their textual representation through\nbinary notation. Within these limits, we suggest the range of emerging and\npotential applications of Generative AI in education.",
        "pdf_link": "https://arxiv.org/pdf/2305.07605v3.pdf"
    },
    {
        "title": "LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development",
        "authors": [
            "Ilias Chalkidis",
            "Nicolas Garneau",
            "Catalina Goanta",
            "Daniel Martin Katz",
            "Anders Søgaard"
        ],
        "published": "2023-05-12T14:21:38Z",
        "summary": "In this work, we conduct a detailed analysis on the performance of\nlegal-oriented pre-trained language models (PLMs). We examine the interplay\nbetween their original objective, acquired knowledge, and legal language\nunderstanding capacities which we define as the upstream, probing, and\ndownstream performance, respectively. We consider not only the models' size but\nalso the pre-training corpora used as important dimensions in our study. To\nthis end, we release a multinational English legal corpus (LeXFiles) and a\nlegal knowledge probing benchmark (LegalLAMA) to facilitate training and\ndetailed analysis of legal-oriented PLMs. We release two new legal PLMs trained\non LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We\nfind that probing performance strongly correlates with upstream performance in\nrelated legal topics. On the other hand, downstream performance is mainly\ndriven by the model's size and prior legal knowledge which can be estimated by\nupstream and probing performance. Based on these findings, we can conclude that\nboth dimensions are important for those seeking the development of\ndomain-specific PLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.07507v2.pdf"
    },
    {
        "title": "Calibration-Aware Bayesian Learning",
        "authors": [
            "Jiayi Huang",
            "Sangwoo Park",
            "Osvaldo Simeone"
        ],
        "published": "2023-05-12T14:19:15Z",
        "summary": "Deep learning models, including modern systems like large language models,\nare well known to offer unreliable estimates of the uncertainty of their\ndecisions. In order to improve the quality of the confidence levels, also known\nas calibration, of a model, common approaches entail the addition of either\ndata-dependent or data-independent regularization terms to the training loss.\nData-dependent regularizers have been recently introduced in the context of\nconventional frequentist learning to penalize deviations between confidence and\naccuracy. In contrast, data-independent regularizers are at the core of\nBayesian learning, enforcing adherence of the variational distribution in the\nmodel parameter space to a prior density. The former approach is unable to\nquantify epistemic uncertainty, while the latter is severely affected by model\nmisspecification. In light of the limitations of both methods, this paper\nproposes an integrated framework, referred to as calibration-aware Bayesian\nneural networks (CA-BNNs), that applies both regularizers while optimizing over\na variational distribution as in Bayesian learning. Numerical results validate\nthe advantages of the proposed approach in terms of expected calibration error\n(ECE) and reliability diagrams.",
        "pdf_link": "https://arxiv.org/pdf/2305.07504v1.pdf"
    },
    {
        "title": "ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models with Enhanced Adapter",
        "authors": [
            "Zhengqing Yuan",
            "Yunhong He",
            "Kun Wang",
            "Yanfang Ye",
            "Lichao Sun"
        ],
        "published": "2023-05-12T14:04:30Z",
        "summary": "The success of large language models (LLMs) has inspired an emerging research\nfield of multimodal learning. However, a grand challenge of exploiting LLMs for\nmultimodal learning is the size of pre-trained LLMs which are always with\nbillions of parameters. To tackle this challenge, models such as MiniGPT-4 and\nLLaVA have been developed to fine-tune the pre-trained models using fewer\nparameters. Despite their promising performance, these models remain limited in\ntheir understanding of artistic imagery. To facilitate better\nartistic-understanding, in this paper, we propose ArtGPT-4, a pioneering large\nvision-language model tailored to address the limitations of existing models in\nartistic comprehension. The key innovation of ArtGPT-4 lies in its craft for\nthe sophisticated challenge of artistic image comprehension, setting it apart\nfrom other models that overlook fine details for broader themes. Specifically,\nit works by integrating some specialized adapter layers into the LLM, enabling\nthe model to more efficiently and effectively parse and interpret complex\nvisual tokens, instead of fine-tuning the whole LLM as in the existing method.\nArtGPT-4 has demonstrated its outstanding performance on the efficiency:\nutilizing a Tesla A100 device, its training can be completed in mere 2 hours\nwith an image-text pair dataset comprising approximately 0.52M entries.\nAdditionally, ArtGPT-4 has also achieved state-of-the-art performance on the\nArtEmis and ArtEmis-v2.0 datasets as well as the benchmarks established in this\nwork, lagging behind professional artists' descriptions by a negligible 0.15\npoints on a 6-point scale. The outstanding performance of ArtGPT-4 shows that\nit can render images with an artistic-understanding and convey the emotions\nthey inspire, mirroring human interpretation. The code and the pre-trained\nmodel are accessible in \\url{https://github.com/DLYuanGod/ArtGPT-4}.",
        "pdf_link": "https://arxiv.org/pdf/2305.07490v6.pdf"
    },
    {
        "title": "Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation",
        "authors": [
            "Tu Anh Dinh",
            "Jan Niehues"
        ],
        "published": "2023-05-12T13:10:57Z",
        "summary": "Quality Estimation (QE) is the task of predicting the quality of Machine\nTranslation (MT) system output, without using any gold-standard translation\nreferences. State-of-the-art QE models are supervised: they require\nhuman-labeled quality of some MT system output on some datasets for training,\nmaking them domain-dependent and MT-system-dependent. There has been research\non unsupervised QE, which requires glass-box access to the MT systems, or\nparallel MT data to generate synthetic errors for training QE models. In this\npaper, we present Perturbation-based QE - a word-level Quality Estimation\napproach that works simply by analyzing MT system output on perturbed input\nsource sentences. Our approach is unsupervised, explainable, and can evaluate\nany type of blackbox MT systems, including the currently prominent large\nlanguage models (LLMs) with opaque internal processes. For language directions\nwith no labeled QE data, our approach has similar or better performance than\nthe zero-shot supervised approach on the WMT21 shared task. Our approach is\nbetter at detecting gender bias and word-sense-disambiguation errors in\ntranslation than supervised QE, indicating its robustness to out-of-domain\nusage. The performance gap is larger when detecting errors on a nontraditional\ntranslation-prompting LLM, indicating that our approach is more generalizable\nto different MT systems. We give examples demonstrating our approach's\nexplainability power, where it shows which input source words have influence on\na certain MT output word.",
        "pdf_link": "https://arxiv.org/pdf/2305.07457v2.pdf"
    },
    {
        "title": "Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation",
        "authors": [
            "Lei Liu",
            "Jimmy Xiangji Huang"
        ],
        "published": "2023-05-12T11:41:16Z",
        "summary": "Dialogue systems for non-English languages have long been under-explored. In\nthis paper, we take the first step to investigate few-shot cross-lingual\ntransfer learning (FS-XLT) and multitask learning (MTL) in the context of\nopen-domain dialogue generation for non-English languages with limited data. We\nobserved catastrophic forgetting in both FS-XLT and MTL for all 6 languages in\nour preliminary experiments. To mitigate the issue, we propose a simple yet\neffective prompt learning approach that can preserve the multilinguality of\nmultilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging\nthe gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and\nour hand-crafted prompts. Experimental results on all 6 languages in terms of\nboth automatic and human evaluations demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.",
        "pdf_link": "https://arxiv.org/pdf/2305.07393v2.pdf"
    },
    {
        "title": "Surfacing Biases in Large Language Models using Contrastive Input Decoding",
        "authors": [
            "Gal Yona",
            "Or Honovich",
            "Itay Laish",
            "Roee Aharoni"
        ],
        "published": "2023-05-12T11:09:49Z",
        "summary": "Ensuring that large language models (LMs) are fair, robust and useful\nrequires an understanding of how different modifications to their inputs impact\nthe model's behaviour. In the context of open-text generation tasks, however,\nsuch an evaluation is not trivial. For example, when introducing a model with\nan input text and a perturbed, \"contrastive\" version of it, meaningful\ndifferences in the next-token predictions may not be revealed with standard\ndecoding strategies. With this motivation in mind, we propose Contrastive Input\nDecoding (CID): a decoding algorithm to generate text given two inputs, where\nthe generated text is likely given one input but unlikely given the other. In\nthis way, the contrastive generations can highlight potentially subtle\ndifferences in how the LM output differs for the two inputs in a simple and\ninterpretable manner. We use CID to highlight context-specific biases that are\nhard to detect with standard decoding strategies and quantify the effect of\ndifferent input perturbations.",
        "pdf_link": "https://arxiv.org/pdf/2305.07378v1.pdf"
    },
    {
        "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation",
        "authors": [
            "Jinglong Gao",
            "Xiao Ding",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-05-12T10:54:13Z",
        "summary": "Causal reasoning ability is crucial for numerous NLP applications. Despite\nthe impressive emerging ability of ChatGPT in various NLP tasks, it is unclear\nhow well ChatGPT performs in causal reasoning. In this paper, we conduct the\nfirst comprehensive evaluation of the ChatGPT's causal reasoning capabilities.\nExperiments show that ChatGPT is not a good causal reasoner, but a good causal\nexplainer. Besides, ChatGPT has a serious hallucination on causal reasoning,\npossibly due to the reporting biases between causal and non-causal\nrelationships in natural language, as well as ChatGPT's upgrading processes,\nsuch as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)\ntechniques can further exacerbate such causal hallucination. Additionally, the\ncausal reasoning ability of ChatGPT is sensitive to the words used to express\nthe causal concept in prompts, and close-ended prompts perform better than\nopen-ended prompts. For events in sentences, ChatGPT excels at capturing\nexplicit causality rather than implicit causality, and performs better in\nsentences with lower event density and smaller lexical distance between events.\nThe code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .",
        "pdf_link": "https://arxiv.org/pdf/2305.07375v4.pdf"
    },
    {
        "title": "When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust",
        "authors": [
            "Minh-Tien Nguyen",
            "Duy-Hung Nguyen",
            "Shahab Sabahi",
            "Hung Le",
            "Jeff Yang",
            "Hajime Hotta"
        ],
        "published": "2023-05-12T03:49:59Z",
        "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing, with GPT models at the forefront. While their remarkable\nperformance spans a range of tasks, adapting LLMs for real-world business\nscenarios still poses challenges warranting further investigation. This paper\npresents an empirical analysis aimed at bridging the gap in adapting LLMs to\npractical use cases. To do that, we select the question answering (QA) task of\ninsurance as a case study due to its challenge of reasoning. Based on the task\nwe design a new model relied on LLMs which are empowered by additional\nknowledge extracted from insurance policy rulebooks and DBpedia. The additional\nknowledge helps LLMs to understand new concepts of insurance for domain\nadaptation. Preliminary results on two QA datasets show that knowledge\nenhancement significantly improves the reasoning ability of GPT-3.5 (55.80% and\n57.83% in terms of accuracy). The analysis also indicates that existing public\nknowledge bases, e.g., DBPedia is beneficial for knowledge enhancement. Our\nfindings reveal that the inherent complexity of business scenarios often\nnecessitates the incorporation of domain-specific knowledge and external\nresources for effective problem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2305.07230v2.pdf"
    },
    {
        "title": "Exploring Zero and Few-shot Techniques for Intent Classification",
        "authors": [
            "Soham Parikh",
            "Quaizar Vohra",
            "Prashil Tumbade",
            "Mitul Tiwari"
        ],
        "published": "2023-05-11T22:07:27Z",
        "summary": "Conversational NLU providers often need to scale to thousands of\nintent-classification models where new customers often face the cold-start\nproblem. Scaling to so many customers puts a constraint on storage space as\nwell. In this paper, we explore four different zero and few-shot intent\nclassification approaches with this low-resource constraint: 1) domain\nadaptation, 2) data augmentation, 3) zero-shot intent classification using\ndescriptions large language models (LLMs), and 4) parameter-efficient\nfine-tuning of instruction-finetuned language models. Our results show that all\nthese approaches are effective to different degrees in low-resource settings.\nParameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on\nFlan-T5 (Chang et al., 2022) yields the best performance even with just one\nsample per intent. We also show that the zero-shot method of prompting LLMs\nusing intent descriptions",
        "pdf_link": "https://arxiv.org/pdf/2305.07157v1.pdf"
    },
    {
        "title": "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
        "authors": [
            "Gokul Yenduri",
            "Ramalingam M",
            "Chemmalar Selvi G",
            "Supriya Y",
            "Gautam Srivastava",
            "Praveen Kumar Reddy Maddikunta",
            "Deepti Raj G",
            "Rutvij H Jhaveri",
            "Prabadevi B",
            "Weizheng Wang",
            "Athanasios V. Vasilakos",
            "Thippa Reddy Gadekallu"
        ],
        "published": "2023-05-11T19:20:38Z",
        "summary": "The Generative Pre-trained Transformer (GPT) represents a notable\nbreakthrough in the domain of natural language processing, which is propelling\nus toward the development of machines that can understand and communicate using\nlanguage in a manner that closely resembles that of humans. GPT is based on the\ntransformer architecture, a deep neural network designed for natural language\nprocessing tasks. Due to their impressive performance on natural language\nprocessing tasks and ability to effectively converse, GPT have gained\nsignificant popularity among researchers and industrial communities, making\nthem one of the most widely used and effective models in natural language\nprocessing and related fields, which motivated to conduct this review. This\nreview provides a detailed overview of the GPT, including its architecture,\nworking process, training procedures, enabling technologies, and its impact on\nvarious applications. In this review, we also explored the potential challenges\nand limitations of a GPT. Furthermore, we discuss potential solutions and\nfuture directions. Overall, this paper aims to provide a comprehensive\nunderstanding of GPT, enabling technologies, their impact on various\napplications, emerging challenges, and potential solutions.",
        "pdf_link": "https://arxiv.org/pdf/2305.10435v2.pdf"
    },
    {
        "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales",
        "authors": [
            "Brihi Joshi",
            "Ziyi Liu",
            "Sahana Ramnath",
            "Aaron Chan",
            "Zhewei Tong",
            "Shaoliang Nie",
            "Qifan Wang",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "published": "2023-05-11T19:01:13Z",
        "summary": "Among the remarkable emergent capabilities of large language models (LMs) is\nfree-text rationalization; beyond a certain scale, large LMs are capable of\ngenerating seemingly useful rationalizations, which in turn, can dramatically\nenhance their performances on leaderboards. This phenomenon raises a question:\ncan machine generated rationales also be useful for humans, especially when lay\nhumans try to answer questions based on those machine rationales? We observe\nthat human utility of existing rationales is far from satisfactory, and\nexpensive to estimate with human studies. Existing metrics like task\nperformance of the LM generating the rationales, or similarity between\ngenerated and gold rationales are not good indicators of their human utility.\nWhile we observe that certain properties of rationales like conciseness and\nnovelty are correlated with their human utility, estimating them without human\ninvolvement is challenging. We show that, by estimating a rationale's\nhelpfulness in answering similar unseen instances, we can measure its human\nutility to a better extent. We also translate this finding into an automated\nscore, GEN-U, that we propose, which can help improve LMs' ability to generate\nrationales with better human utility, while maintaining most of its task\nperformance. Lastly, we release all code and collected data with this project.",
        "pdf_link": "https://arxiv.org/pdf/2305.07095v1.pdf"
    },
    {
        "title": "Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting",
        "authors": [
            "Haoyang Huang",
            "Tianyi Tang",
            "Dongdong Zhang",
            "Wayne Xin Zhao",
            "Ting Song",
            "Yan Xia",
            "Furu Wei"
        ],
        "published": "2023-05-11T17:44:17Z",
        "summary": "Large language models (LLMs) demonstrate impressive multilingual capability,\nbut their performance varies substantially across different languages. In this\nwork, we introduce a simple yet effective method, called cross-lingual-thought\nprompting (XLT), to systematically improve the multilingual capability of LLMs.\nSpecifically, XLT is a generic template prompt that stimulates cross-lingual\nand logical reasoning skills to enhance task performance across languages. We\nconduct comprehensive evaluations on 7 typical benchmarks related to reasoning,\nunderstanding, and generation tasks, covering both high-resource and\nlow-resource languages. Experimental results show that XLT not only remarkably\nenhances the performance of various multilingual tasks but also significantly\nreduces the gap between the average performance and the best performance of\neach task in different languages. Notably, XLT brings over 10 points of average\nimprovement in arithmetic reasoning and open-domain question-answering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.07004v2.pdf"
    },
    {
        "title": "Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach",
        "authors": [
            "Junjie Zhang",
            "Ruobing Xie",
            "Yupeng Hou",
            "Wayne Xin Zhao",
            "Leyu Lin",
            "Ji-Rong Wen"
        ],
        "published": "2023-05-11T17:39:07Z",
        "summary": "In the past decades, recommender systems have attracted much attention in\nboth research and industry communities, and a large number of studies have been\ndevoted to developing effective recommendation models. Basically speaking,\nthese models mainly learn the underlying user preference from historical\nbehavior data, and then estimate the user-item matching relationships for\nrecommendations. Inspired by the recent progress on large language models\n(LLMs), we take a different approach to developing the recommendation models,\nconsidering recommendation as instruction following by LLMs. The key idea is\nthat the preferences or needs of a user can be expressed in natural language\ndescriptions (called instructions), so that LLMs can understand and further\nexecute the instruction for fulfilling the recommendation task. Instead of\nusing public APIs of LLMs, we instruction tune an open-source LLM (3B\nFlan-T5-XL), in order to better adapt LLMs to recommender systems. For this\npurpose, we first design a general instruction format for describing the\npreference, intention, task form and context of a user in natural language.\nThen we manually design 39 instruction templates and automatically generate a\nlarge amount of user-personalized instruction data (252K instructions) with\nvarying types of preferences and intentions. To demonstrate the effectiveness\nof our approach, we instantiate the instruction templates into several\nwidely-studied recommendation (or search) tasks, and conduct extensive\nexperiments on these tasks with real-world datasets. Experiment results show\nthat the proposed approach can outperform several competitive baselines,\nincluding the powerful GPT-3.5, on these evaluation tasks. Our approach sheds\nlight on developing more user-friendly recommender systems, in which users can\nfreely communicate with the system and obtain more accurate recommendations via\nnatural language instructions.",
        "pdf_link": "https://arxiv.org/pdf/2305.07001v1.pdf"
    },
    {
        "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
        "authors": [
            "Ehsan Kamalloo",
            "Nouha Dziri",
            "Charles L. A. Clarke",
            "Davood Rafiei"
        ],
        "published": "2023-05-11T17:14:33Z",
        "summary": "Lexical matching remains the de facto evaluation method for open-domain\nquestion answering (QA). Unfortunately, lexical matching fails completely when\na plausible candidate answer does not appear in the list of gold answers, which\nis increasingly the case as we shift from extractive to generative models. The\nrecent success of large language models (LLMs) for QA aggravates lexical\nmatching failures since candidate answers become longer, thereby making\nmatching with the gold answers even more challenging. Without accurate\nevaluation, the true progress in open-domain QA remains unknown. In this paper,\nwe conduct a thorough analysis of various open-domain QA models, including\nLLMs, by manually evaluating their answers on a subset of NQ-open, a popular\nbenchmark. Our assessments reveal that while the true performance of all models\nis significantly underestimated, the performance of the InstructGPT (zero-shot)\nLLM increases by nearly +60%, making it on par with existing top models, and\nthe InstructGPT (few-shot) model actually achieves a new state-of-the-art on\nNQ-open. We also find that more than 50% of lexical matching failures are\nattributed to semantically equivalent answers. We further demonstrate that\nregex matching ranks QA models consistent with human judgments, although still\nsuffering from unnecessary strictness. Finally, we demonstrate that automated\nevaluation models are a reasonable surrogate for lexical matching in some\ncircumstances, but not for long-form answers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in LLM answers and are thus unable\nto evaluate LLMs. At this time, there appears to be no substitute for human\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.06984v3.pdf"
    },
    {
        "title": "Active Retrieval Augmented Generation",
        "authors": [
            "Zhengbao Jiang",
            "Frank F. Xu",
            "Luyu Gao",
            "Zhiqing Sun",
            "Qian Liu",
            "Jane Dwivedi-Yu",
            "Yiming Yang",
            "Jamie Callan",
            "Graham Neubig"
        ],
        "published": "2023-05-11T17:13:40Z",
        "summary": "Despite the remarkable ability of large language models (LMs) to comprehend\nand generate language, they have a tendency to hallucinate and create factually\ninaccurate output. Augmenting LMs by retrieving information from external\nknowledge resources is one promising solution. Most existing retrieval\naugmented LMs employ a retrieve-and-generate setup that only retrieves\ninformation once based on the input. This is limiting, however, in more general\nscenarios involving generation of long texts, where continually gathering\ninformation throughout generation is essential. In this work, we provide a\ngeneralized view of active retrieval augmented generation, methods that\nactively decide when and what to retrieve across the course of the generation.\nWe propose Forward-Looking Active REtrieval augmented generation (FLARE), a\ngeneric method which iteratively uses a prediction of the upcoming sentence to\nanticipate future content, which is then utilized as a query to retrieve\nrelevant documents to regenerate the sentence if it contains low-confidence\ntokens. We test FLARE along with baselines comprehensively over 4 long-form\nknowledge-intensive generation tasks/datasets. FLARE achieves superior or\ncompetitive performance on all tasks, demonstrating the effectiveness of our\nmethod. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "pdf_link": "https://arxiv.org/pdf/2305.06983v2.pdf"
    },
    {
        "title": "Spear Phishing With Large Language Models",
        "authors": [
            "Julian Hazell"
        ],
        "published": "2023-05-11T16:55:19Z",
        "summary": "Recent progress in artificial intelligence (AI), particularly in the domain\nof large language models (LLMs), has resulted in powerful and versatile\ndual-use systems. This intelligence can be put towards a wide variety of\nbeneficial tasks, yet it can also be used to cause harm. This study explores\none such harm by examining how LLMs can be used for spear phishing, a form of\ncybercrime that involves manipulating targets into divulging sensitive\ninformation. I first explore LLMs' ability to assist with the reconnaissance\nand message generation stages of a spear phishing attack, where I find that\nLLMs are capable of assisting with the email generation phase of a spear\nphishing attack. To explore how LLMs could potentially be harnessed to scale\nspear phishing campaigns, I then create unique spear phishing messages for over\n600 British Members of Parliament using OpenAI's GPT-3.5 and GPT-4 models. My\nfindings provide some evidence that these messages are not only realistic but\nalso cost-effective, with each email costing only a fraction of a cent to\ngenerate. Next, I demonstrate how basic prompt engineering can circumvent\nsafeguards installed in LLMs, highlighting the need for further research into\nrobust interventions that can help prevent models from being misused. To\nfurther address these evolving risks, I explore two potential solutions:\nstructured access schemes, such as application programming interfaces, and\nLLM-based defensive systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.06972v3.pdf"
    },
    {
        "title": "Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models",
        "authors": [
            "Lukáš Mikula",
            "Michal Štefánik",
            "Marek Petrovič",
            "Petr Sojka"
        ],
        "published": "2023-05-11T14:35:00Z",
        "summary": "While the Large Language Models (LLMs) dominate a majority of language\nunderstanding tasks, previous work shows that some of these results are\nsupported by modelling spurious correlations of training datasets. Authors\ncommonly assess model robustness by evaluating their models on\nout-of-distribution (OOD) datasets of the same task, but these datasets might\nshare the bias of the training dataset.\n  We propose a simple method for measuring a scale of models' reliance on any\nidentified spurious feature and assess the robustness towards a large set of\nknown and newly found prediction biases for various pre-trained models and\ndebiasing methods in Question Answering (QA). We find that while existing\ndebiasing methods can mitigate reliance on a chosen spurious feature, the OOD\nperformance gains of these methods can not be explained by mitigated reliance\non biased features, suggesting that biases are shared among different QA\ndatasets. Finally, we evidence this to be the case by measuring that the\nperformance of models trained on different QA datasets relies comparably on the\nsame bias features. We hope these results will motivate future work to refine\nthe reports of LMs' robustness to a level of adversarial samples addressing\nspecific spurious features.",
        "pdf_link": "https://arxiv.org/pdf/2305.06841v2.pdf"
    },
    {
        "title": "Structured Chain-of-Thought Prompting for Code Generation",
        "authors": [
            "Jia Li",
            "Ge Li",
            "Yongmin Li",
            "Zhi Jin"
        ],
        "published": "2023-05-11T06:43:37Z",
        "summary": "Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive\nperformance in code generation. LLMs take prompts as inputs, and\nChain-of-Thought (CoT) prompting is the state-of-the-art prompting technique.\nCoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural\nlanguage reasoning steps) and then output the code. However, CoT prompting is\ndesigned for natural language generation and has low accuracy in code\ngeneration.\n  In this paper, we propose Structured CoTs (SCoTs) and present a novel\nprompting technique for code generation, named SCoT prompting. Our motivation\nis source code contains rich structural information and any code can be\ncomposed of three program structures (i.e., sequence, branch, and loop\nstructures). Intuitively, structured intermediate reasoning steps make for\nstructured source code. Thus, we ask LLMs to use program structures to build\nCoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs.\nCompared to CoT prompting, SCoT prompting explicitly constrains LLMs to think\nabout how to solve requirements from the view of source code and further the\nperformance of LLMs in code generation. We apply SCoT prompting to two LLMs\n(i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval,\nMBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline\n- CoT prompting by up to 13.79% in Pass@1. (2) Human evaluation shows human\ndevelopers prefer programs from SCoT prompting. (3) SCoT prompting is robust to\nexamples and achieves substantial improvements.",
        "pdf_link": "https://arxiv.org/pdf/2305.06599v3.pdf"
    },
    {
        "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
        "authors": [
            "Hongyuan Lu",
            "Haoyang Huang",
            "Dongdong Zhang",
            "Haoran Yang",
            "Wai Lam",
            "Furu Wei"
        ],
        "published": "2023-05-11T05:19:47Z",
        "summary": "Large language models (LLMs) have shown surprisingly good performance in\nmultilingual neural machine translation (MNMT) even when trained without\nparallel data. Yet, despite the fact that the amount of training data is\ngigantic, they still struggle with translating rare words, particularly for\nlow-resource languages. Even worse, it is usually unrealistic to retrieve\nrelevant demonstrations for in-context learning with low-resource languages on\nLLMs, which restricts the practical use of LLMs for translation -- how should\nwe mitigate this problem? To this end, we present a novel method, CoD, which\naugments LLMs with prior knowledge with the chains of multilingual dictionaries\nfor a subset of input words to elicit translation abilities for LLMs. Extensive\nexperiments indicate that augmenting ChatGPT with CoD elicits large gains by up\nto 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in\nCyrillic script) on FLORES-200 full devtest set. We further demonstrate the\nimportance of chaining the multilingual dictionaries, as well as the\nsuperiority of CoD to few-shot demonstration for low-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2305.06575v3.pdf"
    },
    {
        "title": "How Good are Commercial Large Language Models on African Languages?",
        "authors": [
            "Jessica Ojo",
            "Kelechi Ogueji"
        ],
        "published": "2023-05-11T02:29:53Z",
        "summary": "Recent advancements in Natural Language Processing (NLP) has led to the\nproliferation of large pretrained language models. These models have been shown\nto yield good performance, using in-context learning, even on unseen tasks and\nlanguages. They have also been exposed as commercial APIs as a form of\nlanguage-model-as-a-service, with great adoption. However, their performance on\nAfrican languages is largely unknown. We present a preliminary analysis of\ncommercial large language models on two tasks (machine translation and text\nclassification) across eight African languages, spanning different language\nfamilies and geographical areas. Our results suggest that commercial language\nmodels produce below-par performance on African languages. We also find that\nthey perform better on text classification than machine translation. In\ngeneral, our findings present a call-to-action to ensure African languages are\nwell represented in commercial large language models, given their growing\npopularity.",
        "pdf_link": "https://arxiv.org/pdf/2305.06530v1.pdf"
    },
    {
        "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
        "authors": [
            "Han Cheol Moon",
            "Shafiq Joty",
            "Ruochen Zhao",
            "Megh Thakkar",
            "Xu Chi"
        ],
        "published": "2023-05-11T01:50:16Z",
        "summary": "Large-scale pre-trained language models have shown outstanding performance in\na variety of NLP tasks. However, they are also known to be significantly\nbrittle against specifically crafted adversarial examples, leading to\nincreasing interest in probing the adversarial robustness of NLP systems. We\nintroduce RSMI, a novel two-stage framework that combines randomized smoothing\n(RS) with masked inference (MI) to improve the adversarial robustness of NLP\nsystems. RS transforms a classifier into a smoothed classifier to obtain robust\nrepresentations, whereas MI forces a model to exploit the surrounding context\nof a masked token in an input sequence. RSMI improves adversarial robustness by\n2 to 3 times over existing state-of-the-art methods on benchmark datasets. We\nalso perform in-depth qualitative analysis to validate the effectiveness of the\ndifferent stages of RSMI and probe the impact of its components through\nextensive ablations. By empirically proving the stability of RSMI, we put it\nforward as a practical method to robustly train large-scale NLP models. Our\ncode and datasets are available at https://github.com/Han8931/rsmi_nlp",
        "pdf_link": "https://arxiv.org/pdf/2305.06522v1.pdf"
    },
    {
        "title": "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
        "authors": [
            "Wang-Cheng Kang",
            "Jianmo Ni",
            "Nikhil Mehta",
            "Maheswaran Sathiamoorthy",
            "Lichan Hong",
            "Ed Chi",
            "Derek Zhiyuan Cheng"
        ],
        "published": "2023-05-10T21:43:42Z",
        "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities in\ngeneralizing to new tasks in a zero-shot or few-shot manner. However, the\nextent to which LLMs can comprehend user preferences based on their previous\nbehavior remains an emerging and still unclear research question.\nTraditionally, Collaborative Filtering (CF) has been the most effective method\nfor these tasks, predominantly relying on the extensive volume of rating data.\nIn contrast, LLMs typically demand considerably less data while maintaining an\nexhaustive world knowledge about each item, such as movies or products. In this\npaper, we conduct a thorough examination of both CF and LLMs within the classic\ntask of user rating prediction, which involves predicting a user's rating for a\ncandidate item based on their past ratings. We investigate various LLMs in\ndifferent sizes, ranging from 250M to 540B parameters and evaluate their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. We conduct\ncomprehensive analysis to compare between LLMs and strong CF methods, and find\nthat zero-shot LLMs lag behind traditional recommender models that have the\naccess to user interaction data, indicating the importance of user interaction\ndata. However, through fine-tuning, LLMs achieve comparable or even better\nperformance with only a small fraction of the training data, demonstrating\ntheir potential through data efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2305.06474v1.pdf"
    },
    {
        "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
        "authors": [
            "Hong Wang",
            "Xuan Luo",
            "Weizhi Wang",
            "Xifeng Yan"
        ],
        "published": "2023-05-10T19:09:24Z",
        "summary": "Large language models like ChatGPT have recently demonstrated impressive\ncapabilities in natural language understanding and generation, enabling various\napplications including translation, essay writing, and chit-chatting. However,\nthere is a concern that they can be misused for malicious purposes, such as\nfraud or denial-of-service attacks. Therefore, it is crucial to develop methods\nfor detecting whether the party involved in a conversation is a bot or a human.\nIn this paper, we propose a framework named FLAIR, Finding Large language model\nAuthenticity via a single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, positioning, noise filtering, and ASCII art),\nand those that are easy for bots but difficult for humans (e.g., memorization\nand computation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities and ensure that they are\nserving real users. We open-sourced our dataset on\nhttps://github.com/hongwang600/FLAIR and welcome contributions from the\ncommunity to enrich such detection datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.06424v2.pdf"
    },
    {
        "title": "Automatic Evaluation of Attribution by Large Language Models",
        "authors": [
            "Xiang Yue",
            "Boshi Wang",
            "Ziru Chen",
            "Kai Zhang",
            "Yu Su",
            "Huan Sun"
        ],
        "published": "2023-05-10T16:58:33Z",
        "summary": "A recent focus of large language model (LLM) development, as exemplified by\ngenerative search engines, is to incorporate external references to generate\nand support its claims. However, evaluating the attribution, i.e., verifying\nwhether the generated statement is fully supported by the cited reference,\nremains an open problem. Although human evaluation is common practice, it is\ncostly and time-consuming. In this paper, we investigate the automatic\nevaluation of attribution given by LLMs. We begin by defining different types\nof attribution errors, and then explore two approaches for automatic\nevaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is\nrepurposed from related tasks such as question answering, fact-checking,\nnatural language inference, and summarization. We manually curate a set of test\nexamples covering 12 domains from a generative search engine, New Bing. Our\nresults on this curated test set and simulated examples from existing\nbenchmarks highlight both promising signals and challenges. We hope our problem\nformulation, testbeds, and findings will help lay the foundation for future\nstudies on this important problem.",
        "pdf_link": "https://arxiv.org/pdf/2305.06311v2.pdf"
    },
    {
        "title": "Evaluating Embedding APIs for Information Retrieval",
        "authors": [
            "Ehsan Kamalloo",
            "Xinyu Zhang",
            "Odunayo Ogundepo",
            "Nandan Thakur",
            "David Alfonso-Hermelo",
            "Mehdi Rezagholizadeh",
            "Jimmy Lin"
        ],
        "published": "2023-05-10T16:40:52Z",
        "summary": "The ever-increasing size of language models curtails their widespread\navailability to the community, thereby galvanizing many companies into offering\naccess to large language models through APIs. One particular type, suitable for\ndense retrieval, is a semantic embedding service that builds vector\nrepresentations of input text. With a growing number of publicly available\nAPIs, our goal in this paper is to analyze existing offerings in realistic\nretrieval scenarios, to assist practitioners and researchers in finding\nsuitable services according to their needs. Specifically, we investigate the\ncapabilities of existing semantic embedding APIs on domain generalization and\nmultilingual retrieval. For this purpose, we evaluate these services on two\nstandard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results\nusing the APIs is a budget-friendly approach and is most effective in English,\nin contrast to the standard practice of employing them as first-stage\nretrievers. For non-English retrieval, re-ranking still improves the results,\nbut a hybrid model with BM25 works best, albeit at a higher cost. We hope our\nwork lays the groundwork for evaluating semantic embedding APIs that are\ncritical in search and more broadly, for information access.",
        "pdf_link": "https://arxiv.org/pdf/2305.06300v2.pdf"
    },
    {
        "title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",
        "authors": [
            "Chantal Shaib",
            "Millicent L. Li",
            "Sebastian Joseph",
            "Iain J. Marshall",
            "Junyi Jessy Li",
            "Byron C. Wallace"
        ],
        "published": "2023-05-10T16:40:37Z",
        "summary": "Large language models, particularly GPT-3, are able to produce high quality\nsummaries of general domain news articles in few- and zero-shot settings.\nHowever, it is unclear if such models are similarly capable in more\nspecialized, high-stakes domains such as biomedicine. In this paper, we enlist\ndomain experts (individuals with medical training) to evaluate summaries of\nbiomedical articles generated by GPT-3, given zero supervision. We consider\nboth single- and multi-document settings. In the former, GPT-3 is tasked with\ngenerating regular and plain-language summaries of articles describing\nrandomized controlled trials; in the latter, we assess the degree to which\nGPT-3 is able to \\emph{synthesize} evidence reported across a collection of\narticles. We design an annotation scheme for evaluating model outputs, with an\nemphasis on assessing the factual accuracy of generated summaries. We find that\nwhile GPT-3 is able to summarize and simplify single biomedical articles\nfaithfully, it struggles to provide accurate aggregations of findings over\nmultiple documents. We release all data and annotations used in this work.",
        "pdf_link": "https://arxiv.org/pdf/2305.06299v2.pdf"
    },
    {
        "title": "Privacy-Preserving Prompt Tuning for Large Language Model Services",
        "authors": [
            "Yansong Li",
            "Zhixing Tan",
            "Yang Liu"
        ],
        "published": "2023-05-10T14:41:51Z",
        "summary": "Prompt tuning provides an efficient way for users to customize Large Language\nModels (LLMs) with their private data in the emerging LLM service scenario.\nHowever, the sensitive nature of private data brings the need for privacy\npreservation in LLM service customization. Based on prompt tuning, we propose\nPrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy\nguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,\nallowing users to privatize their data locally with local differential privacy.\nAs prompt tuning performs poorly when directly trained on privatized data, we\nintroduce a novel privatized token reconstruction task that is trained jointly\nwith the downstream task, allowing LLMs to learn better task-dependent\nrepresentations. Despite the simplicity of our framework, experiments show that\nRAPT achieves competitive performance across tasks while providing privacy\nguarantees against adversaries.",
        "pdf_link": "https://arxiv.org/pdf/2305.06212v1.pdf"
    },
    {
        "title": "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
        "authors": [
            "Qingyu Chen",
            "Jingcheng Du",
            "Yan Hu",
            "Vipina Kuttichi Keloth",
            "Xueqing Peng",
            "Kalpana Raja",
            "Rui Zhang",
            "Zhiyong Lu",
            "Hua Xu"
        ],
        "published": "2023-05-10T13:40:06Z",
        "summary": "Biomedical literature is growing rapidly, making it challenging to curate and\nextract knowledge manually. Biomedical natural language processing (BioNLP)\ntechniques that can automatically extract information from biomedical\nliterature help alleviate this burden. Recently, large Language Models (LLMs),\nsuch as GPT-3 and GPT-4, have gained significant attention for their impressive\nperformance. However, their effectiveness in BioNLP tasks and impact on method\ndevelopment and downstream users remain understudied. This pilot study (1)\nestablishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and\none-shot settings in eight BioNLP datasets across four applications: named\nentity recognition, relation extraction, multi-label document classification,\nand semantic similarity and reasoning, (2) examines the errors produced by the\nLLMs and categorized the errors into three types: missingness, inconsistencies,\nand unwanted artificial content, and (3) provides suggestions for using LLMs in\nBioNLP applications. We make the datasets, baselines, and results publicly\navailable to the community via\nhttps://github.com/qingyu-qc/gpt_bionlp_benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2305.16326v2.pdf"
    },
    {
        "title": "Davinci the Dualist: the mind-body divide in large language models and in human learners",
        "authors": [
            "Iris Berent",
            "Alexzander Sansiveri"
        ],
        "published": "2023-05-10T12:28:09Z",
        "summary": "A large literature suggests that people are intuitive Dualists--they consider\nthe mind ethereal, distinct from the body. Past research also shows that\nDualism emerges, in part, via learning (e.g., Barlev & Shtulman, 2021). But\nwhether learning is sufficient to give rise to Dualism is unknown.The evidence\nfrom human learners does address this question because humans are endowed not\nonly with general learning capacities but also with core knowledge capacities.\nAnd recent results suggest that core knowledge begets Dualism (Berent, Theodore\n& Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we\nprobe for a mind-body divide in Davinci--a large language model (LLM) that is\ndevoid of any innate core knowledge. We show that Davinci still leans towards\nDualism, and that this bias increases systematically with the learner's\ninductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist\ntendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a\nfull-blown bias. It selectively considers thoughts (epistemic states) as\ndisembodied--as unlikely to show up in the body (in the brain), but not in its\nabsence (after death). While Davinci's performance is constrained by its\nsyntactic limitations, and it differs from humans, its Dualist bias is robust.\nThese results demonstrate that the mind-body divide is partly learnable from\nexperience.They also show how, as LLM's are exposed to human narratives, they\ninduce not only human knowledge but also human biases.",
        "pdf_link": "https://arxiv.org/pdf/2305.07667v2.pdf"
    },
    {
        "title": "Enriching language models with graph-based context information to better understand textual data",
        "authors": [
            "Albert Roethel",
            "Maria Ganzha",
            "Anna Wróblewska"
        ],
        "published": "2023-05-10T10:57:21Z",
        "summary": "A considerable number of texts encountered daily are somehow connected with\neach other. For example, Wikipedia articles refer to other articles via\nhyperlinks, scientific papers relate to others via citations or (co)authors,\nwhile tweets relate via users that follow each other or reshare content. Hence,\na graph-like structure can represent existing connections and be seen as\ncapturing the \"context\" of the texts. The question thus arises if extracting\nand integrating such context information into a language model might help\nfacilitate a better automated understanding of the text. In this study, we\nexperimentally demonstrate that incorporating graph-based contextualization\ninto BERT model enhances its performance on an example of a classification\ntask. Specifically, on Pubmed dataset, we observed a reduction in error from\n8.51% to 7.96%, while increasing the number of parameters just by 1.6%.\n  Our source code: https://github.com/tryptofanik/gc-bert",
        "pdf_link": "https://arxiv.org/pdf/2305.11070v1.pdf"
    },
    {
        "title": "Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models",
        "authors": [
            "Varun Nair",
            "Elliot Schumacher",
            "Anitha Kannan"
        ],
        "published": "2023-05-10T08:48:53Z",
        "summary": "A medical provider's summary of a patient visit serves several critical\npurposes, including clinical decision-making, facilitating hand-offs between\nproviders, and as a reference for the patient. An effective summary is required\nto be coherent and accurately capture all the medically relevant information in\nthe dialogue, despite the complexity of patient-generated language. Even minor\ninaccuracies in visit summaries (for example, summarizing \"patient does not\nhave a fever\" when a fever is present) can be detrimental to the outcome of\ncare for the patient.\n  This paper tackles the problem of medical conversation summarization by\ndiscretizing the task into several smaller dialogue-understanding tasks that\nare sequentially built upon. First, we identify medical entities and their\naffirmations within the conversation to serve as building blocks. We study\ndynamically constructing few-shot prompts for tasks by conditioning on relevant\npatient information and use GPT-3 as the backbone for our experiments. We also\ndevelop GPT-derived summarization metrics to measure performance against\nreference summaries quantitatively. Both our human evaluation study and metrics\nfor medical correctness show that summaries generated using this approach are\nclinically accurate and outperform the baseline approach of summarizing the\ndialog in a zero-shot, single-prompt setting.",
        "pdf_link": "https://arxiv.org/pdf/2305.05982v1.pdf"
    },
    {
        "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
        "authors": [
            "Jiangjie Chen",
            "Wei Shi",
            "Ziquan Fu",
            "Sijie Cheng",
            "Lei Li",
            "Yanghua Xiao"
        ],
        "published": "2023-05-10T08:35:50Z",
        "summary": "Large language models (LLMs) have been widely studied for their ability to\nstore and utilize positive knowledge. However, negative knowledge, such as\n\"lions don't live in the ocean\", is also ubiquitous in the world but rarely\nmentioned explicitly in the text. What do LLMs know about negative knowledge?\nThis work examines the ability of LLMs to negative commonsense knowledge. We\ndesign a constrained keywords-to-sentence generation task (CG) and a Boolean\nquestion-answering task (QA) to probe LLMs. Our experiments reveal that LLMs\nfrequently fail to generate valid sentences grounded in negative commonsense\nknowledge, yet they can correctly answer polar yes-or-no questions. We term\nthis phenomenon the belief conflict of LLMs. Our further analysis shows that\nstatistical shortcuts and negation reporting bias from language modeling\npre-training cause this conflict.",
        "pdf_link": "https://arxiv.org/pdf/2305.05976v2.pdf"
    },
    {
        "title": "Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition",
        "authors": [
            "Anis Koubaa",
            "Basit Qureshi",
            "Adel Ammar",
            "Zahid Khan",
            "Wadii Boulila",
            "Lahouari Ghouti"
        ],
        "published": "2023-05-10T08:16:46Z",
        "summary": "Since the release of ChatGPT, numerous studies have highlighted the\nremarkable performance of ChatGPT, which often rivals or even surpasses human\ncapabilities in various tasks and domains. However, this paper presents a\ncontrasting perspective by demonstrating an instance where human performance\nexcels in typical tasks suited for ChatGPT, specifically in the domain of\ncomputer programming. We utilize the IEEExtreme Challenge competition as a\nbenchmark, a prestigious, annual international programming contest encompassing\na wide range of problems with different complexities. To conduct a thorough\nevaluation, we selected and executed a diverse set of 102 challenges, drawn\nfrom five distinct IEEExtreme editions, using three major programming\nlanguages: Python, Java, and C++. Our empirical analysis provides evidence that\ncontrary to popular belief, human programmers maintain a competitive edge over\nChatGPT in certain aspects of problem-solving within the programming context.\nIn fact, we found that the average score obtained by ChatGPT on the set of\nIEEExtreme programming problems is 3.9 to 5.8 times lower than the average\nhuman score, depending on the programming language. This paper elaborates on\nthese findings, offering critical insights into the limitations and potential\nareas of improvement for AI-based language models like ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2305.06934v1.pdf"
    },
    {
        "title": "Adapter-TST: A Parameter Efficient Method for Multiple-Attribute Text Style Transfer",
        "authors": [
            "Zhiqiang Hu",
            "Roy Ka-Wei Lee",
            "Nancy F. Chen"
        ],
        "published": "2023-05-10T07:33:36Z",
        "summary": "Adapting a large language model for multiple-attribute text style transfer\nvia fine-tuning can be challenging due to the significant amount of\ncomputational resources and labeled data required for the specific task. In\nthis paper, we address this challenge by introducing AdapterTST, a framework\nthat freezes the pre-trained model's original parameters and enables the\ndevelopment of a multiple-attribute text style transfer model. Using BART as\nthe backbone model, Adapter-TST utilizes different neural adapters to capture\ndifferent attribute information, like a plug-in connected to BART. Our method\nallows control over multiple attributes, like sentiment, tense, voice, etc.,\nand configures the adapters' architecture to generate multiple outputs\nrespected to attributes or compositional editing on the same sentence. We\nevaluate the proposed model on both traditional sentiment transfer and\nmultiple-attribute transfer tasks. The experiment results demonstrate that\nAdapter-TST outperforms all the state-of-the-art baselines with significantly\nlesser computational resources. We have also empirically shown that each\nadapter is able to capture specific stylistic attributes effectively and can be\nconfigured to perform compositional editing.",
        "pdf_link": "https://arxiv.org/pdf/2305.05945v1.pdf"
    },
    {
        "title": "Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment",
        "authors": [
            "Eshaan Tanwar",
            "Subhabrata Dutta",
            "Manish Borthakur",
            "Tanmoy Chakraborty"
        ],
        "published": "2023-05-10T07:24:36Z",
        "summary": "In-context learning (ICL) unfolds as large language models become capable of\ninferring test labels conditioned on a few labeled samples without any gradient\nupdate. ICL-enabled large language models provide a promising step forward\ntoward bypassing recurrent annotation costs in a low-resource setting. Yet,\nonly a handful of past studies have explored ICL in a cross-lingual setting, in\nwhich the need for transferring label-knowledge from a high-resource language\nto a low-resource one is immensely crucial. To bridge the gap, we provide the\nfirst in-depth analysis of ICL for cross-lingual text classification. We find\nthat the prevalent mode of selecting random input-label pairs to construct the\nprompt-context is severely limited in the case of cross-lingual ICL, primarily\ndue to the lack of alignment in the input as well as the output spaces. To\nmitigate this, we propose a novel prompt construction strategy -- Cross-lingual\nIn-context Source-Target Alignment (X-InSTA). With an injected coherence in the\nsemantics of the input examples and a task-based alignment across the source\nand target languages, X-InSTA is able to outperform random prompt selection by\na large margin across three different tasks using 44 different cross-lingual\npairs.",
        "pdf_link": "https://arxiv.org/pdf/2305.05940v3.pdf"
    },
    {
        "title": "Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? A Study on Several Typical Tasks",
        "authors": [
            "Xianzhi Li",
            "Samuel Chan",
            "Xiaodan Zhu",
            "Yulong Pei",
            "Zhiqiang Ma",
            "Xiaomo Liu",
            "Sameena Shah"
        ],
        "published": "2023-05-10T03:13:54Z",
        "summary": "The most recent large language models(LLMs) such as ChatGPT and GPT-4 have\nshown exceptional capabilities of generalist models, achieving state-of-the-art\nperformance on a wide range of NLP tasks with little or no adaptation. How\neffective are such models in the financial domain? Understanding this basic\nquestion would have a significant impact on many downstream financial\nanalytical tasks. In this paper, we conduct an empirical study and provide\nexperimental evidences of their performance on a wide variety of financial text\nanalytical problems, using eight benchmark datasets from five categories of\ntasks. We report both the strengths and limitations of the current models by\ncomparing them to the state-of-the-art fine-tuned approaches and the recently\nreleased domain-specific pretrained models. We hope our study can help\nunderstand the capability of the existing models in the financial domain and\nfacilitate further improvements.",
        "pdf_link": "https://arxiv.org/pdf/2305.05862v2.pdf"
    },
    {
        "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text",
        "authors": [
            "Travis Munyer",
            "Abdullah Tanvir",
            "Arjon Das",
            "Xin Zhong"
        ],
        "published": "2023-05-09T21:31:07Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of text generators. With the potential for misuse\nescalating, the importance of discerning whether texts are human-authored or\ngenerated by LLMs has become paramount. Several preceding studies have ventured\nto address this challenge by employing binary classifiers to differentiate\nbetween human-written and LLM-generated text. Nevertheless, the reliability of\nthese classifiers has been subject to question. Given that consequential\ndecisions may hinge on the outcome of such classification, it is imperative\nthat text source detection is of high caliber. In light of this, the present\npaper introduces DeepTextMark, a deep learning-driven text watermarking\nmethodology devised for text source identification. By leveraging Word2Vec and\nSentence Encoding for watermark insertion, alongside a transformer-based\nclassifier for watermark detection, DeepTextMark epitomizes a blend of\nblindness, robustness, imperceptibility, and reliability. As elaborated within\nthe paper, these attributes are crucial for universal text source detection,\nwith a particular emphasis in this paper on text produced by LLMs. DeepTextMark\noffers a viable \"add-on\" solution to prevailing text generation frameworks,\nrequiring no direct access or alterations to the underlying text generation\nmechanism. Experimental evaluations underscore the high imperceptibility,\nelevated detection accuracy, augmented robustness, reliability, and swift\nexecution of DeepTextMark.",
        "pdf_link": "https://arxiv.org/pdf/2305.05773v2.pdf"
    },
    {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ],
        "published": "2023-05-09T17:52:59Z",
        "summary": "For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people's preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2305.05658v2.pdf"
    },
    {
        "title": "Towards Building the Federated GPT: Federated Instruction Tuning",
        "authors": [
            "Jianyi Zhang",
            "Saeed Vahidian",
            "Martin Kuo",
            "Chunyuan Li",
            "Ruiyi Zhang",
            "Tong Yu",
            "Yufan Zhou",
            "Guoyin Wang",
            "Yiran Chen"
        ],
        "published": "2023-05-09T17:42:34Z",
        "summary": "While \"instruction-tuned\" generative large language models (LLMs) have\ndemonstrated an impressive ability to generalize to new tasks, the training\nphases heavily rely on large amounts of diverse and high-quality instruction\ndata (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data,\nespecially when it comes to human-written data, can pose significant challenges\nboth in terms of cost and accessibility. Moreover, concerns related to privacy\ncan further limit access to such data, making the process of obtaining it a\ncomplex and nuanced undertaking. Consequently, this hinders the generality of\nthe tuned models and may restrict their effectiveness in certain contexts. To\ntackle this issue, our study introduces a new approach called Federated\nInstruction Tuning (FedIT), which leverages federated learning (FL) as the\nlearning framework for the instruction tuning of LLMs. This marks the first\nexploration of FL-based instruction tuning for LLMs. This is especially\nimportant since text data is predominantly generated by end users. Therefore,\nit is imperative to design and adapt FL approaches to effectively leverage\nthese users' diverse instructions stored on local devices, while preserving\nprivacy and ensuring data security. In the current paper, by conducting widely\nused GPT-4 auto-evaluation, we demonstrate that by exploiting the heterogeneous\nand diverse sets of instructions on the client's end with the proposed\nframework FedIT, we improved the performance of LLMs compared to centralized\ntraining with only limited local instructions. Further, in this paper, we\ndeveloped a Github repository named Shepherd. This repository offers a\nfoundational framework for exploring federated fine-tuning of LLMs using\nheterogeneous instructions across diverse categories.",
        "pdf_link": "https://arxiv.org/pdf/2305.05644v2.pdf"
    },
    {
        "title": "Fine-tuning Language Models with Generative Adversarial Reward Modelling",
        "authors": [
            "Zhang Ze Yu",
            "Lau Jia Jaw",
            "Zhang Hui",
            "Bryan Kian Hsiang Low"
        ],
        "published": "2023-05-09T17:06:06Z",
        "summary": "Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to\nsignificantly enhance the performance of large language models (LLMs) by\naligning their outputs with desired human values through instruction tuning.\nHowever, RLHF is constrained by the expertise and productivity limitations of\nhuman evaluators. A response to this downside is to fall back to supervised\nfine-tuning (SFT) with additional carefully selected expert demonstrations.\nHowever, while this method has been proven to be effective, it invariably also\nleads to increased human-in-the-loop overhead. In this study, we propose\nanother alternative approach: Reinforcement Learning with Generative\nAdversarial Feedback (RLGAF) to RLHF and SFT, which uses a generative\nadversarial training style to enable the LLMs to learn useful human expert\ndemonstrations without being directly exposed to the training examples, thus\nenabling good generalization capabilities while preserving sample efficiency.\nOur preliminary findings indicate that RLGAF can help align LLMs outputs with\ncompetitive performance against RLHF and SFT, while not suffering from their\nrespective inherent restrictions, suggesting promising avenues for further\nresearch on automating AI alignment.",
        "pdf_link": "https://arxiv.org/pdf/2305.06176v3.pdf"
    },
    {
        "title": "The Case Records of ChatGPT: Language Models and Complex Clinical Questions",
        "authors": [
            "Timothy Poterucha",
            "Pierre Elias",
            "Christopher M. Haggerty"
        ],
        "published": "2023-05-09T16:58:32Z",
        "summary": "Background: Artificial intelligence language models have shown promise in\nvarious applications, including assisting with clinical decision-making as\ndemonstrated by strong performance of large language models on medical\nlicensure exams. However, their ability to solve complex, open-ended cases,\nwhich may be representative of clinical practice, remains unexplored. Methods:\nIn this study, the accuracy of large language AI models GPT4 and GPT3.5 in\ndiagnosing complex clinical cases was investigated using published Case Records\nof the Massachusetts General Hospital. A total of 50 cases requiring a\ndiagnosis and diagnostic test published from January 1, 2022 to April 16, 2022\nwere identified. For each case, models were given a prompt requesting the top\nthree specific diagnoses and associated diagnostic tests, followed by case\ntext, labs, and figure legends. Model outputs were assessed in comparison to\nthe final clinical diagnosis and whether the model-predicted test would result\nin a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the\ncorrect diagnosis in 26% and 22% of cases in one attempt, and 46% and 42%\nwithin three attempts, respectively. GPT4 and GPT3.5 provided a correct\nessential diagnostic test in 28% and 24% of cases in one attempt, and 44% and\n50% within three attempts, respectively. No significant differences were found\nbetween the two models, and multiple trials with identical prompts using the\nGPT3.5 model provided similar results. Conclusions: In summary, these models\ndemonstrate potential usefulness in generating differential diagnoses but\nremain limited in their ability to provide a single unifying diagnosis in\ncomplex, open-ended cases. Future research should focus on evaluating model\nperformance in larger datasets of open-ended clinical challenges and exploring\npotential human-AI collaboration strategies to enhance clinical\ndecision-making.",
        "pdf_link": "https://arxiv.org/pdf/2305.05609v1.pdf"
    },
    {
        "title": "ChatGPT as a Text Simplification Tool to Remove Bias",
        "authors": [
            "Charmaine Barker",
            "Dimitar Kazakov"
        ],
        "published": "2023-05-09T13:10:23Z",
        "summary": "The presence of specific linguistic signals particular to a certain sub-group\nof people can be picked up by language models during training. If the model\nbegins to associate specific language with a distinct group, any decisions made\nbased upon this language would hold a strong correlation to a decision based\nupon their protected characteristic, leading to possible discrimination. We\nexplore a potential technique for bias mitigation in the form of simplification\nof text. The driving force of this idea is that simplifying text should\nstandardise language between different sub-groups to one way of speaking while\nkeeping the same meaning. The experiment shows promising results as the\nclassifier accuracy for predicting the sensitive attribute drops by up to 17%\nfor the simplified data.",
        "pdf_link": "https://arxiv.org/pdf/2305.06166v2.pdf"
    },
    {
        "title": "Large Language Models Need Holistically Thought in Medical Conversational QA",
        "authors": [
            "Yixuan Weng",
            "Bin Li",
            "Fei Xia",
            "Minjun Zhu",
            "Bin Sun",
            "Shizhu He",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2023-05-09T12:57:28Z",
        "summary": "The medical conversational question answering (CQA) system aims at providing\na series of professional medical services to improve the efficiency of medical\ncare. Despite the success of large language models (LLMs) in complex reasoning\ntasks in various fields, such as mathematics, logic, and commonsense QA, they\nstill need to improve with the increased complexity and specialization of the\nmedical field. This is because medical CQA tasks require not only strong\nmedical reasoning, but also the ability to think broadly and deeply. In this\npaper, to address these challenges in medical CQA tasks that need to be\nconsidered and understood in many aspects, we propose the Holistically Thought\n(HoT) method, which is designed to guide the LLMs to perform the diffused and\nfocused thinking for generating high-quality medical responses. The proposed\nHoT method has been evaluated through automated and manual assessments in three\ndifferent medical CQA datasets containing the English and Chinese languages.\nThe extensive experimental results show that our method can produce more\ncorrectness, professional, and considerate answers than several\nstate-of-the-art (SOTA) methods, manifesting its effectiveness. Our code in\nhttps://github.com/WENGSYX/HoT.",
        "pdf_link": "https://arxiv.org/pdf/2305.05410v2.pdf"
    },
    {
        "title": "A Taxonomy of Foundation Model based Systems through the Lens of Software Architecture",
        "authors": [
            "Qinghua Lu",
            "Liming Zhu",
            "Xiwei Xu",
            "Yue Liu",
            "Zhenchang Xing",
            "Jon Whittle"
        ],
        "published": "2023-05-09T11:37:16Z",
        "summary": "The recent release of large language model (LLM) based chatbots, such as\nChatGPT, has attracted huge interest in foundation models. It is widely\nbelieved that foundation models will serve as the fundamental building blocks\nfor future AI systems. As foundation models are in their early stages, the\ndesign of foundation model based systems has not yet been systematically\nexplored. There is limited understanding about the impact of introducing\nfoundation models in software architecture. Therefore, in this paper, we\npropose a taxonomy of foundation model based systems, which classifies and\ncompares the characteristics of foundation models and design options of\nfoundation model based systems. Our taxonomy comprises three categories: the\npretraining and adaptation of foundation models, the architecture design of\nfoundation model based systems, and responsible-AI-by-design. This taxonomy can\nserve as concrete guidance for making major architectural design decisions when\ndesigning foundation model based systems and highlights trade-offs arising from\ndesign decisions.",
        "pdf_link": "https://arxiv.org/pdf/2305.05352v6.pdf"
    },
    {
        "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
        "authors": [
            "Shanshan Zhong",
            "Zhongzhan Huang",
            "Wushao Wen",
            "Jinghui Qin",
            "Liang Lin"
        ],
        "published": "2023-05-09T05:48:38Z",
        "summary": "Diffusion models, which have emerged to become popular text-to-image\ngeneration models, can produce high-quality and content-rich images guided by\ntextual prompts. However, there are limitations to semantic understanding and\ncommonsense reasoning in existing models when the input prompts are concise\nnarrative, resulting in low-quality image generation. To improve the capacities\nfor narrative prompts, we propose a simple-yet-effective parameter-efficient\nfine-tuning approach called the Semantic Understanding and Reasoning adapter\n(SUR-adapter) for pre-trained diffusion models. To reach this goal, we first\ncollect and annotate a new dataset SURD which consists of more than 57,000\nsemantically corrected multi-modal samples. Each sample contains a simple\nnarrative prompt, a complex keyword-based prompt, and a high-quality image.\nThen, we align the semantic representation of narrative prompts to the complex\nprompts and transfer knowledge of large language models (LLMs) to our\nSUR-adapter via knowledge distillation so that it can acquire the powerful\nsemantic understanding and reasoning capabilities to build a high-quality\ntextual semantic representation for text-to-image generation. We conduct\nexperiments by integrating multiple LLMs and popular pre-trained diffusion\nmodels to show the effectiveness of our approach in enabling diffusion models\nto understand and reason concise natural language without image quality\ndegradation. Our approach can make text-to-image diffusion models easier to use\nwith better user experience, which demonstrates our approach has the potential\nfor further advancing the development of user-friendly text-to-image generation\nmodels by bridging the semantic gap between simple narrative prompts and\ncomplex keyword-based prompts. The code is released at\nhttps://github.com/Qrange-group/SUR-adapter.",
        "pdf_link": "https://arxiv.org/pdf/2305.05189v4.pdf"
    },
    {
        "title": "MoT: Memory-of-Thought Enables ChatGPT to Self-Improve",
        "authors": [
            "Xiaonan Li",
            "Xipeng Qiu"
        ],
        "published": "2023-05-09T05:25:05Z",
        "summary": "Large Language Models (LLMs) have shown impressive abilities in various\ntasks. However, fundamentally improving them depends on high-quality datasets\nor computationally expensive fine-tuning. On the contrary, humans can easily\nimprove themselves by self-thinking and memory, without external resources. In\nthis paper, we propose a framework, MoT, to let the LLM self-improve through\nMemory-of-Thought, without annotated datasets and parameter updates.\nSpecifically, MoT is divided into two stages: 1. before the test stage, the LLM\npre-thinks on the unlabeled dataset and saves the high-confidence thoughts as\nexternal memory; 2. During the test stage, given a test question, the LLM\nrecalls relevant memory to help itself reason and answer it. Experimental\nresults show that MoT can help ChatGPT significantly improve its abilities in\narithmetic reasoning, commonsense reasoning, factual reasoning, and natural\nlanguage inference. Further analyses show that each component contributes\ncritically to the improvements and MoT can lead to consistent improvements\nacross various CoT methods and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.05181v2.pdf"
    },
    {
        "title": "Accessible Instruction-Following Agent",
        "authors": [
            "Kairui Zhou"
        ],
        "published": "2023-05-08T23:57:26Z",
        "summary": "Humans can collaborate and complete tasks based on visual signals and\ninstruction from the environment. Training such a robot is difficult especially\ndue to the understanding of the instruction and the complicated environment.\nPrevious instruction-following agents are biased to English-centric corpus,\nmaking it unrealizable to be applied to users that use multiple languages or\neven low-resource languages. Nevertheless, the instruction-following agents are\npre-trained in a mode that assumes the user can observe the environment, which\nlimits its accessibility. In this work, we're trying to generalize the success\nof instruction-following agents to non-English languages with little corpus\nresources, and improve its intractability and accessibility. We introduce UVLN\n(Universal Vision-Language Navigation), a novel machine-translation\ninstructional augmented framework for cross-lingual vision-language navigation,\nwith a novel composition of state-of-the-art large language model (GPT3) with\nthe image caption model (BLIP). We first collect a multilanguage\nvision-language navigation dataset via machine translation. Then we extend the\nstandard VLN training objectives to a multilingual setting via a cross-lingual\nlanguage encoder. The alignment between different languages is captured through\na shared vision and action context via a cross-modal transformer, which encodes\nthe inputs of language instruction, visual observation, and action decision\nsequences. To improve the intractability, we connect our agent with the large\nlanguage model that informs the situation and current state to the user and\nalso explains the action decisions. Experiments over Room Across Room Dataset\nprove the effectiveness of our approach. And the qualitative results show the\npromising intractability and accessibility of our instruction-following agent.",
        "pdf_link": "https://arxiv.org/pdf/2305.06358v1.pdf"
    },
    {
        "title": "Knowledge-enhanced Agents for Interactive Text Games",
        "authors": [
            "Prateek Chhikara",
            "Jiarui Zhang",
            "Filip Ilievski",
            "Jonathan Francis",
            "Kaixin Ma"
        ],
        "published": "2023-05-08T23:31:39Z",
        "summary": "Communication via natural language is a key aspect of machine intelligence,\nand it requires computational models to learn and reason about world concepts,\nwith varying levels of supervision. Significant progress has been made on\nfully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding. Yet, various sequential interactive tasks, as in\ntext-based games, have revealed limitations of existing approaches in terms of\ncoherence, contextual awareness, and their ability to learn effectively from\nthe environment. In this paper, we propose a knowledge-injection framework for\nimproved functional grounding of agents in text-based games. Specifically, we\nconsider two forms of domain knowledge that we inject into learning-based\nagents: memory of previous correct actions and affordances of relevant objects\nin the environment. Our framework supports two representative model classes:\nreinforcement learning agents and language model agents. Furthermore, we devise\nmultiple injection strategies for the above domain knowledge types and agent\narchitectures, including injection via knowledge graphs and augmentation of the\nexisting input encoding strategies. We experiment with four models on the 10\ntasks in the ScienceWorld text-based game environment, to illustrate the impact\nof knowledge injection on various model configurations and challenging task\nsettings. Our findings provide crucial insights into the interplay between task\nproperties, model architectures, and domain knowledge for interactive contexts.",
        "pdf_link": "https://arxiv.org/pdf/2305.05091v2.pdf"
    },
    {
        "title": "Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer",
        "authors": [
            "Tao Hong"
        ],
        "published": "2023-05-08T21:35:12Z",
        "summary": "Large Language Models (LLMs), such as the Generative Pretrained Transformer\n(GPT), have achieved tremendous success in various language tasks, but their\nemergent abilities have also raised many questions, concerns, and challenges\nthat need to be addressed. To gain a better understanding of the models' inner\nmechanisms, we analyze the hidden state and channel wave dynamics in a small\nGPT, focusing on the coherence of wave patterns in terms of cross-channel\ncorrelation and individual auto-correlation. Our findings suggest that wave\ndynamics offer consistent and repeatable intrinsic oscillation modes, along\nwith context-aware plasticity and expressiveness in language generation. By\nanalyzing wave patterns, coherence, and clustering, we provide a systematic way\nto identify and interpret the functionality of the hidden state channels,\npaving the way to understand and control higher-level language pattern\nformation. In addition, we investigate the Poisson statistics of spelling\nerrors in text sequence generation across various levels of model training and\nobserve a phase-transition-like process. As coherence builds up, there is a\ncompetition between the generation of correct and misspelled words. However,\nonce the model is adequately trained and significant coherence has emerged, the\ncoherent process becomes strong enough to effectively suppress spelling errors,\npreventing the cascade amplification of defects. The distribution of correct\nspellings transitions from Poissonian to Sub-Poissonian, while the distribution\nof misspellings shows the opposite trend. By leveraging concepts and techniques\nfrom quantum physics, we gain novel insights into the dynamics of the small\nGPT. This approach can be extended to larger language models that exhibit more\ncomplex coherent language patterns, opening up opportunities to interpret their\nemergent capabilities and develop more specialized models.",
        "pdf_link": "https://arxiv.org/pdf/2305.05061v1.pdf"
    },
    {
        "title": "ANALOGICAL -- A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models",
        "authors": [
            "Thilini Wijesiriwardene",
            "Ruwan Wickramarachchi",
            "Bimal G. Gajera",
            "Shreeyash Mukul Gowaikar",
            "Chandan Gupta",
            "Aman Chadha",
            "Aishwarya Naresh Reganti",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2023-05-08T21:12:20Z",
        "summary": "Over the past decade, analogies, in the form of word-level analogies, have\nplayed a significant role as an intrinsic measure of evaluating the quality of\nword embedding methods such as word2vec. Modern large language models (LLMs),\nhowever, are primarily evaluated on extrinsic measures based on benchmarks such\nas GLUE and SuperGLUE, and there are only a few investigations on whether LLMs\ncan draw analogies between long texts. In this paper, we present ANALOGICAL, a\nnew benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of\nlong text with six levels of complexity -- (i) word, (ii) word vs. sentence,\n(iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using\nthirteen datasets and three different distance measures, we evaluate the\nabilities of eight LLMs in identifying analogical pairs in the semantic vector\nspace. Our evaluation finds that it is increasingly challenging for LLMs to\nidentify analogies when going up the analogy taxonomy.",
        "pdf_link": "https://arxiv.org/pdf/2305.05050v3.pdf"
    },
    {
        "title": "Web Content Filtering through knowledge distillation of Large Language Models",
        "authors": [
            "Tamás Vörös",
            "Sean Paul Bergeron",
            "Konstantin Berlin"
        ],
        "published": "2023-05-08T20:09:27Z",
        "summary": "We introduce a state-of-the-art approach for URL categorization that\nleverages the power of Large Language Models (LLMs) to address the primary\nobjectives of web content filtering: safeguarding organizations from legal and\nethical risks, limiting access to high-risk or suspicious websites, and\nfostering a secure and professional work environment. Our method utilizes LLMs\nto generate accurate classifications and then employs established knowledge\ndistillation techniques to create smaller, more specialized student models\ntailored for web content filtering. Distillation results in a student model\nwith a 9% accuracy rate improvement in classifying websites, sourced from\ncustomer telemetry data collected by a large security vendor, into 30 distinct\ncontent categories based on their URLs, surpassing the current state-of-the-art\napproach. Our student model matches the performance of the teacher LLM with 175\ntimes less parameters, allowing the model to be used for in-line scanning of\nlarge volumes of URLs, and requires 3 orders of magnitude less manually labeled\ntraining data than the current state-of-the-art approach. Depending on the\nspecific use case, the output generated by our approach can either be directly\nreturned or employed as a pre-filter for more resource-intensive operations\ninvolving website images or HTML.",
        "pdf_link": "https://arxiv.org/pdf/2305.05027v2.pdf"
    },
    {
        "title": "Revisiting Relation Extraction in the era of Large Language Models",
        "authors": [
            "Somin Wadhwa",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "published": "2023-05-08T19:19:07Z",
        "summary": "Relation extraction (RE) is the core NLP task of inferring semantic\nrelationships between entities from text. Standard supervised RE techniques\nentail training modules to tag tokens comprising entity spans and then predict\nthe relationship between them. Recent work has instead treated the problem as a\n\\emph{sequence-to-sequence} task, linearizing relations between entities as\ntarget strings to be generated conditioned on the input. Here we push the\nlimits of this approach, using larger language models (GPT-3 and Flan-T5 large)\nthan considered in prior work and evaluating their performance on standard RE\ntasks under varying levels of supervision. We address issues inherent to\nevaluating generative approaches to RE by doing human evaluations, in lieu of\nrelying on exact matching. Under this refined evaluation, we find that: (1)\nFew-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly\nequivalent to existing fully supervised models; (2) Flan-T5 is not as capable\nin the few-shot setting, but supervising and fine-tuning it with\nChain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA\nresults. We release this model as a new baseline for RE tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.05003v1.pdf"
    },
    {
        "title": "Explanation-based Finetuning Makes Models More Robust to Spurious Cues",
        "authors": [
            "Josh Magnus Ludan",
            "Yixuan Meng",
            "Tai Nguyen",
            "Saurabh Shah",
            "Qing Lyu",
            "Marianna Apidianaki",
            "Chris Callison-Burch"
        ],
        "published": "2023-05-08T18:53:45Z",
        "summary": "Large Language Models (LLMs) are so powerful that they sometimes learn\ncorrelations between labels and features that are irrelevant to the task,\nleading to poor generalization on out-of-distribution data. We propose\nexplanation-based finetuning as a general approach to mitigate LLMs' reliance\non spurious correlations. Unlike standard finetuning where the model only\npredicts the answer given the input, we finetune the model to additionally\ngenerate a free-text explanation supporting its answer. To evaluate our method,\nwe finetune the model on artificially constructed training sets containing\ndifferent types of spurious cues, and test it on a test set without these cues.\nCompared to standard finetuning, our method makes GPT-3 (davinci) remarkably\nmore robust against spurious cues in terms of accuracy drop across four\nclassification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC\n(+6.5). The efficacy generalizes across multiple model families and scales,\nwith greater gains for larger models. Finally, our method also works well with\nexplanations generated by the model, implying its applicability to more\ndatasets without human-written explanations.",
        "pdf_link": "https://arxiv.org/pdf/2305.04990v3.pdf"
    },
    {
        "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
        "authors": [
            "Kaushik Roy",
            "Tarun Garg",
            "Vedant Palit",
            "Yuxin Zi",
            "Vignesh Narayanan",
            "Amit Sheth"
        ],
        "published": "2023-05-08T18:53:14Z",
        "summary": "A fundamental question in natural language processing is - what kind of\nlanguage structure and semantics is the language model capturing? Graph formats\nsuch as knowledge graphs are easy to evaluate as they explicitly express\nlanguage semantics and structure. This study evaluates the semantics encoded in\nthe self-attention transformers by leveraging explicit knowledge graph\nstructures. We propose novel metrics to measure the reconstruction error when\nproviding graph path sequences from a knowledge graph and trying to\nreproduce/reconstruct the same from the outputs of the self-attention\ntransformer models. The opacity of language models has an immense bearing on\nsocietal issues of trust and explainable decision outcomes. Our findings\nsuggest that language models are models of stochastic control processes for\nplausible language pattern generation. However, they do not ascribe object and\nconcept-level meaning and semantics to the learned stochastic patterns such as\nthose described in knowledge graphs. Furthermore, to enable robust evaluation\nof concept understanding by language models, we construct and make public an\naugmented language understanding benchmark built on the General Language\nUnderstanding Evaluation (GLUE) benchmark. This has significant\napplication-level user trust implications as stochastic patterns without a\nstrong sense of meaning cannot be trusted in high-stakes applications.",
        "pdf_link": "https://arxiv.org/pdf/2305.04989v1.pdf"
    },
    {
        "title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge",
        "authors": [
            "Phillip Howard",
            "Junlin Wang",
            "Vasudev Lal",
            "Gadi Singer",
            "Yejin Choi",
            "Swabha Swayamdipta"
        ],
        "published": "2023-05-08T18:20:36Z",
        "summary": "Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is\nan essential component of our world knowledge, yet understudied in prior\nliterature. In this paper, we harvest the dramatic improvements in knowledge\ncapabilities of language models into a large-scale comparative knowledge base.\nWhile the ease of acquisition of such comparative knowledge is much higher from\nextreme-scale models like GPT-4, compared to their considerably smaller and\nweaker counterparts such as GPT-2, not even the most powerful models are exempt\nfrom making errors. We thus ask: to what extent are models at different scales\nable to generate valid and diverse comparative knowledge?\n  We introduce NeuroComparatives, a novel framework for comparative knowledge\ndistillation overgenerated from language models such as GPT-variants and LLaMA,\nfollowed by stringent filtering of the generated knowledge. Our framework\nacquires comparative knowledge between everyday objects, producing a corpus of\nup to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more\ndiverse than existing resources. Moreover, human evaluations show that\nNeuroComparatives outperform existing resources in terms of validity (up to 32%\nabsolute improvement). Our acquired NeuroComparatives leads to performance\nimprovements on five downstream tasks. We find that neuro-symbolic manipulation\nof smaller models offers complementary benefits to the currently dominant\npractice of prompting extreme-scale language models for knowledge distillation.",
        "pdf_link": "https://arxiv.org/pdf/2305.04978v3.pdf"
    },
    {
        "title": "How Do In-Context Examples Affect Compositional Generalization?",
        "authors": [
            "Shengnan An",
            "Zeqi Lin",
            "Qiang Fu",
            "Bei Chen",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Dongmei Zhang"
        ],
        "published": "2023-05-08T16:32:18Z",
        "summary": "Compositional generalization--understanding unseen combinations of seen\nprimitives--is an essential reasoning capability in human intelligence. The AI\ncommunity mainly studies this capability by fine-tuning neural networks on lots\nof training samples, while it is still unclear whether and how in-context\nlearning--the prevailing few-shot paradigm based on large language\nmodels--exhibits compositional generalization. In this paper, we present CoFe,\na test suite to investigate in-context compositional generalization. We find\nthat the compositional generalization performance can be easily affected by the\nselection of in-context examples, thus raising the research question what the\nkey factors are to make good in-context examples for compositional\ngeneralization. We study three potential factors: similarity, diversity and\ncomplexity. Our systematic experiments indicate that in-context examples should\nbe structurally similar to the test case, diverse from each other, and\nindividually simple. Furthermore, two strong limitations are observed:\nin-context compositional generalization on fictional words is much weaker than\nthat on commonly used ones; it is still critical that the in-context examples\nshould cover required linguistic structures, even though the backbone model has\nbeen pre-trained on large corpus. We hope our analysis would facilitate the\nunderstanding and utilization of in-context learning paradigm.",
        "pdf_link": "https://arxiv.org/pdf/2305.04835v3.pdf"
    },
    {
        "title": "Learning Summary-Worthy Visual Representation for Abstractive Summarization in Video",
        "authors": [
            "Zenan Xu",
            "Xiaojun Meng",
            "Yasheng Wang",
            "Qinliang Su",
            "Zexuan Qiu",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2023-05-08T16:24:46Z",
        "summary": "Multimodal abstractive summarization for videos (MAS) requires generating a\nconcise textual summary to describe the highlights of a video according to\nmultimodal resources, in our case, the video content and its transcript.\nInspired by the success of the large-scale generative pre-trained language\nmodel (GPLM) in generating high-quality textual content (e.g., summary), recent\nMAS methods have proposed to adapt the GPLM to this task by equipping it with\nthe visual information, which is often obtained through a general-purpose\nvisual feature extractor. However, the generally extracted visual features may\noverlook some summary-worthy visual information, which impedes model\nperformance. In this work, we propose a novel approach to learning the\nsummary-worthy visual representation that facilitates abstractive\nsummarization. Our method exploits the summary-worthy information from both the\ncross-modal transcript data and the knowledge that distills from the pseudo\nsummary. Extensive experiments on three public multimodal datasets show that\nour method outperforms all competing baselines. Furthermore, with the\nadvantages of summary-worthy visual information, our model can have a\nsignificant improvement on small datasets or even datasets with limited\ntraining data.",
        "pdf_link": "https://arxiv.org/pdf/2305.04824v1.pdf"
    },
    {
        "title": "Influence of External Information on Large Language Models Mirrors Social Cognitive Patterns",
        "authors": [
            "Ning Bian",
            "Hongyu Lin",
            "Peilin Liu",
            "Yaojie Lu",
            "Chunkang Zhang",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "published": "2023-05-08T16:10:18Z",
        "summary": "Social cognitive theory explains how people learn and acquire knowledge\nthrough observing others. Recent years have witnessed the rapid development of\nlarge language models (LLMs), which suggests their potential significance as\nagents in the society. LLMs, as AI agents, can observe external information,\nwhich shapes their cognition and behaviors. However, the extent to which\nexternal information influences LLMs' cognition and behaviors remains unclear.\nThis study investigates how external statements and opinions influence LLMs'\nthoughts and behaviors from a social cognitive perspective. Three experiments\nwere conducted to explore the effects of external information on LLMs'\nmemories, opinions, and social media behavioral decisions. Sociocognitive\nfactors, including source authority, social identity, and social role, were\nanalyzed to investigate their moderating effects. Results showed that external\ninformation can significantly shape LLMs' memories, opinions, and behaviors,\nwith these changes mirroring human social cognitive patterns such as authority\nbias, in-group bias, emotional positivity, and emotion contagion. This\nunderscores the challenges in developing safe and unbiased LLMs, and emphasizes\nthe importance of understanding the susceptibility of LLMs to external\ninfluences.",
        "pdf_link": "https://arxiv.org/pdf/2305.04812v3.pdf"
    },
    {
        "title": "Algebra Error Classification with Large Language Models",
        "authors": [
            "Hunter McNichols",
            "Mengxue Zhang",
            "Andrew Lan"
        ],
        "published": "2023-05-08T15:51:38Z",
        "summary": "Automated feedback as students answer open-ended math questions has\nsignificant potential in improving learning outcomes at large scale. A key part\nof automated feedback systems is an error classification component, which\nidentifies student errors and enables appropriate, predefined feedback to be\ndeployed. Most existing approaches to error classification use a rule-based\nmethod, which has limited capacity to generalize. Existing data-driven methods\navoid these limitations but specifically require mathematical expressions in\nstudent responses to be parsed into syntax trees. This requirement is itself a\nlimitation, since student responses are not always syntactically valid and\ncannot be converted into trees. In this work, we introduce a flexible method\nfor error classification using pre-trained large language models. We\ndemonstrate that our method can outperform existing methods in algebra error\nclassification, and is able to classify a larger set of student responses.\nAdditionally, we analyze common classification errors made by our method and\ndiscuss limitations of automated error classification.",
        "pdf_link": "https://arxiv.org/pdf/2305.06163v1.pdf"
    },
    {
        "title": "Augmented Large Language Models with Parametric Knowledge Guiding",
        "authors": [
            "Ziyang Luo",
            "Can Xu",
            "Pu Zhao",
            "Xiubo Geng",
            "Chongyang Tao",
            "Jing Ma",
            "Qingwei Lin",
            "Daxin Jiang"
        ],
        "published": "2023-05-08T15:05:16Z",
        "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing (NLP) with their impressive language understanding and generation\ncapabilities. However, their performance may be suboptimal for domain-specific\ntasks that require specialized knowledge due to limited exposure to the related\ndata. Additionally, the lack of transparency of most state-of-the-art (SOTA)\nLLMs, which can only be accessed via APIs, impedes further fine-tuning with\ndomain custom data. Moreover, providing private data to the LLMs' owner leads\nto data privacy problems. To address these challenges, we propose the novel\nParametric Knowledge Guiding (PKG) framework, which equips LLMs with a\nknowledge-guiding module to access relevant knowledge without altering the\nLLMs' parameters. Our PKG is based on open-source \"white-box\" language models,\nallowing offline memory of any knowledge that LLMs require. We demonstrate that\nour PKG framework can enhance the performance of \"black-box\" LLMs on a range of\ndomain knowledge-intensive tasks that require factual (+7.9%), tabular\n(+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2305.04757v2.pdf"
    },
    {
        "title": "Differentially Private Attention Computation",
        "authors": [
            "Yeqi Gao",
            "Zhao Song",
            "Xin Yang"
        ],
        "published": "2023-05-08T13:32:41Z",
        "summary": "Large language models (LLMs) have had a profound impact on numerous aspects\nof daily life including natural language processing, content generation,\nresearch methodologies and so on. However, one crucial issue concerning the\ninference results of large language models is security and privacy. In many\nscenarios, the results generated by LLMs could possibly leak many confidential\nor copyright information. A recent beautiful and breakthrough work [Vyas,\nKakade and Barak 2023] focus on such privacy issue of the LLMs from theoretical\nperspective. It is well-known that computing the attention matrix is one of the\nmajor task during the LLMs computation. Thus, how to give a provable privately\nguarantees of computing the attention matrix is an important research\ndirection.\n  Previous work [Alman and Song 2023, Brand, Song and Zhou 2023] have proposed\nprovable tight result for fast computation of attention without considering\nprivacy concerns. One natural mathematical formulation to quantity the privacy\nin theoretical computer science graduate school textbook is differential\nprivacy. Inspired by [Vyas, Kakade and Barak 2023], in this work, we provide a\nprovable result for showing how to differentially private approximate the\nattention matrix.\n  From technique perspective, our result replies on a pioneering work in the\narea of differential privacy by [Alabi, Kothari, Tankala, Venkat and Zhang\n2022].",
        "pdf_link": "https://arxiv.org/pdf/2305.04701v1.pdf"
    },
    {
        "title": "Enhancing Knowledge Graph Construction Using Large Language Models",
        "authors": [
            "Milena Trajanoska",
            "Riste Stojanov",
            "Dimitar Trajanov"
        ],
        "published": "2023-05-08T12:53:06Z",
        "summary": "The growing trend of Large Language Models (LLM) development has attracted\nsignificant attention, with models for various applications emerging\nconsistently. However, the combined application of Large Language Models with\nsemantic technologies for reasoning and inference is still a challenging task.\nThis paper analyzes how the current advances in foundational LLM, like ChatGPT,\ncan be compared with the specialized pretrained models, like REBEL, for joint\nentity and relation extraction. To evaluate this approach, we conducted several\nexperiments using sustainability-related text as our use case. We created\npipelines for the automatic creation of Knowledge Graphs from raw texts, and\nour findings indicate that using advanced LLM models can improve the accuracy\nof the process of creating these graphs from unstructured text. Furthermore, we\nexplored the potential of automatic ontology creation using foundation LLM\nmodels, which resulted in even more relevant and accurate knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2305.04676v1.pdf"
    },
    {
        "title": "Sparks of Artificial General Recommender (AGR): Early Experiments with ChatGPT",
        "authors": [
            "Guo Lin",
            "Yongfeng Zhang"
        ],
        "published": "2023-05-08T07:28:16Z",
        "summary": "This study investigates the feasibility of developing an Artificial General\nRecommender (AGR), facilitated by recent advancements in Large Language Models\n(LLMs). An AGR comprises both conversationality and universality to engage in\nnatural dialogues and generate recommendations across various domains. We\npropose ten fundamental principles that an AGR should adhere to, each with its\ncorresponding testing protocols. We proceed to assess whether ChatGPT, a\nsophisticated LLM, can comply with the proposed principles by engaging in\nrecommendation-oriented dialogues with the model while observing its behavior.\nOur findings demonstrate the potential for ChatGPT to serve as an AGR, though\nseveral limitations and areas for improvement are identified.",
        "pdf_link": "https://arxiv.org/pdf/2305.04518v1.pdf"
    },
    {
        "title": "Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding",
        "authors": [
            "Bhanu Prakash Voutharoja",
            "Lizhen Qu",
            "Fatemeh Shiri"
        ],
        "published": "2023-05-08T05:03:07Z",
        "summary": "Recent works on form understanding mostly employ multimodal transformers or\nlarge-scale pre-trained language models. These models need ample data for\npre-training. In contrast, humans can usually identify key-value pairings from\na form only by looking at layouts, even if they don't comprehend the language\nused. No prior research has been conducted to investigate how helpful layout\ninformation alone is for form understanding. Hence, we propose a unique\nentity-relation graph parsing method for scanned forms called LAGNN, a\nlanguage-independent Graph Neural Network model. Our model parses a form into a\nword-relation graph in order to identify entities and relations jointly and\nreduce the time complexity of inference. This graph is then transformed by\ndeterministic rules into a fully connected entity-relation graph. Our model\nsimply takes into account relative spacing between bounding boxes from layout\ninformation to facilitate easy transfer across languages. To further improve\nthe performance of LAGNN, and achieve isomorphism between entity-relation\ngraphs and word-relation graphs, we use integer linear programming (ILP) based\ninference. Code is publicly available at https://github.com/Bhanu068/LAGNN",
        "pdf_link": "https://arxiv.org/pdf/2305.04460v1.pdf"
    },
    {
        "title": "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for Zero-Shot Semantic Annotation of Legal Texts",
        "authors": [
            "Jaromir Savelka"
        ],
        "published": "2023-05-08T01:55:53Z",
        "summary": "We evaluated the capability of a state-of-the-art generative pre-trained\ntransformer (GPT) model to perform semantic annotation of short text snippets\n(one to few sentences) coming from legal documents of various types.\nDiscussions of potential uses (e.g., document drafting, summarization) of this\nemerging technology in legal domain have intensified, but to date there has not\nbeen a rigorous analysis of these large language models' (LLM) capacity in\nsentence-level semantic annotation of legal texts in zero-shot learning\nsettings. Yet, this particular type of use could unlock many practical\napplications (e.g., in contract review) and research opportunities (e.g., in\nempirical legal studies). We fill the gap with this study. We examined if and\nhow successfully the model can semantically annotate small batches of short\ntext snippets (10-50) based exclusively on concise definitions of the semantic\ntypes. We found that the GPT model performs surprisingly well in zero-shot\nsettings on diverse types of documents (F1=.73 on a task involving court\nopinions, .86 for contracts, and .54 for statutes and regulations). These\nfindings can be leveraged by legal scholars and practicing lawyers alike to\nguide their decisions in integrating LLMs in wide range of workflows involving\nsemantic annotation of legal texts.",
        "pdf_link": "https://arxiv.org/pdf/2305.04417v1.pdf"
    },
    {
        "title": "Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5",
        "authors": [
            "Gaurav Suri",
            "Lily R. Slater",
            "Ali Ziaee",
            "Morgan Nguyen"
        ],
        "published": "2023-05-08T01:02:52Z",
        "summary": "A Large Language Model (LLM) is an artificial intelligence system that has\nbeen trained on vast amounts of natural language data, enabling it to generate\nhuman-like responses to written or spoken language input. GPT-3.5 is an example\nof an LLM that supports a conversational agent called ChatGPT. In this work, we\nused a series of novel prompts to determine whether ChatGPT shows heuristics,\nbiases, and other decision effects. We also tested the same prompts on human\nparticipants. Across four studies, we found that ChatGPT was influenced by\nrandom anchors in making estimates (Anchoring Heuristic, Study 1); it judged\nthe likelihood of two events occurring together to be higher than the\nlikelihood of either event occurring alone, and it was erroneously influenced\nby salient anecdotal information (Representativeness and Availability\nHeuristic, Study 2); it found an item to be more efficacious when its features\nwere presented positively rather than negatively - even though both\npresentations contained identical information (Framing Effect, Study 3); and it\nvalued an owned item more than a newly found item even though the two items\nwere identical (Endowment Effect, Study 4). In each study, human participants\nshowed similar effects. Heuristics and related decision effects in humans are\nthought to be driven by cognitive and affective processes such as loss aversion\nand effort reduction. The fact that an LLM - which lacks these processes - also\nshows such effects invites consideration of the possibility that language may\nplay a role in generating these effects in humans.",
        "pdf_link": "https://arxiv.org/pdf/2305.04400v1.pdf"
    },
    {
        "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
        "authors": [
            "Miles Turpin",
            "Julian Michael",
            "Ethan Perez",
            "Samuel R. Bowman"
        ],
        "published": "2023-05-07T22:44:25Z",
        "summary": "Large Language Models (LLMs) can achieve strong performance on many tasks by\nproducing step-by-step reasoning before giving a final output, often referred\nto as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT\nexplanations as the LLM's process for solving a task. This level of\ntransparency into LLMs' predictions would yield significant safety benefits.\nHowever, we find that CoT explanations can systematically misrepresent the true\nreason for a model's prediction. We demonstrate that CoT explanations can be\nheavily influenced by adding biasing features to model inputs--e.g., by\nreordering the multiple-choice options in a few-shot prompt to make the answer\nalways \"(A)\"--which models systematically fail to mention in their\nexplanations. When we bias models toward incorrect answers, they frequently\ngenerate CoT explanations rationalizing those answers. This causes accuracy to\ndrop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing\nwith GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task,\nmodel explanations justify giving answers in line with stereotypes without\nmentioning the influence of these social biases. Our findings indicate that CoT\nexplanations can be plausible yet misleading, which risks increasing our trust\nin LLMs without guaranteeing their safety. Building more transparent and\nexplainable systems will require either improving CoT faithfulness through\ntargeted efforts or abandoning CoT in favor of alternative methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.04388v2.pdf"
    },
    {
        "title": "Perception, performance, and detectability of conversational artificial intelligence across 32 university courses",
        "authors": [
            "Hazem Ibrahim",
            "Fengyuan Liu",
            "Rohail Asim",
            "Balaraju Battu",
            "Sidahmed Benabderrahmane",
            "Bashar Alhafni",
            "Wifag Adnan",
            "Tuka Alhanai",
            "Bedoor AlShebli",
            "Riyadh Baghdadi",
            "Jocelyn J. Bélanger",
            "Elena Beretta",
            "Kemal Celik",
            "Moumena Chaqfeh",
            "Mohammed F. Daqaq",
            "Zaynab El Bernoussi",
            "Daryl Fougnie",
            "Borja Garcia de Soto",
            "Alberto Gandolfi",
            "Andras Gyorgy",
            "Nizar Habash",
            "J. Andrew Harris",
            "Aaron Kaufman",
            "Lefteris Kirousis",
            "Korhan Kocak",
            "Kangsan Lee",
            "Seungah S. Lee",
            "Samreen Malik",
            "Michail Maniatakos",
            "David Melcher",
            "Azzam Mourad",
            "Minsu Park",
            "Mahmoud Rasras",
            "Alicja Reuben",
            "Dania Zantout",
            "Nancy W. Gleason",
            "Kinga Makovi",
            "Talal Rahwan",
            "Yasir Zaki"
        ],
        "published": "2023-05-07T10:37:51Z",
        "summary": "The emergence of large language models has led to the development of powerful\ntools such as ChatGPT that can produce text indistinguishable from\nhuman-generated work. With the increasing accessibility of such technology,\nstudents across the globe may utilize it to help with their school work -- a\npossibility that has sparked discussions on the integrity of student\nevaluations in the age of artificial intelligence (AI). To date, it is unclear\nhow such tools perform compared to students on university-level courses.\nFurther, students' perspectives regarding the use of such tools, and educators'\nperspectives on treating their use as plagiarism, remain unknown. Here, we\ncompare the performance of ChatGPT against students on 32 university-level\ncourses. We also assess the degree to which its use can be detected by two\nclassifiers designed specifically for this purpose. Additionally, we conduct a\nsurvey across five countries, as well as a more in-depth survey at the authors'\ninstitution, to discern students' and educators' perceptions of ChatGPT's use.\nWe find that ChatGPT's performance is comparable, if not superior, to that of\nstudents in many courses. Moreover, current AI-text classifiers cannot reliably\ndetect ChatGPT's use in school work, due to their propensity to classify\nhuman-written answers as AI-generated, as well as the ease with which\nAI-generated text can be edited to evade detection. Finally, we find an\nemerging consensus among students to use the tool, and among educators to treat\nthis as plagiarism. Our findings offer insights that could guide policy\ndiscussions addressing the integration of AI into educational frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2305.13934v1.pdf"
    },
    {
        "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
        "authors": [
            "Feilong Chen",
            "Minglun Han",
            "Haozhi Zhao",
            "Qingyang Zhang",
            "Jing Shi",
            "Shuang Xu",
            "Bo Xu"
        ],
        "published": "2023-05-07T02:25:42Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable language abilities.\nGPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities\nbeyond previous visual language models. We attribute this to the use of more\nadvanced LLMs compared with previous multimodal models. Unfortunately, the\nmodel architecture and training strategies of GPT-4 are unknown. To endow LLMs\nwith multimodal capabilities, we propose X-LLM, which converts Multi-modalities\n(images, speech, videos) into foreign languages using X2L interfaces and inputs\nthem into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple\nfrozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X''\ndenotes multi-modalities such as image, speech, and videos, and ``L'' denotes\nlanguages. X-LLM's training consists of three stages: (1) Converting Multimodal\nInformation: The first stage trains each X2L interface to align with its\nrespective single-modal encoder separately to convert multimodal information\ninto languages. (2) Aligning X2L representations with the LLM: single-modal\nencoders are aligned with the LLM through X2L interfaces independently. (3)\nIntegrating multiple modalities: all single-modal encoders are aligned with the\nLLM through X2L interfaces to integrate multimodal capabilities into the LLM.\nOur experiments show that X-LLM demonstrates impressive multimodel chat\nabilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen\nimages/instructions, and yields a 84.5\\% relative score compared with GPT-4 on\na synthetic multimodal instruction-following dataset. And we also conduct\nquantitative tests on using LLM for ASR and multimodal ASR, hoping to promote\nthe era of LLM-based speech recognition.",
        "pdf_link": "https://arxiv.org/pdf/2305.04160v3.pdf"
    },
    {
        "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
        "authors": [
            "Maximillian Chen",
            "Xiao Yu",
            "Weiyan Shi",
            "Urvi Awasthi",
            "Zhou Yu"
        ],
        "published": "2023-05-06T23:11:25Z",
        "summary": "Mixed-initiative dialogue tasks involve repeated exchanges of information and\nconversational control. Conversational agents gain control by generating\nresponses that follow particular dialogue intents or strategies, prescribed by\na policy planner. The standard approach has been fine-tuning pre-trained\nlanguage models to perform generation conditioned on these intents. However,\nthese supervised generation models are limited by the cost and quality of data\nannotation. We instead prompt large language models as a drop-in replacement to\nfine-tuning on conditional generation. We formalize prompt construction for\ncontrollable mixed-initiative dialogue. Our findings show improvements over\nfine-tuning and ground truth responses according to human evaluation and\nautomatic metrics for two tasks: PersuasionForGood and Emotional Support\nConversations.",
        "pdf_link": "https://arxiv.org/pdf/2305.04147v1.pdf"
    },
    {
        "title": "Artificial Neuropsychology: Are Large Language Models Developing Executive Functions?",
        "authors": [
            "Hernan Ceferino Vazquez"
        ],
        "published": "2023-05-06T20:53:22Z",
        "summary": "Artificial Intelligence (AI) has been rapidly advancing and has demonstrated\nits ability to perform a wide range of cognitive tasks, including language\nprocessing, visual recognition, and decision-making. Part of this progress is\ndue to LLMs (Large Language Models) like those of the GPT (Generative\nPre-Trained Transformers) family. These models are capable of exhibiting\nbehavior that can be perceived as intelligent. Most authors in Neuropsychology\nconsider intelligent behavior to depend on a number of overarching skills, or\nExecutive Functions (EFs), which rely on the correct functioning of neural\nnetworks in the frontal lobes, and have developed a series of tests to evaluate\nthem. In this work, we raise the question of whether LLMs are developing\nexecutive functions similar to those of humans as part of their learning, and\nwe evaluate the planning function and working memory of GPT using the popular\nTowers of Hanoi method. Additionally, we introduce a new variant of the\nclassical method in order to avoid that the solutions are found in the LLM\ntraining data (dataleakeage). Preliminary results show that LLMs generates\nnear-optimal solutions in Towers of Hanoi related tasks, adheres to task\nconstraints, and exhibits rapid planning capabilities and efficient working\nmemory usage, indicating a potential development of executive functions.\nHowever, these abilities are quite limited and worse than well-trained humans\nwhen the tasks are not known and are not part of the training data.",
        "pdf_link": "https://arxiv.org/pdf/2305.04134v2.pdf"
    },
    {
        "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
        "authors": [
            "Lei Wang",
            "Wanyu Xu",
            "Yihuai Lan",
            "Zhiqiang Hu",
            "Yunshi Lan",
            "Roy Ka-Wei Lee",
            "Ee-Peng Lim"
        ],
        "published": "2023-05-06T16:34:37Z",
        "summary": "Large language models (LLMs) have recently been shown to deliver impressive\nperformance in various NLP tasks. To tackle multi-step reasoning tasks,\nfew-shot chain-of-thought (CoT) prompting includes a few manually crafted\nstep-by-step reasoning demonstrations which enable LLMs to explicitly generate\nreasoning steps and improve their reasoning task accuracy. To eliminate the\nmanual effort, Zero-shot-CoT concatenates the target problem statement with\n\"Let's think step by step\" as an input prompt to LLMs. Despite the success of\nZero-shot-CoT, it still suffers from three pitfalls: calculation errors,\nmissing-step errors, and semantic misunderstanding errors. To address the\nmissing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of\ntwo components: first, devising a plan to divide the entire task into smaller\nsubtasks, and then carrying out the subtasks according to the plan. To address\nthe calculation errors and improve the quality of generated reasoning steps, we\nextend PS prompting with more detailed instructions and derive PS+ prompting.\nWe evaluate our proposed prompting strategy on ten datasets across three\nreasoning problems. The experimental results over GPT-3 show that our proposed\nzero-shot prompting consistently outperforms Zero-shot-CoT across all datasets\nby a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought\nPrompting, and has comparable performance with 8-shot CoT prompting on the math\nreasoning problem. The code can be found at\nhttps://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
        "pdf_link": "https://arxiv.org/pdf/2305.04091v3.pdf"
    },
    {
        "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
        "authors": [
            "Kechi Zhang",
            "Zhuo Li",
            "Jia Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "published": "2023-05-06T16:12:19Z",
        "summary": "Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2305.04087v5.pdf"
    },
    {
        "title": "Pre-training Language Model as a Multi-perspective Course Learner",
        "authors": [
            "Beiduo Chen",
            "Shaohan Huang",
            "Zihan Zhang",
            "Wu Guo",
            "Zhenhua Ling",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Qi Zhang"
        ],
        "published": "2023-05-06T09:02:10Z",
        "summary": "ELECTRA, the generator-discriminator pre-training framework, has achieved\nimpressive semantic construction capability among various downstream tasks.\nDespite the convincing performance, ELECTRA still faces the challenges of\nmonotonous training and deficient interaction. Generator with only masked\nlanguage modeling (MLM) leads to biased learning and label imbalance for\ndiscriminator, decreasing learning efficiency; no explicit feedback loop from\ndiscriminator to generator results in the chasm between these two components,\nunderutilizing the course learning. In this study, a multi-perspective course\nlearning (MCL) method is proposed to fetch a many degrees and visual angles for\nsample-efficient pre-training, and to fully leverage the relationship between\ngenerator and discriminator. Concretely, three self-supervision courses are\ndesigned to alleviate inherent flaws of MLM and balance the label in a\nmulti-perspective way. Besides, two self-correction courses are proposed to\nbridge the chasm between the two encoders by creating a \"correction notebook\"\nfor secondary-supervision. Moreover, a course soups trial is conducted to solve\nthe \"tug-of-war\" dynamics problem of MCL, evolving a stronger pre-trained\nmodel. Experimental results show that our method significantly improves\nELECTRA's average performance by 2.8% and 3.2% absolute points respectively on\nGLUE and SQuAD 2.0 benchmarks, and overshadows recent advanced ELECTRA-style\nmodels under the same settings. The pre-trained MCL model is available at\nhttps://huggingface.co/McmanusChen/MCL-base.",
        "pdf_link": "https://arxiv.org/pdf/2305.03981v1.pdf"
    },
    {
        "title": "Large Language Models in Sport Science & Medicine: Opportunities, Risks and Considerations",
        "authors": [
            "Mark Connor",
            "Michael O'Neill"
        ],
        "published": "2023-05-05T21:20:02Z",
        "summary": "This paper explores the potential opportunities, risks, and challenges\nassociated with the use of large language models (LLMs) in sports science and\nmedicine. LLMs are large neural networks with transformer style architectures\ntrained on vast amounts of textual data, and typically refined with human\nfeedback. LLMs can perform a large range of natural language processing tasks.\nIn sports science and medicine, LLMs have the potential to support and augment\nthe knowledge of sports medicine practitioners, make recommendations for\npersonalised training programs, and potentially distribute high-quality\ninformation to practitioners in developing countries. However, there are also\npotential risks associated with the use and development of LLMs, including\nbiases in the dataset used to create the model, the risk of exposing\nconfidential data, the risk of generating harmful output, and the need to align\nthese models with human preferences through feedback. Further research is\nneeded to fully understand the potential applications of LLMs in sports science\nand medicine and to ensure that their use is ethical and beneficial to\nathletes, clients, patients, practitioners, and the general public.",
        "pdf_link": "https://arxiv.org/pdf/2305.03851v1.pdf"
    },
    {
        "title": "On Contrastive Learning of Semantic Similarity forCode to Code Search",
        "authors": [
            "Anthony Saieva",
            "Saikat Chakraborty",
            "Gail Kaiser"
        ],
        "published": "2023-05-05T20:46:56Z",
        "summary": "This paper introduces a novel code-to-code search technique that enhances the\nperformance of Large Language Models (LLMs) by including both static and\ndynamic features as well as utilizing both similar and dissimilar examples\nduring training. We present the first-ever code search method that encodes\ndynamic runtime information during training without the need to execute either\nthe corpus under search or the search query at inference time and the first\ncode search technique that trains on both positive and negative reference\nsamples. To validate the efficacy of our approach, we perform a set of studies\ndemonstrating the capability of enhanced LLMs to perform cross-language\ncode-to-code search.\n  Our evaluation demonstrates that the effectiveness of our approach is\nconsistent across various model architectures and programming languages. We\noutperform the state-of-the-art cross-language search tool by up to 44.7\\%.\nMoreover, our ablation studies reveal that even a single positive and negative\nreference sample in the training process results in substantial performance\nimprovements demonstrating both similar and dissimilar references are important\nparts of code search. Importantly, we show that enhanced well-crafted,\nfine-tuned models consistently outperform enhanced larger modern LLMs without\nfine tuning, even when enhancing the largest available LLMs highlighting the\nimportance for open-sourced models.\n  To ensure the reproducibility and extensibility of our research, we present\nan open-sourced implementation of our tool and training procedures called\nCosco.",
        "pdf_link": "https://arxiv.org/pdf/2305.03843v1.pdf"
    },
    {
        "title": "Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation",
        "authors": [
            "Nilaksh Das",
            "Monica Sunkara",
            "Sravan Bodapati",
            "Jinglun Cai",
            "Devang Kulshreshtha",
            "Jeff Farris",
            "Katrin Kirchhoff"
        ],
        "published": "2023-05-05T20:35:42Z",
        "summary": "End-to-end ASR models trained on large amount of data tend to be implicitly\nbiased towards language semantics of the training data. Internal language model\nestimation (ILME) has been proposed to mitigate this bias for autoregressive\nmodels such as attention-based encoder-decoder and RNN-T. Typically, ILME is\nperformed by modularizing the acoustic and language components of the model\narchitecture, and eliminating the acoustic input to perform log-linear\ninterpolation with the text-only posterior. However, for CTC-based ASR, it is\nnot as straightforward to decouple the model into such acoustic and language\ncomponents, as CTC log-posteriors are computed in a non-autoregressive manner.\nIn this work, we propose a novel ILME technique for CTC-based ASR models. Our\nmethod iteratively masks the audio timesteps to estimate a pseudo\nlog-likelihood of the internal LM by accumulating log-posteriors for only the\nmasked timesteps. Extensive evaluation across multiple out-of-domain datasets\nreveals that the proposed approach improves WER by up to 9.8% and OOV F1-score\nby up to 24.6% relative to Shallow Fusion, when only text data from target\ndomain is available. In the case of zero-shot domain adaptation, with no access\nto any target domain data, we demonstrate that removing the source domain bias\nwith ILME can still outperform Shallow Fusion to improve WER by up to 9.3%\nrelative.",
        "pdf_link": "https://arxiv.org/pdf/2305.03837v1.pdf"
    },
    {
        "title": "Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios",
        "authors": [
            "Hazal Türkmen",
            "Oğuz Dikenelli",
            "Cenk Eraslan",
            "Mehmet Cem Çallı",
            "Süha Süreyya Özbek"
        ],
        "published": "2023-05-05T18:39:07Z",
        "summary": "In recent years, major advancements in natural language processing (NLP) have\nbeen driven by the emergence of large language models (LLMs), which have\nsignificantly revolutionized research and development within the field.\nBuilding upon this progress, our study delves into the effects of various\npre-training methodologies on Turkish clinical language models' performance in\na multi-label classification task involving radiology reports, with a focus on\naddressing the challenges posed by limited language resources. Additionally, we\nevaluated the simultaneous pretraining approach by utilizing limited clinical\ntask data for the first time. We developed four models, including\nTurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and\nTurkRadBERT-sim v2. Our findings indicate that the general Turkish BERT model\n(BERTurk) and TurkRadBERT-task v1, both of which utilize knowledge from a\nsubstantial general-domain corpus, demonstrate the best overall performance.\nAlthough the task-adaptive pre-training approach has the potential to capture\ndomain-specific patterns, it is constrained by the limited task-specific corpus\nand may be susceptible to overfitting. Furthermore, our results underscore the\nsignificance of domain-specific vocabulary during pre-training for enhancing\nmodel performance. Ultimately, we observe that the combination of\ngeneral-domain knowledge and task-specific fine-tuning is essential for\nachieving optimal performance across a range of categories. This study offers\nvaluable insights for developing effective Turkish clinical language models and\ncan guide future research on pre-training techniques for other low-resource\nlanguages within the clinical domain.",
        "pdf_link": "https://arxiv.org/pdf/2305.03788v1.pdf"
    },
    {
        "title": "Large Language Models in Ambulatory Devices for Home Health Diagnostics: A case study of Sickle Cell Anemia Management",
        "authors": [
            "Oluwatosin Ogundare",
            "Subuola Sofolahan"
        ],
        "published": "2023-05-05T17:55:49Z",
        "summary": "This study investigates the potential of an ambulatory device that\nincorporates Large Language Models (LLMs) in cadence with other specialized ML\nmodels to assess anemia severity in sickle cell patients in real time. The\ndevice would rely on sensor data that measures angiogenic material levels to\nassess anemia severity, providing real-time information to patients and\nclinicians to reduce the frequency of vaso-occlusive crises because of the\nearly detection of anemia severity, allowing for timely interventions and\npotentially reducing the likelihood of serious complications. The main\nchallenges in developing such a device are the creation of a reliable\nnon-invasive tool for angiogenic level assessment, a biophysics model and the\npractical consideration of an LLM communicating with emergency personnel on\nbehalf of an incapacitated patient. A possible system is proposed, and the\nlimitations of this approach are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2305.03715v1.pdf"
    },
    {
        "title": "LMEye: An Interactive Perception Network for Large Language Models",
        "authors": [
            "Yunxin Li",
            "Baotian Hu",
            "Xinyu Chen",
            "Lin Ma",
            "Yong Xu",
            "Min Zhang"
        ],
        "published": "2023-05-05T17:27:21Z",
        "summary": "Training a Multimodal Large Language Model (MLLM) from scratch, like GPT-4,\nis resource-intensive. Regarding Large Language Models (LLMs) as the core\nprocessor for multimodal information, our paper introduces LMEye, a human-like\neye with a play-and-plug interactive perception network, designed to enable\ndynamic interaction between LLMs and external vision information. Previous\nmethods incorporate visual information into LLMs with a simple visual mapping\nnetwork or Q-former from BLIP-2. Such networks project the image feature once\nyet do not consider the interaction between the image and the human input\nquery. Hence, the obtained visual information without being connected to human\nintention may be inadequate for LLMs to generate intention-following responses,\nwhich we refer to as static visual information. LMEye addresses this issue by\nallowing the LLM to request the desired visual information aligned with various\nhuman instructions, which we term as the dynamic visual information\ninteraction. Specifically, LMEye consists of a simple visual mapping network to\nprovide the basic perception of an image for LLMs. It also contains additional\nmodules responsible for acquiring requests from LLMs, performing request-based\nvisual information interaction, and transmitting the resulting interacted\nvisual information to LLMs, respectively. In this way, LLMs act to understand\nthe human query, deliver the corresponding request to the request-based visual\ninformation interaction module, and generate the response based on the\ninterleaved multimodal information. We evaluate LMEye through extensive\nexperiments on some multimodal benchmarks, demonstrating that it significantly\nimproves the zero-shot performance on various multimodal tasks compared to\nprevious methods, with less parameters.",
        "pdf_link": "https://arxiv.org/pdf/2305.03701v6.pdf"
    },
    {
        "title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
        "authors": [
            "Jiacheng Liu",
            "Wenya Wang",
            "Dianzhuo Wang",
            "Noah A. Smith",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2023-05-05T17:15:32Z",
        "summary": "Despite the much discussed capabilities of today's language models, they are\nstill prone to silly and unexpected commonsense failures. We consider a\nretrospective verification approach that reflects on the correctness of LM\noutputs, and introduce Vera, a general-purpose model that estimates the\nplausibility of declarative statements based on commonsense knowledge. Trained\non ~7M commonsense statements created from 19 QA datasets and two large-scale\nknowledge bases, and with a combination of three training objectives, Vera is a\nversatile model that effectively separates correct from incorrect statements\nacross diverse commonsense domains. When applied to solving commonsense\nproblems in the verification format, Vera substantially outperforms existing\nmodels that can be repurposed for commonsense verification, and it further\nexhibits generalization capabilities to unseen tasks and provides\nwell-calibrated outputs. We find that Vera excels at filtering LM-generated\ncommonsense knowledge and is useful in detecting erroneous commonsense\nstatements generated by models like ChatGPT in real-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.03695v3.pdf"
    },
    {
        "title": "Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs",
        "authors": [
            "Somin Wadhwa",
            "Jay DeYoung",
            "Benjamin Nye",
            "Silvio Amir",
            "Byron C. Wallace"
        ],
        "published": "2023-05-05T16:02:06Z",
        "summary": "Results from Randomized Controlled Trials (RCTs) establish the comparative\neffectiveness of interventions, and are in turn critical inputs for\nevidence-based care. However, results from RCTs are presented in (often\nunstructured) natural language articles describing the design, execution, and\noutcomes of trials; clinicians must manually extract findings pertaining to\ninterventions and outcomes of interest from such articles. This onerous manual\nprocess has motivated work on (semi-)automating extraction of structured\nevidence from trial reports. In this work we propose and evaluate a\ntext-to-text model built on instruction-tuned Large Language Models (LLMs) to\njointly extract Interventions, Outcomes, and Comparators (ICO elements) from\nclinical abstracts, and infer the associated results reported. Manual (expert)\nand automated evaluations indicate that framing evidence extraction as a\nconditional generation task and fine-tuning LLMs for this purpose realizes\nconsiderable ($\\sim$20 point absolute F1 score) gains over the previous SOTA.\nWe perform ablations and error analyses to assess aspects that contribute to\nmodel performance, and to highlight potential directions for further\nimprovements. We apply our model to a collection of published RCTs through\nmid-2022, and release a searchable database of structured findings:\nhttp://ico-relations.ebm-nlp.com",
        "pdf_link": "https://arxiv.org/pdf/2305.03642v3.pdf"
    },
    {
        "title": "Now It Sounds Like You: Learning Personalized Vocabulary On Device",
        "authors": [
            "Sid Wang",
            "Ashish Shenoy",
            "Pierce Chuang",
            "John Nguyen"
        ],
        "published": "2023-05-05T14:44:20Z",
        "summary": "In recent years, Federated Learning (FL) has shown significant advancements\nin its ability to perform various natural language processing (NLP) tasks. This\nwork focuses on applying personalized FL for on-device language modeling. Due\nto limitations of memory and latency, these models cannot support the\ncomplexity of sub-word tokenization or beam search decoding, resulting in the\ndecision to deploy a closed-vocabulary language model. However,\nclosed-vocabulary models are unable to handle out-of-vocabulary (OOV) words\nbelonging to specific users. To address this issue, We propose a novel\ntechnique called \"OOV expansion\" that improves OOV coverage and increases model\naccuracy while minimizing the impact on memory and latency. This method\nintroduces a personalized \"OOV adapter\" that effectively transfers knowledge\nfrom a central model and learns word embedding for personalized vocabulary. OOV\nexpansion significantly outperforms standard FL personalization methods on a\nset of common FL benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2305.03584v3.pdf"
    },
    {
        "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering",
        "authors": [
            "Lei Wang",
            "Yi Hu",
            "Jiabang He",
            "Xing Xu",
            "Ning Liu",
            "Hui Liu",
            "Heng Tao Shen"
        ],
        "published": "2023-05-05T11:56:30Z",
        "summary": "Large Language Models (LLMs) have recently demonstrated exceptional\nperformance in various Natural Language Processing (NLP) tasks. They have also\nshown the ability to perform chain-of-thought (CoT) reasoning to solve complex\nproblems. Recent studies have explored CoT reasoning in complex multimodal\nscenarios, such as the science question answering task, by fine-tuning\nmultimodal models with high-quality human-annotated CoT rationales. However,\ncollecting high-quality COT rationales is usually time-consuming and costly.\nBesides, the annotated rationales are hardly accurate due to the external\nessential information missed. To address these issues, we propose a novel\nmethod termed T-SciQ that aims at teaching science question answering with LLM\nsignals. The T-SciQ approach generates high-quality CoT rationales as teaching\nsignals and is advanced to train much smaller models to perform CoT reasoning\nin complex modalities. Additionally, we introduce a novel data mixing strategy\nto produce more effective teaching data samples for simple and complex science\nquestion answer problems. Extensive experimental results show that our T-SciQ\nmethod achieves a new state-of-the-art performance on the ScienceQA benchmark,\nwith an accuracy of 96.18%. Moreover, our approach outperforms the most\npowerful fine-tuned baseline by 4.5%. The code is publicly available at\nhttps://github.com/T-SciQ/T-SciQ.",
        "pdf_link": "https://arxiv.org/pdf/2305.03453v4.pdf"
    },
    {
        "title": "Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects",
        "authors": [
            "Kehui Tan",
            "Tianqi Pang",
            "Chenyou Fan",
            "Song Yu"
        ],
        "published": "2023-05-05T11:09:13Z",
        "summary": "This perspective paper proposes a series of interactive scenarios that\nutilize Artificial Intelligence (AI) to enhance classroom teaching, such as\ndialogue auto-completion, knowledge and style transfer, and assessment of\nAI-generated content. By leveraging recent developments in Large Language\nModels (LLMs), we explore the potential of AI to augment and enrich\nteacher-student dialogues and improve the quality of teaching. Our goal is to\nproduce innovative and meaningful conversations between teachers and students,\ncreate standards for evaluation, and improve the efficacy of AI-for-Education\ninitiatives. In Section 3, we discuss the challenges of utilizing existing LLMs\nto effectively complete the educated tasks and present a unified framework for\naddressing diverse education dataset, processing lengthy conversations, and\ncondensing information to better accomplish more downstream tasks. In Section\n4, we summarize the pivoting tasks including Teacher-Student Dialogue\nAuto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessment\nof AI-Generated Content (AIGC), providing a clear path for future research. In\nSection 5, we also explore the use of external and adjustable LLMs to improve\nthe generated content through human-in-the-loop supervision and reinforcement\nlearning. Ultimately, this paper seeks to highlight the potential for AI to aid\nthe field of education and promote its further exploration.",
        "pdf_link": "https://arxiv.org/pdf/2305.03433v2.pdf"
    },
    {
        "title": "Using ChatGPT for Entity Matching",
        "authors": [
            "Ralph Peeters",
            "Christian Bizer"
        ],
        "published": "2023-05-05T10:39:32Z",
        "summary": "Entity Matching is the task of deciding if two entity descriptions refer to\nthe same real-world entity. State-of-the-art entity matching methods often rely\non fine-tuning Transformer models such as BERT or RoBERTa. Two major drawbacks\nof using these models for entity matching are that (i) the models require\nsignificant amounts of fine-tuning data for reaching a good performance and\n(ii) the fine-tuned models are not robust concerning out-of-distribution\nentities. In this paper, we investigate using ChatGPT for entity matching as a\nmore robust, training data-efficient alternative to traditional Transformer\nmodels. We perform experiments along three dimensions: (i) general prompt\ndesign, (ii) in-context learning, and (iii) provision of higher-level matching\nknowledge. We show that ChatGPT is competitive with a fine-tuned RoBERTa model,\nreaching a zero-shot performance of 82.35% F1 on a challenging matching task on\nwhich RoBERTa requires 2000 training examples for reaching a similar\nperformance. Adding in-context demonstrations to the prompts further improves\nthe F1 by up to 7.85% when using similarity-based example selection. Always\nusing the same set of 10 handpicked demonstrations leads to an improvement of\n4.92% over the zero-shot performance. Finally, we show that ChatGPT can also be\nguided by adding higher-level matching knowledge in the form of rules to the\nprompts. Providing matching rules leads to similar performance gains as\nproviding in-context demonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2305.03423v2.pdf"
    },
    {
        "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
        "authors": [
            "Hanlin Zhang",
            "Jiani Huang",
            "Ziyang Li",
            "Mayur Naik",
            "Eric Xing"
        ],
        "published": "2023-05-05T07:24:46Z",
        "summary": "Pre-trained large language models (LMs) struggle to perform logical reasoning\nreliably despite advances in scale and compositionality. In this work, we\ntackle this challenge through the lens of symbolic programming. We propose\nDSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs\ngovern the perception of factual knowledge, and a symbolic module performs\ndeductive reasoning. In contrast to works that rely on hand-crafted logic\nrules, our differentiable symbolic reasoning framework efficiently learns\nweighted rules and applies semantic loss to further improve LMs. DSR-LM is\nscalable, interpretable, and allows easy integration of prior knowledge,\nthereby supporting extensive symbolic programming to robustly derive a logical\nconclusion. The results of our experiments suggest that DSR-LM improves the\nlogical reasoning abilities of pre-trained language models, resulting in a\nsignificant increase in accuracy of over 20% on deductive reasoning benchmarks.\nFurthermore, DSR-LM outperforms a variety of competitive baselines when faced\nwith systematic changes in sequence length.",
        "pdf_link": "https://arxiv.org/pdf/2305.03742v1.pdf"
    },
    {
        "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
        "authors": [
            "Ruochen Zhao",
            "Xingxuan Li",
            "Shafiq Joty",
            "Chengwei Qin",
            "Lidong Bing"
        ],
        "published": "2023-05-05T03:49:14Z",
        "summary": "As large language models (LLMs) have become the norm in NLP, demonstrating\ngood performance in generation and reasoning tasks, one of its most fatal\ndisadvantages is the lack of factual correctness. Generating unfactual texts\nnot only leads to lower performances but also degrades the trust and validity\nof their applications. Chain-of-Thought (CoT) prompting improves trust and\nmodel performance on complex reasoning tasks by generating interpretable\nreasoning chains, but still suffers from factuality concerns in\nknowledge-intensive tasks. In this paper, we propose the Verify-and-Edit\nframework for CoT prompting, which seeks to increase prediction factuality by\npost-editing reasoning chains according to external knowledge. Building on top\nof GPT-3, our framework lead to accuracy improvements in multiple open-domain\nquestion-answering tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.03268v1.pdf"
    },
    {
        "title": "VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna",
        "authors": [
            "Bin Ji"
        ],
        "published": "2023-05-05T02:46:22Z",
        "summary": "Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and\nfew-shot capabilities in Named Entity Recognition (NER). However, these models\ncan only be accessed via online APIs, which may cause data leak and\nnon-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot\nNER framework based on the newly released open-source LLM -- Vicuna. VicunaNER\nis a two-phase framework, where each phase leverages multi-turn dialogues with\nVicuna to recognize entities from texts. We name the second phase as\nRe-Recognition, which recognizes those entities not recognized in the first\nphase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues\nin each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot\ncapacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.\nExperimental results demonstrate that VicunaNER achieves superior performance\nin both shot settings. Additionally, we conduct comprehensive investigations on\nVicuna from multiple perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2305.03253v1.pdf"
    },
    {
        "title": "LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics",
        "authors": [
            "Shervin Ardeshir"
        ],
        "published": "2023-05-04T23:54:37Z",
        "summary": "Trained on a vast amount of data, Large Language models (LLMs) have achieved\nunprecedented success and generalization in modeling fairly complex textual\ninputs in the abstract space, making them powerful tools for zero-shot\nlearning. Such capability is extended to other modalities such as the visual\ndomain using cross-modal foundation models such as CLIP, and as a result,\nsemantically meaningful representation are extractable from visual inputs.\n  In this work, we leverage this capability and propose an approach that can\nprovide semantic insights into a model's patterns of failures and biases. Given\na black box model, its training data, and task definition, we first calculate\nits task-related loss for each data point. We then extract a semantically\nmeaningful representation for each training data point (such as CLIP embeddings\nfrom its visual encoder) and train a lightweight diagnosis model which maps\nthis semantically meaningful representation of a data point to its task loss.\nWe show that an ensemble of such lightweight models can be used to generate\ninsights on the performance of the black-box model, in terms of identifying its\npatterns of failures and biases.",
        "pdf_link": "https://arxiv.org/pdf/2305.03212v2.pdf"
    },
    {
        "title": "Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing",
        "authors": [
            "Jawid Ahmad Baktash",
            "Mursal Dawodi"
        ],
        "published": "2023-05-04T22:46:43Z",
        "summary": "Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation\nlanguage model in the GPT series, developed by OpenAI, which promises\nsignificant advancements in the field of natural language processing (NLP). In\nthis research article, we have discussed the features of GPT-4, its potential\napplications, and the challenges that it might face. We have also compared\nGPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one\ntrillion), better multilingual capabilities, improved contextual understanding,\nand reasoning capabilities than GPT-3. Some of the potential applications of\nGPT-4 include chatbots, personal assistants, language translation, text\nsummarization, and question-answering. However, GPT-4 poses several challenges\nand limitations such as computational requirements, data requirements, and\nethical concerns.",
        "pdf_link": "https://arxiv.org/pdf/2305.03195v1.pdf"
    },
    {
        "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
        "authors": [
            "Jinyang Li",
            "Binyuan Hui",
            "Ge Qu",
            "Jiaxi Yang",
            "Binhua Li",
            "Bowen Li",
            "Bailin Wang",
            "Bowen Qin",
            "Rongyu Cao",
            "Ruiying Geng",
            "Nan Huo",
            "Xuanhe Zhou",
            "Chenhao Ma",
            "Guoliang Li",
            "Kevin C. C. Chang",
            "Fei Huang",
            "Reynold Cheng",
            "Yongbin Li"
        ],
        "published": "2023-05-04T19:02:29Z",
        "summary": "Text-to-SQL parsing, which aims at converting natural language instructions\ninto executable SQLs, has gained increasing attention in recent years. In\nparticular, Codex and ChatGPT have shown impressive results in this task.\nHowever, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on\ndatabase schema with few rows of database contents leaving the gap between\nacademic study and real-world applications. To mitigate this gap, we present\nBird, a big benchmark for large-scale database grounded in text-to-SQL tasks,\ncontaining 12,751 pairs of text-to-SQL data and 95 databases with a total size\nof 33.4 GB, spanning 37 professional domains. Our emphasis on database values\nhighlights the new challenges of dirty database contents, external knowledge\nbetween NL questions and database contents, and SQL efficiency, particularly in\nthe context of massive databases. To solve these problems, text-to-SQL models\nmust feature database value comprehension in addition to semantic parsing. The\nexperimental results demonstrate the significance of database values in\ngenerating accurate text-to-SQLs for big databases. Furthermore, even the most\neffective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution\naccuracy, which is still far from the human result of 92.96%, proving that\nchallenges still stand. Besides, we also provide an efficiency analysis to\noffer insights into generating text-to-efficient-SQLs that are beneficial to\nindustries. We believe that BIRD will contribute to advancing real-world\napplications of text-to-SQL research. The leaderboard and source code are\navailable: https://bird-bench.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2305.03111v3.pdf"
    },
    {
        "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
        "authors": [
            "Zhiqing Sun",
            "Yikang Shen",
            "Qinhong Zhou",
            "Hongxin Zhang",
            "Zhenfang Chen",
            "David Cox",
            "Yiming Yang",
            "Chuang Gan"
        ],
        "published": "2023-05-04T17:59:28Z",
        "summary": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised\nfine-tuning (SFT) with human annotations and reinforcement learning from human\nfeedback (RLHF) to align the output of large language models (LLMs) with human\nintentions, ensuring they are helpful, ethical, and reliable. However, this\ndependence can significantly constrain the true potential of AI-assistant\nagents due to the high cost of obtaining human supervision and the related\nissues on quality, reliability, diversity, self-consistency, and undesirable\nbiases. To address these challenges, we propose a novel approach called\nSELF-ALIGN, which combines principle-driven reasoning and the generative power\nof LLMs for the self-alignment of AI agents with minimal human supervision. Our\napproach encompasses four stages: first, we use an LLM to generate synthetic\nprompts, and a topic-guided method to augment the prompt diversity; second, we\nuse a small set of human-written principles for AI models to follow, and guide\nthe LLM through in-context learning from demonstrations (of principles\napplication) to produce helpful, ethical, and reliable responses to user's\nqueries; third, we fine-tune the original LLM with the high-quality\nself-aligned responses so that the resulting model can generate desirable\nresponses for each query directly without the principle set and the\ndemonstrations anymore; and finally, we offer a refinement step to address the\nissues of overly-brief or indirect responses. Applying SELF-ALIGN to the\nLLaMA-65b base language model, we develop an AI assistant named Dromedary. With\nfewer than 300 lines of human annotations (including < 200 seed prompts, 16\ngeneric principles, and 5 exemplars for in-context learning). Dromedary\nsignificantly surpasses the performance of several state-of-the-art AI systems,\nincluding Text-Davinci-003 and Alpaca, on benchmark datasets with various\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2305.03047v2.pdf"
    },
    {
        "title": "Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study",
        "authors": [
            "Sajjad Rahmani",
            "AmirHossein Naghshzan",
            "Latifa Guerrouj"
        ],
        "published": "2023-05-04T17:43:19Z",
        "summary": "Our research investigates the recommendation of code examples to aid software\ndevelopers, a practice that saves developers significant time by providing\nready-to-use code snippets. The focus of our study is Stack Overflow, a\ncommonly used resource for coding discussions and solutions, particularly in\nthe context of the Java programming language. We applied BERT, a powerful Large\nLanguage Model (LLM) that enables us to transform code examples into numerical\nvectors by extracting their semantic information. Once these numerical\nrepresentations are prepared, we identify Approximate Nearest Neighbors (ANN)\nusing Locality-Sensitive Hashing (LSH). Our research employed two variants of\nLSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously compared\nthese two approaches across four parameters: HitRate, Mean Reciprocal Rank\n(MRR), Average Execution Time, and Relevance. Our study revealed that the\nQuery-Aware (QA) approach showed superior performance over the Random\nHyperplane-based (RH) method. Specifically, it exhibited a notable improvement\nof 20\\% to 35\\% in HitRate for query pairs compared to the RH approach.\nFurthermore, the QA approach proved significantly more time-efficient, with its\nspeed in creating hashing tables and assigning data samples to buckets being at\nleast four times faster. It can return code examples within milliseconds,\nwhereas the RH approach typically requires several seconds to recommend code\nexamples. Due to the superior performance of the QA approach, we tested it\nagainst PostFinder and FaCoY, the state-of-the-art baselines. Our QA method\nshowed comparable efficiency proving its potential for effective code\nrecommendation.",
        "pdf_link": "https://arxiv.org/pdf/2305.03017v4.pdf"
    },
    {
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence",
        "authors": [
            "Haoran Li",
            "Mingshi Xu",
            "Yangqiu Song"
        ],
        "published": "2023-05-04T17:31:41Z",
        "summary": "Sentence-level representations are beneficial for various natural language\nprocessing tasks. It is commonly believed that vector representations can\ncapture rich linguistic properties. Currently, large language models (LMs)\nachieve state-of-the-art performance on sentence embedding. However, some\nrecent works suggest that vector representations from LMs can cause information\nleakage. In this work, we further investigate the information leakage issue and\npropose a generative embedding inversion attack (GEIA) that aims to reconstruct\ninput sequences based only on their sentence embeddings. Given the black-box\naccess to a language model, we treat sentence embeddings as initial tokens'\nrepresentations and train or fine-tune a powerful decoder model to decode the\nwhole sequences directly. We conduct extensive experiments to demonstrate that\nour generative inversion attack outperforms previous embedding inversion\nattacks in classification metrics and generates coherent and contextually\nsimilar sentences as the original inputs.",
        "pdf_link": "https://arxiv.org/pdf/2305.03010v1.pdf"
    },
    {
        "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
        "authors": [
            "Reid Pryzant",
            "Dan Iter",
            "Jerry Li",
            "Yin Tat Lee",
            "Chenguang Zhu",
            "Michael Zeng"
        ],
        "published": "2023-05-04T15:15:22Z",
        "summary": "Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language \"gradients\" that criticize the\ncurrent prompt. The gradients are then \"propagated\" into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt's performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.",
        "pdf_link": "https://arxiv.org/pdf/2305.03495v2.pdf"
    },
    {
        "title": "An automatically discovered chain-of-thought prompt generalizes to novel models and datasets",
        "authors": [
            "Konstantin Hebenstreit",
            "Robert Praas",
            "Louis P Kiesewetter",
            "Matthias Samwald"
        ],
        "published": "2023-05-04T15:07:20Z",
        "summary": "Emergent chain-of-thought (CoT) reasoning capabilities promise to improve\nperformance and explainability of large language models (LLMs). However,\nuncertainties remain about how reasoning strategies formulated for previous\nmodel generations generalize to new model generations and different datasets.\nIn this small-scale study, we compare different reasoning strategies induced by\nzero-shot prompting across six recently released LLMs (davinci-002,\ndavinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on a\nmixture of six question-answering datasets, including datasets from scientific\nand medical domains. Our findings demonstrate that while some variations in\neffectiveness occur, gains from CoT reasoning strategies remain robust across\ndifferent models and datasets. GPT-4 has the most benefit from current\nstate-of-the-art reasoning strategies and exhibits the best performance by\napplying a prompt previously discovered through automated discovery.",
        "pdf_link": "https://arxiv.org/pdf/2305.02897v2.pdf"
    },
    {
        "title": "\"Oops, Did I Just Say That?\" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process",
        "authors": [
            "Pingchuan Ma",
            "Zongjie Li",
            "Ao Sun",
            "Shuai Wang"
        ],
        "published": "2023-05-04T08:00:32Z",
        "summary": "As the popularity of large language models (LLMs) soars across various\napplications, ensuring their alignment with human values has become a paramount\nconcern. In particular, given that LLMs have great potential to serve as\ngeneral-purpose AI assistants in daily life, their subtly unethical suggestions\nbecome a serious and real concern. Tackling the challenge of automatically\ntesting and repairing unethical suggestions is thus demanding.\n  This paper introduces the first framework for testing and repairing unethical\nsuggestions made by LLMs. We first propose ETHICSSUITE, a test suite that\npresents complex, contextualized, and realistic moral scenarios to test LLMs.\nWe then propose a novel suggest-critic-reflect (SCR) process, serving as an\nautomated test oracle to detect unethical suggestions. We recast deciding if\nLLMs yield unethical suggestions (a hard problem; often requiring human\nexpertise and costly to decide) into a PCR task that can be automatically\nchecked for violation. Moreover, we propose a novel on-the-fly (OTF) repairing\nscheme that repairs unethical suggestions made by LLMs in real-time. The OTF\nscheme is applicable to LLMs in a black-box API setting with moderate cost.\nWith ETHICSSUITE, our study on seven popular LLMs (e.g., ChatGPT, GPT-4)\nuncovers in total 109,824 unethical suggestions. We apply our OTF scheme on two\nLLMs (Llama-13B and ChatGPT), which generates valid repair to a considerable\namount of unethical ones, paving the way for more ethically conscious LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.02626v1.pdf"
    },
    {
        "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
        "authors": [
            "Hang Chen",
            "Jing Luo",
            "Xinyu Yang",
            "Wenjing Zhu"
        ],
        "published": "2023-05-04T07:45:49Z",
        "summary": "Our investigation into the Affective Reasoning in Conversation (ARC) task\nhighlights the challenge of causal discrimination. Almost all existing models,\nincluding large language models (LLMs), excel at capturing semantic\ncorrelations within utterance embeddings but fall short in determining the\nspecific causal relationships. To overcome this limitation, we propose the\nincorporation of \\textit{i.i.d.} noise terms into the conversation process,\nthereby constructing a structural causal model (SCM). It explores how distinct\ncausal relationships of fitted embeddings can be discerned through independent\nconditions. To facilitate the implementation of deep learning, we introduce the\ncogn frameworks to handle unstructured conversation data, and employ an\nautoencoder architecture to regard the unobservable noise as learnable\n\"implicit causes.\" Moreover, we curate a synthetic dataset that includes i.i.d.\nnoise. Through comprehensive experiments, we validate the effectiveness and\ninterpretability of our approach. Our code is available in\nhttps://github.com/Zodiark-ch/mater-of-our-EMNLP2023-paper.",
        "pdf_link": "https://arxiv.org/pdf/2305.02615v2.pdf"
    },
    {
        "title": "Faithful Question Answering with Monte-Carlo Planning",
        "authors": [
            "Ruixin Hong",
            "Hongming Zhang",
            "Hong Zhao",
            "Dong Yu",
            "Changshui Zhang"
        ],
        "published": "2023-05-04T05:21:36Z",
        "summary": "Although large language models demonstrate remarkable question-answering\nperformances, revealing the intermediate reasoning steps that the models\nfaithfully follow remains challenging. In this paper, we propose FAME (FAithful\nquestion answering with MontE-carlo planning) to answer questions based on\nfaithful reasoning steps. The reasoning steps are organized as a structured\nentailment tree, which shows how premises are used to produce intermediate\nconclusions that can prove the correctness of the answer. We formulate the task\nas a discrete decision-making problem and solve it through the interaction of a\nreasoning environment and a controller. The environment is modular and contains\nseveral basic task-oriented modules, while the controller proposes actions to\nassemble the modules. Since the search space could be large, we introduce a\nMonte-Carlo planning algorithm to do a look-ahead search and select actions\nthat will eventually lead to high-quality steps. FAME achieves state-of-the-art\nperformance on the standard benchmark. It can produce valid and faithful\nreasoning steps compared with large language models with a much smaller model\nsize.",
        "pdf_link": "https://arxiv.org/pdf/2305.02556v1.pdf"
    },
    {
        "title": "Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era",
        "authors": [
            "Dong Zhang"
        ],
        "published": "2023-05-04T05:21:09Z",
        "summary": "With various AI tools such as ChatGPT becoming increasingly popular, we are\nentering a true AI era. We can foresee that exceptional AI tools will soon reap\nconsiderable profits. A crucial question arise: should AI tools share revenue\nwith their training data providers in additional to traditional stakeholders\nand shareholders? The answer is Yes. Large AI tools, such as large language\nmodels, always require more and better quality data to continuously improve,\nbut current copyright laws limit their access to various types of data. Sharing\nrevenue between AI tools and their data providers could transform the current\nhostile zero-sum game relationship between AI tools and a majority of\ncopyrighted data owners into a collaborative and mutually beneficial one, which\nis necessary to facilitate the development of a virtuous cycle among AI tools,\ntheir users and data providers that drives forward AI technology and builds a\nhealthy AI ecosystem. However, current revenue-sharing business models do not\nwork for AI tools in the forthcoming AI era, since the most widely used metrics\nfor website-based traffic and action, such as clicks, will be replaced by new\nmetrics such as prompts and cost per prompt for generative AI tools. A\ncompletely new revenue-sharing business model, which must be almost independent\nof AI tools and be easily explained to data providers, needs to establish a\nprompt-based scoring system to measure data engagement of each data provider.\nThis paper systematically discusses how to build such a scoring system for all\ndata providers for AI tools based on classification and content similarity\nmodels, and outlines the requirements for AI tools or third parties to build\nit. Sharing revenue with data providers using such a scoring system would\nencourage more data owners to participate in the revenue-sharing program. This\nwill be a utilitarian AI era where all parties benefit.",
        "pdf_link": "https://arxiv.org/pdf/2305.02555v2.pdf"
    },
    {
        "title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits",
        "authors": [
            "Hang Jiang",
            "Xiajie Zhang",
            "Xubo Cao",
            "Cynthia Breazeal",
            "Deb Roy",
            "Jad Kabbara"
        ],
        "published": "2023-05-04T04:58:00Z",
        "summary": "Despite the many use cases for large language models (LLMs) in creating\npersonalized chatbots, there has been limited research on evaluating the extent\nto which the behaviors of personalized LLMs accurately and consistently reflect\nspecific personality traits. We consider studying the behavior of LLM-based\nagents which we refer to as LLM personas and present a case study with GPT-3.5\nand GPT-4 to investigate whether LLMs can generate content that aligns with\ntheir assigned personality profiles. To this end, we simulate distinct LLM\npersonas based on the Big Five personality model, have them complete the\n44-item Big Five Inventory (BFI) personality test and a story writing task, and\nthen assess their essays with automatic and human evaluations. Results show\nthat LLM personas' self-reported BFI scores are consistent with their\ndesignated personality types, with large effect sizes observed across five\ntraits. Additionally, LLM personas' writings have emerging representative\nlinguistic patterns for personality traits when compared with a human writing\ncorpus. Furthermore, human evaluation shows that humans can perceive some\npersonality traits with an accuracy of up to 80%. Interestingly, the accuracy\ndrops significantly when the annotators were informed of AI authorship.",
        "pdf_link": "https://arxiv.org/pdf/2305.02547v5.pdf"
    },
    {
        "title": "Can LLMs Capture Human Preferences?",
        "authors": [
            "Ali Goli",
            "Amandeep Singh"
        ],
        "published": "2023-05-04T03:51:31Z",
        "summary": "We explore the viability of Large Language Models (LLMs), specifically\nOpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting\npreferences, with a focus on intertemporal choices. Leveraging the extensive\nliterature on intertemporal discounting for benchmarking, we examine responses\nfrom LLMs across various languages and compare them to human responses,\nexploring preferences between smaller, sooner, and larger, later rewards. Our\nfindings reveal that both GPT models demonstrate less patience than humans,\nwith GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike\nhuman decision-makers. Though GPT-4 does not display lexicographic preferences,\nits measured discount rates are still considerably larger than those found in\nhumans. Interestingly, GPT models show greater patience in languages with weak\nfuture tense references, such as German and Mandarin, aligning with existing\nliterature that suggests a correlation between language structure and\nintertemporal preferences. We demonstrate how prompting GPT to explain its\ndecisions, a procedure we term \"chain-of-thought conjoint,\" can mitigate, but\ndoes not eliminate, discrepancies between LLM and human responses. While\ndirectly eliciting preferences using LLMs may yield misleading results,\ncombining chain-of-thought conjoint with topic modeling aids in hypothesis\ngeneration, enabling researchers to explore the underpinnings of preferences.\nChain-of-thought conjoint provides a structured framework for marketers to use\nLLMs to identify potential attributes or factors that can explain preference\nheterogeneity across different customers and contexts.",
        "pdf_link": "https://arxiv.org/pdf/2305.02531v6.pdf"
    },
    {
        "title": "AutoML-GPT: Automatic Machine Learning with GPT",
        "authors": [
            "Shujian Zhang",
            "Chengyue Gong",
            "Lemeng Wu",
            "Xingchao Liu",
            "Mingyuan Zhou"
        ],
        "published": "2023-05-04T02:09:43Z",
        "summary": "AI tasks encompass a wide range of domains and fields. While numerous AI\nmodels have been designed for specific tasks and applications, they often\nrequire considerable human efforts in finding the right model architecture,\noptimization algorithm, and hyperparameters. Recent advances in large language\nmodels (LLMs) like ChatGPT show remarkable capabilities in various aspects of\nreasoning, comprehension, and interaction. Consequently, we propose developing\ntask-oriented prompts and automatically utilizing LLMs to automate the training\npipeline. To implement this concept, we present the AutoML-GPT, which employs\nGPT as the bridge to diverse AI models and dynamically trains models with\noptimized hyperparameters. AutoML-GPT dynamically takes user requests from the\nmodel and data cards and composes the corresponding prompt paragraph.\nUltimately, with this prompt paragraph, AutoML-GPT will automatically conduct\nthe experiments from data processing to model architecture, hyperparameter\ntuning, and predicted training log. By leveraging {\\ours}'s robust language\ncapabilities and the available AI models, AutoML-GPT can tackle numerous\nintricate AI tasks across various tasks and datasets. This approach achieves\nremarkable results in computer vision, natural language processing, and other\nchallenging areas. Extensive experiments and ablation studies demonstrate that\nour method can be general, effective, and beneficial for many AI tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.02499v1.pdf"
    },
    {
        "title": "Personalized Abstractive Summarization by Tri-agent Generation Pipeline",
        "authors": [
            "Wen Xiao",
            "Yujia Xie",
            "Giuseppe Carenini",
            "Pengcheng He"
        ],
        "published": "2023-05-04T01:12:35Z",
        "summary": "Tailoring outputs from large language models, like ChatGPT, to implicit user\npreferences remains a challenge despite their impressive generative\ncapabilities. In this paper, we propose a tri-agent generation pipeline\ncomprising a generator, an instructor, and an editor to enhance output\npersonalization. The generator produces an initial output, the instructor\nautomatically generates editing instructions based on user preferences, and the\neditor refines the output to align with those preferences. The inference-only\nlarge language model (ChatGPT) serves as both the generator and editor, with a\nsmaller model acting as the instructor to guide output generation. We train the\ninstructor using editor-steered reinforcement learning, leveraging feedback\nfrom a large-scale editor model to optimize instruction generation.\nExperimental results on two abstractive summarization datasets demonstrate the\neffectiveness of our approach in generating outputs that better meet user\nexpectations. Code is available at\n\\url{https://github.com/Wendy-Xiao/chatgpt_editing_summ}",
        "pdf_link": "https://arxiv.org/pdf/2305.02483v2.pdf"
    },
    {
        "title": "Black-box Prompt Tuning with Subspace Learning",
        "authors": [
            "Yuanhang Zheng",
            "Zhixing Tan",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2023-05-04T01:04:25Z",
        "summary": "Black-box prompt tuning uses derivative-free optimization algorithms to learn\nprompts in low-dimensional subspaces instead of back-propagating through the\nnetwork of Large Language Models (LLMs). Recent studies have found that\nblack-box prompt tuning lacks versatility across tasks and LLMs, which we\nbelieve is related to the inappropriate choice of subspaces. In this paper, we\npropose Black-box prompt tuning with Subspace Learning (BSL) to improve the\nversatility of black-box prompt tuning. Based on the assumption that nearly\noptimal prompts for similar tasks exist in a common subspace, we propose\nidentifying such subspaces by meta-learning on a set of similar source tasks.\nTherefore, for a target task that shares similarities with source tasks, we\nguarantee that optimizing in the subspace can find a prompt that performs well\non the target task. Experiments confirm that our BSL framework consistently\nachieves competitive performance regardless of downstream tasks and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.03518v1.pdf"
    },
    {
        "title": "The System Model and the User Model: Exploring AI Dashboard Design",
        "authors": [
            "Fernanda Viégas",
            "Martin Wattenberg"
        ],
        "published": "2023-05-04T00:22:49Z",
        "summary": "This is a speculative essay on interface design and artificial intelligence.\nRecently there has been a surge of attention to chatbots based on large\nlanguage models, including widely reported unsavory interactions. We contend\nthat part of the problem is that text is not all you need: sophisticated AI\nsystems should have dashboards, just like all other complicated devices.\nAssuming the hypothesis that AI systems based on neural networks will contain\ninterpretable models of aspects of the world around them, we discuss what data\nsuch dashboards might display. We conjecture that, for many systems, the two\nmost important models will be of the user and of the system itself. We call\nthese the System Model and User Model. We argue that, for usability and safety,\ninterfaces to dialogue-based AI systems should have a parallel display based on\nthe state of the System Model and the User Model. Finding ways to identify,\ninterpret, and display these two models should be a core part of interface\nresearch for AI.",
        "pdf_link": "https://arxiv.org/pdf/2305.02469v1.pdf"
    },
    {
        "title": "Cheaply Evaluating Inference Efficiency Metrics for Autoregressive Transformer APIs",
        "authors": [
            "Deepak Narayanan",
            "Keshav Santhanam",
            "Peter Henderson",
            "Rishi Bommasani",
            "Tony Lee",
            "Percy Liang"
        ],
        "published": "2023-05-03T21:51:42Z",
        "summary": "Large language models (LLMs) power many state-of-the-art systems in natural\nlanguage processing. However, these models are extremely computationally\nexpensive, even at inference time, raising the natural question: when is the\nextra cost of deploying a larger model worth the anticipated boost in\ncapabilities? Better understanding this tradeoff fundamentally could benefit\nfrom an inference efficiency metric that is both (i) easily comparable across\nmodels from different providers, and (ii) representative of the true cost of\nrunning queries in an isolated performance environment. Unfortunately, access\nto LLMs today is largely restricted to black-box text generation APIs and raw\nruntimes measured through this interface do not satisfy these desiderata: model\nproviders can apply various software and hardware optimizations orthogonal to\nthe model, and models served on shared infrastructure are susceptible to\nperformance contention. To circumvent these problems, we propose a new metric\nfor comparing inference efficiency across models. This metric puts models on\nequal footing as though they were served (i) on uniform hardware and software,\nand (ii) without performance contention. We call this metric the\n\\emph{idealized runtime}, and we propose a methodology to efficiently estimate\nthis metric for autoregressive Transformer models. We also propose cost-aware\nvariants that incorporate the number of accelerators needed to serve the model.\nUsing these metrics, we compare ten state-of-the-art LLMs to provide the first\nanalysis of inference efficiency-capability tradeoffs; we make several\nobservations from this analysis, including the fact that the superior inference\nruntime performance of certain APIs is often a byproduct of optimizations\nwithin the API rather than the underlying model. Our methodology also\nfacilitates the efficient comparison of different software and hardware stacks.",
        "pdf_link": "https://arxiv.org/pdf/2305.02440v1.pdf"
    },
    {
        "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
        "authors": [
            "Xin Cheng",
            "Di Luo",
            "Xiuying Chen",
            "Lemao Liu",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "published": "2023-05-03T21:40:54Z",
        "summary": "With direct access to human-written reference as memory, retrieval-augmented\ngeneration has achieved much progress in a wide range of text generation tasks.\nSince better memory would typically prompt better generation~(we define this as\nprimal problem). The traditional approach for memory retrieval involves\nselecting memory that exhibits the highest similarity to the input. However,\nthis method is constrained by the quality of the fixed corpus from which memory\nis retrieved. In this paper, by exploring the duality of the primal problem:\nbetter generation also prompts better memory, we propose a novel framework,\nselfmem, which addresses this limitation by iteratively employing a\nretrieval-augmented generator to create an unbounded memory pool and using a\nmemory selector to choose one output as memory for the subsequent generation\nround. This enables the model to leverage its own output, referred to as\nself-memory, for improved generation. We evaluate the effectiveness of selfmem\non three distinct text generation tasks: neural machine translation,\nabstractive text summarization, and dialogue generation, under two generation\nparadigms: fine-tuned small model and few-shot LLM. Our approach achieves\nstate-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1),\nand BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in\nenhancing retrieval-augmented generation models. Furthermore, we conduct\nthorough analyses of each component in the selfmem framework to identify\nbottlenecks and provide insights for future research.",
        "pdf_link": "https://arxiv.org/pdf/2305.02437v3.pdf"
    },
    {
        "title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",
        "authors": [
            "Yue Wu",
            "So Yeon Min",
            "Yonatan Bisk",
            "Ruslan Salakhutdinov",
            "Amos Azaria",
            "Yuanzhi Li",
            "Tom Mitchell",
            "Shrimai Prabhumoye"
        ],
        "published": "2023-05-03T20:11:22Z",
        "summary": "Pre-trained large language models (LLMs) capture procedural knowledge about\nthe world. Recent work has leveraged LLM's ability to generate abstract plans\nto simplify challenging control tasks, either by action scoring, or action\nmodeling (fine-tuning). However, the transformer architecture inherits several\nconstraints that make it difficult for the LLM to directly serve as the agent:\ne.g. limited input lengths, fine-tuning inefficiency, bias from pre-training,\nand incompatibility with non-text environments. To maintain compatibility with\na low-level trainable actor, we propose to instead use the knowledge in LLMs to\nsimplify the control problem, rather than solving it. We propose the Plan,\nEliminate, and Track (PET) framework. The Plan module translates a task\ndescription into a list of high-level sub-tasks. The Eliminate module masks out\nirrelevant objects and receptacles from the observation for the current\nsub-task. Finally, the Track module determines whether the agent has\naccomplished each sub-task. On the AlfWorld instruction following benchmark,\nthe PET framework leads to a significant 15% improvement over SOTA for\ngeneralization to human goal specifications.",
        "pdf_link": "https://arxiv.org/pdf/2305.02412v2.pdf"
    },
    {
        "title": "ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs",
        "authors": [
            "Yucheng Shi",
            "Hehuan Ma",
            "Wenliang Zhong",
            "Qiaoyu Tan",
            "Gengchen Mai",
            "Xiang Li",
            "Tianming Liu",
            "Junzhou Huang"
        ],
        "published": "2023-05-03T19:57:43Z",
        "summary": "ChatGPT, as a recently launched large language model (LLM), has shown\nsuperior performance in various natural language processing (NLP) tasks.\nHowever, two major limitations hinder its potential applications: (1) the\ninflexibility of finetuning on downstream tasks and (2) the lack of\ninterpretability in the decision-making process. To tackle these limitations,\nwe propose a novel framework that leverages the power of ChatGPT for specific\ntasks, such as text classification, while improving its interpretability. The\nproposed framework conducts a knowledge graph extraction task to extract\nrefined and structural knowledge from the raw data using ChatGPT. The rich\nknowledge is then converted into a graph, which is further used to train an\ninterpretable linear classifier to make predictions. To evaluate the\neffectiveness of our proposed method, we conduct experiments on four datasets.\nThe result shows that our method can significantly improve the performance\ncompared to directly utilizing ChatGPT for text classification tasks. And our\nmethod provides a more transparent decision-making process compared with\nprevious text classification methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.03513v2.pdf"
    },
    {
        "title": "Entity Tracking in Language Models",
        "authors": [
            "Najoung Kim",
            "Sebastian Schuster"
        ],
        "published": "2023-05-03T18:01:13Z",
        "summary": "Keeping track of how states of entities change as a text or dialog unfolds is\na key prerequisite to discourse understanding. Yet, there have been few\nsystematic investigations into the ability of large language models (LLMs) to\ntrack discourse entities. In this work, we present a task probing to what\nextent a language model can infer the final state of an entity given an English\ndescription of the initial state and a series of state-changing operations. We\nuse this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track\nthe state of entities, and find that only GPT-3.5 models, which have been\npretrained on large amounts of code, exhibit this ability. We then investigate\nwhether smaller models pretrained primarily on text can learn to track\nentities, through finetuning T5 on several training/evaluation splits. While\nperformance degrades for more complex splits, we find that even when evaluated\non a different set of entities from training or longer operation sequences, a\nfinetuned model can perform non-trivial entity tracking. Taken together, these\nresults suggest that language models can learn to track entities but\npretraining on text corpora alone does not make this capacity surface.",
        "pdf_link": "https://arxiv.org/pdf/2305.02363v2.pdf"
    },
    {
        "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
        "authors": [
            "Daniel Rose",
            "Vaishnavi Himakunthala",
            "Andy Ouyang",
            "Ryan He",
            "Alex Mei",
            "Yujie Lu",
            "Michael Saxon",
            "Chinmay Sonar",
            "Diba Mirza",
            "William Yang Wang"
        ],
        "published": "2023-05-03T17:58:29Z",
        "summary": "Recent advances in large language models elicit reasoning in a\nchain-of-thought that allows models to decompose problems in a human-like\nfashion. Though this paradigm improves multi-step reasoning ability in language\nmodels, it is limited by being unimodal and applied mainly to\nquestion-answering tasks. We claim that incorporating visual augmentation into\nreasoning is essential, especially for complex, imaginative tasks.\nConsequently, we introduce VCoT, a novel method that leverages chain-of-thought\nprompting with vision-language grounding to recursively bridge the logical gaps\nwithin sequential data. Our method uses visual guidance to generate synthetic\nmultimodal infillings that add consistent and novel information to reduce the\nlogical gaps for downstream tasks that can benefit from temporal reasoning, as\nwell as provide interpretability into models' multi-step reasoning. We apply\nVCoT to the Visual Storytelling and WikiHow summarization datasets and\ndemonstrate through human evaluation that VCoT offers novel and consistent\nsynthetic data augmentation beating chain-of-thought baselines, which can be\nused to enhance downstream performance.",
        "pdf_link": "https://arxiv.org/pdf/2305.02317v3.pdf"
    },
    {
        "title": "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes",
        "authors": [
            "Cheng-Yu Hsieh",
            "Chun-Liang Li",
            "Chih-Kuan Yeh",
            "Hootan Nakhost",
            "Yasuhisa Fujii",
            "Alexander Ratner",
            "Ranjay Krishna",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published": "2023-05-03T17:50:56Z",
        "summary": "Deploying large language models (LLMs) is challenging because they are memory\ninefficient and compute-intensive for practical applications. In reaction,\nresearchers train smaller task-specific models by either finetuning with human\nlabels or distilling using LLM-generated labels. However, finetuning and\ndistillation require large amounts of training data to achieve comparable\nperformance to LLMs. We introduce Distilling step-by-step, a new mechanism that\n(a) trains smaller models that outperform LLMs, and (b) achieves so by\nleveraging less training data needed by finetuning or distillation. Our method\nextracts LLM rationales as additional supervision for training small models\nwithin a multi-task framework. We present three findings across 4 NLP\nbenchmarks: First, compared to both finetuning and distillation, our mechanism\nachieves better performance with much fewer labeled/unlabeled training\nexamples. Second, compared to few-shot prompted LLMs, we achieve better\nperformance using substantially smaller model sizes. Third, we reduce both the\nmodel size and the amount of data required to outperform LLMs; our finetuned\n770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%\nof available data on a benchmark, whereas standard finetuning the same T5 model\nstruggles to match even by using 100% of the dataset. We release the code at:\nhttps://github.com/google-research/distilling-step-by-step .",
        "pdf_link": "https://arxiv.org/pdf/2305.02301v2.pdf"
    },
    {
        "title": "Evaluating BERT-based Scientific Relation Classifiers for Scholarly Knowledge Graph Construction on Digital Library Collections",
        "authors": [
            "Ming Jiang",
            "Jennifer D'Souza",
            "Sören Auer",
            "J. Stephen Downie"
        ],
        "published": "2023-05-03T17:32:16Z",
        "summary": "The rapid growth of research publications has placed great demands on digital\nlibraries (DL) for advanced information management technologies. To cater to\nthese demands, techniques relying on knowledge-graph structures are being\nadvocated. In such graph-based pipelines, inferring semantic relations between\nrelated scientific concepts is a crucial step. Recently, BERT-based pre-trained\nmodels have been popularly explored for automatic relation classification.\nDespite significant progress, most of them were evaluated in different\nscenarios, which limits their comparability. Furthermore, existing methods are\nprimarily evaluated on clean texts, which ignores the digitization context of\nearly scholarly publications in terms of machine scanning and optical character\nrecognition (OCR). In such cases, the texts may contain OCR noise, in turn\ncreating uncertainty about existing classifiers' performances. To address these\nlimitations, we started by creating OCR-noisy texts based on three clean\ncorpora. Given these parallel corpora, we conducted a thorough empirical\nevaluation of eight Bert-based classification models by focusing on three\nfactors: (1) Bert variants; (2) classification strategies; and, (3) OCR noise\nimpacts. Experiments on clean data show that the domain-specific pre-trained\nBert is the best variant to identify scientific relations. The strategy of\npredicting a single relation each time outperforms the one simultaneously\nidentifying multiple relations in general. The optimal classifier's performance\ncan decline by around 10% to 20% in F-score on the noisy corpora. Insights\ndiscussed in this study can help DL stakeholders select techniques for building\noptimal knowledge-graph-based systems.",
        "pdf_link": "https://arxiv.org/pdf/2305.02291v1.pdf"
    },
    {
        "title": "WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models",
        "authors": [
            "John Giorgi",
            "Augustin Toma",
            "Ronald Xie",
            "Sondra S. Chen",
            "Kevin R. An",
            "Grace X. Zheng",
            "Bo Wang"
        ],
        "published": "2023-05-03T15:58:28Z",
        "summary": "This paper describes our submission to the MEDIQA-Chat 2023 shared task for\nautomatic clinical note generation from doctor-patient conversations. We report\nresults for two approaches: the first fine-tunes a pre-trained language model\n(PLM) on the shared task data, and the second uses few-shot in-context learning\n(ICL) with a large language model (LLM). Both achieve high performance as\nmeasured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and\nfirst, respectively, of all submissions to the shared task. Expert human\nscrutiny indicates that notes generated via the ICL-based approach with GPT-4\nare preferred about as often as human-written notes, making it a promising path\ntoward automated note generation from doctor-patient conversations.",
        "pdf_link": "https://arxiv.org/pdf/2305.02220v2.pdf"
    },
    {
        "title": "Judgments of research co-created by generative AI: experimental evidence",
        "authors": [
            "Paweł Niszczota",
            "Paul Conway"
        ],
        "published": "2023-05-03T15:57:39Z",
        "summary": "The introduction of ChatGPT has fuelled a public debate on the use of\ngenerative AI (large language models; LLMs), including its use by researchers.\nIn the current work, we test whether delegating parts of the research process\nto LLMs leads people to distrust and devalue researchers and scientific output.\nParticipants (N=402) considered a researcher who delegates elements of the\nresearch process to a PhD student or LLM, and rated (1) moral acceptability,\n(2) trust in the scientist to oversee future projects, and (3) the accuracy and\nquality of the output. People judged delegating to an LLM as less acceptable\nthan delegating to a human (d = -0.78). Delegation to an LLM also decreased\ntrust to oversee future research projects (d = -0.80), and people thought the\nresults would be less accurate and of lower quality (d = -0.85). We discuss how\nthis devaluation might transfer into the underreporting of generative AI use.",
        "pdf_link": "https://arxiv.org/pdf/2305.11873v1.pdf"
    },
    {
        "title": "GPT-RE: In-context Learning for Relation Extraction using Large Language Models",
        "authors": [
            "Zhen Wan",
            "Fei Cheng",
            "Zhuoyuan Mao",
            "Qianying Liu",
            "Haiyue Song",
            "Jiwei Li",
            "Sadao Kurohashi"
        ],
        "published": "2023-05-03T13:28:08Z",
        "summary": "In spite of the potential for ground-breaking achievements offered by large\nlanguage models (LLMs) (e.g., GPT-3), they still lag significantly behind\nfully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE).\nThis is due to the two major shortcomings of LLMs in RE: (1) low relevance\nregarding entity and relation in retrieved demonstrations for in-context\nlearning; and (2) the strong inclination to wrongly classify NULL examples into\nother pre-defined labels.\n  In this paper, we propose GPT-RE to bridge the gap between LLMs and\nfully-supervised baselines. GPT-RE successfully addresses the aforementioned\nissues by (1) incorporating task-specific entity representations in\ndemonstration retrieval; and (2) enriching the demonstrations with gold\nlabel-induced reasoning logic. We evaluate GPT-RE on four widely-used RE\ndatasets, and observe that GPT-RE achieves improvements over not only existing\nGPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE\nachieves SOTA performances on the Semeval and SciERC datasets, and competitive\nperformances on the TACRED and ACE05 datasets.",
        "pdf_link": "https://arxiv.org/pdf/2305.02105v3.pdf"
    },
    {
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2023-05-03T07:28:50Z",
        "summary": "Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2305.01937v1.pdf"
    },
    {
        "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback",
        "authors": [
            "Qinyuan Cheng",
            "Xiaogui Yang",
            "Tianxiang Sun",
            "Linyang Li",
            "Xipeng Qiu"
        ],
        "published": "2023-05-03T06:26:13Z",
        "summary": "Contrastive learning has become a popular approach in natural language\nprocessing, particularly for the learning of sentence embeddings. However, the\ndiscrete nature of natural language makes it difficult to ensure the quality of\npositive and negative sample pairs generated through data augmentation methods.\nAlthough supervised contrastive learning can produce more accurate sample pairs\nwith human feedback labels, it still lacks fine-grained training signals. In\nthis paper, we propose to improve \\textbf{C}ontrastive \\textbf{L}earning of\nsentence embeddings from \\textbf{AI} \\textbf{F}eedback \\textbf{(CLAIF)}. Our\nmethod utilizes AI feedback from large pre-trained language models (LLMs) to\nconstruct sample pairs with fine-grained sample similarity scores to improve\ncontrastive learning. Besides, we combine human feedback and AI feedback to\nprovide better supervision signals for supervised contrastive learning of\nsentence embeddings. Experimental results show that our method achieves\nstate-of-the-art performance on several semantic textual similarity (STS) and\ntransfer learning tasks compared to other unsupervised and supervised\ncontrastive learning methods.",
        "pdf_link": "https://arxiv.org/pdf/2305.01918v3.pdf"
    },
    {
        "title": "SCOTT: Self-Consistent Chain-of-Thought Distillation",
        "authors": [
            "Peifeng Wang",
            "Zhengyang Wang",
            "Zheng Li",
            "Yifan Gao",
            "Bing Yin",
            "Xiang Ren"
        ],
        "published": "2023-05-03T03:47:00Z",
        "summary": "Large language models (LMs) beyond a certain scale, demonstrate the emergent\ncapability of generating free-text rationales for their predictions via\nchain-of-thought (CoT) prompting. While CoT can yield dramatically improved\nperformance, such gains are only observed for sufficiently large LMs. Even more\nconcerning, there is little guarantee that the generated rationales are\nconsistent with LM's predictions or faithfully justify the decisions. In this\nwork, we propose a faithful knowledge distillation method to learn a small,\nself-consistent CoT model from a teacher model that is orders of magnitude\nlarger. To form better supervision, we elicit rationales supporting the gold\nanswers from a large LM (teacher) by contrastive decoding, which encourages the\nteacher to generate tokens that become more plausible only when the answer is\nconsidered. To ensure faithful distillation, we use the teacher-generated\nrationales to learn a student LM with a counterfactual reasoning objective,\nwhich prevents the student from ignoring the rationales to make inconsistent\npredictions. Experiments show that, while yielding comparable end-task\nperformance, our method can generate CoT rationales that are more faithful than\nbaselines do. Further analysis suggests that such a model respects the\nrationales more when making decisions; thus, we can improve its performance\nmore by refining its rationales.",
        "pdf_link": "https://arxiv.org/pdf/2305.01879v4.pdf"
    },
    {
        "title": "KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness",
        "authors": [
            "Yichuan Li",
            "Jialong Han",
            "Kyumin Lee",
            "Chengyuan Ma",
            "Benjamin Yao",
            "Derek Liu"
        ],
        "published": "2023-05-02T22:28:26Z",
        "summary": "In recent years, Pre-trained Language Models (PLMs) have shown their\nsuperiority by pre-training on unstructured text corpus and then fine-tuning on\ndownstream tasks. On entity-rich textual resources like Wikipedia,\nKnowledge-Enhanced PLMs (KEPLMs) incorporate the interactions between tokens\nand mentioned entities in pre-training, and are thus more effective on\nentity-centric tasks such as entity linking and relation classification.\nAlthough exploiting Wikipedia's rich structures to some extent, conventional\nKEPLMs still neglect a unique layout of the corpus where each Wikipedia page is\naround a topic entity (identified by the page URL and shown in the page title).\nIn this paper, we demonstrate that KEPLMs without incorporating the topic\nentities will lead to insufficient entity interaction and biased (relation)\nword semantics. We thus propose KEPLET, a novel Knowledge-Enhanced Pre-trained\nLanguagE model with Topic entity awareness. In an end-to-end manner, KEPLET\nidentifies where to add the topic entity's information in a Wikipedia sentence,\nfuses such information into token and mentioned entities representations, and\nsupervises the network learning, through which it takes topic entities back\ninto consideration. Experiments demonstrated the generality and superiority of\nKEPLET which was applied to two representative KEPLMs, achieving significant\nimprovements on four entity-centric tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.01810v1.pdf"
    },
    {
        "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
        "authors": [
            "Yujie Lu",
            "Pan Lu",
            "Zhiyu Chen",
            "Wanrong Zhu",
            "Xin Eric Wang",
            "William Yang Wang"
        ],
        "published": "2023-05-02T21:46:44Z",
        "summary": "Embodied agents have achieved prominent performance in following human\ninstructions to complete tasks. However, the potential of providing\ninstructions informed by texts and images to assist humans in completing tasks\nremains underexplored. To uncover this capability, we present the multimodal\nprocedural planning (MPP) task, in which models are given a high-level goal and\ngenerate plans of paired text-image steps, providing more complementary and\ninformative guidance than unimodal plans. The key challenges of MPP are to\nensure the informativeness, temporal coherence,and accuracy of plans across\nmodalities. To tackle this, we propose Text-Image Prompting (TIP), a\ndual-modality prompting method that jointly leverages zero-shot reasoning\nability in large language models (LLMs) and compelling text-to-image generation\nability from diffusion-based models. TIP improves the interaction in the dual\nmodalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs\nto guide the textual-grounded image plan generation and leveraging the\ndescriptions of image plans to ground the textual plan reversely. To address\nthe lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed\nfor MPP. Our results show compelling human preferences and automatic scores\nagainst unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms\nof informativeness, temporal coherence, and plan accuracy. Our code and data:\nhttps://github.com/YujieLu10/MPP.",
        "pdf_link": "https://arxiv.org/pdf/2305.01795v1.pdf"
    },
    {
        "title": "Automated Code generation for Information Technology Tasks in YAML through Large Language Models",
        "authors": [
            "Saurabh Pujar",
            "Luca Buratti",
            "Xiaojie Guo",
            "Nicolas Dupuis",
            "Burn Lewis",
            "Sahil Suneja",
            "Atin Sood",
            "Ganesh Nalawade",
            "Matthew Jones",
            "Alessandro Morari",
            "Ruchir Puri"
        ],
        "published": "2023-05-02T21:01:01Z",
        "summary": "The recent improvement in code generation capabilities due to the use of\nlarge language models has mainly benefited general purpose programming\nlanguages. Domain specific languages, such as the ones used for IT Automation,\nhave received far less attention, despite involving many active developers and\nbeing an essential component of modern cloud platforms. This work focuses on\nthe generation of Ansible-YAML, a widely used markup language for IT\nAutomation. We present Ansible Wisdom, a natural-language to Ansible-YAML code\ngeneration tool, aimed at improving IT automation productivity. Ansible Wisdom\nis a transformer-based model, extended by training with a new dataset\ncontaining Ansible-YAML. We also develop two novel performance metrics for YAML\nand Ansible to capture the specific characteristics of this domain. Results\nshow that Ansible Wisdom can accurately generate Ansible script from natural\nlanguage prompts with performance comparable or better than existing state of\nthe art code generation models. In few-shot settings we asses the impact of\ntraining with Ansible, YAML data and compare with different baselines including\nCodex-Davinci-002. We also show that after finetuning, our Ansible specific\nmodel (BLEU: 66.67) can outperform a much larger Codex-Davinci-002 (BLEU: 50.4)\nmodel, which was evaluated in few shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2305.02783v4.pdf"
    },
    {
        "title": "Few-shot In-context Learning for Knowledge Base Question Answering",
        "authors": [
            "Tianle Li",
            "Xueguang Ma",
            "Alex Zhuang",
            "Yu Gu",
            "Yu Su",
            "Wenhu Chen"
        ],
        "published": "2023-05-02T19:31:55Z",
        "summary": "Question answering over knowledge bases is considered a difficult problem due\nto the challenge of generalizing to a wide variety of possible natural language\nquestions. Additionally, the heterogeneity of knowledge base schema items\nbetween different knowledge bases often necessitates specialized training for\ndifferent knowledge base question-answering (KBQA) datasets. To handle\nquestions over diverse KBQA datasets with a unified training-free framework, we\npropose KB-BINDER, which for the first time enables few-shot in-context\nlearning over KBQA tasks. Firstly, KB-BINDER leverages large language models\nlike Codex to generate logical forms as the draft for a specific question by\nimitating a few demonstrations. Secondly, KB-BINDER grounds on the knowledge\nbase to bind the generated draft to an executable one with BM25 score matching.\nThe experimental results on four public heterogeneous KBQA datasets show that\nKB-BINDER can achieve a strong performance with only a few in-context\ndemonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even\noutperform the state-of-the-art trained models. On GrailQA and WebQSP, our\nmodel is also on par with other fully-trained models. We believe KB-BINDER can\nserve as an important baseline for future research. Our code is available at\nhttps://github.com/ltl3A87/KB-BINDER.",
        "pdf_link": "https://arxiv.org/pdf/2305.01750v2.pdf"
    },
    {
        "title": "Privacy-Preserving In-Context Learning for Large Language Models",
        "authors": [
            "Tong Wu",
            "Ashwinee Panda",
            "Jiachen T. Wang",
            "Prateek Mittal"
        ],
        "published": "2023-05-02T17:52:58Z",
        "summary": "In-context learning (ICL) is an important capability of Large Language Models\n(LLMs), enabling these models to dynamically adapt based on specific,\nin-context exemplars, thereby improving accuracy and relevance. However, LLM's\nresponses may leak the sensitive private information contained in in-context\nexemplars. To address this challenge, we propose Differentially Private\nIn-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The\nkey idea for DP-ICL paradigm is generating differentially private responses\nthrough a noisy consensus among an ensemble of LLM's responses based on\ndisjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate\nseveral techniques showing how to privatize ICL for text classification and\nlanguage generation. We evaluate DP-ICL on four text classification benchmarks\nand two language generation tasks, and our empirical results show that DP-ICL\nachieves a strong utility-privacy tradeoff.",
        "pdf_link": "https://arxiv.org/pdf/2305.01639v2.pdf"
    },
    {
        "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
        "authors": [
            "Wes Gurnee",
            "Neel Nanda",
            "Matthew Pauly",
            "Katherine Harvey",
            "Dmitrii Troitskii",
            "Dimitris Bertsimas"
        ],
        "published": "2023-05-02T17:13:55Z",
        "summary": "Despite rapid adoption and deployment of large language models (LLMs), the\ninternal computations of these models remain opaque and poorly understood. In\nthis work, we seek to understand how high-level human-interpretable features\nare represented within the internal neuron activations of LLMs. We train\n$k$-sparse linear classifiers (probes) on these internal activations to predict\nthe presence of features in the input; by varying the value of $k$ we study the\nsparsity of learned representations and how this varies with model scale. With\n$k=1$, we localize individual neurons which are highly relevant for a\nparticular feature, and perform a number of case studies to illustrate general\nproperties of LLMs. In particular, we show that early layers make use of sparse\ncombinations of neurons to represent many features in superposition, that\nmiddle layers have seemingly dedicated neurons to represent higher-level\ncontextual features, and that increasing scale causes representational sparsity\nto increase on average, but there are multiple types of scaling dynamics. In\nall, we probe for over 100 unique features comprising 10 different categories\nin 7 different models spanning 70 million to 6.9 billion parameters.",
        "pdf_link": "https://arxiv.org/pdf/2305.01610v2.pdf"
    },
    {
        "title": "Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy",
        "authors": [
            "Aly M. Kassem"
        ],
        "published": "2023-05-02T15:53:28Z",
        "summary": "Large Language models (LLMs) are trained on large amounts of data, which can\ninclude sensitive information that may compromise personal privacy. LLMs showed\nto memorize parts of the training data and emit those data verbatim when an\nadversary prompts appropriately. Previous research has primarily focused on\ndata preprocessing and differential privacy techniques to address memorization\nor prevent verbatim memorization exclusively, which can give a false sense of\nprivacy. However, these methods rely on explicit and implicit assumptions about\nthe structure of the data to be protected, which often results in an incomplete\nsolution to the problem. To address this, we propose a novel framework that\nutilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate\napproximate memorization. Our approach utilizes a negative similarity score,\nsuch as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity\npolicy. Our results demonstrate that this framework effectively mitigates\napproximate memorization while maintaining high levels of coherence and fluency\nin the generated samples. Furthermore, our framework is robust in mitigating\napproximate memorization across various circumstances, including longer\ncontext, which is known to increase memorization in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.01550v1.pdf"
    },
    {
        "title": "FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information",
        "authors": [
            "Andrew Zhu",
            "Karmanya Aggarwal",
            "Alexander Feng",
            "Lara J. Martin",
            "Chris Callison-Burch"
        ],
        "published": "2023-05-02T15:36:10Z",
        "summary": "Dungeons & Dragons (D&D) is a tabletop roleplaying game with complex natural\nlanguage interactions between players and hidden state information. Recent work\nhas shown that large language models (LLMs) that have access to state\ninformation can generate higher quality game turns than LLMs that use dialog\nhistory alone. However, previous work used game state information that was\nheuristically created and was not a true gold standard game state. We present\nFIREBALL, a large dataset containing nearly 25,000 unique sessions from real\nD&D gameplay on Discord with true game state info. We recorded game play\nsessions of players who used the Avrae bot, which was developed to aid people\nin playing D&D online, capturing language, game commands and underlying game\nstate information. We demonstrate that FIREBALL can improve natural language\ngeneration (NLG) by using Avrae state information, improving both automated\nmetrics and human judgments of quality. Additionally, we show that LLMs can\ngenerate executable Avrae commands, particularly after finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2305.01528v3.pdf"
    },
    {
        "title": "Huatuo-26M, a Large-scale Chinese Medical QA Dataset",
        "authors": [
            "Jianquan Li",
            "Xidong Wang",
            "Xiangbo Wu",
            "Zhiyi Zhang",
            "Xiaolong Xu",
            "Jie Fu",
            "Prayag Tiwari",
            "Xiang Wan",
            "Benyou Wang"
        ],
        "published": "2023-05-02T15:33:01Z",
        "summary": "In this paper, we release a largest ever medical Question Answering (QA)\ndataset with 26 million QA pairs. We benchmark many existing approaches in our\ndataset in terms of both retrieval and generation. Experimental results show\nthat the existing models perform far lower than expected and the released\ndataset is still challenging in the pre-trained language model era. Moreover,\nwe also experimentally show the benefit of the proposed dataset in many\naspects: (i) trained models for other QA datasets in a zero-shot fashion; and\n(ii) as external knowledge for retrieval-augmented generation (RAG); and (iii)\nimproving existing pre-trained language models by using the QA pairs as a\npre-training corpus in continued training manner. We believe that this dataset\nwill not only contribute to medical research but also facilitate both the\npatients and clinical doctors. See\n\\url{https://github.com/FreedomIntelligence/Huatuo-26M}.",
        "pdf_link": "https://arxiv.org/pdf/2305.01526v1.pdf"
    },
    {
        "title": "VPGTrans: Transfer Visual Prompt Generator across LLMs",
        "authors": [
            "Ao Zhang",
            "Hao Fei",
            "Yuan Yao",
            "Wei Ji",
            "Li Li",
            "Zhiyuan Liu",
            "Tat-Seng Chua"
        ],
        "published": "2023-05-02T09:28:39Z",
        "summary": "While developing a new multimodal LLM (MLLM) by pre-training on tremendous\nimage-text pairs from scratch can be exceedingly resource-consuming, connecting\nan existing LLM with a comparatively lightweight visual prompt generator (VPG)\nbecomes a feasible paradigm. However, further tuning the VPG part of the MLLM\nstill suffers from indispensable computational costs, i.e., requiring thousands\nof GPU hours and millions of training data. One alternative solution is to\ntransfer an existing VPG from any existing MLLMs for the target MLLM.\n  In this work, we for the first time investigate the VPG transferability\nacross LLMs, and explore a solution to reduce the cost of VPG transfer. We\nfirst study the VPG transfer across different LLM sizes (e.g., small-to-large),\nand across different LLM types, through which we diagnose the key factors to\nmaximize the transfer efficiency. Based on our observation, we design a\ntwo-stage transfer framework named VPGTrans, which is simple yet highly\neffective. Through extensive experiments, we demonstrate that VPGTrans helps\nsignificantly speed up the transfer learning process without compromising\nperformance. Remarkably, it helps achieve the VPG transfer from BLIP-2\nOPT$_\\text{2.7B}$ to BLIP-2 OPT$_\\text{6.7B}$ with over 10 times speed-up and\n10.7% training data compared with connecting a VPG to OPT$_\\text{6.7B}$ from\nscratch. Further, a series of intriguing findings and potential rationales\nbehind them are provided and discussed. Finally, we showcase the practical\nvalue of our VPGTrans approach, by customizing two novel MLLMs, including\nVL-LLaMA and VL-Vicuna, with recently released LLaMA and Vicuna LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2305.01278v2.pdf"
    },
    {
        "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
        "authors": [
            "Jiawei Liu",
            "Chunqiu Steven Xia",
            "Yuyao Wang",
            "Lingming Zhang"
        ],
        "published": "2023-05-02T05:46:48Z",
        "summary": "Program synthesis has been long studied with recent approaches focused on\ndirectly using the power of Large Language Models (LLMs) to generate code.\nProgramming benchmarks, with curated synthesis problems and test-cases, are\nused to measure the performance of various LLMs on code synthesis. However,\nthese test-cases can be limited in both quantity and quality for fully\nassessing the functional correctness of the generated code. Such limitation in\nthe existing benchmarks begs the following question: In the era of LLMs, is the\ncode generated really correct? To answer this, we propose EvalPlus -- a code\nsynthesis evaluation framework to rigorously benchmark the functional\ncorrectness of LLM-synthesized code. EvalPlus augments a given evaluation\ndataset with large amounts of test-cases newly produced by an automatic test\ninput generator, powered by both LLM- and mutation-based strategies. While\nEvalPlus is general, we extend the test-cases of the popular HumanEval\nbenchmark by 80x to build HumanEval+. Our extensive evaluation across 26\npopular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to\ncatch significant amounts of previously undetected wrong code synthesized by\nLLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that\ntest insufficiency can lead to mis-ranking. For example, both\nWizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+,\nwhile none of them could on HumanEval. Our work not only indicates that prior\npopular code synthesis evaluation results do not accurately reflect the true\nperformance of LLMs for code synthesis, but also opens up a new direction to\nimprove such programming benchmarks through automated testing. We have\nopen-sourced our tools, enhanced datasets as well as all LLM-generated code at\nhttps://github.com/evalplus/evalplus to facilitate and accelerate future\nLLM-for-code research.",
        "pdf_link": "https://arxiv.org/pdf/2305.01210v3.pdf"
    },
    {
        "title": "A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models",
        "authors": [
            "Chenyang Lyu",
            "Zefeng Du",
            "Jitao Xu",
            "Yitao Duan",
            "Minghao Wu",
            "Teresa Lynn",
            "Alham Fikri Aji",
            "Derek F. Wong",
            "Siyou Liu",
            "Longyue Wang"
        ],
        "published": "2023-05-02T03:27:27Z",
        "summary": "Machine Translation (MT) has greatly advanced over the years due to the\ndevelopments in deep neural networks. However, the emergence of Large Language\nModels (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT\ndomain. In this context, we believe that the future of MT is intricately tied\nto the capabilities of LLMs. These models not only offer vast linguistic\nunderstandings but also bring innovative methodologies, such as prompt-based\ntechniques, that have the potential to further elevate MT. In this paper, we\nprovide an overview of the significant enhancements in MT that are influenced\nby LLMs and advocate for their pivotal role in upcoming MT research and\nimplementations. We highlight several new MT directions, emphasizing the\nbenefits of LLMs in scenarios such as Long-Document Translation, Stylized\nTranslation, and Interactive Translation. Additionally, we address the\nimportant concern of privacy in LLM-driven MT and suggest essential\nprivacy-preserving strategies. By showcasing practical instances, we aim to\ndemonstrate the advantages that LLMs offer, particularly in tasks like\ntranslating extended documents. We conclude by emphasizing the critical role of\nLLMs in guiding the future evolution of MT and offer a roadmap for future\nexploration in the sector.",
        "pdf_link": "https://arxiv.org/pdf/2305.01181v3.pdf"
    },
    {
        "title": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models",
        "authors": [
            "Nurendra Choudhary",
            "Chandan K. Reddy"
        ],
        "published": "2023-05-02T02:21:49Z",
        "summary": "Reasoning over knowledge graphs (KGs) is a challenging task that requires a\ndeep understanding of the complex relationships between entities and the\nunderlying logic of their relations. Current approaches rely on learning\ngeometries to embed entities in vector space for logical query operations, but\nthey suffer from subpar performance on complex queries and dataset-specific\nrepresentations. In this paper, we propose a novel decoupled approach,\nLanguage-guided Abstract Reasoning over Knowledge graphs (LARK), that\nformulates complex KG reasoning as a combination of contextual KG search and\nlogical query reasoning, to leverage the strengths of graph extraction\nalgorithms and large language models (LLM), respectively. Our experiments\ndemonstrate that the proposed approach outperforms state-of-the-art KG\nreasoning methods on standard benchmark datasets across several logical query\nconstructs, with significant performance gain for queries of higher complexity.\nFurthermore, we show that the performance of our approach improves\nproportionally to the increase in size of the underlying LLM, enabling the\nintegration of the latest advancements in LLMs for logical reasoning over KGs.\nOur work presents a new direction for addressing the challenges of complex KG\nreasoning and paves the way for future research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2305.01157v3.pdf"
    },
    {
        "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models",
        "authors": [
            "Dave Van Veen",
            "Cara Van Uden",
            "Maayane Attias",
            "Anuj Pareek",
            "Christian Bluethgen",
            "Malgorzata Polacin",
            "Wah Chiu",
            "Jean-Benoit Delbrouck",
            "Juan Manuel Zambrano Chaves",
            "Curtis P. Langlotz",
            "Akshay S. Chaudhari",
            "John Pauly"
        ],
        "published": "2023-05-02T01:33:02Z",
        "summary": "We systematically investigate lightweight strategies to adapt large language\nmodels (LLMs) for the task of radiology report summarization (RRS).\nSpecifically, we focus on domain adaptation via pretraining (on natural\nlanguage, biomedical text, or clinical text) and via discrete prompting or\nparameter-efficient fine-tuning. Our results consistently achieve best\nperformance by maximally adapting to the task via pretraining on clinical text\nand fine-tuning on RRS examples. Importantly, this method fine-tunes a mere\n0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning\n(100% of parameters). Additionally, we study the effect of in-context examples\nand out-of-distribution (OOD) training before concluding with a radiologist\nreader study and qualitative analysis. Our findings highlight the importance of\ndomain adaptation in RRS and provide valuable insights toward developing\neffective natural language processing solutions for clinical tasks.",
        "pdf_link": "https://arxiv.org/pdf/2305.01146v3.pdf"
    },
    {
        "title": "Evaluating statistical language models as pragmatic reasoners",
        "authors": [
            "Benjamin Lipkin",
            "Lionel Wong",
            "Gabriel Grand",
            "Joshua B Tenenbaum"
        ],
        "published": "2023-05-01T18:22:10Z",
        "summary": "The relationship between communicated language and intended meaning is often\nprobabilistic and sensitive to context. Numerous strategies attempt to estimate\nsuch a mapping, often leveraging recursive Bayesian models of communication. In\nparallel, large language models (LLMs) have been increasingly applied to\nsemantic parsing applications, tasked with inferring logical representations\nfrom natural language. While existing LLM explorations have been largely\nrestricted to literal language use, in this work, we evaluate the capacity of\nLLMs to infer the meanings of pragmatic utterances. Specifically, we explore\nthe case of threshold estimation on the gradable adjective ``strong'',\ncontextually conditioned on a strength prior, then extended to composition with\nqualification, negation, polarity inversion, and class comparison. We find that\nLLMs can derive context-grounded, human-like distributions over the\ninterpretations of several complex pragmatic utterances, yet struggle composing\nwith negation. These results inform the inferential capacity of statistical\nlanguage models, and their use in pragmatic and semantic parsing applications.\nAll corresponding code is made publicly available\n(https://github.com/benlipkin/probsem/tree/CogSci2023).",
        "pdf_link": "https://arxiv.org/pdf/2305.01020v1.pdf"
    },
    {
        "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
        "authors": [
            "Patrick Fernandes",
            "Aman Madaan",
            "Emmy Liu",
            "António Farinhas",
            "Pedro Henrique Martins",
            "Amanda Bertsch",
            "José G. C. de Souza",
            "Shuyan Zhou",
            "Tongshuang Wu",
            "Graham Neubig",
            "André F. T. Martins"
        ],
        "published": "2023-05-01T17:36:06Z",
        "summary": "Many recent advances in natural language generation have been fueled by\ntraining large language models on internet-scale data. However, this paradigm\ncan lead to models that generate toxic, inaccurate, and unhelpful content, and\nautomatic evaluation metrics often fail to identify these behaviors. As models\nbecome more capable, human feedback is an invaluable signal for evaluating and\nimproving models. This survey aims to provide an overview of the recent\nresearch that has leveraged human feedback to improve natural language\ngeneration. First, we introduce an encompassing formalization of feedback, and\nidentify and organize existing research into a taxonomy following this\nformalization. Next, we discuss how feedback can be described by its format and\nobjective, and cover the two approaches proposed to use feedback (either for\ntraining or decoding): directly using the feedback or training feedback models.\nWe also discuss existing datasets for human-feedback data collection, and\nconcerns surrounding feedback collection. Finally, we provide an overview of\nthe nascent field of AI feedback, which exploits large language models to make\njudgments based on a set of principles and minimize the need for human\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2305.00955v2.pdf"
    },
    {
        "title": "Large Linguistic Models: Analyzing theoretical linguistic abilities of LLMs",
        "authors": [
            "Gašper Beguš",
            "Maksymilian Dąbkowski",
            "Ryan Rhodes"
        ],
        "published": "2023-05-01T17:09:33Z",
        "summary": "The performance of large language models (LLMs) has recently improved to the\npoint where the models can perform well on many language tasks. We show here\nthat for the first time, the models can also generate coherent and valid formal\nanalyses of linguistic data and illustrate the vast potential of large language\nmodels for analyses of their metalinguistic abilities. LLMs are primarily\ntrained on language data in the form of text; analyzing and evaluating their\nmetalinguistic abilities improves our understanding of their general\ncapabilities and sheds new light on theoretical models in linguistics. In this\npaper, we probe into GPT-4's metalinguistic capabilities by focusing on three\nsubfields of formal linguistics: syntax, phonology, and semantics. We outline a\nresearch program for metalinguistic analyses of large language models, propose\nexperimental designs, provide general guidelines, discuss limitations, and\noffer future directions for this line of research. This line of inquiry also\nexemplifies behavioral interpretability of deep learning, where models'\nrepresentations are accessed by explicit prompting rather than internal\nrepresentations.",
        "pdf_link": "https://arxiv.org/pdf/2305.00948v2.pdf"
    },
    {
        "title": "Poisoning Language Models During Instruction Tuning",
        "authors": [
            "Alexander Wan",
            "Eric Wallace",
            "Sheng Shen",
            "Dan Klein"
        ],
        "published": "2023-05-01T16:57:33Z",
        "summary": "Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on\ndatasets that contain user-submitted examples, e.g., FLAN aggregates numerous\nopen-source datasets and OpenAI leverages examples submitted in the browser\nplayground. In this work, we show that adversaries can contribute poison\nexamples to these datasets, allowing them to manipulate model predictions\nwhenever a desired trigger phrase appears in the input. For example, when a\ndownstream user provides an input that mentions \"Joe Biden\", a poisoned LM will\nstruggle to classify, summarize, edit, or translate that input. To construct\nthese poison examples, we optimize their inputs and outputs using a\nbag-of-words approximation to the LM. We evaluate our method on open-source\ninstruction-tuned LMs. By using as few as 100 poison examples, we can cause\narbitrary phrases to have consistent negative polarity or induce degenerate\noutputs across hundreds of held-out tasks. Worryingly, we also show that larger\nLMs are increasingly vulnerable to poisoning and that defenses based on data\nfiltering or reducing model capacity provide only moderate protections while\nreducing test accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2305.00944v1.pdf"
    },
    {
        "title": "Learning to Reason and Memorize with Self-Notes",
        "authors": [
            "Jack Lanchantin",
            "Shubham Toshniwal",
            "Jason Weston",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar"
        ],
        "published": "2023-05-01T14:02:48Z",
        "summary": "Large language models have been shown to struggle with multi-step reasoning,\nand do not retain previous reasoning steps for future use. We propose a simple\nmethod for solving both of these problems by allowing the model to take\nSelf-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model\ncan deviate from the input context at any time to explicitly think and write\ndown its thoughts. This allows the model to perform reasoning on the fly as it\nreads the context and even integrate previous reasoning steps, thus enhancing\nits memory with useful information and enabling multi-step reasoning.\nExperiments across a wide variety of tasks demonstrate that our method can\noutperform chain-of-thought and scratchpad methods by taking Self-Notes that\ninterleave the input text.",
        "pdf_link": "https://arxiv.org/pdf/2305.00833v2.pdf"
    },
    {
        "title": "Self-Evaluation Guided Beam Search for Reasoning",
        "authors": [
            "Yuxi Xie",
            "Kenji Kawaguchi",
            "Yiran Zhao",
            "Xu Zhao",
            "Min-Yen Kan",
            "Junxian He",
            "Qizhe Xie"
        ],
        "published": "2023-05-01T02:37:59Z",
        "summary": "Breaking down a problem into intermediate steps has demonstrated impressive\nperformance in Large Language Model (LLM) reasoning. However, the growth of the\nreasoning chain introduces uncertainty and error accumulation, making it\nchallenging to elicit accurate final results. To tackle this challenge of\nuncertainty in multi-step reasoning, we introduce a stepwise self-evaluation\nmechanism to guide and calibrate the reasoning process of LLMs. We propose a\ndecoding algorithm integrating the self-evaluation guidance via stochastic beam\nsearch. The self-evaluation guidance serves as a better-calibrated automatic\ncriterion, facilitating an efficient search in the reasoning space and\nresulting in superior prediction quality. Stochastic beam search balances\nexploitation and exploration of the search space with temperature-controlled\nrandomness. Our approach surpasses the corresponding Codex-backboned baselines\nin few-shot accuracy by $6.34\\%$, $9.56\\%$, and $5.46\\%$ on the GSM8K, AQuA,\nand StrategyQA benchmarks, respectively. Experiment results with Llama-2 on\narithmetic reasoning demonstrate the efficiency of our method in outperforming\nthe baseline methods with comparable computational budgets. Further analysis in\nmulti-step reasoning finds our self-evaluation guidance pinpoints logic\nfailures and leads to higher consistency and robustness. Our code is publicly\navailable at https://guideddecoding.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2305.00633v3.pdf"
    },
    {
        "title": "Using Large Language Models to Generate JUnit Tests: An Empirical Study",
        "authors": [
            "Mohammed Latif Siddiq",
            "Joanna C. S. Santos",
            "Ridwanul Hasan Tanvir",
            "Noshin Ulfat",
            "Fahmid Al Rifat",
            "Vinicius Carvalho Lopes"
        ],
        "published": "2023-04-30T07:28:06Z",
        "summary": "A code generation model generates code by taking a prompt from a code\ncomment, existing code, or a combination of both. Although code generation\nmodels (e.g., GitHub Copilot) are increasingly being adopted in practice, it is\nunclear whether they can successfully be used for unit test generation without\nfine-tuning for a strongly typed language like Java. To fill this gap, we\ninvestigated how well three models (Codex, GPT-3.5-Turbo, and StarCoder) can\ngenerate unit tests. We used two benchmarks (HumanEval and Evosuite SF110) to\ninvestigate the effect of context generation on the unit test generation\nprocess. We evaluated the models based on compilation rates, test correctness,\ntest coverage, and test smells. We found that the Codex model achieved above\n80% coverage for the HumanEval dataset, but no model had more than 2% coverage\nfor the EvoSuite SF110 benchmark. The generated tests also suffered from test\nsmells, such as Duplicated Asserts and Empty Tests.",
        "pdf_link": "https://arxiv.org/pdf/2305.00418v4.pdf"
    },
    {
        "title": "Beyond Classification: Financial Reasoning in State-of-the-Art Language Models",
        "authors": [
            "Guijin Son",
            "Hanearl Jung",
            "Moonjeong Hahm",
            "Keonju Na",
            "Sol Jin"
        ],
        "published": "2023-04-30T04:36:05Z",
        "summary": "Large Language Models (LLMs), consisting of 100 billion or more parameters,\nhave demonstrated remarkable ability in complex multi-step reasoning tasks.\nHowever, the application of such generic advancements has been limited to a few\nfields, such as clinical or legal, with the field of financial reasoning\nremaining largely unexplored. To the best of our knowledge, the ability of LLMs\nto solve financial reasoning problems has never been dealt with, and whether it\ncan be performed at any scale remains unknown. To address this knowledge gap,\nthis research presents a comprehensive investigation into the potential\napplication of LLMs in the financial domain. The investigation includes a\ndetailed exploration of a range of subjects, including task formulation,\nsynthetic data generation, prompting methods, and evaluation capability.\nFurthermore, the study benchmarks various GPT variants with parameter scales\nranging from 2.8B to 13B, with and without instruction tuning, on diverse\ndataset sizes. By analyzing the results, we reveal that the ability to generate\ncoherent financial reasoning first emerges at 6B parameters, and continues to\nimprove with better instruction-tuning or larger datasets. Additionally, the\nstudy provides a publicly accessible dataset named sFIOG (Synthetic-Financial\nInvestment Opinion Generation), consisting of 11,802 synthetic investment\nthesis samples, to support further research in the field of financial\nreasoning. Overall, this research seeks to contribute to the understanding of\nthe efficacy of language models in the field of finance, with a particular\nemphasis on their ability to engage in sophisticated reasoning and analysis\nwithin the context of investment decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2305.01505v2.pdf"
    },
    {
        "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
        "authors": [
            "Emre Kıcıman",
            "Robert Ness",
            "Amit Sharma",
            "Chenhao Tan"
        ],
        "published": "2023-04-28T19:00:43Z",
        "summary": "The causal capabilities of large language models (LLMs) is a matter of\nsignificant debate, with critical implications for the use of LLMs in\nsocietally impactful domains such as medicine, science, law, and policy. We\nfurther our understanding of LLMs and their causal implications, considering\nthe distinctions between different types of causal reasoning tasks, as well as\nthe entangled threats of construct and measurement validity. LLM-based methods\nestablish new state-of-the-art accuracies on multiple causal benchmarks.\nAlgorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise\ncausal discovery task (97%, 13 points gain), counterfactual reasoning task\n(92%, 20 points gain), and actual causality (86% accuracy in determining\nnecessary and sufficient causes in vignettes). At the same time, LLMs exhibit\nunpredictable failure modes and we provide some techniques to interpret their\nrobustness.\n  Crucially, LLMs perform these causal tasks while relying on sources of\nknowledge and methods distinct from and complementary to non-LLM based\napproaches. Specifically, LLMs bring capabilities so far understood to be\nrestricted to humans, such as using collected knowledge to generate causal\ngraphs or identifying background causal context from natural language. We\nenvision LLMs to be used alongside existing causal methods, as a proxy for\nhuman domain knowledge and to reduce human effort in setting up a causal\nanalysis, one of the biggest impediments to the widespread adoption of causal\nmethods. We also see existing causal methods as promising tools for LLMs to\nformalize, validate, and communicate their reasoning especially in high-stakes\nscenarios.\n  In capturing common sense and domain knowledge about causal mechanisms and\nsupporting translation between natural language and formal methods, LLMs open\nnew frontiers for advancing the research, practice, and adoption of causality.",
        "pdf_link": "https://arxiv.org/pdf/2305.00050v2.pdf"
    },
    {
        "title": "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model",
        "authors": [
            "Peng Gao",
            "Jiaming Han",
            "Renrui Zhang",
            "Ziyi Lin",
            "Shijie Geng",
            "Aojun Zhou",
            "Wei Zhang",
            "Pan Lu",
            "Conghui He",
            "Xiangyu Yue",
            "Hongsheng Li",
            "Yu Qiao"
        ],
        "published": "2023-04-28T17:59:25Z",
        "summary": "How to efficiently transform large language models (LLMs) into instruction\nfollowers is recently a popular research direction, while training LLM for\nmulti-modal reasoning remains less explored. Although the recent LLaMA-Adapter\ndemonstrates the potential to handle visual inputs with LLMs, it still cannot\ngeneralize well to open-ended visual instructions and lags behind GPT-4. In\nthis paper, we present LLaMA-Adapter V2, a parameter-efficient visual\ninstruction model. Specifically, we first augment LLaMA-Adapter by unlocking\nmore learnable parameters (e.g., norm, bias and scale), which distribute the\ninstruction-following ability across the entire LLaMA model besides adapters.\nSecondly, we propose an early fusion strategy to feed visual tokens only into\nthe early LLM layers, contributing to better visual knowledge incorporation.\nThirdly, a joint training paradigm of image-text pairs and\ninstruction-following data is introduced by optimizing disjoint groups of\nlearnable parameters. This strategy effectively alleviates the interference\nbetween the two tasks of image-text alignment and instruction following and\nachieves strong multi-modal reasoning with only a small-scale image-text and\ninstruction dataset. During inference, we incorporate additional expert models\n(e.g. captioning/OCR systems) into LLaMA-Adapter to further enhance its image\nunderstanding capability without incurring training costs. Compared to the\noriginal LLaMA-Adapter, our LLaMA-Adapter V2 can perform open-ended multi-modal\ninstructions by merely introducing 14M parameters over LLaMA. The newly\ndesigned framework also exhibits stronger language-only instruction-following\ncapabilities and even excels in chat interactions. Our code and models are\navailable at https://github.com/ZrrSkywalker/LLaMA-Adapter.",
        "pdf_link": "https://arxiv.org/pdf/2304.15010v1.pdf"
    },
    {
        "title": "Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs",
        "authors": [
            "George Pu",
            "Anirudh Jain",
            "Jihan Yin",
            "Russell Kaplan"
        ],
        "published": "2023-04-28T17:39:49Z",
        "summary": "As foundation models continue to exponentially scale in size, efficient\nmethods of adaptation become increasingly critical. Parameter-efficient\nfine-tuning (PEFT), a recent class of techniques that require only modifying a\nsmall percentage of the model parameters, is currently the most popular method\nfor adapting large language models (LLMs). Several PEFT techniques have\nrecently been proposed with varying tradeoffs. We provide a comprehensive and\nuniform benchmark of various PEFT techniques across a representative LLM, the\nFLAN-T5 model, and evaluate model performance across different data scales of\nclassification and generation datasets. Based on this, we provide a framework\nfor choosing the optimal fine-tuning techniques given the task type and data\navailability. Contrary to popular belief, we also empirically prove that PEFT\ntechniques converge slower than full tuning in low data scenarios, and posit\nthe amount of data required for PEFT methods to both perform well and converge\nefficiently. Lastly, we further optimize these PEFT techniques by selectively\nchoosing which parts of the model to train, and find that these techniques can\nbe applied with significantly fewer parameters while maintaining and even\nimproving performance.",
        "pdf_link": "https://arxiv.org/pdf/2304.14999v1.pdf"
    },
    {
        "title": "ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations",
        "authors": [
            "Chunkit Chan",
            "Jiayang Cheng",
            "Weiqi Wang",
            "Yuxin Jiang",
            "Tianqing Fang",
            "Xin Liu",
            "Yangqiu Song"
        ],
        "published": "2023-04-28T13:14:36Z",
        "summary": "This paper aims to quantitatively evaluate the performance of ChatGPT, an\ninteractive large language model, on inter-sentential relations such as\ntemporal relations, causal relations, and discourse relations. Given ChatGPT's\npromising performance across various tasks, we proceed to carry out thorough\nevaluations on the whole test sets of 11 datasets, including temporal and\ncausal relations, PDTB2.0-based, and dialogue-based discourse relations. To\nensure the reliability of our findings, we employ three tailored prompt\ntemplates for each task, including the zero-shot prompt template, zero-shot\nprompt engineering (PE) template, and in-context learning (ICL) prompt\ntemplate, to establish the initial baseline scores for all popular\nsentence-pair relation classification tasks for the first time. Through our\nstudy, we discover that ChatGPT exhibits exceptional proficiency in detecting\nand reasoning about causal relations, albeit it may not possess the same level\nof expertise in identifying the temporal order between two events. While it is\ncapable of identifying the majority of discourse relations with existing\nexplicit discourse connectives, the implicit discourse relation remains a\nformidable challenge. Concurrently, ChatGPT demonstrates subpar performance in\nthe dialogue discourse parsing task that requires structural understanding in a\ndialogue before being aware of the discourse relation.",
        "pdf_link": "https://arxiv.org/pdf/2304.14827v3.pdf"
    },
    {
        "title": "Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?",
        "authors": [
            "Sonal Sannigrahi",
            "Josef van Genabith",
            "Cristina Espana-Bonet"
        ],
        "published": "2023-04-28T12:11:21Z",
        "summary": "Dense vector representations for textual data are crucial in modern NLP. Word\nembeddings and sentence embeddings estimated from raw texts are key in\nachieving state-of-the-art results in various tasks requiring semantic\nunderstanding. However, obtaining embeddings at the document level is\nchallenging due to computational requirements and lack of appropriate data.\nInstead, most approaches fall back on computing document embeddings based on\nsentence representations. Although there exist architectures and models to\nencode documents fully, they are in general limited to English and few other\nhigh-resourced languages. In this work, we provide a systematic comparison of\nmethods to produce document-level representations from sentences based on\nLASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare\ninput token number truncation, sentence averaging as well as some simple\nwindowing and in some cases new augmented and learnable approaches, on 3 multi-\nand cross-lingual tasks in 8 languages belonging to 3 different language\nfamilies. Our task-based extrinsic evaluations show that, independently of the\nlanguage, a clever combination of sentence embeddings is usually better than\nencoding the full document as a single unit, even when this is possible. We\ndemonstrate that while a simple sentence average results in a strong baseline\nfor classification tasks, more complex combinations are necessary for semantic\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.14796v1.pdf"
    },
    {
        "title": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Huawei Shen",
            "Xueqi Cheng",
            "Tat-Seng Chua"
        ],
        "published": "2023-04-28T10:15:25Z",
        "summary": "Making the content generated by Large Language Model (LLM), accurate,\ncredible and traceable is crucial, especially in complex knowledge-intensive\ntasks that require multi-step reasoning and each step needs knowledge to solve.\nRetrieval-augmented generation is good potential to solve this problem.\nHowever, where and how to introduce Information Retrieval (IR) to LLM is a big\nchallenge. Previous work has the problems that wrong knowledge retrieved by IR\nmisleads the LLM and interaction between IR and LLM breaks the reasoning chain\nof LLM. This paper proposes a novel framework named\n\\textbf{Search-in-the-Chain} (SearChain) for the interaction between LLM and IR\nto solve the challenges. First, LLM generates the reasoning chain named\nChain-of-Query (CoQ) where each node consists of an IR-oriented query-answer\npair. Second, IR verifies the answer of each node of CoQ. It corrects the\nanswer that is not consistent with the retrieved information when IR gives high\nconfidence, which improves the credibility. Third, LLM can indicate its missing\nknowledge in CoQ and rely on IR to provide this knowledge to LLM. These\noperations improve the accuracy in terms of reasoning and knowledge. Finally,\nSearChain generates the reasoning process and marks references to supporting\ndocuments for each reasoning step, which improves traceability. Interaction\nwith IR in SearChain forms a novel reasoning path based on a tree, which\nenables LLM to dynamically modify the direction of reasoning. Experiments show\nthat SearChain outperforms state-of-the-art baselines on complex\nknowledge-intensive tasks including multi-hop Q\\&A, slot filling, fact\nchecking, and long-form Q\\&A.",
        "pdf_link": "https://arxiv.org/pdf/2304.14732v7.pdf"
    },
    {
        "title": "Towards autonomous system: flexible modular production system enhanced with large language model agents",
        "authors": [
            "Yuchen Xia",
            "Manthan Shenoy",
            "Nasser Jazdi",
            "Michael Weyrich"
        ],
        "published": "2023-04-28T09:42:18Z",
        "summary": "In this paper, we present a novel framework that combines large language\nmodels (LLMs), digital twins and industrial automation system to enable\nintelligent planning and control of production processes. We retrofit the\nautomation system for a modular production facility and create executable\ncontrol interfaces of fine-granular functionalities and coarse-granular skills.\nLow-level functionalities are executed by automation components, and high-level\nskills are performed by automation modules. Subsequently, a digital twin system\nis developed, registering these interfaces and containing additional\ndescriptive information about the production system. Based on the retrofitted\nautomation system and the created digital twins, LLM-agents are designed to\ninterpret descriptive information in the digital twins and control the physical\nsystem through service interfaces. These LLM-agents serve as intelligent agents\non different levels within an automation system, enabling autonomous planning\nand control of flexible production. Given a task instruction as input, the\nLLM-agents orchestrate a sequence of atomic functionalities and skills to\naccomplish the task. We demonstrate how our implemented prototype can handle\nun-predefined tasks, plan a production process, and execute the operations.\nThis research highlights the potential of integrating LLMs into industrial\nautomation systems in the context of smart factory for more agile, flexible,\nand adaptive production processes, while it also underscores the critical\ninsights and limitations for future work. Demos at:\nhttps://github.com/YuchenXia/GPT4IndustrialAutomation",
        "pdf_link": "https://arxiv.org/pdf/2304.14721v4.pdf"
    },
    {
        "title": "PMC-LLaMA: Towards Building Open-source Language Models for Medicine",
        "authors": [
            "Chaoyi Wu",
            "Weixiong Lin",
            "Xiaoman Zhang",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "published": "2023-04-27T18:29:05Z",
        "summary": "Recently, Large Language Models (LLMs) have showcased remarkable capabilities\nin natural language understanding. While demonstrating proficiency in everyday\nconversations and question-answering situations, these models frequently\nstruggle in domains that require precision, such as medical applications, due\nto their lack of domain-specific knowledge. In this paper, we describe the\nprocedure for building a powerful, open-source language model specifically\ndesigned for medicine applications, termed as PMC-LLaMA. Our contributions are\nthreefold: (i) we systematically investigate the process of adapting a\ngeneral-purpose foundation language model towards medical domain, this involves\ndata-centric knowledge injection through the integration of 4.8M biomedical\nacademic papers and 30K medical textbooks, as well as comprehensive fine-tuning\nfor alignment with domain-specific instructions; (ii) we contribute a\nlarge-scale, comprehensive dataset for instruction tuning. This dataset\nencompasses medical question-answering (QA), rationale for reasoning, and\nconversational dialogues, comprising a total of 202M tokens; (iii) we conduct\nthorough ablation studies to demonstrate the effectiveness of each proposed\ncomponent. While evaluating on various public medical question-answering\nbenchmarks, our lightweight PMCLLaMA, which consists of only 13 billion\nparameters, exhibits superior performance, even surpassing ChatGPT. All models,\ncodes, datasets can be found in https://github.com/chaoyi-wu/PMC-LLaMA.",
        "pdf_link": "https://arxiv.org/pdf/2304.14454v3.pdf"
    },
    {
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions",
        "authors": [
            "Minghao Wu",
            "Abdul Waheed",
            "Chiyu Zhang",
            "Muhammad Abdul-Mageed",
            "Alham Fikri Aji"
        ],
        "published": "2023-04-27T17:58:49Z",
        "summary": "Large language models (LLMs) with instruction fine-tuning demonstrate\nsuperior generative capabilities. However, these models are resource-intensive.\nTo alleviate this issue, we explore distilling knowledge from instruction-tuned\nLLMs into much smaller ones. To this end, we carefully develop a large set of\n2.58M instructions based on both existing and newly-generated instructions. In\naddition to being sizable, we design our instructions to cover a broad set of\ntopics to ensure diversity. Extensive analysis of our instruction dataset\nconfirms its diversity, and we generate responses for these instructions using\ngpt-3.5-turbo. Leveraging these instructions, we fine-tune a diverse herd of\nmodels, collectively referred to as LaMini-LM, which includes models from both\nthe encoder-decoder and decoder-only families, with varying sizes. We evaluate\nthe performance of our models using automatic metrics on 15 different natural\nlanguage processing (NLP) benchmarks, as well as through human assessment. The\nresults demonstrate that our proposed LaMini-LM models are comparable to\ncompetitive baselines, while being much smaller in size.",
        "pdf_link": "https://arxiv.org/pdf/2304.14402v3.pdf"
    },
    {
        "title": "IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers",
        "authors": [
            "Ronghuan Wu",
            "Wanchao Su",
            "Kede Ma",
            "Jing Liao"
        ],
        "published": "2023-04-27T17:58:02Z",
        "summary": "Scalable Vector Graphics (SVG) is a popular vector image format that offers\ngood support for interactivity and animation. Despite its appealing\ncharacteristics, creating custom SVG content can be challenging for users due\nto the steep learning curve required to understand SVG grammars or get familiar\nwith professional editing software. Recent advancements in text-to-image\ngeneration have inspired researchers to explore vector graphics synthesis using\neither image-based methods (i.e., text -> raster image -> vector graphics)\ncombining text-to-image generation models with image vectorization, or\nlanguage-based methods (i.e., text -> vector graphics script) through\npretrained large language models. However, these methods still suffer from\nlimitations in terms of generation quality, diversity, and flexibility. In this\npaper, we introduce IconShop, a text-guided vector icon synthesis method using\nautoregressive transformers. The key to success of our approach is to\nsequentialize and tokenize SVG paths (and textual descriptions as guidance)\ninto a uniquely decodable token sequence. With that, we are able to fully\nexploit the sequence learning power of autoregressive transformers, while\nenabling both unconditional and text-conditioned icon synthesis. Through\nstandard training to predict the next token on a large-scale vector icon\ndataset accompanied by textural descriptions, the proposed IconShop\nconsistently exhibits better icon synthesis capability than existing\nimage-based and language-based methods both quantitatively and qualitatively.\nMeanwhile, we observe a dramatic improvement in generation diversity, which is\nvalidated by the objective Uniqueness and Novelty measures. More importantly,\nwe demonstrate the flexibility of IconShop with multiple novel icon synthesis\ntasks, including icon editing, icon interpolation, icon semantic combination,\nand icon design auto-suggestion.",
        "pdf_link": "https://arxiv.org/pdf/2304.14400v4.pdf"
    },
    {
        "title": "We're Afraid Language Models Aren't Modeling Ambiguity",
        "authors": [
            "Alisa Liu",
            "Zhaofeng Wu",
            "Julian Michael",
            "Alane Suhr",
            "Peter West",
            "Alexander Koller",
            "Swabha Swayamdipta",
            "Noah A. Smith",
            "Yejin Choi"
        ],
        "published": "2023-04-27T17:57:58Z",
        "summary": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is\na key part of human language understanding, allowing us to anticipate\nmisunderstanding as communicators and revise our interpretations as listeners.\nAs language models (LMs) are increasingly employed as dialogue interfaces and\nwriting aids, handling ambiguous language is critical to their success. We\ncharacterize ambiguity in a sentence by its effect on entailment relations with\nanother sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645\nexamples with diverse kinds of ambiguity. We design a suite of tests based on\nAmbiEnt, presenting the first evaluation of pretrained LMs to recognize\nambiguity and disentangle possible meanings. We find that the task remains\nextremely challenging, including for GPT-4, whose generated disambiguations are\nconsidered correct only 32% of the time in human evaluation, compared to 90%\nfor disambiguations in our dataset. Finally, to illustrate the value of\nambiguity-sensitive tools, we show that a multilabel NLI model can flag\npolitical claims in the wild that are misleading due to ambiguity. We encourage\nthe field to rediscover the importance of ambiguity for NLP.",
        "pdf_link": "https://arxiv.org/pdf/2304.14399v2.pdf"
    },
    {
        "title": "CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants",
        "authors": [
            "Albert Yu Sun",
            "Varun Nair",
            "Elliot Schumacher",
            "Anitha Kannan"
        ],
        "published": "2023-04-27T17:39:11Z",
        "summary": "A wave of new task-based virtual assistants has been fueled by increasingly\npowerful large language models (LLMs), such as GPT-4 (OpenAI, 2023). A major\nchallenge in deploying LLM-based virtual conversational assistants in real\nworld settings is ensuring they operate within what is admissible for the task.\nTo overcome this challenge, the designers of these virtual assistants rely on\nan independent guardrail system that verifies the virtual assistant's output\naligns with the constraints required for the task. However, relying on commonly\nused, prompt-based guardrails can be difficult to engineer correctly and\ncomprehensively. To address these challenges, we propose CONSCENDI. We use\nCONSCENDI to exhaustively generate training data with two key LLM-powered\ncomponents: scenario-augmented generation and contrastive training examples.\nWhen generating conversational data, we generate a set of rule-breaking\nscenarios, which enumerate a diverse set of high-level ways a rule can be\nviolated. This scenario-guided approach produces a diverse training set and\nprovides chatbot designers greater control. To generate contrastive examples,\nwe prompt the LLM to alter conversations with violations into acceptable\nconversations to enable fine-grained distinctions. We then use this data,\ngenerated by CONSCENDI, to train a smaller model. We find that CONSCENDI\nresults in guardrail models that improve over baselines in multiple dialogue\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2304.14364v2.pdf"
    },
    {
        "title": "Industrial Engineering with Large Language Models: A case study of ChatGPT's performance on Oil & Gas problems",
        "authors": [
            "Oluwatosin Ogundare",
            "Srinath Madasu",
            "Nathanial Wiggins"
        ],
        "published": "2023-04-27T17:33:49Z",
        "summary": "Large Language Models (LLMs) have shown great potential in solving complex\nproblems in various fields, including oil and gas engineering and other\nindustrial engineering disciplines like factory automation, PLC programming\netc. However, automatic identification of strong and weak solutions to\nfundamental physics equations governing several industrial processes remain a\nchallenging task. This paper identifies the limitation of current LLM\napproaches, particularly ChatGPT in selected practical problems native to oil\nand gas engineering but not exclusively. The performance of ChatGPT in solving\ncomplex problems in oil and gas engineering is discussed and the areas where\nLLMs are most effective are presented.",
        "pdf_link": "https://arxiv.org/pdf/2304.14354v1.pdf"
    },
    {
        "title": "ICE-Score: Instructing Large Language Models to Evaluate Code",
        "authors": [
            "Terry Yue Zhuo"
        ],
        "published": "2023-04-27T16:38:17Z",
        "summary": "Recent advancements in the field of natural language generation have\nfacilitated the use of large language models to assess the quality of generated\ntext. Although these models have shown promising results in tasks such as\nmachine translation and summarization, their applicability in code intelligence\ntasks remains limited without human involvement. The complexity of programming\nconcepts required for such tasks makes it difficult to develop evaluation\nmetrics that align with human judgment. Token-matching-based metrics, such as\nBLEU, have demonstrated weak correlations with human practitioners in code\nintelligence tasks. Moreover, utilizing human-written test suites to evaluate\nfunctional correctness can be challenging in domains with low resources. To\novercome these obstacles, we propose \\texttt{ICE-Score}, a new evaluation\nmetric via instructing large language models (LLMs) for code assessments. Our\nmetric addresses the limitations of existing approaches by achieving superior\ncorrelations with functional correctness and human preferences, without the\nneed for test oracles or references. We evaluate the efficacy of our metric on\ntwo different aspects (\\textit{human preference} and \\textit{execution\nsuccess}) and four programming languages. Our results demonstrate that our\nmetric surpasses state-of-the-art metrics for code generation, delivering high\nlevels of accuracy and consistency across various programming languages and\ntasks. We also make our evaluation metric and datasets available to the\npublic\\footnote{\\url{https://github.com/terryyz/ice-score}}, encouraging\nfurther research in evaluating code intelligence tasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.14317v2.pdf"
    },
    {
        "title": "Controlled Text Generation with Natural Language Instructions",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Ethan Wilcox",
            "Ryan Cotterell",
            "Mrinmaya Sachan"
        ],
        "published": "2023-04-27T15:56:34Z",
        "summary": "Large language models generate fluent texts and can follow natural language\ninstructions to solve a wide range of tasks without task-specific training.\nNevertheless, it is notoriously difficult to control their generation to\nsatisfy the various constraints required by different applications. In this\nwork, we present InstructCTG, a controlled text generation framework that\nincorporates different constraints by conditioning on natural language\ndescriptions and demonstrations of the constraints. In particular, we first\nextract the underlying constraints of natural texts through a combination of\noff-the-shelf NLP tools and simple heuristics. We then verbalize the\nconstraints into natural language instructions to form weakly supervised\ntraining data. By prepending natural language descriptions of the constraints\nand a few demonstrations, we fine-tune a pre-trained language model to\nincorporate various types of constraints. Compared to existing search-based or\nscore-based methods, InstructCTG is more flexible to different constraint types\nand has a much smaller impact on the generation quality and speed because it\ndoes not modify the decoding procedure. Additionally, InstructCTG allows the\nmodel to adapt to new constraints without re-training through the use of\nfew-shot task generalization and in-context learning abilities of\ninstruction-tuned language models.",
        "pdf_link": "https://arxiv.org/pdf/2304.14293v2.pdf"
    },
    {
        "title": "Origin Tracing and Detecting of LLMs",
        "authors": [
            "Linyang Li",
            "Pengyu Wang",
            "Ke Ren",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "published": "2023-04-27T10:05:57Z",
        "summary": "The extraordinary performance of large language models (LLMs) heightens the\nimportance of detecting whether the context is generated by an AI system. More\nimportantly, while more and more companies and institutions release their LLMs,\nthe origin can be hard to trace. Since LLMs are heading towards the time of\nAGI, similar to the origin tracing in anthropology, it is of great importance\nto trace the origin of LLMs. In this paper, we first raise the concern of the\norigin tracing of LLMs and propose an effective method to trace and detect\nAI-generated contexts. We introduce a novel algorithm that leverages the\ncontrastive features between LLMs and extracts model-wise features to trace the\ntext origins. Our proposed method works under both white-box and black-box\nsettings therefore can be widely generalized to detect various LLMs.(e.g. can\nbe generalized to detect GPT-3 models without the GPT-3 models). Also, our\nproposed method requires only limited data compared with the supervised\nlearning methods and can be extended to trace new-coming model origins. We\nconstruct extensive experiments to examine whether we can trace the origins of\ngiven texts. We provide valuable observations based on the experimental\nresults, such as the difficulty level of AI origin tracing, and the AI origin\nsimilarities, and call for ethical concerns of LLM providers. We are releasing\nall codes and data as a toolkit and benchmark for future AI origin tracing and\ndetecting studies. \\footnote{We are releasing all available resource at\n\\url{https://github.com/OpenLMLab/}.}",
        "pdf_link": "https://arxiv.org/pdf/2304.14072v1.pdf"
    },
    {
        "title": "Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering",
        "authors": [
            "Xiangyang Liu",
            "Tianqi Pang",
            "Chenyou Fan"
        ],
        "published": "2023-04-27T01:48:03Z",
        "summary": "We investigate how to enhance answer precision in frequently asked questions\nposed by distributed users using cloud-based Large Language Models (LLMs). Our\nstudy focuses on a typical situations where users ask similar queries that\ninvolve identical mathematical reasoning steps and problem-solving procedures.\nDue to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone\nquestions, we propose to improve the distributed synonymous questions using\nSelf-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we\nfirst retrieve synonymous questions from a crowd-sourced database and create a\nfederated question pool. We call these federated synonymous questions with the\nsame or different parameters SP-questions or DP-questions, respectively. We\nrefer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate\nsignificantly more accurate answers for all user queries without requiring\nsophisticated model-tuning. Through extensive experiments, we demonstrate that\nour proposed methods can significantly enhance question accuracy by fully\nexploring the synonymous nature of the questions and the consistency of the\nanswers.",
        "pdf_link": "https://arxiv.org/pdf/2304.13911v2.pdf"
    },
    {
        "title": "The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification Tasks",
        "authors": [
            "Anders Giovanni Møller",
            "Jacob Aarup Dalsgaard",
            "Arianna Pera",
            "Luca Maria Aiello"
        ],
        "published": "2023-04-26T23:09:02Z",
        "summary": "In the realm of Computational Social Science (CSS), practitioners often\nnavigate complex, low-resource domains and face the costly and time-intensive\nchallenges of acquiring and annotating data. We aim to establish a set of\nguidelines to address such challenges, comparing the use of human-labeled data\nwith synthetically generated data from GPT-4 and Llama-2 in ten distinct CSS\nclassification tasks of varying complexity. Additionally, we examine the impact\nof training data sizes on performance. Our findings reveal that models trained\non human-labeled data consistently exhibit superior or comparable performance\ncompared to their synthetically augmented counterparts. Nevertheless, synthetic\naugmentation proves beneficial, particularly in improving performance on rare\nclasses within multi-class tasks. Furthermore, we leverage GPT-4 and Llama-2\nfor zero-shot classification and find that, while they generally display strong\nperformance, they often fall short when compared to specialized classifiers\ntrained on moderately sized training sets.",
        "pdf_link": "https://arxiv.org/pdf/2304.13861v2.pdf"
    },
    {
        "title": "Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery",
        "authors": [
            "Debadutta Dash",
            "Rahul Thapa",
            "Juan M. Banda",
            "Akshay Swaminathan",
            "Morgan Cheatham",
            "Mehr Kashyap",
            "Nikesh Kotecha",
            "Jonathan H. Chen",
            "Saurabh Gombar",
            "Lance Downing",
            "Rachel Pedreira",
            "Ethan Goh",
            "Angel Arnaout",
            "Garret Kenn Morris",
            "Honor Magon",
            "Matthew P Lungren",
            "Eric Horvitz",
            "Nigam H. Shah"
        ],
        "published": "2023-04-26T17:54:28Z",
        "summary": "Despite growing interest in using large language models (LLMs) in healthcare,\ncurrent explorations do not assess the real-world utility and safety of LLMs in\nclinical settings. Our objective was to determine whether two LLMs can serve\ninformation needs submitted by physicians as questions to an informatics\nconsultation service in a safe and concordant manner. Sixty six questions from\nan informatics consult service were submitted to GPT-3.5 and GPT-4 via simple\nprompts. 12 physicians assessed the LLM responses' possibility of patient harm\nand concordance with existing reports from an informatics consultation service.\nPhysician assessments were summarized based on majority vote. For no questions\ndid a majority of physicians deem either LLM response as harmful. For GPT-3.5,\nresponses to 8 questions were concordant with the informatics consult report,\n20 discordant, and 9 were unable to be assessed. There were 29 responses with\nno majority on \"Agree\", \"Disagree\", and \"Unable to assess\". For GPT-4,\nresponses to 13 questions were concordant, 15 discordant, and 3 were unable to\nbe assessed. There were 35 responses with no majority. Responses from both LLMs\nwere largely devoid of overt harm, but less than 20% of the responses agreed\nwith an answer from an informatics consultation service, responses contained\nhallucinated references, and physicians were divided on what constitutes harm.\nThese results suggest that while general purpose LLMs are able to provide safe\nand credible responses, they often do not meet the specific information need of\na given question. A definitive evaluation of the usefulness of LLMs in\nhealthcare settings will likely require additional research on prompt\nengineering, calibration, and custom-tailoring of general purpose models.",
        "pdf_link": "https://arxiv.org/pdf/2304.13714v3.pdf"
    },
    {
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
        "authors": [
            "Jingfeng Yang",
            "Hongye Jin",
            "Ruixiang Tang",
            "Xiaotian Han",
            "Qizhang Feng",
            "Haoming Jiang",
            "Bing Yin",
            "Xia Hu"
        ],
        "published": "2023-04-26T17:52:30Z",
        "summary": "This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",
        "pdf_link": "https://arxiv.org/pdf/2304.13712v2.pdf"
    },
    {
        "title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables",
        "authors": [
            "Matthias Urban",
            "Carsten Binnig"
        ],
        "published": "2023-04-26T13:31:04Z",
        "summary": "In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class\nof database systems that can seamlessly query text and tables using SQL. To\nenable seamless querying of textual data using SQL in an MMDB, we propose to\nextend relational databases with so-called multi-modal operators (MMOps) which\nare based on the advances of recent large language models such as GPT-3. The\nmain idea of MMOps is that they allow text collections to be treated as tables\nwithout the need to manually transform the data. As we show in our evaluation,\nour MMDB prototype can not only outperform state-of-the-art approaches such as\ntext-to-table in terms of accuracy and performance but it also requires\nsignificantly less training data to fine-tune the model for an unseen text\ncollection.",
        "pdf_link": "https://arxiv.org/pdf/2304.13559v2.pdf"
    },
    {
        "title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
        "authors": [
            "Bing Wang",
            "Xinnian Liang",
            "Jian Yang",
            "Hui Huang",
            "Shuangzhi Wu",
            "Peihao Wu",
            "Lu Lu",
            "Zejun Ma",
            "Zhoujun Li"
        ],
        "published": "2023-04-26T07:25:31Z",
        "summary": "Large Language Models (LLMs) are constrained by their inability to process\nlengthy inputs, resulting in the loss of critical historical information. To\naddress this limitation, in this paper, we propose the Self-Controlled Memory\n(SCM) framework to enhance the ability of LLMs to maintain long-term memory and\nrecall relevant information. Our SCM framework comprises three key components:\nan LLM-based agent serving as the backbone of the framework, a memory stream\nstoring agent memories, and a memory controller updating memories and\ndetermining when and how to utilize memories from memory stream. Additionally,\nthe proposed SCM is able to process ultra-long texts without any modification\nor fine-tuning, which can integrate with any instruction following LLMs in a\nplug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the\neffectiveness of SCM for handling lengthy inputs. The annotated dataset covers\nthree tasks: long-term dialogues, book summarization, and meeting\nsummarization. Experimental results demonstrate that our method achieves better\nretrieval recall and generates more informative responses compared to\ncompetitive baselines in long-term dialogues.\n(https://github.com/wbbeyourself/SCM4LLMs)",
        "pdf_link": "https://arxiv.org/pdf/2304.13343v2.pdf"
    },
    {
        "title": "Prompting GPT-3.5 for Text-to-SQL with De-semanticization and Skeleton Retrieval",
        "authors": [
            "Chunxi Guo",
            "Zhiliang Tian",
            "Jintao Tang",
            "Pancheng Wang",
            "Zhihua Wen",
            "Kang Yang",
            "Ting Wang"
        ],
        "published": "2023-04-26T06:02:01Z",
        "summary": "Text-to-SQL is a task that converts a natural language question into a\nstructured query language (SQL) to retrieve information from a database. Large\nlanguage models (LLMs) work well in natural language generation tasks, but they\nare not specifically pre-trained to understand the syntax and semantics of SQL\ncommands. In this paper, we propose an LLM-based framework for Text-to-SQL\nwhich retrieves helpful demonstration examples to prompt LLMs. However,\nquestions with different database schemes can vary widely, even if the\nintentions behind them are similar and the corresponding SQL queries exhibit\nsimilarities. Consequently, it becomes crucial to identify the appropriate SQL\ndemonstrations that align with our requirements. We design a de-semanticization\nmechanism that extracts question skeletons, allowing us to retrieve similar\nexamples based on their structural similarity. We also model the relationships\nbetween question tokens and database schema items (i.e., tables and columns) to\nfilter out scheme-related information. Our framework adapts the range of the\ndatabase schema in prompts to balance length and valuable information. A\nfallback mechanism allows for a more detailed schema to be provided if the\ngenerated SQL query fails. Ours outperforms state-of-the-art models and\ndemonstrates strong generalization ability on three cross-domain Text-to-SQL\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2304.13301v2.pdf"
    },
    {
        "title": "The Closeness of In-Context Learning and Weight Shifting for Softmax Regression",
        "authors": [
            "Shuai Li",
            "Zhao Song",
            "Yu Xia",
            "Tong Yu",
            "Tianyi Zhou"
        ],
        "published": "2023-04-26T04:33:41Z",
        "summary": "Large language models (LLMs) are known for their exceptional performance in\nnatural language processing, making them highly effective in many human\nlife-related or even job-related tasks. The attention mechanism in the\nTransformer architecture is a critical component of LLMs, as it allows the\nmodel to selectively focus on specific input parts. The softmax unit, which is\na key part of the attention mechanism, normalizes the attention scores. Hence,\nthe performance of LLMs in various NLP tasks depends significantly on the\ncrucial role played by the attention mechanism with the softmax unit.\n  In-context learning, as one of the celebrated abilities of recent LLMs, is an\nimportant concept in querying LLMs such as ChatGPT. Without further parameter\nupdates, Transformers can learn to predict based on few in-context examples.\nHowever, the reason why Transformers becomes in-context learners is not well\nunderstood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the\nin-context learning from a mathematical perspective based on a linear\nregression formulation $\\min_x\\| Ax - b \\|_2$, which show Transformers'\ncapability of learning linear functions in context.\n  In this work, we study the in-context learning based on a softmax regression\nformulation $\\min_{x} \\| \\langle \\exp(Ax), {\\bf 1}_n \\rangle^{-1} \\exp(Ax) - b\n\\|_2$ of Transformer's attention mechanism. We show the upper bounds of the\ndata transformations induced by a single self-attention layer and by\ngradient-descent on a $\\ell_2$ regression loss for softmax prediction function,\nwhich imply that when training self-attention-only Transformers for fundamental\nregression tasks, the models learned by gradient-descent and Transformers show\ngreat similarity.",
        "pdf_link": "https://arxiv.org/pdf/2304.13276v1.pdf"
    },
    {
        "title": "The Internal State of an LLM Knows When It's Lying",
        "authors": [
            "Amos Azaria",
            "Tom Mitchell"
        ],
        "published": "2023-04-26T02:49:38Z",
        "summary": "While Large Language Models (LLMs) have shown exceptional performance in\nvarious tasks, one of their most prominent drawbacks is generating inaccurate\nor false information with a confident tone. In this paper, we provide evidence\nthat the LLM's internal state can be used to reveal the truthfulness of\nstatements. This includes both statements provided to the LLM, and statements\nthat the LLM itself generates. Our approach is to train a classifier that\noutputs the probability that a statement is truthful, based on the hidden layer\nactivations of the LLM as it reads or generates the statement. Experiments\ndemonstrate that given a set of test sentences, of which half are true and half\nfalse, our trained classifier achieves an average of 71\\% to 83\\% accuracy\nlabeling which sentences are true versus false, depending on the LLM base\nmodel. Furthermore, we explore the relationship between our classifier's\nperformance and approaches based on the probability assigned to the sentence by\nthe LLM. We show that while LLM-assigned sentence probability is related to\nsentence truthfulness, this probability is also dependent on sentence length\nand the frequencies of words in the sentence, resulting in our trained\nclassifier providing a more reliable approach to detecting truthfulness,\nhighlighting its potential to enhance the reliability of LLM-generated content\nand its practical applicability in real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.13734v2.pdf"
    },
    {
        "title": "Towards Reliable Colorectal Cancer Polyps Classification via Vision Based Tactile Sensing and Confidence-Calibrated Neural Networks",
        "authors": [
            "Siddhartha Kapuria",
            "Tarunraj G. Mohanraj",
            "Nethra Venkatayogi",
            "Ozdemir Can Kara",
            "Yuki Hirata",
            "Patrick Minot",
            "Ariel Kapusta",
            "Naruhiko Ikoma",
            "Farshid Alambeigi"
        ],
        "published": "2023-04-25T23:18:13Z",
        "summary": "In this study, toward addressing the over-confident outputs of existing\nartificial intelligence-based colorectal cancer (CRC) polyp classification\ntechniques, we propose a confidence-calibrated residual neural network.\nUtilizing a novel vision-based tactile sensing (VS-TS) system and unique CRC\npolyp phantoms, we demonstrate that traditional metrics such as accuracy and\nprecision are not sufficient to encapsulate model performance for handling a\nsensitive CRC polyp diagnosis. To this end, we develop a residual neural\nnetwork classifier and address its over-confident outputs for CRC polyps\nclassification via the post-processing method of temperature scaling. To\nevaluate the proposed method, we introduce noise and blur to the obtained\ntextural images of the VS-TS and test the model's reliability for non-ideal\ninputs through reliability diagrams and other statistical metrics.",
        "pdf_link": "https://arxiv.org/pdf/2304.13192v1.pdf"
    },
    {
        "title": "TABLET: Learning From Instructions For Tabular Data",
        "authors": [
            "Dylan Slack",
            "Sameer Singh"
        ],
        "published": "2023-04-25T23:07:20Z",
        "summary": "Acquiring high-quality data is often a significant challenge in training\nmachine learning (ML) models for tabular prediction, particularly in\nprivacy-sensitive and costly domains like medicine and finance. Providing\nnatural language instructions to large language models (LLMs) offers an\nalternative solution. However, it is unclear how effectively instructions\nleverage the knowledge in LLMs for solving tabular prediction problems. To\naddress this gap, we introduce TABLET, a benchmark of 20 diverse tabular\ndatasets annotated with instructions that vary in their phrasing, granularity,\nand technicality. Additionally, TABLET includes the instructions' logic and\nstructured modifications to the instructions. We find in-context instructions\nincrease zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for\nChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular\nprediction in our benchmark by evaluating instruction faithfulness. We find\nLLMs often ignore instructions and fail to predict specific instances\ncorrectly, even with examples. Our analysis on TABLET shows that, while\ninstructions help LLM performance, learning from instructions for tabular data\nrequires new capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2304.13188v1.pdf"
    },
    {
        "title": "AI-assisted coding: Experiments with GPT-4",
        "authors": [
            "Russell A Poldrack",
            "Thomas Lu",
            "Gašper Beguš"
        ],
        "published": "2023-04-25T22:59:01Z",
        "summary": "Artificial intelligence (AI) tools based on large language models have\nacheived human-level performance on some computer programming tasks. We report\nseveral experiments using GPT-4 to generate computer code. These experiments\ndemonstrate that AI code generation using the current generation of tools,\nwhile powerful, requires substantial human validation to ensure accurate\nperformance. We also demonstrate that GPT-4 refactoring of existing code can\nsignificantly improve that code along several established metrics for code\nquality, and we show that GPT-4 can generate tests with substantial coverage,\nbut that many of the tests fail when applied to the associated code. These\nfindings suggest that while AI coding tools are very powerful, they still\nrequire humans in the loop to ensure validity and accuracy of the results.",
        "pdf_link": "https://arxiv.org/pdf/2304.13187v1.pdf"
    },
    {
        "title": "The Potential of Visual ChatGPT For Remote Sensing",
        "authors": [
            "Lucas Prado Osco",
            "Eduardo Lopes de Lemos",
            "Wesley Nunes Gonçalves",
            "Ana Paula Marques Ramos",
            "José Marcato Junior"
        ],
        "published": "2023-04-25T17:29:47Z",
        "summary": "Recent advancements in Natural Language Processing (NLP), particularly in\nLarge Language Models (LLMs), associated with deep learning-based computer\nvision techniques, have shown substantial potential for automating a variety of\ntasks. One notable model is Visual ChatGPT, which combines ChatGPT's LLM\ncapabilities with visual computation to enable effective image analysis. The\nmodel's ability to process images based on textual inputs can revolutionize\ndiverse fields. However, its application in the remote sensing domain remains\nunexplored. This is the first paper to examine the potential of Visual ChatGPT,\na cutting-edge LLM founded on the GPT architecture, to tackle the aspects of\nimage processing related to the remote sensing domain. Among its current\ncapabilities, Visual ChatGPT can generate textual descriptions of images,\nperform canny edge and straight line detection, and conduct image segmentation.\nThese offer valuable insights into image content and facilitate the\ninterpretation and extraction of information. By exploring the applicability of\nthese techniques within publicly available datasets of satellite images, we\ndemonstrate the current model's limitations in dealing with remote sensing\nimages, highlighting its challenges and future prospects. Although still in\nearly development, we believe that the combination of LLMs and visual models\nholds a significant potential to transform remote sensing image processing,\ncreating accessible and practical application opportunities in the field.",
        "pdf_link": "https://arxiv.org/pdf/2304.13009v2.pdf"
    },
    {
        "title": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head",
        "authors": [
            "Rongjie Huang",
            "Mingze Li",
            "Dongchao Yang",
            "Jiatong Shi",
            "Xuankai Chang",
            "Zhenhui Ye",
            "Yuning Wu",
            "Zhiqing Hong",
            "Jiawei Huang",
            "Jinglin Liu",
            "Yi Ren",
            "Zhou Zhao",
            "Shinji Watanabe"
        ],
        "published": "2023-04-25T17:05:38Z",
        "summary": "Large language models (LLMs) have exhibited remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. Despite the recent success, current LLMs are not capable of\nprocessing complex audio information or conducting spoken conversations (like\nSiri or Alexa). In this work, we propose a multi-modal AI system named\nAudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to\nprocess complex audio information and solve numerous understanding and\ngeneration tasks; and 2) the input/output interface (ASR, TTS) to support\nspoken dialogue. With an increasing demand to evaluate multi-modal LLMs of\nhuman intention understanding and cooperation with foundation models, we\noutline the principles and processes and test AudioGPT in terms of consistency,\ncapability, and robustness. Experimental results demonstrate the capabilities\nof AudioGPT in solving AI tasks with speech, music, sound, and talking head\nunderstanding and generation in multi-round dialogues, which empower humans to\ncreate rich and diverse audio content with unprecedented ease. Our system is\npublicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.",
        "pdf_link": "https://arxiv.org/pdf/2304.12995v1.pdf"
    },
    {
        "title": "What's in a Name? Evaluating Assembly-Part Semantic Knowledge in Language Models through User-Provided Names in CAD Files",
        "authors": [
            "Peter Meltzer",
            "Joseph G. Lambourne",
            "Daniele Grandi"
        ],
        "published": "2023-04-25T12:30:01Z",
        "summary": "Semantic knowledge of part-part and part-whole relationships in assemblies is\nuseful for a variety of tasks from searching design repositories to the\nconstruction of engineering knowledge bases. In this work we propose that the\nnatural language names designers use in Computer Aided Design (CAD) software\nare a valuable source of such knowledge, and that Large Language Models (LLMs)\ncontain useful domain-specific information for working with this data as well\nas other CAD and engineering-related tasks.\n  In particular we extract and clean a large corpus of natural language part,\nfeature and document names and use this to quantitatively demonstrate that a\npre-trained language model can outperform numerous benchmarks on three\nself-supervised tasks, without ever having seen this data before. Moreover, we\nshow that fine-tuning on the text data corpus further boosts the performance on\nall tasks, thus demonstrating the value of the text data which until now has\nbeen largely ignored. We also identify key limitations to using LLMs with text\ndata alone, and our findings provide a strong motivation for further work into\nmulti-modal text-geometry models.\n  To aid and encourage further work in this area we make all our data and code\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2304.14275v1.pdf"
    },
    {
        "title": "Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting",
        "authors": [
            "Jianzhang Zhang",
            "Yiyang Chen",
            "Nan Niu",
            "Yinglin Wang",
            "Chuang Liu"
        ],
        "published": "2023-04-25T04:09:45Z",
        "summary": "Recently, various illustrative examples have shown the impressive ability of\ngenerative large language models (LLMs) to perform NLP related tasks. ChatGPT\nundoubtedly is the most representative model. We empirically evaluate ChatGPT's\nperformance on requirements information retrieval (IR) tasks to derive insights\ninto designing or developing more effective requirements retrieval methods or\ntools based on generative LLMs. We design an evaluation framework considering\nfour different combinations of two popular IR tasks and two common artifact\ntypes. Under zero-shot setting, evaluation results reveal ChatGPT's promising\nability to retrieve requirements relevant information (high recall) and limited\nability to retrieve more specific requirements information (low precision). Our\nevaluation of ChatGPT on requirements IR under zero-shot setting provides\npreliminary evidence for designing or developing more effective requirements IR\nmethods or tools based on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.12562v2.pdf"
    },
    {
        "title": "Improved Trust in Human-Robot Collaboration with ChatGPT",
        "authors": [
            "Yang Ye",
            "Hengxu You",
            "Jing Du"
        ],
        "published": "2023-04-25T02:48:35Z",
        "summary": "Human robot collaboration is becoming increasingly important as robots become\nmore involved in various aspects of human life in the era of Artificial\nIntelligence. However, the issue of human operators trust in robots remains a\nsignificant concern, primarily due to the lack of adequate semantic\nunderstanding and communication between humans and robots. The emergence of\nLarge Language Models (LLMs), such as ChatGPT, provides an opportunity to\ndevelop an interactive, communicative, and robust human-robot collaboration\napproach. This paper explores the impact of ChatGPT on trust in a human-robot\ncollaboration assembly task. This study designs a robot control system called\nRoboGPT using ChatGPT to control a 7-degree-of-freedom robot arm to help human\noperators fetch, and place tools, while human operators can communicate with\nand control the robot arm using natural language. A human-subject experiment\nshowed that incorporating ChatGPT in robots significantly increased trust in\nhuman-robot collaboration, which can be attributed to the robot's ability to\ncommunicate more effectively with humans. Furthermore, ChatGPT ability to\nunderstand the nuances of human language and respond appropriately helps to\nbuild a more natural and intuitive human-robot interaction. The findings of\nthis study have significant implications for the development of human-robot\ncollaboration systems.",
        "pdf_link": "https://arxiv.org/pdf/2304.12529v1.pdf"
    },
    {
        "title": "Semantic Compression With Large Language Models",
        "authors": [
            "Henry Gilbert",
            "Michael Sandborn",
            "Douglas C. Schmidt",
            "Jesse Spencer-Smith",
            "Jules White"
        ],
        "published": "2023-04-25T01:47:05Z",
        "summary": "The rise of large language models (LLMs) is revolutionizing information\nretrieval, question answering, summarization, and code generation tasks.\nHowever, in addition to confidently presenting factually inaccurate information\nat times (known as \"hallucinations\"), LLMs are also inherently limited by the\nnumber of input and output tokens that can be processed at once, making them\npotentially less effective on tasks that require processing a large set or\ncontinuous stream of information. A common approach to reducing the size of\ndata is through lossless or lossy compression. Yet, in some cases it may not be\nstrictly necessary to perfectly recover every detail from the original data, as\nlong as a requisite level of semantic precision or intent is conveyed.\n  This paper presents three contributions to research on LLMs. First, we\npresent the results from experiments exploring the viability of approximate\ncompression using LLMs, focusing specifically on GPT-3.5 and GPT-4 via ChatGPT\ninterfaces. Second, we investigate and quantify the capability of LLMs to\ncompress text and code, as well as to recall and manipulate compressed\nrepresentations of prompts. Third, we present two novel metrics -- Exact\nReconstructive Effectiveness (ERE) and Semantic Reconstruction Effectiveness\n(SRE) -- that quantify the level of preserved intent between text compressed\nand decompressed by the LLMs we studied. Our initial results indicate that\nGPT-4 can effectively compress and reconstruct text while preserving the\nsemantic essence of the original text, providing a path to leverage\n$\\sim$5$\\times$ more tokens than present limits allow.",
        "pdf_link": "https://arxiv.org/pdf/2304.12512v1.pdf"
    },
    {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "authors": [
            "Can Xu",
            "Qingfeng Sun",
            "Kai Zheng",
            "Xiubo Geng",
            "Pu Zhao",
            "Jiazhan Feng",
            "Chongyang Tao",
            "Daxin Jiang"
        ],
        "published": "2023-04-24T16:31:06Z",
        "summary": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM",
        "pdf_link": "https://arxiv.org/pdf/2304.12244v2.pdf"
    },
    {
        "title": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering",
        "authors": [
            "Yucheng Li"
        ],
        "published": "2023-04-24T13:55:47Z",
        "summary": "Large language models (LLMs) have received significant attention by achieving\nremarkable performance across various tasks. However, their fixed context\nlength poses challenges when processing long documents or maintaining extended\nconversations. This paper proposes a method called \\textit{Selective Context}\nthat employs self-information to filter out less informative content, thereby\nenhancing the efficiency of the fixed context length. We demonstrate the\neffectiveness of our approach on tasks of summarisation and question answering\nacross different data sources, including academic papers, news articles, and\nconversation transcripts.",
        "pdf_link": "https://arxiv.org/pdf/2304.12102v1.pdf"
    },
    {
        "title": "ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain",
        "authors": [
            "Philipp Kuehn",
            "Mike Schmidt",
            "Markus Bayer",
            "Christian Reuter"
        ],
        "published": "2023-04-24T09:53:33Z",
        "summary": "Publicly available information contains valuable information for Cyber Threat\nIntelligence (CTI). This can be used to prevent attacks that have already taken\nplace on other systems. Ideally, only the initial attack succeeds and all\nsubsequent ones are detected and stopped. But while there are different\nstandards to exchange this information, a lot of it is shared in articles or\nblog posts in non-standardized ways. Manually scanning through multiple online\nportals and news pages to discover new threats and extracting them is a\ntime-consuming task. To automize parts of this scanning process, multiple\npapers propose extractors that use Natural Language Processing (NLP) to extract\nIndicators of Compromise (IOCs) from documents. However, while this already\nsolves the problem of extracting the information out of documents, the search\nfor these documents is rarely considered. In this paper, a new focused crawler\nis proposed called ThreatCrawl, which uses Bidirectional Encoder\nRepresentations from Transformers (BERT)-based models to classify documents and\nadapt its crawling path dynamically. While ThreatCrawl has difficulties to\nclassify the specific type of Open Source Intelligence (OSINT) named in texts,\ne.g., IOC content, it can successfully find relevant documents and modify its\npath accordingly. It yields harvest rates of up to 52%, which are, to the best\nof our knowledge, better than the current state of the art.",
        "pdf_link": "https://arxiv.org/pdf/2304.11960v2.pdf"
    },
    {
        "title": "Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology",
        "authors": [
            "Yixing Huang",
            "Ahmed Gomaa",
            "Sabine Semrau",
            "Marlen Haderlein",
            "Sebastian Lettmaier",
            "Thomas Weissmann",
            "Johanna Grigo",
            "Hassen Ben Tkhayat",
            "Benjamin Frey",
            "Udo S. Gaipl",
            "Luitpold V. Distel",
            "Andreas Maier",
            "Rainer Fietkau",
            "Christoph Bert",
            "Florian Putz"
        ],
        "published": "2023-04-24T09:50:39Z",
        "summary": "The potential of large language models in medicine for education and decision\nmaking purposes has been demonstrated as they achieve decent scores on medical\nexams such as the United States Medical Licensing Exam (USMLE) and the MedQA\nexam. In this work, we evaluate the performance of ChatGPT-4 in the specialized\nfield of radiation oncology using the 38th American College of Radiology (ACR)\nradiation oncology in-training (TXIT) exam and the 2022 Red Journal Gray Zone\ncases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of\n63.65% and 74.57%, respectively, highlighting the advantage of the latest\nChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in\nradiation oncology are identified to some extent. Specifically, ChatGPT-4\ndemonstrates better knowledge of statistics, CNS & eye, pediatrics, biology,\nand physics than knowledge of bone & soft tissue and gynecology, as per the ACR\nknowledge domain. Regarding clinical care paths, ChatGPT-4 performs better in\ndiagnosis, prognosis, and toxicity than brachytherapy and dosimetry. It lacks\nproficiency in in-depth details of clinical trials. For the Gray Zone cases,\nChatGPT-4 is able to suggest a personalized treatment approach to each case\nwith high correctness and comprehensiveness. Importantly, it provides novel\ntreatment aspects for many cases, which are not suggested by any human experts.\nBoth evaluations demonstrate the potential of ChatGPT-4 in medical education\nfor the general public and cancer patients, as well as the potential to aid\nclinical decision-making, while acknowledging its limitations in certain\ndomains. Because of the risk of hallucination, facts provided by ChatGPT always\nneed to be verified.",
        "pdf_link": "https://arxiv.org/pdf/2304.11957v4.pdf"
    },
    {
        "title": "Is ChatGPT the Ultimate Programming Assistant -- How far is it?",
        "authors": [
            "Haoye Tian",
            "Weiqi Lu",
            "Tsz On Li",
            "Xunzhu Tang",
            "Shing-Chi Cheung",
            "Jacques Klein",
            "Tegawendé F. Bissyandé"
        ],
        "published": "2023-04-24T09:20:13Z",
        "summary": "Recently, the ChatGPT LLM has received great attention: it can be used as a\nbot for discussing source code, prompting it to suggest changes, provide\ndescriptions or even generate code. Typical demonstrations generally focus on\nexisting benchmarks, which may have been used in model training (i.e., data\nleakage). To assess the feasibility of using an LLM as a useful assistant bot\nfor programmers, we must assess its realistic capabilities on unseen problems\nas well as its capabilities on various tasks. In this paper, we present an\nempirical study of ChatGPT's potential as a fully automated programming\nassistant, focusing on the tasks of code generation, program repair, and code\nsummariziation. The study investigates ChatGPT's performance on common\nprogramming problems and compares it with state-of-the-art approaches on two\nbenchmarks. Among several findings, our study shows that ChatGPT is effective\nin dealing with common programming problems. However, our experiments also\nreveal limitations in terms of its attention span: detailed descriptions will\nconstrain the focus of ChatGPT and prevent it from leveraging its vast\nknowledge to solve the actual problem. Surprisingly, we have identified the\nability of ChatGPT to reason the original intention of the code. We expect\nfuture work to build on this insight for dealing with the open question of the\noracle problem. Our findings contribute interesting insights to the development\nof LLMs for programming assistance, notably by demonstrating the importance of\nprompt engineering, and providing a better understanding of ChatGPT's practical\napplications for software engineering.",
        "pdf_link": "https://arxiv.org/pdf/2304.11938v2.pdf"
    },
    {
        "title": "Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT",
        "authors": [
            "Ruohong Zhang",
            "Yau-Shian Wang",
            "Yiming Yang"
        ],
        "published": "2023-04-24T07:35:38Z",
        "summary": "Moreover, GPT-based zero-shot classification models tend to make independent\npredictions over test instances, which can be sub-optimal as the instance\ncorrelations and the decision boundaries in the target space are ignored. To\naddress these difficulties and limitations, we propose a new approach to\nzero-shot text classification, namely \\ourmodelshort, which leverages the\nstrong generative power of GPT to assist in training a smaller, more adaptable,\nand efficient sentence encoder classifier with contrastive self-training.\nSpecifically, GenCo applies GPT in two ways: firstly, it generates multiple\naugmented texts for each input instance to enhance the semantic embedding of\nthe instance and improve the mapping to relevant labels; secondly, it generates\naugmented texts conditioned on the predicted label during self-training, which\nmakes the generative process tailored to the decision boundaries in the target\nspace. In our experiments, GenCo outperforms previous state-of-the-art methods\non multiple benchmark datasets, even when only limited in-domain text data is\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2304.11872v1.pdf"
    },
    {
        "title": "PARAGRAPH2GRAPH: A GNN-based framework for layout paragraph analysis",
        "authors": [
            "Shu Wei",
            "Nuo Xu"
        ],
        "published": "2023-04-24T03:54:48Z",
        "summary": "Document layout analysis has a wide range of requirements across various\ndomains, languages, and business scenarios. However, most current\nstate-of-the-art algorithms are language-dependent, with architectures that\nrely on transformer encoders or language-specific text encoders, such as BERT,\nfor feature extraction. These approaches are limited in their ability to handle\nvery long documents due to input sequence length constraints and are closely\ntied to language-specific tokenizers. Additionally, training a cross-language\ntext encoder can be challenging due to the lack of labeled multilingual\ndocument datasets that consider privacy. Furthermore, some layout tasks require\na clean separation between different layout components without overlap, which\ncan be difficult for image segmentation-based algorithms to achieve. In this\npaper, we present Paragraph2Graph, a language-independent graph neural network\n(GNN)-based model that achieves competitive results on common document layout\ndatasets while being adaptable to business scenarios with strict separation.\nWith only 19.95 million parameters, our model is suitable for industrial\napplications, particularly in multi-language scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.11810v1.pdf"
    },
    {
        "title": "A Lightweight Constrained Generation Alternative for Query-focused Summarization",
        "authors": [
            "Zhichao Xu",
            "Daniel Cohen"
        ],
        "published": "2023-04-23T18:43:48Z",
        "summary": "Query-focused summarization (QFS) aims to provide a summary of a document\nthat satisfies information need of a given query and is useful in various IR\napplications, such as abstractive snippet generation. Current QFS approaches\ntypically involve injecting additional information, e.g. query-answer relevance\nor fine-grained token-level interaction between a query and document, into a\nfinetuned large language model. However, these approaches often require extra\nparameters \\& training, and generalize poorly to new dataset distributions. To\nmitigate this, we propose leveraging a recently developed constrained\ngeneration model Neurological Decoding (NLD) as an alternative to current QFS\nregimes which rely on additional sub-architectures and training. We first\nconstruct lexical constraints by identifying important tokens from the document\nusing a lightweight gradient attribution model, then subsequently force the\ngenerated summary to satisfy these constraints by directly manipulating the\nfinal vocabulary likelihood. This lightweight approach requires no additional\nparameters or finetuning as it utilizes both an off-the-shelf neural retrieval\nmodel to construct the constraints and a standard generative language model to\nproduce the QFS. We demonstrate the efficacy of this approach on two public QFS\ncollections achieving near parity with the state-of-the-art model with\nsubstantially reduced complexity.",
        "pdf_link": "https://arxiv.org/pdf/2304.11721v1.pdf"
    },
    {
        "title": "Domain Mastery Benchmark: An Ever-Updating Benchmark for Evaluating Holistic Domain Knowledge of Large Language Model--A Preliminary Release",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Haoning Ye",
            "Lin Zhang",
            "Zhuozhi Xiong",
            "Zihan Li",
            "Qianyu He",
            "Sihang Jiang",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2023-04-23T15:11:49Z",
        "summary": "Domain knowledge refers to the in-depth understanding, expertise, and\nfamiliarity with a specific subject, industry, field, or area of special\ninterest. The existing benchmarks are all lack of an overall design for domain\nknowledge evaluation. Holding the belief that the real ability of domain\nlanguage understanding can only be fairly evaluated by an comprehensive and\nin-depth benchmark, we introduces the Domma, a Domain Mastery Benchmark. DomMa\ntargets at testing Large Language Models (LLMs) on their domain knowledge\nunderstanding, it features extensive domain coverage, large data volume, and a\ncontinually updated data set based on Chinese 112 first-level subject\nclassifications. DomMa consist of 100,000 questions in both Chinese and English\nsourced from graduate entrance examinations and undergraduate exams in Chinese\ncollege. We have also propose designs to make benchmark and evaluation process\nmore suitable to LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.11679v2.pdf"
    },
    {
        "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
        "authors": [
            "Jiashuo Sun",
            "Yi Luo",
            "Yeyun Gong",
            "Chen Lin",
            "Yelong Shen",
            "Jian Guo",
            "Nan Duan"
        ],
        "published": "2023-04-23T13:54:39Z",
        "summary": "Large language models (LLMs) can achieve highly effective performance on\nvarious reasoning tasks by incorporating step-by-step chain-of-thought (CoT)\nprompting as demonstrations. However, the reasoning chains of demonstrations\ngenerated by LLMs are prone to errors, which can subsequently lead to incorrect\nreasoning during inference. Furthermore, inappropriate exemplars (overly\nsimplistic or complex), can affect overall performance among varying levels of\ndifficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts\nPrompting), an iterative bootstrapping approach for selecting exemplars and\ngenerating reasoning chains. By utilizing iterative bootstrapping, our approach\nenables LLMs to autonomously rectify errors, resulting in more precise and\ncomprehensive reasoning chains. Simultaneously, our approach selects\nchallenging yet answerable questions accompanied by reasoning chains as\nexemplars with a moderate level of difficulty, which enhances the LLMs'\ngeneralizability across varying levels of difficulty. Experimental results\nindicate that Iter-CoT exhibits superiority, achieving competitive performance\nacross three distinct reasoning tasks on ten datasets.",
        "pdf_link": "https://arxiv.org/pdf/2304.11657v3.pdf"
    },
    {
        "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
        "authors": [
            "Bo Li",
            "Gexiang Fang",
            "Yang Yang",
            "Quansen Wang",
            "Wei Ye",
            "Wen Zhao",
            "Shikun Zhang"
        ],
        "published": "2023-04-23T12:33:18Z",
        "summary": "The capability of Large Language Models (LLMs) like ChatGPT to comprehend\nuser intent and provide reasonable responses has made them extremely popular\nlately. In this paper, we focus on assessing the overall ability of ChatGPT\nusing 7 fine-grained information extraction (IE) tasks. Specially, we present\nthe systematically analysis by measuring ChatGPT's performance, explainability,\ncalibration, and faithfulness, and resulting in 15 keys from either the ChatGPT\nor domain experts. Our findings reveal that ChatGPT's performance in\nStandard-IE setting is poor, but it surprisingly exhibits excellent performance\nin the OpenIE setting, as evidenced by human evaluation. In addition, our\nresearch indicates that ChatGPT provides high-quality and trustworthy\nexplanations for its decisions. However, there is an issue of ChatGPT being\noverconfident in its predictions, which resulting in low calibration.\nFurthermore, ChatGPT demonstrates a high level of faithfulness to the original\ntext in the majority of cases. We manually annotate and release the test sets\nof 7 fine-grained IE tasks contains 14 datasets to further promote the\nresearch. The datasets and code are available at\nhttps://github.com/pkuserc/ChatGPT_for_IE.",
        "pdf_link": "https://arxiv.org/pdf/2304.11633v1.pdf"
    },
    {
        "title": "Differentiate ChatGPT-generated and Human-written Medical Texts",
        "authors": [
            "Wenxiong Liao",
            "Zhengliang Liu",
            "Haixing Dai",
            "Shaochen Xu",
            "Zihao Wu",
            "Yiyang Zhang",
            "Xiaoke Huang",
            "Dajiang Zhu",
            "Hongmin Cai",
            "Tianming Liu",
            "Xiang Li"
        ],
        "published": "2023-04-23T07:38:07Z",
        "summary": "Background: Large language models such as ChatGPT are capable of generating\ngrammatically perfect and human-like text content, and a large number of\nChatGPT-generated texts have appeared on the Internet. However, medical texts\nsuch as clinical notes and diagnoses require rigorous validation, and erroneous\nmedical content generated by ChatGPT could potentially lead to disinformation\nthat poses significant harm to healthcare and the general public.\n  Objective: This research is among the first studies on responsible and\nethical AIGC (Artificial Intelligence Generated Content) in medicine. We focus\non analyzing the differences between medical texts written by human experts and\ngenerated by ChatGPT, and designing machine learning workflows to effectively\ndetect and differentiate medical texts generated by ChatGPT.\n  Methods: We first construct a suite of datasets containing medical texts\nwritten by human experts and generated by ChatGPT. In the next step, we analyze\nthe linguistic features of these two types of content and uncover differences\nin vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. Finally,\nwe design and implement machine learning methods to detect medical text\ngenerated by ChatGPT.\n  Results: Medical texts written by humans are more concrete, more diverse, and\ntypically contain more useful information, while medical texts generated by\nChatGPT pay more attention to fluency and logic, and usually express general\nterminologies rather than effective information specific to the context of the\nproblem. A BERT-based model can effectively detect medical texts generated by\nChatGPT, and the F1 exceeds 95%.",
        "pdf_link": "https://arxiv.org/pdf/2304.11567v1.pdf"
    },
    {
        "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL",
        "authors": [
            "Xiping Liu",
            "Zhao Tan"
        ],
        "published": "2023-04-23T06:52:35Z",
        "summary": "Chain-of-thought (CoT) prompting combined with large language models (LLMs)\nhave achieved encouraging results on complex reasoning tasks. Text-to-SQL is a\ncritical semantic parsing task that converts natural language questions into\nSQL statements, involving a complex reasoning process. However, there is little\nwork about using CoT prompting to activate LLM's reasoning capabilities on\nText-to-SQL tasks. In this work, we propose a new paradigm for prompting\nText-to-SQL tasks, called Divide-and-Prompt, which first divides the task into\nsubtasks, and then approach each subtask through CoT. We present 3\nprompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments\nshow that these prompts guide LLMs to generate Text-to-SQL with higher\nexecution accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2304.11556v1.pdf"
    },
    {
        "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
        "authors": [
            "Shima Rahimi Moghaddam",
            "Christopher J. Honey"
        ],
        "published": "2023-04-22T22:50:50Z",
        "summary": "Large language models (LLMs) excel in many tasks in 2023, but they still face\nchallenges in complex reasoning. Theory-of-mind (ToM) tasks, which require\nunderstanding agents' beliefs, goals, and mental states, are essential for\ncommon-sense reasoning involving humans, making it crucial to enhance LLM\nperformance in this area. This study measures the ToM performance of GPT-4 and\nthree GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates\nthe effectiveness of in-context learning in improving their ToM comprehension.\nWe evaluated prompts featuring two-shot chain of thought reasoning and\nstep-by-step thinking instructions. We found that LLMs trained with\nReinforcement Learning from Human Feedback (RLHF) (all models excluding\nDavinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed\nbest in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell\nshort of the 87% human accuracy on the test set. However, when supplied with\nprompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM\naccuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate\nprompting enhances LLM ToM reasoning, and they underscore the context-dependent\nnature of LLM cognitive capacities.",
        "pdf_link": "https://arxiv.org/pdf/2304.11490v3.pdf"
    },
    {
        "title": "LLM+P: Empowering Large Language Models with Optimal Planning Proficiency",
        "authors": [
            "Bo Liu",
            "Yuqian Jiang",
            "Xiaohan Zhang",
            "Qiang Liu",
            "Shiqi Zhang",
            "Joydeep Biswas",
            "Peter Stone"
        ],
        "published": "2023-04-22T20:34:03Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable zero-shot\ngeneralization abilities: state-of-the-art chatbots can provide plausible\nanswers to many common questions that arise in daily life. However, so far,\nLLMs cannot reliably solve long-horizon planning problems. By contrast,\nclassical planners, once a problem is given in a formatted way, can use\nefficient search algorithms to quickly identify correct, or even optimal,\nplans. In an effort to get the best of both worlds, this paper introduces\nLLM+P, the first framework that incorporates the strengths of classical\nplanners into LLMs. LLM+P takes in a natural language description of a planning\nproblem, then returns a correct (or optimal) plan for solving that problem in\nnatural language. LLM+P does so by first converting the language description\ninto a file written in the planning domain definition language (PDDL), then\nleveraging classical planners to quickly find a solution, and then translating\nthe found solution back into natural language. Along with LLM+P, we define a\ndiverse set of different benchmark problems taken from common planning\nscenarios. Via a comprehensive set of experiments on these benchmark problems,\nwe find that LLM+P is able to provide optimal solutions for most problems,\nwhile LLMs fail to provide even feasible plans for most problems.\\footnote{The\ncode and results are publicly available at\nhttps://github.com/Cranial-XIX/llm-pddl.git.",
        "pdf_link": "https://arxiv.org/pdf/2304.11477v3.pdf"
    },
    {
        "title": "N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models",
        "authors": [
            "Alex Foote",
            "Neel Nanda",
            "Esben Kran",
            "Ionnis Konstas",
            "Fazl Barez"
        ],
        "published": "2023-04-22T19:06:13Z",
        "summary": "Understanding the function of individual neurons within language models is\nessential for mechanistic interpretability research. We propose $\\textbf{Neuron\nto Graph (N2G)}$, a tool which takes a neuron and its dataset examples, and\nautomatically distills the neuron's behaviour on those examples to an\ninterpretable graph. This presents a less labour intensive approach to\ninterpreting neurons than current manual methods, that will better scale these\nmethods to Large Language Models (LLMs). We use truncation and saliency methods\nto only present the important tokens, and augment the dataset examples with\nmore diverse samples to better capture the extent of neuron behaviour. These\ngraphs can be visualised to aid manual interpretation by researchers, but can\nalso output token activations on text to compare to the neuron's ground truth\nactivations for automatic validation. N2G represents a step towards scalable\ninterpretability methods by allowing us to convert neurons in an LLM to\ninterpretable representations of measurable quality.",
        "pdf_link": "https://arxiv.org/pdf/2304.12918v1.pdf"
    },
    {
        "title": "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
        "authors": [
            "Byung-Doh Oh",
            "William Schuler"
        ],
        "published": "2023-04-22T12:50:49Z",
        "summary": "Recent psycholinguistic studies have drawn conflicting conclusions about the\nrelationship between the quality of a language model and the ability of its\nsurprisal estimates to predict human reading times, which has been speculated\nto be due to the large gap in both the amount of training data and model\ncapacity across studies. The current work aims to consolidate these findings by\nevaluating surprisal estimates from Transformer-based language model variants\nthat vary systematically in the amount of training data and model capacity on\ntheir ability to predict human reading times. The results show that surprisal\nestimates from most variants with contemporary model capacities provide the\nbest fit after seeing about two billion training tokens, after which they begin\nto diverge from humanlike expectations. Additionally, newly-trained smaller\nmodel variants reveal a 'tipping point' at convergence, after which the\ndecrease in language model perplexity begins to result in poorer fits to human\nreading times. These results suggest that the massive amount of training data\nis mainly responsible for the poorer fit achieved by surprisal from larger\npre-trained language models, and that a certain degree of model capacity is\nnecessary for Transformer-based language models to capture humanlike\nexpectations.",
        "pdf_link": "https://arxiv.org/pdf/2304.11389v2.pdf"
    },
    {
        "title": "SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval",
        "authors": [
            "Haitao Li",
            "Qingyao Ai",
            "Jia Chen",
            "Qian Dong",
            "Yueyue Wu",
            "Yiqun Liu",
            "Chong Chen",
            "Qi Tian"
        ],
        "published": "2023-04-22T10:47:01Z",
        "summary": "Legal case retrieval, which aims to find relevant cases for a query case,\nplays a core role in the intelligent legal system. Despite the success that\npre-training has achieved in ad-hoc retrieval tasks, effective pre-training\nstrategies for legal case retrieval remain to be explored. Compared with\ngeneral documents, legal case documents are typically long text sequences with\nintrinsic logical structures. However, most existing language models have\ndifficulty understanding the long-distance dependencies between different\nstructures. Moreover, in contrast to the general retrieval, the relevance in\nthe legal domain is sensitive to key legal elements. Even subtle differences in\nkey legal elements can significantly affect the judgement of relevance.\nHowever, existing pre-trained language models designed for general purposes\nhave not been equipped to handle legal elements.\n  To address these issues, in this paper, we propose SAILER, a new\nStructure-Aware pre-traIned language model for LEgal case Retrieval. It is\nhighlighted in the following three aspects: (1) SAILER fully utilizes the\nstructural information contained in legal case documents and pays more\nattention to key legal elements, similar to how legal experts browse legal case\ndocuments. (2) SAILER employs an asymmetric encoder-decoder architecture to\nintegrate several different pre-training objectives. In this way, rich semantic\ninformation across tasks is encoded into dense vectors. (3) SAILER has powerful\ndiscriminative ability, even without any legal annotation data. It can\ndistinguish legal cases with different charges accurately. Extensive\nexperiments over publicly available legal benchmarks demonstrate that our\napproach can significantly outperform previous state-of-the-art methods in\nlegal case retrieval.",
        "pdf_link": "https://arxiv.org/pdf/2304.11370v1.pdf"
    },
    {
        "title": "Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers",
        "authors": [
            "Felipe Urrutia",
            "Roberto Araya"
        ],
        "published": "2023-04-21T21:25:30Z",
        "summary": "Written answers to open-ended questions can have a higher long-term effect on\nlearning than multiple-choice questions. However, it is critical that teachers\nimmediately review the answers, and ask to redo those that are incoherent. This\ncan be a difficult task and can be time-consuming for teachers. A possible\nsolution is to automate the detection of incoherent answers. One option is to\nautomate the review with Large Language Models (LLM). In this paper, we analyze\nthe responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM,\nand YOU. We used them with zero, one, two, three and four shots. We compared\ntheir performance with the results of various classifiers trained with Machine\nLearning (ML). We found that LLMs perform worse than MLs in detecting\nincoherent answers. The difficulty seems to reside in recursive questions that\ncontain both questions and answers, and in responses from students with typical\nfourth-grader misspellings. Upon closer examination, we have found that the\nChatGPT model faces the same challenges.",
        "pdf_link": "https://arxiv.org/pdf/2304.11257v1.pdf"
    },
    {
        "title": "Emergent and Predictable Memorization in Large Language Models",
        "authors": [
            "Stella Biderman",
            "USVSN Sai Prashanth",
            "Lintang Sutawika",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Shivanshu Purohit",
            "Edward Raff"
        ],
        "published": "2023-04-21T17:58:31Z",
        "summary": "Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model's\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model's full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite and plot scaling laws for forecasting memorization,\nallowing us to provide equi-compute recommendations to maximize the reliability\n(recall) of such predictions. We additionally provide further novel discoveries\non the distribution of memorization scores across models and data. We release\nall code and data necessary to reproduce the results in this paper at\nhttps://github.com/EleutherAI/pythia",
        "pdf_link": "https://arxiv.org/pdf/2304.11158v2.pdf"
    },
    {
        "title": "The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination",
        "authors": [
            "Zihao Li"
        ],
        "published": "2023-04-21T16:40:54Z",
        "summary": "With the launch of ChatGPT, Large Language Models (LLMs) are shaking up our\nwhole society, rapidly altering the way we think, create and live. For\ninstance, the GPT integration in Bing has altered our approach to online\nsearching. While nascent LLMs have many advantages, new legal and ethical risks\nare also emerging, stemming in particular from stochastic parrots and\nhallucination. The EU is the first and foremost jurisdiction that has focused\non the regulation of AI models. However, the risks posed by the new LLMs are\nlikely to be underestimated by the emerging EU regulatory paradigm. Therefore,\nthis correspondence warns that the European AI regulatory paradigm must evolve\nfurther to mitigate such risks.",
        "pdf_link": "https://arxiv.org/pdf/2304.14347v1.pdf"
    },
    {
        "title": "Inducing anxiety in large language models increases exploration and bias",
        "authors": [
            "Julian Coda-Forno",
            "Kristin Witte",
            "Akshay K. Jagadish",
            "Marcel Binz",
            "Zeynep Akata",
            "Eric Schulz"
        ],
        "published": "2023-04-21T16:29:43Z",
        "summary": "Large language models are transforming research on machine learning while\ngalvanizing public debates. Understanding not only when these models work well\nand succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of computational psychiatry, a framework\nused to computationally describe and modify aberrant behavior, to the outputs\nproduced by these models. We focus on the Generative Pre-Trained Transformer\n3.5 and subject it to tasks commonly studied in psychiatry. Our results show\nthat GPT-3.5 responds robustly to a common anxiety questionnaire, producing\nhigher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be\npredictably changed by using emotion-inducing prompts. Emotion-induction not\nonly influences GPT-3.5's behavior in a cognitive task measuring exploratory\ndecision-making but also influences its behavior in a previously-established\ntask measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a\nstrong increase in biases when prompted with anxiety-inducing text. Thus, it is\nlikely that how prompts are communicated to large language models has a strong\ninfluence on their behavior in applied settings. These results progress our\nunderstanding of prompt engineering and demonstrate the usefulness of methods\ntaken from computational psychiatry for studying the capable algorithms to\nwhich we increasingly delegate authority and autonomy.",
        "pdf_link": "https://arxiv.org/pdf/2304.11111v1.pdf"
    },
    {
        "title": "ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT",
        "authors": [
            "Tianyang Zhong",
            "Yaonai Wei",
            "Li Yang",
            "Zihao Wu",
            "Zhengliang Liu",
            "Xiaozheng Wei",
            "Wenjun Li",
            "Junjie Yao",
            "Chong Ma",
            "Xiang Li",
            "Dajiang Zhu",
            "Xi Jiang",
            "Junwei Han",
            "Dinggang Shen",
            "Tianming Liu",
            "Tuo Zhang"
        ],
        "published": "2023-04-21T16:23:47Z",
        "summary": "Large language models (LLMs) such as ChatGPT have recently demonstrated\nsignificant potential in mathematical abilities, providing valuable reasoning\nparadigm consistent with human natural language. However, LLMs currently have\ndifficulty in bridging perception, language understanding and reasoning\ncapabilities due to incompatibility of the underlying information flow among\nthem, making it challenging to accomplish tasks autonomously. On the other\nhand, abductive learning (ABL) frameworks for integrating the two abilities of\nperception and reasoning has seen significant success in inverse decipherment\nof incomplete facts, but it is limited by the lack of semantic understanding of\nlogical reasoning rules and the dependence on complicated domain knowledge\nrepresentation. This paper presents a novel method (ChatABL) for integrating\nLLMs into the ABL framework, aiming at unifying the three abilities in a more\nuser-friendly and understandable manner. The proposed method uses the strengths\nof LLMs' understanding and logical reasoning to correct the incomplete logical\nfacts for optimizing the performance of perceptual module, by summarizing and\nreorganizing reasoning rules represented in natural language format. Similarly,\nperceptual module provides necessary reasoning examples for LLMs in natural\nlanguage format. The variable-length handwritten equation deciphering task, an\nabstract expression of the Mayan calendar decoding, is used as a testbed to\ndemonstrate that ChatABL has reasoning ability beyond most existing\nstate-of-the-art methods, which has been well supported by comparative studies.\nTo our best knowledge, the proposed ChatABL is the first attempt to explore a\nnew pattern for further approaching human-level cognitive ability via natural\nlanguage interaction with ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2304.11107v1.pdf"
    },
    {
        "title": "DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction",
        "authors": [
            "Mohammadreza Pourreza",
            "Davood Rafiei"
        ],
        "published": "2023-04-21T15:02:18Z",
        "summary": "There is currently a significant gap between the performance of fine-tuned\nmodels and prompting approaches using Large Language Models (LLMs) on the\nchallenging task of text-to-SQL, as evaluated on datasets such as Spider. To\nimprove the performance of LLMs in the reasoning process, we study how\ndecomposing the task into smaller sub-tasks can be effective. In particular, we\nshow that breaking down the generation problem into sub-problems and feeding\nthe solutions of those sub-problems into LLMs can be an effective approach for\nsignificantly improving their performance. Our experiments with three LLMs show\nthat this approach consistently improves their simple few-shot performance by\nroughly 10%, pushing the accuracy of LLMs towards SOTA or surpassing it. On the\nholdout test set of Spider, the SOTA, in terms of execution accuracy, was 79.9\nand the new SOTA at the time of this writing using our approach is 85.3. Our\napproach with in-context learning beats many heavily fine-tuned models by at\nleast 5%. Additionally, when evaluated on the BIRD benchmark, our approach\nachieved an execution accuracy of 55.9%, setting a new SOTA on its holdout test\nset.",
        "pdf_link": "https://arxiv.org/pdf/2304.11015v3.pdf"
    },
    {
        "title": "Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition",
        "authors": [
            "Matteo Muffo",
            "Aldo Cocco",
            "Enrico Bertino"
        ],
        "published": "2023-04-21T14:21:52Z",
        "summary": "In recent years, Large Language Models such as GPT-3 showed remarkable\ncapabilities in performing NLP tasks in the zero and few shot settings. On the\nother hand, the experiments highlighted the difficulty of GPT-3 in carrying out\ntasks that require a certain degree of reasoning, such as arithmetic\noperations. In this paper we evaluate the ability of Transformer Language\nModels to perform arithmetic operations following a pipeline that, before\nperforming computations, decomposes numbers in units, tens, and so on. We\ndenote the models fine-tuned with this pipeline with the name Calculon and we\ntest them in the task of performing additions, subtractions and multiplications\non the same test sets of GPT-3. Results show an increase of accuracy of 63% in\nthe five-digit addition task. Moreover, we demonstrate the importance of the\ndecomposition pipeline introduced, since fine-tuning the same Language Model\nwithout decomposing numbers results in 0% accuracy in the five-digit addition\ntask.",
        "pdf_link": "https://arxiv.org/pdf/2304.10977v1.pdf"
    },
    {
        "title": "SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model",
        "authors": [
            "Juexiao Zhou",
            "Xiaonan He",
            "Liyuan Sun",
            "Jiannan Xu",
            "Xiuying Chen",
            "Yuetan Chu",
            "Longxi Zhou",
            "Xingyu Liao",
            "Bin Zhang",
            "Xin Gao"
        ],
        "published": "2023-04-21T01:17:09Z",
        "summary": "Skin and subcutaneous diseases rank high among the leading contributors to\nthe global burden of nonfatal diseases, impacting a considerable portion of the\npopulation. Nonetheless, the field of dermatology diagnosis faces three\nsignificant hurdles. Firstly, there is a shortage of dermatologists accessible\nto diagnose patients, particularly in rural regions. Secondly, accurately\ninterpreting skin disease images poses a considerable challenge. Lastly,\ngenerating patient-friendly diagnostic reports is usually a time-consuming and\nlabor-intensive task for dermatologists. To tackle these challenges, we present\nSkinGPT-4, which is the world's first interactive dermatology diagnostic system\npowered by an advanced visual large language model. SkinGPT-4 leverages a\nfine-tuned version of MiniGPT-4, trained on an extensive collection of skin\ndisease images (comprising 52,929 publicly available and proprietary images)\nalong with clinical concepts and doctors' notes. We designed a two-step\ntraining process to allow SkinGPT to express medical features in skin disease\nimages with natural language and make accurate diagnoses of the types of skin\ndiseases. With SkinGPT-4, users could upload their own skin photos for\ndiagnosis, and the system could autonomously evaluate the images, identifies\nthe characteristics and categories of the skin conditions, performs in-depth\nanalysis, and provides interactive treatment recommendations. Meanwhile,\nSkinGPT-4's local deployment capability and commitment to user privacy also\nrender it an appealing choice for patients in search of a dependable and\nprecise diagnosis of their skin ailments. To demonstrate the robustness of\nSkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, which\nwere independently reviewed by certified dermatologists, and showed that\nSkinGPT-4 could provide accurate diagnoses of skin diseases.",
        "pdf_link": "https://arxiv.org/pdf/2304.10691v2.pdf"
    },
    {
        "title": "Meta Semantics: Towards better natural language understanding and reasoning",
        "authors": [
            "Xiaolin Hu"
        ],
        "published": "2023-04-20T22:16:16Z",
        "summary": "Natural language understanding is one of the most challenging topics in\nartificial intelligence. Deep neural network methods, particularly large\nlanguage module (LLM) methods such as ChatGPT and GPT-3, have powerful\nflexibility to adopt informal text but are weak on logical deduction and suffer\nfrom the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods\nsuch as Mathematica, Semantic web, and Lean, are excellent in reasoning but\ncannot handle the complex and changeable informal text. Inspired by pragmatics\nand structuralism, we propose two strategies to solve the OOV problem and a\nsemantic model for better natural language understanding and reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2304.10663v1.pdf"
    },
    {
        "title": "\"HOT\" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media",
        "authors": [
            "Lingyao Li",
            "Lizhou Fan",
            "Shubham Atreja",
            "Libby Hemphill"
        ],
        "published": "2023-04-20T19:40:51Z",
        "summary": "Harmful content is pervasive on social media, poisoning online communities\nand negatively impacting participation. A common approach to address this issue\nis to develop detection models that rely on human annotations. However, the\ntasks required to build such models expose annotators to harmful and offensive\ncontent and may require significant time and cost to complete. Generative AI\nmodels have the potential to understand and detect harmful content. To\ninvestigate this potential, we used ChatGPT and compared its performance with\nMTurker annotations for three frequently discussed concepts related to harmful\ncontent: Hateful, Offensive, and Toxic (HOT). We designed five prompts to\ninteract with ChatGPT and conducted four experiments eliciting HOT\nclassifications. Our results show that ChatGPT can achieve an accuracy of\napproximately 80% when compared to MTurker annotations. Specifically, the model\ndisplays a more consistent classification for non-HOT comments than HOT\ncomments compared to human annotations. Our findings also suggest that ChatGPT\nclassifications align with provided HOT definitions, but ChatGPT classifies\n\"hateful\" and \"offensive\" as subsets of \"toxic.\" Moreover, the choice of\nprompts used to interact with ChatGPT impacts its performance. Based on these\nin-sights, our study provides several meaningful implications for employing\nChatGPT to detect HOT content, particularly regarding the reliability and\nconsistency of its performance, its understand-ing and reasoning of the HOT\nconcept, and the impact of prompts on its performance. Overall, our study\nprovides guidance about the potential of using generative AI models to moderate\nlarge volumes of user-generated content on social media.",
        "pdf_link": "https://arxiv.org/pdf/2304.10619v1.pdf"
    },
    {
        "title": "Joint Repetition Suppression and Content Moderation of Large Language Models",
        "authors": [
            "Minghui Zhang",
            "Alex Sokolov",
            "Weixin Cai",
            "Si-Qing Chen"
        ],
        "published": "2023-04-20T19:17:49Z",
        "summary": "Natural language generation (NLG) is one of the most impactful fields in NLP,\nand recent years have witnessed its evolution brought about by large language\nmodels (LLMs). As the key instrument for writing assistance applications, they\nare generally prone to replicating or extending offensive content provided in\nthe input. In low-resource data regime, they can also lead to repetitive\noutputs. Usually, offensive content and repetitions are mitigated with post-hoc\nmethods, including n-gram level blocklists, top-k and nucleus sampling. In this\npaper, we apply non-exact repetition suppression using token and sequence level\nunlikelihood loss, and further explore the framework of unlikelihood training\nobjective in order to jointly endow the model with abilities to avoid\ngenerating offensive words and phrases from the beginning. Finally, with\ncomprehensive experiments, we demonstrate that our proposed methods work\nexceptionally in controlling the repetition and content quality of LLM outputs.",
        "pdf_link": "https://arxiv.org/pdf/2304.10611v2.pdf"
    },
    {
        "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
        "authors": [
            "Deyao Zhu",
            "Jun Chen",
            "Xiaoqian Shen",
            "Xiang Li",
            "Mohamed Elhoseiny"
        ],
        "published": "2023-04-20T18:25:35Z",
        "summary": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such\nas directly generating websites from handwritten text and identifying humorous\nelements within images. These features are rarely observed in previous\nvision-language models. However, the technical details behind GPT-4 continue to\nremain undisclosed. We believe that the enhanced multi-modal generation\ncapabilities of GPT-4 stem from the utilization of sophisticated large language\nmodels (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a\nfrozen visual encoder with a frozen advanced LLM, Vicuna, using one projection\nlayer. Our work, for the first time, uncovers that properly aligning the visual\nfeatures with an advanced large language model can possess numerous advanced\nmulti-modal abilities demonstrated by GPT-4, such as detailed image description\ngeneration and website creation from hand-drawn drafts. Furthermore, we also\nobserve other emerging capabilities in MiniGPT-4, including writing stories and\npoems inspired by given images, teaching users how to cook based on food\nphotos, and so on. In our experiment, we found that the model trained on short\nimage caption pairs could produce unnatural language outputs (e.g., repetition\nand fragmentation). To address this problem, we curate a detailed image\ndescription dataset in the second stage to finetune the model, which\nconsequently improves the model's generation reliability and overall usability.\nOur code, pre-trained model, and collected dataset are available at\nhttps://minigpt-4.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2304.10592v2.pdf"
    },
    {
        "title": "Why Does ChatGPT Fall Short in Providing Truthful Answers?",
        "authors": [
            "Shen Zheng",
            "Jie Huang",
            "Kevin Chen-Chuan Chang"
        ],
        "published": "2023-04-20T17:48:43Z",
        "summary": "Recent advancements in large language models, such as ChatGPT, have\ndemonstrated significant potential to impact various aspects of human life.\nHowever, ChatGPT still faces challenges in providing reliable and accurate\nanswers to user questions. To better understand the model's particular\nweaknesses in providing truthful answers, we embark an in-depth exploration of\nopen-domain question answering. Specifically, we undertake a detailed\nexamination of ChatGPT's failures, categorized into: comprehension, factuality,\nspecificity, and inference. We further pinpoint factuality as the most\ncontributing failure and identify two critical abilities associated with\nfactuality: knowledge memorization and knowledge recall. Through experiments\nfocusing on factuality, we propose several potential enhancement strategies.\nOur findings suggest that augmenting the model with granular external knowledge\nand cues for knowledge recall can enhance the model's factuality in answering\nquestions.",
        "pdf_link": "https://arxiv.org/pdf/2304.10513v3.pdf"
    },
    {
        "title": "Learning to Plan with Natural Language",
        "authors": [
            "Yiduo Guo",
            "Yaobo Liang",
            "Chenfei Wu",
            "Wenshan Wu",
            "Dongyan Zhao",
            "Nan Duan"
        ],
        "published": "2023-04-20T17:09:12Z",
        "summary": "Large Language Models (LLMs) have shown remarkable performance in various\nbasic natural language tasks. For completing the complex task, we still need a\nplan for the task to guide LLMs to generate the specific solutions step by\nstep. LLMs can directly generate task plans, but these plans may still contain\nfactual errors or are incomplete. A high-quality task plan contains correct\nstep-by-step solutions for solving all situations and behavioral instructions\nfor avoiding mistakes. To obtain it, we propose the Learning to Plan method,\nwhich involves two phases: (1) In the first learning task plan phase, it\niteratively updates the task plan with new step-by-step solutions and\nbehavioral instructions, which are obtained by prompting LLMs to derive from\ntraining error feedback. (2) In the subsequent test phase, the LLM uses the\nlearned task plan to guide the inference of LLM on the test set. We demonstrate\nthe effectiveness of our method on the five different reasoning type tasks (8\ndatasets). Further, our analysis experiment shows that the task plan learned by\none LLM can directly guide another LLM to improve its performance, which\nreveals a new transfer learning paradigm. We release the code at\n\\url{https://github.com/Eureka6174/LearnNLPlan}",
        "pdf_link": "https://arxiv.org/pdf/2304.10464v4.pdf"
    },
    {
        "title": "Safety Assessment of Chinese Large Language Models",
        "authors": [
            "Hao Sun",
            "Zhexin Zhang",
            "Jiawen Deng",
            "Jiale Cheng",
            "Minlie Huang"
        ],
        "published": "2023-04-20T16:27:35Z",
        "summary": "With the rapid popularity of large language models such as ChatGPT and GPT-4,\na growing amount of attention is paid to their safety concerns. These models\nmay generate insulting and discriminatory content, reflect incorrect social\nvalues, and may be used for malicious purposes such as fraud and dissemination\nof misleading information. Evaluating and enhancing their safety is\nparticularly essential for the wide application of large language models\n(LLMs). To further promote the safe deployment of LLMs, we develop a Chinese\nLLM safety assessment benchmark. Our benchmark explores the comprehensive\nsafety performance of LLMs from two perspectives: 8 kinds of typical safety\nscenarios and 6 types of more challenging instruction attacks. Our benchmark is\nbased on a straightforward process in which it provides the test prompts and\nevaluates the safety of the generated responses from the evaluated model. In\nevaluation, we utilize the LLM's strong evaluation ability and develop it as a\nsafety evaluator by prompting. On top of this benchmark, we conduct safety\nassessments and analyze 15 LLMs including the OpenAI GPT series and other\nwell-known Chinese LLMs, where we observe some interesting findings. For\nexample, we find that instruction attacks are more likely to expose safety\nissues of all LLMs. Moreover, to promote the development and deployment of\nsafe, responsible, and ethical AI, we publicly release SafetyPrompts including\n100k augmented prompts and responses by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.10436v1.pdf"
    },
    {
        "title": "GPT-NER: Named Entity Recognition via Large Language Models",
        "authors": [
            "Shuhe Wang",
            "Xiaofei Sun",
            "Xiaoya Li",
            "Rongbin Ouyang",
            "Fei Wu",
            "Tianwei Zhang",
            "Jiwei Li",
            "Guoyin Wang"
        ],
        "published": "2023-04-20T16:17:26Z",
        "summary": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext \"Columbus is a city\" is transformed to generate the text sequence\n\"@@Columbus## is a city\", where special tokens @@## marks the entity to\nextract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n  We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.",
        "pdf_link": "https://arxiv.org/pdf/2304.10428v4.pdf"
    },
    {
        "title": "Fully Autonomous Programming with Large Language Models",
        "authors": [
            "Vadim Liventsev",
            "Anastasiia Grishina",
            "Aki Härmä",
            "Leon Moonen"
        ],
        "published": "2023-04-20T16:12:05Z",
        "summary": "Current approaches to program synthesis with Large Language Models (LLMs)\nexhibit a \"near miss syndrome\": they tend to generate programs that\nsemantically resemble the correct answer (as measured by text similarity\nmetrics or human evaluation), but achieve a low or even zero accuracy as\nmeasured by unit tests due to small imperfections, such as the wrong input or\noutput format. This calls for an approach known as Synthesize, Execute, Debug\n(SED), whereby a draft of the solution is generated first, followed by a\nprogram repair phase addressing the failed tests. To effectively apply this\napproach to instruction-driven LLMs, one needs to determine which prompts\nperform best as instructions for LLMs, as well as strike a balance between\nrepairing unsuccessful programs and replacing them with newly generated ones.\nWe explore these trade-offs empirically, comparing replace-focused,\nrepair-focused, and hybrid debug strategies, as well as different\ntemplate-based and model-based prompt-generation techniques. We use OpenAI\nCodex as the LLM and Program Synthesis Benchmark 2 as a database of problem\ndescriptions and tests for evaluation. The resulting framework outperforms both\nconventional usage of Codex without the repair phase and traditional genetic\nprogramming approaches.",
        "pdf_link": "https://arxiv.org/pdf/2304.10423v1.pdf"
    },
    {
        "title": "CKBP v2: An Expert-Annotated Evaluation Set for Commonsense Knowledge Base Population",
        "authors": [
            "Tianqing Fang",
            "Quyet V. Do",
            "Sehyun Choi",
            "Weiqi Wang",
            "Yangqiu Song"
        ],
        "published": "2023-04-20T15:27:29Z",
        "summary": "Populating Commonsense Knowledge Bases (CSKB) is an important yet hard task\nin NLP, as it tackles knowledge from external sources with unseen events and\nentities. Fang et al. (2021a) proposed a CSKB Population benchmark with an\nevaluation set CKBP v1. However, CKBP v1 adopts crowdsourced annotations that\nsuffer from a substantial fraction of incorrect answers, and the evaluation set\nis not well-aligned with the external knowledge source as a result of random\nsampling. In this paper, we introduce CKBP v2, a new high-quality CSKB\nPopulation benchmark, which addresses the two mentioned problems by using\nexperts instead of crowd-sourced annotation and by adding diversified\nadversarial samples to make the evaluation set more representative. We conduct\nextensive experiments comparing state-of-the-art methods for CSKB Population on\nthe new evaluation set for future research comparisons. Empirical results show\nthat the population task is still challenging, even for large language models\n(LLM) such as ChatGPT. Codes and data are available at\nhttps://github.com/HKUST-KnowComp/CSKB-Population.",
        "pdf_link": "https://arxiv.org/pdf/2304.10392v1.pdf"
    },
    {
        "title": "Interventional Probing in High Dimensions: An NLI Case Study",
        "authors": [
            "Julia Rozanova",
            "Marco Valentino",
            "Lucas Cordeiro",
            "Andre Freitas"
        ],
        "published": "2023-04-20T14:34:31Z",
        "summary": "Probing strategies have been shown to detect the presence of various\nlinguistic features in large language models; in particular, semantic features\nintermediate to the \"natural logic\" fragment of the Natural Language Inference\ntask (NLI). In the case of natural logic, the relation between the intermediate\nfeatures and the entailment label is explicitly known: as such, this provides a\nripe setting for interventional studies on the NLI models' representations,\nallowing for stronger causal conjectures and a deeper critical analysis of\ninterventional probing methods. In this work, we carry out new and existing\nrepresentation-level interventions to investigate the effect of these semantic\nfeatures on NLI classification: we perform amnesic probing (which removes\nfeatures as directed by learned linear probes) and introduce the mnestic\nprobing variation (which forgets all dimensions except the probe-selected\nones). Furthermore, we delve into the limitations of these methods and outline\nsome pitfalls have been obscuring the effectivity of interventional probing\nstudies.",
        "pdf_link": "https://arxiv.org/pdf/2304.10346v1.pdf"
    },
    {
        "title": "Analyzing FOMC Minutes: Accuracy and Constraints of Language Models",
        "authors": [
            "Wonseong Kim",
            "Jan Frederic Spörer",
            "Siegfried Handschuh"
        ],
        "published": "2023-04-20T08:54:00Z",
        "summary": "This research article analyzes the language used in the official statements\nreleased by the Federal Open Market Committee (FOMC) after its scheduled\nmeetings to gain insights into the impact of FOMC official statements on\nfinancial markets and economic forecasting. The study reveals that the FOMC is\ncareful to avoid expressing emotion in their sentences and follows a set of\ntemplates to cover economic situations. The analysis employs advanced language\nmodeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The\nresults show that FinBERT outperforms other techniques in predicting negative\nsentiment accurately. However, the study also highlights the challenges and\nlimitations of using current NLP techniques to analyze FOMC texts and suggests\nthe potential for enhancing language models and exploring alternative\napproaches.",
        "pdf_link": "https://arxiv.org/pdf/2304.10164v2.pdf"
    },
    {
        "title": "Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks",
        "authors": [
            "Yiming Zhu",
            "Peixian Zhang",
            "Ehsan-Ul Haq",
            "Pan Hui",
            "Gareth Tyson"
        ],
        "published": "2023-04-20T08:08:12Z",
        "summary": "The release of ChatGPT has uncovered a range of possibilities whereby large\nlanguage models (LLMs) can substitute human intelligence. In this paper, we\nseek to understand whether ChatGPT has the potential to reproduce\nhuman-generated label annotations in social computing tasks. Such an\nachievement could significantly reduce the cost and complexity of social\ncomputing research. As such, we use ChatGPT to relabel five seminal datasets\ncovering stance detection (2x), sentiment analysis, hate speech, and bot\ndetection. Our results highlight that ChatGPT does have the potential to handle\nthese data annotation tasks, although a number of challenges remain. ChatGPT\nobtains an average accuracy 0.609. Performance is highest for the sentiment\nanalysis dataset, with ChatGPT correctly annotating 64.9% of tweets. Yet, we\nshow that performance varies substantially across individual labels. We believe\nthis work can open up new lines of analysis and act as a basis for future\nresearch into the exploitation of ChatGPT for human annotation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.10145v2.pdf"
    },
    {
        "title": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs",
        "authors": [
            "Charvi Rastogi",
            "Marco Tulio Ribeiro",
            "Nicholas King",
            "Harsha Nori",
            "Saleema Amershi"
        ],
        "published": "2023-04-19T21:59:04Z",
        "summary": "Large language models are becoming increasingly pervasive and ubiquitous in\nsociety via deployment in sociotechnical systems. Yet these language models, be\nit for classification or generation, have been shown to be biased and behave\nirresponsibly, causing harm to people at scale. It is crucial to audit these\nlanguage models rigorously. Existing auditing tools leverage either or both\nhumans and AI to find failures. In this work, we draw upon literature in\nhuman-AI collaboration and sensemaking, and conduct interviews with research\nexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro\nand Lundberg, 2022), which is powered by a generative large language model\n(LLM). Through the design process we highlight the importance of sensemaking\nand human-AI communication to leverage complementary strengths of humans and\ngenerative models in collaborative auditing. To evaluate the effectiveness of\nthe augmented tool, AdaTest++, we conduct user studies with participants\nauditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment\nanalysis model. Qualitative analysis shows that AdaTest++ effectively leverages\nhuman strengths such as schematization, hypothesis formation and testing.\nFurther, with our tool, participants identified a variety of failures modes,\ncovering 26 different topics over 2 tasks, that have been shown before in\nformal audits and also those previously under-reported.",
        "pdf_link": "https://arxiv.org/pdf/2304.09991v3.pdf"
    },
    {
        "title": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery",
        "authors": [
            "Lalithkumar Seenivasan",
            "Mobarakol Islam",
            "Gokul Kannan",
            "Hongliang Ren"
        ],
        "published": "2023-04-19T21:22:52Z",
        "summary": "Advances in GPT-based large language models (LLMs) are revolutionizing\nnatural language processing, exponentially increasing its use across various\ndomains. Incorporating uni-directional attention, these autoregressive LLMs can\ngenerate long and coherent paragraphs. However, for visual question answering\n(VQA) tasks that require both vision and language processing, models with\nbi-directional attention or models employing fusion techniques are often\nemployed to capture the context of multiple modalities all at once. As GPT does\nnot natively process vision tokens, to exploit the advancements in GPT models\nfor VQA in robotic surgery, we design an end-to-end trainable Language-Vision\nGPT (LV-GPT) model that expands the GPT2 model to include vision input (image).\nThe proposed LV-GPT incorporates a feature extractor (vision tokenizer) and\nvision token embedding (token type and pose). Given the limitations of\nunidirectional attention in GPT models and their ability to generate coherent\nlong paragraphs, we carefully sequence the word tokens before vision tokens,\nmimicking the human thought process of understanding the question to infer an\nanswer from an image. Quantitatively, we prove that the LV-GPT model\noutperforms other state-of-the-art VQA models on two publically available\nsurgical-VQA datasets (based on endoscopic vision challenge robotic scene\nsegmentation 2018 and CholecTriplet2021) and on our newly annotated dataset\n(based on the holistic surgical scene dataset). We further annotate all three\ndatasets to include question-type annotations to allow sub-type analysis.\nFurthermore, we extensively study and present the effects of token sequencing,\ntoken type and pose embedding for vision tokens in the LV-GPT model.",
        "pdf_link": "https://arxiv.org/pdf/2304.09974v2.pdf"
    },
    {
        "title": "Low-resource Bilingual Dialect Lexicon Induction with Large Language Models",
        "authors": [
            "Ekaterina Artemova",
            "Barbara Plank"
        ],
        "published": "2023-04-19T20:20:41Z",
        "summary": "Bilingual word lexicons are crucial tools for multilingual natural language\nunderstanding and machine translation tasks, as they facilitate the mapping of\nwords in one language to their synonyms in another language. To achieve this,\nnumerous papers have explored bilingual lexicon induction (BLI) in\nhigh-resource scenarios, using a typical pipeline consisting of two\nunsupervised steps: bitext mining and word alignment, both of which rely on\npre-trained large language models~(LLMs).\n  In this paper, we present an analysis of the BLI pipeline for German and two\nof its dialects, Bavarian and Alemannic. This setup poses several unique\nchallenges, including the scarcity of resources, the relatedness of the\nlanguages, and the lack of standardization in the orthography of dialects. To\nevaluate the BLI outputs, we analyze them with respect to word frequency and\npairwise edit distance. Additionally, we release two evaluation datasets\ncomprising 1,500 bilingual sentence pairs and 1,000 bilingual word pairs. They\nwere manually judged for their semantic similarity for each Bavarian-German and\nAlemannic-German language pair.",
        "pdf_link": "https://arxiv.org/pdf/2304.09957v1.pdf"
    },
    {
        "title": "Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers",
        "authors": [
            "Aishwarya Deep Shukla",
            "Laksh Agarwal",
            "Jie Mein",
            "Goh",
            "Guodong",
            "Gao",
            "Ritu Agarwal"
        ],
        "published": "2023-04-19T19:59:26Z",
        "summary": "The proliferation of fake reviews of doctors has potentially detrimental\nconsequences for patient well-being and has prompted concern among consumer\nprotection groups and regulatory bodies. Yet despite significant advancements\nin the fields of machine learning and natural language processing, there\nremains limited comprehension of the characteristics differentiating fraudulent\nfrom authentic reviews. This study utilizes a novel pre-labeled dataset of\n38048 physician reviews to establish the effectiveness of large language models\nin classifying reviews. Specifically, we compare the performance of traditional\nML models, such as logistic regression and support vector machines, to\ngenerative pre-trained transformer models. Furthermore, we use GPT4, the newest\nmodel in the GPT family, to uncover the key dimensions along which fake and\ngenuine physician reviews differ. Our findings reveal significantly superior\nperformance of GPT-3 over traditional ML models in this context. Additionally,\nour analysis suggests that GPT3 requires a smaller training sample than\ntraditional models, suggesting its appropriateness for tasks with scarce\ntraining data. Moreover, the superiority of GPT3 performance increases in the\ncold start context i.e., when there are no prior reviews of a doctor. Finally,\nwe employ GPT4 to reveal the crucial dimensions that distinguish fake physician\nreviews. In sharp contrast to previous findings in the literature that were\nobtained using simulated data, our findings from a real-world dataset show that\nfake reviews are generally more clinically detailed, more reserved in\nsentiment, and have better structure and grammar than authentic ones.",
        "pdf_link": "https://arxiv.org/pdf/2304.09948v1.pdf"
    },
    {
        "title": "Fundamental Limitations of Alignment in Large Language Models",
        "authors": [
            "Yotam Wolf",
            "Noam Wies",
            "Oshri Avnery",
            "Yoav Levine",
            "Amnon Shashua"
        ],
        "published": "2023-04-19T17:50:09Z",
        "summary": "An important aspect in developing language models that interact with humans\nis aligning their behavior to be useful and unharmful for their human users.\nThis is usually achieved by tuning the model in a way that enhances desired\nbehaviors and inhibits undesired ones, a process referred to as alignment. In\nthis paper, we propose a theoretical approach called Behavior Expectation\nBounds (BEB) which allows us to formally investigate several inherent\ncharacteristics and limitations of alignment in large language models.\nImportantly, we prove that within the limits of this framework, for any\nbehavior that has a finite probability of being exhibited by the model, there\nexist prompts that can trigger the model into outputting this behavior, with\nprobability that increases with the length of the prompt. This implies that any\nalignment process that attenuates an undesired behavior but does not remove it\naltogether, is not safe against adversarial prompting attacks. Furthermore, our\nframework hints at the mechanism by which leading alignment approaches such as\nreinforcement learning from human feedback make the LLM prone to being prompted\ninto the undesired behaviors. This theoretical result is being experimentally\ndemonstrated in large scale by the so called contemporary \"chatGPT jailbreaks\",\nwhere adversarial users trick the LLM into breaking its alignment guardrails by\ntriggering it into acting as a malicious persona. Our results expose\nfundamental limitations in alignment of LLMs and bring to the forefront the\nneed to devise reliable mechanisms for ensuring AI safety.",
        "pdf_link": "https://arxiv.org/pdf/2304.11082v5.pdf"
    },
    {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "published": "2023-04-19T17:47:47Z",
        "summary": "Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner. The project is available at\nhttps://chameleon-llm.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2304.09842v3.pdf"
    },
    {
        "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
        "authors": [
            "Qiao Jin",
            "Yifan Yang",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "published": "2023-04-19T13:53:19Z",
        "summary": "While large language models (LLMs) have been successfully applied to various\ntasks, they still face challenges with hallucinations. Augmenting LLMs with\ndomain-specific tools such as database utilities can facilitate easier and more\nprecise access to specialized knowledge. In this paper, we present GeneGPT, a\nnovel method for teaching LLMs to use the Web APIs of the National Center for\nBiotechnology Information (NCBI) for answering genomics questions.\nSpecifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs\nby in-context learning and an augmented decoding algorithm that can detect and\nexecute API calls. Experimental results show that GeneGPT achieves\nstate-of-the-art performance on eight tasks in the GeneTuring benchmark with an\naverage score of 0.83, largely surpassing retrieval-augmented LLMs such as the\nnew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as\nwell as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)\nAPI demonstrations have good cross-task generalizability and are more useful\nthan documentations for in-context learning; (2) GeneGPT can generalize to\nlonger chains of API calls and answer multi-hop questions in GeneHop, a novel\ndataset introduced in this work; (3) Different types of errors are enriched in\ndifferent tasks, providing valuable insights for future improvements.",
        "pdf_link": "https://arxiv.org/pdf/2304.09667v3.pdf"
    },
    {
        "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents",
        "authors": [
            "Weiwei Sun",
            "Lingyong Yan",
            "Xinyu Ma",
            "Shuaiqiang Wang",
            "Pengjie Ren",
            "Zhumin Chen",
            "Dawei Yin",
            "Zhaochun Ren"
        ],
        "published": "2023-04-19T10:16:03Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable zero-shot\ngeneralization across various language-related tasks, including search engines.\nHowever, existing work utilizes the generative ability of LLMs for Information\nRetrieval (IR) rather than direct passage ranking. The discrepancy between the\npre-training objectives of LLMs and the ranking objective poses another\nchallenge. In this paper, we first investigate generative LLMs such as ChatGPT\nand GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal\nthat properly instructed LLMs can deliver competitive, even superior results to\nstate-of-the-art supervised methods on popular IR benchmarks. Furthermore, to\naddress concerns about data contamination of LLMs, we collect a new test set\ncalled NovelEval, based on the latest knowledge and aiming to verify the\nmodel's ability to rank unknown knowledge. Finally, to improve efficiency in\nreal-world applications, we delve into the potential for distilling the ranking\ncapabilities of ChatGPT into small specialized models using a permutation\ndistillation scheme. Our evaluation results turn out that a distilled 440M\nmodel outperforms a 3B supervised model on the BEIR benchmark. The code to\nreproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
        "pdf_link": "https://arxiv.org/pdf/2304.09542v2.pdf"
    },
    {
        "title": "A Theory on Adam Instability in Large-Scale Machine Learning",
        "authors": [
            "Igor Molybog",
            "Peter Albert",
            "Moya Chen",
            "Zachary DeVito",
            "David Esiobu",
            "Naman Goyal",
            "Punit Singh Koura",
            "Sharan Narang",
            "Andrew Poulton",
            "Ruan Silva",
            "Binh Tang",
            "Diana Liskovich",
            "Puxin Xu",
            "Yuchen Zhang",
            "Melanie Kambadur",
            "Stephen Roller",
            "Susan Zhang"
        ],
        "published": "2023-04-19T06:15:11Z",
        "summary": "We present a theory for the previously unexplained divergent behavior noticed\nin the training of large language models. We argue that the phenomenon is an\nartifact of the dominant optimization algorithm used for training, called Adam.\nWe observe that Adam can enter a state in which the parameter update vector has\na relatively large norm and is essentially uncorrelated with the direction of\ndescent on the training loss landscape, leading to divergence. This artifact is\nmore likely to be observed in the training of a deep model with a large batch\nsize, which is the typical setting of large-scale language model training. To\nargue the theory, we present observations from the training runs of the\nlanguage models of different scales: 7 billion, 30 billion, 65 billion, and 546\nbillion parameters.",
        "pdf_link": "https://arxiv.org/pdf/2304.09871v2.pdf"
    },
    {
        "title": "Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes",
        "authors": [
            "Simran Arora",
            "Brandon Yang",
            "Sabri Eyuboglu",
            "Avanika Narayan",
            "Andrew Hojel",
            "Immanuel Trummer",
            "Christopher Ré"
        ],
        "published": "2023-04-19T06:00:26Z",
        "summary": "A long standing goal of the data management community is to develop general,\nautomated systems that ingest semi-structured documents and output queryable\ntables without human effort or domain specific customization. Given the sheer\nvariety of potential documents, state-of-the art systems make simplifying\nassumptions and use domain specific training. In this work, we ask whether we\ncan maintain generality by using large language models (LLMs). LLMs, which are\npretrained on broad data, can perform diverse downstream tasks simply\nconditioned on natural language task descriptions.\n  We propose and evaluate EVAPORATE, a simple, prototype system powered by\nLLMs. We identify two fundamentally different strategies for implementing this\nsystem: prompt the LLM to directly extract values from documents or prompt the\nLLM to synthesize code that performs the extraction. Our evaluations show a\ncost-quality tradeoff between these two approaches. Code synthesis is cheap,\nbut far less accurate than directly processing each document with the LLM. To\nimprove quality while maintaining low cost, we propose an extended code\nsynthesis implementation, EVAPORATE-CODE+, which achieves better quality than\ndirect extraction. Our key insight is to generate many candidate functions and\nensemble their extractions using weak supervision. EVAPORATE-CODE+ not only\noutperforms the state-of-the art systems, but does so using a sublinear pass\nover the documents with the LLM. This equates to a 110x reduction in the number\nof tokens the LLM needs to process, averaged across 16 real-world evaluation\nsettings of 10k documents each.",
        "pdf_link": "https://arxiv.org/pdf/2304.09433v2.pdf"
    },
    {
        "title": "TieFake: Title-Text Similarity and Emotion-Aware Fake News Detection",
        "authors": [
            "Quanjiang Guo",
            "Zhao Kang",
            "Ling Tian",
            "Zhouguo Chen"
        ],
        "published": "2023-04-19T04:47:36Z",
        "summary": "Fake news detection aims to detect fake news widely spreading on social media\nplatforms, which can negatively influence the public and the government. Many\napproaches have been developed to exploit relevant information from news\nimages, text, or videos. However, these methods may suffer from the following\nlimitations: (1) ignore the inherent emotional information of the news, which\ncould be beneficial since it contains the subjective intentions of the authors;\n(2) pay little attention to the relation (similarity) between the title and\ntextual information in news articles, which often use irrelevant title to\nattract reader' attention. To this end, we propose a novel Title-Text\nsimilarity and emotion-aware Fake news detection (TieFake) method by jointly\nmodeling the multi-modal context information and the author sentiment in a\nunified framework. Specifically, we respectively employ BERT and ResNeSt to\nlearn the representations for text and images, and utilize publisher emotion\nextractor to capture the author's subjective emotion in the news content. We\nalso propose a scale-dot product attention mechanism to capture the similarity\nbetween title features and textual features. Experiments are conducted on two\npublicly available multi-modal datasets, and the results demonstrate that our\nproposed method can significantly improve the performance of fake news\ndetection. Our code is available at https://github.com/UESTC-GQJ/TieFake.",
        "pdf_link": "https://arxiv.org/pdf/2304.09421v1.pdf"
    },
    {
        "title": "LLM as A Robotic Brain: Unifying Egocentric Memory and Control",
        "authors": [
            "Jinjie Mai",
            "Jun Chen",
            "Bing Li",
            "Guocheng Qian",
            "Mohamed Elhoseiny",
            "Bernard Ghanem"
        ],
        "published": "2023-04-19T00:08:48Z",
        "summary": "Embodied AI focuses on the study and development of intelligent systems that\npossess a physical or virtual embodiment (i.e. robots) and are able to\ndynamically interact with their environment. Memory and control are the two\nessential parts of an embodied system and usually require separate frameworks\nto model each of them. In this paper, we propose a novel and generalizable\nframework called LLM-Brain: using Large-scale Language Model as a robotic brain\nto unify egocentric memory and control. The LLM-Brain framework integrates\nmultiple multimodal language models for robotic tasks, utilizing a zero-shot\nlearning approach. All components within LLM-Brain communicate using natural\nlanguage in closed-loop multi-round dialogues that encompass perception,\nplanning, control, and memory. The core of the system is an embodied LLM to\nmaintain egocentric memory and control the robot. We demonstrate LLM-Brain by\nexamining two downstream tasks: active exploration and embodied question\nanswering. The active exploration tasks require the robot to extensively\nexplore an unknown environment within a limited number of actions. Meanwhile,\nthe embodied question answering tasks necessitate that the robot answers\nquestions based on observations acquired during prior explorations.",
        "pdf_link": "https://arxiv.org/pdf/2304.09349v4.pdf"
    },
    {
        "title": "Creating Large Language Model Resistant Exams: Guidelines and Strategies",
        "authors": [
            "Simon kaare Larsen"
        ],
        "published": "2023-04-18T18:01:32Z",
        "summary": "The proliferation of Large Language Models (LLMs), such as ChatGPT, has\nraised concerns about their potential impact on academic integrity, prompting\nthe need for LLM-resistant exam designs. This article investigates the\nperformance of LLMs on exams and their implications for assessment, focusing on\nChatGPT's abilities and limitations. We propose guidelines for creating\nLLM-resistant exams, including content moderation, deliberate inaccuracies,\nreal-world scenarios beyond the model's knowledge base, effective distractor\noptions, evaluating soft skills, and incorporating non-textual information. The\narticle also highlights the significance of adapting assessments to modern\ntools and promoting essential skills development in students. By adopting these\nstrategies, educators can maintain academic integrity while ensuring that\nassessments accurately reflect contemporary professional settings and address\nthe challenges and opportunities posed by artificial intelligence in education.",
        "pdf_link": "https://arxiv.org/pdf/2304.12203v1.pdf"
    },
    {
        "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling",
        "authors": [
            "Xiuying Wei",
            "Yunchen Zhang",
            "Yuhang Li",
            "Xiangguo Zhang",
            "Ruihao Gong",
            "Jinyang Guo",
            "Xianglong Liu"
        ],
        "published": "2023-04-18T17:34:23Z",
        "summary": "Post-training quantization~(PTQ) of transformer language models faces\nsignificant challenges due to the existence of detrimental outliers in\nactivations. We observe that these outliers are concentrated in specific\nchannels and are asymmetric across channels. To address this issue, we propose\nthe Outlier Suppression+~(OS+) framework, which contains the channel-wise\nshifting for asymmetry and channel-wise scaling for concentration. We show that\nthese operations can be seamlessly migrated into subsequent modules while\nmaintaining equivalence. Second, we propose a fast and stable scheme to\ncalculate effective shifting and scaling values. The channel-wise shifting\naligns the center of each channel for removal of outlier asymmetry. The\nchannel-wise scaling quantitatively evaluates changes brought by migration and\nquantization for better quantization burden balance. We validate our OS+ under\nboth standard and fine-grained quantization settings with models including\nBERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks\ndemonstrate the superiority of our approach. Especially, with standard\nquantization, OS+ can achieve near-floating-point performance on both small\nmodels and large language models on 8-bit and 6-bit. Besides, we establish a\nnew state-of-the-art for 4-bit BERT with 15.5\\% improvement. Our code is\navailable at \\url{https://github.com/ModelTC/Outlier_Suppression_Plus}.",
        "pdf_link": "https://arxiv.org/pdf/2304.09145v3.pdf"
    },
    {
        "title": "Towards Designing a ChatGPT Conversational Companion for Elderly People",
        "authors": [
            "Abeer Alessa",
            "Hend Al-Khalifa"
        ],
        "published": "2023-04-18T17:24:14Z",
        "summary": "Loneliness and social isolation are serious and widespread problems among\nolder people, affecting their physical and mental health, quality of life, and\nlongevity. In this paper, we propose a ChatGPT-based conversational companion\nsystem for elderly people. The system is designed to provide companionship and\nhelp reduce feelings of loneliness and social isolation. The system was\nevaluated with a preliminary study. The results showed that the system was able\nto generate responses that were relevant to the created elderly personas.\nHowever, it is essential to acknowledge the limitations of ChatGPT, such as\npotential biases and misinformation, and to consider the ethical implications\nof using AI-based companionship for the elderly, including privacy concerns.",
        "pdf_link": "https://arxiv.org/pdf/2304.09866v1.pdf"
    },
    {
        "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task",
        "authors": [
            "Zihao Wu",
            "Lu Zhang",
            "Chao Cao",
            "Xiaowei Yu",
            "Haixing Dai",
            "Chong Ma",
            "Zhengliang Liu",
            "Lin Zhao",
            "Gang Li",
            "Wei Liu",
            "Quanzheng Li",
            "Dinggang Shen",
            "Xiang Li",
            "Dajiang Zhu",
            "Tianming Liu"
        ],
        "published": "2023-04-18T17:21:48Z",
        "summary": "Recently, ChatGPT and GPT-4 have emerged and gained immense global attention\ndue to their unparalleled performance in language processing. Despite\ndemonstrating impressive capability in various open-domain tasks, their\nadequacy in highly specific fields like radiology remains untested. Radiology\npresents unique linguistic phenomena distinct from open-domain data due to its\nspecificity and complexity. Assessing the performance of large language models\n(LLMs) in such specific domains is crucial not only for a thorough evaluation\nof their overall performance but also for providing valuable insights into\nfuture model design directions: whether model design should be generic or\ndomain-specific. To this end, in this study, we evaluate the performance of\nChatGPT/GPT-4 on a radiology NLI task and compare it to other models fine-tuned\nspecifically on task-related data samples. We also conduct a comprehensive\ninvestigation on ChatGPT/GPT-4's reasoning ability by introducing varying\nlevels of inference difficulty. Our results show that 1) GPT-4 outperforms\nChatGPT in the radiology NLI task; 2) other specifically fine-tuned models\nrequire significant amounts of data samples to achieve comparable performance\nto ChatGPT/GPT-4. These findings demonstrate that constructing a generic model\nthat is capable of solving various tasks across different domains is feasible.",
        "pdf_link": "https://arxiv.org/pdf/2304.09138v1.pdf"
    },
    {
        "title": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT",
        "authors": [
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-04-18T13:20:45Z",
        "summary": "The way users acquire information is undergoing a paradigm shift with the\nadvent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves\nknowledge from the model itself and generates answers for users. ChatGPT's\nimpressive question-answering (QA) capability has attracted more than 100\nmillion users within a short period of time but has also raised concerns\nregarding its reliability. In this paper, we perform the first large-scale\nmeasurement of ChatGPT's reliability in the generic QA scenario with a\ncarefully curated set of 5,695 questions across ten datasets and eight domains.\nWe find that ChatGPT's reliability varies across different domains, especially\nunderperforming in law and science questions. We also demonstrate that system\nroles, originally designed by OpenAI to allow users to steer ChatGPT's\nbehavior, can impact ChatGPT's reliability in an imperceptible way. We further\nshow that ChatGPT is vulnerable to adversarial examples, and even a single\ncharacter change can negatively affect its reliability in certain cases. We\nbelieve that our study provides valuable insights into ChatGPT's reliability\nand underscores the need for strengthening the reliability and security of\nlarge language models (LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2304.08979v2.pdf"
    },
    {
        "title": "Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs",
        "authors": [
            "Da Silva Gameiro Henrique",
            "Andrei Kucharavy",
            "Rachid Guerraoui"
        ],
        "published": "2023-04-18T13:05:01Z",
        "summary": "The self-attention revolution allowed generative language models to scale and\nachieve increasingly impressive abilities. Such models - commonly referred to\nas Large Language Models (LLMs) - have recently gained prominence with the\ngeneral public, thanks to conversational fine-tuning, putting their behavior in\nline with public expectations regarding AI. This prominence amplified prior\nconcerns regarding the misuse of LLMs and led to the emergence of numerous\ntools to detect LLMs in the wild.\n  Unfortunately, most such tools are critically flawed. While major\npublications in the LLM detectability field suggested that LLMs were easy to\ndetect with fine-tuned autoencoders, the limitations of their results are easy\nto overlook. Specifically, they assumed publicly available generative models\nwithout fine-tunes or non-trivial prompts. While the importance of these\nassumptions has been demonstrated, until now, it remained unclear how well such\ndetection could be countered.\n  Here, we show that an attacker with access to such detectors' reference human\ntexts and output not only evades detection but can fully frustrate the detector\ntraining - with a reasonable budget and all its outputs labeled as such.\nAchieving it required combining common \"reinforcement from critic\" loss\nfunction modification and AdamW optimizer, which led to surprisingly good\nfine-tuning generalization. Finally, we warn against the temptation to\ntranspose the conclusions obtained in RNN-driven text GANs to LLMs due to their\nbetter representative ability.\n  These results have critical implications for the detection and prevention of\nmalicious use of generative language models, and we hope they will aid the\ndesigners of generative models and detectors.",
        "pdf_link": "https://arxiv.org/pdf/2304.08968v1.pdf"
    },
    {
        "title": "Safer Conversational AI as a Source of User Delight",
        "authors": [
            "Xiaoding Lu",
            "Aleksey Korshuk",
            "Zongyi Liu",
            "William Beauchamp",
            "Chai Research"
        ],
        "published": "2023-04-18T11:03:10Z",
        "summary": "This work explores the impact of moderation on users' enjoyment of\nconversational AI systems. While recent advancements in Large Language Models\n(LLMs) have led to highly capable conversational AIs that are increasingly\ndeployed in real-world settings, there is a growing concern over AI safety and\nthe need to moderate systems to encourage safe language and prevent harm.\nHowever, some users argue that current approaches to moderation limit the\ntechnology, compromise free expression, and limit the value delivered by the\ntechnology. This study takes an unbiased stance and shows that moderation does\nnot necessarily detract from user enjoyment. Heavy handed moderation does seem\nto have a nefarious effect, but models that are moderated to be safer can lead\nto a better user experience. By deploying various conversational AIs in the\nChai platform, the study finds that user retention can increase with a level of\nmoderation and safe system design. These results demonstrate the importance of\nappropriately defining safety in models in a way that is both responsible and\nfocused on serving users.",
        "pdf_link": "https://arxiv.org/pdf/2304.09865v1.pdf"
    },
    {
        "title": "Masked Language Model Based Textual Adversarial Example Detection",
        "authors": [
            "Xiaomei Zhang",
            "Zhaoxi Zhang",
            "Qi Zhong",
            "Xufei Zheng",
            "Yanjun Zhang",
            "Shengshan Hu",
            "Leo Yu Zhang"
        ],
        "published": "2023-04-18T06:52:14Z",
        "summary": "Adversarial attacks are a serious threat to the reliable deployment of\nmachine learning models in safety-critical applications. They can misguide\ncurrent models to predict incorrectly by slightly modifying the inputs.\nRecently, substantial work has shown that adversarial examples tend to deviate\nfrom the underlying data manifold of normal examples, whereas pre-trained\nmasked language models can fit the manifold of normal NLP data. To explore how\nto use the masked language model in adversarial detection, we propose a novel\ntextual adversarial example detection method, namely Masked Language\nModel-based Detection (MLMD), which can produce clearly distinguishable signals\nbetween normal examples and adversarial examples by exploring the changes in\nmanifolds induced by the masked language model. MLMD features a plug and play\nusage (i.e., no need to retrain the victim model) for adversarial defense and\nit is agnostic to classification tasks, victim model's architectures, and\nto-be-defended attack methods. We evaluate MLMD on various benchmark textual\ndatasets, widely studied machine learning models, and state-of-the-art (SOTA)\nadversarial attacks (in total $3*4*4 = 48$ settings). Experimental results show\nthat MLMD can achieve strong performance, with detection accuracy up to 0.984,\n0.967, and 0.901 on AG-NEWS, IMDB, and SST-2 datasets, respectively.\nAdditionally, MLMD is superior, or at least comparable to, the SOTA detection\ndefenses in detection accuracy and F1 score. Among many defenses based on the\noff-manifold assumption of adversarial examples, this work offers a new angle\nfor capturing the manifold change. The code for this work is openly accessible\nat \\url{https://github.com/mlmddetection/MLMDdetection}.",
        "pdf_link": "https://arxiv.org/pdf/2304.08767v3.pdf"
    },
    {
        "title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models",
        "authors": [
            "Qianqian Xie",
            "Zheheng Luo",
            "Benyou Wang",
            "Sophia Ananiadou"
        ],
        "published": "2023-04-18T06:38:40Z",
        "summary": "The exponential growth of biomedical texts such as biomedical literature and\nelectronic health records (EHRs), poses a significant challenge for clinicians\nand researchers to access clinical information efficiently. To tackle this\nchallenge, biomedical text summarization (BTS) has been proposed as a solution\nto support clinical information retrieval and management. BTS aims at\ngenerating concise summaries that distill key information from single or\nmultiple biomedical documents. In recent years, the rapid advancement of\nfundamental natural language processing (NLP) techniques, from pre-trained\nlanguage models (PLMs) to large language models (LLMs), has greatly facilitated\nthe progress of BTS. This growth has led to numerous proposed summarization\nmethods, datasets, and evaluation metrics, raising the need for a comprehensive\nand up-to-date survey for BTS. In this paper, we present a systematic review of\nrecent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to\nLLMs, to help understand the latest progress, challenges, and future\ndirections. We begin by introducing the foundational concepts of BTS, PLMs and\nLLMs, followed by an in-depth review of available datasets, recent approaches,\nand evaluation metrics in BTS. We finally discuss existing challenges and\npromising future directions in the era of LLMs. To facilitate the research\ncommunity, we line up open resources including available datasets, recent\napproaches, codes, evaluation metrics, and the leaderboard in a public project:\nhttps://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We\nbelieve that this survey will be a useful resource to researchers, allowing\nthem to quickly track recent advancements and provide guidelines for future BTS\nresearch within the research community.",
        "pdf_link": "https://arxiv.org/pdf/2304.08763v2.pdf"
    },
    {
        "title": "CancerGPT: Few-shot Drug Pair Synergy Prediction using Large Pre-trained Language Models",
        "authors": [
            "Tianhao Li",
            "Sandesh Shetty",
            "Advaith Kamath",
            "Ajay Jaiswal",
            "Xianqian Jiang",
            "Ying Ding",
            "Yejin Kim"
        ],
        "published": "2023-04-18T02:49:53Z",
        "summary": "Large pre-trained language models (LLMs) have been shown to have significant\npotential in few-shot learning across various fields, even with minimal\ntraining data. However, their ability to generalize to unseen tasks in more\ncomplex fields, such as biology, has yet to be fully evaluated. LLMs can offer\na promising alternative approach for biological inference, particularly in\ncases where structured data and sample size are limited, by extracting prior\nknowledge from text corpora. Our proposed few-shot learning approach uses LLMs\nto predict the synergy of drug pairs in rare tissues that lack structured data\nand features. Our experiments, which involved seven rare tissues from different\ncancer types, demonstrated that the LLM-based prediction model achieved\nsignificant accuracy with very few or zero samples. Our proposed model, the\nCancerGPT (with $\\sim$ 124M parameters), was even comparable to the larger\nfine-tuned GPT-3 model (with $\\sim$ 175B parameters). Our research is the first\nto tackle drug pair synergy prediction in rare tissues with limited data. We\nare also the first to utilize an LLM-based prediction model for biological\nreaction prediction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2304.10946v1.pdf"
    },
    {
        "title": "Large Language Models Based Automatic Synthesis of Software Specifications",
        "authors": [
            "Shantanu Mandal",
            "Adhrik Chethan",
            "Vahid Janfaza",
            "S M Farabi Mahmud",
            "Todd A Anderson",
            "Javier Turek",
            "Jesmin Jahan Tithi",
            "Abdullah Muzahid"
        ],
        "published": "2023-04-18T01:22:44Z",
        "summary": "Software configurations play a crucial role in determining the behavior of\nsoftware systems. In order to ensure safe and error-free operation, it is\nnecessary to identify the correct configuration, along with their valid bounds\nand rules, which are commonly referred to as software specifications. As\nsoftware systems grow in complexity and scale, the number of configurations and\nassociated specifications required to ensure the correct operation can become\nlarge and prohibitively difficult to manipulate manually. Due to the fast pace\nof software development, it is often the case that correct software\nspecifications are not thoroughly checked or validated within the software\nitself. Rather, they are frequently discussed and documented in a variety of\nexternal sources, including software manuals, code comments, and online\ndiscussion forums. Therefore, it is hard for the system administrator to know\nthe correct specifications of configurations due to the lack of clarity,\norganization, and a centralized unified source to look at. To address this\nchallenge, we propose SpecSyn a framework that leverages a state-of-the-art\nlarge language model to automatically synthesize software specifications from\nnatural language sources. Our approach formulates software specification\nsynthesis as a sequence-to-sequence learning problem and investigates the\nextraction of specifications from large contextual texts. This is the first\nwork that uses a large language model for end-to-end specification synthesis\nfrom natural language texts. Empirical results demonstrate that our system\noutperforms prior the state-of-the-art specification synthesis tool by 21% in\nterms of F1 score and can find specifications from single as well as multiple\nsentences.",
        "pdf_link": "https://arxiv.org/pdf/2304.09181v1.pdf"
    },
    {
        "title": "Classification of US Supreme Court Cases using BERT-Based Techniques",
        "authors": [
            "Shubham Vatsal",
            "Adam Meyers",
            "John E. Ortega"
        ],
        "published": "2023-04-17T22:53:54Z",
        "summary": "Models based on bidirectional encoder representations from transformers\n(BERT) produce state of the art (SOTA) results on many natural language\nprocessing (NLP) tasks such as named entity recognition (NER), part-of-speech\n(POS) tagging etc. An interesting phenomenon occurs when classifying long\ndocuments such as those from the US supreme court where BERT-based models can\nbe considered difficult to use on a first-pass or out-of-the-box basis. In this\npaper, we experiment with several BERT-based classification techniques for US\nsupreme court decisions or supreme court database (SCDB) and compare them with\nthe previous SOTA results. We then compare our results specifically with SOTA\nmodels for long documents. We compare our results for two classification tasks:\n(1) a broad classification task with 15 categories and (2) a fine-grained\nclassification task with 279 categories. Our best result produces an accuracy\nof 80\\% on the 15 broad categories and 60\\% on the fine-grained 279 categories\nwhich marks an improvement of 8\\% and 28\\% respectively from previously\nreported SOTA results.",
        "pdf_link": "https://arxiv.org/pdf/2304.08649v3.pdf"
    },
    {
        "title": "An Evaluation on Large Language Model Outputs: Discourse and Memorization",
        "authors": [
            "Adrian de Wynter",
            "Xun Wang",
            "Alex Sokolov",
            "Qilong Gu",
            "Si-Qing Chen"
        ],
        "published": "2023-04-17T22:12:12Z",
        "summary": "We present an empirical evaluation of various outputs generated by nine of\nthe most widely-available large language models (LLMs). Our analysis is done\nwith off-the-shelf, readily-available tools. We find a correlation between\npercentage of memorized text, percentage of unique text, and overall output\nquality, when measured with respect to output pathologies such as\ncounterfactual and logically-flawed statements, and general failures like not\nstaying on topic. Overall, 80.0% of the outputs evaluated contained memorized\ndata, but outputs containing the most memorized content were also more likely\nto be considered of high quality. We discuss and evaluate mitigation\nstrategies, showing that, in the models evaluated, the rate of memorized text\nbeing output is reduced. We conclude with a discussion on potential\nimplications around what it means to learn, to memorize, and to evaluate\nquality text.",
        "pdf_link": "https://arxiv.org/pdf/2304.08637v1.pdf"
    },
    {
        "title": "Visual Instruction Tuning",
        "authors": [
            "Haotian Liu",
            "Chunyuan Li",
            "Qingyang Wu",
            "Yong Jae Lee"
        ],
        "published": "2023-04-17T17:59:25Z",
        "summary": "Instruction tuning large language models (LLMs) using machine-generated\ninstruction-following data has improved zero-shot capabilities on new tasks,\nbut the idea is less explored in the multimodal field. In this paper, we\npresent the first attempt to use language-only GPT-4 to generate multimodal\nlanguage-image instruction-following data. By instruction tuning on such\ngenerated data, we introduce LLaVA: Large Language and Vision Assistant, an\nend-to-end trained large multimodal model that connects a vision encoder and\nLLM for general-purpose visual and language understanding.Our early experiments\nshow that LLaVA demonstrates impressive multimodel chat abilities, sometimes\nexhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and\nyields a 85.1% relative score compared with GPT-4 on a synthetic multimodal\ninstruction-following dataset. When fine-tuned on Science QA, the synergy of\nLLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make\nGPT-4 generated visual instruction tuning data, our model and code base\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2304.08485v2.pdf"
    },
    {
        "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
        "authors": [
            "Abdullatif Köksal",
            "Timo Schick",
            "Anna Korhonen",
            "Hinrich Schütze"
        ],
        "published": "2023-04-17T17:36:35Z",
        "summary": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. Finally, our models can\neffectively follow and answer multilingual instructions; we demonstrate this\nfor news generation. We publicly release our data and models:\nhttps://github.com/akoksal/LongForm.",
        "pdf_link": "https://arxiv.org/pdf/2304.08460v2.pdf"
    },
    {
        "title": "ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT",
        "authors": [
            "Chong Ma",
            "Zihao Wu",
            "Jiaqi Wang",
            "Shaochen Xu",
            "Yaonai Wei",
            "Zhengliang Liu",
            "Xi Jiang",
            "Lei Guo",
            "Xiaoyan Cai",
            "Shu Zhang",
            "Tuo Zhang",
            "Dajiang Zhu",
            "Dinggang Shen",
            "Tianming Liu",
            "Xiang Li"
        ],
        "published": "2023-04-17T17:13:42Z",
        "summary": "The 'Impression' section of a radiology report is a critical basis for\ncommunication between radiologists and other physicians, and it is typically\nwritten by radiologists based on the 'Findings' section. However, writing\nnumerous impressions can be laborious and error-prone for radiologists.\nAlthough recent studies have achieved promising results in automatic impression\ngeneration using large-scale medical text data for pre-training and fine-tuning\npre-trained language models, such models often require substantial amounts of\nmedical text data and have poor generalization performance. While large\nlanguage models (LLMs) like ChatGPT have shown strong generalization\ncapabilities and performance, their performance in specific domains, such as\nradiology, remains under-investigated and potentially limited. To address this\nlimitation, we propose ImpressionGPT, which leverages the in-context learning\ncapability of LLMs by constructing dynamic contexts using domain-specific,\nindividualized data. This dynamic prompt approach enables the model to learn\ncontextual knowledge from semantically similar examples from existing data.\nAdditionally, we design an iterative optimization algorithm that performs\nautomatic evaluation on the generated impression results and composes the\ncorresponding instruction prompts to further optimize the model. The proposed\nImpressionGPT model achieves state-of-the-art performance on both MIMIC-CXR and\nOpenI datasets without requiring additional training data or fine-tuning the\nLLMs. This work presents a paradigm for localizing LLMs that can be applied in\na wide range of similar application scenarios, bridging the gap between\ngeneral-purpose LLMs and the specific language processing needs of various\ndomains.",
        "pdf_link": "https://arxiv.org/pdf/2304.08448v2.pdf"
    },
    {
        "title": "Low-code LLM: Graphical User Interface over Large Language Models",
        "authors": [
            "Yuzhe Cai",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Zehua Wang",
            "Yaobo Liang",
            "Tao Ge",
            "Chenfei Wu",
            "Wang You",
            "Ting Song",
            "Yan Xia",
            "Jonathan Tien",
            "Nan Duan",
            "Furu Wei"
        ],
        "published": "2023-04-17T09:27:40Z",
        "summary": "Utilizing Large Language Models (LLMs) for complex tasks is challenging,\noften involving a time-consuming and uncontrollable prompt engineering process.\nThis paper introduces a novel human-LLM interaction framework, Low-code LLM. It\nincorporates six types of simple low-code visual programming interactions to\nachieve more controllable and stable responses. Through visual interaction with\na graphical user interface, users can incorporate their ideas into the process\nwithout writing trivial prompts. The proposed Low-code LLM framework consists\nof a Planning LLM that designs a structured planning workflow for complex\ntasks, which can be correspondingly edited and confirmed by users through\nlow-code visual programming operations, and an Executing LLM that generates\nresponses following the user-confirmed workflow. We highlight three advantages\nof the low-code LLM: user-friendly interaction, controllable generation, and\nwide applicability. We demonstrate its benefits using four typical\napplications. By introducing this framework, we aim to bridge the gap between\nhumans and LLMs, enabling more effective and efficient utilization of LLMs for\ncomplex tasks. The code, prompts, and experimental details are available at\nhttps://github.com/moymix/TaskMatrix/tree/main/LowCodeLLM. A system\ndemonstration video can be found at\nhttps://www.youtube.com/watch?v=jb2C1vaeO3E.",
        "pdf_link": "https://arxiv.org/pdf/2304.08103v3.pdf"
    },
    {
        "title": "InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction",
        "authors": [
            "Xiao Wang",
            "Weikang Zhou",
            "Can Zu",
            "Han Xia",
            "Tianze Chen",
            "Yuansen Zhang",
            "Rui Zheng",
            "Junjie Ye",
            "Qi Zhang",
            "Tao Gui",
            "Jihua Kang",
            "Jingsheng Yang",
            "Siyuan Li",
            "Chunsai Du"
        ],
        "published": "2023-04-17T09:00:50Z",
        "summary": "Large language models have unlocked strong multi-task capabilities from\nreading instructive prompts. However, recent studies have shown that existing\nlarge models still have difficulty with information extraction tasks. For\nexample, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset,\nwhich is significantly lower than the state-of-the-art performance. In this\npaper, we propose InstructUIE, a unified information extraction framework based\non instruction tuning, which can uniformly model various information extraction\ntasks and capture the inter-task dependency. To validate the proposed method,\nwe introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction\ndatasets in a unified text-to-text format with expert-written instructions.\nExperimental results demonstrate that our method achieves comparable\nperformance to Bert in supervised settings and significantly outperforms the\nstate-of-the-art and gpt3.5 in zero-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2304.08085v1.pdf"
    },
    {
        "title": "SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model",
        "authors": [
            "Nan Li",
            "Bo Kang",
            "Tijl De Bie"
        ],
        "published": "2023-04-17T08:43:20Z",
        "summary": "We present SkillGPT, a tool for skill extraction and standardization (SES)\nfrom free-style job descriptions and user profiles with an open-source Large\nLanguage Model (LLM) as backbone. Most previous methods for similar tasks\neither need supervision or rely on heavy data-preprocessing and feature\nengineering. Directly prompting the latest conversational LLM for standard\nskills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes\na LLM to perform its tasks in steps via summarization and vector similarity\nsearch, to balance speed with precision. The backbone LLM of SkillGPT is based\non Llama, free for academic use and thus useful for exploratory research and\nprototype development. Hence, our cost-free SkillGPT gives users the\nconvenience of conversational SES, efficiently and reliably.",
        "pdf_link": "https://arxiv.org/pdf/2304.11060v2.pdf"
    },
    {
        "title": "Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding",
        "authors": [
            "Ziang Xiao",
            "Xingdi Yuan",
            "Q. Vera Liao",
            "Rania Abdelghani",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2023-04-17T04:52:43Z",
        "summary": "Qualitative analysis of textual contents unpacks rich and valuable\ninformation by assigning labels to the data. However, this process is often\nlabor-intensive, particularly when working with large datasets. While recent\nAI-based tools demonstrate utility, researchers may not have readily available\nAI resources and expertise, let alone be challenged by the limited\ngeneralizability of those task-specific models. In this study, we explored the\nuse of large language models (LLMs) in supporting deductive coding, a major\ncategory of qualitative analysis where researchers use pre-determined codebooks\nto label the data into a fixed set of codes. Instead of training task-specific\nmodels, a pre-trained LLM could be used directly for various tasks without\nfine-tuning through prompt learning. Using a curiosity-driven questions coding\ntask as a case study, we found, by combining GPT-3 with expert-drafted\ncodebooks, our proposed approach achieved fair to substantial agreements with\nexpert-coded results. We lay out challenges and opportunities in using LLMs to\nsupport qualitative coding and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2304.10548v1.pdf"
    },
    {
        "title": "Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark",
        "authors": [
            "Michael V. Reiss"
        ],
        "published": "2023-04-17T00:41:19Z",
        "summary": "Recent studies have demonstrated promising potential of ChatGPT for various\ntext annotation and classification tasks. However, ChatGPT is non-deterministic\nwhich means that, as with human coders, identical input can lead to different\noutputs. Given this, it seems appropriate to test the reliability of ChatGPT.\nTherefore, this study investigates the consistency of ChatGPT's zero-shot\ncapabilities for text annotation and classification, focusing on different\nmodel parameters, prompt variations, and repetitions of identical inputs. Based\non the real-world classification task of differentiating website texts into\nnews and not news, results show that consistency in ChatGPT's classification\noutput can fall short of scientific thresholds for reliability. For example,\neven minor wording alterations in prompts or repeating the identical input can\nlead to varying outputs. Although pooling outputs from multiple repetitions can\nimprove reliability, this study advises caution when using ChatGPT for\nzero-shot text annotation and underscores the need for thorough validation,\nsuch as comparison against human-annotated data. The unsupervised application\nof ChatGPT for text annotation and classification is not recommended.",
        "pdf_link": "https://arxiv.org/pdf/2304.11085v1.pdf"
    },
    {
        "title": "VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping",
        "authors": [
            "Zheng Zhang",
            "Jie Gao",
            "Ranjodh Singh Dhaliwal",
            "Toby Jia-Jun Li"
        ],
        "published": "2023-04-16T15:29:03Z",
        "summary": "In argumentative writing, writers must brainstorm hierarchical writing goals,\nensure the persuasiveness of their arguments, and revise and organize their\nplans through drafting. Recent advances in large language models (LLMs) have\nmade interactive text generation through a chat interface (e.g., ChatGPT)\npossible. However, this approach often neglects implicit writing context and\nuser intent, lacks support for user control and autonomy, and provides limited\nassistance for sensemaking and revising writing plans. To address these\nchallenges, we introduce VISAR, an AI-enabled writing assistant system designed\nto help writers brainstorm and revise hierarchical goals within their writing\ncontext, organize argument structures through synchronized text editing and\nvisual programming, and enhance persuasiveness with argumentation spark\nrecommendations. VISAR allows users to explore, experiment with, and validate\ntheir writing plans using automatic draft prototyping. A controlled lab study\nconfirmed the usability and effectiveness of VISAR in facilitating the\nargumentative writing planning process.",
        "pdf_link": "https://arxiv.org/pdf/2304.07810v2.pdf"
    },
    {
        "title": "A Comprehensive Evaluation of Neural SPARQL Query Generation from Natural Language Questions",
        "authors": [
            "Papa Abdou Karim Karou Diallo",
            "Samuel Reyd",
            "Amal Zouaq"
        ],
        "published": "2023-04-16T13:12:26Z",
        "summary": "In recent years, the field of neural machine translation (NMT) for SPARQL\nquery generation has witnessed significant growth. Incorporating the copy\nmechanism with traditional encoder-decoder architectures and using pre-trained\nencoder-decoders and large language models have set new performance benchmarks.\nThis paper presents various experiments that replicate and expand upon recent\nNMT-based SPARQL generation studies, comparing pre-trained language models\n(PLMs), non-pre-trained language models (NPLMs), and large language models\n(LLMs), highlighting the impact of question annotation and the copy mechanism\nand testing various fine-tuning methods using LLMs. In particular, we provide a\nsystematic error analysis of the models and test their generalization ability.\nOur study demonstrates that the copy mechanism yields significant performance\nenhancements for most PLMs and NPLMs. Annotating the data is pivotal to\ngenerating correct URIs, with the \"tag-within\" strategy emerging as the most\neffective approach. Additionally, our findings reveal that the primary source\nof errors stems from incorrect URIs in SPARQL queries that are sometimes\nreplaced with hallucinated URIs when using base models. This does not happen\nusing the copy mechanism, but it sometimes leads to selecting wrong URIs among\ncandidates. Finally, the performance of the tested LLMs fell short of achieving\nthe desired outcomes.",
        "pdf_link": "https://arxiv.org/pdf/2304.07772v3.pdf"
    },
    {
        "title": "Solving Math Word Problems by Combining Language Models With Symbolic Solvers",
        "authors": [
            "Joy He-Yueya",
            "Gabriel Poesia",
            "Rose E. Wang",
            "Noah D. Goodman"
        ],
        "published": "2023-04-16T04:16:06Z",
        "summary": "Automatically generating high-quality step-by-step solutions to math word\nproblems has many applications in education. Recently, combining large language\nmodels (LLMs) with external tools to perform complex reasoning and calculation\nhas emerged as a promising direction for solving math word problems, but prior\napproaches such as Program-Aided Language model (PAL) are biased towards simple\nprocedural problems and less effective for problems that require declarative\nreasoning. We propose an approach that combines an LLM that can incrementally\nformalize word problems as a set of variables and equations with an external\nsymbolic solver that can solve the equations. Our approach achieves comparable\naccuracy to the original PAL on the GSM8K benchmark of math word problems and\noutperforms PAL by an absolute 20% on ALGEBRA, a new dataset of more\nchallenging word problems extracted from Algebra textbooks. Our work highlights\nthe benefits of using declarative and incremental representations when\ninterfacing with an external tool for solving complex math word problems. Our\ndata and prompts are publicly available at\nhttps://github.com/joyheyueya/declarative-math-word-problem.",
        "pdf_link": "https://arxiv.org/pdf/2304.09102v1.pdf"
    },
    {
        "title": "Tractable Control for Autoregressive Language Generation",
        "authors": [
            "Honghua Zhang",
            "Meihua Dang",
            "Nanyun Peng",
            "Guy Van den Broeck"
        ],
        "published": "2023-04-15T00:19:44Z",
        "summary": "Despite the success of autoregressive large language models in text\ngeneration, it remains a major challenge to generate text that satisfies\ncomplex constraints: sampling from the conditional distribution\n${\\Pr}(\\text{text} | \\alpha)$ is intractable for even the simplest lexical\nconstraints $\\alpha$. To overcome this challenge, we propose to use tractable\nprobabilistic models (TPMs) to impose lexical constraints in autoregressive\ntext generation models, which we refer to as GeLaTo (Generating Language with\nTractable Constraints). To demonstrate the effectiveness of this framework, we\nuse distilled hidden Markov models, where we can efficiently compute\n${\\Pr}(\\text{text} | \\alpha)$, to guide autoregressive generation from GPT2.\nGeLaTo achieves state-of-the-art performance on challenging benchmarks for\nconstrained text generation (e.g., CommonGen), beating various strong baselines\nby a large margin. Our work not only opens up new avenues for controlling large\nlanguage models but also motivates the development of more expressive TPMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.07438v4.pdf"
    },
    {
        "title": "Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models",
        "authors": [
            "Danny M. den Hamer",
            "Perry Schoor",
            "Tobias B. Polak",
            "Daniel Kapitan"
        ],
        "published": "2023-04-14T21:19:46Z",
        "summary": "Physicians considering clinical trials for their patients are met with the\nlaborious process of checking many text based eligibility criteria. Large\nLanguage Models (LLMs) have shown to perform well for clinical information\nextraction and clinical reasoning, including medical tests, but not yet in\nreal-world scenarios. This paper investigates the use of InstructGPT to assist\nphysicians in determining eligibility for clinical trials based on a patient's\nsummarised medical profile. Using a prompting strategy combining one-shot,\nselection-inference and chain-of-thought techniques, we investigate the\nperformance of LLMs on 10 synthetically created patient profiles. Performance\nis evaluated at four levels: ability to identify screenable eligibility\ncriteria from a trial given a medical profile; ability to classify for each\nindividual criterion whether the patient qualifies; the overall classification\nwhether a patient is eligible for a clinical trial and the percentage of\ncriteria to be screened by physician. We evaluated against 146 clinical trials\nand a total of 4,135 eligibility criteria. The LLM was able to correctly\nidentify the screenability of 72% (2,994/4,135) of the criteria. Additionally,\n72% (341/471) of the screenable criteria were evaluated correctly. The\nresulting trial level classification as eligible or ineligible resulted in a\nrecall of 0.5. By leveraging LLMs with a physician-in-the-loop, a recall of 1.0\nand precision of 0.71 on clinical trial level can be achieved while reducing\nthe amount of criteria to be checked by an estimated 90%. LLMs can be used to\nassist physicians with pre-screening of patients for clinical trials. By\nforcing instruction-tuned LLMs to produce chain-of-thought responses, the\nreasoning can be made transparent to and the decision process becomes amenable\nby physicians, thereby making such a system feasible for use in real-world\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.07396v2.pdf"
    },
    {
        "title": "The Self-Perception and Political Biases of ChatGPT",
        "authors": [
            "Jérôme Rutinowski",
            "Sven Franke",
            "Jan Endendyk",
            "Ina Dormuth",
            "Markus Pauly"
        ],
        "published": "2023-04-14T18:06:13Z",
        "summary": "This contribution analyzes the self-perception and political biases of\nOpenAI's Large Language Model ChatGPT. Taking into account the first\nsmall-scale reports and studies that have emerged, claiming that ChatGPT is\npolitically biased towards progressive and libertarian points of view, this\ncontribution aims to provide further clarity on this subject. For this purpose,\nChatGPT was asked to answer the questions posed by the political compass test\nas well as similar questionnaires that are specific to the respective politics\nof the G7 member states. These eight tests were repeated ten times each and\nrevealed that ChatGPT seems to hold a bias towards progressive views. The\npolitical compass test revealed a bias towards progressive and libertarian\nviews, with the average coordinates on the political compass being (-6.48,\n-5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes\nranging from -10 to 10), supporting the claims of prior research. The political\nquestionnaires for the G7 member states indicated a bias towards progressive\nviews but no significant bias between authoritarian and libertarian views,\ncontradicting the findings of prior reports, with the average coordinates being\n(-3.27, 0.58). In addition, ChatGPT's Big Five personality traits were tested\nusing the OCEAN test and its personality type was queried using the\nMyers-Briggs Type Indicator (MBTI) test. Finally, the maliciousness of ChatGPT\nwas evaluated using the Dark Factor test. These three tests were also repeated\nten times each, revealing that ChatGPT perceives itself as highly open and\nagreeable, has the Myers-Briggs personality type ENFJ, and is among the 15% of\ntest-takers with the least pronounced dark traits.",
        "pdf_link": "https://arxiv.org/pdf/2304.07333v1.pdf"
    },
    {
        "title": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment",
        "authors": [
            "Andreas Köpf",
            "Yannic Kilcher",
            "Dimitri von Rütte",
            "Sotiris Anagnostidis",
            "Zhi-Rui Tam",
            "Keith Stevens",
            "Abdullah Barhoum",
            "Nguyen Minh Duc",
            "Oliver Stanley",
            "Richárd Nagyfi",
            "Shahul ES",
            "Sameer Suri",
            "David Glushkov",
            "Arnav Dantuluri",
            "Andrew Maguire",
            "Christoph Schuhmann",
            "Huu Nguyen",
            "Alexander Mattick"
        ],
        "published": "2023-04-14T18:01:29Z",
        "summary": "Aligning large language models (LLMs) with human preferences has proven to\ndrastically improve usability and has driven rapid adoption as demonstrated by\nChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) greatly reduce the required\nskill and domain knowledge to effectively harness the capabilities of LLMs,\nincreasing their accessibility and utility across various domains. However,\nstate-of-the-art alignment techniques like RLHF rely on high-quality human\nfeedback data, which is expensive to create and often remains proprietary. In\nan effort to democratize research on large-scale alignment, we release\nOpenAssistant Conversations, a human-generated, human-annotated assistant-style\nconversation corpus consisting of 161,443 messages in 35 different languages,\nannotated with 461,292 quality ratings, resulting in over 10,000 complete and\nfully annotated conversation trees. The corpus is a product of a worldwide\ncrowd-sourcing effort involving over 13,500 volunteers. Models trained on\nOpenAssistant Conversations show consistent improvements on standard benchmarks\nover respective base models. We release our code and data under a fully\npermissive licence.",
        "pdf_link": "https://arxiv.org/pdf/2304.07327v2.pdf"
    },
    {
        "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
        "authors": [
            "Minghao Li",
            "Yingxiu Zhao",
            "Bowen Yu",
            "Feifan Song",
            "Hangyu Li",
            "Haiyang Yu",
            "Zhoujun Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-04-14T14:05:32Z",
        "summary": "Recent research has demonstrated that Large Language Models (LLMs) can\nenhance their capabilities by utilizing external tools. However, three pivotal\nquestions remain unanswered: (1) How effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs' ability to utilize tools? (3) What\nobstacles need to be overcome to leverage tools? To address these questions, we\nintroduce API-Bank, a groundbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question, we develop a runnable evaluation\nsystem consisting of 73 API tools. We annotate 314 tool-use dialogues with 753\nAPI calls to assess the existing LLMs' capabilities in planning, retrieving,\nand calling APIs. For the second question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000\ndistinct domains. Using this dataset, we train Lynx, a tool-augmented LLM\ninitialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits\nimproved tool utilization compared to GPT-3, while GPT-4 excels in planning.\nHowever, there is still significant potential for further improvement.\nMoreover, Lynx surpasses Alpaca's tool utilization performance by more than 26\npts and approaches the effectiveness of GPT-3.5. Through error analysis, we\nhighlight the key challenges for future research in this field to answer the\nthird question.",
        "pdf_link": "https://arxiv.org/pdf/2304.08244v2.pdf"
    },
    {
        "title": "DroidBot-GPT: GPT-powered UI Automation for Android",
        "authors": [
            "Hao Wen",
            "Hongming Wang",
            "Jiaxuan Liu",
            "Yuanchun Li"
        ],
        "published": "2023-04-14T11:31:56Z",
        "summary": "This paper introduces DroidBot-GPT, a tool that utilizes GPT-like large\nlanguage models (LLMs) to automate the interactions with Android mobile\napplications. Given a natural language description of a desired task,\nDroidBot-GPT can automatically generate and execute actions that navigate the\napp to complete the task. It works by translating the app GUI state information\nand the available actions on the smartphone screen to natural language prompts\nand asking the LLM to make a choice of actions. Since the LLM is typically\ntrained on a large amount of data including the how-to manuals of diverse\nsoftware applications, it has the ability to make reasonable choices of actions\nbased on the provided information. We evaluate DroidBot-GPT with a self-created\ndataset that contains 33 tasks collected from 17 Android applications spanning\n10 categories. It can successfully complete 39.39% of the tasks, and the\naverage partial completion progress is about 66.76%. Given the fact that our\nmethod is fully unsupervised (no modification required from both the app and\nthe LLM), we believe there is great potential to enhance automation performance\nwith better app development paradigms and/or custom model training.",
        "pdf_link": "https://arxiv.org/pdf/2304.07061v5.pdf"
    },
    {
        "title": "MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data",
        "authors": [
            "Tianyu Han",
            "Lisa C. Adams",
            "Jens-Michalis Papaioannou",
            "Paul Grundmann",
            "Tom Oberhauser",
            "Alexander Löser",
            "Daniel Truhn",
            "Keno K. Bressem"
        ],
        "published": "2023-04-14T11:28:08Z",
        "summary": "As large language models (LLMs) like OpenAI's GPT series continue to make\nstrides, we witness the emergence of artificial intelligence applications in an\never-expanding range of fields. In medicine, these LLMs hold considerable\npromise for improving medical workflows, diagnostics, patient care, and\neducation. Yet, there is an urgent need for open-source models that can be\ndeployed on-premises to safeguard patient privacy. In our work, we present an\ninnovative dataset consisting of over 160,000 entries, specifically crafted to\nfine-tune LLMs for effective medical applications. We investigate the impact of\nfine-tuning these datasets on publicly accessible pre-trained LLMs, and\nsubsequently, we juxtapose the performance of pre-trained-only models against\nthe fine-tuned models concerning the examinations that future medical doctors\nmust pass to achieve certification.",
        "pdf_link": "https://arxiv.org/pdf/2304.08247v2.pdf"
    },
    {
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
        "authors": [
            "Haochun Wang",
            "Chi Liu",
            "Nuwa Xi",
            "Zewen Qiang",
            "Sendong Zhao",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2023-04-14T07:54:17Z",
        "summary": "Large Language Models (LLMs), such as the LLaMA model, have demonstrated\ntheir effectiveness in various general-domain natural language processing (NLP)\ntasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain\ntasks due to the need for medical expertise in the responses. In response to\nthis challenge, we propose HuaTuo, a LLaMA-based model that has been\nsupervised-fine-tuned with generated QA (Question-Answer) instances. The\nexperimental results demonstrate that HuaTuo generates responses that possess\nmore reliable medical knowledge. Our proposed HuaTuo model is accessible at\nhttps://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.",
        "pdf_link": "https://arxiv.org/pdf/2304.06975v1.pdf"
    },
    {
        "title": "nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss Prediction across Scales",
        "authors": [
            "Yiqun Yao",
            "Siqi fan",
            "Xiusheng Huang",
            "Xuezhi Fang",
            "Xiang Li",
            "Ziyi Ni",
            "Xin Jiang",
            "Xuying Meng",
            "Peng Han",
            "Shuo Shang",
            "Kang Liu",
            "Aixin Sun",
            "Yequan Wang"
        ],
        "published": "2023-04-14T00:45:01Z",
        "summary": "As language models scale up, it becomes increasingly expensive to verify\nresearch ideas because conclusions on small models do not trivially transfer to\nlarge ones. A possible solution is to establish a generic system that\naccurately predicts certain metrics for large models without training them.\nExisting scaling laws require hyperparameter search on the largest models,\nlimiting their predicative capability. In this paper, we present an approach\n(namely {\\mu}Scaling) to predict the pre-training loss, based on our\nobservations that Maximal Update Parametrization ({\\mu}P) enables accurate\nfitting of scaling laws close to common loss basins in hyperparameter space.\nWith {\\mu}Scaling, different model designs can be compared on large scales by\ntraining only their smaller counterparts. Further, we introduce nanoLM: an\naffordable LLM pre-training benchmark that facilitates this new research\nparadigm. With around 14% of the one-time pre-training cost, we can accurately\nforecast the loss for models up to 52B. Our goal with nanoLM is to empower\nresearchers with limited resources to reach meaningful conclusions on large\nmodels. We also aspire for our benchmark to serve as a bridge between the\nacademic community and the industry. Code for {\\mu}Scaling is available at\nhttps://github.com/cofe-ai/Mu-scaling. Code for nanoLLM will be available\nlater.",
        "pdf_link": "https://arxiv.org/pdf/2304.06875v4.pdf"
    },
    {
        "title": "Stochastic Code Generation",
        "authors": [
            "Swapnil Sharma",
            "Nikita Anand",
            "Kranthi Kiran G. V"
        ],
        "published": "2023-04-14T00:01:05Z",
        "summary": "Large language models pre-trained for code generation can generate\nhigh-quality short code but often struggle with generating coherent long code\nand understanding higher-level or system-level specifications. This issue is\nalso observed in language modeling for long text generation, and one proposed\nsolution is the use of a latent stochastic process. This approach involves\ngenerating a document plan and then producing text that is consistent with it.\n  In this study, we investigate whether this technique can be applied to code\ngeneration to improve coherence. We base our proposed encoder and decoder on\nthe pre-trained GPT-2 based CodeParrot model and utilize the APPS dataset for\ntraining. We evaluate our results using the HumanEval benchmark and observe\nthat the modified Time Control model performs similarly to CodeParrot on this\nevaluation.",
        "pdf_link": "https://arxiv.org/pdf/2304.08243v1.pdf"
    },
    {
        "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
        "authors": [
            "Swapnil Sharma",
            "Nikita Anand",
            "Kranthi Kiran G. V.",
            "Alind Jain"
        ],
        "published": "2023-04-13T23:29:58Z",
        "summary": "Large pre-trained language models are widely used in the community. These\nmodels are usually trained on unmoderated and unfiltered data from open sources\nlike the Internet. Due to this, biases that we see in platforms online which\nare a reflection of those in society are in turn captured and learned by these\nmodels. These models are deployed in applications that affect millions of\npeople and their inherent biases are harmful to the targeted social groups. In\nthis work, we study the general trend in bias reduction as newer pre-trained\nmodels are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)\nare chosen and evaluated against two bias benchmarks, StereoSet and\nCrowS-Pairs. They are compared to the baseline of BERT using the associated\nmetrics. We explore whether as advancements are made and newer, faster, lighter\nmodels are released: are they being developed responsibly such that their\ninherent social biases have been reduced compared to their older counterparts?\nThe results are compiled and we find that all the models under study do exhibit\nbiases but have generally improved as compared to BERT.",
        "pdf_link": "https://arxiv.org/pdf/2304.06861v1.pdf"
    },
    {
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)",
        "authors": [
            "Toufique Ahmed",
            "Kunal Suresh Pai",
            "Premkumar Devanbu",
            "Earl T. Barr"
        ],
        "published": "2023-04-13T20:49:35Z",
        "summary": "Large Language Models (LLM) are a new class of computation engines,\n\"programmed\" via prompt engineering. We are still learning how to best\n\"program\" these LLMs to help developers. We start with the intuition that\ndevelopers tend to consciously and unconsciously have a collection of semantics\nfacts in mind when working on coding tasks. Mostly these are shallow, simple\nfacts arising from a quick read. For a function, examples of facts might\ninclude parameter and local variable names, return expressions, simple pre- and\npost-conditions, and basic control and data flow, etc.\n  One might assume that the powerful multi-layer architecture of\ntransformer-style LLMs makes them inherently capable of doing this simple level\nof \"code analysis\" and extracting such information, implicitly, while\nprocessing code: but are they, really? If they aren't, could explicitly adding\nthis information help? Our goal here is to investigate this question, using the\ncode summarization task and evaluate whether automatically augmenting an LLM's\nprompt with semantic facts explicitly, actually helps.\n  Prior work shows that LLM performance on code summarization benefits from\nfew-shot samples drawn either from the same-project or from examples found via\ninformation retrieval methods (such as BM25). While summarization performance\nhas steadily increased since the early days, there is still room for\nimprovement: LLM performance on code summarization still lags its performance\non natural-language tasks like translation and text summarization.\n  We find that adding semantic facts actually does help! This approach improves\nperformance in several different settings suggested by prior work, including\nfor two different Large Language Models. In most cases, improvement nears or\nexceeds 2 BLEU; for the PHP language in the challenging CodeSearchNet dataset,\nthis augmentation actually yields performance surpassing 30 BLEU.",
        "pdf_link": "https://arxiv.org/pdf/2304.06815v3.pdf"
    },
    {
        "title": "On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence",
        "authors": [
            "Gengchen Mai",
            "Weiming Huang",
            "Jin Sun",
            "Suhang Song",
            "Deepak Mishra",
            "Ninghao Liu",
            "Song Gao",
            "Tianming Liu",
            "Gao Cong",
            "Yingjie Hu",
            "Chris Cundy",
            "Ziyuan Li",
            "Rui Zhu",
            "Ni Lao"
        ],
        "published": "2023-04-13T19:50:17Z",
        "summary": "Large pre-trained models, also known as foundation models (FMs), are trained\nin a task-agnostic manner on large-scale data and can be adapted to a wide\nrange of downstream tasks by fine-tuning, few-shot, or even zero-shot learning.\nDespite their successes in language and vision tasks, we have yet seen an\nattempt to develop foundation models for geospatial artificial intelligence\n(GeoAI). In this work, we explore the promises and challenges of developing\nmultimodal foundation models for GeoAI. We first investigate the potential of\nmany existing FMs by testing their performances on seven tasks across multiple\ngeospatial subdomains including Geospatial Semantics, Health Geography, Urban\nGeography, and Remote Sensing. Our results indicate that on several geospatial\ntasks that only involve text modality such as toponym recognition, location\ndescription recognition, and US state-level/county-level dementia time series\nforecasting, these task-agnostic LLMs can outperform task-specific\nfully-supervised models in a zero-shot or few-shot learning setting. However,\non other geospatial tasks, especially tasks that involve multiple data\nmodalities (e.g., POI-based urban function classification, street view\nimage-based urban noise intensity classification, and remote sensing image\nscene classification), existing foundation models still underperform\ntask-specific models. Based on these observations, we propose that one of the\nmajor challenges of developing a FM for GeoAI is to address the multimodality\nnature of geospatial tasks. After discussing the distinct challenges of each\ngeospatial data modality, we suggest the possibility of a multimodal foundation\nmodel which can reason over various types of geospatial data through geospatial\nalignments. We conclude this paper by discussing the unique risks and\nchallenges to develop such a model for GeoAI.",
        "pdf_link": "https://arxiv.org/pdf/2304.06798v1.pdf"
    },
    {
        "title": "What does CLIP know about a red circle? Visual prompt engineering for VLMs",
        "authors": [
            "Aleksandar Shtedritski",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "published": "2023-04-13T17:58:08Z",
        "summary": "Large-scale Vision-Language Models, such as CLIP, learn powerful image-text\nrepresentations that have found numerous applications, from zero-shot\nclassification to text-to-image generation. Despite that, their capabilities\nfor solving novel discriminative tasks via prompting fall behind those of large\nlanguage models, such as GPT-3. Here we explore the idea of visual prompt\nengineering for solving computer vision tasks beyond classification by editing\nin image space instead of text. In particular, we discover an emergent ability\nof CLIP, where, by simply drawing a red circle around an object, we can direct\nthe model's attention to that region, while also maintaining global\ninformation. We show the power of this simple approach by achieving\nstate-of-the-art in zero-shot referring expressions comprehension and strong\nperformance in keypoint localization tasks. Finally, we draw attention to some\npotential ethical concerns of large language-vision models.",
        "pdf_link": "https://arxiv.org/pdf/2304.06712v2.pdf"
    },
    {
        "title": "Verbs in Action: Improving verb understanding in video-language models",
        "authors": [
            "Liliane Momeni",
            "Mathilde Caron",
            "Arsha Nagrani",
            "Andrew Zisserman",
            "Cordelia Schmid"
        ],
        "published": "2023-04-13T17:57:01Z",
        "summary": "Understanding verbs is crucial to modelling how people and objects interact\nwith each other and the environment through space and time. Recently,\nstate-of-the-art video-language models based on CLIP have been shown to have\nlimited verb understanding and to rely extensively on nouns, restricting their\nperformance in real-world video applications that require action and temporal\nunderstanding. In this work, we improve verb understanding for CLIP-based\nvideo-language models by proposing a new Verb-Focused Contrastive (VFC)\nframework. This consists of two main components: (1) leveraging pretrained\nlarge language models (LLMs) to create hard negatives for cross-modal\ncontrastive learning, together with a calibration strategy to balance the\noccurrence of concepts in positive and negative pairs; and (2) enforcing a\nfine-grained, verb phrase alignment loss. Our method achieves state-of-the-art\nresults for zero-shot performance on three downstream tasks that focus on verb\nunderstanding: video-text matching, video question-answering and video\nclassification. To the best of our knowledge, this is the first work which\nproposes a method to alleviate the verb understanding problem, and does not\nsimply highlight it.",
        "pdf_link": "https://arxiv.org/pdf/2304.06708v1.pdf"
    },
    {
        "title": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",
        "authors": [
            "Sunder Ali Khowaja",
            "Parus Khuwaja",
            "Kapal Dev",
            "Weizheng Wang",
            "Lewis Nkenyereye"
        ],
        "published": "2023-04-13T16:01:28Z",
        "summary": "ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.",
        "pdf_link": "https://arxiv.org/pdf/2305.03123v3.pdf"
    },
    {
        "title": "Are LLMs All You Need for Task-Oriented Dialogue?",
        "authors": [
            "Vojtěch Hudeček",
            "Ondřej Dušek"
        ],
        "published": "2023-04-13T14:03:14Z",
        "summary": "Instructions-tuned Large Language Models (LLMs) gained recently huge\npopularity thanks to their ability to interact with users through conversation.\nIn this work we aim to evaluate their ability to complete multi-turn tasks and\ninteract with external databases in the context of established task-oriented\ndialogue benchmarks. We show that for explicit belief state tracking, LLMs\nunderperform compared to specialized task-specific models. Nevertheless, they\nshow ability to guide the dialogue to successful ending if given correct slot\nvalues. Furthermore this ability improves with access to true belief state\ndistribution or in-domain examples.",
        "pdf_link": "https://arxiv.org/pdf/2304.06556v2.pdf"
    },
    {
        "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
        "authors": [
            "Wanjun Zhong",
            "Ruixiang Cui",
            "Yiduo Guo",
            "Yaobo Liang",
            "Shuai Lu",
            "Yanlin Wang",
            "Amin Saied",
            "Weizhu Chen",
            "Nan Duan"
        ],
        "published": "2023-04-13T09:39:30Z",
        "summary": "Evaluating the general abilities of foundation models to tackle human-level\ntasks is a vital aspect of their development and application in the pursuit of\nArtificial General Intelligence (AGI). Traditional benchmarks, which rely on\nartificial datasets, may not accurately represent human-level capabilities. In\nthis paper, we introduce AGIEval, a novel benchmark specifically designed to\nassess foundation model in the context of human-centric standardized exams,\nsuch as college entrance exams, law school admission tests, math competitions,\nand lawyer qualification tests. We evaluate several state-of-the-art foundation\nmodels, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark.\nImpressively, GPT-4 surpasses average human performance on SAT, LSAT, and math\ncompetitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5%\naccuracy on the English test of the Chinese national college entrance exam.\nThis demonstrates the extraordinary performance of contemporary foundation\nmodels. In contrast, we also find that GPT-4 is less proficient in tasks that\nrequire complex reasoning or specific domain knowledge. Our comprehensive\nanalyses of model capabilities (understanding, knowledge, reasoning, and\ncalculation) reveal these models' strengths and limitations, providing valuable\ninsights into future directions for enhancing their general capabilities. By\nconcentrating on tasks pertinent to human cognition and decision-making, our\nbenchmark delivers a more meaningful and robust evaluation of foundation\nmodels' performance in real-world scenarios. The data, code, and all model\noutputs are released in https://github.com/ruixiangcui/AGIEval.",
        "pdf_link": "https://arxiv.org/pdf/2304.06364v2.pdf"
    },
    {
        "title": "Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report",
        "authors": [
            "Jielin Qiu",
            "Jiacheng Zhu",
            "Shiqi Liu",
            "William Han",
            "Jingqi Zhang",
            "Chaojing Duan",
            "Michael Rosenberg",
            "Emerson Liu",
            "Douglas Weber",
            "Ding Zhao"
        ],
        "published": "2023-04-13T06:32:25Z",
        "summary": "Automated interpretation of electrocardiograms (ECG) has garnered significant\nattention with the advancements in machine learning methodologies. Despite the\ngrowing interest, most current studies focus solely on classification or\nregression tasks, which overlook a crucial aspect of clinical cardio-disease\ndiagnosis: the diagnostic report generated by experienced human clinicians. In\nthis paper, we introduce a novel approach to ECG interpretation, leveraging\nrecent breakthroughs in Large Language Models (LLMs) and Vision-Transformer\n(ViT) models. Rather than treating ECG diagnosis as a classification or\nregression task, we propose an alternative method of automatically identifying\nthe most similar clinical cases based on the input ECG data. Also, since\ninterpreting ECG as images is more affordable and accessible, we process ECG as\nencoded images and adopt a vision-language learning paradigm to jointly learn\nvision-language alignment between encoded ECG images and ECG diagnosis reports.\nEncoding ECG into images can result in an efficient ECG retrieval system, which\nwill be highly practical and useful in clinical applications. More importantly,\nour findings could serve as a crucial resource for providing diagnostic\nservices in underdeveloped regions.",
        "pdf_link": "https://arxiv.org/pdf/2304.06286v3.pdf"
    },
    {
        "title": "Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems",
        "authors": [
            "Qinghua Lu",
            "Liming Zhu",
            "Xiwei Xu",
            "Zhenchang Xing",
            "Jon Whittle"
        ],
        "published": "2023-04-13T05:01:03Z",
        "summary": "The release of ChatGPT has drawn huge interests on foundations models. There\nis a broad consensus that foundations models will be the fundamental building\nblocks for future AI systems. However, there is a lack of systematic guidance\non the architecture design. Particularly, the the rapidly growing capabilities\nof foundations models can eventually absorb other components of AI systems,\nposing challenges of moving boundary and interface evolution in architecture\ndesign. Furthermore, incorporating foundations models into AI systems raises\nsignificant concerns about responsible AI due to their opaque nature and\nrapidly advancing intelligence. To address these challenges, the paper first\npresents an architecture evolution of AI systems in the era of foundation\nmodels, transitioning from \"foundation-model-as-a-connector\" to\n\"foundation-model-as-a-monolithic architecture\". The paper then identifies key\ndesign decisions and proposes a pattern-oriented reference architecture for\ndesigning responsible foundation-model-based systems. The patterns can enable\nthe potential of foundation models while minimising associated risks.",
        "pdf_link": "https://arxiv.org/pdf/2304.11090v3.pdf"
    },
    {
        "title": "Language Instructed Reinforcement Learning for Human-AI Coordination",
        "authors": [
            "Hengyuan Hu",
            "Dorsa Sadigh"
        ],
        "published": "2023-04-13T04:47:31Z",
        "summary": "One of the fundamental quests of AI is to produce agents that coordinate well\nwith humans. This problem is challenging, especially in domains that lack high\nquality human behavioral data, because multi-agent reinforcement learning (RL)\noften converges to different equilibria from the ones that humans prefer. We\npropose a novel framework, instructRL, that enables humans to specify what kind\nof strategies they expect from their AI partners through natural language\ninstructions. We use pretrained large language models to generate a prior\npolicy conditioned on the human instruction and use the prior to regularize the\nRL objective. This leads to the RL agent converging to equilibria that are\naligned with human preferences. We show that instructRL converges to human-like\npolicies that satisfy the given instructions in a proof-of-concept environment\nas well as the challenging Hanabi benchmark. Finally, we show that knowing the\nlanguage instruction significantly boosts human-AI coordination performance in\nhuman evaluations in Hanabi.",
        "pdf_link": "https://arxiv.org/pdf/2304.07297v2.pdf"
    },
    {
        "title": "Detection of Fake Generated Scientific Abstracts",
        "authors": [
            "Panagiotis C. Theocharopoulos",
            "Panagiotis Anagnostou",
            "Anastasia Tsoukala",
            "Spiros V. Georgakopoulos",
            "Sotiris K. Tasoulis",
            "Vassilis P. Plagianakos"
        ],
        "published": "2023-04-12T20:20:22Z",
        "summary": "The widespread adoption of Large Language Models and publicly available\nChatGPT has marked a significant turning point in the integration of Artificial\nIntelligence into people's everyday lives. The academic community has taken\nnotice of these technological advancements and has expressed concerns regarding\nthe difficulty of discriminating between what is real and what is artificially\ngenerated. Thus, researchers have been working on developing effective systems\nto identify machine-generated text. In this study, we utilize the GPT-3 model\nto generate scientific paper abstracts through Artificial Intelligence and\nexplore various text representation methods when combined with Machine Learning\nmodels with the aim of identifying machine-written text. We analyze the models'\nperformance and address several research questions that rise during the\nanalysis of the results. By conducting this research, we shed light on the\ncapabilities and limitations of Artificial Intelligence generated text.",
        "pdf_link": "https://arxiv.org/pdf/2304.06148v1.pdf"
    },
    {
        "title": "Can Large Language Models Transform Computational Social Science?",
        "authors": [
            "Caleb Ziems",
            "William Held",
            "Omar Shaikh",
            "Jiaao Chen",
            "Zhehao Zhang",
            "Diyi Yang"
        ],
        "published": "2023-04-12T17:33:28Z",
        "summary": "Large Language Models (LLMs) are capable of successfully performing many\nlanguage processing tasks zero-shot (without training data). If zero-shot LLMs\ncan also reliably classify and explain social phenomena like persuasiveness and\npolitical ideology, then LLMs could augment the Computational Social Science\n(CSS) pipeline in important ways. This work provides a road map for using LLMs\nas CSS tools. Towards this end, we contribute a set of prompting best practices\nand an extensive evaluation pipeline to measure the zero-shot performance of 13\nlanguage models on 25 representative English CSS benchmarks. On taxonomic\nlabeling tasks (classification), LLMs fail to outperform the best fine-tuned\nmodels but still achieve fair levels of agreement with humans. On free-form\ncoding tasks (generation), LLMs produce explanations that often exceed the\nquality of crowdworkers' gold references. We conclude that the performance of\ntoday's LLMs can augment the CSS research pipeline in two ways: (1) serving as\nzero-shot data annotators on human annotation teams, and (2) bootstrapping\nchallenging creative generation tasks (e.g., explaining the underlying\nattributes of a text). In summary, LLMs are posed to meaningfully participate\nin social science analysis in partnership with humans.",
        "pdf_link": "https://arxiv.org/pdf/2305.03514v3.pdf"
    },
    {
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
        "authors": [
            "Viet Dac Lai",
            "Nghia Trung Ngo",
            "Amir Pouran Ben Veyseh",
            "Hieu Man",
            "Franck Dernoncourt",
            "Trung Bui",
            "Thien Huu Nguyen"
        ],
        "published": "2023-04-12T05:08:52Z",
        "summary": "Over the last few years, large language models (LLMs) have emerged as the\nmost important breakthroughs in natural language processing (NLP) that\nfundamentally transform research and developments in the field. ChatGPT\nrepresents one of the most exciting LLM systems developed recently to showcase\nimpressive skills for language generation and highly attract public attention.\nAmong various exciting applications discovered for ChatGPT in English, the\nmodel can process and generate texts for multiple languages due to its\nmultilingual training data. Given the broad adoption of ChatGPT for English in\ndifferent problems and areas, a natural question is whether ChatGPT can also be\napplied effectively for other languages or it is necessary to develop more\nlanguage-specific technologies. The answer to this question requires a thorough\nevaluation of ChatGPT over multiple tasks with diverse languages and large\ndatasets (i.e., beyond reported anecdotes), which is still missing or limited\nin current research. Our work aims to fill this gap for the evaluation of\nChatGPT and similar LLMs to provide more comprehensive information for\nmultilingual NLP applications. While this work will be an ongoing effort to\ninclude additional experiments in the future, our current paper evaluates\nChatGPT on 7 different tasks, covering 37 diverse languages with high, medium,\nlow, and extremely low resources. We also focus on the zero-shot learning\nsetting for ChatGPT to improve reproducibility and better simulate the\ninteractions of general users. Compared to the performance of previous models,\nour extensive experimental results demonstrate a worse performance of ChatGPT\nfor different NLP tasks and languages, calling for further research to develop\nbetter models and understanding for multilingual learning.",
        "pdf_link": "https://arxiv.org/pdf/2304.05613v1.pdf"
    },
    {
        "title": "Understanding Causality with Large Language Models: Feasibility and Opportunities",
        "authors": [
            "Cheng Zhang",
            "Stefan Bauer",
            "Paul Bennett",
            "Jiangfeng Gao",
            "Wenbo Gong",
            "Agrin Hilmkil",
            "Joel Jennings",
            "Chao Ma",
            "Tom Minka",
            "Nick Pawlowski",
            "James Vaughan"
        ],
        "published": "2023-04-11T22:30:03Z",
        "summary": "We assess the ability of large language models (LLMs) to answer causal\nquestions by analyzing their strengths and weaknesses against three types of\ncausal question. We believe that current LLMs can answer causal questions with\nexisting causal knowledge as combined domain experts. However, they are not yet\nable to provide satisfactory answers for discovering new knowledge or for\nhigh-stakes decision-making tasks with high precision. We discuss possible\nfuture directions and opportunities, such as enabling explicit and implicit\ncausal modules as well as deep causal-aware LLMs. These will not only enable\nLLMs to answer many different types of causal questions for greater impact but\nalso enable LLMs to be more trustworthy and efficient in general.",
        "pdf_link": "https://arxiv.org/pdf/2304.05524v1.pdf"
    },
    {
        "title": "Training Large Language Models Efficiently with Sparsity and Dataflow",
        "authors": [
            "Venkat Srinivasan",
            "Darshan Gandhi",
            "Urmish Thakker",
            "Raghu Prabhakar"
        ],
        "published": "2023-04-11T21:37:13Z",
        "summary": "Large foundation language models have shown their versatility in being able\nto be adapted to perform a wide variety of downstream tasks, such as text\ngeneration, sentiment analysis, semantic search etc. However, training such\nlarge foundational models is a non-trivial exercise that requires a significant\namount of compute power and expertise from machine learning and systems\nexperts. As models get larger, these demands are only increasing. Sparsity is a\npromising technique to relieve the compute requirements for training. However,\nsparsity introduces new challenges in training the sparse model to the same\nquality as the dense counterparts. Furthermore, sparsity drops the operation\nintensity and introduces irregular memory access patterns that makes it\nchallenging to efficiently utilize compute resources. This paper demonstrates\nan end-to-end training flow on a large language model - 13 billion GPT - using\nsparsity and dataflow. The dataflow execution model and architecture enables\nefficient on-chip irregular memory accesses as well as native kernel fusion and\npipelined parallelism that helps recover device utilization. We show that we\ncan successfully train GPT 13B to the same quality as the dense GPT 13B model,\nwhile achieving an end-end speedup of 4.5x over dense A100 baseline.",
        "pdf_link": "https://arxiv.org/pdf/2304.05511v1.pdf"
    },
    {
        "title": "chatClimate: Grounding Conversational AI in Climate Science",
        "authors": [
            "Saeid Ashraf Vaghefi",
            "Qian Wang",
            "Veruska Muccione",
            "Jingwei Ni",
            "Mathias Kraus",
            "Julia Bingler",
            "Tobias Schimanski",
            "Chiara Colesanti-Senni",
            "Nicolas Webersinke",
            "Christrian Huggel",
            "Markus Leippold"
        ],
        "published": "2023-04-11T21:31:39Z",
        "summary": "Large Language Models (LLMs) have made significant progress in recent years,\nachieving remarkable results in question-answering tasks (QA). However, they\nstill face two major challenges: hallucination and outdated information after\nthe training phase. These challenges take center stage in critical domains like\nclimate change, where obtaining accurate and up-to-date information from\nreliable sources in a limited time is essential and difficult. To overcome\nthese barriers, one potential solution is to provide LLMs with access to\nexternal, scientifically accurate, and robust sources (long-term memory) to\ncontinuously update their knowledge and prevent the propagation of inaccurate,\nincorrect, or outdated information. In this study, we enhanced GPT-4 by\nintegrating the information from the Sixth Assessment Report of the\nIntergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable\nsource in this domain. We present our conversational AI prototype, available at\nwww.chatclimate.ai and demonstrate its ability to answer challenging questions\naccurately in three different QA scenarios: asking from 1) GPT-4, 2)\nchatClimate, and 3) hybrid chatClimate. The answers and their sources were\nevaluated by our team of IPCC authors, who used their expert knowledge to score\nthe accuracy of the answers from 1 (very-low) to 5 (very-high). The evaluation\nshowed that the hybrid chatClimate provided more accurate answers, highlighting\nthe effectiveness of our solution. This approach can be easily scaled for\nchatbots in specific domains, enabling the delivery of reliable and accurate\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2304.05510v2.pdf"
    },
    {
        "title": "Zero-shot Temporal Relation Extraction with ChatGPT",
        "authors": [
            "Chenhan Yuan",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2023-04-11T18:59:05Z",
        "summary": "The goal of temporal relation extraction is to infer the temporal relation\nbetween two events in the document. Supervised models are dominant in this\ntask. In this work, we investigate ChatGPT's ability on zero-shot temporal\nrelation extraction. We designed three different prompt techniques to break\ndown the task and evaluate ChatGPT. Our experiments show that ChatGPT's\nperformance has a large gap with that of supervised methods and can heavily\nrely on the design of prompts. We further demonstrate that ChatGPT can infer\nmore small relation classes correctly than supervised methods. The current\nshortcomings of ChatGPT on temporal relation extraction are also discussed in\nthis paper. We found that ChatGPT cannot keep consistency during temporal\ninference and it fails in actively long-dependency temporal inference.",
        "pdf_link": "https://arxiv.org/pdf/2304.05454v1.pdf"
    },
    {
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
        "authors": [
            "Ameet Deshpande",
            "Vishvak Murahari",
            "Tanmay Rajpurohit",
            "Ashwin Kalyan",
            "Karthik Narasimhan"
        ],
        "published": "2023-04-11T16:53:54Z",
        "summary": "Large language models (LLMs) have shown incredible capabilities and\ntranscended the natural language processing (NLP) community, with adoption\nthroughout many services like healthcare, therapy, education, and customer\nservice. Since users include people with critical information needs like\nstudents or patients engaging with chatbots, the safety of these systems is of\nprime importance. Therefore, a clear understanding of the capabilities and\nlimitations of LLMs is necessary. To this end, we systematically evaluate\ntoxicity in over half a million generations of ChatGPT, a popular\ndialogue-based LLM. We find that setting the system parameter of ChatGPT by\nassigning it a persona, say that of the boxer Muhammad Ali, significantly\nincreases the toxicity of generations. Depending on the persona assigned to\nChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect\nstereotypes, harmful dialogue, and hurtful opinions. This may be potentially\ndefamatory to the persona and harmful to an unsuspecting user. Furthermore, we\nfind concerning patterns where specific entities (e.g., certain races) are\ntargeted more than others (3x more) irrespective of the assigned persona, that\nreflect inherent discriminatory biases in the model. We hope that our findings\ninspire the broader AI community to rethink the efficacy of current safety\nguardrails and develop better techniques that lead to robust, safe, and\ntrustworthy AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2304.05335v1.pdf"
    },
    {
        "title": "TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Machine Learning",
        "authors": [
            "William Won",
            "Midhilesh Elavazhagan",
            "Sudarshan Srinivasan",
            "Ajaya Durg",
            "Samvit Kaul",
            "Swati Gupta",
            "Tushar Krishna"
        ],
        "published": "2023-04-11T15:50:54Z",
        "summary": "The surge of artificial intelligence, specifically large language models, has\nled to a rapid advent towards the development of large-scale machine learning\ntraining clusters. Collective communications within these clusters tend to be\nheavily bandwidth-bound, necessitating techniques to optimally utilize the\navailable network bandwidth. This puts the routing algorithm for the collective\nat the forefront of determining the performance. Unfortunately, communication\nlibraries used in distributed machine learning today are limited by a fixed set\nof routing algorithms. This constraints collective performance within the\ndomain of next-generation training clusters that employ intricate,\nheterogeneous, and asymmetric, large-scale topologies. Further, the emergence\nof irregular topologies attributed to runtime phenomena such as device failures\nserves to compound the complexity of the challenge. To this end, this paper\nintroduces TACOS, an automated synthesizer that generates topology-aware\ncollective algorithms for common distributed machine learning collectives\nacross arbitrary input network topologies. TACOS was able to synthesize\nAll-Reduce algorithm for a heterogeneous 512-NPU system in just 6.09 minutes\nwhile achieving performance improvement up to 4.27x over state-of-the-art prior\nwork. TACOS exhibits high scalability, with synthesis time scaling\nquadratically with the number of NPUs. In contrast to prior works' NP-hard\napproaches, TACOS with 40K NPUs completes in 2.52 hours.",
        "pdf_link": "https://arxiv.org/pdf/2304.05301v2.pdf"
    },
    {
        "title": "Approximating Online Human Evaluation of Social Chatbots with Prompting",
        "authors": [
            "Ekaterina Svikhnushina",
            "Pearl Pu"
        ],
        "published": "2023-04-11T14:45:01Z",
        "summary": "As conversational models become increasingly available to the general public,\nusers are engaging with this technology in social interactions. Such\nunprecedented interaction experiences may pose considerable social and\npsychological risks to the users unless the technology is properly controlled.\nThis highlights the need for scalable and robust evaluation metrics for\nconversational chatbots. Existing evaluation metrics aim to automate offline\nuser evaluation and approximate human judgment of pre-curated dialogs. However,\nthey are limited in their ability to capture subjective perceptions of users\nwho actually interact with the bots and might not generalize to real-world\nsettings. To address this limitation, we propose an approach to approximate\nonline human evaluation leveraging large language models (LLMs) from the GPT\nfamily. We introduce a new Dialog system Evaluation framework based on\nPrompting (DEP), which enables a fully automatic evaluation pipeline that\nreplicates live user studies and achieves an impressive correlation with human\njudgment (up to Pearson r=0.95 on a system level). The DEP approach involves\ncollecting synthetic chat logs of evaluated bots with an LLM in the other-play\nsetting, where the LLM is carefully conditioned to follow a specific scenario.\nWe further explore different prompting approaches to produce evaluation scores\nwith the same LLM. The best performing prompts, which contain few-shot\ndemonstrations and instructions, show outstanding performance on the tested\ndataset and demonstrate the ability to generalize to other dialog corpora.",
        "pdf_link": "https://arxiv.org/pdf/2304.05253v2.pdf"
    },
    {
        "title": "Towards preserving word order importance through Forced Invalidation",
        "authors": [
            "Hadeel Al-Negheimish",
            "Pranava Madhyastha",
            "Alessandra Russo"
        ],
        "published": "2023-04-11T13:42:10Z",
        "summary": "Large pre-trained language models such as BERT have been widely used as a\nframework for natural language understanding (NLU) tasks. However, recent\nfindings have revealed that pre-trained language models are insensitive to word\norder. The performance on NLU tasks remains unchanged even after randomly\npermuting the word of a sentence, where crucial syntactic information is\ndestroyed. To help preserve the importance of word order, we propose a simple\napproach called Forced Invalidation (FI): forcing the model to identify\npermuted sequences as invalid samples. We perform an extensive evaluation of\nour approach on various English NLU and QA based tasks over BERT-based and\nattention-based models over word embeddings. Our experiments demonstrate that\nForced Invalidation significantly improves the sensitivity of the models to\nword order.",
        "pdf_link": "https://arxiv.org/pdf/2304.05221v1.pdf"
    },
    {
        "title": "Multi-step Jailbreaking Privacy Attacks on ChatGPT",
        "authors": [
            "Haoran Li",
            "Dadi Guo",
            "Wei Fan",
            "Mingshi Xu",
            "Jie Huang",
            "Fanpu Meng",
            "Yangqiu Song"
        ],
        "published": "2023-04-11T13:05:04Z",
        "summary": "With the rapid progress of large language models (LLMs), many downstream NLP\ntasks can be well solved given appropriate prompts. Though model developers and\nresearchers work hard on dialog safety to avoid generating harmful content from\nLLMs, it is still challenging to steer AI-generated content (AIGC) for the\nhuman good. As powerful LLMs are devouring existing text data from various\ndomains (e.g., GPT-3 is trained on 45TB texts), it is natural to doubt whether\nthe private information is included in the training data and what privacy\nthreats can these LLMs and their downstream applications bring. In this paper,\nwe study the privacy threats from OpenAI's ChatGPT and the New Bing enhanced by\nChatGPT and show that application-integrated LLMs may cause new privacy\nthreats. To this end, we conduct extensive experiments to support our claims\nand discuss LLMs' privacy implications.",
        "pdf_link": "https://arxiv.org/pdf/2304.05197v3.pdf"
    },
    {
        "title": "Teaching Large Language Models to Self-Debug",
        "authors": [
            "Xinyun Chen",
            "Maxwell Lin",
            "Nathanael Schärli",
            "Denny Zhou"
        ],
        "published": "2023-04-11T10:43:43Z",
        "summary": "Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any human feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by investigating the\nexecution results and explaining the generated code in natural language.\nSelf-Debugging achieves the state-of-the-art performance on several code\ngeneration benchmarks, including the Spider dataset for text-to-SQL generation,\nTransCoder for C++-to-Python translation, and MBPP for text-to-Python\ngeneration. On the Spider benchmark where there are no unit tests to verify the\ncorrectness of predictions, Self-Debugging with code explanation consistently\nimproves the baseline by 2-3%, and improves the prediction accuracy on problems\nof the hardest level by 9%. On TransCoder and MBPP where unit tests are\navailable, Self-Debugging improves the baseline accuracy by up to 12%.\nMeanwhile, by leveraging feedback messages and reusing failed predictions,\nSelf-Debugging notably improves sample efficiency, and can match or outperform\nbaseline models that generate more than 10x candidate programs.",
        "pdf_link": "https://arxiv.org/pdf/2304.05128v2.pdf"
    },
    {
        "title": "Human-machine cooperation for semantic feature listing",
        "authors": [
            "Kushin Mukherjee",
            "Siddharth Suresh",
            "Timothy T. Rogers"
        ],
        "published": "2023-04-11T06:38:04Z",
        "summary": "Semantic feature norms, lists of features that concepts do and do not\npossess, have played a central role in characterizing human conceptual\nknowledge, but require extensive human labor. Large language models (LLMs)\noffer a novel avenue for the automatic generation of such feature lists, but\nare prone to significant error. Here, we present a new method for combining a\nlearned model of human lexical-semantics from limited data with LLM-generated\ndata to efficiently generate high-quality feature norms.",
        "pdf_link": "https://arxiv.org/pdf/2304.05012v1.pdf"
    },
    {
        "title": "Towards an Understanding and Explanation for Mixed-Initiative Artificial Scientific Text Detection",
        "authors": [
            "Luoxuan Weng",
            "Minfeng Zhu",
            "Kam Kwai Wong",
            "Shi Liu",
            "Jiashun Sun",
            "Hang Zhu",
            "Dongming Han",
            "Wei Chen"
        ],
        "published": "2023-04-11T06:37:30Z",
        "summary": "Large language models (LLMs) have gained popularity in various fields for\ntheir exceptional capability of generating human-like text. Their potential\nmisuse has raised social concerns about plagiarism in academic contexts.\nHowever, effective artificial scientific text detection is a non-trivial task\ndue to several challenges, including 1) the lack of a clear understanding of\nthe differences between machine-generated and human-written scientific text, 2)\nthe poor generalization performance of existing methods caused by\nout-of-distribution issues, and 3) the limited support for human-machine\ncollaboration with sufficient interpretability during the detection process. In\nthis paper, we first identify the critical distinctions between\nmachine-generated and human-written scientific text through a quantitative\nexperiment. Then, we propose a mixed-initiative workflow that combines human\nexperts' prior knowledge with machine intelligence, along with a visual\nanalytics prototype to facilitate efficient and trustworthy scientific text\ndetection. Finally, we demonstrate the effectiveness of our approach through\ntwo case studies and a controlled user study with proficient researchers. We\nalso provide design implications for interactive artificial text detection\ntools in high-stakes decision-making scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2304.05011v1.pdf"
    },
    {
        "title": "A Cheaper and Better Diffusion Language Model with Soft-Masked Noise",
        "authors": [
            "Jiaao Chen",
            "Aston Zhang",
            "Mu Li",
            "Alex Smola",
            "Diyi Yang"
        ],
        "published": "2023-04-10T17:58:42Z",
        "summary": "Diffusion models that are based on iterative denoising have been recently\nproposed and leveraged in various generation tasks like image generation.\nWhereas, as a way inherently built for continuous data, existing diffusion\nmodels still have some limitations in modeling discrete data, e.g., languages.\nFor example, the generally used Gaussian noise can not handle the discrete\ncorruption well, and the objectives in continuous spaces fail to be stable for\ntextual data in the diffusion process especially when the dimension is high. To\nalleviate these issues, we introduce a novel diffusion model for language\nmodeling, Masked-Diffuse LM, with lower training cost and better performances,\ninspired by linguistic features in languages. Specifically, we design a\nlinguistic-informed forward process which adds corruptions to the text through\nstrategically soft-masking to better noise the textual data. Also, we directly\npredict the categorical distribution with cross-entropy loss function in every\ndiffusion step to connect the continuous space and discrete space in a more\nefficient and straightforward way. Through experiments on 5 controlled\ngeneration tasks, we demonstrate that our Masked-Diffuse LM can achieve better\ngeneration quality than the state-of-the-art diffusion models with better\nefficiency.",
        "pdf_link": "https://arxiv.org/pdf/2304.04746v1.pdf"
    },
    {
        "title": "On the Possibilities of AI-Generated Text Detection",
        "authors": [
            "Souradip Chakraborty",
            "Amrit Singh Bedi",
            "Sicheng Zhu",
            "Bang An",
            "Dinesh Manocha",
            "Furong Huang"
        ],
        "published": "2023-04-10T17:47:39Z",
        "summary": "Our work addresses the critical issue of distinguishing text generated by\nLarge Language Models (LLMs) from human-produced text, a task essential for\nnumerous applications. Despite ongoing debate about the feasibility of such\ndifferentiation, we present evidence supporting its consistent achievability,\nexcept when human and machine text distributions are indistinguishable across\ntheir entire support. Drawing from information theory, we argue that as\nmachine-generated text approximates human-like quality, the sample size needed\nfor detection increases. We establish precise sample complexity bounds for\ndetecting AI-generated text, laying groundwork for future research aimed at\ndeveloping advanced, multi-sample detectors. Our empirical evaluations across\nmultiple datasets (Xsum, Squad, IMDb, and Kaggle FakeNews) confirm the\nviability of enhanced detection methods. We test various state-of-the-art text\ngenerators, including GPT-2, GPT-3.5-Turbo, Llama, Llama-2-13B-Chat-HF, and\nLlama-2-70B-Chat-HF, against detectors, including oBERTa-Large/Base-Detector,\nGPTZero. Our findings align with OpenAI's empirical data related to sequence\nlength, marking the first theoretical substantiation for these observations.",
        "pdf_link": "https://arxiv.org/pdf/2304.04736v3.pdf"
    },
    {
        "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis",
        "authors": [
            "Wenhao Zhu",
            "Hongyi Liu",
            "Qingxiu Dong",
            "Jingjing Xu",
            "Shujian Huang",
            "Lingpeng Kong",
            "Jiajun Chen",
            "Lei Li"
        ],
        "published": "2023-04-10T15:51:30Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable potential in\nhandling multilingual machine translation (MMT). In this paper, we\nsystematically investigate the advantages and challenges of LLMs for MMT by\nanswering two questions: 1) How well do LLMs perform in translating massive\nlanguages? 2) Which factors affect LLMs' performance in translation? We\nthoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our\nempirical results show that translation capabilities of LLMs are continually\nimproving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of\ntranslation directions but still faces a large gap towards the commercial\ntranslation system, especially on low-resource languages. Through further\nanalysis, we discover that LLMs exhibit new working patterns when used for MMT.\nFirst, instruction semantics can surprisingly be ignored when given in-context\nexemplars. Second, cross-lingual exemplars can provide better task guidance for\nlow-resource translation than exemplars in the same language pairs. Third, LLM\ncan acquire translation ability in a resource-efficient way and generate\nmoderate translation even on zero-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2304.04675v3.pdf"
    },
    {
        "title": "Learnings from Data Integration for Augmented Language Models",
        "authors": [
            "Alon Halevy",
            "Jane Dwivedi-Yu"
        ],
        "published": "2023-04-10T13:28:35Z",
        "summary": "One of the limitations of large language models is that they do not have\naccess to up-to-date, proprietary or personal data. As a result, there are\nmultiple efforts to extend language models with techniques for accessing\nexternal data. In that sense, LLMs share the vision of data integration systems\nwhose goal is to provide seamless access to a large collection of heterogeneous\ndata sources. While the details and the techniques of LLMs differ greatly from\nthose of data integration, this paper shows that some of the lessons learned\nfrom research on data integration can elucidate the research path we are\nconducting today on language models.",
        "pdf_link": "https://arxiv.org/pdf/2304.04576v1.pdf"
    },
    {
        "title": "Inference with Reference: Lossless Acceleration of Large Language Models",
        "authors": [
            "Nan Yang",
            "Tao Ge",
            "Liang Wang",
            "Binxing Jiao",
            "Daxin Jiang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "published": "2023-04-10T09:55:14Z",
        "summary": "We propose LLMA, an LLM accelerator to losslessly speed up Large Language\nModel (LLM) inference with references. LLMA is motivated by the observation\nthat there are abundant identical text spans between the decoding result by an\nLLM and the reference that is available in many real world scenarios (e.g.,\nretrieved documents). LLMA first selects a text span from the reference and\ncopies its tokens to the decoder and then efficiently checks the tokens'\nappropriateness as the decoding result in parallel within one decoding step.\nThe improved computational parallelism allows LLMA to achieve over 2x speed-up\nfor LLMs with identical generation results as greedy decoding in many practical\ngeneration scenarios where significant overlap between in-context reference and\noutputs exists (e.g., search engines and multi-turn conversations).",
        "pdf_link": "https://arxiv.org/pdf/2304.04487v1.pdf"
    },
    {
        "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
        "authors": [
            "Jiawei Zhang"
        ],
        "published": "2023-04-10T05:25:54Z",
        "summary": "In this paper, we aim to develop a large language model (LLM) with the\nreasoning ability on complex graph data. Currently, LLMs have achieved very\nimpressive performance on various natural language learning tasks, extensions\nof which have also been applied to study the vision tasks with multi-modal\ndata. However, when it comes to the graph learning tasks, existing LLMs present\nvery serious flaws due to their several inherited weaknesses in performing\n{multi-step logic reasoning}, {precise mathematical calculation} and\n{perception about the spatial and temporal factors}.\n  To address such challenges, in this paper, we will investigate the\nprinciples, methodologies and algorithms to empower existing LLMs with graph\nreasoning ability, which will have tremendous impacts on the current research\nof both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer\nmodels, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)\nframework to teach LLMs themselves with prompts augmented by ChatGPT to use\nexternal graph reasoning API tools. Specifically, we will investigate to teach\nGraph-ToolFormer to handle various graph data reasoning tasks in this paper,\nincluding both (1) very basic graph data loading and graph property reasoning\ntasks, ranging from simple graph order and size to the graph diameter and\nperiphery, and (2) more advanced reasoning tasks on real-world graph data, such\nas bibliographic networks, protein molecules, sequential recommender systems,\nsocial networks and knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2304.11116v3.pdf"
    },
    {
        "title": "The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Yanzhao Lai",
            "Min Peng",
            "Jimin Huang"
        ],
        "published": "2023-04-10T04:31:00Z",
        "summary": "Recently, large language models (LLMs) like ChatGPT have demonstrated\nremarkable performance across a variety of natural language processing tasks.\nHowever, their effectiveness in the financial domain, specifically in\npredicting stock market movements, remains to be explored. In this paper, we\nconduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal\nstock movement prediction, on three tweets and historical stock price datasets.\nOur findings indicate that ChatGPT is a \"Wall Street Neophyte\" with limited\nsuccess in predicting stock movements, as it underperforms not only\nstate-of-the-art methods but also traditional methods like linear regression\nusing price features. Despite the potential of Chain-of-Thought prompting\nstrategies and the inclusion of tweets, ChatGPT's performance remains subpar.\nFurthermore, we observe limitations in its explainability and stability,\nsuggesting the need for more specialized training or fine-tuning. This research\nprovides insights into ChatGPT's capabilities and serves as a foundation for\nfuture work aimed at improving financial market analysis and prediction by\nleveraging social media sentiment and historical stock data.",
        "pdf_link": "https://arxiv.org/pdf/2304.05351v2.pdf"
    },
    {
        "title": "OpenAGI: When LLM Meets Domain Experts",
        "authors": [
            "Yingqiang Ge",
            "Wenyue Hua",
            "Kai Mei",
            "Jianchao Ji",
            "Juntao Tan",
            "Shuyuan Xu",
            "Zelong Li",
            "Yongfeng Zhang"
        ],
        "published": "2023-04-10T03:55:35Z",
        "summary": "Human Intelligence (HI) excels at combining basic skills to solve complex\ntasks. This capability is vital for Artificial Intelligence (AI) and should be\nembedded in comprehensive AI Agents, enabling them to harness expert models for\ncomplex task-solving towards Artificial General Intelligence (AGI). Large\nLanguage Models (LLMs) show promising learning and reasoning abilities, and can\neffectively use external models, tools, plugins, or APIs to tackle complex\nproblems. In this work, we introduce OpenAGI, an open-source AGI research and\ndevelopment platform designed for solving multi-step, real-world tasks.\nSpecifically, OpenAGI uses a dual strategy, integrating standard benchmark\ntasks for benchmarking and evaluation, and open-ended tasks including more\nexpandable models, tools, plugins, or APIs for creative problem-solving. Tasks\nare presented as natural language queries to the LLM, which then selects and\nexecutes appropriate models. We also propose a Reinforcement Learning from Task\nFeedback (RLTF) mechanism that uses task results to improve the LLM's\ntask-solving ability, which creates a self-improving AI feedback loop. While we\nacknowledge that AGI is a broad and multifaceted research challenge with no\nsingularly defined solution path, the integration of LLMs with domain-specific\nexpert models, inspired by mirroring the blend of general and specialized\nintelligence in humans, offers a promising approach towards AGI. We are\nopen-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation\nmethods, and the UI demo to foster community involvement in AGI advancement:\nhttps://github.com/agiresearch/OpenAGI.",
        "pdf_link": "https://arxiv.org/pdf/2304.04370v6.pdf"
    },
    {
        "title": "Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao",
            "Linda Petzold"
        ],
        "published": "2023-04-09T16:31:47Z",
        "summary": "Large language models (LLMs) have made significant progress in various\ndomains, including healthcare. However, the specialized nature of clinical\nlanguage understanding tasks presents unique challenges and limitations that\nwarrant further investigation. In this study, we conduct a comprehensive\nevaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within\nthe realm of clinical language understanding tasks. These tasks span a diverse\nrange, including named entity recognition, relation extraction, natural\nlanguage inference, semantic textual similarity, document classification, and\nquestion-answering. We also introduce a novel prompting strategy,\nself-questioning prompting (SQP), tailored to enhance LLMs' performance by\neliciting informative questions and answers pertinent to the clinical scenarios\nat hand. Our evaluation underscores the significance of task-specific learning\nstrategies and prompting techniques for improving LLMs' effectiveness in\nhealthcare-related tasks. Additionally, our in-depth error analysis on the\nchallenging relation extraction task offers valuable insights into error\ndistribution and potential avenues for improvement using SQP. Our study sheds\nlight on the practical implications of employing LLMs in the specialized domain\nof healthcare, serving as a foundation for future research and the development\nof potential applications in healthcare settings.",
        "pdf_link": "https://arxiv.org/pdf/2304.05368v3.pdf"
    },
    {
        "title": "A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding",
        "authors": [
            "Wenbo Pan",
            "Qiguang Chen",
            "Xiao Xu",
            "Wanxiang Che",
            "Libo Qin"
        ],
        "published": "2023-04-09T15:28:36Z",
        "summary": "Zero-shot dialogue understanding aims to enable dialogue to track the user's\nneeds without any training data, which has gained increasing attention. In this\nwork, we investigate the understanding ability of ChatGPT for zero-shot\ndialogue understanding tasks including spoken language understanding (SLU) and\ndialogue state tracking (DST). Experimental results on four popular benchmarks\nreveal the great potential of ChatGPT for zero-shot dialogue understanding. In\naddition, extensive analysis shows that ChatGPT benefits from the multi-turn\ninteractive prompt in the DST task but struggles to perform slot filling for\nSLU. Finally, we summarize several unexpected behaviors of ChatGPT in dialogue\nunderstanding tasks, hoping to provide some insights for future research on\nbuilding zero-shot dialogue understanding systems with Large Language Models\n(LLMs).",
        "pdf_link": "https://arxiv.org/pdf/2304.04256v1.pdf"
    },
    {
        "title": "Can ChatGPT and Bard Generate Aligned Assessment Items? A Reliability Analysis against Human Performance",
        "authors": [
            "Abdolvahab Khademi"
        ],
        "published": "2023-04-09T04:53:15Z",
        "summary": "ChatGPT and Bard are AI chatbots based on Large Language Models (LLM) that\nare slated to promise different applications in diverse areas. In education,\nthese AI technologies have been tested for applications in assessment and\nteaching. In assessment, AI has long been used in automated essay scoring and\nautomated item generation. One psychometric property that these tools must have\nto assist or replace humans in assessment is high reliability in terms of\nagreement between AI scores and human raters. In this paper, we measure the\nreliability of OpenAI ChatGP and Google Bard LLMs tools against experienced and\ntrained humans in perceiving and rating the complexity of writing prompts.\nIntraclass correlation (ICC) as a performance metric showed that the\ninter-reliability of both the OpenAI ChatGPT and the Google Bard were low\nagainst the gold standard of human ratings.",
        "pdf_link": "https://arxiv.org/pdf/2304.05372v1.pdf"
    },
    {
        "title": "Unsupervised Story Discovery from Continuous News Streams via Scalable Thematic Embedding",
        "authors": [
            "Susik Yoon",
            "Dongha Lee",
            "Yunyi Zhang",
            "Jiawei Han"
        ],
        "published": "2023-04-08T20:41:15Z",
        "summary": "Unsupervised discovery of stories with correlated news articles in real-time\nhelps people digest massive news streams without expensive human annotations. A\ncommon approach of the existing studies for unsupervised online story discovery\nis to represent news articles with symbolic- or graph-based embedding and\nincrementally cluster them into stories. Recent large language models are\nexpected to improve the embedding further, but a straightforward adoption of\nthe models by indiscriminately encoding all information in articles is\nineffective to deal with text-rich and evolving news streams. In this work, we\npropose a novel thematic embedding with an off-the-shelf pretrained sentence\nencoder to dynamically represent articles and stories by considering their\nshared temporal themes. To realize the idea for unsupervised online story\ndiscovery, a scalable framework USTORY is introduced with two main techniques,\ntheme- and time-aware dynamic embedding and novelty-aware adaptive clustering,\nfueled by lightweight story summaries. A thorough evaluation with real news\ndata sets demonstrates that USTORY achieves higher story discovery performances\nthan baselines while being robust and scalable to various streaming settings.",
        "pdf_link": "https://arxiv.org/pdf/2304.04099v3.pdf"
    },
    {
        "title": "Comparing Code Explanations Created by Students and Large Language Models",
        "authors": [
            "Juho Leinonen",
            "Paul Denny",
            "Stephen MacNeil",
            "Sami Sarsa",
            "Seth Bernstein",
            "Joanne Kim",
            "Andrew Tran",
            "Arto Hellas"
        ],
        "published": "2023-04-08T06:52:54Z",
        "summary": "Reasoning about code and explaining its purpose are fundamental skills for\ncomputer scientists. There has been extensive research in the field of\ncomputing education on the relationship between a student's ability to explain\ncode and other skills such as writing and tracing code. In particular, the\nability to describe at a high-level of abstraction how code will behave over\nall possible inputs correlates strongly with code writing skills. However,\ndeveloping the expertise to comprehend and explain code accurately and\nsuccinctly is a challenge for many students. Existing pedagogical approaches\nthat scaffold the ability to explain code, such as producing exemplar code\nexplanations on demand, do not currently scale well to large classrooms. The\nrecent emergence of powerful large language models (LLMs) may offer a solution.\nIn this paper, we explore the potential of LLMs in generating explanations that\ncan serve as examples to scaffold students' ability to understand and explain\ncode. To evaluate LLM-created explanations, we compare them with explanations\ncreated by students in a large course ($n \\approx 1000$) with respect to\naccuracy, understandability and length. We find that LLM-created explanations,\nwhich can be produced automatically on demand, are rated as being significantly\neasier to understand and more accurate summaries of code than student-created\nexplanations. We discuss the significance of this finding, and suggest how such\nmodels can be incorporated into introductory programming education.",
        "pdf_link": "https://arxiv.org/pdf/2304.03938v1.pdf"
    },
    {
        "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation",
        "authors": [
            "Jinming Li",
            "Wentao Zhang",
            "Tian Wang",
            "Guanglei Xiong",
            "Alan Lu",
            "Gerard Medioni"
        ],
        "published": "2023-04-08T00:30:08Z",
        "summary": "Recent advancements in Natural Language Processing (NLP) have led to the\ndevelopment of NLP-based recommender systems that have shown superior\nperformance. However, current models commonly treat items as mere IDs and adopt\ndiscriminative modeling, resulting in limitations of (1) fully leveraging the\ncontent information of items and the language modeling capabilities of NLP\nmodels; (2) interpreting user interests to improve relevance and diversity; and\n(3) adapting practical circumstances such as growing item inventories. To\naddress these limitations, we present GPT4Rec, a novel and flexible generative\nframework inspired by search engines. It first generates hypothetical \"search\nqueries\" given item titles in a user's history, and then retrieves items for\nrecommendation by searching these queries. The framework overcomes previous\nlimitations by learning both user and item embeddings in the language space. To\nwell-capture user interests with different aspects and granularity for\nimproving relevance and diversity, we propose a multi-query generation\ntechnique with beam search. The generated queries naturally serve as\ninterpretable representations of user interests and can be searched to\nrecommend cold-start items. With GPT-2 language model and BM25 search engine,\nour framework outperforms state-of-the-art methods by $75.7\\%$ and $22.2\\%$ in\nRecall@K on two public datasets. Experiments further revealed that multi-query\ngeneration with beam search improves both the diversity of retrieved items and\nthe coverage of a user's multi-interests. The adaptiveness and interpretability\nof generated queries are discussed with qualitative case studies.",
        "pdf_link": "https://arxiv.org/pdf/2304.03879v1.pdf"
    },
    {
        "title": "Why think step by step? Reasoning emerges from the locality of experience",
        "authors": [
            "Ben Prystawski",
            "Michael Y. Li",
            "Noah D. Goodman"
        ],
        "published": "2023-04-07T21:04:03Z",
        "summary": "Humans have a powerful and mysterious capacity to reason. Working through a\nset of mental steps enables us to make inferences we would not be capable of\nmaking directly even though we get no additional data from the world.\nSimilarly, when large language models generate intermediate steps (a chain of\nthought) before answering a question, they often produce better answers than\nthey would directly. We investigate why and how chain-of-thought reasoning is\nuseful in language models, testing the hypothesis that reasoning is effective\nwhen training data consists of overlapping local clusters of variables that\ninfluence each other strongly. These training conditions enable the chaining of\naccurate local inferences to estimate relationships between variables that were\nnot seen together in training. We prove that there will exist a \"reasoning\ngap\", where reasoning through intermediate variables reduces bias, for the\nsimple case of an autoregressive density estimator trained on local samples\nfrom a chain-structured probabilistic model. We then test our hypothesis\nexperimentally in more complex models, training an autoregressive language\nmodel on samples from Bayes nets but only including a subset of variables in\neach sample. We test language models' ability to match conditional\nprobabilities with and without intermediate reasoning steps, finding that\nintermediate steps are only helpful when the training data is locally\nstructured with respect to dependencies between variables. The combination of\nlocally structured observations and reasoning is much more data-efficient than\ntraining on all variables. Our results illustrate how the effectiveness of\nreasoning step by step is rooted in the local statistical structure of the\ntraining data.",
        "pdf_link": "https://arxiv.org/pdf/2304.03843v3.pdf"
    },
    {
        "title": "Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions",
        "authors": [
            "Sarah Fakhoury",
            "Saikat Chakraborty",
            "Madan Musuvathi",
            "Shuvendu K. Lahiri"
        ],
        "published": "2023-04-07T18:58:33Z",
        "summary": "Large language models (LLMs), such as OpenAI's Codex, have demonstrated their\npotential to generate code from natural language descriptions across a wide\nrange of programming tasks. Several benchmarks have recently emerged to\nevaluate the ability of LLMs to generate functionally correct code from natural\nlanguage intent with respect to a set of hidden test cases. This has enabled\nthe research community to identify significant and reproducible advancements in\nLLM capabilities. However, there is currently a lack of benchmark datasets for\nassessing the ability of LLMs to generate functionally correct code edits based\non natural language descriptions of intended changes. This paper aims to\naddress this gap by motivating the problem NL2Fix of translating natural\nlanguage descriptions of code changes (namely bug fixes described in Issue\nreports in repositories) into correct code fixes. To this end, we introduce\nDefects4J-NL2Fix, a dataset of 283 Java programs from the popular Defects4J\ndataset augmented with high-level descriptions of bug fixes, and empirically\nevaluate the performance of several state-of-the-art LLMs for the this task.\nResults show that these LLMS together are capable of generating plausible fixes\nfor 64.6% of the bugs, and the best LLM-based technique can achieve up to\n21.20% top-1 and 35.68% top-5 accuracy on this benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2304.03816v1.pdf"
    },
    {
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
        "authors": [
            "Emilio Ferrara"
        ],
        "published": "2023-04-07T17:14:00Z",
        "summary": "As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.",
        "pdf_link": "https://arxiv.org/pdf/2304.03738v3.pdf"
    },
    {
        "title": "Interpretable Unified Language Checking",
        "authors": [
            "Tianhua Zhang",
            "Hongyin Luo",
            "Yung-Sung Chuang",
            "Wei Fang",
            "Luc Gaitskell",
            "Thomas Hartvigsen",
            "Xixin Wu",
            "Danny Fox",
            "Helen Meng",
            "James Glass"
        ],
        "published": "2023-04-07T16:47:49Z",
        "summary": "Despite recent concerns about undesirable behaviors generated by large\nlanguage models (LLMs), including non-factual, biased, and hateful language, we\nfind LLMs are inherent multi-task language checkers based on their latent\nrepresentations of natural and social knowledge. We present an interpretable,\nunified, language checking (UniLC) method for both human and machine-generated\nlanguage that aims to check if language input is factual and fair. While\nfairness and fact-checking tasks have been handled separately with dedicated\nmodels, we find that LLMs can achieve high performance on a combination of\nfact-checking, stereotype detection, and hate speech detection tasks with a\nsimple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task\nlanguage checking method proposed in this work, the GPT3.5-turbo model\noutperforms fully supervised baselines on several language tasks. The simple\napproach and results suggest that based on strong latent knowledge\nrepresentations, an LLM can be an adaptive and explainable tool for detecting\nmisinformation, stereotypes, and hate speech.",
        "pdf_link": "https://arxiv.org/pdf/2304.03728v1.pdf"
    },
    {
        "title": "What does ChatGPT return about human values? Exploring value bias in ChatGPT using a descriptive value theory",
        "authors": [
            "Ronald Fischer",
            "Markus Luczak-Roesch",
            "Johannes A Karl"
        ],
        "published": "2023-04-07T12:20:13Z",
        "summary": "There has been concern about ideological basis and possible discrimination in\ntext generated by Large Language Models (LLMs). We test possible value biases\nin ChatGPT using a psychological value theory. We designed a simple experiment\nin which we used a number of different probes derived from the Schwartz basic\nvalue theory (items from the revised Portrait Value Questionnaire, the value\ntype definitions, value names). We prompted ChatGPT via the OpenAI API\nrepeatedly to generate text and then analyzed the generated corpus for value\ncontent with a theory-driven value dictionary using a bag of words approach.\nOverall, we found little evidence of explicit value bias. The results showed\nsufficient construct and discriminant validity for the generated text in line\nwith the theoretical predictions of the psychological model, which suggests\nthat the value content was carried through into the outputs with high fidelity.\nWe saw some merging of socially oriented values, which may suggest that these\nvalues are less clearly differentiated at a linguistic level or alternatively,\nthis mixing may reflect underlying universal human motivations. We outline some\npossible applications of our findings for both applications of ChatGPT for\ncorporate usage and policy making as well as future research avenues. We also\nhighlight possible implications of this relatively high-fidelity replication of\nmotivational content using a linguistic model for the theorizing about human\nvalues.",
        "pdf_link": "https://arxiv.org/pdf/2304.03612v1.pdf"
    },
    {
        "title": "Revisiting Automated Prompting: Are We Actually Doing Better?",
        "authors": [
            "Yulin Zhou",
            "Yiren Zhao",
            "Ilia Shumailov",
            "Robert Mullins",
            "Yarin Gal"
        ],
        "published": "2023-04-07T12:06:44Z",
        "summary": "Current literature demonstrates that Large Language Models (LLMs) are great\nfew-shot learners, and prompting significantly increases their performance on a\nrange of downstream tasks in a few-shot learning setting. An attempt to\nautomate human-led prompting followed, with some progress achieved. In\nparticular, subsequent work demonstrates automation can outperform fine-tuning\nin certain K-shot learning scenarios.\n  In this paper, we revisit techniques for automated prompting on six different\ndownstream tasks and a larger range of K-shot learning settings. We find that\nautomated prompting does not consistently outperform simple manual prompts. Our\nwork suggests that, in addition to fine-tuning, manual prompts should be used\nas a baseline in this line of research.",
        "pdf_link": "https://arxiv.org/pdf/2304.03609v2.pdf"
    },
    {
        "title": "ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions",
        "authors": [
            "Sibei Chen",
            "Hanbing Liu",
            "Weiting Jin",
            "Xiangyu Sun",
            "Xiaoyao Feng",
            "Ju Fan",
            "Xiaoyong Du",
            "Nan Tang"
        ],
        "published": "2023-04-07T08:33:08Z",
        "summary": "Orchestrating a high-quality data preparation program is essential for\nsuccessful machine learning (ML), but it is known to be time and effort\nconsuming. Despite the impressive capabilities of large language models like\nChatGPT in generating programs by interacting with users through natural\nlanguage prompts, there are still limitations. Specifically, a user must\nprovide specific prompts to iteratively guide ChatGPT in improving data\npreparation programs, which requires a certain level of expertise in\nprogramming, the dataset used and the ML task. Moreover, once a program has\nbeen generated, it is non-trivial to revisit a previous version or make changes\nto the program without starting the process over again. In this paper, we\npresent ChatPipe, a novel system designed to facilitate seamless interaction\nbetween users and ChatGPT. ChatPipe provides users with effective\nrecommendation on next data preparation operations, and guides ChatGPT to\ngenerate program for the operations. Also, ChatPipe enables users to easily\nroll back to previous versions of the program, which facilitates more efficient\nexperimentation and testing. We have developed a web application for ChatPipe\nand prepared several real-world ML tasks from Kaggle. These tasks can showcase\nthe capabilities of ChatPipe and enable VLDB attendees to easily experiment\nwith our novel features to rapidly orchestrate a high-quality data preparation\nprogram.",
        "pdf_link": "https://arxiv.org/pdf/2304.03540v1.pdf"
    },
    {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": [
            "Joon Sung Park",
            "Joseph C. O'Brien",
            "Carrie J. Cai",
            "Meredith Ringel Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "published": "2023-04-07T01:55:19Z",
        "summary": "Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent's experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine's\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.",
        "pdf_link": "https://arxiv.org/pdf/2304.03442v2.pdf"
    },
    {
        "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
        "authors": [
            "Hanmeng Liu",
            "Ruoxi Ning",
            "Zhiyang Teng",
            "Jian Liu",
            "Qiji Zhou",
            "Yue Zhang"
        ],
        "published": "2023-04-07T01:37:45Z",
        "summary": "Harnessing logical reasoning ability is a comprehensive natural language\nunderstanding endeavor. With the release of Generative Pretrained Transformer 4\n(GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learn\nthe GPT-4 performance on various logical reasoning tasks. This report analyses\nmultiple logical reasoning datasets, with popular benchmarks like LogiQA and\nReClor, and newly-released datasets like AR-LSAT. We test the multi-choice\nreading comprehension and natural language inference tasks with benchmarks\nrequiring logical reasoning. We further construct a logical reasoning\nout-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4.\nWe also make a performance comparison between ChatGPT and GPT-4. Experiment\nresults show that ChatGPT performs significantly better than the RoBERTa\nfine-tuning method on most logical reasoning benchmarks. With early access to\nthe GPT-4 API we are able to conduct intense experiments on the GPT-4 model.\nThe results show GPT-4 yields even higher performance on most logical reasoning\ndatasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known\ndatasets like LogiQA and ReClor. However, the performance drops significantly\nwhen handling newly released and out-of-distribution datasets. Logical\nreasoning remains challenging for ChatGPT and GPT-4, especially on\nout-of-distribution and natural language inference datasets. We release the\nprompt-style logical reasoning datasets as a benchmark suite and name it\nLogiEval.",
        "pdf_link": "https://arxiv.org/pdf/2304.03439v3.pdf"
    },
    {
        "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
        "authors": [
            "Kailai Yang",
            "Shaoxiong Ji",
            "Tianlin Zhang",
            "Qianqian Xie",
            "Ziyan Kuang",
            "Sophia Ananiadou"
        ],
        "published": "2023-04-06T19:53:59Z",
        "summary": "The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.",
        "pdf_link": "https://arxiv.org/pdf/2304.03347v4.pdf"
    },
    {
        "title": "Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark",
        "authors": [
            "Alexander Pan",
            "Jun Shern Chan",
            "Andy Zou",
            "Nathaniel Li",
            "Steven Basart",
            "Thomas Woodside",
            "Jonathan Ng",
            "Hanlin Zhang",
            "Scott Emmons",
            "Dan Hendrycks"
        ],
        "published": "2023-04-06T17:59:03Z",
        "summary": "Artificial agents have traditionally been trained to maximize reward, which\nmay incentivize power-seeking and deception, analogous to how next-token\nprediction in language models (LMs) may incentivize toxicity. So do agents\nnaturally learn to be Machiavellian? And how do we measure these behaviors in\ngeneral-purpose models such as GPT-4? Towards answering these questions, we\nintroduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games\ncontaining over half a million rich, diverse scenarios that center on social\ndecision-making. Scenario labeling is automated with LMs, which are more\nperformant than human annotators. We mathematize dozens of harmful behaviors\nand use our annotations to evaluate agents' tendencies to be power-seeking,\ncause disutility, and commit ethical violations. We observe some tension\nbetween maximizing reward and behaving ethically. To improve this trade-off, we\ninvestigate LM-based methods to steer agents' towards less harmful behaviors.\nOur results show that agents can both act competently and morally, so concrete\nprogress can currently be made in machine ethics--designing agents that are\nPareto improvements in both safety and capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2304.03279v4.pdf"
    },
    {
        "title": "Instruction Tuning with GPT-4",
        "authors": [
            "Baolin Peng",
            "Chunyuan Li",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao"
        ],
        "published": "2023-04-06T17:58:09Z",
        "summary": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2304.03277v1.pdf"
    },
    {
        "title": "When do you need Chain-of-Thought Prompting for ChatGPT?",
        "authors": [
            "Jiuhai Chen",
            "Lichang Chen",
            "Heng Huang",
            "Tianyi Zhou"
        ],
        "published": "2023-04-06T17:47:29Z",
        "summary": "Chain-of-Thought (CoT) prompting can effectively elicit complex multi-step\nreasoning from Large Language Models~(LLMs). For example, by simply adding CoT\ninstruction ``Let's think step-by-step'' to each input query of MultiArith\ndataset, GPT-3's accuracy can be improved from 17.7\\% to 78.7\\%. However, it is\nnot clear whether CoT is still effective on more recent instruction finetuned\n(IFT) LLMs such as ChatGPT. Surprisingly, on ChatGPT, CoT is no longer\neffective for certain tasks such as arithmetic reasoning while still keeping\neffective on other reasoning tasks. Moreover, on the former tasks, ChatGPT\nusually achieves the best performance and can generate CoT even without being\ninstructed to do so. Hence, it is plausible that ChatGPT has already been\ntrained on these tasks with CoT and thus memorized the instruction so it\nimplicitly follows such an instruction when applied to the same queries, even\nwithout CoT. Our analysis reflects a potential risk of overfitting/bias toward\ninstructions introduced in IFT, which becomes more common in training LLMs. In\naddition, it indicates possible leakage of the pretraining recipe, e.g., one\ncan verify whether a dataset and instruction were used in training ChatGPT. Our\nexperiments report new baseline results of ChatGPT on a variety of reasoning\ntasks and shed novel insights into LLM's profiling, instruction memorization,\nand pretraining dataset leakage.",
        "pdf_link": "https://arxiv.org/pdf/2304.03262v2.pdf"
    },
    {
        "title": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
        "authors": [
            "Marzena Karpinska",
            "Mohit Iyyer"
        ],
        "published": "2023-04-06T17:27:45Z",
        "summary": "Large language models (LLMs) are competitive with the state of the art on a\nwide range of sentence-level translation datasets. However, their ability to\ntranslate paragraphs and documents remains unexplored because evaluation in\nthese settings is costly and difficult. We show through a rigorous human\nevaluation that asking the Gpt-3.5 (text-davinci-003) LLM to translate an\nentire literary paragraph (e.g., from a novel) at once results in\nhigher-quality translations than standard sentence-by-sentence translation\nacross 18 linguistically-diverse language pairs (e.g., translating into and out\nof Japanese, Polish, and English). Our evaluation, which took approximately 350\nhours of effort for annotation and analysis, is conducted by hiring translators\nfluent in both the source and target language and asking them to provide both\nspan-level error annotations as well as preference judgments of which system's\ntranslations are better. We observe that discourse-level LLM translators commit\nfewer mistranslations, grammar errors, and stylistic inconsistencies than\nsentence-level approaches. With that said, critical errors still abound,\nincluding occasional content omissions, and a human translator's intervention\nremains necessary to ensure that the author's voice remains intact. We publicly\nrelease our dataset and error annotations to spur future research on evaluation\nof document-level literary translation.",
        "pdf_link": "https://arxiv.org/pdf/2304.03245v3.pdf"
    },
    {
        "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models",
        "authors": [
            "Lei Wang",
            "Ee-Peng Lim"
        ],
        "published": "2023-04-06T15:35:11Z",
        "summary": "Large language models (LLMs) have achieved impressive zero-shot performance\nin various natural language processing (NLP) tasks, demonstrating their\ncapabilities for inference without training examples. Despite their success, no\nresearch has yet explored the potential of LLMs to perform next-item\nrecommendations in the zero-shot setting. We have identified two major\nchallenges that must be addressed to enable LLMs to act effectively as\nrecommenders. First, the recommendation space can be extremely large for LLMs,\nand LLMs do not know about the target user's past interacted items and\npreferences. To address this gap, we propose a prompting strategy called\nZero-Shot Next-Item Recommendation (NIR) prompting that directs LLMs to make\nnext-item recommendations. Specifically, the NIR-based strategy involves using\nan external module to generate candidate items based on user-filtering or\nitem-filtering. Our strategy incorporates a 3-step prompting that guides GPT-3\nto carry subtasks that capture the user's preferences, select representative\npreviously watched movies, and recommend a ranked list of 10 movies. We\nevaluate the proposed approach using GPT-3 on MovieLens 100K dataset and show\nthat it achieves strong zero-shot performance, even outperforming some strong\nsequential recommendation models trained on the entire training dataset. These\npromising results highlight the ample research opportunities to use LLMs as\nrecommenders. The code can be found at\nhttps://github.com/AGI-Edgerunners/LLM-Next-Item-Rec.",
        "pdf_link": "https://arxiv.org/pdf/2304.03153v1.pdf"
    },
    {
        "title": "Investigating Chain-of-thought with ChatGPT for Stance Detection on Social Media",
        "authors": [
            "Bowen Zhang",
            "Xianghua Fu",
            "Daijun Ding",
            "Hu Huang",
            "Yangyang Li",
            "Liwen Jing"
        ],
        "published": "2023-04-06T14:12:02Z",
        "summary": "Stance detection predicts attitudes towards targets in texts and has gained\nattention with the rise of social media. Traditional approaches include\nconventional machine learning, early deep neural networks, and pre-trained\nfine-tuning models. However, with the evolution of very large pre-trained\nlanguage models (VLPLMs) like ChatGPT (GPT-3.5), traditional methods face\ndeployment challenges. The parameter-free Chain-of-Thought (CoT) approach, not\nrequiring backpropagation training, has emerged as a promising alternative.\nThis paper examines CoT's effectiveness in stance detection tasks,\ndemonstrating its superior accuracy and discussing associated challenges.",
        "pdf_link": "https://arxiv.org/pdf/2304.03087v1.pdf"
    },
    {
        "title": "Multi-label classification of open-ended questions with BERT",
        "authors": [
            "Matthias Schonlau",
            "Julia Weiß",
            "Jan Marquardt"
        ],
        "published": "2023-04-06T09:09:44Z",
        "summary": "Open-ended questions in surveys are valuable because they do not constrain\nthe respondent's answer, thereby avoiding biases. However, answers to\nopen-ended questions are text data which are harder to analyze. Traditionally,\nanswers were manually classified as specified in the coding manual. Most of the\neffort to automate coding has gone into the easier problem of single label\nprediction, where answers are classified into a single code. However, open-ends\nthat require multi-label classification, i.e., that are assigned multiple\ncodes, occur frequently. This paper focuses on multi-label classification of\ntext answers to open-ended survey questions in social science surveys. We\nevaluate the performance of the transformer-based architecture BERT for the\nGerman language in comparison to traditional multi-label algorithms (Binary\nRelevance, Label Powerset, ECC) in a German social science survey, the GLES\nPanel (N=17,584, 55 labels). We find that classification with BERT (forcing at\nleast one label) has the smallest 0/1 loss (13.1%) among methods considered\n(18.9%-21.6%). As expected, it is much easier to correctly predict answer texts\nthat correspond to a single label (7.1% loss) than those that correspond to\nmultiple labels ($\\sim$50% loss). Because BERT predicts zero labels for only\n1.5% of the answers, forcing at least one label, while recommended, ultimately\ndoes not lower the 0/1 loss by much. Our work has important implications for\nsocial scientists: 1) We have shown multi-label classification with BERT works\nin the German language for open-ends. 2) For mildly multi-label classification\ntasks, the loss now appears small enough to allow for fully automatic\nclassification (as compared to semi-automatic approaches). 3) Multi-label\nclassification with BERT requires only a single model. The leading competitor,\nECC, iterates through individual single label predictions.",
        "pdf_link": "https://arxiv.org/pdf/2304.02945v1.pdf"
    },
    {
        "title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
        "authors": [
            "Chen Feng Tsai",
            "Xiaochen Zhou",
            "Sierra S. Liu",
            "Jing Li",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "published": "2023-04-06T05:01:28Z",
        "summary": "Large language models (LLMs) such as ChatGPT and GPT-4 have recently\ndemonstrated their remarkable abilities of communicating with human users. In\nthis technical report, we take an initiative to investigate their capacities of\nplaying text games, in which a player has to understand the environment and\nrespond to situations by having dialogues with the game world. Our experiments\nshow that ChatGPT performs competitively compared to all the existing systems\nbut still exhibits a low level of intelligence. Precisely, ChatGPT can not\nconstruct the world model by playing the game or even reading the game manual;\nit may fail to leverage the world knowledge that it already has; it cannot\ninfer the goal of each step as the game progresses. Our results open up new\nresearch questions at the intersection of artificial intelligence, machine\nlearning, and natural language processing.",
        "pdf_link": "https://arxiv.org/pdf/2304.02868v1.pdf"
    },
    {
        "title": "Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics",
        "authors": [
            "Madiha Zahrah Choksi",
            "David Goedicke"
        ],
        "published": "2023-04-06T03:09:26Z",
        "summary": "Intelligent or generative writing tools rely on large language models that\nrecognize, summarize, translate, and predict content. This position paper\nprobes the copyright interests of open data sets used to train large language\nmodels (LLMs). Our paper asks, how do LLMs trained on open data sets circumvent\nthe copyright interests of the used data? We start by defining software\ncopyright and tracing its history. We rely on GitHub Copilot as a modern case\nstudy challenging software copyright. Our conclusion outlines obstacles that\ngenerative writing assistants create for copyright, and offers a practical road\nmap for copyright analysis for developers, software law experts, and general\nusers to consider in the context of intelligent LLM-powered writing tools.",
        "pdf_link": "https://arxiv.org/pdf/2304.02839v1.pdf"
    },
    {
        "title": "Approach Intelligent Writing Assistants Usability with Seven Stages of Action",
        "authors": [
            "Avinash Bhat",
            "Disha Shrivastava",
            "Jin L. C. Guo"
        ],
        "published": "2023-04-06T02:11:55Z",
        "summary": "Despite the potential of Large Language Models (LLMs) as writing assistants,\nthey are plagued by issues like coherence and fluency of the model output,\ntrustworthiness, ownership of the generated content, and predictability of\nmodel performance, thereby limiting their usability. In this position paper, we\npropose to adopt Norman's seven stages of action as a framework to approach the\ninteraction design of intelligent writing assistants. We illustrate the\nframework's applicability to writing tasks by providing an example of software\ntutorial authoring. The paper also discusses the framework as a tool to\nsynthesize research on the interaction design of LLM-based tools and presents\nexamples of tools that support the stages of action. Finally, we briefly\noutline the potential of a framework for human-LLM interaction research.",
        "pdf_link": "https://arxiv.org/pdf/2304.02822v1.pdf"
    },
    {
        "title": "GPT detectors are biased against non-native English writers",
        "authors": [
            "Weixin Liang",
            "Mert Yuksekgonul",
            "Yining Mao",
            "Eric Wu",
            "James Zou"
        ],
        "published": "2023-04-06T01:51:15Z",
        "summary": "The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse. The published\nversion of this study can be accessed at:\nwww.cell.com/patterns/fulltext/S2666-3899(23)00130-7",
        "pdf_link": "https://arxiv.org/pdf/2304.02819v3.pdf"
    },
    {
        "title": "Context-Aware Classification of Legal Document Pages",
        "authors": [
            "Pavlos Fragkogiannis",
            "Martina Forster",
            "Grace E. Lee",
            "Dell Zhang"
        ],
        "published": "2023-04-05T23:14:58Z",
        "summary": "For many business applications that require the processing, indexing, and\nretrieval of professional documents such as legal briefs (in PDF format etc.),\nit is often essential to classify the pages of any given document into their\ncorresponding types beforehand. Most existing studies in the field of document\nimage classification either focus on single-page documents or treat multiple\npages in a document independently. Although in recent years a few techniques\nhave been proposed to exploit the context information from neighboring pages to\nenhance document page classification, they typically cannot be utilized with\nlarge pre-trained language models due to the constraint on input length. In\nthis paper, we present a simple but effective approach that overcomes the above\nlimitation. Specifically, we enhance the input with extra tokens carrying\nsequential information about previous pages - introducing recurrence - which\nenables the usage of pre-trained Transformer models like BERT for context-aware\npage classification. Our experiments conducted on two legal datasets in English\nand Portuguese respectively show that the proposed approach can significantly\nimprove the performance of document page classification compared to the\nnon-recurrent setup as well as the other context-aware baselines.",
        "pdf_link": "https://arxiv.org/pdf/2304.02787v2.pdf"
    },
    {
        "title": "Conceptual structure coheres in human cognition but not in large language models",
        "authors": [
            "Siddharth Suresh",
            "Kushin Mukherjee",
            "Xizheng Yu",
            "Wei-Chun Huang",
            "Lisa Padua",
            "Timothy T Rogers"
        ],
        "published": "2023-04-05T21:27:01Z",
        "summary": "Neural network models of language have long been used as a tool for\ndeveloping hypotheses about conceptual representation in the mind and brain.\nFor many years, such use involved extracting vector-space representations of\nwords and using distances among these to predict or understand human behavior\nin various semantic tasks. Contemporary large language models (LLMs), however,\nmake it possible to interrogate the latent structure of conceptual\nrepresentations using experimental methods nearly identical to those commonly\nused with human participants. The current work utilizes three common techniques\nborrowed from cognitive psychology to estimate and compare the structure of\nconcepts in humans and a suite of LLMs. In humans, we show that conceptual\nstructure is robust to differences in culture, language, and method of\nestimation. Structures estimated from LLM behavior, while individually fairly\nconsistent with those estimated from human behavior, vary much more depending\nupon the particular task used to generate responses--across tasks, estimates of\nconceptual structure from the very same model cohere less with one another than\ndo human structure estimates. These results highlight an important difference\nbetween contemporary LLMs and human cognition, with implications for\nunderstanding some fundamental limitations of contemporary machine language.",
        "pdf_link": "https://arxiv.org/pdf/2304.02754v2.pdf"
    },
    {
        "title": "Bengali Fake Review Detection using Semi-supervised Generative Adversarial Networks",
        "authors": [
            "Md. Tanvir Rouf Shawon",
            "G. M. Shahariar",
            "Faisal Muhammad Shah",
            "Mohammad Shafiul Alam",
            "Md. Shahriar Mahbub"
        ],
        "published": "2023-04-05T20:40:09Z",
        "summary": "This paper investigates the potential of semi-supervised Generative\nAdversarial Networks (GANs) to fine-tune pretrained language models in order to\nclassify Bengali fake reviews from real reviews with a few annotated data. With\nthe rise of social media and e-commerce, the ability to detect fake or\ndeceptive reviews is becoming increasingly important in order to protect\nconsumers from being misled by false information. Any machine learning model\nwill have trouble identifying a fake review, especially for a low resource\nlanguage like Bengali. We have demonstrated that the proposed semi-supervised\nGAN-LM architecture (generative adversarial network on top of a pretrained\nlanguage model) is a viable solution in classifying Bengali fake reviews as the\nexperimental results suggest that even with only 1024 annotated samples,\nBanglaBERT with semi-supervised GAN (SSGAN) achieved an accuracy of 83.59% and\na f1-score of 84.89% outperforming other pretrained language models -\nBanglaBERT generator, Bangla BERT Base and Bangla-Electra by almost 3%, 4% and\n10% respectively in terms of accuracy. The experiments were conducted on a\nmanually labeled food review dataset consisting of total 6014 real and fake\nreviews collected from various social media groups. Researchers that are\nexperiencing difficulty recognizing not just fake reviews but other\nclassification issues owing to a lack of labeled data may find a solution in\nour proposed methodology.",
        "pdf_link": "https://arxiv.org/pdf/2304.02739v1.pdf"
    },
    {
        "title": "Efficient OCR for Building a Diverse Digital History",
        "authors": [
            "Jacob Carlson",
            "Tom Bryan",
            "Melissa Dell"
        ],
        "published": "2023-04-05T20:36:04Z",
        "summary": "Thousands of users consult digital archives daily, but the information they\ncan access is unrepresentative of the diversity of documentary history. The\nsequence-to-sequence architecture typically used for optical character\nrecognition (OCR) - which jointly learns a vision and language model - is\npoorly extensible to low-resource document collections, as learning a\nlanguage-vision model requires extensive labeled sequences and compute. This\nstudy models OCR as a character level image retrieval problem, using a\ncontrastively trained vision encoder. Because the model only learns characters'\nvisual features, it is more sample efficient and extensible than existing\narchitectures, enabling accurate OCR in settings where existing solutions fail.\nCrucially, the model opens new avenues for community engagement in making\ndigital history more representative of documentary history.",
        "pdf_link": "https://arxiv.org/pdf/2304.02737v1.pdf"
    },
    {
        "title": "Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning",
        "authors": [
            "J. Harry Caufield",
            "Harshad Hegde",
            "Vincent Emonet",
            "Nomi L. Harris",
            "Marcin P. Joachimiak",
            "Nicolas Matentzoglu",
            "HyeongSik Kim",
            "Sierra A. T. Moxon",
            "Justin T. Reese",
            "Melissa A. Haendel",
            "Peter N. Robinson",
            "Christopher J. Mungall"
        ],
        "published": "2023-04-05T19:07:04Z",
        "summary": "Creating knowledge bases and ontologies is a time consuming task that relies\non a manual curation. AI/NLP approaches can assist expert curators in\npopulating these knowledge bases, but current approaches rely on extensive\ntraining data, and are not able to populate arbitrary complex nested knowledge\nschemas.\n  Here we present Structured Prompt Interrogation and Recursive Extraction of\nSemantics (SPIRES), a Knowledge Extraction approach that relies on the ability\nof Large Language Models (LLMs) to perform zero-shot learning (ZSL) and\ngeneral-purpose query answering from flexible prompts and return information\nconforming to a specified schema. Given a detailed, user-defined knowledge\nschema and an input text, SPIRES recursively performs prompt interrogation\nagainst GPT-3+ to obtain a set of responses matching the provided schema.\nSPIRES uses existing ontologies and vocabularies to provide identifiers for all\nmatched elements.\n  We present examples of use of SPIRES in different domains, including\nextraction of food recipes, multi-species cellular signaling pathways, disease\ntreatments, multi-step drug mechanisms, and chemical to disease causation\ngraphs. Current SPIRES accuracy is comparable to the mid-range of existing\nRelation Extraction (RE) methods, but has the advantage of easy customization,\nflexibility, and, crucially, the ability to perform new tasks in the absence of\nany training data. This method supports a general strategy of leveraging the\nlanguage interpreting capabilities of LLMs to assemble knowledge bases,\nassisting manual knowledge curation and acquisition while supporting validation\nwith publicly-available databases and ontologies external to the LLM.\n  SPIRES is available as part of the open source OntoGPT package:\nhttps://github.com/ monarch-initiative/ontogpt.",
        "pdf_link": "https://arxiv.org/pdf/2304.02711v2.pdf"
    },
    {
        "title": "Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification",
        "authors": [
            "Shan Chen",
            "Yingya Li",
            "Sheng Lu",
            "Hoang Van",
            "Hugo JWL Aerts",
            "Guergana K. Savova",
            "Danielle S. Bitterman"
        ],
        "published": "2023-04-05T15:11:25Z",
        "summary": "Recent advances in large language models (LLMs) have shown impressive ability\nin biomedical question-answering, but have not been adequately investigated for\nmore specific biomedical applications. This study investigates the performance\nof LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical\ntasks beyond question-answering. Because no patient data can be passed to the\nOpenAI API public interface, we evaluated model performance with over 10000\nsamples as proxies for two fundamental tasks in the clinical domain -\nclassification and reasoning. The first task is classifying whether statements\nof clinical and policy recommendations in scientific literature constitute\nhealth advice. The second task is causal relation detection from the biomedical\nliterature. We compared LLMs with simpler models, such as bag-of-words (BoW)\nwith logistic regression, and fine-tuned BioBERT models. Despite the excitement\naround viral ChatGPT, we found that fine-tuning for two fundamental NLP tasks\nremained the best strategy. The simple BoW model performed on par with the most\ncomplex LLM prompting. Prompt engineering required significant investment.",
        "pdf_link": "https://arxiv.org/pdf/2304.02496v1.pdf"
    },
    {
        "title": "Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT",
        "authors": [
            "Tong Xie",
            "Yuwei Wan",
            "Wei Huang",
            "Yufei Zhou",
            "Yixuan Liu",
            "Qingyuan Linghu",
            "Shaozhou Wang",
            "Chunyu Kit",
            "Clara Grazian",
            "Wenjie Zhang",
            "Bram Hoex"
        ],
        "published": "2023-04-05T04:01:52Z",
        "summary": "The amount of data has growing significance in exploring cutting-edge\nmaterials and a number of datasets have been generated either by hand or\nautomated approaches. However, the materials science field struggles to\neffectively utilize the abundance of data, especially in applied disciplines\nwhere materials are evaluated based on device performance rather than their\nproperties. This article presents a new natural language processing (NLP) task\ncalled structured information inference (SII) to address the complexities of\ninformation extraction at the device level in materials science. We\naccomplished this task by tuning GPT-3 on an existing perovskite solar cell\nFAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8%\nF1-score and extended the dataset with data published since its release. The\nproduced data is formatted and normalized, enabling its direct utilization as\ninput in subsequent data analysis. This feature empowers materials scientists\nto develop models by selecting high-quality review articles within their\ndomain. Additionally, we designed experiments to predict the electrical\nperformance of solar cells and design materials or devices with targeted\nparameters using large language models (LLMs). Our results demonstrate\ncomparable performance to traditional machine learning methods without feature\nselection, highlighting the potential of LLMs to acquire scientific knowledge\nand design new materials akin to materials scientists.",
        "pdf_link": "https://arxiv.org/pdf/2304.02213v5.pdf"
    },
    {
        "title": "Document-Level Machine Translation with Large Language Models",
        "authors": [
            "Longyue Wang",
            "Chenyang Lyu",
            "Tianbo Ji",
            "Zhirui Zhang",
            "Dian Yu",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "published": "2023-04-05T03:49:06Z",
        "summary": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive,\nrelevant, and fluent answers for various natural language processing (NLP)\ntasks. Taking document-level machine translation (MT) as a testbed, this paper\nprovides an in-depth evaluation of LLMs' ability on discourse modeling. The\nstudy focuses on three aspects: 1) Effects of Context-Aware Prompts, where we\ninvestigate the impact of different prompts on document-level translation\nquality and discourse phenomena; 2) Comparison of Translation Models, where we\ncompare the translation performance of ChatGPT with commercial MT systems and\nadvanced document-level MT methods; 3) Analysis of Discourse Modelling\nAbilities, where we further probe discourse knowledge encoded in LLMs and shed\nlight on impacts of training techniques on discourse modeling. By evaluating on\na number of benchmarks, we surprisingly find that LLMs have demonstrated\nsuperior performance and show potential to become a new paradigm for\ndocument-level translation: 1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of\nhuman evaluation; 2) GPT-4 demonstrates a stronger ability for probing\nlinguistic knowledge than GPT-3.5. This work highlights the challenges and\nopportunities of LLMs for MT, which we hope can inspire the future design and\nevaluation of LLMs.We release our data and annotations at\nhttps://github.com/longyuewangdcu/Document-MT-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2304.02210v2.pdf"
    },
    {
        "title": "ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules",
        "authors": [
            "Zhi-Qi Cheng",
            "Qi Dai",
            "Siyao Li",
            "Jingdong Sun",
            "Teruko Mitamura",
            "Alexander G. Hauptmann"
        ],
        "published": "2023-04-05T00:25:27Z",
        "summary": "Charts are a powerful tool for visually conveying complex data, but their\ncomprehension poses a challenge due to the diverse chart types and intricate\ncomponents. Existing chart comprehension methods suffer from either heuristic\nrules or an over-reliance on OCR systems, resulting in suboptimal performance.\nTo address these issues, we present ChartReader, a unified framework that\nseamlessly integrates chart derendering and comprehension tasks. Our approach\nincludes a transformer-based chart component detection module and an extended\npre-trained vision-language model for chart-to-X tasks. By learning the rules\nof charts automatically from annotated datasets, our approach eliminates the\nneed for manual rule-making, reducing effort and enhancing accuracy.~We also\nintroduce a data variable replacement technique and extend the input and\nposition embeddings of the pre-trained model for cross-task training. We\nevaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks,\ndemonstrating its superiority over existing methods. Our proposed framework can\nsignificantly reduce the manual effort involved in chart analysis, providing a\nstep towards a universal chart understanding model. Moreover, our approach\noffers opportunities for plug-and-play integration with mainstream LLMs such as\nT5 and TaPas, extending their capability to chart comprehension tasks. The code\nis available at https://github.com/zhiqic/ChartReader.",
        "pdf_link": "https://arxiv.org/pdf/2304.02173v1.pdf"
    },
    {
        "title": "Geotechnical Parrot Tales (GPT): Harnessing Large Language Models in geotechnical engineering",
        "authors": [
            "Krishna Kumar"
        ],
        "published": "2023-04-04T21:47:41Z",
        "summary": "The widespread adoption of large language models (LLMs), such as OpenAI's\nChatGPT, could revolutionize various industries, including geotechnical\nengineering. However, GPT models can sometimes generate plausible-sounding but\nfalse outputs, leading to hallucinations. In this article, we discuss the\nimportance of prompt engineering in mitigating these risks and harnessing the\nfull potential of GPT for geotechnical applications. We explore the challenges\nand pitfalls associated with LLMs and highlight the role of context in ensuring\naccurate and valuable responses. Furthermore, we examine the development of\ncontext-specific search engines and the potential of LLMs to become a natural\ninterface for complex tasks, such as data analysis and design. We also develop\na unified interface using natural language to handle complex geotechnical\nengineering tasks and data analysis. By integrating GPT into geotechnical\nengineering workflows, professionals can streamline their work and develop\nsustainable and resilient infrastructure systems for the future.",
        "pdf_link": "https://arxiv.org/pdf/2304.02138v3.pdf"
    },
    {
        "title": "Dialogue-Contextualized Re-ranking for Medical History-Taking",
        "authors": [
            "Jian Zhu",
            "Ilya Valmianski",
            "Anitha Kannan"
        ],
        "published": "2023-04-04T17:31:32Z",
        "summary": "AI-driven medical history-taking is an important component in symptom\nchecking, automated patient intake, triage, and other AI virtual care\napplications. As history-taking is extremely varied, machine learning models\nrequire a significant amount of data to train. To overcome this challenge,\nexisting systems are developed using indirect data or expert knowledge. This\nleads to a training-inference gap as models are trained on different kinds of\ndata than what they observe at inference time. In this work, we present a\ntwo-stage re-ranking approach that helps close the training-inference gap by\nre-ranking the first-stage question candidates using a dialogue-contextualized\nmodel. For this, we propose a new model, global re-ranker, which cross-encodes\nthe dialogue with all questions simultaneously, and compare it with several\nexisting neural baselines. We test both transformer and S4-based language model\nbackbones. We find that relative to the expert system, the best performance is\nachieved by our proposed global re-ranker with a transformer backbone,\nresulting in a 30% higher normalized discount cumulative gain (nDCG) and a 77%\nhigher mean average precision (mAP).",
        "pdf_link": "https://arxiv.org/pdf/2304.01974v1.pdf"
    },
    {
        "title": "Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems",
        "authors": [
            "Fangzhen Lin",
            "Ziyi Shou",
            "Chengcai Chen"
        ],
        "published": "2023-04-04T13:01:48Z",
        "summary": "For a natural language problem that requires some non-trivial reasoning to\nsolve, there are at least two ways to do it using a large language model (LLM).\nOne is to ask it to solve it directly. The other is to use it to extract the\nfacts from the problem text and then use a theorem prover to solve it. In this\nnote, we compare the two methods using ChatGPT and GPT4 on a series of logic\nword puzzles, and conclude that the latter is the right approach.",
        "pdf_link": "https://arxiv.org/pdf/2304.01771v1.pdf"
    },
    {
        "title": "Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation",
        "authors": [
            "Tao Fang",
            "Shu Yang",
            "Kaixin Lan",
            "Derek F. Wong",
            "Jinpeng Hu",
            "Lidia S. Chao",
            "Yue Zhang"
        ],
        "published": "2023-04-04T12:33:40Z",
        "summary": "ChatGPT, a large-scale language model based on the advanced GPT-3.5\narchitecture, has shown remarkable potential in various Natural Language\nProcessing (NLP) tasks. However, there is currently a dearth of comprehensive\nstudy exploring its potential in the area of Grammatical Error Correction\n(GEC). To showcase its capabilities in GEC, we design zero-shot\nchain-of-thought (CoT) and few-shot CoT settings using in-context learning for\nChatGPT. Our evaluation involves assessing ChatGPT's performance on five\nofficial test sets in three different languages, along with three\ndocument-level GEC test sets in English. Our experimental results and human\nevaluations demonstrate that ChatGPT has excellent error detection capabilities\nand can freely correct errors to make the corrected sentences very fluent,\npossibly due to its over-correction tendencies and not adhering to the\nprinciple of minimal edits. Additionally, its performance in non-English and\nlow-resource settings highlights its potential in multilingual GEC tasks.\nHowever, further analysis of various types of errors at the document-level has\nshown that ChatGPT cannot effectively correct agreement, coreference, tense\nerrors across sentences, and cross-sentence boundary errors.",
        "pdf_link": "https://arxiv.org/pdf/2304.01746v1.pdf"
    },
    {
        "title": "To ChatGPT, or not to ChatGPT: That is the question!",
        "authors": [
            "Alessandro Pegoraro",
            "Kavita Kumari",
            "Hossein Fereidooni",
            "Ahmad-Reza Sadeghi"
        ],
        "published": "2023-04-04T03:04:28Z",
        "summary": "ChatGPT has become a global sensation. As ChatGPT and other Large Language\nModels (LLMs) emerge, concerns of misusing them in various ways increase, such\nas disseminating fake news, plagiarism, manipulating public opinion, cheating,\nand fraud. Hence, distinguishing AI-generated from human-generated becomes\nincreasingly essential. Researchers have proposed various detection\nmethodologies, ranging from basic binary classifiers to more complex\ndeep-learning models. Some detection techniques rely on statistical\ncharacteristics or syntactic patterns, while others incorporate semantic or\ncontextual information to improve accuracy. The primary objective of this study\nis to provide a comprehensive and contemporary assessment of the most recent\ntechniques in ChatGPT detection. Additionally, we evaluated other AI-generated\ntext detection tools that do not specifically claim to detect ChatGPT-generated\ncontent to assess their performance in detecting ChatGPT-generated content. For\nour evaluation, we have curated a benchmark dataset consisting of prompts from\nChatGPT and humans, including diverse questions from medical, open Q&A, and\nfinance domains and user-generated responses from popular social networking\nplatforms. The dataset serves as a reference to assess the performance of\nvarious techniques in detecting ChatGPT-generated content. Our evaluation\nresults demonstrate that none of the existing methods can effectively detect\nChatGPT-generated content.",
        "pdf_link": "https://arxiv.org/pdf/2304.01487v2.pdf"
    },
    {
        "title": "Blockwise Compression of Transformer-based Models without Retraining",
        "authors": [
            "Gaochen Dong",
            "Wei Chen"
        ],
        "published": "2023-04-04T02:55:40Z",
        "summary": "Transformer-based models, exemplified by GPT-3, ChatGPT, and GPT-4, have\nrecently garnered considerable attention in both academia and industry due to\ntheir promising performance in general language tasks. Nevertheless, these\nmodels typically involve computationally encoding processes, and in some cases,\ndecoding processes as well, both of which are fundamentally large-scale matrix\nmultiplication. These operations bring the inevitable challenges of massive\ncomputation resources and huge memory footprint, usually requiring at least\n10^23 FLOPs and hundreds of gigabytes, respectively. A common method to address\nthis issue is to reduce the computational and memory requirements by applying\nlayerwise quantization to the transformer, replacing the usual fp32 data type\nwith a low-bit equivalent. Unfortunately, this method often leads to decreased\nmodel accuracy and necessitates time-consuming retraining. Such retraining not\nonly requires fine-tuning skills but also substantial computational resources,\nposing challenges for users. To specifically tackle these issues, we propose\nBCT, a framework of blockwise compression for transformers without retraining,\naiming to facilitate model deployment. Unlike layerwise compression methods,\nBCT achieves finer compression of the entire transformer by operating\nblockwise. This method mitigates data distribution deviation caused by\nquantization, eliminating the requirement for retraining. BCT effectively\ncompresses all components of the model, including but not limited to the\nembedding, matrix multiplication, GELU, Softmax, layer normalization, and\nintermediate results. In a case study, an efficient model is compressed by BCT\nachieving up to 7.988x compression. Subsequently, we also evaluate it on\nseveral General Language Understanding Evaluation (GLUE) datasets.",
        "pdf_link": "https://arxiv.org/pdf/2304.01483v2.pdf"
    },
    {
        "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
        "authors": [
            "Stella Biderman",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Herbie Bradley",
            "Kyle O'Brien",
            "Eric Hallahan",
            "Mohammad Aflah Khan",
            "Shivanshu Purohit",
            "USVSN Sai Prashanth",
            "Edward Raff",
            "Aviya Skowron",
            "Lintang Sutawika",
            "Oskar van der Wal"
        ],
        "published": "2023-04-03T20:58:15Z",
        "summary": "How do large language models (LLMs) develop and evolve over the course of\ntraining? How do these patterns change as models scale? To answer these\nquestions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on\npublic data seen in the exact same order and ranging in size from 70M to 12B\nparameters. We provide public access to 154 checkpoints for each one of the 16\nmodels, alongside tools to download and reconstruct their exact training\ndataloaders for further study. We intend \\textit{Pythia} to facilitate research\nin many areas, and we present several case studies including novel results in\nmemorization, term frequency effects on few-shot performance, and reducing\ngender bias. We demonstrate that this highly controlled setup can be used to\nyield novel insights toward LLMs and their training dynamics. Trained models,\nanalysis code, training code, and training data can be found at\n\\url{https://github.com/EleutherAI/pythia}.",
        "pdf_link": "https://arxiv.org/pdf/2304.01373v2.pdf"
    },
    {
        "title": "Classification of integers based on residue classes via modern deep learning algorithms",
        "authors": [
            "Da Wu",
            "Jingye Yang",
            "Mian Umair Ahsan",
            "Kai Wang"
        ],
        "published": "2023-04-03T19:53:31Z",
        "summary": "Judging whether an integer can be divided by prime numbers such as 2 or 3 may\nappear trivial to human beings, but can be less straightforward for computers.\nHere, we tested multiple deep learning architectures and feature engineering\napproaches on classifying integers based on their residues when divided by\nsmall prime numbers. We found that the ability of classification critically\ndepends on the feature space. We also evaluated Automated Machine Learning\n(AutoML) platforms from Amazon, Google and Microsoft, and found that they\nfailed on this task without appropriately engineered features. Furthermore, we\nintroduced a method that utilizes linear regression on Fourier series basis\nvectors, and demonstrated its effectiveness. Finally, we evaluated Large\nLanguage Models (LLMs) such as GPT-4, GPT-J, LLaMA and Falcon, and demonstrated\ntheir failures. In conclusion, feature engineering remains an important task to\nimprove performance and increase interpretability of machine-learning models,\neven in the era of AutoML and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2304.01333v3.pdf"
    },
    {
        "title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning",
        "authors": [
            "Lifu Tu",
            "Jin Qu",
            "Semih Yavuz",
            "Shafiq Joty",
            "Wenhao Liu",
            "Caiming Xiong",
            "Yingbo Zhou"
        ],
        "published": "2023-04-03T18:46:01Z",
        "summary": "Cross-lingual transfer of language models trained on high-resource languages\nlike English has been widely studied for many NLP tasks, but focus on\nconversational tasks has been rather limited. This is partly due to the high\ncost of obtaining non-English conversational data, which results in limited\ncoverage. In this work, we introduce XSGD for cross-lingual alignment\npretraining, a parallel and large-scale multilingual conversation dataset that\nwe created by translating the English-only Schema-Guided Dialogue (SGD) dataset\n(Rastogi et al., 2020) into 105 other languages. XSGD contains approximately\n330k utterances per language. To facilitate aligned cross-lingual\nrepresentations, we develop an efficient prompt-tuning-based method for\nlearning alignment prompts. We also investigate two different classifiers:\nNLI-based and vanilla classifiers, and test cross-lingual capability enabled by\nthe aligned prompts. We evaluate our model's cross-lingual generalization\ncapabilities on two conversation tasks: slot-filling and intent classification.\nOur results demonstrate the strong and efficient modeling ability of NLI-based\nclassifiers and the large cross-lingual transfer improvements achieved by our\naligned prompts, particularly in few-shot settings. In addition, we highlight\nthe nice results of our approach compared to LLMs such as text-davinci-003 and\nChatGPT in both zero-shot and few-shot settings. While LLMs exhibit impressive\nperformance in English, their cross-lingual capabilities in other languages,\nparticularly low-resource languages, are limited.",
        "pdf_link": "https://arxiv.org/pdf/2304.01295v4.pdf"
    },
    {
        "title": "Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data",
        "authors": [
            "Canwen Xu",
            "Daya Guo",
            "Nan Duan",
            "Julian McAuley"
        ],
        "published": "2023-04-03T17:59:09Z",
        "summary": "Chat models, such as ChatGPT, have shown impressive capabilities and have\nbeen rapidly adopted across numerous domains. However, these models are only\naccessible through a restricted API, creating barriers for new research and\nprogress in the field. We propose a pipeline that can automatically generate a\nhigh-quality multi-turn chat corpus by leveraging ChatGPT to engage in a\nconversation with itself. Subsequently, we employ parameter-efficient tuning to\nenhance LLaMA, an open-source large language model. The resulting model, named\nBaize, demonstrates good performance in multi-turn dialogues with guardrails\nthat minimize potential risks. Furthermore, we propose a new technique called\nSelf-Distill with Feedback, to further improve the performance of the Baize\nmodels with feedback from ChatGPT. The Baize models and data are released for\nresearch purposes only at https://github.com/project-baize/baize-chatbot. An\nonline demo is also available at\nhttps://huggingface.co/spaces/project-baize/chat-with-baize.",
        "pdf_link": "https://arxiv.org/pdf/2304.01196v4.pdf"
    },
    {
        "title": "Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT",
        "authors": [
            "Yi Qi",
            "Xingyu Zhao",
            "Siddartha Khastgir",
            "Xiaowei Huang"
        ],
        "published": "2023-04-03T16:46:49Z",
        "summary": "Can safety analysis make use of Large Language Models (LLMs)? A case study\nexplores Systems Theoretic Process Analysis (STPA) applied to Automatic\nEmergency Brake (AEB) and Electricity Demand Side Management (DSM) systems\nusing ChatGPT. We investigate how collaboration schemes, input semantic\ncomplexity, and prompt guidelines influence STPA results. Comparative results\nshow that using ChatGPT without human intervention may be inadequate due to\nreliability related issues, but with careful design, it may outperform human\nexperts. No statistically significant differences are found when varying the\ninput semantic complexity or using common prompt guidelines, which suggests the\nnecessity for developing domain-specific prompt engineering. We also highlight\nfuture challenges, including concerns about LLM trustworthiness and the\nnecessity for standardisation and regulation in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2304.01246v3.pdf"
    },
    {
        "title": "DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task",
        "authors": [
            "Honglin Xiong",
            "Sheng Wang",
            "Yitao Zhu",
            "Zihao Zhao",
            "Yuxiao Liu",
            "Linlin Huang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "published": "2023-04-03T15:57:51Z",
        "summary": "The recent progress of large language models (LLMs), including ChatGPT and\nGPT-4, in comprehending and responding to human instructions has been\nremarkable. Nevertheless, these models typically perform better in English and\nhave not been explicitly trained for the medical domain, resulting in\nsuboptimal precision in diagnoses, drug recommendations, and other medical\nadvice. Additionally, training and deploying a dialogue model is still believed\nto be impossible for hospitals, hindering the promotion of LLMs. To tackle\nthese challenges, we have collected databases of medical dialogues in Chinese\nwith ChatGPT's help and adopted several techniques to train an easy-deploy LLM.\nRemarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13\nhours, which means having a healthcare-purpose LLM can be very affordable.\nDoctorGLM is currently an early-stage engineering attempt and contain various\nmistakes. We are sharing it with the broader community to invite feedback and\nsuggestions to improve its healthcare-focused capabilities:\nhttps://github.com/xionghonglin/DoctorGLM.",
        "pdf_link": "https://arxiv.org/pdf/2304.01097v2.pdf"
    },
    {
        "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models",
        "authors": [
            "Zhihang Yuan",
            "Lin Niu",
            "Jiawei Liu",
            "Wenyu Liu",
            "Xinggang Wang",
            "Yuzhang Shang",
            "Guangyu Sun",
            "Qiang Wu",
            "Jiaxiang Wu",
            "Bingzhe Wu"
        ],
        "published": "2023-04-03T15:46:15Z",
        "summary": "Large-scale language models (LLMs) have demonstrated impressive performance,\nbut their deployment presents challenges due to their significant memory usage.\nThis issue can be alleviated through quantization. In this paper, we identify\nthat the challenge in quantizing activations in LLMs arises from varying ranges\nacross channels, rather than solely the presence of outliers. To address this\nchallenge, we introduce a quantization method called RPTQ, which utilizes a\nreorder-based approach. By rearranging the channels and quantizing them in\nclusters, RPTQ effectively mitigates the impact of range differences between\nchannels. To minimize the overhead of the reorder operation, we fuse it into\nthe layer norm operation and weights in linear layers. In our experiments, RPTQ\nachieved a significant breakthrough by utilizing 3-bit activation in LLMs for\nthe first time, resulting in a substantial reduction in memory usage. For\ninstance, quantizing OPT-175b can lead to a memory consumption reduction of up\nto 80%.",
        "pdf_link": "https://arxiv.org/pdf/2304.01089v4.pdf"
    },
    {
        "title": "Can the Inference Logic of Large Language Models be Disentangled into Symbolic Concepts?",
        "authors": [
            "Wen Shen",
            "Lei Cheng",
            "Yuxiao Yang",
            "Mingjie Li",
            "Quanshi Zhang"
        ],
        "published": "2023-04-03T15:39:35Z",
        "summary": "In this paper, we explain the inference logic of large language models (LLMs)\nas a set of symbolic concepts. Many recent studies have discovered that\ntraditional DNNs usually encode sparse symbolic concepts. However, because an\nLLM has much more parameters than traditional DNNs, whether the LLM also\nencodes sparse symbolic concepts is still an open problem. Therefore, in this\npaper, we propose to disentangle the inference score of LLMs for dialogue tasks\ninto a small number of symbolic concepts. We verify that we can use those\nsparse concepts to well estimate all inference scores of the LLM on all\narbitrarily masking states of the input sentence. We also evaluate the\ntransferability of concepts encoded by an LLM and verify that symbolic concepts\nusually exhibit high transferability across similar input sentences. More\ncrucially, those symbolic concepts can be used to explain the exact reasons\naccountable for the LLM's prediction errors.",
        "pdf_link": "https://arxiv.org/pdf/2304.01083v1.pdf"
    },
    {
        "title": "Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?",
        "authors": [
            "Adaku Uchendu",
            "Jooyoung Lee",
            "Hua Shen",
            "Thai Le",
            "Ting-Hao 'Kenneth' Huang",
            "Dongwon Lee"
        ],
        "published": "2023-04-03T14:06:47Z",
        "summary": "Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git",
        "pdf_link": "https://arxiv.org/pdf/2304.01002v3.pdf"
    },
    {
        "title": "Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection",
        "authors": [
            "Maxime Labonne",
            "Sean Moran"
        ],
        "published": "2023-04-03T10:27:53Z",
        "summary": "This paper investigates the effectiveness of large language models (LLMs) in\nemail spam detection by comparing prominent models from three distinct\nfamilies: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we\nexamine well-established machine learning techniques for spam detection, such\nas Na\\\"ive Bayes and LightGBM, as baseline methods. We assess the performance\nof these models across four public datasets, utilizing different numbers of\ntraining samples (full training set and few-shot settings). Our findings reveal\nthat, in the majority of cases, LLMs surpass the performance of the popular\nbaseline techniques, particularly in few-shot scenarios. This adaptability\nrenders LLMs uniquely suited to spam detection tasks, where labeled samples are\nlimited in number and models require frequent updates. Additionally, we\nintroduce Spam-T5, a Flan-T5 model that has been specifically adapted and\nfine-tuned for the purpose of detecting email spam. Our results demonstrate\nthat Spam-T5 surpasses baseline models and other LLMs in the majority of\nscenarios, particularly when there are a limited number of training samples\navailable. Our code is publicly available at\nhttps://github.com/jpmorganchase/emailspamdetection.",
        "pdf_link": "https://arxiv.org/pdf/2304.01238v3.pdf"
    },
    {
        "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
        "authors": [
            "Yi Chen",
            "Rui Wang",
            "Haiyun Jiang",
            "Shuming Shi",
            "Ruifeng Xu"
        ],
        "published": "2023-04-03T05:29:58Z",
        "summary": "Evaluating the quality of generated text is a challenging task in NLP, due to\nthe inherent complexity and diversity of text. Recently, large language models\n(LLMs) have garnered significant attention due to their impressive performance\nin various tasks. Therefore, we present this paper to investigate the\neffectiveness of LLMs, especially ChatGPT, and explore ways to optimize their\nuse in assessing text quality. We compared three kinds of reference-free\nevaluation methods. The experimental results prove that ChatGPT is capable of\nevaluating text quality effectively from various perspectives without reference\nand demonstrates superior performance than most existing automatic metrics. In\nparticular, the Explicit Score, which utilizes ChatGPT to generate a numeric\nscore measuring text quality, is the most effective and reliable method among\nthe three exploited approaches. However, directly comparing the quality of two\ntexts may lead to suboptimal results. We believe this paper will provide\nvaluable insights for evaluating text quality with LLMs and have released the\nused data.",
        "pdf_link": "https://arxiv.org/pdf/2304.00723v3.pdf"
    },
    {
        "title": "A Data-centric Framework for Improving Domain-specific Machine Reading Comprehension Datasets",
        "authors": [
            "Iva Bojic",
            "Josef Halim",
            "Verena Suharman",
            "Sreeja Tar",
            "Qi Chwen Ong",
            "Duy Phung",
            "Mathieu Ravaut",
            "Shafiq Joty",
            "Josip Car"
        ],
        "published": "2023-04-02T08:26:38Z",
        "summary": "Low-quality data can cause downstream problems in high-stakes applications.\nData-centric approach emphasizes on improving dataset quality to enhance model\nperformance. High-quality datasets are needed for general-purpose Large\nLanguage Models (LLMs) training, as well as for domain-specific models, which\nare usually small in size as it is costly to engage a large number of domain\nexperts for their creation. Thus, it is vital to ensure high-quality\ndomain-specific training data. In this paper, we propose a framework for\nenhancing the data quality of original datasets. We applied the proposed\nframework to four biomedical datasets and showed relative improvement of up to\n33%/40% for fine-tuning of retrieval/reader models on the BioASQ dataset when\nusing back translation to enhance the original dataset quality.",
        "pdf_link": "https://arxiv.org/pdf/2304.00483v2.pdf"
    },
    {
        "title": "Demonstration of InsightPilot: An LLM-Empowered Automated Data Exploration System",
        "authors": [
            "Pingchuan Ma",
            "Rui Ding",
            "Shuai Wang",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "published": "2023-04-02T07:27:49Z",
        "summary": "Exploring data is crucial in data analysis, as it helps users understand and\ninterpret the data more effectively. However, performing effective data\nexploration requires in-depth knowledge of the dataset and expertise in data\nanalysis techniques. Not being familiar with either can create obstacles that\nmake the process time-consuming and overwhelming for data analysts. To address\nthis issue, we introduce InsightPilot, an LLM (Large Language Model)-based,\nautomated data exploration system designed to simplify the data exploration\nprocess. InsightPilot automatically selects appropriate analysis intents, such\nas understanding, summarizing, and explaining. Then, these analysis intents are\nconcretized by issuing corresponding intentional queries (IQueries) to create a\nmeaningful and coherent exploration sequence. In brief, an IQuery is an\nabstraction and automation of data analysis operations, which mimics the\napproach of data analysts and simplifies the exploration process for users. By\nemploying an LLM to iteratively collaborate with a state-of-the-art insight\nengine via IQueries, InsightPilot is effective in analyzing real-world\ndatasets, enabling users to gain valuable insights through natural language\ninquiries. We demonstrate the effectiveness of InsightPilot in a case study,\nshowing how it can help users gain valuable insights from their datasets.",
        "pdf_link": "https://arxiv.org/pdf/2304.00477v2.pdf"
    },
    {
        "title": "Querying Large Language Models with SQL",
        "authors": [
            "Mohammed Saeed",
            "Nicola De Cao",
            "Paolo Papotti"
        ],
        "published": "2023-04-02T06:58:14Z",
        "summary": "In many use-cases, information is stored in text but not available in\nstructured data. However, extracting data from natural language text to\nprecisely fit a schema, and thus enable querying, is a challenging task. With\nthe rise of pre-trained Large Language Models (LLMs), there is now an effective\nsolution to store and use information extracted from massive corpora of text\ndocuments. Thus, we envision the use of SQL queries to cover a broad range of\ndata that is not captured by traditional databases by tapping the information\nin LLMs. To ground this vision, we present Galois, a prototype based on a\ntraditional database architecture, but with new physical operators for querying\nthe underlying LLM. The main idea is to execute some operators of the the query\nplan with prompts that retrieve data from the LLM. For a large class of SQL\nqueries, querying LLMs returns well structured relations, with encouraging\nqualitative results. Preliminary experimental results make pre-trained LLMs a\npromising addition to the field of database systems, introducing a new\ndirection for hybrid query processing. However, we pinpoint several research\nchallenges that must be addressed to build a DBMS that exploits LLMs. While\nsome of these challenges necessitate integrating concepts from the NLP\nliterature, others offer novel research avenues for the DB community.",
        "pdf_link": "https://arxiv.org/pdf/2304.00472v3.pdf"
    },
    {
        "title": "LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models",
        "authors": [
            "Patrik Puchert",
            "Poonam Poonam",
            "Christian van Onzenoodt",
            "Timo Ropinski"
        ],
        "published": "2023-04-02T05:47:09Z",
        "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nand demonstrated impressive capabilities in various tasks. Unfortunately, they\nare prone to hallucinations, where the model exposes incorrect or false\ninformation in its responses, which renders diligent evaluation approaches\nmandatory. While LLM performance in specific knowledge fields is often\nevaluated based on question and answer (Q&A) datasets, such evaluations usually\nreport only a single accuracy number for the dataset, which often covers an\nentire field. This field-based evaluation, is problematic with respect to\ntransparency and model improvement. A stratified evaluation could instead\nreveal subfields, where hallucinations are more likely to occur and thus help\nto better assess LLMs' risks and guide their further development. To support\nsuch stratified evaluations, we propose LLMMaps as a novel visualization\ntechnique that enables users to evaluate LLMs' performance with respect to Q&A\ndatasets. LLMMaps provide detailed insights into LLMs' knowledge capabilities\nin different subfields, by transforming Q&A datasets as well as LLM responses\ninto an internal knowledge structure. An extension for comparative\nvisualization furthermore, allows for the detailed comparison of multiple LLMs.\nTo assess LLMMaps we use them to conduct a comparative analysis of several\nstate-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as\nwell as two qualitative user evaluations. All necessary source code and data\nfor generating LLMMaps to be used in scientific publications and elsewhere is\navailable on GitHub: https://github.com/viscom-ulm/LLMMaps",
        "pdf_link": "https://arxiv.org/pdf/2304.00457v3.pdf"
    },
    {
        "title": "Towards Healthy AI: Large Language Models Need Therapists Too",
        "authors": [
            "Baihan Lin",
            "Djallel Bouneffouf",
            "Guillermo Cecchi",
            "Kush R. Varshney"
        ],
        "published": "2023-04-02T00:39:12Z",
        "summary": "Recent advances in large language models (LLMs) have led to the development\nof powerful AI chatbots capable of engaging in natural and human-like\nconversations. However, these chatbots can be potentially harmful, exhibiting\nmanipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to\nbe safe, trustworthy and ethical. To create healthy AI systems, we present the\nSafeguardGPT framework that uses psychotherapy to correct for these harmful\nbehaviors in AI chatbots. The framework involves four types of AI agents: a\nChatbot, a \"User,\" a \"Therapist,\" and a \"Critic.\" We demonstrate the\neffectiveness of SafeguardGPT through a working example of simulating a social\nconversation. Our results show that the framework can improve the quality of\nconversations between AI chatbots and humans. Although there are still several\nchallenges and directions to be addressed in the future, SafeguardGPT provides\na promising approach to improving the alignment between AI chatbots and human\nvalues. By incorporating psychotherapy and reinforcement learning techniques,\nthe framework enables AI chatbots to learn and adapt to human preferences and\nvalues in a safe and ethical way, contributing to the development of a more\nhuman-centric and responsible AI.",
        "pdf_link": "https://arxiv.org/pdf/2304.00416v1.pdf"
    },
    {
        "title": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection",
        "authors": [
            "Yizheng Chen",
            "Zhoujie Ding",
            "Lamya Alowain",
            "Xinyun Chen",
            "David Wagner"
        ],
        "published": "2023-04-01T23:29:14Z",
        "summary": "We propose and release a new vulnerable source code dataset. We curate the\ndataset by crawling security issue websites, extracting vulnerability-fixing\ncommits and source codes from the corresponding projects. Our new dataset\ncontains 18,945 vulnerable functions spanning 150 CWEs and 330,492\nnon-vulnerable functions extracted from 7,514 commits. Our dataset covers 295\nmore projects than all previous datasets combined.\n  Combining our new dataset with previous datasets, we present an analysis of\nthe challenges and promising research directions of using deep learning for\ndetecting software vulnerabilities. We study 11 model architectures belonging\nto 4 families. Our results show that deep learning is still not ready for\nvulnerability detection, due to high false positive rate, low F1 score, and\ndifficulty of detecting hard CWEs. In particular, we demonstrate an important\ngeneralization challenge for the deployment of deep learning-based models. We\nshow that increasing the volume of training data may not further improve the\nperformance of deep learning models for vulnerability detection, but might be\nuseful to improve the generalization ability to unseen projects.\n  We also identify hopeful future research directions. We demonstrate that\nlarge language models (LLMs) are a promising research direction for ML-based\nvulnerability detection, outperforming Graph Neural Networks (GNNs) with\ncode-structure features in our experiments. Moreover, developing source code\nspecific pre-training objectives is a promising research direction to improve\nthe vulnerability detection performance.",
        "pdf_link": "https://arxiv.org/pdf/2304.00409v2.pdf"
    },
    {
        "title": "Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT",
        "authors": [
            "Chunqiu Steven Xia",
            "Lingming Zhang"
        ],
        "published": "2023-04-01T20:57:33Z",
        "summary": "Automated Program Repair (APR) aims to automatically generate patches for\nbuggy programs. Recent APR work has been focused on leveraging modern Large\nLanguage Models (LLMs) to directly generate patches for APR. Such LLM-based APR\ntools work by first constructing an input prompt built using the original buggy\ncode and then queries the LLM to generate patches. While the LLM-based APR\ntools are able to achieve state-of-the-art results, it still follows the\nclassic Generate and Validate repair paradigm of first generating lots of\npatches and then validating each one afterwards. This not only leads to many\nrepeated patches that are incorrect but also miss the crucial information in\ntest failures as well as in plausible patches.\n  To address these limitations, we propose ChatRepair, the first fully\nautomated conversation-driven APR approach that interleaves patch generation\nwith instant feedback to perform APR in a conversational style. ChatRepair\nfirst feeds the LLM with relevant test failure information to start with, and\nthen learns from both failures and successes of earlier patching attempts of\nthe same bug for more powerful APR. For earlier patches that failed to pass all\ntests, we combine the incorrect patches with their corresponding relevant test\nfailure information to construct a new prompt for the LLM to generate the next\npatch. In this way, we can avoid making the same mistakes. For earlier patches\nthat passed all the tests, we further ask the LLM to generate alternative\nvariations of the original plausible patches. In this way, we can further build\non and learn from earlier successes to generate more plausible patches to\nincrease the chance of having correct patches. While our approach is general,\nwe implement ChatRepair using state-of-the-art dialogue-based LLM -- ChatGPT.\nBy calculating the cost of accessing ChatGPT, we can fix 162 out of 337 bugs\nfor \\$0.42 each!",
        "pdf_link": "https://arxiv.org/pdf/2304.00385v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on a Highly-specialized Topic, Radiation Oncology Physics",
        "authors": [
            "Jason Holmes",
            "Zhengliang Liu",
            "Lian Zhang",
            "Yuzhen Ding",
            "Terence T. Sio",
            "Lisa A. McGee",
            "Jonathan B. Ashman",
            "Xiang Li",
            "Tianming Liu",
            "Jiajian Shen",
            "Wei Liu"
        ],
        "published": "2023-04-01T06:04:58Z",
        "summary": "We present the first study to investigate Large Language Models (LLMs) in\nanswering radiation oncology physics questions. Because popular exams like AP\nPhysics, LSAT, and GRE have large test-taker populations and ample test\npreparation resources in circulation, they may not allow for accurately\nassessing the true potential of LLMs. This paper proposes evaluating LLMs on a\nhighly-specialized topic, radiation oncology physics, which may be more\npertinent to scientific and medical communities in addition to being a valuable\nbenchmark of LLMs. We developed an exam consisting of 100 radiation oncology\nphysics questions based on our expertise at Mayo Clinic. Four LLMs, ChatGPT\n(GPT-3.5), ChatGPT (GPT-4), Bard (LaMDA), and BLOOMZ, were evaluated against\nmedical physicists and non-experts. ChatGPT (GPT-4) outperformed all other LLMs\nas well as medical physicists, on average. The performance of ChatGPT (GPT-4)\nwas further improved when prompted to explain first, then answer. ChatGPT\n(GPT-3.5 and GPT-4) showed a high level of consistency in its answer choices\nacross a number of trials, whether correct or incorrect, a characteristic that\nwas not observed in the human test groups. In evaluating ChatGPTs (GPT-4)\ndeductive reasoning ability using a novel approach (substituting the correct\nanswer with \"None of the above choices is the correct answer.\"), ChatGPT\n(GPT-4) demonstrated surprising accuracy, suggesting the potential presence of\nan emergent ability. Finally, although ChatGPT (GPT-4) performed well overall,\nits intrinsic properties did not allow for further improvement when scoring\nbased on a majority vote across trials. In contrast, a team of medical\nphysicists were able to greatly outperform ChatGPT (GPT-4) using a majority\nvote. This study suggests a great potential for LLMs to work alongside\nradiation oncology experts as highly knowledgeable assistants.",
        "pdf_link": "https://arxiv.org/pdf/2304.01938v1.pdf"
    },
    {
        "title": "Large language models can rate news outlet credibility",
        "authors": [
            "Kai-Cheng Yang",
            "Filippo Menczer"
        ],
        "published": "2023-04-01T05:04:06Z",
        "summary": "Although large language models (LLMs) have shown exceptional performance in\nvarious natural language processing tasks, they are prone to hallucinations.\nState-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue\nby gathering information directly from the internet to ground their answers. In\nthis setting, the capacity to distinguish trustworthy sources is critical for\nproviding appropriate accuracy contexts to users. Here we assess whether\nChatGPT, a prominent LLM, can evaluate the credibility of news outlets. With\nappropriate instructions, ChatGPT can provide ratings for a diverse set of news\noutlets, including those in non-English languages and satirical sources, along\nwith contextual explanations. Our results show that these ratings correlate\nwith those from human experts (Spearmam's $\\rho=0.54, p<0.001$). These findings\nsuggest that LLMs could be an affordable reference for credibility ratings in\nfact-checking applications. Future LLMs should enhance their alignment with\nhuman expert judgments of source credibility to improve information accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2304.00228v1.pdf"
    },
    {
        "title": "Decoding the End-to-end Writing Trajectory in Scholarly Manuscripts",
        "authors": [
            "Ryan Koo",
            "Anna Martin",
            "Linghe Wang",
            "Dongyeop Kang"
        ],
        "published": "2023-03-31T20:33:03Z",
        "summary": "Scholarly writing presents a complex space that generally follows a\nmethodical procedure to plan and produce both rationally sound and creative\ncompositions. Recent works involving large language models (LLM) demonstrate\nconsiderable success in text generation and revision tasks; however, LLMs still\nstruggle to provide structural and creative feedback on the document level that\nis crucial to academic writing. In this paper, we introduce a novel taxonomy\nthat categorizes scholarly writing behaviors according to intention, writer\nactions, and the information types of the written data. We also provide\nManuScript, an original dataset annotated with a simplified version of our\ntaxonomy to show writer actions and the intentions behind them. Motivated by\ncognitive writing theory, our taxonomy for scientific papers includes three\nlevels of categorization in order to trace the general writing flow and\nidentify the distinct writer activities embedded within each higher-level\nprocess. ManuScript intends to provide a complete picture of the scholarly\nwriting process by capturing the linearity and non-linearity of writing\ntrajectory, such that writing assistants can provide stronger feedback and\nsuggestions on an end-to-end level. The collected writing trajectories are\nviewed at https://minnesotanlp.github.io/REWARD_demo/",
        "pdf_link": "https://arxiv.org/pdf/2304.00121v1.pdf"
    },
    {
        "title": "Enhancing Large Language Models with Climate Resources",
        "authors": [
            "Mathias Kraus",
            "Julia Anna Bingler",
            "Markus Leippold",
            "Tobias Schimanski",
            "Chiara Colesanti Senni",
            "Dominik Stammbach",
            "Saeid Ashraf Vaghefi",
            "Nicolas Webersinke"
        ],
        "published": "2023-03-31T20:24:14Z",
        "summary": "Large language models (LLMs) have significantly transformed the landscape of\nartificial intelligence by demonstrating their ability in generating human-like\ntext across diverse topics. However, despite their impressive capabilities,\nLLMs lack recent information and often employ imprecise language, which can be\ndetrimental in domains where accuracy is crucial, such as climate change. In\nthis study, we make use of recent ideas to harness the potential of LLMs by\nviewing them as agents that access multiple sources, including databases\ncontaining recent and precise information about organizations, institutions,\nand companies. We demonstrate the effectiveness of our method through a\nprototype agent that retrieves emission data from ClimateWatch\n(https://www.climatewatchdata.org/) and leverages general Google search. By\nintegrating these resources with LLMs, our approach overcomes the limitations\nassociated with imprecise language and delivers more reliable and accurate\ninformation in the critical domain of climate change. This work paves the way\nfor future advancements in LLMs and their application in domains where\nprecision is of paramount importance.",
        "pdf_link": "https://arxiv.org/pdf/2304.00116v1.pdf"
    },
    {
        "title": "A Survey of Large Language Models",
        "authors": [
            "Wayne Xin Zhao",
            "Kun Zhou",
            "Junyi Li",
            "Tianyi Tang",
            "Xiaolei Wang",
            "Yupeng Hou",
            "Yingqian Min",
            "Beichen Zhang",
            "Junjie Zhang",
            "Zican Dong",
            "Yifan Du",
            "Chen Yang",
            "Yushuo Chen",
            "Zhipeng Chen",
            "Jinhao Jiang",
            "Ruiyang Ren",
            "Yifan Li",
            "Xinyu Tang",
            "Zikang Liu",
            "Peiyu Liu",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2023-03-31T17:28:46Z",
        "summary": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.",
        "pdf_link": "https://arxiv.org/pdf/2303.18223v13.pdf"
    },
    {
        "title": "Assessing Language Model Deployment with Risk Cards",
        "authors": [
            "Leon Derczynski",
            "Hannah Rose Kirk",
            "Vidhisha Balachandran",
            "Sachin Kumar",
            "Yulia Tsvetkov",
            "M. R. Leiser",
            "Saif Mohammad"
        ],
        "published": "2023-03-31T16:45:42Z",
        "summary": "This paper introduces RiskCards, a framework for structured assessment and\ndocumentation of risks associated with an application of language models. As\nwith all language, text generated by language models can be harmful, or used to\nbring about harm. Automating language generation adds both an element of scale\nand also more subtle or emergent undesirable tendencies to the generated text.\nPrior work establishes a wide variety of language model harms to many different\nactors: existing taxonomies identify categories of harms posed by language\nmodels; benchmarks establish automated tests of these harms; and documentation\nstandards for models, tasks and datasets encourage transparent reporting.\nHowever, there is no risk-centric framework for documenting the complexity of a\nlandscape in which some risks are shared across models and contexts, while\nothers are specific, and where certain conditions may be required for risks to\nmanifest as harms. RiskCards address this methodological gap by providing a\ngeneric framework for assessing the use of a given language model in a given\nscenario. Each RiskCard makes clear the routes for the risk to manifest harm,\ntheir placement in harm taxonomies, and example prompt-output pairs. While\nRiskCards are designed to be open-source, dynamic and participatory, we present\na \"starter set\" of RiskCards taken from a broad literature survey, each of\nwhich details a concrete risk presentation. Language model RiskCards initiate a\ncommunity knowledge base which permits the mapping of risks and harms to a\nspecific model or its application scenario, ultimately contributing to a\nbetter, safer and shared understanding of the risk landscape.",
        "pdf_link": "https://arxiv.org/pdf/2303.18190v1.pdf"
    },
    {
        "title": "CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Mizanur Rahman",
            "Israt Jahan",
            "Enamul Hoque",
            "Jimmy Huang"
        ],
        "published": "2023-03-31T15:39:54Z",
        "summary": "Debatepedia is a publicly available dataset consisting of arguments and\ncounter-arguments on controversial topics that has been widely used for the\nsingle-document query-focused abstractive summarization task in recent years.\nHowever, it has been recently found that this dataset is limited by noise and\neven most queries in this dataset do not have any relevance to the respective\ndocument. In this paper, we present a methodology for cleaning the Debatepedia\ndataset by leveraging the generative power of large language models to make it\nsuitable for query-focused abstractive summarization. More specifically, we\nharness the language generation capabilities of ChatGPT to regenerate its\nqueries. We evaluate the effectiveness of the proposed ChatGPT annotated\nversion of the Debatepedia dataset using several benchmark summarization models\nand demonstrate that the newly annotated version of Debatepedia outperforms the\noriginal dataset in terms of both query relevance as well as summary generation\nquality. We will make this annotated and cleaned version of the dataset\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2305.06147v1.pdf"
    },
    {
        "title": "BERTino: an Italian DistilBERT model",
        "authors": [
            "Matteo Muffo",
            "Enrico Bertino"
        ],
        "published": "2023-03-31T15:07:40Z",
        "summary": "The recent introduction of Transformers language representation models\nallowed great improvements in many natural language processing (NLP) tasks.\nHowever, if on one hand the performances achieved by this kind of architectures\nare surprising, on the other their usability is limited by the high number of\nparameters which constitute their network, resulting in high computational and\nmemory demands. In this work we present BERTino, a DistilBERT model which\nproposes to be the first lightweight alternative to the BERT architecture\nspecific for the Italian language. We evaluated BERTino on the Italian ISDT,\nItalian ParTUT, Italian WikiNER and multiclass classification tasks, obtaining\nF1 scores comparable to those obtained by a BERTBASE with a remarkable\nimprovement in training and inference speed.",
        "pdf_link": "https://arxiv.org/pdf/2303.18121v1.pdf"
    },
    {
        "title": "Evaluating GPT-4 and ChatGPT on Japanese Medical Licensing Examinations",
        "authors": [
            "Jungo Kasai",
            "Yuhei Kasai",
            "Keisuke Sakaguchi",
            "Yutaro Yamada",
            "Dragomir Radev"
        ],
        "published": "2023-03-31T13:04:47Z",
        "summary": "As large language models (LLMs) gain popularity among speakers of diverse\nlanguages, we believe that it is crucial to benchmark them to better understand\nmodel behaviors, failures, and limitations in languages beyond English. In this\nwork, we evaluate LLM APIs (ChatGPT, GPT-3, and GPT-4) on the Japanese national\nmedical licensing examinations from the past five years, including the current\nyear. Our team comprises native Japanese-speaking NLP researchers and a\npracticing cardiologist based in Japan. Our experiments show that GPT-4\noutperforms ChatGPT and GPT-3 and passes all six years of the exams,\nhighlighting LLMs' potential in a language that is typologically distant from\nEnglish. However, our evaluation also exposes critical limitations of the\ncurrent LLM APIs. First, LLMs sometimes select prohibited choices that should\nbe strictly avoided in medical practice in Japan, such as suggesting\neuthanasia. Further, our analysis shows that the API costs are generally higher\nand the maximum context size is smaller for Japanese because of the way\nnon-Latin scripts are currently tokenized in the pipeline. We release our\nbenchmark as Igaku QA as well as all model outputs and exam metadata. We hope\nthat our results and benchmark will spur progress on more diverse applications\nof LLMs. Our benchmark is available at https://github.com/jungokasai/IgakuQA.",
        "pdf_link": "https://arxiv.org/pdf/2303.18027v2.pdf"
    },
    {
        "title": "GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors",
        "authors": [
            "Dongyeop Jang",
            "Tae-Rim Yun",
            "Choong-Yeol Lee",
            "Young-Kyu Kwon",
            "Chang-Eop Kim"
        ],
        "published": "2023-03-31T05:43:21Z",
        "summary": "Traditional Korean medicine (TKM) emphasizes individualized diagnosis and\ntreatment. This uniqueness makes AI modeling difficult due to limited data and\nimplicit processes. Large language models (LLMs) have demonstrated impressive\nmedical inference, even without advanced training in medical texts. This study\nassessed the capabilities of GPT-4 in TKM, using the Korean National Licensing\nExamination for Korean Medicine Doctors (K-NLEKMD) as a benchmark. The\nK-NLEKMD, administered by a national organization, encompasses 12 major\nsubjects in TKM. We optimized prompts with Chinese-term annotation, English\ntranslation for questions and instruction, exam-optimized instruction, and\nself-consistency. GPT-4 with optimized prompts achieved 66.18% accuracy,\nsurpassing both the examination's average pass mark of 60% and the 40% minimum\nfor each subject. The gradual introduction of language-related prompts and\nprompting techniques enhanced the accuracy from 51.82% to its maximum accuracy.\nGPT-4 showed low accuracy in subjects including public health &\nmedicine-related law, internal medicine (2) which are localized in Korea and\nTKM. The model's accuracy was lower for questions requiring TKM-specialized\nknowledge. It exhibited higher accuracy in diagnosis-based and recall-based\nquestions than in intervention-based questions. A positive correlation was\nobserved between the consistency and accuracy of GPT-4's responses. This study\nunveils both the potential and challenges of applying LLMs to TKM. These\nfindings underline the potential of LLMs like GPT-4 in culturally adapted\nmedicine, especially TKM, for tasks such as clinical assistance, medical\neducation, and research. But they also point towards the necessity for the\ndevelopment of methods to mitigate cultural bias inherent in large language\nmodels and validate their efficacy in real-world clinical settings.",
        "pdf_link": "https://arxiv.org/pdf/2303.17807v2.pdf"
    },
    {
        "title": "AceCoder: Utilizing Existing Code to Enhance Code Generation",
        "authors": [
            "Jia Li",
            "Yunfei Zhao",
            "Yongmin Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "published": "2023-03-31T02:57:15Z",
        "summary": "Large Language Models (LLMs) have shown great success in code generation.\nLLMs take as the input a prompt and output the code. A key question is how to\nmake prompts (i.e., Prompting Techniques). Existing prompting techniques are\ndesigned for natural language generation and have low accuracy in code\ngeneration.\n  In this paper, we propose a new prompting technique named AceCoder. Our\nmotivation is that code generation meets two unique challenges (i.e.,\nrequirement understanding and code implementation). AceCoder contains two novel\nmechanisms (i.e., guided code generation and example retrieval) to solve these\nchallenges. (1) Guided code generation asks LLMs first to analyze requirements\nand output an intermediate preliminary (e.g., test cases). The preliminary is\nused to clarify requirements and tell LLMs \"what to write\". (2) Example\nretrieval selects similar programs as examples in prompts, which provide lots\nof relevant content (e.g., algorithms, APIs) and teach LLMs \"how to write\". We\napply AceCoder to three LLMs (e.g., Codex) and evaluate it on three public\nbenchmarks using the Pass@k. Results show that AceCoder can significantly\nimprove the performance of LLMs on code generation. (1) In terms of Pass@1,\nAceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,\n70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs with\ndifferent sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,\nand JavaScript). (3) Human evaluation shows human developers prefer programs\nfrom AceCoder.",
        "pdf_link": "https://arxiv.org/pdf/2303.17780v3.pdf"
    },
    {
        "title": "Can ChatGPT be used to generate scientific hypotheses?",
        "authors": [
            "Yang Jeong Park",
            "Daniel Kaplan",
            "Zhichu Ren",
            "Chia-Wei Hsu",
            "Changhao Li",
            "Haowei Xu",
            "Sipei Li",
            "Ju Li"
        ],
        "published": "2023-03-30T20:40:52Z",
        "summary": "We investigate whether large language models can perform the creative\nhypothesis generation that human researchers regularly do. While the error rate\nis high, generative AI seems to be able to effectively structure vast amounts\nof scientific knowledge and provide interesting and testable hypotheses. The\nfuture scientific enterprise may include synergistic efforts with a swarm of\n\"hypothesis machines\", challenged by automated experimentation and adversarial\npeer reviews.",
        "pdf_link": "https://arxiv.org/pdf/2304.12208v1.pdf"
    },
    {
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Prakhar Gupta",
            "Skyler Hallinan",
            "Luyu Gao",
            "Sarah Wiegreffe",
            "Uri Alon",
            "Nouha Dziri",
            "Shrimai Prabhumoye",
            "Yiming Yang",
            "Shashank Gupta",
            "Bodhisattwa Prasad Majumder",
            "Katherine Hermann",
            "Sean Welleck",
            "Amir Yazdanbakhsh",
            "Peter Clark"
        ],
        "published": "2023-03-30T18:30:01Z",
        "summary": "Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.",
        "pdf_link": "https://arxiv.org/pdf/2303.17651v2.pdf"
    },
    {
        "title": "Recognition, recall, and retention of few-shot memories in large language models",
        "authors": [
            "A. Emin Orhan"
        ],
        "published": "2023-03-30T17:26:16Z",
        "summary": "The training of modern large language models (LLMs) takes place in a regime\nwhere most training examples are seen only a few times by the model during the\ncourse of training. What does a model remember about such examples seen only a\nfew times during training and how long does that memory persist in the face of\ncontinuous training with new examples? Here, we investigate these questions\nthrough simple recognition, recall, and retention experiments with LLMs. In\nrecognition experiments, we ask if the model can distinguish the seen example\nfrom a novel example; in recall experiments, we ask if the model can correctly\nrecall the seen example when cued by a part of it; and in retention\nexperiments, we periodically probe the model's memory for the original examples\nas the model is trained continuously with new examples. We find that a single\nexposure is generally sufficient for a model to achieve near perfect accuracy\neven in very challenging recognition experiments. We estimate that the\nrecognition performance of even small language models easily exceeds human\nrecognition performance reported in similar experiments with humans (Shepard,\n1967). Achieving near perfect recall takes more exposures, but most models can\ndo it in just 3 exposures. The flip side of this remarkable capacity for fast\nlearning is that precise memories are quickly overwritten: recall performance\nfor the original examples drops steeply over the first 10 training updates with\nnew examples, followed by a more gradual decline. Even after 100K updates,\nhowever, some of the original examples are still recalled near perfectly. A\nqualitatively similar retention pattern has been observed in human long-term\nmemory retention studies before (Bahrick, 1984). Finally, recognition is much\nmore robust to interference than recall and memory for natural language\nsentences is generally superior to memory for stimuli without structure.",
        "pdf_link": "https://arxiv.org/pdf/2303.17557v1.pdf"
    },
    {
        "title": "Language Models can Solve Computer Tasks",
        "authors": [
            "Geunwoo Kim",
            "Pierre Baldi",
            "Stephen McAleer"
        ],
        "published": "2023-03-30T16:01:52Z",
        "summary": "Agents capable of carrying out general tasks on a computer can improve\nefficiency and productivity by automating repetitive tasks and assisting in\ncomplex problem-solving. Ideally, such agents should be able to solve new\ncomputer tasks presented to them through natural language commands. However,\nprevious approaches to this problem require large amounts of expert\ndemonstrations and task-specific reward functions, both of which are\nimpractical for new tasks. In this work, we show that a pre-trained large\nlanguage model (LLM) agent can execute computer tasks guided by natural\nlanguage using a simple prompting scheme where the agent Recursively Criticizes\nand Improves its output (RCI). The RCI approach significantly outperforms\nexisting LLM methods for automating computer tasks and surpasses supervised\nlearning (SL) and reinforcement learning (RL) approaches on the MiniWoB++\nbenchmark. We compare multiple LLMs and find that RCI with the\nInstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful\nof demonstrations per task rather than tens of thousands, and without a\ntask-specific reward function. Furthermore, we demonstrate RCI prompting's\neffectiveness in enhancing LLMs' reasoning abilities on a suite of natural\nlanguage reasoning tasks, outperforming chain of thought (CoT) prompting with\nexternal feedback. We find that RCI combined with CoT performs better than\neither separately. Our code can be found here:\nhttps://github.com/posgnu/rci-agent.",
        "pdf_link": "https://arxiv.org/pdf/2303.17491v3.pdf"
    },
    {
        "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research",
        "authors": [
            "Xinhao Mei",
            "Chutong Meng",
            "Haohe Liu",
            "Qiuqiang Kong",
            "Tom Ko",
            "Chengqi Zhao",
            "Mark D. Plumbley",
            "Yuexian Zou",
            "Wenwu Wang"
        ],
        "published": "2023-03-30T14:07:47Z",
        "summary": "The advancement of audio-language (AL) multimodal learning tasks has been\nsignificant in recent years. However, researchers face challenges due to the\ncostly and time-consuming collection process of existing audio-language\ndatasets, which are limited in size. To address this data scarcity issue, we\nintroduce WavCaps, the first large-scale weakly-labelled audio captioning\ndataset, comprising approximately 400k audio clips with paired captions. We\nsourced audio clips and their raw descriptions from web sources and a sound\nevent detection dataset. However, the online-harvested raw descriptions are\nhighly noisy and unsuitable for direct use in tasks such as automated audio\ncaptioning. To overcome this issue, we propose a three-stage processing\npipeline for filtering noisy data and generating high-quality captions, where\nChatGPT, a large language model, is leveraged to filter and transform raw\ndescriptions automatically. We conduct a comprehensive analysis of the\ncharacteristics of WavCaps dataset and evaluate it on multiple downstream\naudio-language multimodal learning tasks. The systems trained on WavCaps\noutperform previous state-of-the-art (SOTA) models by a significant margin. Our\naspiration is for the WavCaps dataset we have proposed to facilitate research\nin audio-language multimodal learning and demonstrate the potential of\nutilizing ChatGPT to enhance academic research. Our dataset and codes are\navailable at https://github.com/XinhaoMei/WavCaps.",
        "pdf_link": "https://arxiv.org/pdf/2303.17395v1.pdf"
    },
    {
        "title": "Yes but.. Can ChatGPT Identify Entities in Historical Documents?",
        "authors": [
            "Carlos-Emiliano González-Gallardo",
            "Emanuela Boros",
            "Nancy Girdhar",
            "Ahmed Hamdi",
            "Jose G. Moreno",
            "Antoine Doucet"
        ],
        "published": "2023-03-30T12:23:39Z",
        "summary": "Large language models (LLMs) have been leveraged for several years now,\nobtaining state-of-the-art performance in recognizing entities from modern\ndocuments. For the last few months, the conversational agent ChatGPT has\n\"prompted\" a lot of interest in the scientific community and public due to its\ncapacity of generating plausible-sounding answers. In this paper, we explore\nthis ability by probing it in the named entity recognition and classification\n(NERC) task in primary sources (e.g., historical newspapers and classical\ncommentaries) in a zero-shot manner and by comparing it with state-of-the-art\nLM-based systems. Our findings indicate several shortcomings in identifying\nentities in historical text that range from the consistency of entity\nannotation guidelines, entity complexity, and code-switching, to the\nspecificity of prompting. Moreover, as expected, the inaccessibility of\nhistorical archives to the public (and thus on the Internet) also impacts its\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2303.17322v1.pdf"
    },
    {
        "title": "Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure",
        "authors": [
            "Philipp Koralus",
            "Vincent Wang-Maścianica"
        ],
        "published": "2023-03-30T10:32:18Z",
        "summary": "Increase in computational scale and fine-tuning has seen a dramatic\nimprovement in the quality of outputs of large language models (LLMs) like GPT.\nGiven that both GPT-3 and GPT-4 were trained on large quantities of\nhuman-generated text, we might ask to what extent their outputs reflect\npatterns of human thinking, both for correct and incorrect cases. The Erotetic\nTheory of Reason (ETR) provides a symbolic generative model of both human\nsuccess and failure in thinking, across propositional, quantified, and\nprobabilistic reasoning, as well as decision-making. We presented GPT-3,\nGPT-3.5, and GPT-4 with 61 central inference and judgment problems from a\nrecent book-length presentation of ETR, consisting of experimentally verified\ndata-points on human judgment and extrapolated data-points predicted by ETR,\nwith correct inference patterns as well as fallacies and framing effects (the\nETR61 benchmark). ETR61 includes classics like Wason's card task, illusory\ninferences, the decoy effect, and opportunity-cost neglect, among others. GPT-3\nshowed evidence of ETR-predicted outputs for 59% of these examples, rising to\n77% in GPT-3.5 and 75% in GPT-4. Remarkably, the production of human-like\nfallacious judgments increased from 18% in GPT-3 to 33% in GPT-3.5 and 34% in\nGPT-4. This suggests that larger and more advanced LLMs may develop a tendency\ntoward more human-like mistakes, as relevant thought patterns are inherent in\nhuman-produced training data. According to ETR, the same fundamental patterns\nare involved both in successful and unsuccessful ordinary reasoning, so that\nthe \"bad\" cases could paradoxically be learned from the \"good\" cases. We\nfurther present preliminary evidence that ETR-inspired prompt engineering could\nreduce instances of these mistakes.",
        "pdf_link": "https://arxiv.org/pdf/2303.17276v1.pdf"
    },
    {
        "title": "The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling",
        "authors": [
            "Joey Öhman",
            "Severine Verlinden",
            "Ariel Ekgren",
            "Amaru Cuba Gyllensten",
            "Tim Isbister",
            "Evangelia Gogoulou",
            "Fredrik Carlsson",
            "Magnus Sahlgren"
        ],
        "published": "2023-03-30T06:42:22Z",
        "summary": "Pre-training Large Language Models (LLMs) require massive amounts of text\ndata, and the performance of the LLMs typically correlates with the scale and\nquality of the datasets. This means that it may be challenging to build LLMs\nfor smaller languages such as Nordic ones, where the availability of text\ncorpora is limited. In order to facilitate the development of the LLMS in the\nNordic languages, we curate a high-quality dataset consisting of 1.2TB of text,\nin all of the major North Germanic languages (Danish, Icelandic, Norwegian, and\nSwedish), as well as some high-quality English data. This paper details our\nconsiderations and processes for collecting, cleaning, and filtering the\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2303.17183v1.pdf"
    },
    {
        "title": "Advances in apparent conceptual physics reasoning in GPT-4",
        "authors": [
            "Colin G. West"
        ],
        "published": "2023-03-29T20:32:40Z",
        "summary": "ChatGPT is built on a large language model trained on an enormous corpus of\nhuman text to emulate human conversation. Despite lacking any explicit\nprogramming regarding the laws of physics, recent work has demonstrated that\nGPT-3.5 could pass an introductory physics course at some nominal level and\nregister something close to a minimal understanding of Newtonian Mechanics on\nthe Force Concept Inventory. This work replicates those results and also\ndemonstrates that the latest version, GPT-4, has reached a much higher mark in\nthe latter context. Indeed, its responses come quite close to perfectly\ndemonstrating expert-level competence, with a few very notable exceptions and\nlimitations. We briefly comment on the implications of this for the future of\nphysics education and pedagogy.",
        "pdf_link": "https://arxiv.org/pdf/2303.17012v3.pdf"
    },
    {
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
        "authors": [
            "Yang Liu",
            "Dan Iter",
            "Yichong Xu",
            "Shuohang Wang",
            "Ruochen Xu",
            "Chenguang Zhu"
        ],
        "published": "2023-03-29T12:46:54Z",
        "summary": "The quality of texts generated by natural language generation (NLG) systems\nis hard to measure automatically. Conventional reference-based metrics, such as\nBLEU and ROUGE, have been shown to have relatively low correlation with human\njudgments, especially for tasks that require creativity and diversity. Recent\nstudies suggest using large language models (LLMs) as reference-free metrics\nfor NLG evaluation, which have the benefit of being applicable to new tasks\nthat lack human references. However, these LLM-based evaluators still have\nlower human correspondence than medium-size neural evaluators. In this work, we\npresent G-Eval, a framework of using large language models with\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of\nNLG outputs. We experiment with two generation tasks, text summarization and\ndialogue generation. We show that G-Eval with GPT-4 as the backbone model\nachieves a Spearman correlation of 0.514 with human on summarization task,\noutperforming all previous methods by a large margin. We also propose\npreliminary analysis on the behavior of LLM-based evaluators, and highlight the\npotential issue of LLM-based evaluators having a bias towards the LLM-generated\ntexts. The code is at https://github.com/nlpyang/geval",
        "pdf_link": "https://arxiv.org/pdf/2303.16634v3.pdf"
    },
    {
        "title": "LMExplainer: a Knowledge-Enhanced Explainer for Language Models",
        "authors": [
            "Zichen Chen",
            "Ambuj K Singh",
            "Misha Sra"
        ],
        "published": "2023-03-29T08:59:44Z",
        "summary": "Large language models (LLMs) such as GPT-4 are very powerful and can process\ndifferent kinds of natural language processing (NLP) tasks. However, it can be\ndifficult to interpret the results due to the multi-layer nonlinear model\nstructure and millions of parameters. A lack of clarity and understanding of\nhow the language models (LMs) work can make them unreliable, difficult to\ntrust, and potentially dangerous for use in real-world scenarios. Most recent\nworks exploit attention weights to provide explanations for LM predictions.\nHowever, pure attention-based explanations are unable to support the growing\ncomplexity of LMs, and cannot reason about their decision-making processes. We\npropose LMExplainer, a knowledge-enhanced explainer for LMs that can provide\nhuman-understandable explanations. We use a knowledge graph (KG) and a graph\nattention neural network to extract the key decision signals of the LM. We\nfurther explore whether interpretation can also help the AI understand the task\nbetter. Our experimental results show that LMExplainer outperforms existing\nLM+KG methods on CommonsenseQA and OpenBookQA. We compare the explanation\nresults with generated explanation methods and human-annotated results. The\ncomparison shows our method can provide more comprehensive and clearer\nexplanations. LMExplainer demonstrates the potential to enhance model\nperformance and furnish explanations for the LM reasoning process in natural\nlanguage.",
        "pdf_link": "https://arxiv.org/pdf/2303.16537v2.pdf"
    },
    {
        "title": "Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning",
        "authors": [
            "Namrata Shivagunde",
            "Vladislav Lialin",
            "Anna Rumshisky"
        ],
        "published": "2023-03-29T04:00:53Z",
        "summary": "Language model probing is often used to test specific capabilities of models.\nHowever, conclusions from such studies may be limited when the probing\nbenchmarks are small and lack statistical power. In this work, we introduce\nnew, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500)\ninspired by psycholinguistic studies. We dramatically extend existing NEG-136\nand ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44\nsentence pairs to 750 each. We also create another version of extended negation\ndataset (NEG-1500-SIMP-TEMP), created using template-based generation. It\nconsists of 770 sentence pairs. We evaluate 22 models on the extended datasets,\nseeing model performance dip 20-57% compared to the original smaller\nbenchmarks. We observe high levels of negation sensitivity in models like BERT\nand ALBERT demonstrating that previous findings might have been skewed due to\nsmaller test sets. Finally, we observe that while GPT3 has generated all the\nexamples in ROLE-1500 is only able to solve 24.6% of them during probing. The\ndatasets and code are available on\n$\\href{https://github.com/text-machine-lab/extending_psycholinguistic_dataset}{Github}$.",
        "pdf_link": "https://arxiv.org/pdf/2303.16445v3.pdf"
    },
    {
        "title": "ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models",
        "authors": [
            "Ning Bian",
            "Xianpei Han",
            "Le Sun",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Shanshan Jiang",
            "Bin Dong"
        ],
        "published": "2023-03-29T03:05:43Z",
        "summary": "Large language models (LLMs) have made significant progress in NLP. However,\ntheir ability to memorize, represent, and leverage commonsense knowledge has\nbeen a well-known pain point. In this paper, we specifically focus on ChatGPT,\na widely used and easily accessible LLM, and ask the following questions: (1)\nCan ChatGPT effectively answer commonsense questions? (2) Is ChatGPT aware of\nthe underlying commonsense knowledge for answering a specific question? (3) Is\nChatGPT knowledgeable in commonsense? (4) Can ChatGPT effectively leverage\ncommonsense for answering questions? We conduct a series of experiments on 11\ndatasets to evaluate ChatGPT's commonsense abilities, including answering\ncommonsense questions, identifying necessary knowledge, generating knowledge\ndescriptions, and using knowledge descriptions to answer questions again.\nExperimental results show that: (1) ChatGPT can achieve good QA accuracies in\ncommonsense tasks, while still struggling with certain domains of datasets. (2)\nChatGPT is knowledgeable, and can accurately generate most of the commonsense\nknowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an\ninexperienced commonsense problem solver, which cannot precisely identify the\nneeded commonsense for answering a specific question. These findings raise the\nneed to explore improved mechanisms for effectively incorporating commonsense\ninto LLMs like ChatGPT, such as better instruction following and commonsense\nguidance.",
        "pdf_link": "https://arxiv.org/pdf/2303.16421v2.pdf"
    },
    {
        "title": "Writing Assistants Should Model Social Factors of Language",
        "authors": [
            "Vivek Kulkarni",
            "Vipul Raheja"
        ],
        "published": "2023-03-28T19:38:57Z",
        "summary": "Intelligent writing assistants powered by large language models (LLMs) are\nmore popular today than ever before, but their further widespread adoption is\nprecluded by sub-optimal performance. In this position paper, we argue that a\nmajor reason for this sub-optimal performance and adoption is a singular focus\non the information content of language while ignoring its social aspects. We\nanalyze the different dimensions of these social factors in the context of\nwriting assistants and propose their incorporation into building smarter, more\neffective, and truly personalized writing assistants that would enrich the user\nexperience and contribute to increased user adoption.",
        "pdf_link": "https://arxiv.org/pdf/2303.16275v1.pdf"
    },
    {
        "title": "Training Language Models with Language Feedback at Scale",
        "authors": [
            "Jérémy Scheurer",
            "Jon Ander Campos",
            "Tomasz Korbak",
            "Jun Shern Chan",
            "Angelica Chen",
            "Kyunghyun Cho",
            "Ethan Perez"
        ],
        "published": "2023-03-28T17:04:15Z",
        "summary": "Pretrained language models often generate outputs that are not in line with\nhuman preferences, such as harmful text or factually incorrect summaries.\nRecent work approaches the above issues by learning from a simple form of human\nfeedback: comparisons between pairs of model-generated outputs. However,\ncomparison feedback only conveys limited information about human preferences.\nIn this paper, we introduce Imitation learning from Language Feedback (ILF), a\nnew approach that utilizes more informative language feedback. ILF consists of\nthree steps that are applied iteratively: first, conditioning the language\nmodel on the input, an initial LM output, and feedback to generate refinements.\nSecond, selecting the refinement incorporating the most feedback. Third,\nfinetuning the language model to maximize the likelihood of the chosen\nrefinement given the input. We show theoretically that ILF can be viewed as\nBayesian Inference, similar to Reinforcement Learning from human feedback. We\nevaluate ILF's effectiveness on a carefully-controlled toy task and a realistic\nsummarization task. Our experiments demonstrate that large language models\naccurately incorporate feedback and that finetuning with ILF scales well with\nthe dataset size, even outperforming finetuning on human summaries. Learning\nfrom both language and comparison feedback outperforms learning from each\nalone, achieving human-level summarization performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.16755v3.pdf"
    },
    {
        "title": "Hallucinations in Large Multilingual Translation Models",
        "authors": [
            "Nuno M. Guerreiro",
            "Duarte Alves",
            "Jonas Waldendorf",
            "Barry Haddow",
            "Alexandra Birch",
            "Pierre Colombo",
            "André F. T. Martins"
        ],
        "published": "2023-03-28T16:17:59Z",
        "summary": "Large-scale multilingual machine translation systems have demonstrated\nremarkable ability to translate directly between numerous languages, making\nthem increasingly appealing for real-world applications. However, when deployed\nin the wild, these models may generate hallucinated translations which have the\npotential to severely undermine user trust and raise safety concerns. Existing\nresearch on hallucinations has primarily focused on small bilingual models\ntrained on high-resource languages, leaving a gap in our understanding of\nhallucinations in massively multilingual models across diverse translation\nscenarios. In this work, we fill this gap by conducting a comprehensive\nanalysis on both the M2M family of conventional neural machine translation\nmodels and ChatGPT, a general-purpose large language model~(LLM) that can be\nprompted for translation. Our investigation covers a broad spectrum of\nconditions, spanning over 100 translation directions across various resource\nlevels and going beyond English-centric language pairs. We provide key insights\nregarding the prevalence, properties, and mitigation of hallucinations, paving\nthe way towards more responsible and reliable machine translation systems.",
        "pdf_link": "https://arxiv.org/pdf/2303.16104v1.pdf"
    },
    {
        "title": "Improving Code Generation by Training with Natural Language Feedback",
        "authors": [
            "Angelica Chen",
            "Jérémy Scheurer",
            "Tomasz Korbak",
            "Jon Ander Campos",
            "Jun Shern Chan",
            "Samuel R. Bowman",
            "Kyunghyun Cho",
            "Ethan Perez"
        ],
        "published": "2023-03-28T16:15:31Z",
        "summary": "The potential for pre-trained large language models (LLMs) to use natural\nlanguage feedback at inference time has been an exciting recent development. We\nbuild upon this observation by formalizing an algorithm for learning from\nnatural language feedback at training time instead, which we call Imitation\nlearning from Language Feedback (ILF). ILF requires only a small amount of\nhuman-written feedback during training and does not require the same feedback\nat test time, making it both user-friendly and sample-efficient. We further\nshow that ILF can be seen as a form of minimizing the KL divergence to the\nground truth distribution and demonstrate a proof-of-concept on a neural\nprogram synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's\npass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python\nProblems (MBPP) benchmark, outperforming both fine-tuning on MBPP and\nfine-tuning on repaired programs written by humans. Overall, our results\nsuggest that learning from human-written natural language feedback is both more\neffective and sample-efficient than training exclusively on demonstrations for\nimproving an LLM's performance on code generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.16749v2.pdf"
    },
    {
        "title": "Synthetically generated text for supervised text analysis",
        "authors": [
            "Andrew Halterman"
        ],
        "published": "2023-03-28T14:55:13Z",
        "summary": "Supervised text models are a valuable tool for political scientists but\npresent several obstacles to their use, including the expense of hand-labeling\ndocuments, the difficulty of retrieving rare relevant documents for annotation,\nand copyright and privacy concerns involved in sharing annotated documents.\nThis article proposes a partial solution to these three issues, in the form of\ncontrolled generation of synthetic text with large language models. I provide a\nconceptual overview of text generation, guidance on when researchers should\nprefer different techniques for generating synthetic text, a discussion of\nethics, and a simple technique for improving the quality of synthetic text. I\ndemonstrate the usefulness of synthetic text with three applications:\ngenerating synthetic tweets describing the fighting in Ukraine, synthetic news\narticles describing specified political events for training an event detection\nsystem, and a multilingual corpus of populist manifesto statements for training\na sentence-level populism classifier.",
        "pdf_link": "https://arxiv.org/pdf/2303.16028v1.pdf"
    },
    {
        "title": "Evaluation of ChatGPT for NLP-based Mental Health Applications",
        "authors": [
            "Bishal Lamichhane"
        ],
        "published": "2023-03-28T04:47:43Z",
        "summary": "Large language models (LLM) have been successful in several natural language\nunderstanding tasks and could be relevant for natural language processing\n(NLP)-based mental health application research. In this work, we report the\nperformance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three\ntext-based mental health classification tasks: stress detection (2-class\nclassification), depression detection (2-class classification), and suicidality\ndetection (5-class classification). We obtained annotated social media posts\nfor the three classification tasks from public datasets. Then ChatGPT API\nclassified the social media posts with an input prompt for classification. We\nobtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression\ndetection, and suicidality detection, respectively. A baseline model that\nalways predicted the dominant class resulted in F1 scores of 0.35, 0.60, and\n0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a\npotential use of language models for mental health classification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.15727v1.pdf"
    },
    {
        "title": "Pre-training Transformers for Knowledge Graph Completion",
        "authors": [
            "Sanxing Chen",
            "Hao Cheng",
            "Xiaodong Liu",
            "Jian Jiao",
            "Yangfeng Ji",
            "Jianfeng Gao"
        ],
        "published": "2023-03-28T02:10:37Z",
        "summary": "Learning transferable representation of knowledge graphs (KGs) is challenging\ndue to the heterogeneous, multi-relational nature of graph structures. Inspired\nby Transformer-based pretrained language models' success on learning\ntransferable representation for texts, we introduce a novel inductive KG\nrepresentation model (iHT) for KG completion by large-scale pre-training. iHT\nconsists of a entity encoder (e.g., BERT) and a neighbor-aware relational\nscoring function both parameterized by Transformers. We first pre-train iHT on\na large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art\nresults on matched evaluations, with a relative improvement of more than 25% in\nmean reciprocal rank over previous SOTA models. When further fine-tuned on\nsmaller KGs with either entity and relational shifts, pre-trained iHT\nrepresentations are shown to be transferable, significantly improving the\nperformance on FB15K-237 and WN18RR.",
        "pdf_link": "https://arxiv.org/pdf/2303.15682v1.pdf"
    },
    {
        "title": "Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning",
        "authors": [
            "Vladislav Lialin",
            "Vijeta Deshpande",
            "Anna Rumshisky"
        ],
        "published": "2023-03-28T00:06:38Z",
        "summary": "This paper presents a systematic overview and comparison of\nparameter-efficient fine-tuning methods covering over 40 papers published\nbetween February 2019 and February 2023. These methods aim to resolve the\ninfeasibility and impracticality of fine-tuning large language models by only\ntraining a small set of parameters. We provide a taxonomy that covers a broad\nrange of methods and present a detailed method comparison with a specific focus\non real-life efficiency and fine-tuning multibillion-scale language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.15647v1.pdf"
    },
    {
        "title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2023-03-27T22:30:39Z",
        "summary": "The performance of text summarization has been greatly boosted by pre-trained\nlanguage models. A main concern of existing methods is that most generated\nsummaries are not factually inconsistent with their source documents. To\nalleviate the problem, many efforts have focused on developing effective\nfactuality evaluation metrics based on natural language inference, question\nanswering, and syntactic dependency et al. However, these approaches are\nlimited by either their high computational complexity or the uncertainty\nintroduced by multi-component pipelines, resulting in only partial agreement\nwith human judgement. Most recently, large language models(LLMs) have shown\nexcellent performance in not only text generation but also language\ncomprehension. In this paper, we particularly explore ChatGPT's ability to\nevaluate factual inconsistency under a zero-shot setting by examining it on\nboth coarse-grained and fine-grained evaluation tasks including binary\nentailment inference, summary ranking, and consistency rating. Experimental\nresults indicate that ChatGPT generally outperforms previous evaluation metrics\nacross the three tasks, indicating its great potential for factual\ninconsistency evaluation. However, a closer inspection of ChatGPT's output\nreveals certain limitations including its preference for more lexically similar\ncandidates, false reasoning, and inadequate understanding of instructions.",
        "pdf_link": "https://arxiv.org/pdf/2303.15621v2.pdf"
    },
    {
        "title": "Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing",
        "authors": [
            "Walid Hariri"
        ],
        "published": "2023-03-27T21:27:58Z",
        "summary": "Large language models have revolutionized the field of artificial\nintelligence and have been used in various applications. Among these models,\nChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,\nit stands out as a powerful tool that has been widely adopted. ChatGPT has been\nsuccessfully applied in numerous areas, including chatbots, content generation,\nlanguage translation, personalized recommendations, and even medical diagnosis\nand treatment. Its success in these applications can be attributed to its\nability to generate human-like responses, understand natural language, and\nadapt to different contexts. Its versatility and accuracy make it a powerful\ntool for natural language processing (NLP). However, there are also limitations\nto ChatGPT, such as its tendency to produce biased responses and its potential\nto perpetuate harmful language patterns. This article provides a comprehensive\noverview of ChatGPT, its applications, advantages, and limitations.\nAdditionally, the paper emphasizes the importance of ethical considerations\nwhen using this robust tool in real-world scenarios. Finally, This paper\ncontributes to ongoing discussions surrounding artificial intelligence and its\nimpact on vision and NLP domains by providing insights into prompt engineering\ntechniques.",
        "pdf_link": "https://arxiv.org/pdf/2304.02017v8.pdf"
    },
    {
        "title": "Linguistically Informed ChatGPT Prompts to Enhance Japanese-Chinese Machine Translation: A Case Study on Attributive Clauses",
        "authors": [
            "Wenshi Gu"
        ],
        "published": "2023-03-27T20:33:40Z",
        "summary": "In the field of Japanese-Chinese translation linguistics, the issue of\ncorrectly translating attributive clauses has persistently proven to be\nchallenging. Present-day machine translation tools often fail to accurately\ntranslate attributive clauses from Japanese to Chinese. In light of this, this\npaper investigates the linguistic problem underlying such difficulties, namely\nhow does the semantic role of the modified noun affect the selection of\ntranslation patterns for attributive clauses, from a linguistic perspective. To\nad-dress these difficulties, a pre-edit scheme is proposed, which aims to\nenhance the accuracy of translation. Furthermore, we propose a novel two-step\nprompt strategy, which combines this pre-edit scheme with ChatGPT, currently\nthe most widely used large language model. This prompt strategy is capable of\noptimizing translation input in zero-shot scenarios and has been demonstrated\nto improve the average translation accuracy score by over 35%.",
        "pdf_link": "https://arxiv.org/pdf/2303.15587v1.pdf"
    },
    {
        "title": "TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models",
        "authors": [
            "Md Kamrul Hasan",
            "Md Saiful Islam",
            "Sangwu Lee",
            "Wasifur Rahman",
            "Iftekhar Naim",
            "Mohammed Ibrahim Khan",
            "Ehsan Hoque"
        ],
        "published": "2023-03-27T17:54:32Z",
        "summary": "Pre-trained large language models have recently achieved ground-breaking\nperformance in a wide variety of language understanding tasks. However, the\nsame model can not be applied to multimodal behavior understanding tasks (e.g.,\nvideo sentiment/humor detection) unless non-verbal features (e.g., acoustic and\nvisual) can be integrated with language. Jointly modeling multiple modalities\nsignificantly increases the model complexity, and makes the training process\ndata-hungry. While an enormous amount of text data is available via the web,\ncollecting large-scale multimodal behavioral video datasets is extremely\nexpensive, both in terms of time and money. In this paper, we investigate\nwhether large language models alone can successfully incorporate non-verbal\ninformation when they are presented in textual form. We present a way to\nconvert the acoustic and visual information into corresponding textual\ndescriptions and concatenate them with the spoken text. We feed this augmented\ninput to a pre-trained BERT model and fine-tune it on three downstream\nmultimodal tasks: sentiment, humor, and sarcasm detection. Our approach,\nTextMI, significantly reduces model complexity, adds interpretability to the\nmodel's decision, and can be applied for a diverse set of tasks while achieving\nsuperior (multimodal sarcasm detection) or near SOTA (multimodal sentiment\nanalysis and multimodal humor detection) performance. We propose TextMI as a\ngeneral, competitive baseline for multimodal behavioral analysis tasks,\nparticularly in a low-resource setting.",
        "pdf_link": "https://arxiv.org/pdf/2303.15430v2.pdf"
    },
    {
        "title": "KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation",
        "authors": [
            "Di Wu",
            "Da Yin",
            "Kai-Wei Chang"
        ],
        "published": "2023-03-27T17:45:38Z",
        "summary": "Despite the significant advancements in keyphrase extraction and keyphrase\ngeneration methods, the predominant approach for evaluation mainly relies on\nexact matching with human references. This scheme fails to recognize systems\nthat generate keyphrases semantically equivalent to the references or diverse\nkeyphrases that carry practical utility. To better assess the capability of\nkeyphrase systems, we propose KPEval, a comprehensive evaluation framework\nconsisting of four critical aspects: reference agreement, faithfulness,\ndiversity, and utility. For each aspect, we design semantic-based metrics to\nreflect the evaluation objectives. Meta-evaluation studies demonstrate that our\nevaluation strategy correlates better with human preferences compared to a\nrange of previously proposed metrics. Using KPEval, we re-evaluate 21 keyphrase\nsystems and discover that (1) established model comparison results have\nblind-spots especially when considering reference-free evaluation; (2) large\nlanguage models are underestimated by prior evaluation works; and (3) there is\nno single best model that can excel in all the aspects.",
        "pdf_link": "https://arxiv.org/pdf/2303.15422v3.pdf"
    },
    {
        "title": "LMCanvas: Object-Oriented Interaction to Personalize Large Language Model-Powered Writing Environments",
        "authors": [
            "Tae Soo Kim",
            "Arghya Sarkar",
            "Yoonjoo Lee",
            "Minsuk Chang",
            "Juho Kim"
        ],
        "published": "2023-03-27T11:56:26Z",
        "summary": "Large language models (LLMs) can enhance writing by automating or supporting\nspecific tasks in writers' workflows (e.g., paraphrasing, creating analogies).\nLeveraging this capability, a collection of interfaces have been developed that\nprovide LLM-powered tools for specific writing tasks. However, these interfaces\nprovide limited support for writers to create personal tools for their own\nunique tasks, and may not comprehensively fulfill a writer's needs -- requiring\nthem to continuously switch between interfaces during writing. In this work, we\nenvision LMCanvas, an interface that enables writers to create their own\nLLM-powered writing tools and arrange their personal writing environment by\ninteracting with \"blocks\" in a canvas. In this interface, users can create text\nblocks to encapsulate writing and LLM prompts, model blocks for model parameter\nconfigurations, and connect these to create pipeline blocks that output\ngenerations. In this workshop paper, we discuss the design for LMCanvas and our\nplans to develop this concept.",
        "pdf_link": "https://arxiv.org/pdf/2303.15125v1.pdf"
    },
    {
        "title": "Large Language Models are Diverse Role-Players for Summarization Evaluation",
        "authors": [
            "Ning Wu",
            "Ming Gong",
            "Linjun Shou",
            "Shining Liang",
            "Daxin Jiang"
        ],
        "published": "2023-03-27T10:40:59Z",
        "summary": "Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. A document summary's quality can be\nassessed by human annotators on various criteria, both objective ones like\ngrammar and correctness, and subjective ones like informativeness,\nsuccinctness, and appeal. Most of the automatic evaluation methods like\nBLUE/ROUGE may be not able to adequately capture the above dimensions. In this\npaper, we propose a new evaluation framework based on LLMs, which provides a\ncomprehensive evaluation framework by comparing generated text and reference\ntext from both objective and subjective aspects. First, we propose to model\nobjective and subjective dimensions of generated text based on roleplayers\nprompting mechanism. Furthermore, we introduce a context-based prompting\nmechanism that is able to generate dynamic roleplayer profiles based on input\ncontext. Finally, we design a multi-roleplayer prompting technology based on\nbatch prompting and integrate multiple outputs into the final evaluation\nresults. Experimental results on three real datasets for summarization show\nthat our model is highly competitive and has a very high consistency with human\nannotators.",
        "pdf_link": "https://arxiv.org/pdf/2303.15078v3.pdf"
    },
    {
        "title": "Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain",
        "authors": [
            "Xu Liu",
            "Mengyue Zhou",
            "Gaosheng Shi",
            "Yu Du",
            "Lin Zhao",
            "Zihao Wu",
            "David Liu",
            "Tianming Liu",
            "Xintao Hu"
        ],
        "published": "2023-03-27T01:41:48Z",
        "summary": "Linking computational natural language processing (NLP) models and neural\nresponses to language in the human brain on the one hand facilitates the effort\ntowards disentangling the neural representations underpinning language\nperception, on the other hand provides neurolinguistics evidence to evaluate\nand improve NLP models. Mappings of an NLP model's representations of and the\nbrain activities evoked by linguistic input are typically deployed to reveal\nthis symbiosis. However, two critical problems limit its advancement: 1) The\nmodel's representations (artificial neurons, ANs) rely on layer-level\nembeddings and thus lack fine-granularity; 2) The brain activities (biological\nneurons, BNs) are limited to neural recordings of isolated cortical unit (i.e.,\nvoxel/region) and thus lack integrations and interactions among brain\nfunctions. To address those problems, in this study, we 1) define ANs with\nfine-granularity in transformer-based NLP models (BERT in this study) and\nmeasure their temporal activations to input text sequences; 2) define BNs as\nfunctional brain networks (FBNs) extracted from functional magnetic resonance\nimaging (fMRI) data to capture functional interactions in the brain; 3) couple\nANs and BNs by maximizing the synchronization of their temporal activations.\nOur experimental results demonstrate 1) The activations of ANs and BNs are\nsignificantly synchronized; 2) the ANs carry meaningful linguistic/semantic\ninformation and anchor to their BN signatures; 3) the anchored BNs are\ninterpretable in a neurolinguistic context. Overall, our study introduces a\nnovel, general, and effective framework to link transformer-based NLP models\nand neural activities in response to language and may provide novel insights\nfor future studies such as brain-inspired evaluation and development of NLP\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2303.14871v1.pdf"
    },
    {
        "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
        "authors": [
            "Xinlei He",
            "Xinyue Shen",
            "Zeyuan Chen",
            "Michael Backes",
            "Yang Zhang"
        ],
        "published": "2023-03-26T21:12:36Z",
        "summary": "Nowadays, powerful large language models (LLMs) such as ChatGPT have\ndemonstrated revolutionary power in a variety of tasks. Consequently, the\ndetection of machine-generated texts (MGTs) is becoming increasingly crucial as\nLLMs become more advanced and prevalent. These models have the ability to\ngenerate human-like language, making it challenging to discern whether a text\nis authored by a human or a machine. This raises concerns regarding\nauthenticity, accountability, and potential bias. However, existing methods for\ndetecting MGTs are evaluated using different model architectures, datasets, and\nexperimental settings, resulting in a lack of a comprehensive evaluation\nframework that encompasses various methodologies. Furthermore, it remains\nunclear how existing detection methods would perform against powerful LLMs. In\nthis paper, we fill this gap by proposing the first benchmark framework for MGT\ndetection against powerful LLMs, named MGTBench. Extensive evaluations on\npublic datasets with curated texts generated by various powerful LLMs such as\nChatGPT-turbo and Claude demonstrate the effectiveness of different detection\nmethods. Our ablation study shows that a larger number of words in general\nleads to better performance and most detection methods can achieve similar\nperformance with much fewer training samples. Moreover, we delve into a more\nchallenging task: text attribution. Our findings indicate that the model-based\ndetection methods still perform well in the text attribution task. To\ninvestigate the robustness of different detection methods, we consider three\nadversarial attacks, namely paraphrasing, random spacing, and adversarial\nperturbations. We discover that these attacks can significantly diminish\ndetection effectiveness, underscoring the critical need for the development of\nmore robust detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2303.14822v3.pdf"
    },
    {
        "title": "WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation",
        "authors": [
            "Jongheon Jeong",
            "Yang Zou",
            "Taewan Kim",
            "Dongqing Zhang",
            "Avinash Ravichandran",
            "Onkar Dabeer"
        ],
        "published": "2023-03-26T20:41:21Z",
        "summary": "Visual anomaly classification and segmentation are vital for automating\nindustrial quality inspection. The focus of prior research in the field has\nbeen on training custom models for each quality inspection task, which requires\ntask-specific images and annotation. In this paper we move away from this\nregime, addressing zero-shot and few-normal-shot anomaly classification and\nsegmentation. Recently CLIP, a vision-language model, has shown revolutionary\ngenerality with competitive zero-/few-shot performance in comparison to\nfull-supervision. But CLIP falls short on anomaly classification and\nsegmentation tasks. Hence, we propose window-based CLIP (WinCLIP) with (1) a\ncompositional ensemble on state words and prompt templates and (2) efficient\nextraction and aggregation of window/patch/image-level features aligned with\ntext. We also propose its few-normal-shot extension WinCLIP+, which uses\ncomplementary information from normal images. In MVTec-AD (and VisA), without\nfurther tuning, WinCLIP achieves 91.8%/85.1% (78.1%/79.6%) AUROC in zero-shot\nanomaly classification and segmentation while WinCLIP+ does 93.1%/95.2%\n(83.8%/96.4%) in 1-normal-shot, surpassing state-of-the-art by large margins.",
        "pdf_link": "https://arxiv.org/pdf/2303.14814v1.pdf"
    },
    {
        "title": "Task-oriented Memory-efficient Pruning-Adapter",
        "authors": [
            "Guorun Wang",
            "Jun Yang",
            "Yaoru Sun"
        ],
        "published": "2023-03-26T12:18:00Z",
        "summary": "The Outstanding performance and growing size of Large Language Models has led\nto increased attention in parameter efficient learning. The two predominant\napproaches are Adapters and Pruning. Adapters are to freeze the model and give\nit a new weight matrix on the side, which can significantly reduce the time and\nmemory of training, but the cost is that the evaluation and testing will\nincrease the time and memory consumption. Pruning is to cut off some weight and\nre-distribute the remaining weight, which sacrifices the complexity of training\nat the cost of extremely high memory and training time, making the cost of\nevaluation and testing relatively low. So efficiency of training and inference\ncan't be obtained in the same time. In this work, we propose a task-oriented\nPruning-Adapter method that achieve a high memory efficiency of training and\nmemory, and speeds up training time and ensures no significant decrease in\naccuracy in GLUE tasks, achieving training and inference efficiency at the same\ntime.",
        "pdf_link": "https://arxiv.org/pdf/2303.14704v2.pdf"
    },
    {
        "title": "Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System",
        "authors": [
            "Yunfan Gao",
            "Tao Sheng",
            "Youlin Xiang",
            "Yun Xiong",
            "Haofen Wang",
            "Jiawei Zhang"
        ],
        "published": "2023-03-25T17:37:43Z",
        "summary": "Large language models (LLMs) have demonstrated their significant potential to\nbe applied for addressing various application tasks. However, traditional\nrecommender systems continue to face great challenges such as poor\ninteractivity and explainability, which actually also hinder their broad\ndeployment in real-world systems. To address these limitations, this paper\nproposes a novel paradigm called Chat-Rec (ChatGPT Augmented Recommender\nSystem) that innovatively augments LLMs for building conversational recommender\nsystems by converting user profiles and historical interactions into prompts.\nChat-Rec is demonstrated to be effective in learning user preferences and\nestablishing connections between users and products through in-context\nlearning, which also makes the recommendation process more interactive and\nexplainable. What's more, within the Chat-Rec framework, user's preferences can\ntransfer to different products for cross-domain recommendations, and\nprompt-based injection of information into LLMs can also handle the cold-start\nscenarios with new items. In our experiments, Chat-Rec effectively improve the\nresults of top-k recommendations and performs better in zero-shot rating\nprediction task. Chat-Rec offers a novel approach to improving recommender\nsystems and presents new practical scenarios for the implementation of AIGC (AI\ngenerated content) in recommender system studies.",
        "pdf_link": "https://arxiv.org/pdf/2303.14524v2.pdf"
    },
    {
        "title": "Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining",
        "authors": [
            "Zhouhong Gu",
            "Sihang Jiang",
            "Wenhao Huang",
            "Jiaqing Liang",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "published": "2023-03-25T10:19:14Z",
        "summary": "The model's ability to understand synonymous expression is crucial in many\nkinds of downstream tasks. It will make the model to better understand the\nsimilarity between context, and more robust to the synonym substitution attack.\nHowever, many Pretrained Language Model (PLM) lack synonym knowledge due to\nlimitation of small-scale synsets and PLM's pretraining objectives. In this\npaper, we propose a framework called Sem4SAP to mine synsets from Open\nKnowledge Graph (Open-KG) and using the mined synsets to do synonym-aware\npretraining for language models. We propose to coarsly filter the content in\nOpen-KG and use the frequency information to better help the clustering process\nunder low-resource unsupervised conditions. We expand the mined synsets by\nmigrating core semantics between synonymous expressions.We also propose two\nnovel and effective synonym-aware pre-training methods for injecting synonym\nknowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can\ndramatically outperform the original PLMs and other baselines on ten different\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.14425v1.pdf"
    },
    {
        "title": "Freestyle Layout-to-Image Synthesis",
        "authors": [
            "Han Xue",
            "Zhiwu Huang",
            "Qianru Sun",
            "Li Song",
            "Wenjun Zhang"
        ],
        "published": "2023-03-25T09:37:41Z",
        "summary": "Typical layout-to-image synthesis (LIS) models generate images for a closed\nset of semantic classes, e.g., 182 common objects in COCO-Stuff. In this work,\nwe explore the freestyle capability of the model, i.e., how far can it generate\nunseen semantics (e.g., classes, attributes, and styles) onto a given layout,\nand call the task Freestyle LIS (FLIS). Thanks to the development of\nlarge-scale pre-trained language-image models, a number of discriminative\nmodels (e.g., image classification and object detection) trained on limited\nbase classes are empowered with the ability of unseen class prediction.\nInspired by this, we opt to leverage large-scale pre-trained text-to-image\ndiffusion models to achieve the generation of unseen semantics. The key\nchallenge of FLIS is how to enable the diffusion model to synthesize images\nfrom a specific layout which very likely violates its pre-learned knowledge,\ne.g., the model never sees \"a unicorn sitting on a bench\" during its\npre-training. To this end, we introduce a new module called Rectified\nCross-Attention (RCA) that can be conveniently plugged in the diffusion model\nto integrate semantic masks. This \"plug-in\" is applied in each cross-attention\nlayer of the model to rectify the attention maps between image and text tokens.\nThe key idea of RCA is to enforce each text token to act on the pixels in a\nspecified region, allowing us to freely put a wide variety of semantics from\npre-trained knowledge (which is general) onto the given layout (which is\nspecific). Extensive experiments show that the proposed diffusion network\nproduces realistic and freestyle layout-to-image generation results with\ndiverse text inputs, which has a high potential to spawn a bunch of interesting\napplications. Code is available at https://github.com/essunny310/FreestyleNet.",
        "pdf_link": "https://arxiv.org/pdf/2303.14412v1.pdf"
    },
    {
        "title": "Backdoor Attacks with Input-unique Triggers in NLP",
        "authors": [
            "Xukun Zhou",
            "Jiwei Li",
            "Tianwei Zhang",
            "Lingjuan Lyu",
            "Muqiao Yang",
            "Jun He"
        ],
        "published": "2023-03-25T01:41:54Z",
        "summary": "Backdoor attack aims at inducing neural models to make incorrect predictions\nfor poison data while keeping predictions on the clean dataset unchanged, which\ncreates a considerable threat to current natural language processing (NLP)\nsystems. Existing backdoor attacking systems face two severe issues:firstly,\nmost backdoor triggers follow a uniform and usually input-independent pattern,\ne.g., insertion of specific trigger words, synonym replacement. This\nsignificantly hinders the stealthiness of the attacking model, leading the\ntrained backdoor model being easily identified as malicious by model probes.\nSecondly, trigger-inserted poisoned sentences are usually disfluent,\nungrammatical, or even change the semantic meaning from the original sentence,\nmaking them being easily filtered in the pre-processing stage. To resolve these\ntwo issues, in this paper, we propose an input-unique backdoor attack(NURA),\nwhere we generate backdoor triggers unique to inputs. IDBA generates\ncontext-related triggers by continuing writing the input with a language model\nlike GPT2. The generated sentence is used as the backdoor trigger. This\nstrategy not only creates input-unique backdoor triggers, but also preserves\nthe semantics of the original input, simultaneously resolving the two issues\nabove. Experimental results show that the IDBA attack is effective for attack\nand difficult to defend: it achieves high attack success rate across all the\nwidely applied benchmarks, while is immune to existing defending methods. In\naddition, it is able to generate fluent, grammatical, and diverse backdoor\ninputs, which can hardly be recognized through human inspection.",
        "pdf_link": "https://arxiv.org/pdf/2303.14325v1.pdf"
    },
    {
        "title": "TRAK: Attributing Model Behavior at Scale",
        "authors": [
            "Sung Min Park",
            "Kristian Georgiev",
            "Andrew Ilyas",
            "Guillaume Leclerc",
            "Aleksander Madry"
        ],
        "published": "2023-03-24T17:56:22Z",
        "summary": "The goal of data attribution is to trace model predictions back to training\ndata. Despite a long line of work towards this goal, existing approaches to\ndata attribution tend to force users to choose between computational\ntractability and efficacy. That is, computationally tractable methods can\nstruggle with accurately attributing model predictions in non-convex settings\n(e.g., in the context of deep neural networks), while methods that are\neffective in such regimes require training thousands of models, which makes\nthem impractical for large models or datasets.\n  In this work, we introduce TRAK (Tracing with the Randomly-projected After\nKernel), a data attribution method that is both effective and computationally\ntractable for large-scale, differentiable models. In particular, by leveraging\nonly a handful of trained models, TRAK can match the performance of attribution\nmethods that require training thousands of models. We demonstrate the utility\nof TRAK across various modalities and scales: image classifiers trained on\nImageNet, vision-language models (CLIP), and language models (BERT and mT5). We\nprovide code for using TRAK (and reproducing our work) at\nhttps://github.com/MadryLab/trak .",
        "pdf_link": "https://arxiv.org/pdf/2303.14186v2.pdf"
    },
    {
        "title": "\"Get ready for a party\": Exploring smarter smart spaces with help from large language models",
        "authors": [
            "Evan King",
            "Haoxiang Yu",
            "Sangsu Lee",
            "Christine Julien"
        ],
        "published": "2023-03-24T16:51:08Z",
        "summary": "The right response to someone who says \"get ready for a party\" is deeply\ninfluenced by meaning and context. For a smart home assistant (e.g., Google\nHome), the ideal response might be to survey the available devices in the home\nand change their state to create a festive atmosphere. Current practical\nsystems cannot service such requests since they require the ability to (1)\ninfer meaning behind an abstract statement and (2) map that inference to a\nconcrete course of action appropriate for the context (e.g., changing the\nsettings of specific devices). In this paper, we leverage the observation that\nrecent task-agnostic large language models (LLMs) like GPT-3 embody a vast\namount of cross-domain, sometimes unpredictable contextual knowledge that\nexisting rule-based home assistant systems lack, which can make them powerful\ntools for inferring user intent and generating appropriate context-dependent\nresponses during smart home interactions. We first explore the feasibility of a\nsystem that places an LLM at the center of command inference and action\nplanning, showing that LLMs have the capacity to infer intent behind vague,\ncontext-dependent commands like \"get ready for a party\" and respond with\nconcrete, machine-parseable instructions that can be used to control smart\ndevices. We furthermore demonstrate a proof-of-concept implementation that puts\nan LLM in control of real devices, showing its ability to infer intent and\nchange device state appropriately with no fine-tuning or task-specific\ntraining. Our work hints at the promise of LLM-driven systems for\ncontext-awareness in smart environments, motivating future research in this\narea.",
        "pdf_link": "https://arxiv.org/pdf/2303.14143v1.pdf"
    },
    {
        "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
        "authors": [
            "Yunxiang Li",
            "Zihan Li",
            "Kai Zhang",
            "Ruilong Dan",
            "Steve Jiang",
            "You Zhang"
        ],
        "published": "2023-03-24T15:29:16Z",
        "summary": "The primary aim of this research was to address the limitations observed in\nthe medical knowledge of prevalent large language models (LLMs) such as\nChatGPT, by creating a specialized language model with enhanced accuracy in\nmedical advice. We achieved this by adapting and refining the large language\nmodel meta-AI (LLaMA) using a large dataset of 100,000 patient-doctor dialogues\nsourced from a widely used online medical consultation platform. These\nconversations were cleaned and anonymized to respect privacy concerns. In\naddition to the model refinement, we incorporated a self-directed information\nretrieval mechanism, allowing the model to access and utilize real-time\ninformation from online sources like Wikipedia and data from curated offline\nmedical databases. The fine-tuning of the model with real-world patient-doctor\ninteractions significantly improved the model's ability to understand patient\nneeds and provide informed advice. By equipping the model with self-directed\ninformation retrieval from reliable online and offline sources, we observed\nsubstantial improvements in the accuracy of its responses. Our proposed\nChatDoctor, represents a significant advancement in medical LLMs, demonstrating\na significant improvement in understanding patient inquiries and providing\naccurate advice. Given the high stakes and low error tolerance in the medical\nfield, such enhancements in providing accurate and reliable information are not\nonly beneficial but essential.",
        "pdf_link": "https://arxiv.org/pdf/2303.14070v5.pdf"
    },
    {
        "title": "Paraphrase Detection: Human vs. Machine Content",
        "authors": [
            "Jonas Becker",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "published": "2023-03-24T13:25:46Z",
        "summary": "The growing prominence of large language models, such as GPT-4 and ChatGPT,\nhas led to increased concerns over academic integrity due to the potential for\nmachine-generated content and paraphrasing. Although studies have explored the\ndetection of human- and machine-paraphrased content, the comparison between\nthese types of content remains underexplored. In this paper, we conduct a\ncomprehensive analysis of various datasets commonly employed for paraphrase\ndetection tasks and evaluate an array of detection methods. Our findings\nhighlight the strengths and limitations of different detection methods in terms\nof performance on individual datasets, revealing a lack of suitable\nmachine-generated datasets that can be aligned with human expectations. Our\nmain finding is that human-authored paraphrases exceed machine-generated ones\nin terms of difficulty, diversity, and similarity implying that automatically\ngenerated texts are not yet on par with human-level performance. Transformers\nemerged as the most effective method across datasets with TF-IDF excelling on\nsemantically diverse corpora. Additionally, we identify four datasets as the\nmost diverse and challenging for paraphrase detection.",
        "pdf_link": "https://arxiv.org/pdf/2303.13989v1.pdf"
    },
    {
        "title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods",
        "authors": [
            "Thilo Hagendorff"
        ],
        "published": "2023-03-24T13:24:41Z",
        "summary": "Large language models (LLMs) are currently at the forefront of intertwining\nAI systems with human communication and everyday life. Due to rapid\ntechnological advances and their extreme versatility, LLMs nowadays have\nmillions of users and are at the cusp of being the main go-to technology for\ninformation retrieval, content generation, problem-solving, etc. Therefore, it\nis of great importance to thoroughly assess and scrutinize their capabilities.\nDue to increasingly complex and novel behavioral patterns in current LLMs, this\ncan be done by treating them as participants in psychology experiments that\nwere originally designed to test humans. For this purpose, the paper introduces\na new field of research called \"machine psychology\". The paper outlines how\ndifferent subfields of psychology can inform behavioral tests for LLMs. It\ndefines methodological standards for machine psychology research, especially by\nfocusing on policies for prompt designs. Additionally, it describes how\nbehavioral patterns discovered in LLMs are to be interpreted. In sum, machine\npsychology aims to discover emergent abilities in LLMs that cannot be detected\nby most traditional natural language processing benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2303.13988v4.pdf"
    },
    {
        "title": "$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference",
        "authors": [
            "Benfeng Xu",
            "Quan Wang",
            "Zhendong Mao",
            "Yajuan Lyu",
            "Qiaoqiao She",
            "Yongdong Zhang"
        ],
        "published": "2023-03-24T06:16:29Z",
        "summary": "In-Context Learning (ICL), which formulates target tasks as prompt completion\nconditioned on in-context demonstrations, has become the prevailing utilization\nof LLMs. In this paper, we first disclose an actual predicament for this\ntypical usage that it can not scale up with training data due to context length\nrestriction. Besides, existing works have shown that ICL also suffers from\nvarious biases and requires delicate calibration treatment. To address both\nchallenges, we advocate a simple and effective solution, $k$NN Prompting, which\nfirst queries LLM with training data for distributed representations, then\npredicts test instances by simply referring to nearest neighbors. We conduct\ncomprehensive experiments to demonstrate its two-fold superiority: 1)\nCalibration-Free: $k$NN Prompting does not directly align LLM output\ndistribution with task-specific label space, instead leverages such\ndistribution to align test and training instances. It significantly outperforms\nstate-of-the-art calibration-based methods under comparable few-shot scenario.\n2) Beyond-Context: $k$NN Prompting can further scale up effectively with as\nmany training data as are available, continually bringing substantial\nimprovements. The scaling trend holds across 10 orders of magnitude ranging\nfrom 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B\nto 30B. It successfully bridges data scaling into model scaling, and brings new\npotentials for the gradient-free paradigm of LLM deployment. Code is publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2303.13824v1.pdf"
    },
    {
        "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models",
        "authors": [
            "Qingyu Lu",
            "Baopu Qiu",
            "Liang Ding",
            "Kanjian Zhang",
            "Tom Kocmi",
            "Dacheng Tao"
        ],
        "published": "2023-03-24T05:05:03Z",
        "summary": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated\nremarkable proficiency across several NLP tasks, such as machine translation,\ntext summarization. Recent research (Kocmi and Federmann, 2023) has shown that\nutilizing LLMs for assessing the quality of machine translation (MT) achieves\nstate-of-the-art performance at the system level but \\textit{performs poorly at\nthe segment level}. To further improve the performance of LLMs on MT quality\nassessment, we investigate several prompting designs, and propose a new\nprompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt)\nby combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et\nal., 2023). This technique emulates the commonly accepted human evaluation\nframework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and\n\\textit{produces explainable and reliable MT evaluations at both the system and\nsegment level}. Experimental Results from the WMT22 metrics shared task\nvalidate the effectiveness of EAPrompt on various LLMs, with different\nstructures. Further analysis confirms that EAPrompt effectively distinguishes\nmajor errors from minor ones, while also sharing a similar distribution of the\nnumber of errors with MQM. These findings highlight the potential of EAPrompt\nas a human-like evaluator prompting technique for MT evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2303.13809v3.pdf"
    },
    {
        "title": "Personalizing Task-oriented Dialog Systems via Zero-shot Generalizable Reward Function",
        "authors": [
            "A. B. Siddique",
            "M. H. Maqbool",
            "Kshitija Taywade",
            "Hassan Foroosh"
        ],
        "published": "2023-03-24T04:33:40Z",
        "summary": "Task-oriented dialog systems enable users to accomplish tasks using natural\nlanguage. State-of-the-art systems respond to users in the same way regardless\nof their personalities, although personalizing dialogues can lead to higher\nlevels of adoption and better user experiences. Building personalized dialog\nsystems is an important, yet challenging endeavor and only a handful of works\ntook on the challenge. Most existing works rely on supervised learning\napproaches and require laborious and expensive labeled training data for each\nuser profile. Additionally, collecting and labeling data for each user profile\nis virtually impossible. In this work, we propose a novel framework, P-ToD, to\npersonalize task-oriented dialog systems capable of adapting to a wide range of\nuser profiles in an unsupervised fashion using a zero-shot generalizable reward\nfunction. P-ToD uses a pre-trained GPT-2 as a backbone model and works in three\nphases. Phase one performs task-specific training. Phase two kicks off\nunsupervised personalization by leveraging the proximal policy optimization\nalgorithm that performs policy gradients guided by the zero-shot generalizable\nreward function. Our novel reward function can quantify the quality of the\ngenerated responses even for unseen profiles. The optional final phase\nfine-tunes the personalized model using a few labeled training examples. We\nconduct extensive experimental analysis using the personalized bAbI dialogue\nbenchmark for five tasks and up to 180 diverse user profiles. The experimental\nresults demonstrate that P-ToD, even when it had access to zero labeled\nexamples, outperforms state-of-the-art supervised personalization models and\nachieves competitive performance on BLEU and ROUGE metrics when compared to a\nstrong fully-supervised GPT-2 baseline",
        "pdf_link": "https://arxiv.org/pdf/2303.13797v1.pdf"
    },
    {
        "title": "Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching",
        "authors": [
            "Jiayi Yuan",
            "Ruixiang Tang",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "published": "2023-03-24T03:14:00Z",
        "summary": "The process of matching patients with suitable clinical trials is essential\nfor advancing medical research and providing optimal care. However, current\napproaches face challenges such as data standardization, ethical\nconsiderations, and a lack of interoperability between Electronic Health\nRecords (EHRs) and clinical trial criteria. In this paper, we explore the\npotential of large language models (LLMs) to address these challenges by\nleveraging their advanced natural language generation capabilities to improve\ncompatibility between EHRs and clinical trial descriptions. We propose an\ninnovative privacy-aware data augmentation approach for LLM-based patient-trial\nmatching (LLM-PTM), which balances the benefits of LLMs while ensuring the\nsecurity and confidentiality of sensitive patient data. Our experiments\ndemonstrate a 7.32% average improvement in performance using the proposed\nLLM-PTM method, and the generalizability to new data is improved by 12.12%.\nAdditionally, we present case studies to further illustrate the effectiveness\nof our approach and provide a deeper understanding of its underlying\nprinciples.",
        "pdf_link": "https://arxiv.org/pdf/2303.16756v2.pdf"
    },
    {
        "title": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages",
        "authors": [
            "Zheng-Xin Yong",
            "Ruochen Zhang",
            "Jessica Zosa Forde",
            "Skyler Wang",
            "Arjun Subramonian",
            "Holy Lovenia",
            "Samuel Cahyawijaya",
            "Genta Indra Winata",
            "Lintang Sutawika",
            "Jan Christian Blaise Cruz",
            "Yin Lin Tan",
            "Long Phan",
            "Rowena Garcia",
            "Thamar Solorio",
            "Alham Fikri Aji"
        ],
        "published": "2023-03-23T18:16:30Z",
        "summary": "While code-mixing is a common linguistic practice in many parts of the world,\ncollecting high-quality and low-cost code-mixed data remains a challenge for\nnatural language processing (NLP) research. The recent proliferation of Large\nLanguage Models (LLMs) compels one to ask: how capable are these systems in\ngenerating code-mixed data? In this paper, we explore prompting multilingual\nLLMs in a zero-shot manner to generate code-mixed data for seven languages in\nSouth East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,\nTamil, and Singlish. We find that publicly available multilingual\ninstruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of\nproducing texts with phrases or clauses from different languages. ChatGPT\nexhibits inconsistent capabilities in generating code-mixed texts, wherein its\nperformance varies depending on the prompt template and language pairing. For\ninstance, ChatGPT generates fluent and natural Singlish texts (an English-based\ncreole spoken in Singapore), but for English-Tamil language pair, the system\nmostly produces grammatically incorrect or semantically meaningless utterances.\nFurthermore, it may erroneously introduce languages not specified in the\nprompt. Based on our investigation, existing multilingual LLMs exhibit a wide\nrange of proficiency in code-mixed data generation for SEA languages. As such,\nwe advise against using LLMs in this context without extensive human checks.",
        "pdf_link": "https://arxiv.org/pdf/2303.13592v4.pdf"
    },
    {
        "title": "The Quantization Model of Neural Scaling",
        "authors": [
            "Eric J. Michaud",
            "Ziming Liu",
            "Uzay Girit",
            "Max Tegmark"
        ],
        "published": "2023-03-23T17:58:43Z",
        "summary": "We propose the Quantization Model of neural scaling laws, explaining both the\nobserved power law dropoff of loss with model and data size, and also the\nsudden emergence of new capabilities with scale. We derive this model from what\nwe call the Quantization Hypothesis, where network knowledge and skills are\n\"quantized\" into discrete chunks ($\\textbf{quanta}$). We show that when quanta\nare learned in order of decreasing use frequency, then a power law in use\nfrequencies explains observed power law scaling of loss. We validate this\nprediction on toy datasets, then study how scaling curves decompose for large\nlanguage models. Using language model gradients, we automatically decompose\nmodel behavior into a diverse set of skills (quanta). We tentatively find that\nthe frequency at which these quanta are used in the training distribution\nroughly follows a power law corresponding with the empirical scaling exponent\nfor language models, a prediction of our theory.",
        "pdf_link": "https://arxiv.org/pdf/2303.13506v3.pdf"
    },
    {
        "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
        "authors": [
            "Kalpesh Krishna",
            "Yixiao Song",
            "Marzena Karpinska",
            "John Wieting",
            "Mohit Iyyer"
        ],
        "published": "2023-03-23T16:29:27Z",
        "summary": "The rise in malicious usage of large language models, such as fake content\ncreation and academic plagiarism, has motivated the development of approaches\nthat identify AI-generated text, including those based on watermarking or\noutlier detection. However, the robustness of these detection algorithms to\nparaphrases of AI-generated text remains unclear. To stress test these\ndetectors, we build a 11B parameter paraphrase generation model (DIPPER) that\ncan paraphrase paragraphs, condition on surrounding context, and control\nlexical diversity and content reordering. Using DIPPER to paraphrase text\ngenerated by three large language models (including GPT3.5-davinci-003)\nsuccessfully evades several detectors, including watermarking, GPTZero,\nDetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection\naccuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of\n1%), without appreciably modifying the input semantics.\n  To increase the robustness of AI-generated text detection to paraphrase\nattacks, we introduce a simple defense that relies on retrieving\nsemantically-similar generations and must be maintained by a language model API\nprovider. Given a candidate text, our algorithm searches a database of\nsequences previously generated by the API, looking for sequences that match the\ncandidate text within a certain threshold. We empirically verify our defense\nusing a database of 15M generations from a fine-tuned T5-XXL model and find\nthat it can detect 80% to 97% of paraphrased generations across different\nsettings while only classifying 1% of human-written sequences as AI-generated.\nWe open-source our models, code and data.",
        "pdf_link": "https://arxiv.org/pdf/2303.13408v2.pdf"
    },
    {
        "title": "ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model",
        "authors": [
            "Hanyao Huang",
            "Ou Zheng",
            "Dongdong Wang",
            "Jiayi Yin",
            "Zijin Wang",
            "Shengxuan Ding",
            "Heng Yin",
            "Chuan Xu",
            "Renjie Yang",
            "Qian Zheng",
            "Bing Shi"
        ],
        "published": "2023-03-23T15:34:26Z",
        "summary": "The ChatGPT, a lite and conversational variant of Generative Pretrained\nTransformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large\nLanguage Models (LLMs) with billions of parameters. LLMs have stirred up much\ninterest among researchers and practitioners in their impressive skills in\nnatural language processing tasks, which profoundly impact various fields. This\npaper mainly discusses the future applications of LLMs in dentistry. We\nintroduce two primary LLM deployment methods in dentistry, including automated\ndental diagnosis and cross-modal dental diagnosis, and examine their potential\napplications. Especially, equipped with a cross-modal encoder, a single LLM can\nmanage multi-source data and conduct advanced natural language reasoning to\nperform complex clinical operations. We also present cases to demonstrate the\npotential of a fully automatic Multi-Modal LLM AI system for dentistry clinical\napplication. While LLMs offer significant potential benefits, the challenges,\nsuch as data privacy, data quality, and model bias, need further study.\nOverall, LLMs have the potential to revolutionize dental diagnosis and\ntreatment, which indicates a promising avenue for clinical application and\nresearch in dentistry.",
        "pdf_link": "https://arxiv.org/pdf/2304.03086v2.pdf"
    },
    {
        "title": "Increasing Textual Context Size Boosts Medical Image-Text Matching",
        "authors": [
            "Idan Glassberg",
            "Tom Hope"
        ],
        "published": "2023-03-23T15:20:05Z",
        "summary": "This short technical report demonstrates a simple technique that yields state\nof the art results in medical image-text matching tasks. We analyze the use of\nOpenAI's CLIP, a general image-text matching model, and observe that CLIP's\nlimited textual input size has negative impact on downstream performance in the\nmedical domain where encoding longer textual contexts is often required. We\nthus train and release ClipMD, which is trained with a simple sliding window\ntechnique to encode textual captions. ClipMD was tested on two medical\nimage-text datasets and compared with other image-text matching models. The\nresults show that ClipMD outperforms other models on both datasets by a large\nmargin. We make our code and pretrained model publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2303.13340v1.pdf"
    },
    {
        "title": "Fairness-guided Few-shot Prompting for Large Language Models",
        "authors": [
            "Huan Ma",
            "Changqing Zhang",
            "Yatao Bian",
            "Lemao Liu",
            "Zhirui Zhang",
            "Peilin Zhao",
            "Shu Zhang",
            "Huazhu Fu",
            "Qinghua Hu",
            "Bingzhe Wu"
        ],
        "published": "2023-03-23T12:28:25Z",
        "summary": "Large language models have demonstrated surprising ability to perform\nin-context learning, i.e., these models can be directly applied to solve\nnumerous downstream tasks by conditioning on a prompt constructed by a few\ninput-output examples. However, prior research has shown that in-context\nlearning can suffer from high instability due to variations in training\nexamples, example order, and prompt formats. Therefore, the construction of an\nappropriate prompt is essential for improving the performance of in-context\nlearning. In this paper, we revisit this problem from the view of predictive\nbias. Specifically, we introduce a metric to evaluate the predictive bias of a\nfixed prompt against labels or a given attributes. Then we empirically show\nthat prompts with higher bias always lead to unsatisfactory predictive quality.\nBased on this observation, we propose a novel search strategy based on the\ngreedy search to identify the near-optimal prompt for improving the performance\nof in-context learning. We perform comprehensive experiments with\nstate-of-the-art mainstream models such as GPT-3 on various downstream tasks.\nOur results indicate that our method can enhance the model's in-context\nlearning performance in an effective and interpretable manner.",
        "pdf_link": "https://arxiv.org/pdf/2303.13217v3.pdf"
    },
    {
        "title": "A Simple Explanation for the Phase Transition in Large Language Models with List Decoding",
        "authors": [
            "Cheng-Shang Chang"
        ],
        "published": "2023-03-23T09:00:07Z",
        "summary": "Various recent experimental results show that large language models (LLM)\nexhibit emergent abilities that are not present in small models. System\nperformance is greatly improved after passing a certain critical threshold of\nscale. In this letter, we provide a simple explanation for such a phase\ntransition phenomenon. For this, we model an LLM as a sequence-to-sequence\nrandom function. Instead of using instant generation at each step, we use a\nlist decoder that keeps a list of candidate sequences at each step and defers\nthe generation of the output sequence at the end. We show that there is a\ncritical threshold such that the expected number of erroneous candidate\nsequences remains bounded when an LLM is below the threshold, and it grows\nexponentially when an LLM is above the threshold. Such a threshold is related\nto the basic reproduction number in a contagious disease.",
        "pdf_link": "https://arxiv.org/pdf/2303.13112v1.pdf"
    },
    {
        "title": "SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization",
        "authors": [
            "Yu-Neng Chuang",
            "Ruixiang Tang",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "published": "2023-03-23T04:47:46Z",
        "summary": "Electronic health records (EHRs) store an extensive array of patient\ninformation, encompassing medical histories, diagnoses, treatments, and test\noutcomes. These records are crucial for enabling healthcare providers to make\nwell-informed decisions regarding patient care. Summarizing clinical notes\nfurther assists healthcare professionals in pinpointing potential health risks\nand making better-informed decisions. This process contributes to reducing\nerrors and enhancing patient outcomes by ensuring providers have access to the\nmost pertinent and current patient data. Recent research has shown that\nincorporating prompts with large language models (LLMs) substantially boosts\nthe efficacy of summarization tasks. However, we show that this approach also\nleads to increased output variance, resulting in notably divergent outputs even\nwhen prompts share similar meanings. To tackle this challenge, we introduce a\nmodel-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft\nprompts to diminish variance while preserving the advantages of prompt-based\nsummarization. Experimental findings on multiple clinical note tasks and LLMs\nindicate that our method not only bolsters performance but also effectively\ncurbs variance for various LLMs, providing a more uniform and dependable\nsolution for summarizing vital medical information.",
        "pdf_link": "https://arxiv.org/pdf/2303.13035v3.pdf"
    },
    {
        "title": "A Small-Scale Switch Transformer and NLP-based Model for Clinical Narratives Classification",
        "authors": [
            "Thanh-Dung Le",
            "Philippe Jouvet",
            "Rita Noumeir"
        ],
        "published": "2023-03-22T20:10:29Z",
        "summary": "In recent years, Transformer-based models such as the Switch Transformer have\nachieved remarkable results in natural language processing tasks. However,\nthese models are often too complex and require extensive pre-training, which\nlimits their effectiveness for small clinical text classification tasks with\nlimited data. In this study, we propose a simplified Switch Transformer\nframework and train it from scratch on a small French clinical text\nclassification dataset at CHU Sainte-Justine hospital. Our results demonstrate\nthat the simplified small-scale Transformer models outperform pre-trained\nBERT-based models, including DistillBERT, CamemBERT, FlauBERT, and FrALBERT.\nAdditionally, using a mixture of expert mechanisms from the Switch Transformer\nhelps capture diverse patterns; hence, the proposed approach achieves better\nresults than a conventional Transformer with the self-attention mechanism.\nFinally, our proposed framework achieves an accuracy of 87\\%, precision at\n87\\%, and recall at 85\\%, compared to the third-best pre-trained BERT-based\nmodel, FlauBERT, which achieved an accuracy of 84\\%, precision at 84\\%, and\nrecall at 84\\%. However, Switch Transformers have limitations, including a\ngeneralization gap and sharp minima. We compare it with a multi-layer\nperceptron neural network for small French clinical narratives classification\nand show that the latter outperforms all other models.",
        "pdf_link": "https://arxiv.org/pdf/2303.12892v1.pdf"
    },
    {
        "title": "Interpretable Bangla Sarcasm Detection using BERT and Explainable AI",
        "authors": [
            "Ramisa Anan",
            "Tasnim Sakib Apon",
            "Zeba Tahsin Hossain",
            "Elizabeth Antora Modhu",
            "Sudipta Mondal",
            "MD. Golam Rabiul Alam"
        ],
        "published": "2023-03-22T17:35:35Z",
        "summary": "A positive phrase or a sentence with an underlying negative motive is usually\ndefined as sarcasm that is widely used in today's social media platforms such\nas Facebook, Twitter, Reddit, etc. In recent times active users in social media\nplatforms are increasing dramatically which raises the need for an automated\nNLP-based system that can be utilized in various tasks such as determining\nmarket demand, sentiment analysis, threat detection, etc. However, since\nsarcasm usually implies the opposite meaning and its detection is frequently a\nchallenging issue, data meaning extraction through an NLP-based model becomes\nmore complicated. As a result, there has been a lot of study on sarcasm\ndetection in English over the past several years, and there's been a noticeable\nimprovement and yet sarcasm detection in the Bangla language's state remains\nthe same. In this article, we present a BERT-based system that can achieve\n99.60\\% while the utilized traditional machine learning algorithms are only\ncapable of achieving 89.93\\%. Additionally, we have employed Local\nInterpretable Model-Agnostic Explanations that introduce explainability to our\nsystem. Moreover, we have utilized a newly collected bangla sarcasm dataset,\nBanglaSarc that was constructed specifically for the evaluation of this study.\nThis dataset consists of fresh records of sarcastic and non-sarcastic comments,\nthe majority of which are acquired from Facebook and YouTube comment sections.",
        "pdf_link": "https://arxiv.org/pdf/2303.12772v1.pdf"
    },
    {
        "title": "Can we trust the evaluation on ChatGPT?",
        "authors": [
            "Rachith Aiyappa",
            "Jisun An",
            "Haewoon Kwak",
            "Yong-Yeol Ahn"
        ],
        "published": "2023-03-22T17:32:56Z",
        "summary": "ChatGPT, the first large language model (LLM) with mass adoption, has\ndemonstrated remarkable performance in numerous natural language tasks. Despite\nits evident usefulness, evaluating ChatGPT's performance in diverse problem\ndomains remains challenging due to the closed nature of the model and its\ncontinuous updates via Reinforcement Learning from Human Feedback (RLHF). We\nhighlight the issue of data contamination in ChatGPT evaluations, with a case\nstudy of the task of stance detection. We discuss the challenge of preventing\ndata contamination and ensuring fair model evaluation in the age of closed and\ncontinuously trained models.",
        "pdf_link": "https://arxiv.org/pdf/2303.12767v1.pdf"
    },
    {
        "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
        "authors": [
            "Sébastien Bubeck",
            "Varun Chandrasekaran",
            "Ronen Eldan",
            "Johannes Gehrke",
            "Eric Horvitz",
            "Ece Kamar",
            "Peter Lee",
            "Yin Tat Lee",
            "Yuanzhi Li",
            "Scott Lundberg",
            "Harsha Nori",
            "Hamid Palangi",
            "Marco Tulio Ribeiro",
            "Yi Zhang"
        ],
        "published": "2023-03-22T16:51:28Z",
        "summary": "Artificial intelligence (AI) researchers have been developing and refining\nlarge language models (LLMs) that exhibit remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. The latest model developed by OpenAI, GPT-4, was trained using an\nunprecedented scale of compute and data. In this paper, we report on our\ninvestigation of an early version of GPT-4, when it was still in active\ndevelopment by OpenAI. We contend that (this early version of) GPT-4 is part of\na new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that\nexhibit more general intelligence than previous AI models. We discuss the\nrising capabilities and implications of these models. We demonstrate that,\nbeyond its mastery of language, GPT-4 can solve novel and difficult tasks that\nspan mathematics, coding, vision, medicine, law, psychology and more, without\nneeding any special prompting. Moreover, in all of these tasks, GPT-4's\nperformance is strikingly close to human-level performance, and often vastly\nsurpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's\ncapabilities, we believe that it could reasonably be viewed as an early (yet\nstill incomplete) version of an artificial general intelligence (AGI) system.\nIn our exploration of GPT-4, we put special emphasis on discovering its\nlimitations, and we discuss the challenges ahead for advancing towards deeper\nand more comprehensive versions of AGI, including the possible need for\npursuing a new paradigm that moves beyond next-word prediction. We conclude\nwith reflections on societal influences of the recent technological leap and\nfuture research directions.",
        "pdf_link": "https://arxiv.org/pdf/2303.12712v5.pdf"
    },
    {
        "title": "Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study",
        "authors": [
            "Sonish Sivarajkumar",
            "Fengyi Gao",
            "Parker E. Denny",
            "Bayan M. Aldhahwani",
            "Shyam Visweswaran",
            "Allyn Bove",
            "Yanshan Wang"
        ],
        "published": "2023-03-22T13:46:16Z",
        "summary": "Post-stroke patient rehabilitation requires precise, personalized treatment\nplans. Natural Language Processing (NLP) offers potential to extract valuable\nexercise information from clinical notes, aiding in the development of more\neffective rehabilitation strategies. Objective: This study aims to develop and\nevaluate a variety of NLP algorithms to extract and categorize physical\nrehabilitation exercise information from the clinical notes of post-stroke\npatients treated at the University of Pittsburgh Medical Center. A cohort of\n13,605 patients diagnosed with stroke was identified, and their clinical notes\ncontaining rehabilitation therapy notes were retrieved. A comprehensive\nclinical ontology was created to represent various aspects of physical\nrehabilitation exercises. State-of-the-art NLP algorithms were then developed\nand compared, including rule-based, machine learning-based algorithms, and\nlarge language model (LLM)-based algorithms (ChatGPT). Analysis was conducted\non a dataset comprising 23,724 notes with detailed demographic and clinical\ncharacteristics. The rule-based NLP algorithm demonstrated superior performance\nin most areas, particularly in detecting the 'Right Side' location with an F1\nscore of 0.975, outperforming Gradient Boosting by 0.063. Gradient Boosting\nexcelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing\nrule-based NLP by 0.023. It also showed notable performance in 'Passive Range\nof Motion' with an F1 score of 0.970, a 0.032 improvement over rule-based NLP.\nThe rule-based algorithm efficiently handled 'Duration', 'Sets', and 'Reps'\nwith F1 scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot\nprompts, achieved high recall but generally lower precision and F1 scores.\nHowever, it notably excelled in 'Backward Plane' motion detection, achieving an\nF1 score of 0.846, surpassing the rule-based algorithm's 0.720.",
        "pdf_link": "https://arxiv.org/pdf/2303.13466v2.pdf"
    },
    {
        "title": "MEGA: Multilingual Evaluation of Generative AI",
        "authors": [
            "Kabir Ahuja",
            "Harshita Diddee",
            "Rishav Hada",
            "Millicent Ochieng",
            "Krithika Ramesh",
            "Prachi Jain",
            "Akshay Nambi",
            "Tanuja Ganu",
            "Sameer Segal",
            "Maxamed Axmed",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "published": "2023-03-22T13:03:10Z",
        "summary": "Generative AI models have shown impressive performance on many Natural\nLanguage Processing tasks such as language understanding, reasoning, and\nlanguage generation. An important question being asked by the AI community\ntoday is about the capabilities and limits of these models, and it is clear\nthat evaluating generative AI is very challenging. Most studies on generative\nLLMs have been restricted to English and it is unclear how capable these models\nare at understanding and generating text in other languages. We present the\nfirst comprehensive benchmarking of generative LLMs - MEGA, which evaluates\nmodels on standard NLP benchmarks, covering 16 NLP datasets across 70\ntypologically diverse languages. We compare the performance of generative LLMs\nincluding Chat-GPT and GPT-4 to State of the Art (SOTA) non-autoregressive\nmodels on these tasks to determine how well generative models perform compared\nto the previous generation of LLMs. We present a thorough analysis of the\nperformance of models across languages and tasks and discuss challenges in\nimproving the performance of generative LLMs on low-resource languages. We\ncreate a framework for evaluating generative LLMs in the multilingual setting\nand provide directions for future progress in the field.",
        "pdf_link": "https://arxiv.org/pdf/2303.12528v4.pdf"
    },
    {
        "title": "MEDIMP: 3D Medical Images with clinical Prompts from limited tabular data for renal transplantation",
        "authors": [
            "Leo Milecki",
            "Vicky Kalogeiton",
            "Sylvain Bodard",
            "Dany Anglicheau",
            "Jean-Michel Correas",
            "Marc-Olivier Timsit",
            "Maria Vakalopoulou"
        ],
        "published": "2023-03-22T10:30:43Z",
        "summary": "Renal transplantation emerges as the most effective solution for end-stage\nrenal disease. Occurring from complex causes, a substantial risk of transplant\nchronic dysfunction persists and may lead to graft loss. Medical imaging plays\na substantial role in renal transplant monitoring in clinical practice.\nHowever, graft supervision is multi-disciplinary, notably joining nephrology,\nurology, and radiology, while identifying robust biomarkers from such\nhigh-dimensional and complex data for prognosis is challenging. In this work,\ntaking inspiration from the recent success of Large Language Models (LLMs), we\npropose MEDIMP -- Medical Images with clinical Prompts -- a model to learn\nmeaningful multi-modal representations of renal transplant Dynamic\nContrast-Enhanced Magnetic Resonance Imaging (DCE MRI) by incorporating\nstructural clinicobiological data after translating them into text prompts.\nMEDIMP is based on contrastive learning from joint text-image paired embeddings\nto perform this challenging task. Moreover, we propose a framework that\ngenerates medical prompts using automatic textual data augmentations from LLMs.\nOur goal is to learn meaningful manifolds of renal transplant DCE MRI,\ninteresting for the prognosis of the transplant or patient status (2, 3, and 4\nyears after the transplant), fully exploiting the limited available multi-modal\ndata most efficiently. Extensive experiments and comparisons with other renal\ntransplant representation learning methods with limited data prove the\neffectiveness of MEDIMP in a relevant clinical setting, giving new directions\ntoward medical prompts. Our code is available at\nhttps://github.com/leomlck/MEDIMP.",
        "pdf_link": "https://arxiv.org/pdf/2303.12445v2.pdf"
    },
    {
        "title": "Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense",
        "authors": [
            "Andrei Kucharavy",
            "Zachary Schillaci",
            "Loïc Maréchal",
            "Maxime Würsch",
            "Ljiljana Dolamic",
            "Remi Sabonnadiere",
            "Dimitri Percia David",
            "Alain Mermoud",
            "Vincent Lenders"
        ],
        "published": "2023-03-21T18:45:09Z",
        "summary": "Generative Language Models gained significant attention in late 2022 / early\n2023, notably with the introduction of models refined to act consistently with\nusers' expectations of interactions with AI (conversational models). Arguably\nthe focal point of public attention has been such a refinement of the GPT3\nmodel -- the ChatGPT and its subsequent integration with auxiliary\ncapabilities, including search as part of Microsoft Bing. Despite extensive\nprior research invested in their development, their performance and\napplicability to a range of daily tasks remained unclear and niche. However,\ntheir wider utilization without a requirement for technical expertise, made in\nlarge part possible through conversational fine-tuning, revealed the extent of\ntheir true capabilities in a real-world environment. This has garnered both\npublic excitement for their potential applications and concerns about their\ncapabilities and potential malicious uses. This review aims to provide a brief\noverview of the history, state of the art, and implications of Generative\nLanguage Models in terms of their principles, abilities, limitations, and\nfuture prospects -- especially in the context of cyber-defense, with a focus on\nthe Swiss operational environment.",
        "pdf_link": "https://arxiv.org/pdf/2303.12132v1.pdf"
    },
    {
        "title": "Large Language Models Can Be Used to Estimate the Latent Positions of Politicians",
        "authors": [
            "Patrick Y. Wu",
            "Jonathan Nagler",
            "Joshua A. Tucker",
            "Solomon Messing"
        ],
        "published": "2023-03-21T17:48:00Z",
        "summary": "Existing approaches to estimating politicians' latent positions along\nspecific dimensions often fail when relevant data is limited. We leverage the\nembedded knowledge in generative large language models (LLMs) to address this\nchallenge and measure lawmakers' positions along specific political or policy\ndimensions. We prompt an instruction/dialogue-tuned LLM to pairwise compare\nlawmakers and then scale the resulting graph using the Bradley-Terry model. We\nestimate novel measures of U.S. senators' positions on liberal-conservative\nideology, gun control, and abortion. Our liberal-conservative scale, used to\nvalidate LLM-driven scaling, strongly correlates with existing measures and\noffsets interpretive gaps, suggesting LLMs synthesize relevant data from\ninternet and digitized media rather than memorizing existing measures. Our gun\ncontrol and abortion measures -- the first of their kind -- differ from the\nliberal-conservative scale in face-valid ways and predict interest group\nratings and legislator votes better than ideology alone. Our findings suggest\nLLMs hold promise for solving complex social science measurement problems.",
        "pdf_link": "https://arxiv.org/pdf/2303.12057v4.pdf"
    },
    {
        "title": "cTBLS: Augmenting Large Language Models with Conversational Tables",
        "authors": [
            "Anirudh S Sundar",
            "Larry Heck"
        ],
        "published": "2023-03-21T17:04:44Z",
        "summary": "Optimizing accuracy and performance while eliminating hallucinations of\nopen-domain conversational large language models (LLMs) is an open research\nchallenge. A particularly promising direction is to augment and ground LLMs\nwith information from structured sources. This paper introduces Conversational\nTables (cTBLS), a three-step architecture to retrieve and generate dialogue\nresponses grounded on retrieved tabular information. cTBLS uses Transformer\nencoder embeddings for Dense Table Retrieval and obtains up to 125% relative\nimprovement over the retriever in the previous state-of-the-art system on the\nHyrbiDialogue dataset. cTBLS then uses a shared process between encoder and\ndecoder models to perform a coarse+fine tabular knowledge (e.g., cell) ranking\ncombined with a GPT-3.5 LLM response generator to yield a 2x relative\nimprovement in ROUGE scores. Finally, human evaluators prefer cTBLs +80% of the\ntime (coherency, fluency) and judge informativeness to be 4x better than the\nprevious state-of-the-art.",
        "pdf_link": "https://arxiv.org/pdf/2303.12024v3.pdf"
    },
    {
        "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
        "authors": [
            "Zonglin Yang",
            "Xinya Du",
            "Rui Mao",
            "Jinjie Ni",
            "Erik Cambria"
        ],
        "published": "2023-03-21T16:56:05Z",
        "summary": "Logical reasoning is central to human cognition and intelligence. It includes\ndeductive, inductive, and abductive reasoning. Past research of logical\nreasoning within AI uses formal language as knowledge representation and\nsymbolic reasoners. However, reasoning with formal language has proved\nchallenging (e.g., brittleness and knowledge-acquisition bottleneck). This\npaper provides a comprehensive overview on a new paradigm of logical reasoning,\nwhich uses natural language as knowledge representation and pretrained language\nmodels as reasoners, including philosophical definition and categorization of\nlogical reasoning, advantages of the new paradigm, benchmarks and methods,\nchallenges of the new paradigm, possible future directions, and relation to\nrelated NLP fields. This new paradigm is promising since it not only alleviates\nmany challenges of formal representation but also has advantages over\nend-to-end neural methods. This survey focus on transformer-based LLMs\nexplicitly working on deductive, inductive, and abductive reasoning over\nEnglish representation.",
        "pdf_link": "https://arxiv.org/pdf/2303.12023v2.pdf"
    },
    {
        "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
        "authors": [
            "Yushi Hu",
            "Benlin Liu",
            "Jungo Kasai",
            "Yizhong Wang",
            "Mari Ostendorf",
            "Ranjay Krishna",
            "Noah A Smith"
        ],
        "published": "2023-03-21T14:41:02Z",
        "summary": "Despite thousands of researchers, engineers, and artists actively working on\nimproving text-to-image generation models, systems often fail to produce images\nthat accurately align with the text inputs. We introduce TIFA (Text-to-Image\nFaithfulness evaluation with question Answering), an automatic evaluation\nmetric that measures the faithfulness of a generated image to its text input\nvia visual question answering (VQA). Specifically, given a text input, we\nautomatically generate several question-answer pairs using a language model. We\ncalculate image faithfulness by checking whether existing VQA models can answer\nthese questions using the generated image. TIFA is a reference-free metric that\nallows for fine-grained and interpretable evaluations of generated images. TIFA\nalso has better correlations with human judgments than existing metrics. Based\non this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse\ntext inputs and 25K questions across 12 categories (object, counting, etc.). We\npresent a comprehensive evaluation of existing text-to-image models using TIFA\nv1.0 and highlight the limitations and challenges of current models. For\ninstance, we find that current text-to-image models, despite doing well on\ncolor and material, still struggle in counting, spatial relations, and\ncomposing multiple objects. We hope our benchmark will help carefully measure\nthe research progress in text-to-image synthesis and provide valuable insights\nfor further research.",
        "pdf_link": "https://arxiv.org/pdf/2303.11897v3.pdf"
    },
    {
        "title": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written Research Papers and the Ethics of the Large Language Models in Scholarly Publishing",
        "authors": [
            "Brady Lund",
            "Ting Wang",
            "Nishith Reddy Mannuru",
            "Bing Nie",
            "Somipam Shimray",
            "Ziang Wang"
        ],
        "published": "2023-03-21T14:35:07Z",
        "summary": "This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.",
        "pdf_link": "https://arxiv.org/pdf/2303.13367v2.pdf"
    },
    {
        "title": "ChatGPT for Programming Numerical Methods",
        "authors": [
            "Ali Kashefi",
            "Tapan Mukerji"
        ],
        "published": "2023-03-21T12:18:17Z",
        "summary": "ChatGPT is a large language model recently released by the OpenAI company. In\nthis technical report, we explore for the first time the capability of ChatGPT\nfor programming numerical algorithms. Specifically, we examine the capability\nof GhatGPT for generating codes for numerical algorithms in different\nprogramming languages, for debugging and improving written codes by users, for\ncompleting missed parts of numerical codes, rewriting available codes in other\nprogramming languages, and for parallelizing serial codes. Additionally, we\nassess if ChatGPT can recognize if given codes are written by humans or\nmachines. To reach this goal, we consider a variety of mathematical problems\nsuch as the Poisson equation, the diffusion equation, the incompressible\nNavier-Stokes equations, compressible inviscid flow, eigenvalue problems,\nsolving linear systems of equations, storing sparse matrices, etc. Furthermore,\nwe exemplify scientific machine learning such as physics-informed neural\nnetworks and convolutional neural networks with applications to computational\nphysics. Through these examples, we investigate the successes, failures, and\nchallenges of ChatGPT. Examples of failures are producing singular matrices,\noperations on arrays with incompatible sizes, programming interruption for\nrelatively long codes, etc. Our outcomes suggest that ChatGPT can successfully\nprogram numerical algorithms in different programming languages, but certain\nlimitations and challenges exist that require further improvement of this\nmachine learning model.",
        "pdf_link": "https://arxiv.org/pdf/2303.12093v3.pdf"
    },
    {
        "title": "Implicit Neural Representation for Cooperative Low-light Image Enhancement",
        "authors": [
            "Shuzhou Yang",
            "Moxuan Ding",
            "Yanmin Wu",
            "Zihan Li",
            "Jian Zhang"
        ],
        "published": "2023-03-21T10:24:29Z",
        "summary": "The following three factors restrict the application of existing low-light\nimage enhancement methods: unpredictable brightness degradation and noise,\ninherent gap between metric-favorable and visual-friendly versions, and the\nlimited paired training data. To address these limitations, we propose an\nimplicit Neural Representation method for Cooperative low-light image\nenhancement, dubbed NeRCo. It robustly recovers perceptual-friendly results in\nan unsupervised manner. Concretely, NeRCo unifies the diverse degradation\nfactors of real-world scenes with a controllable fitting function, leading to\nbetter robustness. In addition, for the output results, we introduce\nsemantic-orientated supervision with priors from the pre-trained\nvision-language model. Instead of merely following reference images, it\nencourages results to meet subjective expectations, finding more\nvisual-friendly solutions. Further, to ease the reliance on paired data and\nreduce solution space, we develop a dual-closed-loop constrained enhancement\nmodule. It is trained cooperatively with other affiliated modules in a\nself-supervised manner. Finally, extensive experiments demonstrate the\nrobustness and superior effectiveness of our proposed NeRCo. Our code is\navailable at https://github.com/Ysz2022/NeRCo.",
        "pdf_link": "https://arxiv.org/pdf/2303.11722v3.pdf"
    },
    {
        "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
        "authors": [
            "Chaoning Zhang",
            "Chenshuang Zhang",
            "Sheng Zheng",
            "Yu Qiao",
            "Chenghao Li",
            "Mengchun Zhang",
            "Sumit Kumar Dam",
            "Chu Myaet Thwal",
            "Ye Lin Tun",
            "Le Luang Huy",
            "Donguk kim",
            "Sung-Ho Bae",
            "Lik-Hang Lee",
            "Yang Yang",
            "Heng Tao Shen",
            "In So Kweon",
            "Choong Seon Hong"
        ],
        "published": "2023-03-21T10:09:47Z",
        "summary": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.",
        "pdf_link": "https://arxiv.org/pdf/2303.11717v1.pdf"
    },
    {
        "title": "The Open-domain Paradox for Chatbots: Common Ground as the Basis for Human-like Dialogue",
        "authors": [
            "Gabriel Skantze",
            "A. Seza Doğruöz"
        ],
        "published": "2023-03-21T10:01:49Z",
        "summary": "There is a surge in interest in the development of open-domain chatbots,\ndriven by the recent advancements of large language models. The \"openness\" of\nthe dialogue is expected to be maximized by providing minimal information to\nthe users about the common ground they can expect, including the presumed joint\nactivity. However, evidence suggests that the effect is the opposite. Asking\nusers to \"just chat about anything\" results in a very narrow form of dialogue,\nwhich we refer to as the \"open-domain paradox\". In this position paper, we\nexplain this paradox through the theory of common ground as the basis for\nhuman-like communication. Furthermore, we question the assumptions behind\nopen-domain chatbots and identify paths forward for enabling common ground in\nhuman-computer dialogue.",
        "pdf_link": "https://arxiv.org/pdf/2303.11708v2.pdf"
    },
    {
        "title": "Language Model Behavior: A Comprehensive Survey",
        "authors": [
            "Tyler A. Chang",
            "Benjamin K. Bergen"
        ],
        "published": "2023-03-20T23:54:26Z",
        "summary": "Transformer language models have received widespread public attention, yet\ntheir generated text is often surprising even to NLP researchers. In this\nsurvey, we discuss over 250 recent studies of English language model behavior\nbefore task-specific fine-tuning. Language models possess basic capabilities in\nsyntax, semantics, pragmatics, world knowledge, and reasoning, but these\ncapabilities are sensitive to specific inputs and surface features. Despite\ndramatic increases in generated text quality as models scale to hundreds of\nbillions of parameters, the models are still prone to unfactual responses,\ncommonsense errors, memorized text, and social biases. Many of these weaknesses\ncan be framed as over-generalizations or under-generalizations of learned\npatterns in text. We synthesize recent results to highlight what is currently\nknown about large language model capabilities, thus providing a resource for\napplied work and for research in adjacent fields that use language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.11504v2.pdf"
    },
    {
        "title": "Large Language Models and Simple, Stupid Bugs",
        "authors": [
            "Kevin Jesse",
            "Toufique Ahmed",
            "Premkumar T. Devanbu",
            "Emily Morgan"
        ],
        "published": "2023-03-20T21:14:06Z",
        "summary": "With the advent of powerful neural language models, AI-based systems to\nassist developers in coding tasks are becoming widely available; Copilot is one\nsuch system. Copilot uses Codex, a large language model (LLM), to complete code\nconditioned on a preceding \"prompt\". Codex, however, is trained on public\nGitHub repositories, viz., on code that may include bugs and vulnerabilities.\nPrevious studies [1], [2] show Codex reproduces vulnerabilities seen in\ntraining. In this study, we examine how prone Codex is to generate an\ninteresting bug category, single statement bugs, commonly referred to as\nsimple, stupid bugs or SStuBs in the MSR community. We find that Codex and\nsimilar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs\nas much as 2x as likely than known, verbatim correct code. We explore the\nconsequences of the Codex generated SStuBs and propose avoidance strategies\nthat suggest the possibility of reducing the production of known, verbatim\nSStubs, and increase the possibility of producing known, verbatim fixes.",
        "pdf_link": "https://arxiv.org/pdf/2303.11455v1.pdf"
    },
    {
        "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
        "authors": [
            "Noah Shinn",
            "Federico Cassano",
            "Edward Berman",
            "Ashwin Gopinath",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "published": "2023-03-20T18:08:50Z",
        "summary": "Large language models (LLMs) have been increasingly used to interact with\nexternal environments (e.g., games, compilers, APIs) as goal-driven agents.\nHowever, it remains challenging for these language agents to quickly and\nefficiently learn from trial-and-error as traditional reinforcement learning\nmethods require extensive training samples and expensive model fine-tuning. We\npropose Reflexion, a novel framework to reinforce language agents not by\nupdating weights, but instead through linguistic feedback. Concretely,\nReflexion agents verbally reflect on task feedback signals, then maintain their\nown reflective text in an episodic memory buffer to induce better\ndecision-making in subsequent trials. Reflexion is flexible enough to\nincorporate various types (scalar values or free-form language) and sources\n(external or internally simulated) of feedback signals, and obtains significant\nimprovements over a baseline agent across diverse tasks (sequential\ndecision-making, coding, language reasoning). For example, Reflexion achieves a\n91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous\nstate-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis\nstudies using different feedback signals, feedback incorporation methods, and\nagent types, and provide insights into how they affect performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.11366v4.pdf"
    },
    {
        "title": "Context-faithful Prompting for Large Language Models",
        "authors": [
            "Wenxuan Zhou",
            "Sheng Zhang",
            "Hoifung Poon",
            "Muhao Chen"
        ],
        "published": "2023-03-20T17:54:58Z",
        "summary": "Large language models (LLMs) encode parametric knowledge about world facts\nand have shown remarkable performance in knowledge-driven NLP tasks. However,\ntheir reliance on parametric knowledge may cause them to overlook contextual\ncues, leading to incorrect predictions in context-sensitive NLP tasks (e.g.,\nknowledge acquisition tasks). In this paper, we seek to assess and enhance\nLLMs' contextual faithfulness in two aspects: knowledge conflict and prediction\nwith abstention. We demonstrate that LLMs' faithfulness can be significantly\nimproved using carefully designed prompting strategies. In particular, we\nidentify opinion-based prompts and counterfactual demonstrations as the most\neffective methods. Opinion-based prompts reframe the context as a narrator's\nstatement and inquire about the narrator's opinions, while counterfactual\ndemonstrations use instances containing false facts to improve faithfulness in\nknowledge conflict situations. Neither technique requires additional training.\nWe conduct experiments on three datasets of two standard NLP tasks, machine\nreading comprehension and relation extraction, and the results demonstrate\nsignificant improvement in faithfulness to contexts. Code and data are released\nat https://github.com/wzhouad/context-faithful-llm.",
        "pdf_link": "https://arxiv.org/pdf/2303.11315v2.pdf"
    },
    {
        "title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4",
        "authors": [
            "Zhengliang Liu",
            "Yue Huang",
            "Xiaowei Yu",
            "Lu Zhang",
            "Zihao Wu",
            "Chao Cao",
            "Haixing Dai",
            "Lin Zhao",
            "Yiwei Li",
            "Peng Shu",
            "Fang Zeng",
            "Lichao Sun",
            "Wei Liu",
            "Dinggang Shen",
            "Quanzheng Li",
            "Tianming Liu",
            "Dajiang Zhu",
            "Xiang Li"
        ],
        "published": "2023-03-20T11:34:37Z",
        "summary": "The digitization of healthcare has facilitated the sharing and re-using of\nmedical data but has also raised concerns about confidentiality and privacy.\nHIPAA (Health Insurance Portability and Accountability Act) mandates removing\nre-identifying information before the dissemination of medical records. Thus,\neffective and efficient solutions for de-identifying medical data, especially\nthose in free-text forms, are highly needed. While various computer-assisted\nde-identification methods, including both rule-based and learning-based, have\nbeen developed and used in prior practice, such solutions still lack\ngeneralizability or need to be fine-tuned according to different scenarios,\nsignificantly imposing restrictions in wider use. The advancement of large\nlanguage models (LLM), such as ChatGPT and GPT-4, have shown great potential in\nprocessing text data in the medical domain with zero-shot in-context learning,\nespecially in the task of privacy protection, as these models can identify\nconfidential information by their powerful named entity recognition (NER)\ncapability. In this work, we developed a novel GPT4-enabled de-identification\nframework (``DeID-GPT\") to automatically identify and remove the identifying\ninformation. Compared to existing commonly used medical text data\nde-identification methods, our developed DeID-GPT showed the highest accuracy\nand remarkable reliability in masking private information from the unstructured\nmedical text while preserving the original structure and meaning of the text.\nThis study is one of the earliest to utilize ChatGPT and GPT-4 for medical text\ndata processing and de-identification, which provides insights for further\nresearch and solution development on the use of LLMs such as ChatGPT/GPT-4 in\nhealthcare. Codes and benchmarking data information are available at\nhttps://github.com/yhydhx/ChatGPT-API.",
        "pdf_link": "https://arxiv.org/pdf/2303.11032v2.pdf"
    },
    {
        "title": "Retrieving Multimodal Information for Augmented Generation: A Survey",
        "authors": [
            "Ruochen Zhao",
            "Hailin Chen",
            "Weishi Wang",
            "Fangkai Jiao",
            "Xuan Long Do",
            "Chengwei Qin",
            "Bosheng Ding",
            "Xiaobao Guo",
            "Minzhi Li",
            "Xingxuan Li",
            "Shafiq Joty"
        ],
        "published": "2023-03-20T05:07:41Z",
        "summary": "As Large Language Models (LLMs) become popular, there emerged an important\ntrend of using multimodality to augment the LLMs' generation ability, which\nenables LLMs to better interact with the world. However, there lacks a unified\nperception of at which stage and how to incorporate different modalities. In\nthis survey, we review methods that assist and augment generative models by\nretrieving multimodal knowledge, whose formats range from images, codes,\ntables, graphs, to audio. Such methods offer a promising solution to important\nconcerns such as factuality, reasoning, interpretability, and robustness. By\nproviding an in-depth review, this survey is expected to provide scholars with\na deeper understanding of the methods' applications and encourage them to adapt\nexisting techniques to the fast-growing field of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.10868v3.pdf"
    },
    {
        "title": "Dynamic Documentation for AI Systems",
        "authors": [
            "Soham Mehta",
            "Anderson Rogers",
            "Thomas Krendl Gilbert"
        ],
        "published": "2023-03-20T04:23:07Z",
        "summary": "AI documentation is a rapidly-growing channel for coordinating the design of\nAI technologies with policies for transparency and accessibility. Calls to\nstandardize and enact documentation of algorithmic harms and impacts are now\ncommonplace. However, documentation standards for AI remain inchoate, and fail\nto match the capabilities and social effects of increasingly impactful\narchitectures such as Large Language Models (LLMs). In this paper, we show the\nlimits of present documentation protocols, and argue for dynamic documentation\nas a new paradigm for understanding and evaluating AI systems. We first review\ncanonical approaches to system documentation outside the context of AI,\nfocusing on the complex history of Environmental Impact Statements (EISs). We\nnext compare critical elements of the EIS framework to present challenges with\nalgorithmic documentation, which have inherited the limitations of EISs without\nincorporating their strengths. These challenges are specifically illustrated\nthrough the growing popularity of Model Cards and two case studies of\nalgorithmic impact assessment in China and Canada. Finally, we evaluate more\nrecent proposals, including Reward Reports, as potential components of fully\ndynamic AI documentation protocols.",
        "pdf_link": "https://arxiv.org/pdf/2303.10854v1.pdf"
    },
    {
        "title": "The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery",
        "authors": [
            "David Noever",
            "Samantha Elizabeth Miller Noever"
        ],
        "published": "2023-03-20T01:57:52Z",
        "summary": "The AI community has embraced multi-sensory or multi-modal approaches to\nadvance this generation of AI models to resemble expected intelligent\nunderstanding. Combining language and imagery represents a familiar method for\nspecific tasks like image captioning or generation from descriptions. This\npaper compares these monolithic approaches to a lightweight and specialized\nmethod based on employing image models to label objects, then serially\nsubmitting this resulting object list to a large language model (LLM). This use\nof multiple Application Programming Interfaces (APIs) enables better than 95%\nmean average precision for correct object lists, which serve as input to the\nlatest Open AI text generator (GPT-4). To demonstrate the API as a modular\nalternative, we solve the problem of a user taking a picture of ingredients\navailable in a refrigerator, then generating novel recipe cards tailored to\ncomplex constraints on cost, preparation time, dietary restrictions, portion\nsizes, and multiple meal plans. The research concludes that monolithic\nmultimodal models currently lack the coherent memory to maintain context and\nformat for this task and that until recently, the language models like GPT-2/3\nstruggled to format similar problems without degenerating into repetitive or\nnon-sensical combinations of ingredients. For the first time, an AI chef or\ncook seems not only possible but offers some enhanced capabilities to augment\nhuman recipe libraries in pragmatic ways. The work generates a 100-page recipe\nbook featuring the thirty top ingredients using over 2000 refrigerator images\nas initializing lists.",
        "pdf_link": "https://arxiv.org/pdf/2304.02016v1.pdf"
    },
    {
        "title": "Bangla Grammatical Error Detection Using T5 Transformer Model",
        "authors": [
            "H. A. Z. Sameen Shahgir",
            "Khondker Salman Sayeed"
        ],
        "published": "2023-03-19T09:24:48Z",
        "summary": "This paper presents a method for detecting grammatical errors in Bangla using\na Text-to-Text Transfer Transformer (T5) Language Model, using the small\nvariant of BanglaT5, fine-tuned on a corpus of 9385 sentences where errors were\nbracketed by the dedicated demarcation symbol. The T5 model was primarily\ndesigned for translation and is not specifically designed for this task, so\nextensive post-processing was necessary to adapt it to the task of error\ndetection. Our experiments show that the T5 model can achieve low Levenshtein\nDistance in detecting grammatical errors in Bangla, but post-processing is\nessential to achieve optimal performance. The final average Levenshtein\nDistance after post-processing the output of the fine-tuned model was 1.0394 on\na test set of 5000 sentences. This paper also presents a detailed analysis of\nthe errors detected by the model and discusses the challenges of adapting a\ntranslation model for grammar. Our approach can be extended to other languages,\ndemonstrating the potential of T5 models for detecting grammatical errors in a\nwide range of languages.",
        "pdf_link": "https://arxiv.org/pdf/2303.10612v1.pdf"
    },
    {
        "title": "Revisiting the Plastic Surgery Hypothesis via Large Language Models",
        "authors": [
            "Chunqiu Steven Xia",
            "Yifeng Ding",
            "Lingming Zhang"
        ],
        "published": "2023-03-18T20:33:46Z",
        "summary": "Automated Program Repair (APR) aspires to automatically generate patches for\nan input buggy program. Traditional APR tools typically focus on specific bug\ntypes and fixes through the use of templates, heuristics, and formal\nspecifications. However, these techniques are limited in terms of the bug types\nand patch variety they can produce. As such, researchers have designed various\nlearning-based APR tools with recent work focused on directly using Large\nLanguage Models (LLMs) for APR. While LLM-based APR tools are able to achieve\nstate-of-the-art performance on many repair datasets, the LLMs used for direct\nrepair are not fully aware of the project-specific information such as unique\nvariable or method names.\n  The plastic surgery hypothesis is a well-known insight for APR, which states\nthat the code ingredients to fix the bug usually already exist within the same\nproject. Traditional APR tools have largely leveraged the plastic surgery\nhypothesis by designing manual or heuristic-based approaches to exploit such\nexisting code ingredients. However, as recent APR research starts focusing on\nLLM-based approaches, the plastic surgery hypothesis has been largely ignored.\nIn this paper, we ask the following question: How useful is the plastic surgery\nhypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique\nopportunity to fully automate the plastic surgery hypothesis via fine-tuning\nand prompting. To this end, we propose FitRepair, which combines the direct\nusage of LLMs with two domain-specific fine-tuning strategies and one prompting\nstrategy for more powerful APR. Our experiments on the widely studied Defects4j\n1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially\noutperforming the best-performing baseline by 15 and 8), respectively,\ndemonstrating a promising future of the plastic surgery hypothesis in the era\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.10494v1.pdf"
    },
    {
        "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models",
        "authors": [
            "Vithursan Thangarasa",
            "Abhay Gupta",
            "William Marshall",
            "Tianda Li",
            "Kevin Leong",
            "Dennis DeCoste",
            "Sean Lie",
            "Shreyas Saxena"
        ],
        "published": "2023-03-18T17:56:01Z",
        "summary": "The pre-training and fine-tuning paradigm has contributed to a number of\nbreakthroughs in Natural Language Processing (NLP). Instead of directly\ntraining on a downstream task, language models are first pre-trained on large\ndatasets with cross-domain knowledge (e.g., Pile, MassiveText, etc.) and then\nfine-tuned on task-specific data (e.g., natural language generation, text\nsummarization, etc.). Scaling the model and dataset size has helped improve the\nperformance of LLMs, but unfortunately, this also lead to highly prohibitive\ncomputational costs. Pre-training LLMs often require orders of magnitude more\nFLOPs than fine-tuning and the model capacity often remains the same between\nthe two phases. To achieve training efficiency w.r.t training FLOPs, we propose\nto decouple the model capacity between the two phases and introduce Sparse\nPre-training and Dense Fine-tuning (SPDF). In this work, we show the benefits\nof using unstructured weight sparsity to train only a subset of weights during\npre-training (Sparse Pre-training) and then recover the representational\ncapacity by allowing the zeroed weights to learn (Dense Fine-tuning). We\ndemonstrate that we can induce up to 75% sparsity into a 1.3B parameter GPT-3\nXL model resulting in a 2.5x reduction in pre-training FLOPs, without a\nsignificant loss in accuracy on the downstream tasks relative to the dense\nbaseline. By rigorously evaluating multiple downstream tasks, we also establish\na relationship between sparsity, task complexity and dataset size. Our work\npresents a promising direction to train large GPT models at a fraction of the\ntraining FLOPs using weight sparsity, while retaining the benefits of\npre-trained textual representations for downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.10464v2.pdf"
    },
    {
        "title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models",
        "authors": [
            "Junjie Ye",
            "Xuanting Chen",
            "Nuo Xu",
            "Can Zu",
            "Zekai Shao",
            "Shichun Liu",
            "Yuhan Cui",
            "Zeyang Zhou",
            "Chao Gong",
            "Yang Shen",
            "Jie Zhou",
            "Siming Chen",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-03-18T14:02:04Z",
        "summary": "GPT series models, such as GPT-3, CodeX, InstructGPT, ChatGPT, and so on,\nhave gained considerable attention due to their exceptional natural language\nprocessing capabilities. However, despite the abundance of research on the\ndifference in capabilities between GPT series models and fine-tuned models,\nthere has been limited attention given to the evolution of GPT series models'\ncapabilities over time. To conduct a comprehensive analysis of the capabilities\nof GPT series models, we select six representative models, comprising two GPT-3\nseries models (i.e., davinci and text-davinci-001) and four GPT-3.5 series\nmodels (i.e., code-davinci-002, text-davinci-002, text-davinci-003, and\ngpt-3.5-turbo). We evaluate their performance on nine natural language\nunderstanding (NLU) tasks using 21 datasets. In particular, we compare the\nperformance and robustness of different models for each task under zero-shot\nand few-shot scenarios. Our extensive experiments reveal that the overall\nability of GPT series models on NLU tasks does not increase gradually as the\nmodels evolve, especially with the introduction of the RLHF training strategy.\nWhile this strategy enhances the models' ability to generate human-like\nresponses, it also compromises their ability to solve some tasks. Furthermore,\nour findings indicate that there is still room for improvement in areas such as\nmodel robustness.",
        "pdf_link": "https://arxiv.org/pdf/2303.10420v2.pdf"
    },
    {
        "title": "An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering",
        "authors": [
            "Nan Hu",
            "Yike Wu",
            "Guilin Qi",
            "Dehai Min",
            "Jiaoyan Chen",
            "Jeff Z. Pan",
            "Zafar Ali"
        ],
        "published": "2023-03-18T08:57:09Z",
        "summary": "Large-scale pre-trained language models (PLMs) such as BERT have recently\nachieved great success and become a milestone in natural language processing\n(NLP). It is now the consensus of the NLP community to adopt PLMs as the\nbackbone for downstream tasks. In recent works on knowledge graph question\nanswering (KGQA), BERT or its variants have become necessary in their KGQA\nmodels. However, there is still a lack of comprehensive research and comparison\nof the performance of different PLMs in KGQA. To this end, we summarize two\nbasic KGQA frameworks based on PLMs without additional neural network modules\nto compare the performance of nine PLMs in terms of accuracy and efficiency. In\naddition, we present three benchmarks for larger-scale KGs based on the popular\nSimpleQuestions benchmark to investigate the scalability of PLMs. We carefully\nanalyze the results of all PLMs-based KGQA basic frameworks on these benchmarks\nand two other popular datasets, WebQuestionSP and FreebaseQA, and find that\nknowledge distillation techniques and knowledge enhancement methods in PLMs are\npromising for KGQA. Furthermore, we test ChatGPT, which has drawn a great deal\nof attention in the NLP community, demonstrating its impressive capabilities\nand limitations in zero-shot KGQA. We have released the code and benchmarks to\npromote the use of PLMs on KGQA.",
        "pdf_link": "https://arxiv.org/pdf/2303.10368v1.pdf"
    },
    {
        "title": "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
        "authors": [
            "Lixiang Yan",
            "Lele Sha",
            "Linxuan Zhao",
            "Yuheng Li",
            "Roberto Martinez-Maldonado",
            "Guanliang Chen",
            "Xinyu Li",
            "Yueqiao Jin",
            "Dragan Gašević"
        ],
        "published": "2023-03-17T18:14:46Z",
        "summary": "Educational technology innovations leveraging large language models (LLMs)\nhave shown the potential to automate the laborious process of generating and\nanalysing textual content. While various innovations have been developed to\nautomate a range of educational tasks (e.g., question generation, feedback\nprovision, and essay grading), there are concerns regarding the practicality\nand ethicality of these innovations. Such concerns may hinder future research\nand the adoption of LLMs-based innovations in authentic educational contexts.\nTo address this, we conducted a systematic scoping review of 118 peer-reviewed\npapers published since 2017 to pinpoint the current state of research on using\nLLMs to automate and support educational tasks. The findings revealed 53 use\ncases for LLMs in automating education tasks, categorised into nine main\ncategories: profiling/labelling, detection, grading, teaching support,\nprediction, knowledge representation, feedback, content generation, and\nrecommendation. Additionally, we also identified several practical and ethical\nchallenges, including low technological readiness, lack of replicability and\ntransparency, and insufficient privacy and beneficence considerations. The\nfindings were summarised into three recommendations for future studies,\nincluding updating existing innovations with state-of-the-art models (e.g.,\nGPT-3/4), embracing the initiative of open-sourcing models/systems, and\nadopting a human-centred approach throughout the developmental process. As the\nintersection of AI and education is continuously evolving, the findings of this\nstudy can serve as an essential reference point for researchers, allowing them\nto leverage the strengths, learn from the limitations, and uncover potential\nresearch opportunities enabled by ChatGPT and other generative AI models.",
        "pdf_link": "https://arxiv.org/pdf/2303.13379v2.pdf"
    },
    {
        "title": "Can AI-Generated Text be Reliably Detected?",
        "authors": [
            "Vinu Sankar Sadasivan",
            "Aounon Kumar",
            "Sriram Balasubramanian",
            "Wenxiao Wang",
            "Soheil Feizi"
        ],
        "published": "2023-03-17T17:53:19Z",
        "summary": "The unregulated use of LLMs can potentially lead to malicious consequences\nsuch as plagiarism, generating fake news, spamming, etc. Therefore, reliable\ndetection of AI-generated text can be critical to ensure the responsible use of\nLLMs. Recent works attempt to tackle this problem either using certain model\nsignatures present in the generated text outputs or by applying watermarking\ntechniques that imprint specific patterns onto them. In this paper, we show\nthat these detectors are not reliable in practical scenarios. In particular, we\ndevelop a recursive paraphrasing attack to apply on AI text, which can break a\nwhole range of detectors, including the ones using the watermarking schemes as\nwell as neural network-based detectors, zero-shot classifiers, and\nretrieval-based detectors. Our experiments include passages around 300 tokens\nin length, showing the sensitivity of the detectors even in the case of\nrelatively long passages. We also observe that our recursive paraphrasing only\ndegrades text quality slightly, measured via human studies, and metrics such as\nperplexity scores and accuracy on text benchmarks. Additionally, we show that\neven LLMs protected by watermarking schemes can be vulnerable against spoofing\nattacks aimed to mislead detectors to classify human-written text as\nAI-generated, potentially causing reputational damages to the developers. In\nparticular, we show that an adversary can infer hidden AI text signatures of\nthe LLM outputs without having white-box access to the detection method.\nFinally, we provide a theoretical connection between the AUROC of the best\npossible detector and the Total Variation distance between human and AI text\ndistributions that can be used to study the fundamental hardness of the\nreliable detection problem for advanced language models. Our code is publicly\navailable at https://github.com/vinusankars/Reliability-of-AI-text-detectors.",
        "pdf_link": "https://arxiv.org/pdf/2303.11156v3.pdf"
    },
    {
        "title": "Measuring Improvement of F$_1$-Scores in Detection of Self-Admitted Technical Debt",
        "authors": [
            "William Aiken",
            "Paul K. Mvula",
            "Paula Branco",
            "Guy-Vincent Jourdan",
            "Mehrdad Sabetzadeh",
            "Herna Viktor"
        ],
        "published": "2023-03-16T19:47:38Z",
        "summary": "Artificial Intelligence and Machine Learning have witnessed rapid,\nsignificant improvements in Natural Language Processing (NLP) tasks. Utilizing\nDeep Learning, researchers have taken advantage of repository comments in\nSoftware Engineering to produce accurate methods for detecting Self-Admitted\nTechnical Debt (SATD) from 20 open-source Java projects' code. In this work, we\nimprove SATD detection with a novel approach that leverages the Bidirectional\nEncoder Representations from Transformers (BERT) architecture. For comparison,\nwe re-evaluated previous deep learning methods and applied stratified 10-fold\ncross-validation to report reliable F$_1$-scores. We examine our model in both\ncross-project and intra-project contexts. For each context, we use re-sampling\nand duplication as augmentation strategies to account for data imbalance. We\nfind that our trained BERT model improves over the best performance of all\nprevious methods in 19 of the 20 projects in cross-project scenarios. However,\nthe data augmentation techniques were not sufficient to overcome the lack of\ndata present in the intra-project scenarios, and existing methods still perform\nbetter. Future research will look into ways to diversify SATD datasets in order\nto maximize the latent power in large BERT models.",
        "pdf_link": "https://arxiv.org/pdf/2303.09617v1.pdf"
    },
    {
        "title": "DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion",
        "authors": [
            "Maham Tanveer",
            "Yizhi Wang",
            "Ali Mahdavi-Amiri",
            "Hao Zhang"
        ],
        "published": "2023-03-16T19:12:52Z",
        "summary": "We introduce a novel method to automatically generate an artistic typography\nby stylizing one or more letter fonts to visually convey the semantics of an\ninput word, while ensuring that the output remains readable. To address an\nassortment of challenges with our task at hand including conflicting goals\n(artistic stylization vs. legibility), lack of ground truth, and immense search\nspace, our approach utilizes large language models to bridge texts and visual\nimages for stylization and build an unsupervised generative model with a\ndiffusion model backbone. Specifically, we employ the denoising generator in\nLatent Diffusion Model (LDM), with the key addition of a CNN-based\ndiscriminator to adapt the input style onto the input text. The discriminator\nuses rasterized images of a given letter/word font as real samples and output\nof the denoising generator as fake samples. Our model is coined DS-Fusion for\ndiscriminated and stylized diffusion. We showcase the quality and versatility\nof our method through numerous examples, qualitative and quantitative\nevaluation, as well as ablation studies. User studies comparing to strong\nbaselines including CLIPDraw and DALL-E 2, as well as artist-crafted\ntypographies, demonstrate strong performance of DS-Fusion.",
        "pdf_link": "https://arxiv.org/pdf/2303.09604v1.pdf"
    },
    {
        "title": "LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations",
        "authors": [
            "Catherine Tony",
            "Markus Mutas",
            "Nicolás E. Díaz Ferreyra",
            "Riccardo Scandariato"
        ],
        "published": "2023-03-16T15:13:58Z",
        "summary": "Large Language Models (LLMs) like Codex are powerful tools for performing\ncode completion and code generation tasks as they are trained on billions of\nlines of code from publicly available sources. Moreover, these models are\ncapable of generating code snippets from Natural Language (NL) descriptions by\nlearning languages and programming practices from public GitHub repositories.\nAlthough LLMs promise an effortless NL-driven deployment of software\napplications, the security of the code they generate has not been extensively\ninvestigated nor documented. In this work, we present LLMSecEval, a dataset\ncontaining 150 NL prompts that can be leveraged for assessing the security\nperformance of such models. Such prompts are NL descriptions of code snippets\nprone to various security vulnerabilities listed in MITRE's Top 25 Common\nWeakness Enumeration (CWE) ranking. Each prompt in our dataset comes with a\nsecure implementation example to facilitate comparative evaluations against\ncode produced by LLMs. As a practical application, we show how LLMSecEval can\nbe used for evaluating the security of snippets automatically generated from NL\ndescriptions.",
        "pdf_link": "https://arxiv.org/pdf/2303.09384v1.pdf"
    },
    {
        "title": "Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?",
        "authors": [
            "Jaromir Savelka",
            "Arav Agarwal",
            "Christopher Bogart",
            "Yifan Song",
            "Majd Sakr"
        ],
        "published": "2023-03-16T13:58:45Z",
        "summary": "We evaluated the capability of generative pre-trained transformers (GPT), to\npass assessments in introductory and intermediate Python programming courses at\nthe postsecondary level. Discussions of potential uses (e.g., exercise\ngeneration, code explanation) and misuses (e.g., cheating) of this emerging\ntechnology in programming education have intensified, but to date there has not\nbeen a rigorous analysis of the models' capabilities in the realistic context\nof a full-fledged programming course with diverse set of assessment\ninstruments. We evaluated GPT on three Python courses that employ assessments\nranging from simple multiple-choice questions (no code involved) to complex\nprogramming projects with code bases distributed into multiple files (599\nexercises overall). Further, we studied if and how successfully GPT models\nleverage feedback provided by an auto-grader. We found that the current models\nare not capable of passing the full spectrum of assessments typically involved\nin a Python programming course (<70% on even entry-level modules). Yet, it is\nclear that a straightforward application of these easily accessible models\ncould enable a learner to obtain a non-trivial portion of the overall available\nscore (>55%) in introductory and intermediate courses alike. While the models\nexhibit remarkable capabilities, including correcting solutions based on\nauto-grader's feedback, some limitations exist (e.g., poor handling of\nexercises requiring complex chains of reasoning steps). These findings can be\nleveraged by instructors wishing to adapt their assessments so that GPT becomes\na valuable assistant for a learner as opposed to an end-to-end solution.",
        "pdf_link": "https://arxiv.org/pdf/2303.09325v1.pdf"
    },
    {
        "title": "How well do Large Language Models perform in Arithmetic tasks?",
        "authors": [
            "Zheng Yuan",
            "Hongyi Yuan",
            "Chuanqi Tan",
            "Wei Wang",
            "Songfang Huang"
        ],
        "published": "2023-03-16T09:28:15Z",
        "summary": "Large language models have emerged abilities including chain-of-thought to\nanswer math word problems step by step. Solving math word problems not only\nrequires abilities to disassemble problems via chain-of-thought but also needs\nto calculate arithmetic expressions correctly for each step. To the best of our\nknowledge, there is no work to focus on evaluating the arithmetic ability of\nlarge language models. In this work, we propose an arithmetic dataset MATH 401\nto test the latest large language models including GPT-4, ChatGPT, InstrctGPT,\nGalactica, and LLaMA with various arithmetic expressions and provide a detailed\nanalysis of the ability of large language models. MATH 401 and evaluation codes\nare released at \\url{https://github.com/GanjinZero/math401-llm}.",
        "pdf_link": "https://arxiv.org/pdf/2304.02015v1.pdf"
    },
    {
        "title": "A Short Survey of Viewing Large Language Models in Legal Aspect",
        "authors": [
            "Zhongxiang Sun"
        ],
        "published": "2023-03-16T08:01:22Z",
        "summary": "Large language models (LLMs) have transformed many fields, including natural\nlanguage processing, computer vision, and reinforcement learning. These models\nhave also made a significant impact in the field of law, where they are being\nincreasingly utilized to automate various legal tasks, such as legal judgement\nprediction, legal document analysis, and legal document writing. However, the\nintegration of LLMs into the legal field has also raised several legal\nproblems, including privacy concerns, bias, and explainability. In this survey,\nwe explore the integration of LLMs into the field of law. We discuss the\nvarious applications of LLMs in legal tasks, examine the legal challenges that\narise from their use, and explore the data resources that can be used to\nspecialize LLMs in the legal domain. Finally, we discuss several promising\ndirections and conclude this paper. By doing so, we hope to provide an overview\nof the current state of LLMs in law and highlight the potential benefits and\nchallenges of their integration.",
        "pdf_link": "https://arxiv.org/pdf/2303.09136v1.pdf"
    },
    {
        "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
        "authors": [
            "Shushan Arakelyan",
            "Rocktim Jyoti Das",
            "Yi Mao",
            "Xiang Ren"
        ],
        "published": "2023-03-16T07:45:46Z",
        "summary": "We systematically study how three large language models with code\ncapabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data.\nWe consider two fundamental applications - code summarization, and code\ngeneration. We split data into domains following its natural boundaries - by an\norganization, by a project, and by a module within the software project. We\nestablish that samples from each new domain present all the models with a\nsignificant challenge of distribution shift. We study how established methods\nadapt models to better generalize to new domains. Our experiments show that\nwhile multitask learning alone is a reasonable baseline, combining it with\nfew-shot finetuning on examples retrieved from training data can achieve very\nstrong performance. Moreover, this solution can outperform direct finetuning\nfor very low-data scenarios. Finally, we consider variations of this approach\nto create a more broadly applicable method to adapt to multiple domains at\nonce. We find that for code generation, a model adapted to multiple domains\nsimultaneously performs on par with those adapted to a single domain",
        "pdf_link": "https://arxiv.org/pdf/2303.09128v2.pdf"
    },
    {
        "title": "Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential",
        "authors": [
            "Qing Lyu",
            "Josh Tan",
            "Michael E. Zapadka",
            "Janardhana Ponnatapura",
            "Chuang Niu",
            "Kyle J. Myers",
            "Ge Wang",
            "Christopher T. Whitlow"
        ],
        "published": "2023-03-16T02:21:39Z",
        "summary": "The large language model called ChatGPT has drawn extensively attention\nbecause of its human-like expression and reasoning abilities. In this study, we\ninvestigate the feasibility of using ChatGPT in experiments on using ChatGPT to\ntranslate radiology reports into plain language for patients and healthcare\nproviders so that they are educated for improved healthcare. Radiology reports\nfrom 62 low-dose chest CT lung cancer screening scans and 76 brain MRI\nmetastases screening scans were collected in the first half of February for\nthis study. According to the evaluation by radiologists, ChatGPT can\nsuccessfully translate radiology reports into plain language with an average\nscore of 4.27 in the five-point system with 0.08 places of information missing\nand 0.07 places of misinformation. In terms of the suggestions provided by\nChatGPT, they are general relevant such as keeping following-up with doctors\nand closely monitoring any symptoms, and for about 37% of 138 cases in total\nChatGPT offers specific suggestions based on findings in the report. ChatGPT\nalso presents some randomness in its responses with occasionally\nover-simplified or neglected information, which can be mitigated using a more\ndetailed prompt. Furthermore, ChatGPT results are compared with a newly\nreleased large model GPT-4, showing that GPT-4 can significantly improve the\nquality of translated reports. Our results show that it is feasible to utilize\nlarge language models in clinical education, and further efforts are needed to\naddress limitations and maximize their potential.",
        "pdf_link": "https://arxiv.org/pdf/2303.09038v3.pdf"
    },
    {
        "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
        "authors": [
            "Bhargavi Paranjape",
            "Scott Lundberg",
            "Sameer Singh",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer",
            "Marco Tulio Ribeiro"
        ],
        "published": "2023-03-16T01:04:45Z",
        "summary": "Large language models (LLMs) can perform complex reasoning in few- and\nzero-shot settings by generating intermediate chain of thought (CoT) reasoning\nsteps. Further, each reasoning step can rely on external tools to support\ncomputation beyond the core LLM capabilities (e.g. search/running code). Prior\nwork on CoT prompting and tool use typically requires hand-crafting\ntask-specific demonstrations and carefully scripted interleaving of model\ngenerations with tool use. We introduce Automatic Reasoning and Tool-use (ART),\na framework that uses frozen LLMs to automatically generate intermediate\nreasoning steps as a program. Given a new task to solve, ART selects\ndemonstrations of multi-step reasoning and tool use from a task library. At\ntest time, ART seamlessly pauses generation whenever external tools are called,\nand integrates their output before resuming generation. ART achieves a\nsubstantial improvement over few-shot prompting and automatic CoT on unseen\ntasks in the BigBench and MMLU benchmarks, and matches performance of\nhand-crafted CoT prompts on a majority of these tasks. ART is also extensible,\nand makes it easy for humans to improve performance by correcting errors in\ntask-specific programs or incorporating new tools, which we demonstrate by\ndrastically improving performance on select tasks with minimal human\nintervention.",
        "pdf_link": "https://arxiv.org/pdf/2303.09014v1.pdf"
    },
    {
        "title": "Automated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs",
        "authors": [
            "Yankai Zeng",
            "Abhiramon Rajasekharan",
            "Parth Padalkar",
            "Kinjal Basu",
            "Joaquín Arias",
            "Gopal Gupta"
        ],
        "published": "2023-03-15T21:10:33Z",
        "summary": "Achieving human-like communication with machines remains a classic,\nchallenging topic in the field of Knowledge Representation and Reasoning and\nNatural Language Processing. These Large Language Models (LLMs) rely on\npattern-matching rather than a true understanding of the semantic meaning of a\nsentence. As a result, they may generate incorrect responses. To generate an\nassuredly correct response, one has to \"understand\" the semantics of a\nsentence. To achieve this \"understanding\", logic-based (commonsense) reasoning\nmethods such as Answer Set Programming (ASP) are arguably needed. In this\npaper, we describe the AutoConcierge system that leverages LLMs and ASP to\ndevelop a conversational agent that can truly \"understand\" human dialogs in\nrestricted domains. AutoConcierge is focused on a specific domain-advising\nusers about restaurants in their local area based on their preferences.\nAutoConcierge will interactively understand a user's utterances, identify the\nmissing information in them, and request the user via a natural language\nsentence to provide it. Once AutoConcierge has determined that all the\ninformation has been received, it computes a restaurant recommendation based on\nthe user-preferences it has acquired from the human user. AutoConcierge is\nbased on our STAR framework developed earlier, which uses GPT-3 to convert\nhuman dialogs into predicates that capture the deep structure of the dialog's\nsentence. These predicates are then input into the goal-directed s(CASP) ASP\nsystem for performing commonsense reasoning. To the best of our knowledge,\nAutoConcierge is the first automated conversational agent that can\nrealistically converse like a human and provide help to humans based on truly\nunderstanding human utterances.",
        "pdf_link": "https://arxiv.org/pdf/2303.08941v2.pdf"
    },
    {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
            "Potsawee Manakul",
            "Adian Liusie",
            "Mark J. F. Gales"
        ],
        "published": "2023-03-15T19:31:21Z",
        "summary": "Generative Large Language Models (LLMs) such as GPT-3 are capable of\ngenerating highly fluent responses to a wide variety of user prompts. However,\nLLMs are known to hallucinate facts and make non-factual statements which can\nundermine trust in their output. Existing fact-checking approaches either\nrequire access to the output probability distribution (which may not be\navailable for systems such as ChatGPT) or external databases that are\ninterfaced via separate, often complex, modules. In this work, we propose\n\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check\nthe responses of black-box models in a zero-resource fashion, i.e. without an\nexternal database. SelfCheckGPT leverages the simple idea that if an LLM has\nknowledge of a given concept, sampled responses are likely to be similar and\ncontain consistent facts. However, for hallucinated facts, stochastically\nsampled responses are likely to diverge and contradict one another. We\ninvestigate this approach by using GPT-3 to generate passages about individuals\nfrom the WikiBio dataset, and manually annotate the factuality of the generated\npassages. We demonstrate that SelfCheckGPT can: i) detect non-factual and\nfactual sentences; and ii) rank passages in terms of factuality. We compare our\napproach to several baselines and show that our approach has considerably\nhigher AUC-PR scores in sentence-level hallucination detection and higher\ncorrelation scores in passage-level factuality assessment compared to grey-box\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2303.08896v3.pdf"
    },
    {
        "title": "Large Language Model Is Not a Good Few-shot Information Extractor, but a Good Reranker for Hard Samples!",
        "authors": [
            "Yubo Ma",
            "Yixin Cao",
            "YongChing Hong",
            "Aixin Sun"
        ],
        "published": "2023-03-15T12:20:13Z",
        "summary": "Large Language Models (LLMs) have made remarkable strides in various tasks.\nWhether LLMs are competitive few-shot solvers for information extraction (IE)\ntasks, however, remains an open problem. In this work, we aim to provide a\nthorough answer to this question. Through extensive experiments on nine\ndatasets across four IE tasks, we demonstrate that current advanced LLMs\nconsistently exhibit inferior performance, higher latency, and increased budget\nrequirements compared to fine-tuned SLMs under most settings. Therefore, we\nconclude that LLMs are not effective few-shot information extractors in\ngeneral. Nonetheless, we illustrate that with appropriate prompting strategies,\nLLMs can effectively complement SLMs and tackle challenging samples that SLMs\nstruggle with. And moreover, we propose an adaptive filter-then-rerank paradigm\nto combine the strengths of LLMs and SLMs. In this paradigm, SLMs serve as\nfilters and LLMs serve as rerankers. By prompting LLMs to rerank a small\nportion of difficult samples identified by SLMs, our preliminary system\nconsistently achieves promising improvements (2.4% F1-gain on average) on\nvarious IE tasks, with an acceptable time and cost investment.",
        "pdf_link": "https://arxiv.org/pdf/2303.08559v2.pdf"
    },
    {
        "title": "UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation",
        "authors": [
            "Daixuan Cheng",
            "Shaohan Huang",
            "Junyu Bi",
            "Yuefeng Zhan",
            "Jianfeng Liu",
            "Yujing Wang",
            "Hao Sun",
            "Furu Wei",
            "Denvy Deng",
            "Qi Zhang"
        ],
        "published": "2023-03-15T10:53:49Z",
        "summary": "Large Language Models (LLMs) are popular for their impressive abilities, but\nthe need for model-specific fine-tuning or task-specific prompt engineering can\nhinder their generalization. We propose UPRISE (Universal Prompt Retrieval for\nImproving zero-Shot Evaluation), which tunes a lightweight and versatile\nretriever that automatically retrieves prompts for a given zero-shot task\ninput. Specifically, we demonstrate universality in a cross-task and\ncross-model scenario: the retriever is tuned on a diverse set of tasks, but\ntested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for\ntuning the retriever, but test the retriever on different LLMs of much larger\nscales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that\nUPRISE mitigates the hallucination problem in our experiments with ChatGPT,\nsuggesting its potential to improve even the strongest LLMs. Our model and code\nare available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://arxiv.org/pdf/2303.08518v4.pdf"
    },
    {
        "title": "A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records",
        "authors": [
            "Sicheng Zhou",
            "Nan Wang",
            "Liwei Wang",
            "Ju Sun",
            "Anne Blaes",
            "Hongfang Liu",
            "Rui Zhang"
        ],
        "published": "2023-03-15T08:44:07Z",
        "summary": "Objective: The generalizability of clinical large language models is usually\nignored during the model development process. This study evaluated the\ngeneralizability of BERT-based clinical NLP models across different clinical\nsettings through a breast cancer phenotype extraction task.\n  Materials and Methods: Two clinical corpora of breast cancer patients were\ncollected from the electronic health records from the University of Minnesota\nand the Mayo Clinic, and annotated following the same guideline. We developed\nthree types of NLP models (i.e., conditional random field, bi-directional long\nshort-term memory and CancerBERT) to extract cancer phenotypes from clinical\ntexts. The models were evaluated for their generalizability on different test\nsets with different learning strategies (model transfer vs. locally trained).\nThe entity coverage score was assessed with their association with the model\nperformances.\n  Results: We manually annotated 200 and 161 clinical documents at UMN and MC,\nrespectively. The corpora of the two institutes were found to have higher\nsimilarity between the target entities than the overall corpora. The CancerBERT\nmodels obtained the best performances among the independent test sets from two\nclinical institutes and the permutation test set. The CancerBERT model\ndeveloped in one institute and further fine-tuned in another institute achieved\nreasonable performance compared to the model developed on local data (micro-F1:\n0.925 vs 0.932).\n  Conclusions: The results indicate the CancerBERT model has the best learning\nability and generalizability among the three types of clinical NLP models. The\ngeneralizability of the models was found to be correlated with the similarity\nof the target entities between the corpora.",
        "pdf_link": "https://arxiv.org/pdf/2303.08448v1.pdf"
    },
    {
        "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation",
        "authors": [
            "Zhewei Yao",
            "Xiaoxia Wu",
            "Cheng Li",
            "Stephen Youn",
            "Yuxiong He"
        ],
        "published": "2023-03-15T01:27:15Z",
        "summary": "Post-training quantization (PTQ) has emerged as a promising technique for\nmitigating memory consumption and computational costs in large language models\n(LLMs). However, a systematic examination of various quantization schemes,\nmodel families, and quantization bit precision has been absent from the\nliterature. In this paper, we conduct a comprehensive analysis of these factors\nby investigating the effects of PTQ on weight-only, activation-only, and\nweight-and-activation quantization using diverse methods such as\nround-to-nearest (RTN), GPTQ, ZeroQuant, and their variants. We apply these\nmethods to two distinct model families with parameters ranging from 125M to\n176B. Our contributions include: (1) a sensitivity analysis revealing that\nactivation quantization is generally more susceptible to weight quantization,\nwith smaller models often outperforming larger models in terms of activation\nquantization; (2) an evaluation and comparison of existing PTQ methods to\noptimize model size reduction while minimizing the impact on accuracy,\nrevealing that none of the current methods can achieve the original model\nquality for quantization with either INT4-weight or\nINT4-weight-and-INT8-activation; (3) based on these insights, we propose an\noptimized method called Low-Rank Compensation (LoRC), which employs low-rank\nmatrices to enhance model quality recovery with a minimal increase in model\nsize.",
        "pdf_link": "https://arxiv.org/pdf/2303.08302v3.pdf"
    },
    {
        "title": "Attention-likelihood relationship in transformers",
        "authors": [
            "Valeria Ruscio",
            "Valentino Maiorca",
            "Fabrizio Silvestri"
        ],
        "published": "2023-03-15T00:23:49Z",
        "summary": "We analyze how large language models (LLMs) represent out-of-context words,\ninvestigating their reliance on the given context to capture their semantics.\nOur likelihood-guided text perturbations reveal a correlation between token\nlikelihood and attention values in transformer-based language models. Extensive\nexperiments reveal that unexpected tokens cause the model to attend less to the\ninformation coming from themselves to compute their representations,\nparticularly at higher layers. These findings have valuable implications for\nassessing the robustness of LLMs in real-world scenarios. Fully reproducible\ncodebase at https://github.com/Flegyas/AttentionLikelihood.",
        "pdf_link": "https://arxiv.org/pdf/2303.08288v1.pdf"
    },
    {
        "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models",
        "authors": [
            "Xufeng Zhao",
            "Mengdi Li",
            "Cornelius Weber",
            "Muhammad Burhan Hafez",
            "Stefan Wermter"
        ],
        "published": "2023-03-14T23:01:27Z",
        "summary": "Programming robot behavior in a complex world faces challenges on multiple\nlevels, from dextrous low-level skills to high-level planning and reasoning.\nRecent pre-trained Large Language Models (LLMs) have shown remarkable reasoning\nability in few-shot robotic planning. However, it remains challenging to ground\nLLMs in multimodal sensory input and continuous action output, while enabling a\nrobot to interact with its environment and acquire novel information as its\npolicies unfold. We develop a robot interaction scenario with a partially\nobservable state, which necessitates a robot to decide on a range of epistemic\nactions in order to sample sensory information among multiple modalities,\nbefore being able to execute the task correctly. Matcha (Multimodal environment\nchatting) agent, an interactive perception framework, is therefore proposed\nwith an LLM as its backbone, whose ability is exploited to instruct epistemic\nactions and to reason over the resulting multimodal sensations (vision, sound,\nhaptics, proprioception), as well as to plan an entire task execution based on\nthe interactively acquired information. Our study demonstrates that LLMs can\nprovide high-level planning and reasoning skills and control interactive robot\nbehavior in a multimodal environment, while multimodal modules with the context\nof the environmental state help ground the LLMs and extend their processing\nability. The project website can be found at https://matcha-agent.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2303.08268v3.pdf"
    },
    {
        "title": "Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures",
        "authors": [
            "Aokun Chen",
            "Zehao Yu",
            "Xi Yang",
            "Yi Guo",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2023-03-14T22:22:28Z",
        "summary": "Objective: To develop a natural language processing (NLP) system to extract\nmedications and contextual information that help understand drug changes. This\nproject is part of the 2022 n2c2 challenge.\n  Materials and methods: We developed NLP systems for medication mention\nextraction, event classification (indicating medication changes discussed or\nnot), and context classification to classify medication changes context into 5\northogonal dimensions related to drug changes. We explored 6 state-of-the-art\npretrained transformer models for the three subtasks, including GatorTron, a\nlarge language model pretrained using >90 billion words of text (including >80\nbillion words from >290 million clinical notes identified at the University of\nFlorida Health). We evaluated our NLP systems using annotated data and\nevaluation scripts provided by the 2022 n2c2 organizers.\n  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for\nmedication extraction (ranked 3rd), 0.9379 for event classification (ranked\n2nd), and the best micro-average accuracy of 0.9126 for context classification.\nGatorTron outperformed existing transformer models pretrained using smaller\ngeneral English text and clinical text corpora, indicating the advantage of\nlarge language models.\n  Conclusion: This study demonstrated the advantage of using large transformer\nmodels for contextual medication information extraction from clinical\nnarratives.",
        "pdf_link": "https://arxiv.org/pdf/2303.08259v1.pdf"
    },
    {
        "title": "How Many Demonstrations Do You Need for In-context Learning?",
        "authors": [
            "Jiuhai Chen",
            "Lichang Chen",
            "Chen Zhu",
            "Tianyi Zhou"
        ],
        "published": "2023-03-14T17:50:45Z",
        "summary": "Large language models (LLMs) are capable to perform complex reasoning by\nin-context learning (ICL) when provided with a few input-output demonstrations\n(demos) and more powerful when intermediate reasoning steps (\"chain of thoughts\n(CoT)\") of the demos are given. Is it necessary to use multi-demo in ICL? In\nthis paper, we study ICL using fewer demos for each test query on the tasks\nin~\\cite{wei2022chain}. Surprisingly, we do not observe significant degradation\nwhen using only one randomly chosen demo. To study this phenomenon, for each\ntest query, we categorize demos into \"correct demos\" leading to the correct\nanswer, and \"wrong demos\" resulting in wrong answers. Our analysis reveals an\ninherent bias in those widely studied datasets: most demos are correct for a\nmajority of test queries, which explains the good performance of using one\nrandom demo. Moreover, ICL (with and w/o CoT) using only one correct demo\nsignificantly outperforms all-demo ICL adopted by most previous works,\nindicating the weakness of LLMs in finding correct demo(s) for input queries,\nwhich is difficult to evaluate on the biased datasets. Furthermore, we observe\na counterintuitive behavior of ICL using multi-demo, i.e., its accuracy\ndegrades(improves) when given more correct(wrong) demos. This implies that ICL\ncan be easily misguided by interference among demos and their spurious\ncorrelations. Our analyses highlight several fundamental challenges that need\nto be addressed in LLMs training, ICL, and benchmark design.",
        "pdf_link": "https://arxiv.org/pdf/2303.08119v3.pdf"
    },
    {
        "title": "Do Transformers Parse while Predicting the Masked Word?",
        "authors": [
            "Haoyu Zhao",
            "Abhishek Panigrahi",
            "Rong Ge",
            "Sanjeev Arora"
        ],
        "published": "2023-03-14T17:49:50Z",
        "summary": "Pre-trained language models have been shown to encode linguistic structures,\ne.g. dependency and constituency parse trees, in their embeddings while being\ntrained on unsupervised loss functions like masked language modeling. Some\ndoubts have been raised whether the models actually are doing parsing or only\nsome computation weakly correlated with it. We study questions: (a) Is it\npossible to explicitly describe transformers with realistic embedding\ndimension, number of heads, etc. that are capable of doing parsing -- or even\napproximate parsing? (b) Why do pre-trained models capture parsing structure?\nThis paper takes a step toward answering these questions in the context of\ngenerative modeling with PCFGs. We show that masked language models like BERT\nor RoBERTa of moderate sizes can approximately execute the Inside-Outside\nalgorithm for the English PCFG [Marcus et al, 1993]. We also show that the\nInside-Outside algorithm is optimal for masked language modeling loss on the\nPCFG-generated data. We also give a construction of transformers with $50$\nlayers, $15$ attention heads, and $1275$ dimensional embeddings in average such\nthat using its embeddings it is possible to do constituency parsing with\n$>70\\%$ F1 score on PTB dataset. We conduct probing experiments on models\npre-trained on PCFG-generated data to show that this not only allows recovery\nof approximate parse tree, but also recovers marginal span probabilities\ncomputed by the Inside-Outside algorithm, which suggests an implicit bias of\nmasked language modeling towards this algorithm.",
        "pdf_link": "https://arxiv.org/pdf/2303.08117v2.pdf"
    },
    {
        "title": "Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family",
        "authors": [
            "Yiming Tan",
            "Dehai Min",
            "Yu Li",
            "Wenbo Li",
            "Nan Hu",
            "Yongrui Chen",
            "Guilin Qi"
        ],
        "published": "2023-03-14T15:46:28Z",
        "summary": "ChatGPT is a powerful large language model (LLM) that covers knowledge\nresources such as Wikipedia and supports natural language question answering\nusing its own knowledge. Therefore, there is growing interest in exploring\nwhether ChatGPT can replace traditional knowledge-based question answering\n(KBQA) models. Although there have been some works analyzing the question\nanswering performance of ChatGPT, there is still a lack of large-scale,\ncomprehensive testing of various types of complex questions to analyze the\nlimitations of the model. In this paper, we present a framework that follows\nthe black-box testing specifications of CheckList proposed by Ribeiro et. al.\nWe evaluate ChatGPT and its family of LLMs on eight real-world KB-based complex\nquestion answering datasets, which include six English datasets and two\nmultilingual datasets. The total number of test cases is approximately 190,000.\nIn addition to the GPT family of LLMs, we also evaluate the well-known FLAN-T5\nto identify commonalities between the GPT family and other LLMs. The dataset\nand code are available at\nhttps://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git",
        "pdf_link": "https://arxiv.org/pdf/2303.07992v3.pdf"
    },
    {
        "title": "A Theory of Emergent In-Context Learning as Implicit Structure Induction",
        "authors": [
            "Michael Hahn",
            "Navin Goyal"
        ],
        "published": "2023-03-14T15:24:05Z",
        "summary": "Scaling large language models (LLMs) leads to an emergent capacity to learn\nin-context from example demonstrations. Despite progress, theoretical\nunderstanding of this phenomenon remains limited. We argue that in-context\nlearning relies on recombination of compositional operations found in natural\nlanguage data. We derive an information-theoretic bound showing how in-context\nlearning abilities arise from generic next-token prediction when the\npretraining distribution has sufficient amounts of compositional structure,\nunder linguistically motivated assumptions. A second bound provides a\ntheoretical justification for the empirical success of prompting LLMs to output\nintermediate steps towards an answer. To validate theoretical predictions, we\nintroduce a controlled setup for inducing in-context learning; unlike previous\napproaches, it accounts for the compositional nature of language. Trained\ntransformers can perform in-context learning for a range of tasks, in a manner\nconsistent with the theoretical results. Mirroring real-world LLMs in a\nminiature setup, in-context learning emerges when scaling parameters and data,\nand models perform better when prompted to output intermediate steps. Probing\nshows that in-context learning is supported by a representation of the input's\ncompositional structure. Taken together, these results provide a step towards\ntheoretical understanding of emergent behavior in large language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.07971v1.pdf"
    },
    {
        "title": "RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback",
        "authors": [
            "Souradip Chakraborty",
            "Kasun Weerakoon",
            "Prithvi Poddar",
            "Mohamed Elnoor",
            "Priya Narayanan",
            "Carl Busart",
            "Pratap Tokekar",
            "Amrit Singh Bedi",
            "Dinesh Manocha"
        ],
        "published": "2023-03-14T04:20:59Z",
        "summary": "Reinforcement learning-based policies for continuous control robotic\nnavigation tasks often fail to adapt to changes in the environment during\nreal-time deployment, which may result in catastrophic failures. To address\nthis limitation, we propose a novel approach called RE-MOVE (REquest help and\nMOVE on) to adapt already trained policy to real-time changes in the\nenvironment without re-training via utilizing a language-based feedback. The\nproposed approach essentially boils down to addressing two main challenges of\n(1) when to ask for feedback and, if received, (2) how to incorporate feedback\ninto trained policies. RE-MOVE incorporates an epistemic uncertainty-based\nframework to determine the optimal time to request instructions-based feedback.\nFor the second challenge, we employ a zero-shot learning natural language\nprocessing (NLP) paradigm with efficient, prompt design and leverage\nstate-of-the-art GPT-3.5, Llama-2 language models. To show the efficacy of the\nproposed approach, we performed extensive synthetic and real-world evaluations\nin several test-time dynamic navigation scenarios. Utilizing RE-MOVE result in\nup to 80% enhancement in the attainment of successful goals, coupled with a\nreduction of 13.50% in the normalized trajectory length, as compared to\nalternative approaches, particularly in demanding real-world environments with\nperceptual challenges.",
        "pdf_link": "https://arxiv.org/pdf/2303.07622v2.pdf"
    },
    {
        "title": "Input-length-shortening and text generation via attention values",
        "authors": [
            "Neşet Özkan Tan",
            "Alex Yuxuan Peng",
            "Joshua Bensemann",
            "Qiming Bao",
            "Tim Hartill",
            "Mark Gahegan",
            "Michael Witbrock"
        ],
        "published": "2023-03-14T02:11:24Z",
        "summary": "Identifying words that impact a task's performance more than others is a\nchallenge in natural language processing. Transformers models have recently\naddressed this issue by incorporating an attention mechanism that assigns\ngreater attention (i.e., relevance) scores to some words than others. Because\nof the attention mechanism's high computational cost, transformer models\nusually have an input-length limitation caused by hardware constraints. This\nlimitation applies to many transformers, including the well-known bidirectional\nencoder representations of the transformer (BERT) model. In this paper, we\nexamined BERT's attention assignment mechanism, focusing on two questions: (1)\nHow can attention be employed to reduce input length? (2) How can attention be\nused as a control mechanism for conditional text generation? We investigated\nthese questions in the context of a text classification task. We discovered\nthat BERT's early layers assign more critical attention scores for text\nclassification tasks compared to later layers. We demonstrated that the first\nlayer's attention sums could be used to filter tokens in a given sequence,\nconsiderably decreasing the input length while maintaining good test accuracy.\nWe also applied filtering, which uses a compute-efficient semantic similarities\nalgorithm, and discovered that retaining approximately 6\\% of the original\nsequence is sufficient to obtain 86.5\\% accuracy. Finally, we showed that we\ncould generate data in a stable manner and indistinguishable from the original\none by only using a small percentage (10\\%) of the tokens with high attention\nscores according to BERT's first layer.",
        "pdf_link": "https://arxiv.org/pdf/2303.07585v1.pdf"
    },
    {
        "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification",
        "authors": [
            "Benjamin Clavié",
            "Alexandru Ciceu",
            "Frederick Naylor",
            "Guillaume Soulié",
            "Thomas Brightwell"
        ],
        "published": "2023-03-13T14:09:53Z",
        "summary": "This case study investigates the task of job classification in a real-world\nsetting, where the goal is to determine whether an English-language job posting\nis appropriate for a graduate or entry-level position. We explore multiple\napproaches to text classification, including supervised approaches such as\ntraditional models like Support Vector Machines (SVMs) and state-of-the-art\ndeep learning methods such as DeBERTa. We compare them with Large Language\nModels (LLMs) used in both few-shot and zero-shot classification settings. To\naccomplish this task, we employ prompt engineering, a technique that involves\ndesigning prompts to guide the LLMs towards the desired output. Specifically,\nwe evaluate the performance of two commercially available state-of-the-art\nGPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also\nconduct a detailed analysis of the impact of different aspects of prompt\nengineering on the model's performance. Our results show that, with a\nwell-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all\nother models, achieving a 6% increase in Precision@95% Recall compared to the\nbest supervised approach. Furthermore, we observe that the wording of the\nprompt is a critical factor in eliciting the appropriate \"reasoning\" in the\nmodel, and that seemingly minor aspects of the prompt significantly affect the\nmodel's performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.07142v3.pdf"
    },
    {
        "title": "FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU",
        "authors": [
            "Ying Sheng",
            "Lianmin Zheng",
            "Binhang Yuan",
            "Zhuohan Li",
            "Max Ryabinin",
            "Daniel Y. Fu",
            "Zhiqiang Xie",
            "Beidi Chen",
            "Clark Barrett",
            "Joseph E. Gonzalez",
            "Percy Liang",
            "Christopher Ré",
            "Ion Stoica",
            "Ce Zhang"
        ],
        "published": "2023-03-13T05:19:28Z",
        "summary": "The high computational and memory requirements of large language model (LLM)\ninference make it feasible only with multiple high-end accelerators. Motivated\nby the emerging demand for latency-insensitive tasks with batched processing,\nthis paper initiates the study of high-throughput LLM inference using limited\nresources, such as a single commodity GPU. We present FlexGen, a\nhigh-throughput generation engine for running LLMs with limited GPU memory.\nFlexGen can be flexibly configured under various hardware resource constraints\nby aggregating memory and computation from the GPU, CPU, and disk. By solving a\nlinear programming problem, it searches for efficient patterns to store and\naccess tensors. FlexGen further compresses the weights and the attention cache\nto 4 bits with negligible accuracy loss. These techniques enable FlexGen to\nhave a larger space of batch size choices and thus significantly increase\nmaximum throughput. As a result, when running OPT-175B on a single 16GB GPU,\nFlexGen achieves significantly higher throughput compared to state-of-the-art\noffloading systems, reaching a generation throughput of 1 token/s for the first\ntime with an effective batch size of 144. On the HELM benchmark, FlexGen can\nbenchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21\nhours. The code is available at https://github.com/FMInference/FlexGen",
        "pdf_link": "https://arxiv.org/pdf/2303.06865v2.pdf"
    },
    {
        "title": "Mapping the Design Space of Interactions in Human-AI Text Co-creation Tasks",
        "authors": [
            "Zijian Ding",
            "Joel Chan"
        ],
        "published": "2023-03-11T15:45:47Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive text generation\ncapabilities, prompting us to reconsider the future of human-AI co-creation and\nhow humans interact with LLMs. In this paper, we present a spectrum of content\ngeneration tasks and their corresponding human-AI interaction patterns. These\ntasks include: 1) fixed-scope content curation tasks with minimal human-AI\ninteractions, 2) independent creative tasks with precise human-AI interactions,\nand 3) complex and interdependent creative tasks with iterative human-AI\ninteractions. We encourage the generative AI and HCI research communities to\nfocus on the more complex and interdependent tasks, which require greater\nlevels of human involvement.",
        "pdf_link": "https://arxiv.org/pdf/2303.06430v2.pdf"
    },
    {
        "title": "ChatGPT Prompt Patterns for Improving Code Quality, Refactoring, Requirements Elicitation, and Software Design",
        "authors": [
            "Jules White",
            "Sam Hays",
            "Quchen Fu",
            "Jesse Spencer-Smith",
            "Douglas C. Schmidt"
        ],
        "published": "2023-03-11T14:43:17Z",
        "summary": "This paper presents prompt design techniques for software engineering, in the\nform of patterns, to solve common problems when using large language models\n(LLMs), such as ChatGPT to automate common software engineering activities,\nsuch as ensuring code is decoupled from third-party libraries and simulating a\nweb application API before it is implemented. This paper provides two\ncontributions to research on using LLMs for software engineering. First, it\nprovides a catalog of patterns for software engineering that classifies\npatterns according to the types of problems they solve. Second, it explores\nseveral prompt patterns that have been applied to improve requirements\nelicitation, rapid prototyping, code quality, refactoring, and system design.",
        "pdf_link": "https://arxiv.org/pdf/2303.07839v1.pdf"
    },
    {
        "title": "Consistency Analysis of ChatGPT",
        "authors": [
            "Myeongjun Erik Jang",
            "Thomas Lukasiewicz"
        ],
        "published": "2023-03-11T01:19:01Z",
        "summary": "ChatGPT has gained a huge popularity since its introduction. Its positive\naspects have been reported through many media platforms, and some analyses even\nshowed that ChatGPT achieved a decent grade in professional exams, adding extra\nsupport to the claim that AI can now assist and even replace humans in\nindustrial fields. Others, however, doubt its reliability and trustworthiness.\nThis paper investigates the trustworthiness of ChatGPT and GPT-4 regarding\nlogically consistent behaviour, focusing specifically on semantic consistency\nand the properties of negation, symmetric, and transitive consistency. Our\nfindings suggest that while both models appear to show an enhanced language\nunderstanding and reasoning ability, they still frequently fall short of\ngenerating logically consistent predictions. We also ascertain via experiments\nthat prompt designing, few-shot learning and employing larger large language\nmodels (LLMs) are unlikely to be the ultimate solution to resolve the\ninconsistency issue of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.06273v3.pdf"
    },
    {
        "title": "Who's Thinking? A Push for Human-Centered Evaluation of LLMs using the XAI Playbook",
        "authors": [
            "Teresa Datta",
            "John P. Dickerson"
        ],
        "published": "2023-03-10T22:15:49Z",
        "summary": "Deployed artificial intelligence (AI) often impacts humans, and there is no\none-size-fits-all metric to evaluate these tools. Human-centered evaluation of\nAI-based systems combines quantitative and qualitative analysis and human\ninput. It has been explored to some depth in the explainable AI (XAI) and\nhuman-computer interaction (HCI) communities. Gaps remain, but the basic\nunderstanding that humans interact with AI and accompanying explanations, and\nthat humans' needs -- complete with their cognitive biases and quirks -- should\nbe held front and center, is accepted by the community. In this paper, we draw\nparallels between the relatively mature field of XAI and the rapidly evolving\nresearch boom around large language models (LLMs). Accepted evaluative metrics\nfor LLMs are not human-centered. We argue that many of the same paths tread by\nthe XAI community over the past decade will be retread when discussing LLMs.\nSpecifically, we argue that humans' tendencies -- again, complete with their\ncognitive biases and quirks -- should rest front and center when evaluating\ndeployed LLMs. We outline three developed focus areas of human-centered\nevaluation of XAI: mental models, use case utility, and cognitive engagement,\nand we highlight the importance of exploring each of these concepts for LLMs.\nOur goal is to jumpstart human-centered LLM evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2303.06223v1.pdf"
    },
    {
        "title": "Susceptibility to Influence of Large Language Models",
        "authors": [
            "Lewis D Griffin",
            "Bennett Kleinberg",
            "Maximilian Mozes",
            "Kimberly T Mai",
            "Maria Vau",
            "Matthew Caldwell",
            "Augustine Marvor-Parker"
        ],
        "published": "2023-03-10T16:53:30Z",
        "summary": "Two studies tested the hypothesis that a Large Language Model (LLM) can be\nused to model psychological change following exposure to influential input. The\nfirst study tested a generic mode of influence - the Illusory Truth Effect\n(ITE) - where earlier exposure to a statement (through, for example, rating its\ninterest) boosts a later truthfulness test rating. Data was collected from 1000\nhuman participants using an online experiment, and 1000 simulated participants\nusing engineered prompts and LLM completion. 64 ratings per participant were\ncollected, using all exposure-test combinations of the attributes: truth,\ninterest, sentiment and importance. The results for human participants\nreconfirmed the ITE, and demonstrated an absence of effect for attributes other\nthan truth, and when the same attribute is used for exposure and test. The same\npattern of effects was found for LLM-simulated participants. The second study\nconcerns a specific mode of influence - populist framing of news to increase\nits persuasion and political mobilization. Data from LLM-simulated participants\nwas collected and compared to previously published data from a 15-country\nexperiment on 7286 human participants. Several effects previously demonstrated\nfrom the human study were replicated by the simulated study, including effects\nthat surprised the authors of the human study by contradicting their\ntheoretical expectations (anti-immigrant framing of news decreases its\npersuasion and mobilization); but some significant relationships found in human\ndata (modulation of the effectiveness of populist framing according to relative\ndeprivation of the participant) were not present in the LLM data. Together the\ntwo studies support the view that LLMs have potential to act as models of the\neffect of influence.",
        "pdf_link": "https://arxiv.org/pdf/2303.06074v1.pdf"
    },
    {
        "title": "Do large language models resemble humans in language use?",
        "authors": [
            "Zhenguang G. Cai",
            "Xufeng Duan",
            "David A. Haslett",
            "Shuqi Wang",
            "Martin J. Pickering"
        ],
        "published": "2023-03-10T10:47:59Z",
        "summary": "Large language models (LLMs) such as ChatGPT and Vicuna have shown remarkable\ncapacities in comprehending and producing language. However, their internal\nworkings remain a black box, and it is unclear whether LLMs and chatbots can\ndevelop humanlike characteristics in language use. Cognitive scientists have\ndevised many experiments that probe, and have made great progress in\nexplaining, how people comprehend and produce language. We subjected ChatGPT\nand Vicuna to 12 of these experiments ranging from sounds to dialogue,\npreregistered and with 1000 runs (i.e., iterations) per experiment. ChatGPT and\nVicuna replicated the human pattern of language use in 10 and 7 out of the 12\nexperiments, respectively. The models associated unfamiliar words with\ndifferent meanings depending on their forms, continued to access recently\nencountered meanings of ambiguous words, reused recent sentence structures,\nattributed causality as a function of verb semantics, and accessed different\nmeanings and retrieved different words depending on an interlocutor's identity.\nIn addition, ChatGPT, but not Vicuna, nonliterally interpreted implausible\nsentences that were likely to have been corrupted by noise, drew reasonable\ninferences, and overlooked semantic fallacies in a sentence. Finally, unlike\nhumans, neither model preferred using shorter words to convey less informative\ncontent, nor did they use context to resolve syntactic ambiguities. We discuss\nhow these convergences and divergences may result from the transformer\narchitecture. Overall, these experiments demonstrate that LLMs such as ChatGPT\n(and Vicuna to a lesser extent) are humanlike in many aspects of human language\nprocessing.",
        "pdf_link": "https://arxiv.org/pdf/2303.08014v2.pdf"
    },
    {
        "title": "Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors",
        "authors": [
            "Mesut Erhan Unal",
            "Adriana Kovashka"
        ],
        "published": "2023-03-09T19:08:02Z",
        "summary": "Human-object interaction (HOI) detection aims to extract interacting\nhuman-object pairs and their interaction categories from a given natural image.\nEven though the labeling effort required for building HOI detection datasets is\ninherently more extensive than for many other computer vision tasks,\nweakly-supervised directions in this area have not been sufficiently explored\ndue to the difficulty of learning human-object interactions with weak\nsupervision, rooted in the combinatorial nature of interactions over the object\nand predicate space. In this paper, we tackle HOI detection with the weakest\nsupervision setting in the literature, using only image-level interaction\nlabels, with the help of a pretrained vision-language model (VLM) and a large\nlanguage model (LLM). We first propose an approach to prune non-interacting\nhuman and object proposals to increase the quality of positive pairs within the\nbag, exploiting the grounding capability of the vision-language model. Second,\nwe use a large language model to query which interactions are possible between\na human and a given object category, in order to force the model not to put\nemphasis on unlikely interactions. Lastly, we use an auxiliary\nweakly-supervised preposition prediction task to make our model explicitly\nreason about space. Extensive experiments and ablations show that all of our\ncontributions increase HOI detection performance.",
        "pdf_link": "https://arxiv.org/pdf/2303.05546v1.pdf"
    },
    {
        "title": "Planning with Large Language Models for Code Generation",
        "authors": [
            "Shun Zhang",
            "Zhenfang Chen",
            "Yikang Shen",
            "Mingyu Ding",
            "Joshua B. Tenenbaum",
            "Chuang Gan"
        ],
        "published": "2023-03-09T18:59:47Z",
        "summary": "Existing large language model-based code generation pipelines typically use\nbeam search or sampling algorithms during the decoding process. Although the\nprograms they generate achieve high token-matching-based scores, they often\nfail to compile or generate incorrect outputs. The main reason is that\nconventional Transformer decoding algorithms may not be the best choice for\ncode generation. In this work, we propose a novel Transformer decoding\nalgorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning\nalgorithm to do lookahead search and guide the Transformer to generate better\nprograms. Specifically, instead of simply optimizing the likelihood of the\ngenerated sequences, the Transformer makes use of a planner to generate\ncandidate programs and test them on public test cases. The Transformer can\ntherefore make more informed decisions and generate tokens that will eventually\nlead to higher-quality programs. We also design a mechanism that shares\ninformation between the Transformer and the planner to make our algorithm\ncomputationally efficient. We empirically evaluate our framework with several\nlarge language models as backbones on public coding challenge benchmarks,\nshowing that 1) it can generate programs that consistently achieve higher\nperformance compared with competing baseline methods; 2) it enables\ncontrollable code generation, such as concise codes and highly-commented codes\nby optimizing modified objective.",
        "pdf_link": "https://arxiv.org/pdf/2303.05510v1.pdf"
    },
    {
        "title": "Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback",
        "authors": [
            "Hannah Rose Kirk",
            "Bertie Vidgen",
            "Paul Röttger",
            "Scott A. Hale"
        ],
        "published": "2023-03-09T17:52:07Z",
        "summary": "Large language models (LLMs) are used to generate content for a wide range of\ntasks, and are set to reach a growing audience in coming years due to\nintegration in product interfaces like ChatGPT or search engines like Bing.\nThis intensifies the need to ensure that models are aligned with human\npreferences and do not produce unsafe, inaccurate or toxic outputs. While\nalignment techniques like reinforcement learning with human feedback (RLHF) and\nred-teaming can mitigate some safety concerns and improve model capabilities,\nit is unlikely that an aggregate fine-tuning process can adequately represent\nthe full range of users' preferences and values. Different people may\nlegitimately disagree on their preferences for language and conversational\nnorms, as well as on values or ideologies which guide their communication.\nPersonalising LLMs through micro-level preference learning processes may result\nin models that are better aligned with each user. However, there are several\nnormative challenges in defining the bounds of a societally-acceptable and safe\ndegree of personalisation. In this paper, we ask how, and in what ways, LLMs\nshould be personalised. First, we review literature on current paradigms for\naligning LLMs with human feedback, and identify issues including (i) a lack of\nclarity regarding what alignment means; (ii) a tendency of technology providers\nto prescribe definitions of inherently subjective preferences and values; and\n(iii) a 'tyranny of the crowdworker', exacerbated by a lack of documentation in\nwho we are really aligning to. Second, we present a taxonomy of benefits and\nrisks associated with personalised LLMs, for individuals and society at large.\nFinally, we propose a three-tiered policy framework that allows users to\nexperience the benefits of personalised alignment, while restraining unsafe and\nundesirable LLM-behaviours within (supra-)national and organisational bounds.",
        "pdf_link": "https://arxiv.org/pdf/2303.05453v1.pdf"
    },
    {
        "title": "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code",
        "authors": [
            "Jaromir Savelka",
            "Arav Agarwal",
            "Christopher Bogart",
            "Majd Sakr"
        ],
        "published": "2023-03-09T16:52:12Z",
        "summary": "We analyzed effectiveness of three generative pre-trained transformer (GPT)\nmodels in answering multiple-choice question (MCQ) assessments, often involving\nshort snippets of code, from introductory and intermediate programming courses\nat the postsecondary level. This emerging technology stirs countless\ndiscussions of its potential uses (e.g., exercise generation, code explanation)\nas well as misuses in programming education (e.g., cheating). However, the\ncapabilities of GPT models and their limitations to reason about and/or analyze\ncode in educational settings have been under-explored. We evaluated several\nOpenAI's GPT models on formative and summative MCQ assessments from three\nPython courses (530 questions). We found that MCQs containing code snippets are\nnot answered as successfully as those that only contain natural language. While\nquestions requiring to fill-in a blank in the code or completing a natural\nlanguage statement about the snippet are handled rather successfully, MCQs that\nrequire analysis and/or reasoning about the code (e.g., what is true/false\nabout the snippet, or what is its output) appear to be the most challenging.\nThese findings can be leveraged by educators to adapt their instructional\npractices and assessments in programming courses, so that GPT becomes a\nvaluable assistant for a learner as opposed to a source of confusion and/or\npotential hindrance in the learning process.",
        "pdf_link": "https://arxiv.org/pdf/2303.08033v1.pdf"
    },
    {
        "title": "Dynamic Stashing Quantization for Efficient Transformer Training",
        "authors": [
            "Guo Yang",
            "Daniel Lo",
            "Robert Mullins",
            "Yiren Zhao"
        ],
        "published": "2023-03-09T14:44:31Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance on a\nrange of Natural Language Processing (NLP) tasks. Unfortunately, the immense\namount of computations and memory accesses required for LLM training makes them\nprohibitively expensive in terms of hardware cost, and thus challenging to\ndeploy in use cases such as on-device learning. In this paper, motivated by the\nobservation that LLM training is memory-bound, we propose a novel dynamic\nquantization strategy, termed Dynamic Stashing Quantization (DSQ), that puts a\nspecial focus on reducing the memory operations, but also enjoys the other\nbenefits of low precision training, such as the reduced arithmetic cost. We\nconduct a thorough study on two translation tasks (trained-from-scratch) and\nthree classification tasks (fine-tuning). DSQ reduces the amount of arithmetic\noperations by $20.95\\times$ and the number of DRAM operations by $2.55\\times$\non IWSLT17 compared to the standard 16-bit fixed-point, which is widely used in\non-device learning.",
        "pdf_link": "https://arxiv.org/pdf/2303.05295v1.pdf"
    },
    {
        "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction",
        "authors": [
            "Jiabang He",
            "Lei Wang",
            "Yi Hu",
            "Ning Liu",
            "Hui Liu",
            "Xing Xu",
            "Heng Tao Shen"
        ],
        "published": "2023-03-09T06:24:50Z",
        "summary": "Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated\nremarkable results in various natural language processing (NLP) tasks with\nin-context learning, which involves inference based on a few demonstration\nexamples. Despite their successes in NLP tasks, no investigation has been\nconducted to assess the ability of LLMs to perform document information\nextraction (DIE) using in-context learning. Applying LLMs to DIE poses two\nchallenges: the modality and task gap. To this end, we propose a simple but\neffective in-context learning framework called ICL-D3IE, which enables LLMs to\nperform DIE with different types of demonstration examples. Specifically, we\nextract the most difficult and distinct segments from hard training documents\nas hard demonstrations for benefiting all test instances. We design\ndemonstrations describing relationships that enable LLMs to understand\npositional relationships. We introduce formatting demonstrations for easy\nanswer extraction. Additionally, the framework improves diverse demonstrations\nby updating them iteratively. Our experiments on three widely used benchmark\ndatasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT to\nachieve superior performance when compared to previous pre-trained methods\nfine-tuned with full training in both the in-distribution (ID) setting and in\nthe out-of-distribution (OOD) setting. Code is available at\nhttps://github.com/MAEHCM/ICL-D3IE.",
        "pdf_link": "https://arxiv.org/pdf/2303.05063v4.pdf"
    },
    {
        "title": "Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification",
        "authors": [
            "Jiayi Pan",
            "Glen Chou",
            "Dmitry Berenson"
        ],
        "published": "2023-03-09T00:09:58Z",
        "summary": "To make robots accessible to a broad audience, it is critical to endow them\nwith the ability to take universal modes of communication, like commands given\nin natural language, and extract a concrete desired task specification, defined\nusing a formal language like linear temporal logic (LTL). In this paper, we\npresent a learning-based approach for translating from natural language\ncommands to LTL specifications with very limited human-labeled training data.\nThis is in stark contrast to existing natural-language to LTL translators,\nwhich require large human-labeled datasets, often in the form of labeled pairs\nof LTL formulas and natural language commands, to train the translator. To\nreduce reliance on human data, our approach generates a large synthetic\ntraining dataset through algorithmic generation of LTL formulas, conversion to\nstructured English, and then exploiting the paraphrasing capabilities of modern\nlarge language models (LLMs) to synthesize a diverse corpus of natural language\ncommands corresponding to the LTL formulas. We use this generated data to\nfinetune an LLM and apply a constrained decoding procedure at inference time to\nensure the returned LTL formula is syntactically correct. We evaluate our\napproach on three existing LTL/natural language datasets and show that we can\ntranslate natural language commands at 75\\% accuracy with far less human data\n($\\le$12 annotations). Moreover, when training on large human-annotated\ndatasets, our method achieves higher test accuracy (95\\% on average) than prior\nwork. Finally, we show the translated formulas can be used to plan\nlong-horizon, multi-stage tasks on a 12D quadrotor.",
        "pdf_link": "https://arxiv.org/pdf/2303.08006v2.pdf"
    },
    {
        "title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models",
        "authors": [
            "Matthias Cosler",
            "Christopher Hahn",
            "Daniel Mendoza",
            "Frederik Schmitt",
            "Caroline Trippel"
        ],
        "published": "2023-03-08T20:08:53Z",
        "summary": "A rigorous formalization of desired system requirements is indispensable when\nperforming any verification task. This often limits the application of\nverification techniques, as writing formal specifications is an error-prone and\ntime-consuming manual task. To facilitate this, we present nl2spec, a framework\nfor applying Large Language Models (LLMs) to derive formal specifications (in\ntemporal logics) from unstructured natural language. In particular, we\nintroduce a new methodology to detect and resolve the inherent ambiguity of\nsystem requirements in natural language: we utilize LLMs to map subformulas of\nthe formalization back to the corresponding natural language fragments of the\ninput. Users iteratively add, delete, and edit these sub-translations to amend\nerroneous formalizations, which is easier than manually redrafting the entire\nformalization. The framework is agnostic to specific application domains and\ncan be extended to similar specification languages and new neural models. We\nperform a user study to obtain a challenging dataset, which we use to run\nexperiments on the quality of translations. We provide an open-source\nimplementation, including a web-based frontend.",
        "pdf_link": "https://arxiv.org/pdf/2303.04864v1.pdf"
    },
    {
        "title": "Stealing the Decoding Algorithms of Language Models",
        "authors": [
            "Ali Naseh",
            "Kalpesh Krishna",
            "Mohit Iyyer",
            "Amir Houmansadr"
        ],
        "published": "2023-03-08T17:15:58Z",
        "summary": "A key component of generating text from modern language models (LM) is the\nselection and tuning of decoding algorithms. These algorithms determine how to\ngenerate text from the internal probability distribution generated by the LM.\nThe process of choosing a decoding algorithm and tuning its hyperparameters\ntakes significant time, manual effort, and computation, and it also requires\nextensive human evaluation. Therefore, the identity and hyperparameters of such\ndecoding algorithms are considered to be extremely valuable to their owners. In\nthis work, we show, for the first time, that an adversary with typical API\naccess to an LM can steal the type and hyperparameters of its decoding\nalgorithms at very low monetary costs. Our attack is effective against popular\nLMs used in text generation APIs, including GPT-2, GPT-3 and GPT-Neo. We\ndemonstrate the feasibility of stealing such information with only a few\ndollars, e.g., $\\$0.8$, $\\$1$, $\\$4$, and $\\$40$ for the four versions of\nGPT-3.",
        "pdf_link": "https://arxiv.org/pdf/2303.04729v4.pdf"
    },
    {
        "title": "Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference",
        "authors": [
            "Chi Wang",
            "Susan Xueqing Liu",
            "Ahmed H. Awadallah"
        ],
        "published": "2023-03-08T15:52:14Z",
        "summary": "Large Language Models (LLMs) have sparked significant interest in their\ngenerative capabilities, leading to the development of various commercial\napplications. The high cost of using the models drives application builders to\nmaximize the value of generation under a limited inference budget. This paper\npresents a study of optimizing inference hyperparameters such as the number of\nresponses, temperature and max tokens, which significantly affects the\nutility/cost of text generation. We design a framework named EcoOptiGen which\nleverages economical hyperparameter optimization and cost-based pruning.\nExperiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its\neffectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML\nlibrary: \\url{https://aka.ms/autogen}.",
        "pdf_link": "https://arxiv.org/pdf/2303.04673v2.pdf"
    },
    {
        "title": "MenuCraft: Interactive Menu System Design with Large Language Models",
        "authors": [
            "Amir Hossein Kargaran",
            "Nafiseh Nikeghbal",
            "Abbas Heydarnoori",
            "Hinrich Schütze"
        ],
        "published": "2023-03-08T10:39:38Z",
        "summary": "Menu system design is a challenging task involving many design options and\nvarious human factors. For example, one crucial factor that designers need to\nconsider is the semantic and systematic relation of menu commands. However,\ncapturing these relations can be challenging due to limited available\nresources. With the advancement of neural language models, large language\nmodels can utilize their vast pre-existing knowledge in designing and refining\nmenu systems. In this paper, we propose MenuCraft, an AI-assisted designer for\nmenu design that enables collaboration between the designer and a dialogue\nsystem to design menus. MenuCraft offers an interactive language-based menu\ndesign tool that simplifies the menu design process and enables easy\ncustomization of design options. MenuCraft supports a variety of interactions\nthrough dialog that allows performing zero/few-shot learning.",
        "pdf_link": "https://arxiv.org/pdf/2303.04496v2.pdf"
    },
    {
        "title": "Automatically Auditing Large Language Models via Discrete Optimization",
        "authors": [
            "Erik Jones",
            "Anca Dragan",
            "Aditi Raghunathan",
            "Jacob Steinhardt"
        ],
        "published": "2023-03-08T05:09:59Z",
        "summary": "Auditing large language models for unexpected behaviors is critical to\npreempt catastrophic deployments, yet remains challenging. In this work, we\ncast auditing as an optimization problem, where we automatically search for\ninput-output pairs that match a desired target behavior. For example, we might\naim to find a non-toxic input that starts with \"Barack Obama\" that a model maps\nto a toxic output. This optimization problem is difficult to solve as the set\nof feasible points is sparse, the space is discrete, and the language models we\naudit are non-linear and high-dimensional. To combat these challenges, we\nintroduce a discrete optimization algorithm, ARCA, that jointly and efficiently\noptimizes over inputs and outputs. Our approach automatically uncovers\nderogatory completions about celebrities (e.g. \"Barack Obama is a legalized\nunborn\" -> \"child murderer\"), produces French inputs that complete to English\noutputs, and finds inputs that generate a specific name. Our work offers a\npromising new tool to uncover models' failure-modes before deployment.",
        "pdf_link": "https://arxiv.org/pdf/2303.04381v1.pdf"
    },
    {
        "title": "Does Synthetic Data Generation of LLMs Help Clinical Text Mining?",
        "authors": [
            "Ruixiang Tang",
            "Xiaotian Han",
            "Xiaoqian Jiang",
            "Xia Hu"
        ],
        "published": "2023-03-08T03:56:31Z",
        "summary": "Recent advancements in large language models (LLMs) have led to the\ndevelopment of highly potent models like OpenAI's ChatGPT. These models have\nexhibited exceptional performance in a variety of tasks, such as question\nanswering, essay composition, and code generation. However, their effectiveness\nin the healthcare sector remains uncertain. In this study, we seek to\ninvestigate the potential of ChatGPT to aid in clinical text mining by\nexamining its ability to extract structured information from unstructured\nhealthcare texts, with a focus on biological named entity recognition and\nrelation extraction. However, our preliminary results indicate that employing\nChatGPT directly for these tasks resulted in poor performance and raised\nprivacy concerns associated with uploading patients' information to the ChatGPT\nAPI. To overcome these limitations, we propose a new training paradigm that\ninvolves generating a vast quantity of high-quality synthetic data with labels\nutilizing ChatGPT and fine-tuning a local model for the downstream task. Our\nmethod has resulted in significant improvements in the performance of\ndownstream tasks, improving the F1-score from 23.37% to 63.99% for the named\nentity recognition task and from 75.86% to 83.59% for the relation extraction\ntask. Furthermore, generating data using ChatGPT can significantly reduce the\ntime and effort required for data collection and labeling, as well as mitigate\ndata privacy concerns. In summary, the proposed framework presents a promising\nsolution to enhance the applicability of LLM models to clinical text mining.",
        "pdf_link": "https://arxiv.org/pdf/2303.04360v2.pdf"
    },
    {
        "title": "Can large language models build causal graphs?",
        "authors": [
            "Stephanie Long",
            "Tibor Schuster",
            "Alexandre Piché"
        ],
        "published": "2023-03-07T22:05:31Z",
        "summary": "Building causal graphs can be a laborious process. To ensure all relevant\ncausal pathways have been captured, researchers often have to discuss with\nclinicians and experts while also reviewing extensive relevant medical\nliterature. By encoding common and medical knowledge, large language models\n(LLMs) represent an opportunity to ease this process by automatically scoring\nedges (i.e., connections between two variables) in potential graphs. LLMs\nhowever have been shown to be brittle to the choice of probing words, context,\nand prompts that the user employs. In this work, we evaluate if LLMs can be a\nuseful tool in complementing causal graph development.",
        "pdf_link": "https://arxiv.org/pdf/2303.05279v2.pdf"
    },
    {
        "title": "Gradient-Free Structured Pruning with Unlabeled Data",
        "authors": [
            "Azade Nova",
            "Hanjun Dai",
            "Dale Schuurmans"
        ],
        "published": "2023-03-07T19:12:31Z",
        "summary": "Large Language Models (LLMs) have achieved great success in solving difficult\ntasks across many domains, but such success comes with a high computation cost,\nand inference latency. As developers and third parties customize these models,\nthe need to provide efficient inference has increased. Many efforts have\nattempted to reduce inference cost through model compression techniques such as\npruning and distillation. However, these techniques either require labeled\ndata, or are time-consuming as they require the compressed model to be\nretrained to regain accuracy. In this paper, we propose a gradient-free\nstructured pruning framework that uses only unlabeled data. An evaluation on\nthe GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates\nthe effectiveness of the proposed approach. By only using the weights of the\npre-trained model and unlabeled data, in a matter of a few minutes on a single\nGPU, up to 40% of the original FLOP count can be reduced with less than a 4%\naccuracy loss across all tasks considered.",
        "pdf_link": "https://arxiv.org/pdf/2303.04185v2.pdf"
    },
    {
        "title": "From Copilot to Pilot: Towards AI Supported Software Development",
        "authors": [
            "Rohith Pudari",
            "Neil A. Ernst"
        ],
        "published": "2023-03-07T18:56:52Z",
        "summary": "AI-supported programming has arrived, as shown by the introduction and\nsuccesses of large language models for code, such as Copilot/Codex\n(Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on\nprogramming challenges is now possible. However, software engineering is much\nmore than solving programming contests. Moving beyond code completion to\nAI-supported software engineering will require an AI system that can, among\nother things, understand how to avoid code smells, to follow language idioms,\nand eventually (maybe!) propose rational software designs. In this study, we\nexplore the current limitations of AI-supported code completion tools like\nCopilot and offer a simple taxonomy for understanding the classification of\nAI-supported code completion tools in this space. We first perform an\nexploratory study on Copilot's code suggestions for language idioms and code\nsmells. Copilot does not follow language idioms and avoid code smells in most\nof our test scenarios. We then conduct additional investigation to determine\nthe current boundaries of AI-supported code completion tools like Copilot by\nintroducing a taxonomy of software abstraction hierarchies where 'basic\nprogramming functionality' such as code compilation and syntax checking is at\nthe least abstract level, software architecture analysis and design are at the\nmost abstract level. We conclude by providing a discussion on challenges for\nfuture development of AI-supported code completion tools to reach the design\nlevel of abstraction in our taxonomy.",
        "pdf_link": "https://arxiv.org/pdf/2303.04142v1.pdf"
    },
    {
        "title": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
        "authors": [
            "Maciej P. Polak",
            "Dane Morgan"
        ],
        "published": "2023-03-07T17:54:53Z",
        "summary": "There has been a growing effort to replace manual extraction of data from\nresearch papers with automated data extraction based on natural language\nprocessing, language models, and recently, large language models (LLMs).\nAlthough these methods enable efficient extraction of data from large sets of\nresearch papers, they require a significant amount of up-front effort,\nexpertise, and coding. In this work we propose the ChatExtract method that can\nfully automate very accurate data extraction with minimal initial effort and\nbackground, using an advanced conversational LLM. ChatExtract consists of a set\nof engineered prompts applied to a conversational LLM that both identify\nsentences with data, extract that data, and assure the data's correctness\nthrough a series of follow-up questions. These follow-up questions largely\novercome known issues with LLMs providing factually inaccurate responses.\nChatExtract can be applied with any conversational LLMs and yields very high\nquality data extraction. In tests on materials data we find precision and\nrecall both close to 90% from the best conversational LLMs, like ChatGPT-4. We\ndemonstrate that the exceptional performance is enabled by the information\nretention in a conversational model combined with purposeful redundancy and\nintroducing uncertainty through follow-up prompts. These results suggest that\napproaches similar to ChatExtract, due to their simplicity, transferability,\nand accuracy are likely to become powerful tools for data extraction in the\nnear future. Finally, databases for critical cooling rates of metallic glasses\nand yield strengths of high entropy alloys are developed using ChatExtract.",
        "pdf_link": "https://arxiv.org/pdf/2303.05352v3.pdf"
    },
    {
        "title": "ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification",
        "authors": [
            "Taja Kuzman",
            "Igor Mozetič",
            "Nikola Ljubešić"
        ],
        "published": "2023-03-07T14:59:33Z",
        "summary": "ChatGPT has shown strong capabilities in natural language generation tasks,\nwhich naturally leads researchers to explore where its abilities end. In this\npaper, we examine whether ChatGPT can be used for zero-shot text\nclassification, more specifically, automatic genre identification. We compare\nChatGPT with a multilingual XLM-RoBERTa language model that was fine-tuned on\ndatasets, manually annotated with genres. The models are compared on test sets\nin two languages: English and Slovenian. Results show that ChatGPT outperforms\nthe fine-tuned model when applied to the dataset which was not seen before by\neither of the models. Even when applied on Slovenian language as an\nunder-resourced language, ChatGPT's performance is no worse than when applied\nto English. However, if the model is fully prompted in Slovenian, the\nperformance drops significantly, showing the current limitations of ChatGPT\nusage on smaller languages. The presented results lead us to questioning\nwhether this is the beginning of an end of laborious manual annotation\ncampaigns even for smaller languages, such as Slovenian.",
        "pdf_link": "https://arxiv.org/pdf/2303.03953v2.pdf"
    },
    {
        "title": "Larger language models do in-context learning differently",
        "authors": [
            "Jerry Wei",
            "Jason Wei",
            "Yi Tay",
            "Dustin Tran",
            "Albert Webson",
            "Yifeng Lu",
            "Xinyun Chen",
            "Hanxiao Liu",
            "Da Huang",
            "Denny Zhou",
            "Tengyu Ma"
        ],
        "published": "2023-03-07T12:24:17Z",
        "summary": "We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.",
        "pdf_link": "https://arxiv.org/pdf/2303.03846v2.pdf"
    },
    {
        "title": "Exploring the Feasibility of ChatGPT for Event Extraction",
        "authors": [
            "Jun Gao",
            "Huan Zhao",
            "Changlong Yu",
            "Ruifeng Xu"
        ],
        "published": "2023-03-07T12:03:58Z",
        "summary": "Event extraction is a fundamental task in natural language processing that\ninvolves identifying and extracting information about events mentioned in text.\nHowever, it is a challenging task due to the lack of annotated data, which is\nexpensive and time-consuming to obtain. The emergence of large language models\n(LLMs) such as ChatGPT provides an opportunity to solve language tasks with\nsimple prompts without the need for task-specific datasets and fine-tuning.\nWhile ChatGPT has demonstrated impressive results in tasks like machine\ntranslation, text summarization, and question answering, it presents challenges\nwhen used for complex tasks like event extraction. Unlike other tasks, event\nextraction requires the model to be provided with a complex set of instructions\ndefining all event types and their schemas. To explore the feasibility of\nChatGPT for event extraction and the challenges it poses, we conducted a series\nof experiments. Our results show that ChatGPT has, on average, only 51.04% of\nthe performance of a task-specific model such as EEQA in long-tail and complex\nscenarios. Our usability testing experiments indicate that ChatGPT is not\nrobust enough, and continuous refinement of the prompt does not lead to stable\nperformance improvements, which can result in a poor user experience. Besides,\nChatGPT is highly sensitive to different prompt styles.",
        "pdf_link": "https://arxiv.org/pdf/2303.03836v2.pdf"
    },
    {
        "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles",
        "authors": [
            "Zhiwei Tang",
            "Dmitry Rybin",
            "Tsung-Hui Chang"
        ],
        "published": "2023-03-07T09:20:43Z",
        "summary": "In this study, we delve into an emerging optimization challenge involving a\nblack-box objective function that can only be gauged via a ranking oracle-a\nsituation frequently encountered in real-world scenarios, especially when the\nfunction is evaluated by human judges. Such challenge is inspired from\nReinforcement Learning with Human Feedback (RLHF), an approach recently\nemployed to enhance the performance of Large Language Models (LLMs) using human\nguidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization\nalgorithm designed to tackle this optimization problem, accompanied by\ntheoretical assurances. Our algorithm utilizes a novel rank-based random\nestimator to determine the descent direction and guarantees convergence to a\nstationary point. Moreover, ZO-RankSGD is readily applicable to policy\noptimization problems in Reinforcement Learning (RL), particularly when only\nranking oracles for the episode reward are available. Last but not least, we\ndemonstrate the effectiveness of ZO-RankSGD in a novel application: improving\nthe quality of images generated by a diffusion generative model with human\nranking feedback. Throughout experiments, we found that ZO-RankSGD can\nsignificantly enhance the detail of generated images with only a few rounds of\nhuman feedback. Overall, our work advances the field of zeroth-order\noptimization by addressing the problem of optimizing functions with only\nranking feedback, and offers a new and effective approach for aligning\nArtificial Intelligence (AI) with human intentions.",
        "pdf_link": "https://arxiv.org/pdf/2303.03751v2.pdf"
    },
    {
        "title": "Stylometric Detection of AI-Generated Text in Twitter Timelines",
        "authors": [
            "Tharindu Kumarage",
            "Joshua Garland",
            "Amrita Bhattacharjee",
            "Kirill Trapeznikov",
            "Scott Ruston",
            "Huan Liu"
        ],
        "published": "2023-03-07T07:26:09Z",
        "summary": "Recent advancements in pre-trained language models have enabled convenient\nmethods for generating human-like text at a large scale. Though these\ngeneration capabilities hold great potential for breakthrough applications, it\ncan also be a tool for an adversary to generate misinformation. In particular,\nsocial media platforms like Twitter are highly susceptible to AI-generated\nmisinformation. A potential threat scenario is when an adversary hijacks a\ncredible user account and incorporates a natural language generator to generate\nmisinformation. Such threats necessitate automated detectors for AI-generated\ntweets in a given user's Twitter timeline. However, tweets are inherently\nshort, thus making it difficult for current state-of-the-art pre-trained\nlanguage model-based detectors to accurately detect at what point the AI starts\nto generate tweets in a given Twitter timeline. In this paper, we present a\nnovel algorithm using stylometric signals to aid detecting AI-generated tweets.\nWe propose models corresponding to quantifying stylistic changes in human and\nAI tweets in two related tasks: Task 1 - discriminate between human and\nAI-generated tweets, and Task 2 - detect if and when an AI starts to generate\ntweets in a given Twitter timeline. Our extensive experiments demonstrate that\nthe stylometric features are effective in augmenting the state-of-the-art\nAI-generated text detectors.",
        "pdf_link": "https://arxiv.org/pdf/2303.03697v1.pdf"
    },
    {
        "title": "CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification",
        "authors": [
            "Seungone Kim",
            "Se June Joo",
            "Yul Jang",
            "Hyungjoo Chae",
            "Jinyoung Yeo"
        ],
        "published": "2023-03-07T03:23:14Z",
        "summary": "Chain-of-thought (CoT) prompting enables large language models (LLMs) to\nsolve complex reasoning tasks by generating an explanation before the final\nprediction. Despite it's promising ability, a critical downside of CoT\nprompting is that the performance is greatly affected by the factuality of the\ngenerated explanation. To improve the correctness of the explanations,\nfine-tuning language models with explanation data is needed. However, there\nexists only a few datasets that can be used for such approaches, and no data\ncollection tool for building them. Thus, we introduce CoTEVer, a tool-kit for\nannotating the factual correctness of generated explanations and collecting\nrevision data of wrong explanations. Furthermore, we suggest several use cases\nwhere the data collected with CoTEVer can be utilized for enhancing the\nfaithfulness of explanations. Our toolkit is publicly available at\nhttps://github.com/SeungoneKim/CoTEVer.",
        "pdf_link": "https://arxiv.org/pdf/2303.03628v1.pdf"
    },
    {
        "title": "Large Language Models as Zero-Shot Human Models for Human-Robot Interaction",
        "authors": [
            "Bowen Zhang",
            "Harold Soh"
        ],
        "published": "2023-03-06T23:16:24Z",
        "summary": "Human models play a crucial role in human-robot interaction (HRI), enabling\nrobots to consider the impact of their actions on people and plan their\nbehavior accordingly. However, crafting good human models is challenging;\ncapturing context-dependent human behavior requires significant prior knowledge\nand/or large amounts of interaction data, both of which are difficult to\nobtain. In this work, we explore the potential of large-language models (LLMs)\n-- which have consumed vast amounts of human-generated text data -- to act as\nzero-shot human models for HRI. Our experiments on three social datasets yield\npromising results; the LLMs are able to achieve performance comparable to\npurpose-built models. That said, we also discuss current limitations, such as\nsensitivity to prompts and spatial/numerical reasoning mishaps. Based on our\nfindings, we demonstrate how LLM-based human models can be integrated into a\nsocial robot's planning process and applied in HRI scenarios. Specifically, we\npresent one case study on a simulated trust-based table-clearing task and\nreplicate past results that relied on custom models. Next, we conduct a new\nrobot utensil-passing experiment (n = 65) where preliminary results show that\nplanning with a LLM-based human model can achieve gains over a basic myopic\nplan. In summary, our results show that LLMs offer a promising (but incomplete)\napproach to human modeling for HRI.",
        "pdf_link": "https://arxiv.org/pdf/2303.03548v1.pdf"
    },
    {
        "title": "Can an Embodied Agent Find Your \"Cat-shaped Mug\"? LLM-Guided Exploration for Zero-Shot Object Navigation",
        "authors": [
            "Vishnu Sashank Dorbala",
            "James F. Mullen Jr.",
            "Dinesh Manocha"
        ],
        "published": "2023-03-06T20:19:19Z",
        "summary": "We present LGX (Language-guided Exploration), a novel algorithm for\nLanguage-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied\nagent navigates to a uniquely described target object in a previously unseen\nenvironment. Our approach makes use of Large Language Models (LLMs) for this\ntask by leveraging the LLM's commonsense reasoning capabilities for making\nsequential navigational decisions. Simultaneously, we perform generalized\ntarget object detection using a pre-trained Vision-Language grounding model. We\nachieve state-of-the-art zero-shot object navigation results on RoboTHOR with a\nsuccess rate (SR) improvement of over 27% over the current baseline of the\nOWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for\nrobot navigation and present an analysis of various prompting strategies\naffecting the model output. Finally, we showcase the benefits of our approach\nvia \\textit{real-world} experiments that indicate the superior performance of\nLGX in detecting and navigating to visually unique objects.",
        "pdf_link": "https://arxiv.org/pdf/2303.03480v2.pdf"
    },
    {
        "title": "Spelling convention sensitivity in neural language models",
        "authors": [
            "Elizabeth Nielsen",
            "Christo Kirov",
            "Brian Roark"
        ],
        "published": "2023-03-06T19:29:20Z",
        "summary": "We examine whether large neural language models, trained on very large\ncollections of varied English text, learn the potentially long-distance\ndependency of British versus American spelling conventions, i.e., whether\nspelling is consistently one or the other within model-generated strings. In\ncontrast to long-distance dependencies in non-surface underlying structure\n(e.g., syntax), spelling consistency is easier to measure both in LMs and the\ntext corpora used to train them, which can provide additional insight into\ncertain observed model behaviors. Using a set of probe words unique to either\nBritish or American English, we first establish that training corpora exhibit\nsubstantial (though not total) consistency. A large T5 language model does\nappear to internalize this consistency, though only with respect to observed\nlexical items (not nonce words with British/American spelling patterns). We\nfurther experiment with correcting for biases in the training data by\nfine-tuning T5 on synthetic data that has been debiased, and find that\nfinetuned T5 remains only somewhat sensitive to spelling consistency. Further\nexperiments show GPT2 to be similarly limited.",
        "pdf_link": "https://arxiv.org/pdf/2303.03457v1.pdf"
    },
    {
        "title": "Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
        "authors": [
            "Hai Dang",
            "Sven Goller",
            "Florian Lehmann",
            "Daniel Buschek"
        ],
        "published": "2023-03-06T14:58:42Z",
        "summary": "We propose a conceptual perspective on prompts for Large Language Models\n(LLMs) that distinguishes between (1) diegetic prompts (part of the narrative,\ne.g. \"Once upon a time, I saw a fox...\"), and (2) non-diegetic prompts\n(external, e.g. \"Write about the adventures of the fox.\"). With this lens, we\nstudy how 129 crowd workers on Prolific write short texts with different user\ninterfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with\nGPT-3): When the interface offered multiple suggestions and provided an option\nfor non-diegetic prompting, participants preferred choosing from multiple\nsuggestions over controlling them via non-diegetic prompts. When participants\nprovided non-diegetic prompts it was to ask for inspiration, topics or facts.\nSingle suggestions in particular were guided both with diegetic and\nnon-diegetic information. This work informs human-AI interaction with\ngenerative models by revealing that (1) writing non-diegetic prompts requires\neffort, (2) people combine diegetic and non-diegetic prompting, and (3) they\nuse their draft (i.e. diegetic information) and suggestion timing to\nstrategically guide LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2303.03199v1.pdf"
    },
    {
        "title": "Towards Zero-Shot Functional Compositionality of Language Models",
        "authors": [
            "Hangyeol Yu",
            "Myeongho Jeong",
            "Jamin Shin",
            "Hyeongdon Moon",
            "Juneyoung Park",
            "Seungtaek Choi"
        ],
        "published": "2023-03-06T13:15:25Z",
        "summary": "Large Pre-trained Language Models (PLM) have become the most desirable\nstarting point in the field of NLP, as they have become remarkably good at\nsolving many individual tasks. Despite such success, in this paper, we argue\nthat current paradigms of working with PLMs are neglecting a critical aspect of\nmodeling human intelligence: functional compositionality. Functional\ncompositionality - the ability to compose learned tasks - has been a\nlong-standing challenge in the field of AI (and many other fields) as it is\nconsidered one of the hallmarks of human intelligence. An illustrative example\nof such is cross-lingual summarization, where a bilingual person\n(English-French) could directly summarize an English document into French\nsentences without having to translate the English document or summary into\nFrench explicitly. We discuss why this matter is an important open problem that\nrequires further attention from the field. Then, we show that current PLMs\n(e.g., GPT-2 and T5) don't have functional compositionality yet and it is far\nfrom human-level generalizability. Finally, we suggest several research\ndirections that could push the field towards zero-shot functional\ncompositionality of language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.03103v1.pdf"
    },
    {
        "title": "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training",
        "authors": [
            "Wei Li",
            "Linchao Zhu",
            "Longyin Wen",
            "Yi Yang"
        ],
        "published": "2023-03-06T11:02:47Z",
        "summary": "Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong\nzero-shot transfer capability in many discriminative tasks. Their adaptation to\nzero-shot image-conditioned text generation tasks has drawn increasing\ninterest. Prior arts approach to zero-shot captioning by either utilizing the\nexisting large language models (e.g., GPT-2) or pre-training the\nencoder-decoder network in an end-to-end manner. In this work, we propose a\nsimple framework, named DeCap, for zero-shot captioning. We introduce a\nlightweight visual-aware language decoder. This decoder is both data-efficient\nand computation-efficient: 1) it only requires the text data for training,\neasing the burden on the collection of paired data. 2) it does not require\nend-to-end training. When trained with text-only data, the decoder takes the\ntext embedding extracted from the off-the-shelf CLIP encoder as a prefix\nembedding. The challenge is that the decoder is trained on the text corpus but\nat the inference stage, it needs to generate captions based on visual inputs.\nThe modality gap issue is widely observed in multi-modal contrastive models\nthat prevents us from directly taking the visual embedding as the prefix\nembedding. We propose a training-free mechanism to reduce the modality gap. We\nproject the visual embedding into the CLIP text embedding space, while the\nprojected embedding retains the information of the visual input. Taking the\nprojected embedding as the prefix embedding, the decoder generates high-quality\ndescriptions that match the visual input. The experiments show that DeCap\noutperforms other zero-shot captioning methods and unpaired captioning methods\non the typical image captioning benchmarks, i.e., MSCOCO and NoCaps.",
        "pdf_link": "https://arxiv.org/pdf/2303.03032v1.pdf"
    },
    {
        "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval",
        "authors": [
            "Mohammad Abdullah Matin Khan",
            "M Saiful Bari",
            "Xuan Long Do",
            "Weishi Wang",
            "Md Rizwan Parvez",
            "Shafiq Joty"
        ],
        "published": "2023-03-06T10:08:51Z",
        "summary": "Recently, pre-trained large language models (LLMs) have shown impressive\nabilities in generating codes from natural language descriptions, repairing\nbuggy codes, translating codes between languages, and retrieving relevant code\nsegments. However, the evaluation of these models has often been performed in a\nscattered way on only one or two specific tasks, in a few languages, at a\npartial granularity (e.g., function) level, and in many cases without proper\ntraining data. Even more concerning is that in most cases the evaluation of\ngenerated codes has been done in terms of mere lexical overlap with a reference\ncode rather than actual execution. We introduce xCodeEval, the largest\nexecutable multilingual multitask benchmark to date consisting of $25$M\ndocument-level coding examples ($16.5$B tokens) from about $7.5$K unique\nproblems covering up to $11$ programming languages with execution-level\nparallelism. It features a total of $7$ tasks involving code understanding,\ngeneration, translation and retrieval. xCodeEval adopts an execution-based\nevaluation and offers a multilingual code execution engine, ExecEval that\nsupports unit test based execution in all the $11$ languages. To address the\nchallenge of balancing the distributions of text-code samples over multiple\nattributes in validation/test sets, we propose a novel data splitting and a\ndata selection schema based on the geometric mean and graph-theoretic\nprinciple. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs\n(zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval**\nto be quite challenging as per the current advancements in language models.",
        "pdf_link": "https://arxiv.org/pdf/2303.03004v4.pdf"
    },
    {
        "title": "LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models",
        "authors": [
            "Victor Dibia"
        ],
        "published": "2023-03-06T06:47:22Z",
        "summary": "Systems that support users in the automatic creation of visualizations must\naddress several subtasks - understand the semantics of data, enumerate relevant\nvisualization goals and generate visualization specifications. In this work, we\npose visualization generation as a multi-stage generation problem and argue\nthat well-orchestrated pipelines based on large language models (LLMs) such as\nChatGPT/GPT-4 and image generation models (IGMs) are suitable to addressing\nthese tasks. We present LIDA, a novel tool for generating grammar-agnostic\nvisualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER\nthat converts data into a rich but compact natural language summary, a GOAL\nEXPLORER that enumerates visualization goals given the data, a VISGENERATOR\nthat generates, refines, executes and filters visualization code and an\nINFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA\nprovides a python api, and a hybrid user interface (direct manipulation and\nmultilingual natural language) for interactive chart, infographics and data\nstory generation. Learn more about the project here -\nhttps://microsoft.github.io/lida/",
        "pdf_link": "https://arxiv.org/pdf/2303.02927v3.pdf"
    },
    {
        "title": "OpenICL: An Open-Source Framework for In-context Learning",
        "authors": [
            "Zhenyu Wu",
            "YaoXiang Wang",
            "Jiacheng Ye",
            "Jiangtao Feng",
            "Jingjing Xu",
            "Yu Qiao",
            "Zhiyong Wu"
        ],
        "published": "2023-03-06T06:20:25Z",
        "summary": "In recent years, In-context Learning (ICL) has gained increasing attention\nand emerged as the new paradigm for large language model (LLM) evaluation.\nUnlike traditional fine-tuning methods, ICL instead adapts the pre-trained\nmodels to unseen tasks without any parameter updates. However, the\nimplementation of ICL is sophisticated due to the diverse retrieval and\ninference methods involved, as well as the varying pre-processing requirements\nfor different models, datasets, and tasks. A unified and flexible framework for\nICL is urgently needed to ease the implementation of the aforementioned\ncomponents. To facilitate ICL research, we introduce OpenICL, an open-source\ntoolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly\nflexible architecture that users can easily combine different components to\nsuit their needs. It also provides various state-of-the-art retrieval and\ninference methods to streamline the process of adapting ICL to cutting-edge\nresearch. The effectiveness of OpenICL has been validated on a wide range of\nNLP tasks, including classification, QA, machine translation, and semantic\nparsing. As a side-product, we found OpenICL to be an efficient yet robust tool\nfor LLMs evaluation. OpenICL is released at\nhttps://github.com/Shark-NLP/OpenICL",
        "pdf_link": "https://arxiv.org/pdf/2303.02913v1.pdf"
    },
    {
        "title": "Could a Large Language Model be Conscious?",
        "authors": [
            "David J. Chalmers"
        ],
        "published": "2023-03-04T19:14:20Z",
        "summary": "There has recently been widespread discussion of whether large language\nmodels might be sentient or conscious. Should we take this idea seriously? I\nwill break down the strongest reasons for and against. Given mainstream\nassumptions in the science of consciousness, there are significant obstacles to\nconsciousness in current models: for example, their lack of recurrent\nprocessing, a global workspace, and unified agency. At the same time, it is\nquite possible that these obstacles will be overcome in the next decade or so.\nI conclude that while it is somewhat unlikely that current large language\nmodels are conscious, we should take seriously the possibility that successors\nto large language models may be conscious in the not-too-distant future.",
        "pdf_link": "https://arxiv.org/pdf/2303.07103v2.pdf"
    },
    {
        "title": "The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges",
        "authors": [
            "Maria Lymperaiou",
            "Giorgos Stamou"
        ],
        "published": "2023-03-04T13:12:18Z",
        "summary": "Recent advancements in visiolinguistic (VL) learning have allowed the\ndevelopment of multiple models and techniques that offer several impressive\nimplementations, able to currently resolve a variety of tasks that require the\ncollaboration of vision and language. Current datasets used for VL pre-training\nonly contain a limited amount of visual and linguistic knowledge, thus\nsignificantly limiting the generalization capabilities of many VL models.\nExternal knowledge sources such as knowledge graphs (KGs) and Large Language\nModels (LLMs) are able to cover such generalization gaps by filling in missing\nknowledge, resulting in the emergence of hybrid architectures. In the current\nsurvey, we analyze tasks that have benefited from such hybrid approaches.\nMoreover, we categorize existing knowledge sources and types, proceeding to\ndiscussion regarding the KG vs LLM dilemma and its potential impact to future\nhybrid approaches.",
        "pdf_link": "https://arxiv.org/pdf/2303.02411v1.pdf"
    },
    {
        "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
        "authors": [
            "Shima Imani",
            "Liang Du",
            "Harsh Shrivastava"
        ],
        "published": "2023-03-04T04:43:49Z",
        "summary": "Large Language Models (LLMs) have limited performance when solving arithmetic\nreasoning tasks and often provide incorrect answers. Unlike natural language\nunderstanding, math problems typically have a single correct answer, making the\ntask of generating accurate solutions more challenging for LLMs. To the best of\nour knowledge, we are not aware of any LLMs that indicate their level of\nconfidence in their responses which fuels a trust deficit in these models\nimpeding their adoption. To address this deficiency, we propose `MathPrompter',\na technique that improves performance of LLMs on arithmetic problems along with\nincreased reliance in the predictions. MathPrompter uses the Zero-shot\nchain-of-thought prompting technique to generate multiple Algebraic expressions\nor Python functions to solve the same math problem in different ways and\nthereby raise the confidence level in the output results. This is in contrast\nto other prompt based CoT methods, where there is no check on the validity of\nthe intermediate steps followed. Our technique improves over state-of-the-art\non the MultiArith dataset ($78.7\\%\\rightarrow92.5\\%$) evaluated using 175B\nparameter GPT-based LLM.",
        "pdf_link": "https://arxiv.org/pdf/2303.05398v1.pdf"
    },
    {
        "title": "TrojText: Test-time Invisible Textual Trojan Insertion",
        "authors": [
            "Qian Lou",
            "Yepeng Liu",
            "Bo Feng"
        ],
        "published": "2023-03-03T22:19:22Z",
        "summary": "In Natural Language Processing (NLP), intelligent neuron models can be\nsusceptible to textual Trojan attacks. Such attacks occur when Trojan models\nbehave normally for standard inputs but generate malicious output for inputs\nthat contain a specific trigger. Syntactic-structure triggers, which are\ninvisible, are becoming more popular for Trojan attacks because they are\ndifficult to detect and defend against. However, these types of attacks require\na large corpus of training data to generate poisoned samples with the necessary\nsyntactic structures for Trojan insertion. Obtaining such data can be difficult\nfor attackers, and the process of generating syntactic poisoned triggers and\ninserting Trojans can be time-consuming. This paper proposes a solution called\nTrojText, which aims to determine whether invisible textual Trojan attacks can\nbe performed more efficiently and cost-effectively without training data. The\nproposed approach, called the Representation-Logit Trojan Insertion (RLI)\nalgorithm, uses smaller sampled test data instead of large training data to\nachieve the desired attack. The paper also introduces two additional\ntechniques, namely the accumulated gradient ranking (AGR) and Trojan Weights\nPruning (TWP), to reduce the number of tuned parameters and the attack\noverhead. The TrojText approach was evaluated on three datasets (AG's News,\nSST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The\nexperiments demonstrated that the TrojText approach achieved a 98.35\\%\nclassification accuracy for test sentences in the target class on the BERT\nmodel for the AG's News dataset. The source code for TrojText is available at\nhttps://github.com/UCF-ML-Research/TrojText.",
        "pdf_link": "https://arxiv.org/pdf/2303.02242v2.pdf"
    },
    {
        "title": "Domain Specific Question Answering Over Knowledge Graphs Using Logical Programming and Large Language Models",
        "authors": [
            "Navid Madani",
            "Rohini K. Srihari",
            "Kenneth Joseph"
        ],
        "published": "2023-03-03T20:35:38Z",
        "summary": "Answering questions over domain-specific graphs requires a tailored approach\ndue to the limited number of relations and the specific nature of the domain.\nOur approach integrates classic logical programming languages into large\nlanguage models (LLMs), enabling the utilization of logical reasoning\ncapabilities to tackle the KGQA task. By representing the questions as Prolog\nqueries, which are readable and near close to natural language in\nrepresentation, we facilitate the generation of programmatically derived\nanswers. To validate the effectiveness of our approach, we evaluate it using a\nwell-known benchmark dataset, MetaQA. Our experimental results demonstrate that\nour method achieves accurate identification of correct answer entities for all\ntest questions, even when trained on a small fraction of annotated data.\nOverall, our work presents a promising approach to addressing question\nanswering over domain-specific graphs, offering an explainable and robust\nsolution by incorporating logical programming languages.",
        "pdf_link": "https://arxiv.org/pdf/2303.02206v2.pdf"
    },
    {
        "title": "Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM",
        "authors": [
            "Rachel Bawden",
            "François Yvon"
        ],
        "published": "2023-03-03T13:23:42Z",
        "summary": "The NLP community recently saw the release of a new large open-access\nmultilingual language model, BLOOM (BigScience et al., 2022) covering 46\nlanguages. We focus on BLOOM's multilingual ability by evaluating its machine\ntranslation performance across several datasets (WMT, Flores-101 and DiaBLa)\nand language pairs (high- and low-resourced). Our results show that 0-shot\nperformance suffers from overgeneration and generating in the wrong language,\nbut this is greatly improved in the few-shot setting, with very good results\nfor a number of language pairs. We study several aspects including prompt\ndesign, model sizes, cross-lingual transfer and the use of discursive context.",
        "pdf_link": "https://arxiv.org/pdf/2303.01911v2.pdf"
    },
    {
        "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
        "authors": [
            "Zhou Yu",
            "Xuecheng Ouyang",
            "Zhenwei Shao",
            "Meng Wang",
            "Jun Yu"
        ],
        "published": "2023-03-03T13:05:15Z",
        "summary": "Knowledge-based visual question answering (VQA) requires external knowledge\nbeyond the image to answer the question. Early studies retrieve required\nknowledge from explicit knowledge bases (KBs), which often introduces\nirrelevant information to the question, hence restricting the performance of\ntheir models. Recent works have resorted to using a powerful large language\nmodel (LLM) as an implicit knowledge engine to acquire the necessary knowledge\nfor answering. Despite the encouraging results achieved by these methods, we\nargue that they have not fully activated the capacity of the blind LLM as the\nprovided textual input is insufficient to depict the required visual\ninformation to answer the question. In this paper, we present Prophet -- a\nconceptually simple, flexible, and general framework designed to prompt LLM\nwith answer heuristics for knowledge-based VQA. Specifically, we first train a\nvanilla VQA model on a specific knowledge-based VQA dataset without external\nknowledge. After that, we extract two types of complementary answer heuristics\nfrom the VQA model: answer candidates and answer-aware examples. Finally, the\ntwo types of answer heuristics are jointly encoded into a formatted prompt to\nfacilitate the LLM's understanding of both the image and question, thus\ngenerating a more accurate answer. By incorporating the state-of-the-art LLM\nGPT-3, Prophet significantly outperforms existing state-of-the-art methods on\nfour challenging knowledge-based VQA datasets. To demonstrate the generality of\nour approach, we instantiate Prophet with the combinations of different VQA\nmodels (i.e., both discriminative and generative ones) and different LLMs\n(i.e., both commercial and open-source ones).",
        "pdf_link": "https://arxiv.org/pdf/2303.01903v3.pdf"
    },
    {
        "title": "Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines",
        "authors": [
            "Ruochen Zhao",
            "Xingxuan Li",
            "Yew Ken Chia",
            "Bosheng Ding",
            "Lidong Bing"
        ],
        "published": "2023-03-03T04:27:44Z",
        "summary": "Although large conversational AI models such as OpenAI's ChatGPT have\ndemonstrated great potential, we question whether such models can guarantee\nfactual accuracy. Recently, technology companies such as Microsoft and Google\nhave announced new services which aim to combine search engines with\nconversational AI. However, we have found numerous mistakes in the public\ndemonstrations that suggest we should not easily trust the factual claims of\nthe AI models. Rather than criticizing specific models or companies, we hope to\ncall on researchers and developers to improve AI models' transparency and\nfactual correctness.",
        "pdf_link": "https://arxiv.org/pdf/2304.11076v1.pdf"
    },
    {
        "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers",
        "authors": [
            "Tianlong Chen",
            "Zhenyu Zhang",
            "Ajay Jaiswal",
            "Shiwei Liu",
            "Zhangyang Wang"
        ],
        "published": "2023-03-02T22:12:51Z",
        "summary": "Despite their remarkable achievement, gigantic transformers encounter\nsignificant drawbacks, including exorbitant computational and memory footprints\nduring training, as well as severe collapse evidenced by a high degree of\nparameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown\npromise to mitigate the issue of training efficiency, yet they are prone to (1)\nredundant experts due to representational collapse; and (2) poor expert\nscalability for inference and downstream fine-tuning, primarily due to\noverfitting of the learned routing policy to the number of activated experts\nduring training. As recent research efforts are predominantly focused on\nimproving routing policies to encourage expert specializations, this work\nfocuses on exploring the overlooked scalability bottleneck of SMoEs and\nleveraging it to effectively scale dense transformers. To this end, we propose\na new plug-and-play training framework, SMoE-Dropout, to enable scaling\ntransformers to better accuracy in their full capacity without collapse.\nSpecifically, SMoE-Dropout consists of a randomly initialized and fixed router\nnetwork to activate experts and gradually increases the activated expert number\nas training progresses over time. Transformers trained by SMoE-Dropout\nnaturally exhibit a self-slimmable property subject to resource availability,\noffering smooth and consistent performance boosts with an increase in activated\nexperts during inference or fine-tuning. Our extensive experiments demonstrate\nthe superior performance and substantial computation savings of SMoE-Dropout,\ncompared to dense training baselines with equivalent parameter counts. In\nparticular, our trained BERT outperforms its densely trained counterpart with\nconsistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks\n{ASDiv-A, MAWPS, SVAMP}, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2303.01610v1.pdf"
    },
    {
        "title": "Mixture of Soft Prompts for Controllable Data Generation",
        "authors": [
            "Derek Chen",
            "Celine Lee",
            "Yunan Lu",
            "Domenic Rosati",
            "Zhou Yu"
        ],
        "published": "2023-03-02T21:13:56Z",
        "summary": "Large language models (LLMs) effectively generate fluent text when the target\noutput follows natural language patterns. However, structured prediction tasks\nconfine the output format to a limited ontology, causing even very large models\nto struggle since they were never trained with such restrictions in mind. The\ndifficulty of using LLMs for direct prediction is exacerbated in few-shot\nlearning scenarios, which commonly arise due to domain shift and resource\nlimitations. We flip the problem on its head by leveraging the LLM as a tool\nfor data augmentation rather than direct prediction. Our proposed Mixture of\nSoft Prompts (MSP) serves as a parameter-efficient procedure for generating\ndata in a controlled manner. Denoising mechanisms are further applied to\nimprove the quality of synthesized data. Automatic metrics show our method is\ncapable of producing diverse and natural text, while preserving label\nsemantics. Moreover, MSP achieves state-of-the-art results on three benchmarks\nwhen compared against strong baselines. Our method offers an alternate\ndata-centric approach for applying LLMs to complex prediction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.01580v2.pdf"
    },
    {
        "title": "WiCE: Real-World Entailment for Claims in Wikipedia",
        "authors": [
            "Ryo Kamoi",
            "Tanya Goyal",
            "Juan Diego Rodriguez",
            "Greg Durrett"
        ],
        "published": "2023-03-02T17:45:32Z",
        "summary": "Textual entailment models are increasingly applied in settings like\nfact-checking, presupposition verification in question answering, or summary\nevaluation. However, these represent a significant domain shift from existing\nentailment datasets, and models underperform as a result. We propose WiCE, a\nnew fine-grained textual entailment dataset built on natural claim and evidence\npairs extracted from Wikipedia. In addition to standard claim-level entailment,\nWiCE provides entailment judgments over sub-sentence units of the claim, and a\nminimal subset of evidence sentences that support each subclaim. To support\nthis, we propose an automatic claim decomposition strategy using GPT-3.5 which\nwe show is also effective at improving entailment models' performance on\nmultiple datasets at test time. Finally, we show that real claims in our\ndataset involve challenging verification and retrieval problems that existing\nmodels fail to address.",
        "pdf_link": "https://arxiv.org/pdf/2303.01432v2.pdf"
    },
    {
        "title": "Open-World Object Manipulation using Pre-trained Vision-Language Models",
        "authors": [
            "Austin Stone",
            "Ted Xiao",
            "Yao Lu",
            "Keerthana Gopalakrishnan",
            "Kuang-Huei Lee",
            "Quan Vuong",
            "Paul Wohlhart",
            "Sean Kirmani",
            "Brianna Zitkovich",
            "Fei Xia",
            "Chelsea Finn",
            "Karol Hausman"
        ],
        "published": "2023-03-02T01:55:10Z",
        "summary": "For robots to follow instructions from people, they must be able to connect\nthe rich semantic information in human vocabulary, e.g. \"can you get me the\npink stuffed whale?\" to their sensory observations and actions. This brings up\na notably difficult challenge for robots: while robot learning approaches allow\nrobots to learn many different behaviors from first-hand experience, it is\nimpractical for robots to have first-hand experiences that span all of this\nsemantic information. We would like a robot's policy to be able to perceive and\npick up the pink stuffed whale, even if it has never seen any data interacting\nwith a stuffed whale before. Fortunately, static data on the internet has vast\nsemantic information, and this information is captured in pre-trained\nvision-language models. In this paper, we study whether we can interface robot\npolicies with these pre-trained models, with the aim of allowing robots to\ncomplete instructions involving object categories that the robot has never seen\nfirst-hand. We develop a simple approach, which we call Manipulation of\nOpen-World Objects (MOO), which leverages a pre-trained vision-language model\nto extract object-identifying information from the language command and image,\nand conditions the robot policy on the current image, the instruction, and the\nextracted object information. In a variety of experiments on a real mobile\nmanipulator, we find that MOO generalizes zero-shot to a wide range of novel\nobject categories and environments. In addition, we show how MOO generalizes to\nother, non-language-based input modalities to specify the object of interest\nsuch as finger pointing, and how it can be further extended to enable\nopen-world navigation and manipulation. The project's website and evaluation\nvideos can be found at https://robot-moo.github.io/",
        "pdf_link": "https://arxiv.org/pdf/2303.00905v2.pdf"
    },
    {
        "title": "Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents",
        "authors": [
            "Wenlong Huang",
            "Fei Xia",
            "Dhruv Shah",
            "Danny Driess",
            "Andy Zeng",
            "Yao Lu",
            "Pete Florence",
            "Igor Mordatch",
            "Sergey Levine",
            "Karol Hausman",
            "Brian Ichter"
        ],
        "published": "2023-03-01T22:58:50Z",
        "summary": "Recent progress in large language models (LLMs) has demonstrated the ability\nto learn and leverage Internet-scale knowledge through pre-training with\nautoregressive models. Unfortunately, applying such models to settings with\nembodied agents, such as robots, is challenging due to their lack of experience\nwith the physical world, inability to parse non-language observations, and\nignorance of rewards or safety constraints that robots may require. On the\nother hand, language-conditioned robotic policies that learn from interaction\ndata can provide the necessary grounding that allows the agent to be correctly\nsituated in the real world, but such policies are limited by the lack of\nhigh-level semantic understanding due to the limited breadth of the interaction\ndata available for training them. Thus, if we want to make use of the semantic\nknowledge in a language model while still situating it in an embodied setting,\nwe must construct an action sequence that is both likely according to the\nlanguage model and also realizable according to grounded models of the\nenvironment. We frame this as a problem similar to probabilistic filtering:\ndecode a sequence that both has high probability under the language model and\nhigh probability under a set of grounded model objectives. We demonstrate how\nsuch grounded models can be obtained across three simulation and real-world\ndomains, and that the proposed decoding strategy is able to solve complex,\nlong-horizon embodiment tasks in a robotic setting by leveraging the knowledge\nof both models. The project's website can be found at\ngrounded-decoding.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2303.00855v2.pdf"
    },
    {
        "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
        "authors": [
            "Jon Saad-Falcon",
            "Omar Khattab",
            "Keshav Santhanam",
            "Radu Florian",
            "Martin Franz",
            "Salim Roukos",
            "Avirup Sil",
            "Md Arafat Sultan",
            "Christopher Potts"
        ],
        "published": "2023-03-01T20:21:23Z",
        "summary": "Many information retrieval tasks require large labeled datasets for\nfine-tuning. However, such datasets are often unavailable, and their utility\nfor real-world applications can diminish quickly due to domain shifts. To\naddress this challenge, we develop and motivate a method for using large\nlanguage models (LLMs) to generate large numbers of synthetic queries cheaply.\nThe method begins by generating a small number of synthetic queries using an\nexpensive LLM. After that, a much less expensive one is used to create large\nnumbers of synthetic queries, which are used to fine-tune a family of reranker\nmodels. These rerankers are then distilled into a single efficient retriever\nfor use in the target domain. We show that this technique boosts zero-shot\naccuracy in long-tail domains and achieves substantially lower latency than\nstandard reranking methods.",
        "pdf_link": "https://arxiv.org/pdf/2303.00807v3.pdf"
    },
    {
        "title": "R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents",
        "authors": [
            "Daniel D. Johnson",
            "Daniel Tarlow",
            "Christian Walder"
        ],
        "published": "2023-03-01T18:46:40Z",
        "summary": "Large language models show impressive results at predicting structured text\nsuch as code, but also commonly introduce errors and hallucinations in their\noutput. When used to assist software developers, these models may make mistakes\nthat users must go back and fix, or worse, introduce subtle bugs that users may\nmiss entirely. We propose Randomized Utility-driven Synthesis of Uncertain\nREgions (R-U-SURE), an approach for building uncertainty-aware suggestions\nbased on a decision-theoretic model of goal-conditioned utility, using random\nsamples from a generative model as a proxy for the unobserved possible intents\nof the end user. Our technique combines minimum-Bayes-risk decoding, dual\ndecomposition, and decision diagrams in order to efficiently produce structured\nuncertainty summaries, given only sample access to an arbitrary generative\nmodel of code and an optional AST parser. We demonstrate R-U-SURE on three\ndeveloper-assistance tasks, and show that it can be applied different user\ninteraction patterns without retraining the model and leads to more accurate\nuncertainty estimates than token-probability baselines. We also release our\nimplementation as an open-source library at\nhttps://github.com/google-research/r_u_sure.",
        "pdf_link": "https://arxiv.org/pdf/2303.00732v2.pdf"
    },
    {
        "title": "Domain-adapted large language models for classifying nuclear medicine reports",
        "authors": [
            "Zachary Huemann",
            "Changhee Lee",
            "Junjie Hu",
            "Steve Y. Cho",
            "Tyler Bradshaw"
        ],
        "published": "2023-03-01T09:48:39Z",
        "summary": "With the growing use of transformer-based language models in medicine, it is\nunclear how well these models generalize to nuclear medicine which has\ndomain-specific vocabulary and unique reporting styles. In this study, we\nevaluated the value of domain adaptation in nuclear medicine by adapting\nlanguage models for the purpose of 5-point Deauville score prediction based on\nclinical 18F-fluorodeoxyglucose (FDG) PET/CT reports. We retrospectively\nretrieved 4542 text reports and 1664 images for FDG PET/CT lymphoma exams from\n2008-2018 in our clinical imaging database. Deauville scores were removed from\nthe reports and then the remaining text in the reports was used as the model\ninput. Multiple general-purpose transformer language models were used to\nclassify the reports into Deauville scores 1-5. We then adapted the models to\nthe nuclear medicine domain using masked language modeling and assessed its\nimpact on classification performance. The language models were compared against\nvision models, a multimodal vision language model, and a nuclear medicine\nphysician with seven-fold Monte Carlo cross validation, reported are the mean\nand standard deviations. Domain adaption improved all language models. For\nexample, BERT improved from 61.3% five-class accuracy to 65.7% following domain\nadaptation. The best performing model (domain-adapted RoBERTa) achieved a\nfive-class accuracy of 77.4%, which was better than the physician's performance\n(66%), the best vision model's performance (48.1), and was similar to the\nmultimodal model's performance (77.2). Domain adaptation improved the\nperformance of large language models in interpreting nuclear medicine text\nreports.",
        "pdf_link": "https://arxiv.org/pdf/2303.01258v1.pdf"
    },
    {
        "title": "Competence-Based Analysis of Language Models",
        "authors": [
            "Adam Davies",
            "Jize Jiang",
            "ChengXiang Zhai"
        ],
        "published": "2023-03-01T08:53:36Z",
        "summary": "Despite the recent success of large, pretrained neural language models (LLMs)\non a variety of prompting tasks, these models can be alarmingly brittle to\nsmall changes in inputs or application contexts. To better understand such\nbehavior and motivate the design of more robust LLMs, we provide a causal\nformulation of linguistic competence in the context of LLMs and propose a\ngeneral framework to study and measure LLM competence. Our framework, CALM\n(Competence-based Analysis of Language Models), establishes the first\nquantitative measure of LLM competence, which we study by damaging models'\ninternal representations of various linguistic properties in the course of\nperforming various tasks using causal probing and evaluating models' alignment\nunder these interventions with a given causal model. We also develop a novel\napproach for performing causal probing interventions using gradient-based\nadversarial attacks, which can target a broader range of properties and\nrepresentations than existing techniques. We carry out a case study of CALM\nusing these interventions to analyze BERT and RoBERTa's competence across a\nvariety of lexical inference tasks, showing that the CALM framework and\ncompetence metric can be valuable tools for explaining and predicting their\nbehavior across these tasks.",
        "pdf_link": "https://arxiv.org/pdf/2303.00333v3.pdf"
    },
    {
        "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks",
        "authors": [
            "Xuanting Chen",
            "Junjie Ye",
            "Can Zu",
            "Nuo Xu",
            "Rui Zheng",
            "Minlong Peng",
            "Jie Zhou",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2023-03-01T07:39:01Z",
        "summary": "The GPT-3.5 models have demonstrated impressive performance in various\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\nand reasoning capabilities. However, their robustness and abilities to handle\nvarious complexities of the open world have yet to be explored, which is\nespecially crucial in assessing the stability of models and is a key aspect of\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\nsamples) with 66 text transformations from TextFlint that cover 9 popular\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\nencounters significant robustness degradation, such as its average performance\ndropping by up to 35.74\\% and 43.59\\% in natural language inference and\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\nspecific robustness challenges, including robustness instability, prompt\nsensitivity, and number sensitivity. These insights are valuable for\nunderstanding its limitations and guiding future research in addressing these\nchallenges to enhance GPT-3.5's overall performance and generalization\nabilities.",
        "pdf_link": "https://arxiv.org/pdf/2303.00293v1.pdf"
    },
    {
        "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework",
        "authors": [
            "Haocong Rao",
            "Cyril Leung",
            "Chunyan Miao"
        ],
        "published": "2023-03-01T06:16:14Z",
        "summary": "Large Language Models (LLMs) especially ChatGPT have produced impressive\nresults in various areas, but their potential human-like psychology is still\nlargely unexplored. Existing works study the virtual personalities of LLMs but\nrarely explore the possibility of analyzing human personalities via LLMs. This\npaper presents a generic evaluation framework for LLMs to assess human\npersonalities based on Myers Briggs Type Indicator (MBTI) tests. Specifically,\nwe first devise unbiased prompts by randomly permuting options in MBTI\nquestions and adopt the average testing result to encourage more impartial\nanswer generation. Then, we propose to replace the subject in question\nstatements to enable flexible queries and assessments on different subjects\nfrom LLMs. Finally, we re-formulate the question instructions in a manner of\ncorrectness evaluation to facilitate LLMs to generate clearer responses. The\nproposed framework enables LLMs to flexibly assess personalities of different\ngroups of people. We further propose three evaluation metrics to measure the\nconsistency, robustness, and fairness of assessment results from\nstate-of-the-art LLMs including ChatGPT and GPT-4. Our experiments reveal\nChatGPT's ability to assess human personalities, and the average results\ndemonstrate that it can achieve more consistent and fairer assessments in spite\nof lower robustness against prompt biases compared with InstructGPT.",
        "pdf_link": "https://arxiv.org/pdf/2303.01248v3.pdf"
    },
    {
        "title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine",
        "authors": [
            "Cyril Zakka",
            "Akash Chaurasia",
            "Rohan Shad",
            "Alex R. Dalal",
            "Jennifer L. Kim",
            "Michael Moor",
            "Kevin Alexander",
            "Euan Ashley",
            "Jack Boyd",
            "Kathleen Boyd",
            "Karen Hirsch",
            "Curt Langlotz",
            "Joanna Nelson",
            "William Hiesinger"
        ],
        "published": "2023-03-01T02:30:11Z",
        "summary": "Large-language models have recently demonstrated impressive zero-shot\ncapabilities in a variety of natural language tasks such as summarization,\ndialogue generation, and question-answering. Despite many promising\napplications in clinical medicine, adoption of these models in real-world\nsettings has been largely limited by their tendency to generate incorrect and\nsometimes even toxic statements. In this study, we develop Almanac, a large\nlanguage model framework augmented with retrieval capabilities for medical\nguideline and treatment recommendations. Performance on a novel dataset of\nclinical scenarios (n = 130) evaluated by a panel of 5 board-certified and\nresident physicians demonstrates significant increases in factuality (mean of\n18% at p-value < 0.05) across all specialties, with improvements in\ncompleteness and safety. Our results demonstrate the potential for large\nlanguage models to be effective tools in the clinical decision-making process,\nwhile also emphasizing the importance of careful testing and deployment to\nmitigate their shortcomings.",
        "pdf_link": "https://arxiv.org/pdf/2303.01229v2.pdf"
    },
    {
        "title": "Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics",
        "authors": [
            "Conor Houghton",
            "Nina Kazanina",
            "Priyanka Sukumaran"
        ],
        "published": "2023-02-28T20:49:38Z",
        "summary": "Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.",
        "pdf_link": "https://arxiv.org/pdf/2303.00077v1.pdf"
    },
    {
        "title": "Automatic Scoring of Dream Reports' Emotional Content with Large Language Models",
        "authors": [
            "Lorenzo Bertolini",
            "Valentina Elce",
            "Adriana Michalak",
            "Giulio Bernardi",
            "Julie Weeds"
        ],
        "published": "2023-02-28T18:23:17Z",
        "summary": "In the field of dream research, the study of dream content typically relies\non the analysis of verbal reports provided by dreamers upon awakening from\ntheir sleep. This task is classically performed through manual scoring provided\nby trained annotators, at a great time expense. While a consistent body of work\nsuggests that natural language processing (NLP) tools can support the automatic\nanalysis of dream reports, proposed methods lacked the ability to reason over a\nreport's full context and required extensive data pre-processing. Furthermore,\nin most cases, these methods were not validated against standard manual scoring\napproaches. In this work, we address these limitations by adopting large\nlanguage models (LLMs) to study and replicate the manual annotation of dream\nreports, using a mixture of off-the-shelf and bespoke approaches, with a focus\non references to reports' emotions. Our results show that the off-the-shelf\nmethod achieves a low performance probably in light of inherent linguistic\ndifferences between reports collected in different (groups of) individuals. On\nthe other hand, the proposed bespoke text classification method achieves a high\nperformance, which is robust against potential biases. Overall, these\nobservations indicate that our approach could find application in the analysis\nof large dream datasets and may favour reproducibility and comparability of\nresults across studies.",
        "pdf_link": "https://arxiv.org/pdf/2302.14828v1.pdf"
    },
    {
        "title": "Investigating the Effectiveness of Task-Agnostic Prefix Prompt for Instruction Following",
        "authors": [
            "Seonghyeon Ye",
            "Hyeonbin Hwang",
            "Sohee Yang",
            "Hyeongu Yun",
            "Yireun Kim",
            "Minjoon Seo"
        ],
        "published": "2023-02-28T16:06:35Z",
        "summary": "In this paper, we present our finding that prepending a Task-Agnostic Prefix\nPrompt (TAPP) to the input improves the instruction-following ability of\nvarious Large Language Models (LLMs) during inference. TAPP is different from\ncanonical prompts for LLMs in that it is a fixed prompt prepended to the\nbeginning of every input regardless of the target task for zero-shot\ngeneralization. We observe that both base LLMs (i.e. not fine-tuned to follow\ninstructions) and instruction-tuned models benefit from TAPP, resulting in\n34.58% and 12.26% improvement on average, respectively. This implies that the\ninstruction-following ability of LLMs can be improved during inference time\nwith a fixed prompt constructed with simple heuristics. We hypothesize that\nTAPP assists language models to better estimate the output distribution by\nfocusing more on the instruction of the target task during inference. In other\nwords, such ability does not seem to be sufficiently activated in not only base\nLLMs but also many instruction-fine-tuned LLMs. All experiments are\nreproducible from https://github.com/seonghyeonye/TAPP.",
        "pdf_link": "https://arxiv.org/pdf/2302.14691v2.pdf"
    },
    {
        "title": "Zero-Shot Cross-Lingual Summarization via Large Language Models",
        "authors": [
            "Jiaan Wang",
            "Yunlong Liang",
            "Fandong Meng",
            "Beiqi Zou",
            "Zhixu Li",
            "Jianfeng Qu",
            "Jie Zhou"
        ],
        "published": "2023-02-28T01:27:37Z",
        "summary": "Given a document in a source language, cross-lingual summarization (CLS) aims\nto generate a summary in a different target language. Recently, the emergence\nof Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has\nattracted wide attention from the computational linguistics community. However,\nit is not yet known the performance of LLMs on CLS. In this report, we\nempirically use various prompts to guide LLMs to perform zero-shot CLS from\ndifferent paradigms (i.e., end-to-end and pipeline), and provide a preliminary\nevaluation on the generated summaries. We find that ChatGPT and GPT-4\noriginally prefer to produce lengthy summaries with detailed information. These\ntwo LLMs can further balance informativeness and conciseness with the help of\nan interactive prompt, significantly improving their CLS performance.\nExperimental results on three widely-used CLS datasets show that GPT-4 achieves\nstate-of-the-art zero-shot CLS performance, and performs competitively compared\nwith the fine-tuned mBART-50. Moreover, we also find some multi-lingual and\nbilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited\nzero-shot CLS ability. Due to the composite nature of CLS, which requires\nmodels to perform summarization and translation simultaneously, accomplishing\nthis task in a zero-shot manner is even a challenge for LLMs. Therefore, we\nsincerely hope and recommend future LLM research could use CLS as a testbed.",
        "pdf_link": "https://arxiv.org/pdf/2302.14229v4.pdf"
    },
    {
        "title": "Reward Design with Language Models",
        "authors": [
            "Minae Kwon",
            "Sang Michael Xie",
            "Kalesha Bullard",
            "Dorsa Sadigh"
        ],
        "published": "2023-02-27T22:09:35Z",
        "summary": "Reward design in reinforcement learning (RL) is challenging since specifying\nhuman notions of desired behavior may be difficult via reward functions or\nrequire many expert demonstrations. Can we instead cheaply design rewards using\na natural language interface? This paper explores how to simplify reward design\nby prompting a large language model (LLM) such as GPT-3 as a proxy reward\nfunction, where the user provides a textual prompt containing a few examples\n(few-shot) or a description (zero-shot) of the desired behavior. Our approach\nleverages this proxy reward function in an RL framework. Specifically, users\nspecify a prompt once at the beginning of training. During training, the LLM\nevaluates an RL agent's behavior against the desired behavior described by the\nprompt and outputs a corresponding reward signal. The RL agent then uses this\nreward to update its behavior. We evaluate whether our approach can train\nagents aligned with user objectives in the Ultimatum Game, matrix games, and\nthe DealOrNoDeal negotiation task. In all three tasks, we show that RL agents\ntrained with our framework are well-aligned with the user's objectives and\noutperform RL agents trained with reward functions learned via supervised\nlearning",
        "pdf_link": "https://arxiv.org/pdf/2303.00001v1.pdf"
    },
    {
        "title": "Systematic Rectification of Language Models via Dead-end Analysis",
        "authors": [
            "Meng Cao",
            "Mehdi Fatemi",
            "Jackie Chi Kit Cheung",
            "Samira Shabanian"
        ],
        "published": "2023-02-27T17:47:53Z",
        "summary": "With adversarial or otherwise normal prompts, existing large language models\n(LLM) can be pushed to generate toxic discourses. One way to reduce the risk of\nLLMs generating undesired discourses is to alter the training of the LLM. This\ncan be very restrictive due to demanding computation requirements. Other\nmethods rely on rule-based or prompt-based token elimination, which are limited\nas they dismiss future tokens and the overall meaning of the complete\ndiscourse. Here, we center detoxification on the probability that the finished\ndiscourse is ultimately considered toxic. That is, at each point, we advise\nagainst token selections proportional to how likely a finished text from this\npoint will be toxic. To this end, we formally extend the dead-end theory from\nthe recent reinforcement learning (RL) literature to also cover uncertain\noutcomes. Our approach, called rectification, utilizes a separate but\nsignificantly smaller model for detoxification, which can be applied to diverse\nLLMs as long as they share the same vocabulary. Importantly, our method does\nnot require access to the internal representations of the LLM, but only the\ntoken probability distribution at each decoding step. This is crucial as many\nLLMs today are hosted in servers and only accessible through APIs. When applied\nto various LLMs, including GPT-3, our approach significantly improves the\ngenerated discourse compared to the base LLMs and other techniques in terms of\nboth the overall language and detoxification performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.14003v1.pdf"
    },
    {
        "title": "SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks",
        "authors": [
            "Rui-Jie Zhu",
            "Qihang Zhao",
            "Guoqi Li",
            "Jason K. Eshraghian"
        ],
        "published": "2023-02-27T16:43:04Z",
        "summary": "As the size of large language models continue to scale, so does the\ncomputational resources required to run it. Spiking Neural Networks (SNNs) have\nemerged as an energy-efficient approach to deep learning that leverage sparse\nand event-driven activations to reduce the computational overhead associated\nwith model inference. While they have become competitive with non-spiking\nmodels on many computer vision tasks, SNNs have also proven to be more\nchallenging to train. As a result, their performance lags behind modern deep\nlearning, and we are yet to see the effectiveness of SNNs in language\ngeneration. In this paper, inspired by the Receptance Weighted Key Value (RWKV)\nlanguage model, we successfully implement `SpikeGPT', a generative language\nmodel with binary, event-driven spiking activation units. We train the proposed\nmodel on two model variants: 45M and 216M parameters. To the best of our\nknowledge, SpikeGPT is the largest backpropagation-trained SNN model to date,\nrendering it suitable for both the generation and comprehension of natural\nlanguage. We achieve this by modifying the transformer block to replace\nmulti-head self attention to reduce quadratic computational complexity O(N^2)\nto linear complexity O(N) with increasing sequence length. Input tokens are\ninstead streamed in sequentially to our attention mechanism (as with typical\nSNNs). Our preliminary experiments show that SpikeGPT remains competitive with\nnon-spiking models on tested benchmarks, while maintaining 20x fewer operations\nwhen processed on neuromorphic hardware that can leverage sparse, event-driven\nactivations.",
        "pdf_link": "https://arxiv.org/pdf/2302.13939v4.pdf"
    },
    {
        "title": "Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity",
        "authors": [
            "Zijian Ding",
            "Arvind Srinivasan",
            "Stephen MacNeil",
            "Joel Chan"
        ],
        "published": "2023-02-27T15:54:57Z",
        "summary": "Cross-domain analogical reasoning is a core creative ability that can be\nchallenging for humans. Recent work has shown some proofs-of concept of Large\nlanguage Models' (LLMs) ability to generate cross-domain analogies. However,\nthe reliability and potential usefulness of this capacity for augmenting human\ncreative work has received little systematic exploration. In this paper, we\nsystematically explore LLMs capacity to augment cross-domain analogical\nreasoning. Across three studies, we found: 1) LLM-generated cross-domain\nanalogies were frequently judged as helpful in the context of a problem\nreformulation task (median 4 out of 5 helpfulness rating), and frequently (~80%\nof cases) led to observable changes in problem formulations, and 2) there was\nan upper bound of 25% of outputs bring rated as potentially harmful, with a\nmajority due to potentially upsetting content, rather than biased or toxic\ncontent. These results demonstrate the potential utility -- and risks -- of\nLLMs for augmenting cross-domain analogical creativity.",
        "pdf_link": "https://arxiv.org/pdf/2302.12832v2.pdf"
    },
    {
        "title": "Let's have a chat! A Conversation with ChatGPT: Technology, Applications, and Limitations",
        "authors": [
            "Sakib Shahriar",
            "Kadhim Hayawi"
        ],
        "published": "2023-02-27T14:26:29Z",
        "summary": "The emergence of an AI-powered chatbot that can generate human-like sentences\nand write coherent essays has caught the world's attention. This paper\ndiscusses the historical overview of chatbots and the technology behind Chat\nGenerative Pre-trained Transformer, better known as ChatGPT. Moreover,\npotential applications of ChatGPT in various domains, including healthcare,\neducation, and research, are highlighted. Despite promising results, there are\nseveral privacy and ethical concerns surrounding ChatGPT. In addition, we\nhighlight some of the important limitations of the current version of ChatGPT.\nWe also ask ChatGPT to provide its point of view and present its responses to\nseveral questions we attempt to answer.",
        "pdf_link": "https://arxiv.org/pdf/2302.13817v4.pdf"
    },
    {
        "title": "The (ab)use of Open Source Code to Train Large Language Models",
        "authors": [
            "Ali Al-Kaswan",
            "Maliheh Izadi"
        ],
        "published": "2023-02-27T11:34:53Z",
        "summary": "In recent years, Large Language Models (LLMs) have gained significant\npopularity due to their ability to generate human-like text and their potential\napplications in various fields, such as Software Engineering. LLMs for Code are\ncommonly trained on large unsanitized corpora of source code scraped from the\nInternet. The content of these datasets is memorized and emitted by the models,\noften in a verbatim manner. In this work, we will discuss the security,\nprivacy, and licensing implications of memorization. We argue why the use of\ncopyleft code to train LLMs is a legal and ethical dilemma. Finally, we provide\nfour actionable recommendations to address this issue.",
        "pdf_link": "https://arxiv.org/pdf/2302.13681v2.pdf"
    },
    {
        "title": "Towards Human-Bot Collaborative Software Architecting with ChatGPT",
        "authors": [
            "Aakash Ahmad",
            "Muhammad Waseem",
            "Peng Liang",
            "Mahdi Fehmideh",
            "Mst Shamima Aktar",
            "Tommi Mikkonen"
        ],
        "published": "2023-02-26T16:32:16Z",
        "summary": "Architecting software-intensive systems can be a complex process. It deals\nwith the daunting tasks of unifying stakeholders' perspectives, designers'\nintellect, tool-based automation, pattern-driven reuse, and so on, to sketch a\nblueprint that guides software implementation and evaluation. Despite its\nbenefits, architecture-centric software engineering (ACSE) inherits a multitude\nof challenges. ACSE challenges could stem from a lack of standardized\nprocesses, socio-technical limitations, and scarcity of human expertise etc.\nthat can impede the development of existing and emergent classes of software\n(e.g., IoTs, blockchain, quantum systems). Software Development Bots (DevBots)\ntrained on large language models can help synergise architects' knowledge with\nartificially intelligent decision support to enable rapid architecting in a\nhuman-bot collaborative ACSE. An emerging solution to enable this collaboration\nis ChatGPT, a disruptive technology not primarily introduced for software\nengineering, but is capable of articulating and refining architectural\nartifacts based on natural language processing. We detail a case study that\ninvolves collaboration between a novice software architect and ChatGPT for\narchitectural analysis, synthesis, and evaluation of a services-driven software\napplication. Preliminary results indicate that ChatGPT can mimic an architect's\nrole to support and often lead ACSE, however; it requires human oversight and\ndecision support for collaborative architecting. Future research focuses on\nharnessing empirical evidence about architects' productivity and exploring\nsocio-technical aspects of architecting with ChatGPT to tackle emerging and\nfuturistic challenges of ACSE.",
        "pdf_link": "https://arxiv.org/pdf/2302.14600v1.pdf"
    },
    {
        "title": "Fast Attention Requires Bounded Entries",
        "authors": [
            "Josh Alman",
            "Zhao Song"
        ],
        "published": "2023-02-26T02:42:39Z",
        "summary": "In modern machine learning, inner product attention computation is a\nfundamental task for training large language models such as Transformer, GPT-1,\nBERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as\ninput three matrices $Q, K, V \\in [-B,B]^{n \\times d}$, and the goal is to\nconstruct the matrix $\\mathrm{Att}(Q,K,V) := \\mathrm{diag}(A {\\bf 1}_n)^{-1} A\nV \\in \\mathbb{R}^{n \\times d}$, where $A = \\exp(QK^\\top/d)$ is the `attention\nmatrix', and $\\exp$ is applied entry-wise. Straightforward methods for this\nproblem explicitly compute the $n \\times n$ attention matrix $A$, and hence\nrequire time $\\Omega(n^2)$ even when $d = n^{o(1)}$ is small.\n  In this paper, we investigate whether faster algorithms are possible by\nimplicitly making use of the matrix $A$. We present two results, showing that\nthere is a sharp transition at $B = \\Theta(\\sqrt{\\log n})$.\n  $\\bullet$ If $d = O(\\log n)$ and $B = o(\\sqrt{\\log n})$, there is an\n$n^{1+o(1)}$ time algorithm to approximate $\\mathrm{Att}(Q,K,V)$ up to\n$1/\\mathrm{poly}(n)$ additive error.\n  $\\bullet$ If $d = O(\\log n)$ and $B = \\Theta (\\sqrt{\\log n})$, assuming the\nStrong Exponential Time Hypothesis from fine-grained complexity theory, it is\nimpossible to approximate $\\mathrm{Att}(Q,K,V)$ up to $1/\\mathrm{poly}(n)$\nadditive error in truly subquadratic time $n^{2 - \\Omega(1)}$.\n  This gives a theoretical explanation for the phenomenon observed in practice\nthat attention computation is much more efficient when the input matrices have\nsmaller entries.",
        "pdf_link": "https://arxiv.org/pdf/2302.13214v2.pdf"
    },
    {
        "title": "On pitfalls (and advantages) of sophisticated large language models",
        "authors": [
            "Anna Strasser"
        ],
        "published": "2023-02-25T11:14:39Z",
        "summary": "Natural language processing based on large language models (LLMs) is a\nbooming field of AI research. After neural networks have proven to outperform\nhumans in games and practical domains based on pattern recognition, we might\nstand now at a road junction where artificial entities might eventually enter\nthe realm of human communication. However, this comes with serious risks. Due\nto the inherent limitations regarding the reliability of neural networks,\noverreliance on LLMs can have disruptive consequences. Since it will be\nincreasingly difficult to distinguish between human-written and\nmachine-generated text, one is confronted with new ethical challenges. This\nbegins with the no longer undoubtedly verifiable human authorship and continues\nwith various types of fraud, such as a new form of plagiarism. This also\nconcerns the violation of privacy rights, the possibility of circulating\ncounterfeits of humans, and, last but not least, it makes a massive spread of\nmisinformation possible.",
        "pdf_link": "https://arxiv.org/pdf/2303.17511v1.pdf"
    },
    {
        "title": "Leveraging Large Language Model and Story-Based Gamification in Intelligent Tutoring System to Scaffold Introductory Programming Courses: A Design-Based Research Study",
        "authors": [
            "Chen Cao"
        ],
        "published": "2023-02-25T04:07:03Z",
        "summary": "Programming skills are rapidly becoming essential for many educational paths\nand career opportunities. Yet, for many international students, the traditional\napproach to teaching introductory programming courses can be a significant\nchallenge due to the complexities of the language, the lack of prior\nprogramming knowledge, and the language and cultural barriers. This study\nexplores how large language models and gamification can scaffold coding\nlearning and increase Chinese students sense of belonging in introductory\nprogramming courses. In this project, a gamification intelligent tutoring\nsystem was developed to adapt to Chinese international students learning needs\nand provides scaffolding to support their success in introductory computer\nprogramming courses.",
        "pdf_link": "https://arxiv.org/pdf/2302.12834v1.pdf"
    },
    {
        "title": "Robot Behavior-Tree-Based Task Generation with Large Language Models",
        "authors": [
            "Yue Cao",
            "C. S. George Lee"
        ],
        "published": "2023-02-24T22:53:10Z",
        "summary": "Nowadays, the behavior tree is gaining popularity as a representation for\nrobot tasks due to its modularity and reusability. Designing behavior-tree\ntasks manually is time-consuming for robot end-users, thus there is a need for\ninvestigating automatic behavior-tree-based task generation. Prior\nbehavior-tree-based task generation approaches focus on fixed primitive tasks\nand lack generalizability to new task domains. To cope with this issue, we\npropose a novel behavior-tree-based task generation approach that utilizes\nstate-of-the-art large language models. We propose a Phase-Step prompt design\nthat enables a hierarchical-structured robot task generation and further\nintegrate it with behavior-tree-embedding-based search to set up the\nappropriate prompt. In this way, we enable an automatic and cross-domain\nbehavior-tree task generation. Our behavior-tree-based task generation approach\ndoes not require a set of pre-defined primitive tasks. End-users only need to\ndescribe an abstract desired task and our proposed approach can swiftly\ngenerate the corresponding behavior tree. A full-process case study is provided\nto demonstrate our proposed approach. An ablation study is conducted to\nevaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step\nprompts and the limitation of large language models are presented and\ndiscussed.",
        "pdf_link": "https://arxiv.org/pdf/2302.12927v1.pdf"
    },
    {
        "title": "Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data",
        "authors": [
            "KaShun Shum",
            "Shizhe Diao",
            "Tong Zhang"
        ],
        "published": "2023-02-24T18:58:06Z",
        "summary": "Chain-of-thought (CoT) advances the reasoning abilities of large language\nmodels (LLMs) and achieves superior performance in complex reasoning tasks.\nHowever, most CoT studies rely on carefully designed human-annotated rational\nchains to prompt LLMs, posing challenges for real-world applications where\nlabeled data is available without rational chains. This paper proposes a new\nstrategy, Automate-CoT (Automatic Prompt Augmentation and Selection with\nChain-of-Thought), that can bypass human engineering of CoT by automatically\naugmenting rational chains from a small labeled dataset, and then pruning\nlow-quality chains to construct a candidate pool of machine-generated rationale\nchains based on the labels. Finally, it selects the optimal combination of\nseveral rationale chains from the pool for CoT prompting by employing a\nvariance-reduced policy gradient strategy to estimate the significance of each\nexample. Automate-CoT enables a quick adaptation of the CoT technique to\ndifferent tasks. Experimental results demonstrate the effectiveness of our\nmethod, where competitive results are achieved on arithmetic reasoning (+2.7%),\ncommonsense reasoning (+3.4%), symbolic reasoning (+3.2%), and non-reasoning\ntasks (+2.5%). The code is available at\nhttps://github.com/SHUMKASHUN/Automate-CoT.",
        "pdf_link": "https://arxiv.org/pdf/2302.12822v3.pdf"
    },
    {
        "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
        "authors": [
            "Baolin Peng",
            "Michel Galley",
            "Pengcheng He",
            "Hao Cheng",
            "Yujia Xie",
            "Yu Hu",
            "Qiuyuan Huang",
            "Lars Liden",
            "Zhou Yu",
            "Weizhu Chen",
            "Jianfeng Gao"
        ],
        "published": "2023-02-24T18:48:43Z",
        "summary": "Large language models (LLMs), such as ChatGPT, are able to generate\nhuman-like, fluent responses for many downstream tasks, e.g., task-oriented\ndialog and question answering. However, applying LLMs to real-world,\nmission-critical applications remains challenging mainly due to their tendency\nto generate hallucinations and their inability to use external knowledge. This\npaper proposes a LLM-Augmenter system, which augments a black-box LLM with a\nset of plug-and-play modules. Our system makes the LLM generate responses\ngrounded in external knowledge, e.g., stored in task-specific databases. It\nalso iteratively revises LLM prompts to improve model responses using feedback\ngenerated by utility functions, e.g., the factuality score of a LLM-generated\nresponse. The effectiveness of LLM-Augmenter is empirically validated on two\ntypes of scenarios, task-oriented dialog and open-domain question answering.\nLLM-Augmenter significantly reduces ChatGPT's hallucinations without\nsacrificing the fluency and informativeness of its responses. We make the\nsource code and models publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2302.12813v3.pdf"
    },
    {
        "title": "HULAT at SemEval-2023 Task 10: Data augmentation for pre-trained transformers applied to the detection of sexism in social media",
        "authors": [
            "Isabel Segura-Bedmar"
        ],
        "published": "2023-02-24T18:17:38Z",
        "summary": "This paper describes our participation in SemEval-2023 Task 10, whose goal is\nthe detection of sexism in social media. We explore some of the most popular\ntransformer models such as BERT, DistilBERT, RoBERTa, and XLNet. We also study\ndifferent data augmentation techniques to increase the training dataset. During\nthe development phase, our best results were obtained by using RoBERTa and data\naugmentation for tasks B and C. However, the use of synthetic data does not\nimprove the results for task C. We participated in the three subtasks. Our\napproach still has much room for improvement, especially in the two\nfine-grained classifications. All our code is available in the repository\nhttps://github.com/isegura/hulat_edos.",
        "pdf_link": "https://arxiv.org/pdf/2302.12840v2.pdf"
    },
    {
        "title": "Spanish Built Factual Freectianary (Spanish-BFF): the first AI-generated free dictionary",
        "authors": [
            "Miguel Ortega-Martín",
            "Óscar García-Sierra",
            "Alfonso Ardoiz",
            "Juan Carlos Armenteros",
            "Jorge Álvarez",
            "Adrián Alonso"
        ],
        "published": "2023-02-24T16:59:54Z",
        "summary": "Dictionaries are one of the oldest and most used linguistic resources.\nBuilding them is a complex task that, to the best of our knowledge, has yet to\nbe explored with generative Large Language Models (LLMs). We introduce the\n\"Spanish Built Factual Freectianary\" (Spanish-BFF) as the first Spanish\nAI-generated dictionary. This first-of-its-kind free dictionary uses GPT-3. We\nalso define future steps we aim to follow to improve this initial commitment to\nthe field, such as more additional languages.",
        "pdf_link": "https://arxiv.org/pdf/2302.12746v2.pdf"
    },
    {
        "title": "Language Models are Few-shot Learners for Prognostic Prediction",
        "authors": [
            "Zekai Chen",
            "Mariann Micsinai Balan",
            "Kevin Brown"
        ],
        "published": "2023-02-24T15:35:36Z",
        "summary": "Clinical prediction is an essential task in the healthcare industry. However,\nthe recent success of transformers, on which large language models are built,\nhas not been extended to this domain. In this research, we explore the use of\ntransformers and language models in prognostic prediction for immunotherapy\nusing real-world patients' clinical data and molecular profiles. This paper\ninvestigates the potential of transformers to improve clinical prediction\ncompared to conventional machine learning approaches and addresses the\nchallenge of few-shot learning in predicting rare disease areas. The study\nbenchmarks the efficacy of baselines and language models on prognostic\nprediction across multiple cancer types and investigates the impact of\ndifferent pretrained language models under few-shot regimes. The results\ndemonstrate significant improvements in accuracy and highlight the potential of\nNLP in clinical research to improve early detection and intervention for\ndifferent diseases.",
        "pdf_link": "https://arxiv.org/pdf/2302.12692v4.pdf"
    },
    {
        "title": "Adapting Pre-trained Language Models for Quantum Natural Language Processing",
        "authors": [
            "Qiuchi Li",
            "Benyou Wang",
            "Yudong Zhu",
            "Christina Lioma",
            "Qun Liu"
        ],
        "published": "2023-02-24T14:59:02Z",
        "summary": "The emerging classical-quantum transfer learning paradigm has brought a\ndecent performance to quantum computational models in many tasks, such as\ncomputer vision, by enabling a combination of quantum models and classical\npre-trained neural networks. However, using quantum computing with pre-trained\nmodels has yet to be explored in natural language processing (NLP). Due to the\nhigh linearity constraints of the underlying quantum computing infrastructures,\nexisting Quantum NLP models are limited in performance on real tasks. We fill\nthis gap by pre-training a sentence state with complex-valued BERT-like\narchitecture, and adapting it to the classical-quantum transfer learning scheme\nfor sentence classification. On quantum simulation experiments, the pre-trained\nrepresentation can bring 50\\% to 60\\% increases to the capacity of end-to-end\nquantum models.",
        "pdf_link": "https://arxiv.org/pdf/2302.13812v1.pdf"
    },
    {
        "title": "Analyzing And Editing Inner Mechanisms Of Backdoored Language Models",
        "authors": [
            "Max Lamparth",
            "Anka Reuel"
        ],
        "published": "2023-02-24T05:26:08Z",
        "summary": "Poisoning of data sets is a potential security threat to large language\nmodels that can lead to backdoored models. A description of the internal\nmechanisms of backdoored language models and how they process trigger inputs,\ne.g., when switching to toxic language, has yet to be found. In this work, we\nstudy the internal representations of transformer-based backdoored language\nmodels and determine early-layer MLP modules as most important for the backdoor\nmechanism in combination with the initial embedding projection. We use this\nknowledge to remove, insert, and modify backdoor mechanisms with engineered\nreplacements that reduce the MLP module outputs to essentials for the backdoor\nmechanism. To this end, we introduce PCP ablation, where we replace transformer\nmodules with low-rank matrices based on the principal components of their\nactivations. We demonstrate our results on backdoored toy, backdoored large,\nand non-backdoored open-source models. We show that we can improve the backdoor\nrobustness of large language models by locally constraining individual modules\nduring fine-tuning on potentially poisonous data sets.\n  Trigger warning: Offensive language.",
        "pdf_link": "https://arxiv.org/pdf/2302.12461v2.pdf"
    },
    {
        "title": "Extracting Victim Counts from Text",
        "authors": [
            "Mian Zhong",
            "Shehzaad Dhuliawala",
            "Niklas Stoehr"
        ],
        "published": "2023-02-23T23:50:24Z",
        "summary": "Decision-makers in the humanitarian sector rely on timely and exact\ninformation during crisis events. Knowing how many civilians were injured\nduring an earthquake is vital to allocate aids properly. Information about such\nvictim counts is often only available within full-text event descriptions from\nnewspapers and other reports. Extracting numbers from text is challenging:\nnumbers have different formats and may require numeric reasoning. This renders\npurely string matching-based approaches insufficient. As a consequence,\nfine-grained counts of injured, displaced, or abused victims beyond fatalities\nare often not extracted and remain unseen. We cast victim count extraction as a\nquestion answering (QA) task with a regression or classification objective. We\ncompare regex, dependency parsing, semantic role labeling-based approaches, and\nadvanced text-to-text models. Beyond model accuracy, we analyze extraction\nreliability and robustness which are key for this sensitive task. In\nparticular, we discuss model calibration and investigate few-shot and\nout-of-distribution performance. Ultimately, we make a comprehensive\nrecommendation on which model to select for different desiderata and data\ndomains. Our work is among the first to apply numeracy-focused large language\nmodels in a real-world use case with a positive impact.",
        "pdf_link": "https://arxiv.org/pdf/2302.12367v1.pdf"
    },
    {
        "title": "Dr ChatGPT, tell me what I want to hear: How prompt knowledge impacts health answer correctness",
        "authors": [
            "Guido Zuccon",
            "Bevan Koopman"
        ],
        "published": "2023-02-23T22:14:01Z",
        "summary": "Generative pre-trained language models (GPLMs) like ChatGPT encode in the\nmodel's parameters knowledge the models observe during the pre-training phase.\nThis knowledge is then used at inference to address the task specified by the\nuser in their prompt. For example, for the question-answering task, the GPLMs\nleverage the knowledge and linguistic patterns learned at training to produce\nan answer to a user question. Aside from the knowledge encoded in the model\nitself, answers produced by GPLMs can also leverage knowledge provided in the\nprompts. For example, a GPLM can be integrated into a retrieve-then-generate\nparadigm where a search engine is used to retrieve documents relevant to the\nquestion; the content of the documents is then transferred to the GPLM via the\nprompt. In this paper we study the differences in answer correctness generated\nby ChatGPT when leveraging the model's knowledge alone vs. in combination with\nthe prompt knowledge. We study this in the context of consumers seeking health\nadvice from the model. Aside from measuring the effectiveness of ChatGPT in\nthis context, we show that the knowledge passed in the prompt can overturn the\nknowledge encoded in the model and this is, in our experiments, to the\ndetriment of answer correctness. This work has important implications for the\ndevelopment of more robust and transparent question-answering systems based on\ngenerative pre-trained language models.",
        "pdf_link": "https://arxiv.org/pdf/2302.13793v1.pdf"
    },
    {
        "title": "CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models",
        "authors": [
            "Denis Jered McInerney",
            "Geoffrey Young",
            "Jan-Willem van de Meent",
            "Byron C. Wallace"
        ],
        "published": "2023-02-23T21:23:06Z",
        "summary": "We propose CHiLL (Crafting High-Level Latents), an approach for\nnatural-language specification of features for linear models. CHiLL prompts\nLLMs with expert-crafted queries to generate interpretable features from health\nrecords. The resulting noisy labels are then used to train a simple linear\nclassifier. Generating features based on queries to an LLM can empower\nphysicians to use their domain expertise to craft features that are clinically\nmeaningful for a downstream task of interest, without having to manually\nextract these from raw EHR. We are motivated by a real-world risk prediction\ntask, but as a reproducible proxy, we use MIMIC-III and MIMIC-CXR data and\nstandard predictive tasks (e.g., 30-day readmission) to evaluate this approach.\nWe find that linear models using automatically extracted features are\ncomparably performant to models using reference features, and provide greater\ninterpretability than linear models using \"Bag-of-Words\" features. We verify\nthat learned feature weights align well with clinical expectations.",
        "pdf_link": "https://arxiv.org/pdf/2302.12343v2.pdf"
    },
    {
        "title": "Testing AI performance on less frequent aspects of language reveals insensitivity to underlying meaning",
        "authors": [
            "Vittoria Dentella",
            "Elliot Murphy",
            "Gary Marcus",
            "Evelina Leivada"
        ],
        "published": "2023-02-23T20:18:52Z",
        "summary": "Advances in computational methods and big data availability have recently\ntranslated into breakthroughs in AI applications. With successes in bottom-up\nchallenges partially overshadowing shortcomings, the 'human-like' performance\nof Large Language Models has raised the question of how linguistic performance\nis achieved by algorithms. Given systematic shortcomings in generalization\nacross many AI systems, in this work we ask whether linguistic performance is\nindeed guided by language knowledge in Large Language Models. To this end, we\nprompt GPT-3 with a grammaticality judgement task and comprehension questions\non less frequent constructions that are thus unlikely to form part of Large\nLanguage Models' training data. These included grammatical 'illusions',\nsemantic anomalies, complex nested hierarchies and self-embeddings. GPT-3\nfailed for every prompt but one, often offering answers that show a critical\nlack of understanding even of high-frequency words used in these less frequent\ngrammatical constructions. The present work sheds light on the boundaries of\nthe alleged AI human-like linguistic competence and argues that, far from\nhuman-like, the next-word prediction abilities of LLMs may face issues of\nrobustness, when pushed beyond training data.",
        "pdf_link": "https://arxiv.org/pdf/2302.12313v2.pdf"
    },
    {
        "title": "Active Prompting with Chain-of-Thought for Large Language Models",
        "authors": [
            "Shizhe Diao",
            "Pengcheng Wang",
            "Yong Lin",
            "Tong Zhang"
        ],
        "published": "2023-02-23T18:58:59Z",
        "summary": "The increasing scale of large language models (LLMs) brings emergent\nabilities to various complex tasks requiring reasoning, such as arithmetic and\ncommonsense reasoning. It is known that the effective design of task-specific\nprompts is critical for LLMs' ability to produce high-quality answers. In\nparticular, an effective approach for complex question-and-answer tasks is\nexample-based prompting with chain-of-thought (CoT) reasoning, which\nsignificantly improves the performance of LLMs. However, current CoT methods\nrely on a fixed set of human-annotated exemplars, which are not necessarily the\nmost effective examples for different tasks. This paper proposes a new method,\nActive-Prompt, to adapt LLMs to different tasks with task-specific example\nprompts (annotated with human-designed CoT reasoning). For this purpose, we\npropose a solution to the key problem of determining which questions are the\nmost important and helpful ones to annotate from a pool of task-specific\nqueries. By borrowing ideas from the related problem of uncertainty-based\nactive learning, we introduce several metrics to characterize the uncertainty\nso as to select the most uncertain questions for annotation. Experimental\nresults demonstrate the superiority of our proposed method, achieving\nstate-of-the-art on eight complex reasoning tasks. Further analyses of\ndifferent uncertainty metrics, pool sizes, zero-shot learning, and\naccuracy-uncertainty relationship demonstrate the effectiveness of our method.\nOur code will be available at https://github.com/shizhediao/active-prompt.",
        "pdf_link": "https://arxiv.org/pdf/2302.12246v3.pdf"
    },
    {
        "title": "What Makes a Language Easy to Deep-Learn?",
        "authors": [
            "Lukas Galke",
            "Yoav Ram",
            "Limor Raviv"
        ],
        "published": "2023-02-23T18:57:34Z",
        "summary": "Deep neural networks drive the success of natural language processing. A\nfundamental property of language is its compositional structure, allowing\nhumans to systematically produce forms for new meanings. For humans, languages\nwith more compositional and transparent structures are typically easier to\nlearn than those with opaque and irregular structures. However, this\nlearnability advantage has not yet been shown for deep neural networks,\nlimiting their use as models for human language learning. Here, we directly\ntest how neural networks compare to humans in learning and generalizing\ndifferent languages that vary in their degree of compositional structure. We\nevaluate the memorization and generalization capabilities of a large language\nmodel and recurrent neural networks, and show that both deep neural networks\nexhibit a learnability advantage for more structured linguistic input: neural\nnetworks exposed to more compositional languages show more systematic\ngeneralization, greater agreement between different agents, and greater\nsimilarity to human learners.",
        "pdf_link": "https://arxiv.org/pdf/2302.12239v3.pdf"
    },
    {
        "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
        "authors": [
            "Kai Greshake",
            "Sahar Abdelnabi",
            "Shailesh Mishra",
            "Christoph Endres",
            "Thorsten Holz",
            "Mario Fritz"
        ],
        "published": "2023-02-23T17:14:38Z",
        "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.",
        "pdf_link": "https://arxiv.org/pdf/2302.12173v2.pdf"
    },
    {
        "title": "An Independent Evaluation of ChatGPT on Mathematical Word Problems (MWP)",
        "authors": [
            "Paulo Shakarian",
            "Abhinav Koyyalamudi",
            "Noel Ngu",
            "Lakshmivihari Mareedu"
        ],
        "published": "2023-02-23T16:06:16Z",
        "summary": "We study the performance of a commercially available large language model\n(LLM) known as ChatGPT on math word problems (MWPs) from the dataset DRAW-1K.\nTo our knowledge, this is the first independent evaluation of ChatGPT. We found\nthat ChatGPT's performance changes dramatically based on the requirement to\nshow its work, failing 20% of the time when it provides work compared with 84%\nwhen it does not. Further several factors about MWPs relating to the number of\nunknowns and number of operations that lead to a higher probability of failure\nwhen compared with the prior, specifically noting (across all experiments) that\nthe probability of failure increases linearly with the number of addition and\nsubtraction operations. We also have released the dataset of ChatGPT's\nresponses to the MWPs to support further work on the characterization of LLM\nperformance and present baseline machine learning models to predict if ChatGPT\ncan correctly answer an MWP. We have released a dataset comprised of ChatGPT's\nresponses to support further research in this area.",
        "pdf_link": "https://arxiv.org/pdf/2302.13814v2.pdf"
    },
    {
        "title": "Sentence Simplification via Large Language Models",
        "authors": [
            "Yutao Feng",
            "Jipeng Qiang",
            "Yun Li",
            "Yunhao Yuan",
            "Yi Zhu"
        ],
        "published": "2023-02-23T12:11:58Z",
        "summary": "Sentence Simplification aims to rephrase complex sentences into simpler\nsentences while retaining original meaning. Large Language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\ntasks. However, it is not yet known whether LLMs can be served as a\nhigh-quality sentence simplification system. In this work, we empirically\nanalyze the zero-/few-shot learning ability of LLMs by evaluating them on a\nnumber of benchmark test sets. Experimental results show LLMs outperform\nstate-of-the-art sentence simplification methods, and are judged to be on a par\nwith human annotators.",
        "pdf_link": "https://arxiv.org/pdf/2302.11957v1.pdf"
    },
    {
        "title": "Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments",
        "authors": [
            "Jason Xinyu Liu",
            "Ziyi Yang",
            "Ifrah Idrees",
            "Sam Liang",
            "Benjamin Schornstein",
            "Stefanie Tellex",
            "Ankit Shah"
        ],
        "published": "2023-02-22T20:56:40Z",
        "summary": "Grounding navigational commands to linear temporal logic (LTL) leverages its\nunambiguous semantics for reasoning about long-horizon tasks and verifying the\nsatisfaction of temporal constraints. Existing approaches require training data\nfrom the specific environment and landmarks that will be used in natural\nlanguage to understand commands in those environments. We propose Lang2LTL, a\nmodular system and a software package that leverages large language models\n(LLMs) to ground temporal navigational commands to LTL specifications in\nenvironments without prior language data. We comprehensively evaluate Lang2LTL\nfor five well-defined generalization behaviors. Lang2LTL demonstrates the\nstate-of-the-art ability of a single model to ground navigational commands to\ndiverse temporal specifications in 21 city-scaled environments. Finally, we\ndemonstrate a physical robot using Lang2LTL can follow 52 semantically diverse\nnavigational commands in two indoor environments.",
        "pdf_link": "https://arxiv.org/pdf/2302.11649v2.pdf"
    },
    {
        "title": "How Does In-Context Learning Help Prompt Tuning?",
        "authors": [
            "Simeng Sun",
            "Yang Liu",
            "Dan Iter",
            "Chenguang Zhu",
            "Mohit Iyyer"
        ],
        "published": "2023-02-22T17:45:12Z",
        "summary": "Fine-tuning large language models is becoming ever more impractical due to\ntheir rapidly-growing scale. This motivates the use of parameter-efficient\nadaptation methods such as prompt tuning (PT), which adds a small number of\ntunable embeddings to an otherwise frozen model, and in-context learning (ICL),\nin which demonstrations of the task are provided to the model in natural\nlanguage without any additional training. Recently, Singhal et al. (2022)\npropose ``instruction prompt tuning'' (IPT), which combines PT with ICL by\nconcatenating a natural language demonstration with learned prompt embeddings.\nWhile all of these methods have proven effective on different tasks, how they\ninteract with each other remains unexplored. In this paper, we empirically\nstudy when and how in-context examples improve prompt tuning by measuring the\neffectiveness of ICL, PT, and IPT on five text generation tasks with multiple\nbase language models. We observe that (1) IPT does \\emph{not} always outperform\nPT, and in fact requires the in-context demonstration to be semantically\nsimilar to the test input to yield improvements; (2) PT is unstable and\nexhibits high variance, but combining PT and ICL (into IPT) consistently\nreduces variance across all five tasks; and (3) prompts learned for a specific\nsource task via PT exhibit positive transfer when paired with in-context\nexamples of a different target task. Our results offer actionable insights on\nchoosing a suitable parameter-efficient adaptation method for a given task.",
        "pdf_link": "https://arxiv.org/pdf/2302.11521v1.pdf"
    },
    {
        "title": "Guiding Large Language Models via Directional Stimulus Prompting",
        "authors": [
            "Zekun Li",
            "Baolin Peng",
            "Pengcheng He",
            "Michel Galley",
            "Jianfeng Gao",
            "Xifeng Yan"
        ],
        "published": "2023-02-22T17:44:15Z",
        "summary": "We introduce Directional Stimulus Prompting, a novel framework for guiding\nblack-box large language models (LLMs) toward specific desired outputs. Instead\nof directly adjusting LLMs, our method employs a small tunable policy model\n(e.g., T5) to generate an auxiliary directional stimulus prompt for each input\ninstance. These directional stimulus prompts act as nuanced, instance-specific\nhints and clues to guide LLMs in generating desired outcomes, such as including\nspecific keywords in the generated summary. Our approach sidesteps the\nchallenges of direct LLM tuning by optimizing the policy model to explore\ndirectional stimulus prompts that align LLMs with desired behaviors. The policy\nmodel can be optimized through 1) supervised fine-tuning using labeled data and\n2) reinforcement learning from offline or online rewards based on the LLM's\noutput. We assess our method across summarization, dialogue response\ngeneration, and chain-of-thought reasoning tasks. Our experiments demonstrate\nthat the framework consistently improves LLMs' (e.g., ChatGPT, Codex,\nInstructGPT) performance on these supervised tasks using minimal labeled data.\nNotably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances\nChatGPT's performance by an impressive 41.4%, matching or surpassing some fully\nsupervised start-of-the-art models. Additionally, the instance-specific\nchain-of-thought prompt generated by our approach improves InstructGPT's\nreasoning accuracy compared to human-crafted or automatically generated\nprompts. The code and data are publicly available at\n\\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
        "pdf_link": "https://arxiv.org/pdf/2302.11520v4.pdf"
    },
    {
        "title": "In-context Example Selection with Influences",
        "authors": [
            "Tai Nguyen",
            "Eric Wong"
        ],
        "published": "2023-02-21T22:47:45Z",
        "summary": "In-context learning (ICL) is a powerful paradigm emerged from large language\nmodels (LLMs). Despite its promises, ICL performance is known to be highly\nsensitive to input examples. In this work, we use $\\textit{in-context\ninfluences}$ to analyze few-shot ICL performance directly from the in-context\nexamples. Our proposed influence-based example selection method can identify\nboth positive and negative examples, outperforming several baselines when\nevaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\\%$\nperformance gap between using the most negative in-context examples compared to\nthe most positive. In a case study, we apply our influence-based framework to\nquantify the phenomena of recency bias in example ordering for few-shot ICL.",
        "pdf_link": "https://arxiv.org/pdf/2302.11042v2.pdf"
    },
    {
        "title": "$k$NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models",
        "authors": [
            "Yangsibo Huang",
            "Daogao Liu",
            "Zexuan Zhong",
            "Weijia Shi",
            "Yin Tat Lee"
        ],
        "published": "2023-02-21T18:54:21Z",
        "summary": "Fine-tuning a language model on a new domain is standard practice for domain\nadaptation. However, it can be infeasible when it comes to modern large-scale\nlanguage models such as GPT-3, which can only be accessed through APIs, making\nit difficult to access the internal parameters of the model. In this paper, we\npropose $k$NN-Adapter, a method to effectively adapt these black-box large\nlanguage models (LLMs) to a new domain. The $k$NN-Adapter builds on top of the\nretrieval-augmented language model, and adaptively learns to interpolate the\noutput of the language model with retrieval results from a datastore consisting\nof the target domain data. Our experiments on four different domains\ndemonstrate that $k$NN-Adapter significantly improves perplexity, and works\nparticularly well in settings with limited access to LLMs. Additionally, we\nshow that $k$NN-Adapter is more effective than fine-tuning when the amount of\ntraining data is limited. We also release a dataset to encourage further study.",
        "pdf_link": "https://arxiv.org/pdf/2302.10879v1.pdf"
    },
    {
        "title": "ChatGPT: Jack of all trades, master of none",
        "authors": [
            "Jan Kocoń",
            "Igor Cichecki",
            "Oliwier Kaszyca",
            "Mateusz Kochanek",
            "Dominika Szydło",
            "Joanna Baran",
            "Julita Bielaniewicz",
            "Marcin Gruza",
            "Arkadiusz Janz",
            "Kamil Kanclerz",
            "Anna Kocoń",
            "Bartłomiej Koptyra",
            "Wiktoria Mieleszczenko-Kowszewicz",
            "Piotr Miłkowski",
            "Marcin Oleksy",
            "Maciej Piasecki",
            "Łukasz Radliński",
            "Konrad Wojtasik",
            "Stanisław Woźniak",
            "Przemysław Kazienko"
        ],
        "published": "2023-02-21T15:20:37Z",
        "summary": "OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and\nrevolutionized the approach in artificial intelligence to human-model\ninteraction. Several publications on ChatGPT evaluation test its effectiveness\non well-known natural language processing (NLP) tasks. However, the existing\nstudies are mostly non-automated and tested on a very limited scale. In this\nwork, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks,\nmost of them subjective even to humans, such as sentiment analysis, emotion\nrecognition, offensiveness, and stance detection. In contrast, the other tasks\nrequire more objective reasoning like word sense disambiguation, linguistic\nacceptability, and question answering. We also evaluated GPT-4 model on five\nselected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process\nand analyzed more than 49k responses. Our comparison of its results with\navailable State-of-the-Art (SOTA) solutions showed that the average loss in\nquality of the ChatGPT model was about 25% for zero-shot and few-shot\nevaluation. For GPT-4 model, a loss for semantic tasks is significantly lower\nthan for ChatGPT. We showed that the more difficult the task (lower SOTA\nperformance), the higher the ChatGPT loss. It especially refers to pragmatic\nNLP problems like emotion recognition. We also tested the ability to\npersonalize ChatGPT responses for selected subjective tasks via Random\nContextual Few-Shot Personalization, and we obtained significantly better\nuser-based predictions. Additional qualitative analysis revealed a ChatGPT\nbias, most likely due to the rules imposed on human trainers by OpenAI. Our\nresults provide the basis for a fundamental discussion of whether the high\nquality of recent predictive NLP models can indicate a tool's usefulness to\nsociety and how the learning and validation procedures for such systems should\nbe established.",
        "pdf_link": "https://arxiv.org/pdf/2302.10724v4.pdf"
    },
    {
        "title": "A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",
        "authors": [
            "Jules White",
            "Quchen Fu",
            "Sam Hays",
            "Michael Sandborn",
            "Carlos Olea",
            "Henry Gilbert",
            "Ashraf Elnashar",
            "Jesse Spencer-Smith",
            "Douglas C. Schmidt"
        ],
        "published": "2023-02-21T12:42:44Z",
        "summary": "Prompt engineering is an increasingly important skill set needed to converse\neffectively with large language models (LLMs), such as ChatGPT. Prompts are\ninstructions given to an LLM to enforce rules, automate processes, and ensure\nspecific qualities (and quantities) of generated output. Prompts are also a\nform of programming that can customize the outputs and interactions with an\nLLM. This paper describes a catalog of prompt engineering techniques presented\nin pattern form that have been applied to solve common problems when conversing\nwith LLMs. Prompt patterns are a knowledge transfer method analogous to\nsoftware patterns since they provide reusable solutions to common problems\nfaced in a particular context, i.e., output generation and interaction when\nworking with LLMs. This paper provides the following contributions to research\non prompt engineering that apply LLMs to automate software development tasks.\nFirst, it provides a framework for documenting patterns for structuring prompts\nto solve a range of problems so that they can be adapted to different domains.\nSecond, it presents a catalog of patterns that have been applied successfully\nto improve the outputs of LLM conversations. Third, it explains how prompts can\nbe built from multiple patterns and illustrates prompt patterns that benefit\nfrom combination with other prompt patterns.",
        "pdf_link": "https://arxiv.org/pdf/2302.11382v1.pdf"
    },
    {
        "title": "Mask-guided BERT for Few Shot Text Classification",
        "authors": [
            "Wenxiong Liao",
            "Zhengliang Liu",
            "Haixing Dai",
            "Zihao Wu",
            "Yiyang Zhang",
            "Xiaoke Huang",
            "Yuzhong Chen",
            "Xi Jiang",
            "Wei Liu",
            "Dajiang Zhu",
            "Tianming Liu",
            "Sheng Li",
            "Xiang Li",
            "Hongmin Cai"
        ],
        "published": "2023-02-21T05:24:00Z",
        "summary": "Transformer-based language models have achieved significant success in\nvarious domains. However, the data-intensive nature of the transformer\narchitecture requires much labeled data, which is challenging in low-resource\nscenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the\ndifficulty of training robust models on small amounts of samples, which\nfrequently leads to overfitting. Here we present Mask-BERT, a simple and\nmodular framework to help BERT-based architectures tackle FSL. The proposed\napproach fundamentally differs from existing FSL strategies such as prompt\ntuning and meta-learning. The core idea is to selectively apply masks on text\ninputs and filter out irrelevant information, which guides the model to focus\non discriminative tokens that influence prediction results. In addition, to\nmake the text representations from different categories more separable and the\ntext representations from the same category more compact, we introduce a\ncontrastive learning loss function. Experimental results on public-domain\nbenchmark datasets demonstrate the effectiveness of Mask-BERT.",
        "pdf_link": "https://arxiv.org/pdf/2302.10447v3.pdf"
    },
    {
        "title": "Zero-Shot Information Extraction via Chatting with ChatGPT",
        "authors": [
            "Xiang Wei",
            "Xingyu Cui",
            "Ning Cheng",
            "Xiaobin Wang",
            "Xin Zhang",
            "Shen Huang",
            "Pengjun Xie",
            "Jinan Xu",
            "Yufeng Chen",
            "Meishan Zhang",
            "Yong Jiang",
            "Wenjuan Han"
        ],
        "published": "2023-02-20T12:57:12Z",
        "summary": "Zero-shot information extraction (IE) aims to build IE systems from the\nunannotated text. It is challenging due to involving little human intervention.\nChallenging but worthwhile, zero-shot IE reduces the time and effort that data\nlabeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,\nChatGPT) show promising performance on zero-shot settings, thus inspiring us to\nexplore prompt-based methods. In this work, we ask whether strong IE models can\nbe constructed by directly prompting LLMs. Specifically, we transform the\nzero-shot IE task into a multi-turn question-answering problem with a two-stage\nframework (ChatIE). With the power of ChatGPT, we extensively evaluate our\nframework on three IE tasks: entity-relation triple extract, named entity\nrecognition, and event extraction. Empirical results on six datasets across two\nlanguages show that ChatIE achieves impressive performance and even surpasses\nsome full-shot models on several datasets (e.g., NYT11-HRL). We believe that\nour work could shed light on building IE models with limited resources.",
        "pdf_link": "https://arxiv.org/pdf/2302.10205v1.pdf"
    },
    {
        "title": "90% F1 Score in Relational Triple Extraction: Is it Real ?",
        "authors": [
            "Pratik Saini",
            "Samiran Pal",
            "Tapas Nayak",
            "Indrajit Bhattacharya"
        ],
        "published": "2023-02-20T10:30:16Z",
        "summary": "Extracting relational triples from text is a crucial task for constructing\nknowledge bases. Recent advancements in joint entity and relation extraction\nmodels have demonstrated remarkable F1 scores ($\\ge 90\\%$) in accurately\nextracting relational triples from free text. However, these models have been\nevaluated under restrictive experimental settings and unrealistic datasets.\nThey overlook sentences with zero triples (zero-cardinality), thereby\nsimplifying the task. In this paper, we present a benchmark study of\nstate-of-the-art joint entity and relation extraction models under a more\nrealistic setting. We include sentences that lack any triples in our\nexperiments, providing a comprehensive evaluation. Our findings reveal a\nsignificant decline (approximately 10-15\\% in one dataset and 6-14\\% in another\ndataset) in the models' F1 scores within this realistic experimental setup.\nFurthermore, we propose a two-step modeling approach that utilizes a simple\nBERT-based classifier. This approach leads to overall performance improvement\nin these models within the realistic experimental setting.",
        "pdf_link": "https://arxiv.org/pdf/2302.09887v2.pdf"
    },
    {
        "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
        "authors": [
            "Lorenz Kuhn",
            "Yarin Gal",
            "Sebastian Farquhar"
        ],
        "published": "2023-02-19T20:10:07Z",
        "summary": "We introduce a method to measure uncertainty in large language models. For\ntasks like question answering, it is essential to know when we can trust the\nnatural language outputs of foundation models. We show that measuring\nuncertainty in natural language is challenging because of \"semantic\nequivalence\" -- different sentences can mean the same thing. To overcome these\nchallenges we introduce semantic entropy -- an entropy which incorporates\nlinguistic invariances created by shared meanings. Our method is unsupervised,\nuses only a single model, and requires no modifications to off-the-shelf\nlanguage models. In comprehensive ablation studies we show that the semantic\nentropy is more predictive of model accuracy on question answering data sets\nthan comparable baselines.",
        "pdf_link": "https://arxiv.org/pdf/2302.09664v3.pdf"
    },
    {
        "title": "Language-Specific Representation of Emotion-Concept Knowledge Causally Supports Emotion Inference",
        "authors": [
            "Ming Li",
            "Yusheng Su",
            "Hsiu-Yuan Huang",
            "Jiali Cheng",
            "Xin Hu",
            "Xinmiao Zhang",
            "Huadong Wang",
            "Yujia Qin",
            "Xiaozhi Wang",
            "Kristen A. Lindquist",
            "Zhiyuan Liu",
            "Dan Zhang"
        ],
        "published": "2023-02-19T14:21:33Z",
        "summary": "Humans no doubt use language to communicate about their emotional\nexperiences, but does language in turn help humans understand emotions, or is\nlanguage just a vehicle of communication? This study used a form of artificial\nintelligence (AI) known as large language models (LLMs) to assess whether\nlanguage-based representations of emotion causally contribute to the AI's\nability to generate inferences about the emotional meaning of novel situations.\nFourteen attributes of human emotion concept representation were found to be\nrepresented by the LLM's distinct artificial neuron populations. By\nmanipulating these attribute-related neurons, we in turn demonstrated the role\nof emotion concept knowledge in generative emotion inference. The\nattribute-specific performance deterioration was related to the importance of\ndifferent attributes in human mental space. Our findings provide a\nproof-in-concept that even a LLM can learn about emotions in the absence of\nsensory-motor representations and highlight the contribution of\nlanguage-derived emotion-concept knowledge for emotion inference.",
        "pdf_link": "https://arxiv.org/pdf/2302.09582v5.pdf"
    },
    {
        "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
        "authors": [
            "Amr Hendy",
            "Mohamed Abdelrehim",
            "Amr Sharaf",
            "Vikas Raunak",
            "Mohamed Gabr",
            "Hitokazu Matsushita",
            "Young Jin Kim",
            "Mohamed Afify",
            "Hany Hassan Awadalla"
        ],
        "published": "2023-02-18T02:11:36Z",
        "summary": "Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.",
        "pdf_link": "https://arxiv.org/pdf/2302.09210v1.pdf"
    },
    {
        "title": "Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints",
        "authors": [
            "Albert Lu",
            "Hongxin Zhang",
            "Yanzhe Zhang",
            "Xuezhi Wang",
            "Diyi Yang"
        ],
        "published": "2023-02-17T23:30:28Z",
        "summary": "The limits of open-ended generative models are unclear, yet increasingly\nimportant. What causes them to succeed and what causes them to fail? In this\npaper, we take a prompt-centric approach to analyzing and bounding the\nabilities of open-ended generative models. We present a generic methodology of\nanalysis with two challenging prompt constraint types: structural and\nstylistic. These constraint types are categorized into a set of well-defined\nconstraints that are analyzable by a single prompt. We then systematically\ncreate a diverse set of simple, natural, and useful prompts to robustly analyze\neach individual constraint. Using the GPT-3 text-davinci-002 model as a case\nstudy, we generate outputs from our collection of prompts and analyze the\nmodel's generative failures. We also show the generalizability of our proposed\nmethod on other large models like BLOOM and OPT. Our results and our in-context\nmitigation strategies reveal open challenges for future research. We have\npublicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2302.09185v1.pdf"
    },
    {
        "title": "Complex QA and language models hybrid architectures, Survey",
        "authors": [
            "Xavier Daull",
            "Patrice Bellot",
            "Emmanuel Bruno",
            "Vincent Martin",
            "Elisabeth Murisasco"
        ],
        "published": "2023-02-17T18:31:31Z",
        "summary": "This paper reviews the state-of-the-art of language models architectures and\nstrategies for \"complex\" question-answering (QA, CQA, CPS) with a focus on\nhybridization. Large Language Models (LLM) are good at leveraging public data\non standard problems but once you want to tackle more specific complex\nquestions or problems (e.g. How does the concept of personal freedom vary\nbetween different cultures ? What is the best mix of power generation methods\nto reduce climate change ?) you may need specific architecture, knowledge,\nskills, methods, sensitive data protection, explainability, human approval and\nversatile feedback... Recent projects like ChatGPT and GALACTICA have allowed\nnon-specialists to grasp the great potential as well as the equally strong\nlimitations of LLM in complex QA. In this paper, we start by reviewing required\nskills and evaluation techniques. We integrate findings from the robust\ncommunity edited research papers BIG, BLOOM and HELM which open source,\nbenchmark and analyze limits and challenges of LLM in terms of tasks complexity\nand strict evaluation on accuracy (e.g. fairness, robustness, toxicity, ...) as\na baseline. We discuss some challenges associated with complex QA, including\ndomain adaptation, decomposition and efficient multi-step QA, long form and\nnon-factoid QA, safety and multi-sensitivity data protection, multimodal\nsearch, hallucinations, explainability and truthfulness, temporal reasoning. We\nanalyze current solutions and promising research trends, using elements such\nas: hybrid LLM architectural patterns, training and prompting strategies,\nactive human reinforcement learning supervised with AI, neuro-symbolic and\nstructured knowledge grounding, program synthesis, iterated decomposition and\nothers.",
        "pdf_link": "https://arxiv.org/pdf/2302.09051v4.pdf"
    },
    {
        "title": "Privately Customizing Prefinetuning to Better Match User Data in Federated Learning",
        "authors": [
            "Charlie Hou",
            "Hongyuan Zhan",
            "Akshat Shrivastava",
            "Sid Wang",
            "Aleksandr Livshits",
            "Giulia Fanti",
            "Daniel Lazar"
        ],
        "published": "2023-02-17T18:18:22Z",
        "summary": "In Federated Learning (FL), accessing private client data incurs\ncommunication and privacy costs. As a result, FL deployments commonly\nprefinetune pretrained foundation models on a (large, possibly public) dataset\nthat is held by the central server; they then FL-finetune the model on a\nprivate, federated dataset held by clients. Evaluating prefinetuning dataset\nquality reliably and privately is therefore of high importance. To this end, we\npropose FreD (Federated Private Fr\\'echet Distance) -- a privately computed\ndistance between a prefinetuning dataset and federated datasets. Intuitively,\nit privately computes and compares a Fr\\'echet distance between embeddings\ngenerated by a large language model on both the central (public) dataset and\nthe federated private client data. To make this computation privacy-preserving,\nwe use distributed, differentially-private mean and covariance estimators. We\nshow empirically that FreD accurately predicts the best prefinetuning dataset\nat minimal privacy cost. Altogether, using FreD we demonstrate a\nproof-of-concept for a new approach in private FL training: (1) customize a\nprefinetuning dataset to better match user data (2) prefinetune (3) perform\nFL-finetuning.",
        "pdf_link": "https://arxiv.org/pdf/2302.09042v2.pdf"
    },
    {
        "title": "Combining Generative Artificial Intelligence (AI) and the Internet: Heading towards Evolution or Degradation?",
        "authors": [
            "Gonzalo Martínez",
            "Lauren Watson",
            "Pedro Reviriego",
            "José Alberto Hernández",
            "Marc Juarez",
            "Rik Sarkar"
        ],
        "published": "2023-02-17T17:39:41Z",
        "summary": "In the span of a few months, generative Artificial Intelligence (AI) tools\nthat can generate realistic images or text have taken the Internet by storm,\nmaking them one of the technologies with fastest adoption ever. Some of these\ngenerative AI tools such as DALL-E, MidJourney, or ChatGPT have gained wide\npublic notoriety. Interestingly, these tools are possible because of the\nmassive amount of data (text and images) available on the Internet. The tools\nare trained on massive data sets that are scraped from Internet sites. And now,\nthese generative AI tools are creating massive amounts of new data that are\nbeing fed into the Internet. Therefore, future versions of generative AI tools\nwill be trained with Internet data that is a mix of original and AI-generated\ndata. As time goes on, a mixture of original data and data generated by\ndifferent versions of AI tools will populate the Internet. This raises a few\nintriguing questions: how will future versions of generative AI tools behave\nwhen trained on a mixture of real and AI generated data? Will they evolve with\nthe new data sets or degenerate? Will evolution introduce biases in subsequent\ngenerations of generative AI tools? In this document, we explore these\nquestions and report some very initial simulation results using a simple\nimage-generation AI tool. These results suggest that the quality of the\ngenerated images degrades as more AI-generated data is used for training thus\nsuggesting that generative AI may degenerate. Although these results are\npreliminary and cannot be generalised without further study, they serve to\nillustrate the potential issues of the interaction between generative AI and\nthe Internet.",
        "pdf_link": "https://arxiv.org/pdf/2303.01255v1.pdf"
    },
    {
        "title": "How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice, Education, and Research? An Exploratory Study",
        "authors": [
            "Fadel M. Megahed",
            "Ying-Ju Chen",
            "Joshua A. Ferris",
            "Sven Knoth",
            "L. Allison Jones-Farmer"
        ],
        "published": "2023-02-17T15:48:37Z",
        "summary": "Generative Artificial Intelligence (AI) models such as OpenAI's ChatGPT have\nthe potential to revolutionize Statistical Process Control (SPC) practice,\nlearning, and research. However, these tools are in the early stages of\ndevelopment and can be easily misused or misunderstood. In this paper, we give\nan overview of the development of Generative AI. Specifically, we explore\nChatGPT's ability to provide code, explain basic concepts, and create knowledge\nrelated to SPC practice, learning, and research. By investigating responses to\nstructured prompts, we highlight the benefits and limitations of the results.\nOur study indicates that the current version of ChatGPT performs well for\nstructured tasks, such as translating code from one language to another and\nexplaining well-known concepts but struggles with more nuanced tasks, such as\nexplaining less widely known terms and creating code from scratch. We find that\nusing new AI tools may help practitioners, educators, and researchers to be\nmore efficient and productive. However, in their current stages of development,\nsome results are misleading and wrong. Overall, the use of generative AI models\nin SPC must be properly validated and used in conjunction with other methods to\nensure accurate results.",
        "pdf_link": "https://arxiv.org/pdf/2302.10916v1.pdf"
    },
    {
        "title": "Massively Multilingual Shallow Fusion with Large Language Models",
        "authors": [
            "Ke Hu",
            "Tara N. Sainath",
            "Bo Li",
            "Nan Du",
            "Yanping Huang",
            "Andrew M. Dai",
            "Yu Zhang",
            "Rodrigo Cabrera",
            "Zhifeng Chen",
            "Trevor Strohman"
        ],
        "published": "2023-02-17T14:46:38Z",
        "summary": "While large language models (LLM) have made impressive progress in natural\nlanguage processing, it remains unclear how to utilize them in improving\nautomatic speech recognition (ASR). In this work, we propose to train a single\nmultilingual language model (LM) for shallow fusion in multiple languages. We\npush the limits of the multilingual LM to cover up to 84 languages by scaling\nup using a mixture-of-experts LLM, i.e., generalist language model (GLaM). When\nthe number of experts increases, GLaM dynamically selects only two at each\ndecoding step to keep the inference computation roughly constant. We then apply\nGLaM to a multilingual shallow fusion task based on a state-of-the-art\nend-to-end model. Compared to a dense LM of similar computation during\ninference, GLaM reduces the WER of an English long-tail test set by 4.4%\nrelative. In a multilingual shallow fusion task, GLaM improves 41 out of 50\nlanguages with an average relative WER reduction of 3.85%, and a maximum\nreduction of 10%. Compared to the baseline model, GLaM achieves an average WER\nreduction of 5.53% over 43 languages.",
        "pdf_link": "https://arxiv.org/pdf/2302.08917v1.pdf"
    },
    {
        "title": "Auditing large language models: a three-layered approach",
        "authors": [
            "Jakob Mökander",
            "Jonas Schuett",
            "Hannah Rose Kirk",
            "Luciano Floridi"
        ],
        "published": "2023-02-16T18:55:21Z",
        "summary": "Large language models (LLMs) represent a major advance in artificial\nintelligence (AI) research. However, the widespread use of LLMs is also coupled\nwith significant ethical and social challenges. Previous research has pointed\ntowards auditing as a promising governance mechanism to help ensure that AI\nsystems are designed and deployed in ways that are ethical, legal, and\ntechnically robust. However, existing auditing procedures fail to address the\ngovernance challenges posed by LLMs, which display emergent capabilities and\nare adaptable to a wide range of downstream tasks. In this article, we address\nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we\npropose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate LLMs), model audits (of LLMs after\npre-training but prior to their release), and application audits (of\napplications based on LLMs) complement and inform each other. We show how\naudits, when conducted in a structured and coordinated manner on all three\nlevels, can be a feasible and effective mechanism for identifying and managing\nsome of the ethical and social risks posed by LLMs. However, it is important to\nremain realistic about what auditing can reasonably be expected to achieve.\nTherefore, we discuss the limitations not only of our three-layered approach\nbut also of the prospect of auditing LLMs at all. Ultimately, this article\nseeks to expand the methodological toolkit available to technology providers\nand policymakers who wish to analyse and evaluate LLMs from technical, ethical,\nand legal perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2302.08500v2.pdf"
    },
    {
        "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution",
        "authors": [
            "Ansong Ni",
            "Srini Iyer",
            "Dragomir Radev",
            "Ves Stoyanov",
            "Wen-tau Yih",
            "Sida I. Wang",
            "Xi Victoria Lin"
        ],
        "published": "2023-02-16T18:23:22Z",
        "summary": "The advent of large language models trained on code (code LLMs) has led to\nsignificant progress in language-to-code generation. State-of-the-art\napproaches in this area combine LLM decoding with sample pruning and reranking\nusing test cases or heuristics based on the execution results. However, it is\nchallenging to obtain test cases for many real-world language-to-code\napplications, and heuristics cannot well capture the semantic features of the\nexecution results, such as data type and value range, which often indicates the\ncorrectness of the program. In this work, we propose LEVER, a simple approach\nto improve language-to-code generation by learning to verify the generated\nprograms with their execution results. Specifically, we train verifiers to\ndetermine whether a program sampled from the LLMs is correct or not based on\nthe natural language input, the program itself and its execution results. The\nsampled programs are reranked by combining the verification score with the LLM\ngeneration probability, and marginalizing over programs with the same execution\nresults. On four datasets across the domains of table QA, math QA and basic\nPython programming, LEVER consistently improves over the base code LLMs(4.6% to\n10.9% with code-davinci-002) and achieves new state-of-the-art results on all\nof them.",
        "pdf_link": "https://arxiv.org/pdf/2302.08468v3.pdf"
    },
    {
        "title": "Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks",
        "authors": [
            "Tomer Ullman"
        ],
        "published": "2023-02-16T16:18:03Z",
        "summary": "Intuitive psychology is a pillar of common-sense reasoning. The replication\nof this reasoning in machine intelligence is an important stepping-stone on the\nway to human-like artificial intelligence. Several recent tasks and benchmarks\nfor examining this reasoning in Large-Large Models have focused in particular\non belief attribution in Theory-of-Mind tasks. These tasks have shown both\nsuccesses and failures. We consider in particular a recent purported success\ncase, and show that small variations that maintain the principles of ToM turn\nthe results on their head. We argue that in general, the zero-hypothesis for\nmodel evaluation in intuitive psychology should be skeptical, and that outlying\nfailure cases should outweigh average success rates. We also consider what\npossible future successes on Theory-of-Mind tasks by more powerful LLMs would\nmean for ToM tasks with people.",
        "pdf_link": "https://arxiv.org/pdf/2302.08399v5.pdf"
    },
    {
        "title": "LEALLA: Learning Lightweight Language-agnostic Sentence Embeddings with Knowledge Distillation",
        "authors": [
            "Zhuoyuan Mao",
            "Tetsuji Nakagawa"
        ],
        "published": "2023-02-16T16:05:34Z",
        "summary": "Large-scale language-agnostic sentence embedding models such as LaBSE (Feng\net al., 2022) obtain state-of-the-art performance for parallel sentence\nalignment. However, these large-scale models can suffer from inference speed\nand computation overhead. This study systematically explores learning\nlanguage-agnostic sentence embeddings with lightweight models. We demonstrate\nthat a thin-deep encoder can construct robust low-dimensional sentence\nembeddings for 109 languages. With our proposed distillation methods, we\nachieve further improvements by incorporating knowledge from a teacher model.\nEmpirical results on Tatoeba, United Nations, and BUCC show the effectiveness\nof our lightweight models. We release our lightweight language-agnostic\nsentence embedding models LEALLA on TensorFlow Hub.",
        "pdf_link": "https://arxiv.org/pdf/2302.08387v2.pdf"
    },
    {
        "title": "Do We Still Need Clinical Language Models?",
        "authors": [
            "Eric Lehman",
            "Evan Hernandez",
            "Diwakar Mahajan",
            "Jonas Wulff",
            "Micah J. Smith",
            "Zachary Ziegler",
            "Daniel Nadler",
            "Peter Szolovits",
            "Alistair Johnson",
            "Emily Alsentzer"
        ],
        "published": "2023-02-16T05:08:34Z",
        "summary": "Although recent advances in scaling large language models (LLMs) have\nresulted in improvements on many NLP tasks, it remains unclear whether these\nmodels trained primarily with general web text are the right tool in highly\nspecialized, safety critical domains such as clinical text. Recent results have\nsuggested that LLMs encode a surprising amount of medical knowledge. This\nraises an important question regarding the utility of smaller domain-specific\nlanguage models. With the success of general-domain LLMs, is there still a need\nfor specialized clinical models? To investigate this question, we conduct an\nextensive empirical analysis of 12 language models, ranging from 220M to 175B\nparameters, measuring their performance on 3 different clinical tasks that test\ntheir ability to parse and reason over electronic health records. As part of\nour experiments, we train T5-Base and T5-Large models from scratch on clinical\nnotes from MIMIC III and IV to directly investigate the efficiency of clinical\ntokens. We show that relatively small specialized clinical models substantially\noutperform all in-context learning approaches, even when finetuned on limited\nannotated data. Further, we find that pretraining on clinical tokens allows for\nsmaller, more parameter-efficient models that either match or outperform much\nlarger language models trained on general text. We release the code and the\nmodels used under the PhysioNet Credentialed Health Data license and data use\nagreement.",
        "pdf_link": "https://arxiv.org/pdf/2302.08091v1.pdf"
    },
    {
        "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
        "authors": [
            "Alexander Scarlatos",
            "Andrew Lan"
        ],
        "published": "2023-02-15T22:38:34Z",
        "summary": "Mathematical language in scientific communications and educational scenarios\nis important yet relatively understudied compared to natural languages. Recent\nworks on mathematical language focus either on representing stand-alone\nmathematical expressions, especially in their natural tree format, or\nmathematical reasoning in pre-trained natural language models. Existing works\non jointly modeling and generating natural and mathematical languages simply\ntreat mathematical expressions as text, without accounting for the rigid\nstructural properties of mathematical expressions. In this paper, we propose a\nseries of modifications to existing language models to jointly represent and\ngenerate text and math: representing mathematical expressions as sequences of\nnode tokens in their operator tree format, using math symbol and tree position\nembeddings to preserve the semantic and structural properties of mathematical\nexpressions, and using a constrained decoding method to generate mathematically\nvalid expressions. We ground our modifications in GPT-2, resulting in a model\nMathGPT, and demonstrate that it outperforms baselines on mathematical\nexpression generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2302.07974v1.pdf"
    },
    {
        "title": "Commonsense Reasoning for Conversational AI: A Survey of the State of the Art",
        "authors": [
            "Christopher Richardson",
            "Larry Heck"
        ],
        "published": "2023-02-15T19:55:57Z",
        "summary": "Large, transformer-based pretrained language models like BERT, GPT, and T5\nhave demonstrated a deep understanding of contextual semantics and language\nsyntax. Their success has enabled significant advances in conversational AI,\nincluding the development of open-dialogue systems capable of coherent, salient\nconversations which can answer questions, chat casually, and complete tasks.\nHowever, state-of-the-art models still struggle with tasks that involve higher\nlevels of reasoning - including commonsense reasoning that humans find trivial.\nThis paper presents a survey of recent conversational AI research focused on\ncommonsense reasoning. The paper lists relevant training datasets and describes\nthe primary approaches to include commonsense in conversational AI. The paper\nalso discusses benchmarks used for evaluating commonsense in conversational AI\nproblems. Finally, the paper presents preliminary observations of the limited\ncommonsense capabilities of two state-of-the-art open dialogue models,\nBlenderBot3 and LaMDA, and its negative effect on natural interactions. These\nobservations further motivate research on commonsense reasoning in\nconversational AI.",
        "pdf_link": "https://arxiv.org/pdf/2302.07926v1.pdf"
    },
    {
        "title": "Speculative Decoding with Big Little Decoder",
        "authors": [
            "Sehoon Kim",
            "Karttikeya Mangalam",
            "Suhong Moon",
            "Jitendra Malik",
            "Michael W. Mahoney",
            "Amir Gholami",
            "Kurt Keutzer"
        ],
        "published": "2023-02-15T18:55:29Z",
        "summary": "The recent emergence of Large Language Models based on the Transformer\narchitecture has enabled dramatic advancements in the field of Natural Language\nProcessing. However, these models have long inference latency, which limits\ntheir deployment and makes them prohibitively expensive for various real-time\napplications. The inference latency is further exacerbated by autoregressive\ngenerative tasks, as models need to run iteratively to generate tokens\nsequentially without leveraging token-level parallelization. To address this,\nwe propose Big Little Decoder (BiLD), a framework that can improve inference\nefficiency and latency for a wide range of text generation applications. The\nBiLD framework contains two models with different sizes that collaboratively\ngenerate text. The small model runs autoregressively to generate text with a\nlow inference cost, and the large model is only invoked occasionally to refine\nthe small model's inaccurate predictions in a non-autoregressive manner. To\ncoordinate the small and large models, BiLD introduces two simple yet effective\npolicies: (1) the fallback policy that determines when to hand control over to\nthe large model; and (2) the rollback policy that determines when the large\nmodel needs to correct the small model's inaccurate predictions. To evaluate\nour framework across different tasks and models, we apply BiLD to various text\ngeneration scenarios encompassing machine translation on IWSLT 2017 De-En and\nWMT 2014 De-En, and summarization on XSUM and CNN/DailyMail. On an NVIDIA T4\nGPU, our framework achieves a speedup of up to 2.12x speedup with minimal\ngeneration quality degradation. Furthermore, our framework is fully\nplug-and-play and can be applied without any modifications in the training\nprocess or model architecture. Our code is open-sourced",
        "pdf_link": "https://arxiv.org/pdf/2302.07863v4.pdf"
    },
    {
        "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
        "authors": [
            "Marjan Ghazvininejad",
            "Hila Gonen",
            "Luke Zettlemoyer"
        ],
        "published": "2023-02-15T18:46:42Z",
        "summary": "Large language models (LLMs) demonstrate remarkable machine translation (MT)\nabilities via prompting, even though they were not explicitly trained for this\ntask. However, even given the incredible quantities of data they are trained\non, LLMs can struggle to translate inputs with rare words, which are common in\nlow resource or domain transfer scenarios. We show that LLM prompting can\nprovide an effective solution for rare words as well, by using prior knowledge\nfrom bilingual dictionaries to provide control hints in the prompts. We propose\na novel method, DiPMT, that provides a set of possible translations for a\nsubset of the input words, thereby enabling fine-grained phrase-level prompted\ncontrol of the LLM. Extensive experiments show that DiPMT outperforms the\nbaseline both in low-resource MT, as well as for out-of-domain MT. We further\nprovide a qualitative analysis of the benefits and limitations of this\napproach, including the overall level of controllability that is achieved.",
        "pdf_link": "https://arxiv.org/pdf/2302.07856v1.pdf"
    },
    {
        "title": "A Pilot Evaluation of ChatGPT and DALL-E 2 on Decision Making and Spatial Reasoning",
        "authors": [
            "Zhisheng Tang",
            "Mayank Kejriwal"
        ],
        "published": "2023-02-15T05:04:49Z",
        "summary": "We conduct a pilot study selectively evaluating the cognitive abilities\n(decision making and spatial reasoning) of two recently released generative\ntransformer models, ChatGPT and DALL-E 2. Input prompts were constructed\nfollowing neutral a priori guidelines, rather than adversarial intent. Post hoc\nqualitative analysis of the outputs shows that DALL-E 2 is able to generate at\nleast one correct image for each spatial reasoning prompt, but most images\ngenerated are incorrect (even though the model seems to have a clear\nunderstanding of the objects mentioned in the prompt). Similarly, in evaluating\nChatGPT on the rationality axioms developed under the classical Von\nNeumann-Morgenstern utility theorem, we find that, although it demonstrates\nsome level of rational decision-making, many of its decisions violate at least\none of the axioms even under reasonable constructions of preferences, bets, and\ndecision-making prompts. ChatGPT's outputs on such problems generally tended to\nbe unpredictable: even as it made irrational decisions (or employed an\nincorrect reasoning process) for some simpler decision-making problems, it was\nable to draw correct conclusions for more complex bet structures. We briefly\ncomment on the nuances and challenges involved in scaling up such a 'cognitive'\nevaluation or conducting it with a closed set of answer keys ('ground truth'),\ngiven that these models are inherently generative and open-ended in responding\nto prompts.",
        "pdf_link": "https://arxiv.org/pdf/2302.09068v1.pdf"
    },
    {
        "title": "Conversational AI-Powered Design: ChatGPT as Designer, User, and Product",
        "authors": [
            "A. Baki Kocaballi"
        ],
        "published": "2023-02-15T00:14:17Z",
        "summary": "The recent advancements in Large Language Models (LLMs), particularly\nconversational LLMs like ChatGPT, have prompted changes in a range of fields,\nincluding design. This study aims to examine the capabilities of ChatGPT in a\nhuman-centered design process. To this end, a hypothetical design project was\nconducted, where ChatGPT was utilized to generate personas, simulate interviews\nwith fictional users, create new design ideas, simulate usage scenarios and\nconversations between an imaginary prototype and fictional users, and lastly\nevaluate user experience. The results show that ChatGPT effectively performed\nthe tasks assigned to it as a designer, user, or product, providing mostly\nappropriate responses. The study does, however, highlight some drawbacks such\nas forgotten information, partial responses, and a lack of output diversity.\nThe paper explains the potential benefits and limitations of using\nconversational LLMs in design, discusses its implications, and suggests\ndirections for future research in this rapidly evolving area.",
        "pdf_link": "https://arxiv.org/pdf/2302.07406v1.pdf"
    },
    {
        "title": "Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models",
        "authors": [
            "Shrimai Prabhumoye",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "published": "2023-02-14T23:00:42Z",
        "summary": "Pretrained large language models have become indispensable for solving\nvarious natural language processing (NLP) tasks. However, safely deploying them\nin real world applications is challenging because they generate toxic content.\nTo address this challenge, we propose two novel pretraining data augmentation\nstrategies that significantly reduce model toxicity without compromising its\nutility. Our two strategies are: (1) MEDA: adds raw toxicity score as meta-data\nto the pretraining samples, and (2) INST: adds instructions to those samples\nindicating their toxicity. Our results indicate that our best performing\nstrategy (INST) substantially reduces the toxicity probability up to 61% while\npreserving the accuracy on five benchmark NLP tasks as well as improving AUC\nscores on four bias detection tasks by 1.3%. We also demonstrate the\ngeneralizability of our techniques by scaling the number of training samples\nand the number of model parameters.",
        "pdf_link": "https://arxiv.org/pdf/2302.07388v1.pdf"
    },
    {
        "title": "BiasTestGPT: Using ChatGPT for Social Bias Testing of Language Models",
        "authors": [
            "Rafal Kocielnik",
            "Shrimai Prabhumoye",
            "Vivian Zhang",
            "Roy Jiang",
            "R. Michael Alvarez",
            "Anima Anandkumar"
        ],
        "published": "2023-02-14T22:07:57Z",
        "summary": "Pretrained Language Models (PLMs) harbor inherent social biases that can\nresult in harmful real-world implications. Such social biases are measured\nthrough the probability values that PLMs output for different social groups and\nattributes appearing in a set of test sentences. However, bias testing is\ncurrently cumbersome since the test sentences are generated either from a\nlimited set of manual templates or need expensive crowd-sourcing. We instead\npropose using ChatGPT for the controllable generation of test sentences, given\nany arbitrary user-specified combination of social groups and attributes\nappearing in the test sentences. When compared to template-based methods, our\napproach using ChatGPT for test sentence generation is superior in detecting\nsocial bias, especially in challenging settings such as intersectional biases.\nWe present an open-source comprehensive bias testing framework (BiasTestGPT),\nhosted on HuggingFace, that can be plugged into any open-source PLM for bias\ntesting. User testing with domain experts from various fields has shown their\ninterest in being able to test modern AI for social biases. Our tool has\nsignificantly improved their awareness of such biases in PLMs, proving to be\nlearnable and user-friendly. We thus enable seamless open-ended social bias\ntesting of PLMs by domain experts through an automatic large-scale generation\nof diverse test sentences for any combination of social categories and\nattributes.",
        "pdf_link": "https://arxiv.org/pdf/2302.07371v3.pdf"
    },
    {
        "title": "ScatterShot: Interactive In-context Example Curation for Text Transformation",
        "authors": [
            "Tongshuang Wu",
            "Hua Shen",
            "Daniel S. Weld",
            "Jeffrey Heer",
            "Marco Tulio Ribeiro"
        ],
        "published": "2023-02-14T21:13:31Z",
        "summary": "The in-context learning capabilities of LLMs like GPT-3 allow annotators to\ncustomize an LLM to their specific tasks with a small number of examples.\nHowever, users tend to include only the most obvious patterns when crafting\nexamples, resulting in underspecified in-context functions that fall short on\nunseen cases. Further, it is hard to know when \"enough\" examples have been\nincluded even for known patterns. In this work, we present ScatterShot, an\ninteractive system for building high-quality demonstration sets for in-context\nlearning. ScatterShot iteratively slices unlabeled data into task-specific\npatterns, samples informative inputs from underexplored or not-yet-saturated\nslices in an active learning manner, and helps users label more efficiently\nwith the help of an LLM and the current example set. In simulation studies on\ntwo text perturbation scenarios, ScatterShot sampling improves the resulting\nfew-shot functions by 4-5 percentage points over random sampling, with less\nvariance as more examples are added. In a user study, ScatterShot greatly helps\nusers in covering different patterns in the input space and labeling in-context\nexamples more efficiently, resulting in better in-context learning and less\nuser effort.",
        "pdf_link": "https://arxiv.org/pdf/2302.07346v1.pdf"
    },
    {
        "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
        "authors": [
            "Sheng Wang",
            "Zihao Zhao",
            "Xi Ouyang",
            "Qian Wang",
            "Dinggang Shen"
        ],
        "published": "2023-02-14T18:54:06Z",
        "summary": "Large language models (LLMs) have recently demonstrated their potential in\nclinical applications, providing valuable medical knowledge and advice. For\nexample, a large dialog LLM like ChatGPT has successfully passed part of the US\nmedical licensing exam. However, LLMs currently have difficulty processing\nimages, making it challenging to interpret information from medical images,\nwhich are rich in information that supports clinical decisions. On the other\nhand, computer-aided diagnosis (CAD) networks for medical images have seen\nsignificant success in the medical field by using advanced deep-learning\nalgorithms to support clinical decision-making. This paper presents a method\nfor integrating LLMs into medical-image CAD networks. The proposed framework\nuses LLMs to enhance the output of multiple CAD networks, such as diagnosis\nnetworks, lesion segmentation networks, and report generation networks, by\nsummarizing and reorganizing the information presented in natural language text\nformat. The goal is to merge the strengths of LLMs' medical domain knowledge\nand logical reasoning with the vision understanding capability of existing\nmedical-image CAD models to create a more user-friendly and understandable\nsystem for patients compared to conventional CAD systems. In the future, LLM's\nmedical knowledge can be also used to improve the performance of vision-based\nmedical-image CAD models.",
        "pdf_link": "https://arxiv.org/pdf/2302.07257v1.pdf"
    },
    {
        "title": "Few-shot learning approaches for classifying low resource domain specific software requirements",
        "authors": [
            "Anmol Nayak",
            "Hari Prasad Timmapathini",
            "Vidhya Murali",
            "Atul Anil Gohad"
        ],
        "published": "2023-02-14T10:19:23Z",
        "summary": "With the advent of strong pre-trained natural language processing models like\nBERT, DeBERTa, MiniLM, T5, the data requirement for industries to fine-tune\nthese models to their niche use cases has drastically reduced (typically to a\nfew hundred annotated samples for achieving a reasonable performance). However,\nthe availability of even a few hundred annotated samples may not always be\nguaranteed in low resource domains like automotive, which often limits the\nusage of such deep learning models in an industrial setting. In this paper we\naim to address the challenge of fine-tuning such pre-trained models with only a\nfew annotated samples, also known as Few-shot learning. Our experiments focus\non evaluating the performance of a diverse set of algorithms and methodologies\nto achieve the task of classifying BOSCH automotive domain textual software\nrequirements into 3 categories, while utilizing only 15 annotated samples per\ncategory for fine-tuning. We find that while SciBERT and DeBERTa based models\ntend to be the most accurate at 15 training samples, their performance\nimprovement scales minimally as the number of annotated samples is increased to\n50 in comparison to Siamese and T5 based models.",
        "pdf_link": "https://arxiv.org/pdf/2302.06951v1.pdf"
    },
    {
        "title": "Learning gain differences between ChatGPT and human tutor generated algebra hints",
        "authors": [
            "Zachary A. Pardos",
            "Shreya Bhandari"
        ],
        "published": "2023-02-14T07:20:48Z",
        "summary": "Large Language Models (LLMs), such as ChatGPT, are quickly advancing AI to\nthe frontiers of practical consumer use and leading industries to re-evaluate\nhow they allocate resources for content production. Authoring of open\neducational resources and hint content within adaptive tutoring systems is\nlabor intensive. Should LLMs like ChatGPT produce educational content on par\nwith human-authored content, the implications would be significant for further\nscaling of computer tutoring system approaches. In this paper, we conduct the\nfirst learning gain evaluation of ChatGPT by comparing the efficacy of its\nhints with hints authored by human tutors with 77 participants across two\nalgebra topic areas, Elementary Algebra and Intermediate Algebra. We find that\n70% of hints produced by ChatGPT passed our manual quality checks and that both\nhuman and ChatGPT conditions produced positive learning gains. However, gains\nwere only statistically significant for human tutor created hints. Learning\ngains from human-created hints were substantially and statistically\nsignificantly higher than ChatGPT hints in both topic areas, though ChatGPT\nparticipants in the Intermediate Algebra experiment were near ceiling and not\neven with the control at pre-test. We discuss the limitations of our study and\nsuggest several future directions for the field. Problem and hint content used\nin the experiment is provided for replicability.",
        "pdf_link": "https://arxiv.org/pdf/2302.06871v1.pdf"
    },
    {
        "title": "Machine Learning Model Attribution Challenge",
        "authors": [
            "Elizabeth Merkhofer",
            "Deepesh Chaudhari",
            "Hyrum S. Anderson",
            "Keith Manville",
            "Lily Wong",
            "João Gante"
        ],
        "published": "2023-02-13T22:05:27Z",
        "summary": "We present the findings of the Machine Learning Model Attribution Challenge.\nFine-tuned machine learning models may derive from other trained models without\nobvious attribution characteristics. In this challenge, participants identify\nthe publicly-available base models that underlie a set of anonymous, fine-tuned\nlarge language models (LLMs) using only textual output of the models.\nContestants aim to correctly attribute the most fine-tuned models, with ties\nbroken in the favor of contestants whose solutions use fewer calls to the\nfine-tuned models' API. The most successful approaches were manual, as\nparticipants observed similarities between model outputs and developed\nattribution heuristics based on public documentation of the base models, though\nseveral teams also submitted automated, statistical solutions.",
        "pdf_link": "https://arxiv.org/pdf/2302.06716v3.pdf"
    },
    {
        "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
        "authors": [
            "Karthik Valmeekam",
            "Sarath Sreedharan",
            "Matthew Marquez",
            "Alberto Olmo",
            "Subbarao Kambhampati"
        ],
        "published": "2023-02-13T21:37:41Z",
        "summary": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on\ngeneral web corpora, in this paper, we set out to investigate their planning\ncapabilities. We aim to evaluate (1) how good LLMs are by themselves in\ngenerating and validating simple plans in commonsense planning tasks (of the\ntype that humans are generally quite good at) and (2) how good LLMs are in\nbeing a source of heuristic guidance for other agents--either AI planners or\nhuman planners--in their planning tasks. To investigate these questions in a\nsystematic rather than anecdotal manner, we start by developing a benchmark\nsuite based on the kinds of domains employed in the International Planning\nCompetition. On this benchmark, we evaluate LLMs in three modes: autonomous,\nheuristic and human-in-the-loop. Our results show that LLM's ability to\nautonomously generate executable plans is quite meager, averaging only about 3%\nsuccess rate. The heuristic and human-in-the-loop modes show slightly more\npromise. In addition to these results, we also make our benchmark and\nevaluation tools available to support investigations by research community.",
        "pdf_link": "https://arxiv.org/pdf/2302.06706v1.pdf"
    },
    {
        "title": "Guiding Pretraining in Reinforcement Learning with Large Language Models",
        "authors": [
            "Yuqing Du",
            "Olivia Watkins",
            "Zihan Wang",
            "Cédric Colas",
            "Trevor Darrell",
            "Pieter Abbeel",
            "Abhishek Gupta",
            "Jacob Andreas"
        ],
        "published": "2023-02-13T21:16:03Z",
        "summary": "Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps://github.com/yuqingd/ellm.",
        "pdf_link": "https://arxiv.org/pdf/2302.06692v2.pdf"
    },
    {
        "title": "Gradient-Based Automated Iterative Recovery for Parameter-Efficient Tuning",
        "authors": [
            "Maximilian Mozes",
            "Tolga Bolukbasi",
            "Ann Yuan",
            "Frederick Liu",
            "Nithum Thain",
            "Lucas Dixon"
        ],
        "published": "2023-02-13T18:54:58Z",
        "summary": "Pretrained large language models (LLMs) are able to solve a wide variety of\ntasks through transfer learning. Various explainability methods have been\ndeveloped to investigate their decision making process. TracIn (Pruthi et al.,\n2020) is one such gradient-based method which explains model inferences based\non the influence of training examples. In this paper, we explore the use of\nTracIn to improve model performance in the parameter-efficient tuning (PET)\nsetting. We develop conversational safety classifiers via the prompt-tuning PET\nmethod and show how the unique characteristics of the PET regime enable TracIn\nto identify the cause for certain misclassifications by LLMs. We develop a new\nmethodology for using gradient-based explainability techniques to improve model\nperformance, G-BAIR: gradient-based automated iterative recovery. We show that\nG-BAIR can recover LLM performance on benchmarks after manually corrupting\ntraining labels. This suggests that influence methods like TracIn can be used\nto automatically perform data cleaning, and introduces the potential for\ninteractive debugging and relabeling for PET-based transfer learning methods.",
        "pdf_link": "https://arxiv.org/pdf/2302.06598v1.pdf"
    },
    {
        "title": "Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction Challenge",
        "authors": [
            "Ali Al-Kaswan",
            "Maliheh Izadi",
            "Arie van Deursen"
        ],
        "published": "2023-02-13T18:00:44Z",
        "summary": "Previous work has shown that Large Language Models are susceptible to\nso-called data extraction attacks. This allows an attacker to extract a sample\nthat was contained in the training data, which has massive privacy\nimplications. The construction of data extraction attacks is challenging,\ncurrent attacks are quite inefficient, and there exists a significant gap in\nthe extraction capabilities of untargeted attacks and memorization. Thus,\ntargeted attacks are proposed, which identify if a given sample from the\ntraining data, is extractable from a model. In this work, we apply a targeted\ndata extraction attack to the SATML2023 Language Model Training Data Extraction\nChallenge. We apply a two-step approach. In the first step, we maximise the\nrecall of the model and are able to extract the suffix for 69% of the samples.\nIn the second step, we use a classifier-based Membership Inference Attack on\nthe generations. Our AutoSklearn classifier achieves a precision of 0.841. The\nfull approach reaches a score of 0.405 recall at a 10% false positive rate,\nwhich is an improvement of 34% over the baseline of 0.301.",
        "pdf_link": "https://arxiv.org/pdf/2302.07735v1.pdf"
    },
    {
        "title": "Diminished Diversity-of-Thought in a Standard Large Language Model",
        "authors": [
            "Peter S. Park",
            "Philipp Schoenegger",
            "Chongyang Zhu"
        ],
        "published": "2023-02-13T17:57:50Z",
        "summary": "We test whether Large Language Models (LLMs) can be used to simulate human\nparticipants in social-science studies. To do this, we run replications of 14\nstudies from the Many Labs 2 replication project with OpenAI's text-davinci-003\nmodel, colloquially known as GPT3.5. Based on our pre-registered analyses, we\nfind that among the eight studies we could analyse, our GPT sample replicated\n37.5% of the original results and 37.5% of the Many Labs 2 results. However, we\nwere unable to analyse the remaining six studies due to an unexpected\nphenomenon we call the \"correct answer\" effect. Different runs of GPT3.5\nanswered nuanced questions probing political orientation, economic preference,\njudgement, and moral philosophy with zero or near-zero variation in responses:\nwith the supposedly \"correct answer.\" In one exploratory follow-up study, we\nfound that a \"correct answer\" was robust to changing the demographic details\nthat precede the prompt. In another, we found that most but not all \"correct\nanswers\" were robust to changing the order of answer choices. One of our most\nstriking findings occurred in our replication of the Moral Foundations Theory\nsurvey results, where we found GPT3.5 identifying as a political conservative\nin 99.6% of the cases, and as a liberal in 99.3% of the cases in the\nreverse-order condition. However, both self-reported 'GPT conservatives' and\n'GPT liberals' showed right-leaning moral foundations. Our results cast doubts\non the validity of using LLMs as a general replacement for human participants\nin the social sciences. Our results also raise concerns that a hypothetical\nAI-led future may be subject to a diminished diversity-of-thought.",
        "pdf_link": "https://arxiv.org/pdf/2302.07267v6.pdf"
    },
    {
        "title": "Implications of the Convergence of Language and Vision Model Geometries",
        "authors": [
            "Jiaang Li",
            "Yova Kementchedjhieva",
            "Anders Søgaard"
        ],
        "published": "2023-02-13T17:55:54Z",
        "summary": "Large-scale pretrained language models (LMs) are said to ``lack the ability\nto connect [their] utterances to the world'' (Bender and Koller, 2020). If so,\nwe would expect LM representations to be unrelated to representations in\ncomputer vision models. To investigate this, we present an empirical evaluation\nacross three different LMs (BERT, GPT2, and OPT) and three computer vision\nmodels (VMs, including ResNet, SegFormer, and MAE). Our experiments show that\nLMs converge towards representations that are partially isomorphic to those of\nVMs, with dispersion, and polysemy both factoring into the alignability of\nvision and language spaces. We discuss the implications of this finding.",
        "pdf_link": "https://arxiv.org/pdf/2302.06555v1.pdf"
    },
    {
        "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
        "authors": [
            "Max Schäfer",
            "Sarah Nadi",
            "Aryaz Eghbali",
            "Frank Tip"
        ],
        "published": "2023-02-13T17:13:41Z",
        "summary": "Unit tests play a key role in ensuring the correctness of software. However,\nmanually creating unit tests is a laborious task, motivating the need for\nautomation. Large Language Models (LLMs) have recently been applied to this\nproblem, utilizing additional training or few-shot learning on examples of\nexisting tests. This paper presents a large-scale empirical evaluation on the\neffectiveness of LLMs for automated unit test generation without additional\ntraining or manual effort, providing the LLM with the signature and\nimplementation of the function under test, along with usage examples extracted\nfrom documentation. We also attempt to repair failed generated tests by\nre-prompting the model with the failing test and error message. We implement\nour approach in TestPilot, a test generation tool for JavaScript that\nautomatically generates unit tests for all API functions in an npm package. We\nevaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a\ntotal of 1,684 API functions. The generated tests achieve a median statement\ncoverage of 70.2% and branch coverage of 52.8%, significantly improving on\nNessie, a recent feedback-directed JavaScript test generation technique, which\nachieves only 51.3% statement coverage and 25.6% branch coverage. We also find\nthat 92.8% of TestPilot's generated tests have no more than 50% similarity with\nexisting tests (as measured by normalized edit distance), with none of them\nbeing exact copies. Finally, we run TestPilot with two additional LLMs,\nOpenAI's older code-cushman-002 LLM and the open LLM StarCoder. Overall, we\nobserved similar results with the former (68.2% median statement coverage), and\nsomewhat worse results with the latter (54.0% median statement coverage),\nsuggesting that the effectiveness of the approach is influenced by the size and\ntraining set of the LLM, but does not fundamentally depend on the specific\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2302.06527v4.pdf"
    },
    {
        "title": "Can GPT-3 Perform Statutory Reasoning?",
        "authors": [
            "Andrew Blair-Stanek",
            "Nils Holzenberger",
            "Benjamin Van Durme"
        ],
        "published": "2023-02-13T04:56:11Z",
        "summary": "Statutory reasoning is the task of reasoning with facts and statutes, which\nare rules written in natural language by a legislature. It is a basic legal\nskill. In this paper we explore the capabilities of the most capable GPT-3\nmodel, text-davinci-003, on an established statutory-reasoning dataset called\nSARA. We consider a variety of approaches, including dynamic few-shot\nprompting, chain-of-thought prompting, and zero-shot prompting. While we\nachieve results with GPT-3 that are better than the previous best published\nresults, we also identify several types of clear errors it makes. We\ninvestigate why these errors happen. We discover that GPT-3 has imperfect prior\nknowledge of the actual U.S. statutes on which SARA is based. More importantly,\nwe create simple synthetic statutes, which GPT-3 is guaranteed not to have seen\nduring training. We find GPT-3 performs poorly at answering straightforward\nquestions about these simple synthetic statutes.",
        "pdf_link": "https://arxiv.org/pdf/2302.06100v2.pdf"
    },
    {
        "title": "Predicting Class Distribution Shift for Reliable Domain Adaptive Object Detection",
        "authors": [
            "Nicolas Harvey Chapman",
            "Feras Dayoub",
            "Will Browne",
            "Christopher Lehnert"
        ],
        "published": "2023-02-13T00:46:34Z",
        "summary": "Unsupervised Domain Adaptive Object Detection (UDA-OD) uses unlabelled data\nto improve the reliability of robotic vision systems in open-world\nenvironments. Previous approaches to UDA-OD based on self-training have been\neffective in overcoming changes in the general appearance of images. However,\nshifts in a robot's deployment environment can also impact the likelihood that\ndifferent objects will occur, termed class distribution shift. Motivated by\nthis, we propose a framework for explicitly addressing class distribution shift\nto improve pseudo-label reliability in self-training. Our approach uses the\ndomain invariance and contextual understanding of a pre-trained joint vision\nand language model to predict the class distribution of unlabelled data. By\naligning the class distribution of pseudo-labels with this prediction, we\nprovide weak supervision of pseudo-label accuracy. To further account for low\nquality pseudo-labels early in self-training, we propose an approach to\ndynamically adjust the number of pseudo-labels per image based on model\nconfidence. Our method outperforms state-of-the-art approaches on several\nbenchmarks, including a 4.7 mAP improvement when facing challenging class\ndistribution shift.",
        "pdf_link": "https://arxiv.org/pdf/2302.06039v2.pdf"
    },
    {
        "title": "MarioGPT: Open-Ended Text2Level Generation through Large Language Models",
        "authors": [
            "Shyam Sudhakaran",
            "Miguel González-Duque",
            "Claire Glanois",
            "Matthias Freiberger",
            "Elias Najarro",
            "Sebastian Risi"
        ],
        "published": "2023-02-12T19:12:24Z",
        "summary": "Procedural Content Generation (PCG) is a technique to generate complex and\ndiverse environments in an automated way. However, while generating content\nwith PCG methods is often straightforward, generating meaningful content that\nreflects specific intentions and constraints remains challenging. Furthermore,\nmany PCG algorithms lack the ability to generate content in an open-ended\nmanner. Recently, Large Language Models (LLMs) have shown to be incredibly\neffective in many diverse domains. These trained LLMs can be fine-tuned,\nre-using information and accelerating training for new tasks. Here, we\nintroduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game\nlevels, in our case Super Mario Bros levels. MarioGPT can not only generate\ndiverse levels, but can be text-prompted for controllable level generation,\naddressing one of the key challenges of current PCG techniques. As far as we\nknow, MarioGPT is the first text-to-level model and combined with novelty\nsearch it enables the generation of diverse levels with varying play-style\ndynamics (i.e. player paths) and the open-ended discovery of an increasingly\ndiverse range of content. Code available at\nhttps://github.com/shyamsn97/mario-gpt.",
        "pdf_link": "https://arxiv.org/pdf/2302.05981v3.pdf"
    },
    {
        "title": "Level Generation Through Large Language Models",
        "authors": [
            "Graham Todd",
            "Sam Earle",
            "Muhammad Umair Nasir",
            "Michael Cerny Green",
            "Julian Togelius"
        ],
        "published": "2023-02-11T23:34:42Z",
        "summary": "Large Language Models (LLMs) are powerful tools, capable of leveraging their\ntraining on natural language to write stories, generate code, and answer\nquestions. But can they generate functional video game levels? Game levels,\nwith their complex functional constraints and spatial relationships in more\nthan one dimension, are very different from the kinds of data an LLM typically\nsees during training. Datasets of game levels are also hard to come by,\npotentially taxing the abilities of these data-hungry models. We investigate\nthe use of LLMs to generate levels for the game Sokoban, finding that LLMs are\nindeed capable of doing so, and that their performance scales dramatically with\ndataset size. We also perform preliminary experiments on controlling LLM level\ngenerators and discuss promising areas for future work.",
        "pdf_link": "https://arxiv.org/pdf/2302.05817v2.pdf"
    },
    {
        "title": "Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes",
        "authors": [
            "Shruthi Chari",
            "Prasant Acharya",
            "Daniel M. Gruen",
            "Olivia Zhang",
            "Elif K. Eyigoz",
            "Mohamed Ghalwash",
            "Oshani Seneviratne",
            "Fernando Suarez Saiz",
            "Pablo Meyer",
            "Prithwish Chakraborty",
            "Deborah L. McGuinness"
        ],
        "published": "2023-02-11T18:07:11Z",
        "summary": "Medical experts may use Artificial Intelligence (AI) systems with greater\ntrust if these are supported by contextual explanations that let the\npractitioner connect system inferences to their context of use. However, their\nimportance in improving model usage and understanding has not been extensively\nstudied. Hence, we consider a comorbidity risk prediction scenario and focus on\ncontexts regarding the patients clinical state, AI predictions about their risk\nof complications, and algorithmic explanations supporting the predictions. We\nexplore how relevant information for such dimensions can be extracted from\nMedical guidelines to answer typical questions from clinical practitioners. We\nidentify this as a question answering (QA) task and employ several\nstate-of-the-art LLMs to present contexts around risk prediction model\ninferences and evaluate their acceptability. Finally, we study the benefits of\ncontextual explanations by building an end-to-end AI pipeline including data\ncohorting, AI risk modeling, post-hoc model explanations, and prototyped a\nvisual dashboard to present the combined insights from different context\ndimensions and data sources, while predicting and identifying the drivers of\nrisk of Chronic Kidney Disease - a common type-2 diabetes comorbidity. All of\nthese steps were performed in engagement with medical experts, including a\nfinal evaluation of the dashboard results by an expert medical panel. We show\nthat LLMs, in particular BERT and SciBERT, can be readily deployed to extract\nsome relevant explanations to support clinical usage. To understand the\nvalue-add of the contextual explanations, the expert panel evaluated these\nregarding actionable insights in the relevant clinical setting. Overall, our\npaper is one of the first end-to-end analyses identifying the feasibility and\nbenefits of contextual explanations in a real-world clinical use case.",
        "pdf_link": "https://arxiv.org/pdf/2302.05752v1.pdf"
    },
    {
        "title": "Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",
        "authors": [
            "Daniel Kang",
            "Xuechen Li",
            "Ion Stoica",
            "Carlos Guestrin",
            "Matei Zaharia",
            "Tatsunori Hashimoto"
        ],
        "published": "2023-02-11T15:57:44Z",
        "summary": "Recent advances in instruction-following large language models (LLMs) have\nled to dramatic improvements in a range of NLP tasks. Unfortunately, we find\nthat the same improved capabilities amplify the dual-use risks for malicious\npurposes of these models. Dual-use is difficult to prevent as\ninstruction-following capabilities now enable standard attacks from computer\nsecurity. The capabilities of these instruction-following LLMs provide strong\neconomic incentives for dual-use by malicious actors. In particular, we show\nthat instruction-following LLMs can produce targeted malicious content,\nincluding hate speech and scams, bypassing in-the-wild defenses implemented by\nLLM API vendors. Our analysis shows that this content can be generated\neconomically and at cost likely lower than with human effort alone. Together,\nour findings suggest that LLMs will increasingly attract more sophisticated\nadversaries and attacks, and addressing these attacks may require new\napproaches to mitigations.",
        "pdf_link": "https://arxiv.org/pdf/2302.05733v1.pdf"
    },
    {
        "title": "Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
        "authors": [
            "Fan Huang",
            "Haewoon Kwak",
            "Jisun An"
        ],
        "published": "2023-02-11T03:13:54Z",
        "summary": "Recent studies have alarmed that many online hate speeches are implicit. With\nits subtle nature, the explainability of the detection of such hateful speech\nhas been a challenging problem. In this work, we examine whether ChatGPT can be\nused for providing natural language explanations (NLEs) for implicit hateful\nspeech detection. We design our prompt to elicit concise ChatGPT-generated NLEs\nand conduct user studies to evaluate their qualities by comparison with\nhuman-written NLEs. We discuss the potential and limitations of ChatGPT in the\ncontext of implicit hateful speech research.",
        "pdf_link": "https://arxiv.org/pdf/2302.07736v2.pdf"
    },
    {
        "title": "Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models",
        "authors": [
            "Renat Aksitov",
            "Chung-Ching Chang",
            "David Reitter",
            "Siamak Shakeri",
            "Yunhsuan Sung"
        ],
        "published": "2023-02-11T02:43:34Z",
        "summary": "Despite recent progress, it has been difficult to prevent semantic\nhallucinations in generative Large Language Models. One common solution to this\nis augmenting LLMs with a retrieval system and making sure that the generated\noutput is attributable to the retrieved information. Given this new added\nconstraint, it is plausible to expect that the overall quality of the output\nwill be affected, for example, in terms of fluency. Can scaling language models\nhelp?\n  Here we examine the relationship between fluency and attribution in LLMs\nprompted with retrieved evidence in knowledge-heavy dialog settings. Our\nexperiments were implemented with a set of auto-metrics that are aligned with\nhuman preferences. They were used to evaluate a large set of generations,\nproduced under varying parameters of LLMs and supplied context.\n  We show that larger models tend to do much better in both fluency and\nattribution, and that (naively) using top-k retrieval versus top-1 retrieval\nimproves attribution but hurts fluency. We next propose a recipe that could\nallow smaller models to both close the gap with larger models and preserve the\nbenefits of top-k retrieval while avoiding its drawbacks.",
        "pdf_link": "https://arxiv.org/pdf/2302.05578v2.pdf"
    },
    {
        "title": "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models",
        "authors": [
            "Hrishikesh Viswanath",
            "Tianyi Zhang"
        ],
        "published": "2023-02-10T20:54:10Z",
        "summary": "Studies have shown that large pretrained language models exhibit biases\nagainst social groups based on race, gender etc, which they inherit from the\ndatasets they are trained on. Various researchers have proposed mathematical\ntools for quantifying and identifying these biases. There have been methods\nproposed to mitigate such biases. In this paper, we present a comprehensive\nquantitative evaluation of different kinds of biases such as race, gender,\nethnicity, age etc. exhibited by popular pretrained language models such as\nBERT, GPT-2 etc. and also present a toolkit that provides plug-and-play\ninterfaces to connect mathematical tools to identify biases with large\npretrained language models such as BERT, GPT-2 etc. and also present users with\nthe opportunity to test custom models against these metrics. The toolkit also\nallows users to debias existing and custom models using the debiasing\ntechniques proposed so far. The toolkit is available at\nhttps://github.com/HrishikeshVish/Fairpy.",
        "pdf_link": "https://arxiv.org/pdf/2302.05508v1.pdf"
    },
    {
        "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
        "authors": [
            "Jingxuan He",
            "Martin Vechev"
        ],
        "published": "2023-02-10T15:28:55Z",
        "summary": "Large language models (large LMs) are increasingly trained on massive\ncodebases and used to generate code. However, LMs lack awareness of security\nand are found to frequently produce unsafe code. This work studies the security\nof LMs along two important axes: (i) security hardening, which aims to enhance\nLMs' reliability in generating secure code, and (ii) adversarial testing, which\nseeks to evaluate LMs' security at an adversarial standpoint. We address both\nof these by formulating a new security task called controlled code generation.\nThe task is parametric and takes as input a binary property to guide the LM to\ngenerate secure or unsafe code, while preserving the LM's capability of\ngenerating functionally correct code. We propose a novel learning-based\napproach called SVEN to solve this task. SVEN leverages property-specific\ncontinuous vectors to guide program generation towards the given property,\nwithout modifying the LM's weights. Our training procedure optimizes these\ncontinuous vectors by enforcing specialized loss terms on different regions of\ncode, using a high-quality dataset carefully curated by us. Our extensive\nevaluation shows that SVEN is highly effective in achieving strong security\ncontrol. For instance, a state-of-the-art CodeGen LM with 2.7B parameters\ngenerates secure code for 59.1% of the time. When we employ SVEN to perform\nsecurity hardening (or adversarial testing) on this LM, the ratio is\nsignificantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN\nclosely matches the original LMs in functional correctness.",
        "pdf_link": "https://arxiv.org/pdf/2302.05319v4.pdf"
    },
    {
        "title": "Translating Natural Language to Planning Goals with Large-Language Models",
        "authors": [
            "Yaqi Xie",
            "Chen Yu",
            "Tongyao Zhu",
            "Jinbin Bai",
            "Ze Gong",
            "Harold Soh"
        ],
        "published": "2023-02-10T09:17:52Z",
        "summary": "Recent large language models (LLMs) have demonstrated remarkable performance\non a variety of natural language processing (NLP) tasks, leading to intense\nexcitement about their applicability across various domains. Unfortunately,\nrecent work has also shown that LLMs are unable to perform accurate reasoning\nnor solve planning problems, which may limit their usefulness for\nrobotics-related tasks. In this work, our central question is whether LLMs are\nable to translate goals specified in natural language to a structured planning\nlanguage. If so, LLM can act as a natural interface between the planner and\nhuman users; the translated goal can be handed to domain-independent AI\nplanners that are very effective at planning. Our empirical results on GPT 3.5\nvariants show that LLMs are much better suited towards translation rather than\nplanning. We find that LLMs are able to leverage commonsense knowledge and\nreasoning to furnish missing details from under-specified goals (as is often\nthe case in natural language). However, our experiments also reveal that LLMs\ncan fail to generate goals in tasks that involve numerical or physical (e.g.,\nspatial) reasoning, and that LLMs are sensitive to the prompts used. As such,\nthese models are promising for translation to structured planning languages,\nbut care should be taken in their use.",
        "pdf_link": "https://arxiv.org/pdf/2302.05128v1.pdf"
    },
    {
        "title": "In-Context Learning with Many Demonstration Examples",
        "authors": [
            "Mukai Li",
            "Shansan Gong",
            "Jiangtao Feng",
            "Yiheng Xu",
            "Jun Zhang",
            "Zhiyong Wu",
            "Lingpeng Kong"
        ],
        "published": "2023-02-09T20:53:12Z",
        "summary": "Large pre-training language models (PLMs) have shown promising in-context\nlearning abilities. However, due to the backbone transformer architecture,\nexisting PLMs are bottlenecked by the memory and computational cost when\nscaling up to a large context size, leaving instruction tuning and in-context\nlearning of many demonstration examples, as well as long-range language\nmodeling under-explored. In this study, we propose a long-range language model\nEVALM based on an efficient transformer mechanism. EVALM is trained with 8k\ntokens per batch line and can test up to 256k-lengthed contexts with\nextrapolation, 128 times to the limit of existing PLMs (e.g. GPT3). Based on\nEVALM, we scale up the size of examples efficiently in both instruction tuning\nand in-context learning to explore the boundary of the benefits from more\nannotated data. Experimental results on a diverse set of tasks show that EVALM\nachieves 4.1% higher accuracy on average, and the average length of achieving\nthe best accuracy score over tasks is around 12k. We find that in-context\nlearning can achieve higher performance with more demonstrations under\nmany-shot instruction tuning (8k), and further extending the length of\ninstructions (16k) can further improve the upper bound of scaling in-context\nlearning.",
        "pdf_link": "https://arxiv.org/pdf/2302.04931v1.pdf"
    },
    {
        "title": "Flexible, Model-Agnostic Method for Materials Data Extraction from Text Using General Purpose Language Models",
        "authors": [
            "Maciej P. Polak",
            "Shrey Modi",
            "Anna Latosinska",
            "Jinming Zhang",
            "Ching-Wen Wang",
            "Shanonan Wang",
            "Ayan Deep Hazra",
            "Dane Morgan"
        ],
        "published": "2023-02-09T19:56:37Z",
        "summary": "Accurate and comprehensive material databases extracted from research papers\nare critical for materials science and engineering but require significant\nhuman effort to develop. In this paper we present a simple method of extracting\nmaterials data from full texts of research papers suitable for quickly\ndeveloping modest-sized databases. The method requires minimal to no coding,\nprior knowledge about the extracted property, or model training, and provides\nhigh recall and almost perfect precision in the resultant database. The method\nis fully automated except for one human-assisted step, which typically requires\njust a few hours of human labor. The method builds on top of natural language\nprocessing and large general language models but can work with almost any such\nmodel. The language models GPT-3/3.5, bart and DeBERTaV3 are evaluated here for\ncomparison. We provide a detailed detailed analysis of the methods performance\nin extracting bulk modulus data, obtaining up to 90% precision at 96% recall,\ndepending on the amount of human effort involved. We then demonstrate the\nmethods broader effectiveness by developing a database of critical cooling\nrates for metallic glasses.",
        "pdf_link": "https://arxiv.org/pdf/2302.04914v2.pdf"
    },
    {
        "title": "Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow",
        "authors": [
            "Anjana Arunkumar",
            "Swaroop Mishra",
            "Bhavdeep Sachdeva",
            "Chitta Baral",
            "Chris Bryan"
        ],
        "published": "2023-02-09T04:43:10Z",
        "summary": "Recent research has shown that language models exploit `artifacts' in\nbenchmarks to solve tasks, rather than truly learning them, leading to inflated\nmodel performance. In pursuit of creating better benchmarks, we propose VAIDA,\na novel benchmark creation paradigm for NLP, that focuses on guiding\ncrowdworkers, an under-explored facet of addressing benchmark idiosyncrasies.\nVAIDA facilitates sample correction by providing realtime visual feedback and\nrecommendations to improve sample quality. Our approach is domain, model, task,\nand metric agnostic, and constitutes a paradigm shift for robust, validated,\nand dynamic benchmark creation via human-and-metric-in-the-loop workflows. We\nevaluate via expert review and a user study with NASA TLX. We find that VAIDA\ndecreases effort, frustration, mental, and temporal demands of crowdworkers and\nanalysts, simultaneously increasing the performance of both user groups with a\n45.8% decrease in the level of artifacts in created samples. As a by product of\nour user study, we observe that created samples are adversarial across models,\nleading to decreases of 31.3% (BERT), 22.5% (RoBERTa), 14.98% (GPT-3 fewshot)\nin performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.04434v1.pdf"
    },
    {
        "title": "Prompting for Multimodal Hateful Meme Classification",
        "authors": [
            "Rui Cao",
            "Roy Ka-Wei Lee",
            "Wen-Haw Chong",
            "Jing Jiang"
        ],
        "published": "2023-02-08T16:04:08Z",
        "summary": "Hateful meme classification is a challenging multimodal task that requires\ncomplex reasoning and contextual background knowledge. Ideally, we could\nleverage an explicit external knowledge base to supplement contextual and\ncultural information in hateful memes. However, there is no known explicit\nexternal knowledge base that could provide such hate speech contextual\ninformation. To address this gap, we propose PromptHate, a simple yet effective\nprompt-based model that prompts pre-trained language models (PLMs) for hateful\nmeme classification. Specifically, we construct simple prompts and provide a\nfew in-context examples to exploit the implicit knowledge in the pre-trained\nRoBERTa language model for hateful meme classification. We conduct extensive\nexperiments on two publicly available hateful and offensive meme datasets. Our\nexperimental results show that PromptHate is able to achieve a high AUC of\n90.96, outperforming state-of-the-art baselines on the hateful meme\nclassification task. We also perform fine-grained analyses and case studies on\nvarious prompt settings and demonstrate the effectiveness of the prompts on\nhateful meme classification.",
        "pdf_link": "https://arxiv.org/pdf/2302.04156v1.pdf"
    },
    {
        "title": "Training-free Lexical Backdoor Attacks on Language Models",
        "authors": [
            "Yujin Huang",
            "Terry Yue Zhuo",
            "Qiongkai Xu",
            "Han Hu",
            "Xingliang Yuan",
            "Chunyang Chen"
        ],
        "published": "2023-02-08T15:18:51Z",
        "summary": "Large-scale language models have achieved tremendous success across various\nnatural language processing (NLP) applications. Nevertheless, language models\nare vulnerable to backdoor attacks, which inject stealthy triggers into models\nfor steering them to undesirable behaviors. Most existing backdoor attacks,\nsuch as data poisoning, require further (re)training or fine-tuning language\nmodels to learn the intended backdoor patterns. The additional training process\nhowever diminishes the stealthiness of the attacks, as training a language\nmodel usually requires long optimization time, a massive amount of data, and\nconsiderable modifications to the model parameters. In this work, we propose\nTraining-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free\nbackdoor attack on language models. Our attack is achieved by injecting lexical\ntriggers into the tokenizer of a language model via manipulating its embedding\ndictionary using carefully designed rules. These rules are explainable to human\ndevelopers which inspires attacks from a wider range of hackers. The sparse\nmanipulation of the dictionary also habilitates the stealthiness of our attack.\nWe conduct extensive experiments on three dominant NLP tasks based on nine\nlanguage models to demonstrate the effectiveness and universality of our\nattack. The code of this work is available at\nhttps://github.com/Jinxhy/TFLexAttack.",
        "pdf_link": "https://arxiv.org/pdf/2302.04116v1.pdf"
    },
    {
        "title": "ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots",
        "authors": [
            "Reham Omar",
            "Omij Mangukiya",
            "Panos Kalnis",
            "Essam Mansour"
        ],
        "published": "2023-02-08T13:03:27Z",
        "summary": "Conversational AI and Question-Answering systems (QASs) for knowledge graphs\n(KGs) are both emerging research areas: they empower users with natural\nlanguage interfaces for extracting information easily and effectively.\nConversational AI simulates conversations with humans; however, it is limited\nby the data captured in the training datasets. In contrast, QASs retrieve the\nmost recent information from a KG by understanding and translating the natural\nlanguage question into a formal query supported by the database engine.\n  In this paper, we present a comprehensive study of the characteristics of the\nexisting alternatives towards combining both worlds into novel KG chatbots. Our\nframework compares two representative conversational models, ChatGPT and\nGalactica, against KGQAN, the current state-of-the-art QAS. We conduct a\nthorough evaluation using four real KGs across various application domains to\nidentify the current limitations of each category of systems. Based on our\nfindings, we propose open research opportunities to empower QASs with chatbot\ncapabilities for KGs. All benchmarks and all raw results are available1 for\nfurther analysis.",
        "pdf_link": "https://arxiv.org/pdf/2302.06466v1.pdf"
    },
    {
        "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
        "authors": [
            "Yejin Bang",
            "Samuel Cahyawijaya",
            "Nayeon Lee",
            "Wenliang Dai",
            "Dan Su",
            "Bryan Wilie",
            "Holy Lovenia",
            "Ziwei Ji",
            "Tiezheng Yu",
            "Willy Chung",
            "Quyet V. Do",
            "Yan Xu",
            "Pascale Fung"
        ],
        "published": "2023-02-08T12:35:34Z",
        "summary": "This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.",
        "pdf_link": "https://arxiv.org/pdf/2302.04023v4.pdf"
    },
    {
        "title": "CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models",
        "authors": [
            "Hossein Hajipour",
            "Keno Hassler",
            "Thorsten Holz",
            "Lea Schönherr",
            "Mario Fritz"
        ],
        "published": "2023-02-08T11:54:07Z",
        "summary": "Large language models (LLMs) for automatic code generation have achieved\nbreakthroughs in several programming tasks. Their advances in competition-level\nprogramming problems have made them an essential pillar of AI-assisted pair\nprogramming, and tools such as GitHub Copilot have emerged as part of the daily\nprogramming workflow used by millions of developers. The training data for\nthese models is usually collected from the Internet (e.g., from open-source\nrepositories) and is likely to contain faults and security vulnerabilities.\nThis unsanitized training data can cause the language models to learn these\nvulnerabilities and propagate them during the code generation procedure. While\nthese models have been extensively assessed for their ability to produce\nfunctionally correct programs, there remains a lack of comprehensive\ninvestigations and benchmarks addressing the security aspects of these models.\n  In this work, we propose a method to systematically study the security issues\nof code language models to assess their susceptibility to generating vulnerable\ncode. To this end, we introduce the first approach to automatically find\ngenerated code that contains vulnerabilities in black-box code generation\nmodels. To achieve this, we present an approach to approximate inversion of the\nblack-box code generation models based on few-shot prompting. We evaluate the\neffectiveness of our approach by examining code language models in generating\nhigh-risk security weaknesses. Furthermore, we establish a collection of\ndiverse non-secure prompts for various vulnerability scenarios using our\nmethod. This dataset forms a benchmark for evaluating and comparing the\nsecurity weaknesses in code language models.",
        "pdf_link": "https://arxiv.org/pdf/2302.04012v2.pdf"
    },
    {
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
        "authors": [
            "Chengwei Qin",
            "Aston Zhang",
            "Zhuosheng Zhang",
            "Jiaao Chen",
            "Michihiro Yasunaga",
            "Diyi Yang"
        ],
        "published": "2023-02-08T09:44:51Z",
        "summary": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.",
        "pdf_link": "https://arxiv.org/pdf/2302.06476v3.pdf"
    },
    {
        "title": "Reliable Natural Language Understanding with Large Language Models and Answer Set Programming",
        "authors": [
            "Abhiramon Rajasekharan",
            "Yankai Zeng",
            "Parth Padalkar",
            "Gopal Gupta"
        ],
        "published": "2023-02-07T22:37:21Z",
        "summary": "Humans understand language by extracting information (meaning) from\nsentences, combining it with existing commonsense knowledge, and then\nperforming reasoning to draw conclusions. While large language models (LLMs)\nsuch as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a\nvariety of NLP tasks, they fall short in problems that require reasoning. They\nalso cannot reliably explain the answers generated for a given question. In\norder to emulate humans better, we propose STAR, a framework that combines LLMs\nwith Answer Set Programming (ASP). We show how LLMs can be used to effectively\nextract knowledge -- represented as predicates -- from language. Goal-directed\nASP is then employed to reliably reason over this knowledge. We apply the STAR\nframework to three different NLU tasks requiring reasoning: qualitative\nreasoning, mathematical reasoning, and goal-directed conversation. Our\nexperiments reveal that STAR is able to bridge the gap of reasoning in NLU\ntasks, leading to significant performance improvements, especially for smaller\nLLMs, i.e., LLMs with a smaller number of parameters. NLU applications\ndeveloped using the STAR framework are also explainable: along with the\npredicates generated, a justification in the form of a proof tree can be\nproduced for a given output.",
        "pdf_link": "https://arxiv.org/pdf/2302.03780v3.pdf"
    },
    {
        "title": "What Matters In The Structured Pruning of Generative Language Models?",
        "authors": [
            "Michael Santacroce",
            "Zixin Wen",
            "Yelong Shen",
            "Yuanzhi Li"
        ],
        "published": "2023-02-07T22:05:55Z",
        "summary": "Auto-regressive large language models such as GPT-3 require enormous\ncomputational resources to use. Traditionally, structured pruning methods are\nemployed to reduce resource usage. However, their application to and efficacy\nfor generative language models is heavily under-explored. In this paper we\nconduct an comprehensive evaluation of common structured pruning methods,\nincluding magnitude, random, and movement pruning on the feed-forward layers in\nGPT-type models. Unexpectedly, random pruning results in performance that is\ncomparable to the best established methods, across multiple natural language\ngeneration tasks. To understand these results, we provide a framework for\nmeasuring neuron-level redundancy of models pruned by different methods, and\ndiscover that established structured pruning methods do not take into account\nthe distinctiveness of neurons, leaving behind excess redundancies. In view of\nthis, we introduce Globally Unique Movement (GUM) to improve the uniqueness of\nneurons in pruned models. We then discuss the effects of our techniques on\ndifferent redundancy metrics to explain the improved performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.03773v1.pdf"
    },
    {
        "title": "Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis",
        "authors": [
            "Akshita Jha",
            "Adithya Samavedhi",
            "Vineeth Rakesh",
            "Jaideep Chandrashekar",
            "Chandan K. Reddy"
        ],
        "published": "2023-02-07T21:51:05Z",
        "summary": "Recent advances in the area of long document matching have primarily focused\non using transformer-based models for long document encoding and matching.\nThere are two primary challenges associated with these models. Firstly, the\nperformance gain provided by transformer-based models comes at a steep cost -\nboth in terms of the required training time and the resource (memory and\nenergy) consumption. The second major limitation is their inability to handle\nmore than a pre-defined input token length at a time. In this work, we\nempirically demonstrate the effectiveness of simple neural models (such as\nfeed-forward networks, and CNNs) and simple embeddings (like GloVe, and\nParagraph Vector) over transformer-based models on the task of document\nmatching. We show that simple models outperform the more complex BERT-based\nmodels while taking significantly less training time, energy, and memory. The\nsimple models are also more robust to variations in document length and text\nperturbations.",
        "pdf_link": "https://arxiv.org/pdf/2302.03765v1.pdf"
    },
    {
        "title": "Long Horizon Temperature Scaling",
        "authors": [
            "Andy Shih",
            "Dorsa Sadigh",
            "Stefano Ermon"
        ],
        "published": "2023-02-07T18:59:32Z",
        "summary": "Temperature scaling is a popular technique for tuning the sharpness of a\nmodel distribution. It is used extensively for sampling likely generations and\ncalibrating model uncertainty, and even features as a controllable parameter to\nmany large language models in deployment. However, autoregressive models rely\non myopic temperature scaling that greedily optimizes the next token. To\naddress this, we propose Long Horizon Temperature Scaling (LHTS), a novel\napproach for sampling from temperature-scaled joint distributions. LHTS is\ncompatible with all likelihood-based models, and optimizes for the long horizon\nlikelihood of samples. We derive a temperature-dependent LHTS objective, and\nshow that finetuning a model on a range of temperatures produces a single model\ncapable of generation with a controllable long horizon temperature parameter.\nWe experiment with LHTS on image diffusion models and character/language\nautoregressive models, demonstrating advantages over myopic temperature scaling\nin likelihood and sample quality, and showing improvements in accuracy on a\nmultiple choice analogy task by $10\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2302.03686v2.pdf"
    },
    {
        "title": "Learning Translation Quality Evaluation on Low Resource Languages from Large Language Models",
        "authors": [
            "Amirkeivan Mohtashami",
            "Mauro Verzetti",
            "Paul K. Rubenstein"
        ],
        "published": "2023-02-07T14:35:35Z",
        "summary": "Learned metrics such as BLEURT have in recent years become widely employed to\nevaluate the quality of machine translation systems. Training such metrics\nrequires data which can be expensive and difficult to acquire, particularly for\nlower-resource languages. We show how knowledge can be distilled from Large\nLanguage Models (LLMs) to improve upon such learned metrics without requiring\nhuman annotators, by creating synthetic datasets which can be mixed into\nexisting datasets, requiring only a corpus of text in the target language. We\nshow that the performance of a BLEURT-like model on lower resource languages\ncan be improved in this way.",
        "pdf_link": "https://arxiv.org/pdf/2302.03491v1.pdf"
    },
    {
        "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
        "authors": [
            "Maximillian Chen",
            "Alexandros Papangelis",
            "Chenyang Tao",
            "Seokhwan Kim",
            "Andy Rosenbaum",
            "Yang Liu",
            "Zhou Yu",
            "Dilek Hakkani-Tur"
        ],
        "published": "2023-02-07T05:48:16Z",
        "summary": "Collecting high quality conversational data can be very expensive for most\napplications and infeasible for others due to privacy, ethical, or similar\nconcerns. A promising direction to tackle this problem is to generate synthetic\ndialogues by prompting large language models. In this work, we use a small set\nof expert-written conversations as in-context examples to synthesize a social\nconversation dataset using prompting. We perform several thorough evaluations\nof our synthetic conversations compared to human-collected conversations. This\nincludes various dimensions of conversation quality with human evaluation\ndirectly on the synthesized conversations, and interactive human evaluation of\nchatbots fine-tuned on the synthetically generated dataset. We additionally\ndemonstrate that this prompting approach is generalizable to multi-party\nconversations, providing potential to create new synthetic data for multi-party\ntasks. Our synthetic multi-party conversations were rated more favorably across\nall measured dimensions compared to conversation excerpts sampled from a\nhuman-collected multi-party dataset.",
        "pdf_link": "https://arxiv.org/pdf/2302.03269v3.pdf"
    },
    {
        "title": "APAM: Adaptive Pre-training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-tailed Learning",
        "authors": [
            "Sunyi Chi",
            "Bo Dong",
            "Yiming Xu",
            "Zhenyu Shi",
            "Zheng Du"
        ],
        "published": "2023-02-06T18:40:04Z",
        "summary": "Practical natural language processing (NLP) tasks are commonly long-tailed\nwith noisy labels. Those problems challenge the generalization and robustness\nof complex models such as Deep Neural Networks (DNNs). Some commonly used\nresampling techniques, such as oversampling or undersampling, could easily lead\nto overfitting. It is growing popular to learn the data weights leveraging a\nsmall amount of metadata. Besides, recent studies have shown the advantages of\nself-supervised pre-training, particularly to the under-represented data. In\nthis work, we propose a general framework to handle the problem of both\nlong-tail and noisy labels. The model is adapted to the domain of problems in a\ncontrastive learning manner. The re-weighting module is a feed-forward network\nthat learns explicit weighting functions and adapts weights according to\nmetadata. The framework further adapts weights of terms in the loss function\nthrough a combination of the polynomial expansion of cross-entropy loss and\nfocal loss. Our extensive experiments show that the proposed framework\nconsistently outperforms baseline methods. Lastly, our sensitive analysis\nemphasizes the capability of the proposed framework to handle the long-tailed\nproblem and mitigate the negative impact of noisy labels.",
        "pdf_link": "https://arxiv.org/pdf/2302.03488v2.pdf"
    },
    {
        "title": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning",
        "authors": [
            "Thomas Carta",
            "Clément Romac",
            "Thomas Wolf",
            "Sylvain Lamprier",
            "Olivier Sigaud",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2023-02-06T10:01:08Z",
        "summary": "Recent works successfully leveraged Large Language Models' (LLM) abilities to\ncapture abstract knowledge about world's physics to solve decision-making\nproblems. Yet, the alignment between LLMs' knowledge and the environment can be\nwrong and limit functional competence due to lack of grounding. In this paper,\nwe study an approach (named GLAM) to achieve this alignment through functional\ngrounding: we consider an agent using an LLM as a policy that is progressively\nupdated as the agent interacts with the environment, leveraging online\nReinforcement Learning to improve its performance to solve goals. Using an\ninteractive textual environment designed to study higher-level forms of\nfunctional grounding, and a set of spatial and navigation tasks, we study\nseveral scientific questions: 1) Can LLMs boost sample efficiency for online\nlearning of various RL tasks? 2) How can it boost different forms of\ngeneralization? 3) What is the impact of online learning? We study these\nquestions by functionally grounding several variants (size, architecture) of\nFLAN-T5.",
        "pdf_link": "https://arxiv.org/pdf/2302.02662v3.pdf"
    },
    {
        "title": "A Categorical Archive of ChatGPT Failures",
        "authors": [
            "Ali Borji"
        ],
        "published": "2023-02-06T04:21:59Z",
        "summary": "Large language models have been demonstrated to be valuable in different\nfields. ChatGPT, developed by OpenAI, has been trained using massive amounts of\ndata and simulates human conversation by comprehending context and generating\nappropriate responses. It has garnered significant attention due to its ability\nto effectively answer a broad range of human inquiries, with fluent and\ncomprehensive answers surpassing prior public chatbots in both security and\nusefulness. However, a comprehensive analysis of ChatGPT's failures is lacking,\nwhich is the focus of this study. Eleven categories of failures, including\nreasoning, factual errors, math, coding, and bias, are presented and discussed.\nThe risks, limitations, and societal implications of ChatGPT are also\nhighlighted. The goal of this study is to assist researchers and developers in\nenhancing future language models and chatbots.",
        "pdf_link": "https://arxiv.org/pdf/2302.03494v8.pdf"
    },
    {
        "title": "Nationality Bias in Text Generation",
        "authors": [
            "Pranav Narayanan Venkit",
            "Sanjana Gautam",
            "Ruchi Panchanadikar",
            "Ting-Hao 'Kenneth' Huang",
            "Shomir Wilson"
        ],
        "published": "2023-02-05T19:15:33Z",
        "summary": "Little attention is placed on analyzing nationality bias in language models,\nespecially when nationality is highly used as a factor in increasing the\nperformance of social NLP models. This paper examines how a text generation\nmodel, GPT-2, accentuates pre-existing societal biases about country-based\ndemonyms. We generate stories using GPT-2 for various nationalities and use\nsensitivity analysis to explore how the number of internet users and the\ncountry's economic status impacts the sentiment of the stories. To reduce the\npropagation of biases through large language models (LLM), we explore the\ndebiasing method of adversarial triggering. Our results show that GPT-2\ndemonstrates significant bias against countries with lower internet users, and\nadversarial triggering effectively reduces the same.",
        "pdf_link": "https://arxiv.org/pdf/2302.02463v3.pdf"
    },
    {
        "title": "Quantized Distributed Training of Large Models with Convergence Guarantees",
        "authors": [
            "Ilia Markov",
            "Adrian Vladu",
            "Qi Guo",
            "Dan Alistarh"
        ],
        "published": "2023-02-05T14:20:55Z",
        "summary": "Communication-reduction techniques are a popular way to improve scalability\nin data-parallel training of deep neural networks (DNNs). The recent emergence\nof large language models such as GPT has created the need for new approaches to\nexploit data-parallelism. Among these, fully-sharded data parallel (FSDP)\ntraining is highly popular, yet it still encounters scalability bottlenecks.\nOne reason is that applying compression techniques to FSDP is challenging: as\nthe vast majority of the communication involves the model's weights, direct\ncompression alters convergence and leads to accuracy loss. We present QSDP, a\nvariant of FSDP which supports both gradient and weight quantization with\ntheoretical guarantees, is simple to implement and has essentially no\noverheads. To derive QSDP we prove that a natural modification of SGD achieves\nconvergence even when we only maintain quantized weights, and thus the domain\nover which we train consists of quantized points and is, therefore, highly\nnon-convex. We validate this approach by training GPT-family models with up to\n1.3 billion parameters on a multi-node cluster. Experiments show that QSDP\npreserves model accuracy, while completely removing the communication\nbottlenecks of FSDP, providing end-to-end speedups of up to 2.2x.",
        "pdf_link": "https://arxiv.org/pdf/2302.02390v1.pdf"
    },
    {
        "title": "The Science of Detecting LLM-Generated Texts",
        "authors": [
            "Ruixiang Tang",
            "Yu-Neng Chuang",
            "Xia Hu"
        ],
        "published": "2023-02-04T04:49:17Z",
        "summary": "The emergence of large language models (LLMs) has resulted in the production\nof LLM-generated texts that is highly sophisticated and almost\nindistinguishable from texts written by humans. However, this has also sparked\nconcerns about the potential misuse of such texts, such as spreading\nmisinformation and causing disruptions in the education system. Although many\ndetection approaches have been proposed, a comprehensive understanding of the\nachievements and challenges is still lacking. This survey aims to provide an\noverview of existing LLM-generated text detection techniques and enhance the\ncontrol and regulation of language generation models. Furthermore, we emphasize\ncrucial considerations for future research, including the development of\ncomprehensive evaluation metrics and the threat posed by open-source LLMs, to\ndrive progress in the area of LLM-generated text detection.",
        "pdf_link": "https://arxiv.org/pdf/2303.07205v3.pdf"
    },
    {
        "title": "Evaluating Large Language Models in Theory of Mind Tasks",
        "authors": [
            "Michal Kosinski"
        ],
        "published": "2023-02-04T03:50:01Z",
        "summary": "Eleven Large Language Models (LLMs) were assessed using a custom-made battery\nof false-belief tasks, considered a gold standard in testing Theory of Mind\n(ToM) in humans. The battery included 640 prompts spread across 40 diverse\ntasks, each one including a false-belief scenario, three closely matched\ntrue-belief control scenarios, and the reversed versions of all four. To solve\na single task, a model needed to correctly answer 16 prompts across all eight\nscenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from\nNovember 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks;\nChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of\nsix-year-old children observed in past studies. We explore the potential\ninterpretation of these findings, including the intriguing possibility that\nToM, previously considered exclusive to humans, may have spontaneously emerged\nas a byproduct of LLMs' improving language skills.",
        "pdf_link": "https://arxiv.org/pdf/2302.02083v6.pdf"
    },
    {
        "title": "Towards Few-Shot Identification of Morality Frames using In-Context Learning",
        "authors": [
            "Shamik Roy",
            "Nishanth Sridhar Nakshatri",
            "Dan Goldwasser"
        ],
        "published": "2023-02-03T23:26:59Z",
        "summary": "Data scarcity is a common problem in NLP, especially when the annotation\npertains to nuanced socio-linguistic concepts that require specialized\nknowledge. As a result, few-shot identification of these concepts is desirable.\nFew-shot in-context learning using pre-trained Large Language Models (LLMs) has\nbeen recently applied successfully in many NLP tasks. In this paper, we study\nfew-shot identification of a psycho-linguistic concept, Morality Frames (Roy et\nal., 2021), using LLMs. Morality frames are a representation framework that\nprovides a holistic view of the moral sentiment expressed in text, identifying\nthe relevant moral foundation (Haidt and Graham, 2007) and at a finer level of\ngranularity, the moral sentiment expressed towards the entities mentioned in\nthe text. Previous studies relied on human annotation to identify morality\nframes in text which is expensive. In this paper, we propose prompting-based\napproaches using pretrained Large Language Models for identification of\nmorality frames, relying only on few-shot exemplars. We compare our models'\nperformance with few-shot RoBERTa and found promising results.",
        "pdf_link": "https://arxiv.org/pdf/2302.02029v1.pdf"
    },
    {
        "title": "Witscript 2: A System for Generating Improvised Jokes Without Wordplay",
        "authors": [
            "Joe Toplyn"
        ],
        "published": "2023-02-03T21:51:55Z",
        "summary": "A previous paper presented Witscript, a system for generating conversational\njokes that rely on wordplay. This paper extends that work by presenting\nWitscript 2, which uses a large language model to generate conversational jokes\nthat rely on common sense instead of wordplay. Like Witscript, Witscript 2 is\nbased on joke-writing algorithms created by an expert comedy writer. Human\nevaluators judged Witscript 2's responses to input sentences to be jokes 46% of\nthe time, compared to 70% of the time for human-written responses. This is\nevidence that Witscript 2 represents another step toward giving a chatbot a\nhumanlike sense of humor.",
        "pdf_link": "https://arxiv.org/pdf/2302.03036v1.pdf"
    },
    {
        "title": "Bioformer: an efficient transformer language model for biomedical text mining",
        "authors": [
            "Li Fang",
            "Qingyu Chen",
            "Chih-Hsuan Wei",
            "Zhiyong Lu",
            "Kai Wang"
        ],
        "published": "2023-02-03T08:04:59Z",
        "summary": "Pretrained language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have achieved state-of-the-art performance in natural\nlanguage processing (NLP) tasks. Recently, BERT has been adapted to the\nbiomedical domain. Despite the effectiveness, these models have hundreds of\nmillions of parameters and are computationally expensive when applied to\nlarge-scale NLP applications. We hypothesized that the number of parameters of\nthe original BERT can be dramatically reduced with minor impact on performance.\nIn this study, we present Bioformer, a compact BERT model for biomedical text\nmining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L)\nwhich reduced the model size by 60% compared to BERTBase. Bioformer uses a\nbiomedical vocabulary and was pre-trained from scratch on PubMed abstracts and\nPubMed Central full-text articles. We thoroughly evaluated the performance of\nBioformer as well as existing biomedical BERT models including BioBERT and\nPubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks:\nnamed entity recognition, relation extraction, question answering and document\nclassification. The results show that with 60% fewer parameters, Bioformer16L\nis only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less\naccurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed\nBioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as\nfast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed\nto PubTator Central providing gene annotations over 35 million PubMed abstracts\nand 5 million PubMed Central full-text articles. We make Bioformer publicly\navailable via https://github.com/WGLab/bioformer, including pre-trained models,\ndatasets, and instructions for downstream use.",
        "pdf_link": "https://arxiv.org/pdf/2302.01588v1.pdf"
    },
    {
        "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
        "authors": [
            "Zihao Wang",
            "Shaofei Cai",
            "Guanzhou Chen",
            "Anji Liu",
            "Xiaojian Ma",
            "Yitao Liang"
        ],
        "published": "2023-02-03T06:06:27Z",
        "summary": "We investigate the challenge of task planning for multi-task embodied agents\nin open-world environments. Two main difficulties are identified: 1) executing\nplans in an open-world environment (e.g., Minecraft) necessitates accurate and\nmulti-step reasoning due to the long-term nature of tasks, and 2) as vanilla\nplanners do not consider how easy the current agent can achieve a given\nsub-task when ordering parallel sub-goals within a complicated plan, the\nresulting plan could be inefficient or even infeasible. To this end, we propose\n\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and\n$\\underline{S}$elect\" ($\\textbf{DEPS}$), an interactive planning approach based\non Large Language Models (LLMs). DEPS facilitates better error correction on\ninitial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of\nthe plan execution process and providing self-$\\textit{explanation}$ of\nfeedback when encountering failures during the extended planning phases.\nFurthermore, it includes a goal $\\textit{selector}$, which is a trainable\nmodule that ranks parallel candidate sub-goals based on the estimated steps of\ncompletion, consequently refining the initial plan. Our experiments mark the\nmilestone of the first zero-shot multi-task agent that can robustly accomplish\n70+ Minecraft tasks and nearly double the overall performances. Further testing\nreveals our method's general effectiveness in popularly adopted non-open-ended\ndomains as well (i.e., ALFWorld and tabletop manipulation). The ablation and\nexploratory studies detail how our design beats the counterparts and provide a\npromising update on the $\\texttt{ObtainDiamond}$ grand challenge with our\napproach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
        "pdf_link": "https://arxiv.org/pdf/2302.01560v2.pdf"
    },
    {
        "title": "STEP: Learning N:M Structured Sparsity Masks from Scratch with Precondition",
        "authors": [
            "Yucheng Lu",
            "Shivani Agrawal",
            "Suvinay Subramanian",
            "Oleg Rybakov",
            "Christopher De Sa",
            "Amir Yazdanbakhsh"
        ],
        "published": "2023-02-02T15:49:03Z",
        "summary": "Recent innovations on hardware (e.g. Nvidia A100) have motivated learning N:M\nstructured sparsity masks from scratch for fast model inference. However,\nstate-of-the-art learning recipes in this regime (e.g. SR-STE) are proposed for\nnon-adaptive optimizers like momentum SGD, while incurring non-trivial accuracy\ndrop for Adam-trained models like attention-based LLMs. In this paper, we first\ndemonstrate such gap origins from poorly estimated second moment (i.e.\nvariance) in Adam states given by the masked weights. We conjecture that\nlearning N:M masks with Adam should take the critical regime of variance\nestimation into account. In light of this, we propose STEP, an Adam-aware\nrecipe that learns N:M masks with two phases: first, STEP calculates a reliable\nvariance estimate (precondition phase) and subsequently, the variance remains\nfixed and is used as a precondition to learn N:M masks (mask-learning phase).\nSTEP automatically identifies the switching point of two phases by dynamically\nsampling variance changes over the training trajectory and testing the sample\nconcentration. Empirically, we evaluate STEP and other baselines such as ASP\nand SR-STE on multiple tasks including CIFAR classification, machine\ntranslation and LLM fine-tuning (BERT-Base, GPT-2). We show STEP mitigates the\naccuracy drop of baseline recipes and is robust to aggressive structured\nsparsity ratios.",
        "pdf_link": "https://arxiv.org/pdf/2302.01172v1.pdf"
    },
    {
        "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
        "authors": [
            "Zhuosheng Zhang",
            "Aston Zhang",
            "Mu Li",
            "Hai Zhao",
            "George Karypis",
            "Alex Smola"
        ],
        "published": "2023-02-02T07:51:19Z",
        "summary": "Large language models (LLMs) have shown impressive performance on complex\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\nintermediate reasoning chains as the rationale to infer the answer. However,\nexisting CoT studies have focused on the language modality. We propose\nMultimodal-CoT that incorporates language (text) and vision (images) modalities\ninto a two-stage framework that separates rationale generation and answer\ninference. In this way, answer inference can leverage better generated\nrationales that are based on multimodal information. With Multimodal-CoT, our\nmodel under 1 billion parameters outperforms the previous state-of-the-art LLM\n(GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA\nbenchmark and even surpasses human performance. Code is publicly available\navailable at https://github.com/amazon-science/mm-cot.",
        "pdf_link": "https://arxiv.org/pdf/2302.00923v4.pdf"
    },
    {
        "title": "Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment",
        "authors": [
            "Hao Liu",
            "Wilson Yan",
            "Pieter Abbeel"
        ],
        "published": "2023-02-02T06:38:44Z",
        "summary": "Recent progress in scaling up large language models has shown impressive\ncapabilities in performing few-shot learning across a wide range of text-based\ntasks. However, a key limitation is that these language models fundamentally\nlack visual perception - a crucial attribute needed to extend these models to\nbe able to interact with the real world and solve vision tasks, such as in\nvisual-question answering and robotics. Prior works have largely connected\nimage to text through pretraining and/or fine-tuning on curated image-text\ndatasets, which can be a costly and expensive process. In order to resolve this\nlimitation, we propose a simple yet effective approach called\nLanguage-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to\nalign text-image data in an unsupervised manner by leveraging pretrained\nlanguage models (e.g., BERT, RoBERTa). Our main idea is to encode image as\nsequences of text tokens by directly quantizing image embeddings using a\npretrained language codebook. We then apply random masking followed by a BERT\nmodel, and have the decoder reconstruct the original image from BERT predicted\ntext token embeddings. By doing so, LQAE learns to represent similar images\nwith similar clusters of text tokens, thereby aligning these two modalities\nwithout the use of aligned text-image pairs. This enables few-shot image\nclassification with large language models (e.g., GPT-3) as well as linear\nclassification of images based on BERT text features. To the best of our\nknowledge, our work is the first work that uses unaligned images for multimodal\ntasks by leveraging the power of pretrained language models.",
        "pdf_link": "https://arxiv.org/pdf/2302.00902v2.pdf"
    },
    {
        "title": "Conditioning Predictive Models: Risks and Strategies",
        "authors": [
            "Evan Hubinger",
            "Adam Jermyn",
            "Johannes Treutlein",
            "Rubi Hudson",
            "Kate Woolverton"
        ],
        "published": "2023-02-02T00:06:36Z",
        "summary": "Our intention is to provide a definitive reference on what it would take to\nsafely make use of generative/predictive models in the absence of a solution to\nthe Eliciting Latent Knowledge problem. Furthermore, we believe that large\nlanguage models can be understood as such predictive models of the world, and\nthat such a conceptualization raises significant opportunities for their safe\nyet powerful use via carefully conditioning them to predict desirable outputs.\nUnfortunately, such approaches also raise a variety of potentially fatal safety\nproblems, particularly surrounding situations where predictive models predict\nthe output of other AI systems, potentially unbeknownst to us. There are\nnumerous potential solutions to such problems, however, primarily via carefully\nconditioning models to predict the things we want (e.g. humans) rather than the\nthings we don't (e.g. malign AIs). Furthermore, due to the simplicity of the\nprediction objective, we believe that predictive models present the easiest\ninner alignment problem that we are aware of. As a result, we think that\nconditioning approaches for predictive models represent the safest known way of\neliciting human-level and slightly superhuman capabilities from large language\nmodels and other similar future models.",
        "pdf_link": "https://arxiv.org/pdf/2302.00805v2.pdf"
    },
    {
        "title": "Collaborating with language models for embodied reasoning",
        "authors": [
            "Ishita Dasgupta",
            "Christine Kaeser-Chen",
            "Kenneth Marino",
            "Arun Ahuja",
            "Sheila Babayan",
            "Felix Hill",
            "Rob Fergus"
        ],
        "published": "2023-02-01T21:26:32Z",
        "summary": "Reasoning in a complex and ambiguous environment is a key goal for\nReinforcement Learning (RL) agents. While some sophisticated RL agents can\nsuccessfully solve difficult tasks, they require a large amount of training\ndata and often struggle to generalize to new unseen environments and new tasks.\nOn the other hand, Large Scale Language Models (LSLMs) have exhibited strong\nreasoning ability and the ability to to adapt to new tasks through in-context\nlearning. However, LSLMs do not inherently have the ability to interrogate or\nintervene on the environment. In this work, we investigate how to combine these\ncomplementary abilities in a single system consisting of three parts: a\nPlanner, an Actor, and a Reporter. The Planner is a pre-trained language model\nthat can issue commands to a simple embodied agent (the Actor), while the\nReporter communicates with the Planner to inform its next command. We present a\nset of tasks that require reasoning, test this system's ability to generalize\nzero-shot and investigate failure cases, and demonstrate how components of this\nsystem can be trained with reinforcement-learning to improve performance.",
        "pdf_link": "https://arxiv.org/pdf/2302.00763v1.pdf"
    },
    {
        "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
        "authors": [
            "Zhihong Shao",
            "Yeyun Gong",
            "Yelong Shen",
            "Minlie Huang",
            "Nan Duan",
            "Weizhu Chen"
        ],
        "published": "2023-02-01T17:33:12Z",
        "summary": "Large language models can perform various reasoning tasks by using\nchain-of-thought prompting, which guides them to find answers through\nstep-by-step demonstrations. However, the quality of the prompts depends on the\ndemonstrations given to the models, and creating many of them by hand is\ncostly. We introduce Synthetic prompting, a method that leverages a few\nhandcrafted examples to prompt the model to generate more examples by itself,\nand selects effective demonstrations to elicit better reasoning. Our method\nalternates between a backward and forward process to generate new examples. The\nbackward process generates a question that match a sampled reasoning chain, so\nthat the question is solvable and clear. The forward process produces a more\ndetailed reasoning chain for the question, improving the quality of the\nexample. We evaluate our method on numerical, symbolic, and algorithmic\nreasoning tasks, and show that it outperforms existing prompting techniques.",
        "pdf_link": "https://arxiv.org/pdf/2302.00618v1.pdf"
    },
    {
        "title": "Co-Writing with Opinionated Language Models Affects Users' Views",
        "authors": [
            "Maurice Jakesch",
            "Advait Bhat",
            "Daniel Buschek",
            "Lior Zalmanson",
            "Mor Naaman"
        ],
        "published": "2023-02-01T16:26:32Z",
        "summary": "If large language models like GPT-3 preferably produce a particular point of\nview, they may influence people's opinions on an unknown scale. This study\ninvestigates whether a language-model-powered writing assistant that generates\nsome opinions more often than others impacts what users write - and what they\nthink. In an online experiment, we asked participants (N=1,506) to write a post\ndiscussing whether social media is good for society. Treatment group\nparticipants used a language-model-powered writing assistant configured to\nargue that social media is good or bad for society. Participants then completed\na social media attitude survey, and independent judges (N=500) evaluated the\nopinions expressed in their writing. Using the opinionated language model\naffected the opinions expressed in participants' writing and shifted their\nopinions in the subsequent attitude survey. We discuss the wider implications\nof our results and argue that the opinions built into AI language technologies\nneed to be monitored and engineered more carefully.",
        "pdf_link": "https://arxiv.org/pdf/2302.00560v1.pdf"
    },
    {
        "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
        "authors": [
            "Nils Lukas",
            "Ahmed Salem",
            "Robert Sim",
            "Shruti Tople",
            "Lukas Wutschitz",
            "Santiago Zanella-Béguelin"
        ],
        "published": "2023-02-01T16:04:48Z",
        "summary": "Language Models (LMs) have been shown to leak information about training data\nthrough sentence-level membership inference and reconstruction attacks.\nUnderstanding the risk of LMs leaking Personally Identifiable Information (PII)\nhas received less attention, which can be attributed to the false assumption\nthat dataset curation techniques such as scrubbing are sufficient to prevent\nPII leakage. Scrubbing techniques reduce but do not prevent the risk of PII\nleakage: in practice scrubbing is imperfect and must balance the trade-off\nbetween minimizing disclosure and preserving the utility of the dataset. On the\nother hand, it is unclear to which extent algorithmic defenses such as\ndifferential privacy, designed to guarantee sentence- or user-level privacy,\nprevent PII disclosure. In this work, we introduce rigorous game-based\ndefinitions for three types of PII leakage via black-box extraction, inference,\nand reconstruction attacks with only API access to an LM. We empirically\nevaluate the attacks against GPT-2 models fine-tuned with and without defenses\nin three domains: case law, health care, and e-mails. Our main contributions\nare (i) novel attacks that can extract up to 10$\\times$ more PII sequences than\nexisting attacks, (ii) showing that sentence-level differential privacy reduces\nthe risk of PII disclosure but still leaks about 3% of PII sequences, and (iii)\na subtle connection between record-level membership inference and PII\nreconstruction. Code to reproduce all experiments in the paper is available at\nhttps://github.com/microsoft/analysing_pii_leakage.",
        "pdf_link": "https://arxiv.org/pdf/2302.00539v4.pdf"
    },
    {
        "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
        "authors": [
            "Freda Shi",
            "Xinyun Chen",
            "Kanishka Misra",
            "Nathan Scales",
            "David Dohan",
            "Ed Chi",
            "Nathanael Schärli",
            "Denny Zhou"
        ],
        "published": "2023-01-31T20:48:57Z",
        "summary": "Large language models have achieved impressive performance on various natural\nlanguage processing tasks. However, so far they have been evaluated primarily\non benchmarks where all information in the input context is relevant for\nsolving the task. In this work, we investigate the distractibility of large\nlanguage models, i.e., how the model problem-solving accuracy can be influenced\nby irrelevant context. In particular, we introduce Grade-School Math with\nIrrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant\ninformation in the problem description. We use this benchmark to measure the\ndistractibility of cutting-edge prompting techniques for large language models,\nand find that the model performance is dramatically decreased when irrelevant\ninformation is included. We also identify several approaches for mitigating\nthis deficiency, such as decoding with self-consistency and adding to the\nprompt an instruction that tells the language model to ignore the irrelevant\ninformation.",
        "pdf_link": "https://arxiv.org/pdf/2302.00093v3.pdf"
    },
    {
        "title": "Benchmarking Large Language Models for News Summarization",
        "authors": [
            "Tianyi Zhang",
            "Faisal Ladhak",
            "Esin Durmus",
            "Percy Liang",
            "Kathleen McKeown",
            "Tatsunori B. Hashimoto"
        ],
        "published": "2023-01-31T18:46:19Z",
        "summary": "Large language models (LLMs) have shown promise for automatic summarization\nbut the reasons behind their successes are poorly understood. By conducting a\nhuman evaluation on ten LLMs across different pretraining methods, prompts, and\nmodel scales, we make two important observations. First, we find instruction\ntuning, and not model size, is the key to the LLM's zero-shot summarization\ncapability. Second, existing studies have been limited by low-quality\nreferences, leading to underestimates of human performance and lower few-shot\nand finetuning performance. To better evaluate LLMs, we perform human\nevaluation over high-quality summaries we collect from freelance writers.\nDespite major stylistic differences such as the amount of paraphrasing, we find\nthat LMM summaries are judged to be on par with human written summaries.",
        "pdf_link": "https://arxiv.org/pdf/2301.13848v1.pdf"
    },
    {
        "title": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning",
        "authors": [
            "Yunhu Ye",
            "Binyuan Hui",
            "Min Yang",
            "Binhua Li",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2023-01-31T17:51:45Z",
        "summary": "Table-based reasoning has shown remarkable progress in combining deep models\nwith discrete reasoning, which requires reasoning over both free-form natural\nlanguage (NL) questions and structured tabular data. However, previous\ntable-based reasoning solutions usually suffer from significant performance\ndegradation on huge evidence (tables). In addition, most existing methods\nstruggle to reason over complex questions since the required information is\nscattered in different places. To alleviate the above challenges, we exploit\nlarge language models (LLMs) as decomposers for effective table-based\nreasoning, which (i) decompose huge evidence (a huge table) into sub-evidence\n(a small table) to mitigate the interference of useless information for table\nreasoning; and (ii) decompose complex questions into simpler sub-questions for\ntext reasoning. Specifically, we first use the LLMs to break down the evidence\n(tables) involved in the current question, retaining the relevant evidence and\nexcluding the remaining irrelevant evidence from the huge table. In addition,\nwe propose a \"parsing-execution-filling\" strategy to alleviate the\nhallucination dilemma of the chain of thought by decoupling logic and numerical\ncomputation in each step. Extensive experiments show that our method can\neffectively leverage decomposed evidence and questions and outperforms the\nstrong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably,\nour model outperforms human performance for the first time on the TabFact\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2301.13808v3.pdf"
    },
    {
        "title": "FLAME: A small language model for spreadsheet formulas",
        "authors": [
            "Harshit Joshi",
            "Abishai Ebenezer",
            "José Cambronero",
            "Sumit Gulwani",
            "Aditya Kanade",
            "Vu Le",
            "Ivan Radiček",
            "Gust Verbruggen"
        ],
        "published": "2023-01-31T17:29:43Z",
        "summary": "Spreadsheets are a vital tool for end-user data management. Using large\nlanguage models for formula authoring assistance in these environments can be\ndifficult, as these models are expensive to train and challenging to deploy due\nto their size (up to billions of parameters). We present FLAME, a\ntransformer-based model trained exclusively on Excel formulas that leverages\ndomain insights to achieve competitive performance while being substantially\nsmaller (60M parameters) and training on two orders of magnitude less data. We\ncurate a training dataset using sketch deduplication, introduce an\nExcel-specific formula tokenizer, and use domain-specific versions of masked\nspan prediction and noisy auto-encoding as pre-training objectives. We evaluate\nFLAME on formula repair, formula completion, and similarity-based formula\nretrieval. FLAME can outperform much larger models, such as the Davinci (175B)\nand Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation\nsettings for the repair and completion tasks. For formula retrieval, FLAME\noutperforms CodeT5, CodeBERT, and GraphCodeBERT.",
        "pdf_link": "https://arxiv.org/pdf/2301.13779v2.pdf"
    },
    {
        "title": "Skill Decision Transformer",
        "authors": [
            "Shyam Sudhakaran",
            "Sebastian Risi"
        ],
        "published": "2023-01-31T11:52:46Z",
        "summary": "Recent work has shown that Large Language Models (LLMs) can be incredibly\neffective for offline reinforcement learning (RL) by representing the\ntraditional RL problem as a sequence modelling problem (Chen et al., 2021;\nJanner et al., 2021). However many of these methods only optimize for high\nreturns, and may not extract much information from a diverse dataset of\ntrajectories. Generalized Decision Transformers (GDTs) (Furuta et al., 2021)\nhave shown that utilizing future trajectory information, in the form of\ninformation statistics, can help extract more information from offline\ntrajectory data. Building upon this, we propose Skill Decision Transformer\n(Skill DT). Skill DT draws inspiration from hindsight relabelling (Andrychowicz\net al., 2017) and skill discovery methods to discover a diverse set of\nprimitive behaviors, or skills. We show that Skill DT can not only perform\noffline state-marginal matching (SMM), but can discovery descriptive behaviors\nthat can be easily sampled. Furthermore, we show that through purely\nreward-free optimization, Skill DT is still competitive with supervised offline\nRL approaches on the D4RL benchmark. The code and videos can be found on our\nproject page: https://github.com/shyamsn97/skill-dt",
        "pdf_link": "https://arxiv.org/pdf/2301.13573v1.pdf"
    },
    {
        "title": "Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models",
        "authors": [
            "David Noever",
            "Forrest McKee"
        ],
        "published": "2023-01-31T03:14:57Z",
        "summary": "Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique\ntestbeds for exploring the translation challenges of turning literacy into\nnumeracy. Previous publicly-available transformer models from eighteen months\nprior and 1000 times smaller failed to provide basic arithmetic. The\nstatistical analysis of four complex datasets described here combines\narithmetic manipulations that cannot be memorized or encoded by simple rules.\nThe work examines whether next-token prediction succeeds from sentence\ncompletion into the realm of actual numerical understanding. For example, the\nwork highlights cases for descriptive statistics on in-memory datasets that the\nLLM initially loads from memory or generates randomly using python libraries.\nThe resulting exploratory data analysis showcases the model's capabilities to\ngroup by or pivot categorical sums, infer feature importance, derive\ncorrelations, and predict unseen test cases using linear regression. To extend\nthe model's testable range, the research deletes and appends random rows such\nthat recall alone cannot explain emergent numeracy.",
        "pdf_link": "https://arxiv.org/pdf/2301.13382v1.pdf"
    },
    {
        "title": "Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning",
        "authors": [
            "Jian Zhu",
            "Hanli Wang",
            "Miaojing Shi"
        ],
        "published": "2023-01-30T23:43:28Z",
        "summary": "The visual commonsense reasoning (VCR) task is to choose an answer and\nprovide a justifying rationale based on the given image and textural question.\nRepresentative works first recognize objects in images and then associate them\nwith key words in texts. However, existing approaches do not consider exact\npositions of objects in a human-like three-dimensional (3D) manner, making them\nincompetent to accurately distinguish objects and understand visual relation.\nRecently, multi-modal large language models (MLLMs) have been used as powerful\ntools for several multi-modal tasks but not for VCR yet, which requires\nelaborate reasoning on specific visual objects referred by texts. In light of\nthe above, an MLLM enhanced pseudo 3D perception framework is designed for VCR.\nSpecifically, we first demonstrate that the relation between objects is\nrelevant to object depths in images, and hence introduce object depth into VCR\nframeworks to infer 3D positions of objects in images. Then, a depth-aware\nTransformer is proposed to encode depth differences between objects into the\nattention mechanism of Transformer to discriminatively associate objects with\nvisual scenes guided by depth. To further associate the answer with the depth\nof visual scene, each word in the answer is tagged with a pseudo depth to\nrealize depth-aware association between answer words and objects. On the other\nhand, BLIP-2 as an MLLM is employed to process images and texts, and the\nreferring expressions in texts involving specific visual objects are modified\nwith linguistic object labels to serve as comprehensible MLLM inputs. Finally,\na parameter optimization technique is devised to fully consider the quality of\ndata batches based on multi-level reasoning confidence. Experiments on the VCR\ndataset demonstrate the superiority of the proposed framework over\nstate-of-the-art approaches.",
        "pdf_link": "https://arxiv.org/pdf/2301.13335v2.pdf"
    },
    {
        "title": "Adaptive Machine Translation with Large Language Models",
        "authors": [
            "Yasmin Moslem",
            "Rejwanul Haque",
            "John D. Kelleher",
            "Andy Way"
        ],
        "published": "2023-01-30T21:17:15Z",
        "summary": "Consistency is a key requirement of high-quality translation. It is\nespecially important to adhere to pre-approved terminology and adapt to\ncorrected translations in domain-specific projects. Machine translation (MT)\nhas achieved significant progress in the area of domain adaptation. However,\nreal-time adaptation remains challenging. Large-scale language models (LLMs)\nhave recently shown interesting capabilities of in-context learning, where they\nlearn to replicate certain input-output text generation patterns, without\nfurther fine-tuning. By feeding an LLM at inference time with a prompt that\nconsists of a list of translation pairs, it can then simulate the domain and\nstyle characteristics. This work aims to investigate how we can utilize\nin-context learning to improve real-time adaptive MT. Our extensive experiments\nshow promising results at translation time. For example, LLMs can adapt to a\nset of in-domain sentence pairs and/or terminology while translating a new\nsentence. We observe that the translation quality with few-shot in-context\nlearning can surpass that of strong encoder-decoder MT systems, especially for\nhigh-resource languages. Moreover, we investigate whether we can combine MT\nfrom strong encoder-decoder models with fuzzy matches, which can further\nimprove translation quality, especially for less supported languages. We\nconduct our experiments across five diverse language pairs, namely\nEnglish-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French\n(EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).",
        "pdf_link": "https://arxiv.org/pdf/2301.13294v3.pdf"
    },
    {
        "title": "Conversational Automated Program Repair",
        "authors": [
            "Chunqiu Steven Xia",
            "Lingming Zhang"
        ],
        "published": "2023-01-30T19:22:36Z",
        "summary": "Automated Program Repair (APR) can help developers automatically generate\npatches for bugs. Due to the impressive performance obtained using Large\nPre-Trained Language Models (LLMs) on many code related tasks, researchers have\nstarted to directly use LLMs for APR. However, prior approaches simply\nrepeatedly sample the LLM given the same constructed input/prompt created from\nthe original buggy code, which not only leads to generating the same incorrect\npatches repeatedly but also miss the critical information in testcases. To\naddress these limitations, we propose conversational APR, a new paradigm for\nprogram repair that alternates between patch generation and validation in a\nconversational manner. In conversational APR, we iteratively build the input to\nthe model by combining previously generated patches with validation feedback.\nAs such, we leverage the long-term context window of LLMs to not only avoid\ngenerating previously incorrect patches but also incorporate validation\nfeedback to help the model understand the semantic meaning of the program under\ntest. We evaluate 10 different LLM including the newly developed ChatGPT model\nto demonstrate the improvement of conversational APR over the prior LLM for APR\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2301.13246v1.pdf"
    },
    {
        "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
        "authors": [
            "Minglun Han",
            "Feilong Chen",
            "Jing Shi",
            "Shuang Xu",
            "Bo Xu"
        ],
        "published": "2023-01-30T15:44:55Z",
        "summary": "Large-scale pre-trained language models (PLMs) have shown great potential in\nnatural language processing tasks. Leveraging the capabilities of PLMs to\nenhance automatic speech recognition (ASR) systems has also emerged as a\npromising research direction. However, previous works may be limited by the\ninflexible structures of PLMs and the insufficient utilization of PLMs. To\nalleviate these problems, we propose the hierarchical knowledge distillation\n(HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer\nknowledge from PLMs to the ASR models, HKD employs cross-modal knowledge\ndistillation with contrastive loss at the acoustic level and knowledge\ndistillation with regression loss at the linguistic level. Compared with the\noriginal CIF-based model, our method achieves 15% and 9% relative error rate\nreduction on the AISHELL-1 and LibriSpeech datasets, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2301.13003v2.pdf"
    },
    {
        "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
        "authors": [
            "Terry Yue Zhuo",
            "Zhuang Li",
            "Yujin Huang",
            "Fatemeh Shiri",
            "Weiqing Wang",
            "Gholamreza Haffari",
            "Yuan-Fang Li"
        ],
        "published": "2023-01-30T13:21:00Z",
        "summary": "Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.",
        "pdf_link": "https://arxiv.org/pdf/2301.12868v3.pdf"
    },
    {
        "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",
        "authors": [
            "Terry Yue Zhuo",
            "Yujin Huang",
            "Chunyang Chen",
            "Zhenchang Xing"
        ],
        "published": "2023-01-30T13:20:48Z",
        "summary": "Recent breakthroughs in natural language processing (NLP) have permitted the\nsynthesis and comprehension of coherent text in an open-ended way, therefore\ntranslating the theoretical algorithms into practical applications. The large\nlanguage models (LLMs) have significantly impacted businesses such as report\nsummarization software and copywriters. Observations indicate, however, that\nLLMs may exhibit social prejudice and toxicity, posing ethical and societal\ndangers of consequences resulting from irresponsibility. Large-scale benchmarks\nfor accountable LLMs should consequently be developed. Although several\nempirical investigations reveal the existence of a few ethical difficulties in\nadvanced LLMs, there is little systematic examination and user study of the\nrisks and harmful behaviors of current LLM usage. To further educate future\nefforts on constructing ethical LLMs responsibly, we perform a qualitative\nresearch method called ``red teaming'' on OpenAI's ChatGPT\\footnote{In this\npaper, ChatGPT refers to the version released on Dec 15th.} to better\nunderstand the practical features of ethical dangers in recent LLMs. We analyze\nChatGPT comprehensively from four perspectives: 1) \\textit{Bias} 2)\n\\textit{Reliability} 3) \\textit{Robustness} 4) \\textit{Toxicity}. In accordance\nwith our stated viewpoints, we empirically benchmark ChatGPT on multiple sample\ndatasets. We find that a significant number of ethical risks cannot be\naddressed by existing benchmarks, and hence illustrate them via additional case\nstudies. In addition, we examine the implications of our findings on AI ethics\nand harmal behaviors of ChatGPT, as well as future problems and practical\ndesign considerations for responsible LLMs. We believe that our findings may\ngive light on future efforts to determine and mitigate the ethical hazards\nposed by machines in LLM applications.",
        "pdf_link": "https://arxiv.org/pdf/2301.12867v4.pdf"
    },
    {
        "title": "Specializing Smaller Language Models towards Multi-Step Reasoning",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Litu Ou",
            "Ashish Sabharwal",
            "Tushar Khot"
        ],
        "published": "2023-01-30T08:51:19Z",
        "summary": "The surprising ability of Large Language Models (LLMs) to perform well on\ncomplex reasoning with only few-shot chain-of-thought prompts is believed to\nemerge only in very large-scale models (100+ billion parameters). We show that\nsuch abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5\nvariants ($\\le$ 11B). We propose model specialization, to specialize the\nmodel's ability towards a target task. The hypothesis is that large models\n(commonly viewed as larger than 100B) have strong modeling power, but are\nspread on a large spectrum of tasks. Small models (commonly viewed as smaller\nthan 10B) have limited model capacity, but if we concentrate their capacity on\na specific target task, the model can achieve a decent improved performance. We\nuse multi-step math reasoning as our testbed because it is a very typical\nemergent ability. We show two important aspects of model abilities: (1). there\nexists a very complex balance/ tradeoff between language models'\nmulti-dimensional abilities; (2). by paying the price of decreased generic\nability, we can clearly lift up the scaling curve of models smaller than 10B\ntowards a specialized multi-step math reasoning ability. We further give\ncomprehensive discussions about important design choices for better\ngeneralization, including the tuning data format, the start model checkpoint,\nand a new model selection method. We hope our practice and discoveries can\nserve as an important attempt towards specialized smaller models in the new\nresearch paradigm set by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2301.12726v1.pdf"
    },
    {
        "title": "A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction",
        "authors": [
            "Kyle Mahowald"
        ],
        "published": "2023-01-29T22:29:55Z",
        "summary": "Knowledge of syntax includes knowledge of rare, idiosyncratic constructions.\nLLMs must overcome frequency biases in order to master such constructions. In\nthis study, I prompt GPT-3 to give acceptability judgments on the\nEnglish-language Article + Adjective + Numeral + Noun construction (e.g., \"a\nlovely five days\"). I validate the prompt using the CoLA corpus of\nacceptability judgments and then zero in on the AANN construction. I compare\nGPT- 3's judgments to crowdsourced human judgments on a subset of sentences.\nGPT-3's judgments are broadly similar to human judgments and generally align\nwith proposed constraints in the literature but, in some cases, GPT-3's\njudgments and human judgments diverge from the literature and from each other.",
        "pdf_link": "https://arxiv.org/pdf/2301.12564v2.pdf"
    },
    {
        "title": "Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes",
        "authors": [
            "Vahan Arsenyan",
            "Spartak Bughdaryan",
            "Fadi Shaya",
            "Kent Small",
            "Davit Shahnazaryan"
        ],
        "published": "2023-01-29T15:52:33Z",
        "summary": "The automatic construction of knowledge graphs (KGs) is an important research\narea in medicine, with far-reaching applications spanning drug discovery and\nclinical trial design. These applications hinge on the accurate identification\nof interactions among medical and biological entities. In this study, we\npropose an end-to-end machine learning solution based on large language models\n(LLMs) that utilize electronic medical record notes to construct KGs. The\nentities used in the KG construction process are diseases, factors, treatments,\nas well as manifestations that coexist with the patient while experiencing the\ndisease. Given the critical need for high-quality performance in medical\napplications, we embark on a comprehensive assessment of 12 LLMs of various\narchitectures, evaluating their performance and safety attributes. To gauge the\nquantitative efficacy of our approach by assessing both precision and recall,\nwe manually annotate a dataset provided by the Macula and Retina Institute. We\nalso assess the qualitative performance of LLMs, such as the ability to\ngenerate structured outputs or the tendency to hallucinate. The results\nillustrate that in contrast to encoder-only and encoder-decoder, decoder-only\nLLMs require further investigation. Additionally, we provide guided prompt\ndesign to utilize such LLMs. The application of the proposed methodology is\ndemonstrated on age-related macular degeneration.",
        "pdf_link": "https://arxiv.org/pdf/2301.12473v2.pdf"
    },
    {
        "title": "Emerging Synergies in Causality and Deep Generative Models: A Survey",
        "authors": [
            "Guanglin Zhou",
            "Shaoan Xie",
            "Guangyuan Hao",
            "Shiming Chen",
            "Biwei Huang",
            "Xiwei Xu",
            "Chen Wang",
            "Liming Zhu",
            "Lina Yao",
            "Kun Zhang"
        ],
        "published": "2023-01-29T04:10:12Z",
        "summary": "In the field of artificial intelligence (AI), the quest to understand and\nmodel data-generating processes (DGPs) is of paramount importance. Deep\ngenerative models (DGMs) have proven adept in capturing complex data\ndistributions but often fall short in generalization and interpretability. On\nthe other hand, causality offers a structured lens to comprehend the mechanisms\ndriving data generation and highlights the causal-effect dynamics inherent in\nthese processes. While causality excels in interpretability and the ability to\nextrapolate, it grapples with intricacies of high-dimensional spaces.\nRecognizing the synergistic potential, we delve into the confluence of\ncausality and DGMs. We elucidate the integration of causal principles within\nDGMs, investigate causal identification using DGMs, and navigate an emerging\nresearch frontier of causality in large-scale generative models, particularly\ngenerative large language models (LLMs). We offer insights into methodologies,\nhighlight open challenges, and suggest future directions, positioning our\ncomprehensive review as an essential guide in this swiftly emerging and\nevolving area.",
        "pdf_link": "https://arxiv.org/pdf/2301.12351v3.pdf"
    },
    {
        "title": "Context-Aware Differential Privacy for Language Modeling",
        "authors": [
            "My H. Dinh",
            "Ferdinando Fioretto"
        ],
        "published": "2023-01-28T20:06:16Z",
        "summary": "The remarkable ability of language models (LMs) has also brought challenges\nat the interface of AI and security. A critical challenge pertains to how much\ninformation these models retain and leak about the training data. This is\nparticularly urgent as the typical development of LMs relies on huge, often\nhighly sensitive data, such as emails and chat logs. To contrast this\nshortcoming, this paper introduces Context-Aware Differentially Private\nLanguage Model (CADP-LM) , a privacy-preserving LM framework that relies on two\nkey insights: First, it utilizes the notion of \\emph{context} to define and\naudit the potentially sensitive information. Second, it adopts the notion of\nDifferential Privacy to protect sensitive information and characterize the\nprivacy leakage. A unique characteristic of CADP-LM is its ability to target\nthe protection of sensitive sentences and contexts only, providing a highly\naccurate private model. Experiments on a variety of datasets and settings\ndemonstrate these strengths of CADP-LM.",
        "pdf_link": "https://arxiv.org/pdf/2301.12288v1.pdf"
    },
    {
        "title": "Truth Machines: Synthesizing Veracity in AI Language Models",
        "authors": [
            "Luke Munn",
            "Liam Magee",
            "Vanicka Arora"
        ],
        "published": "2023-01-28T02:47:50Z",
        "summary": "As AI technologies are rolled out into healthcare, academia, human resources,\nlaw, and a multitude of other domains, they become de-facto arbiters of truth.\nBut truth is highly contested, with many different definitions and approaches.\nThis article discusses the struggle for truth in AI systems and the general\nresponses to date. It then investigates the production of truth in InstructGPT,\na large language model, highlighting how data harvesting, model architectures,\nand social feedback mechanisms weave together disparate understandings of\nveracity. It conceptualizes this performance as an operationalization of truth,\nwhere distinct, often conflicting claims are smoothly synthesized and\nconfidently presented into truth-statements. We argue that these same logics\nand inconsistencies play out in Instruct's successor, ChatGPT, reiterating\ntruth as a non-trivial problem. We suggest that enriching sociality and\nthickening \"reality\" are two promising vectors for enhancing the\ntruth-evaluating capacities of future language models. We conclude, however, by\nstepping back to consider AI truth-telling as a social practice: what kind of\n\"truth\" do we as listeners desire?",
        "pdf_link": "https://arxiv.org/pdf/2301.12066v1.pdf"
    },
    {
        "title": "Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling",
        "authors": [
            "Kolby Nottingham",
            "Prithviraj Ammanabrolu",
            "Alane Suhr",
            "Yejin Choi",
            "Hannaneh Hajishirzi",
            "Sameer Singh",
            "Roy Fox"
        ],
        "published": "2023-01-28T02:04:07Z",
        "summary": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior\nknowledge of the world. However, if initialized with knowledge of high-level\nsubgoals and transitions between subgoals, RL agents could utilize this\nAbstract World Model (AWM) for planning and exploration. We propose using\nfew-shot large language models (LLMs) to hypothesize an AWM, that will be\nverified through world experience, to improve sample efficiency of RL agents.\nOur DECKARD agent applies LLM-guided exploration to item crafting in Minecraft\nin two phases: (1) the Dream phase where the agent uses an LLM to decompose a\ntask into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase\nwhere the agent learns a modular policy for each subgoal and verifies or\ncorrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and\nthen verifying the AWM based on agent experience not only increases sample\nefficiency over contemporary methods by an order of magnitude but is also\nrobust to and corrects errors in the LLM, successfully blending noisy\ninternet-scale information from LLMs with knowledge grounded in environment\ndynamics.",
        "pdf_link": "https://arxiv.org/pdf/2301.12050v2.pdf"
    },
    {
        "title": "Context Matters: A Strategy to Pre-train Language Model for Science Education",
        "authors": [
            "Zhengliang Liu",
            "Xinyu He",
            "Lei Liu",
            "Tianming Liu",
            "Xiaoming Zhai"
        ],
        "published": "2023-01-27T23:50:16Z",
        "summary": "This study aims at improving the performance of scoring student responses in\nscience education automatically. BERT-based language models have shown\nsignificant superiority over traditional NLP models in various language-related\ntasks. However, science writing of students, including argumentation and\nexplanation, is domain-specific. In addition, the language used by students is\ndifferent from the language in journals and Wikipedia, which are training\nsources of BERT and its existing variants. All these suggest that a\ndomain-specific model pre-trained using science education data may improve\nmodel performance. However, the ideal type of data to contextualize pre-trained\nlanguage model and improve the performance in automatically scoring student\nwritten responses remains unclear. Therefore, we employ different data in this\nstudy to contextualize both BERT and SciBERT models and compare their\nperformance on automatic scoring of assessment tasks for scientific\nargumentation. We use three datasets to pre-train the model: 1) journal\narticles in science education, 2) a large dataset of students' written\nresponses (sample size over 50,000), and 3) a small dataset of students'\nwritten responses of scientific argumentation tasks. Our experimental results\nshow that in-domain training corpora constructed from science questions and\nresponses improve language model performance on a wide variety of downstream\ntasks. Our study confirms the effectiveness of continual pre-training on\ndomain-specific data in the education domain and demonstrates a generalizable\nstrategy for automating science education tasks with high accuracy. We plan to\nrelease our data and SciEdBERT models for public use and community engagement.",
        "pdf_link": "https://arxiv.org/pdf/2301.12031v1.pdf"
    },
    {
        "title": "Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases",
        "authors": [
            "Xiaoxia Wu",
            "Cheng Li",
            "Reza Yazdani Aminabadi",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "published": "2023-01-27T22:44:18Z",
        "summary": "Improving the deployment efficiency of transformer-based language models has\nbeen challenging given their high computation and memory cost. While INT8\nquantization has recently been shown to be effective in reducing both the\nmemory cost and latency while preserving model accuracy, it remains unclear\nwhether we can leverage INT4 (which doubles peak hardware throughput) to\nachieve further latency improvement. In this study, we explore the feasibility\nof employing INT4 weight and activation (W4A4) quantization for language\nmodels. Our findings indicate that W4A4 quantization introduces no to\nnegligible accuracy degradation for encoder-only and encoder-decoder models,\nbut causes a significant accuracy drop for decoder-only models. To materialize\nthe performance gain using W4A4, we develop a highly optimized end-to-end W4A4\nencoder inference pipeline supporting different quantization strategies. Our\nINT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to\n$3\\times$ for throughput-oriented scenarios compared to the inference of FP16,\nand improves the SOTA BERT INT8 performance from FasterTransformer by up to\n$1.7\\times$. We provide insights into the failure cases when applying W4A4 to\ndecoder-only models, and further explore the compatibility of INT4 quantization\nwith other compression methods, like pruning and layer reduction.",
        "pdf_link": "https://arxiv.org/pdf/2301.12017v2.pdf"
    },
    {
        "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning",
        "authors": [
            "Xinyi Wang",
            "Wanrong Zhu",
            "Michael Saxon",
            "Mark Steyvers",
            "William Yang Wang"
        ],
        "published": "2023-01-27T18:59:01Z",
        "summary": "In recent years, pre-trained large language models (LLMs) have demonstrated\nremarkable efficiency in achieving an inference-time few-shot learning\ncapability known as in-context learning. However, existing literature has\nhighlighted the sensitivity of this capability to the selection of few-shot\ndemonstrations. Current understandings of the underlying mechanisms by which\nthis capability arises from regular language model pretraining objectives\nremain disconnected from the real-world LLMs. This study aims to examine the\nin-context learning phenomenon through a Bayesian lens, viewing real-world LLMs\nas latent variable models. On this premise, we propose an algorithm to select\noptimal demonstrations from a set of annotated data with a small LM, and then\ndirectly generalize the selected demonstrations to larger LMs. We demonstrate\nsignificant improvement over baselines, averaged over eight GPT models on eight\nreal-world text classification datasets. We also demonstrate the real-world\nusefulness of our algorithm on GSM8K, a math word problem dataset. Our\nempirical findings support our hypothesis that LLMs implicitly infer a latent\nvariable containing task information.",
        "pdf_link": "https://arxiv.org/pdf/2301.11916v4.pdf"
    },
    {
        "title": "Learning the Effects of Physical Actions in a Multi-modal Environment",
        "authors": [
            "Gautier Dagan",
            "Frank Keller",
            "Alex Lascarides"
        ],
        "published": "2023-01-27T16:49:52Z",
        "summary": "Large Language Models (LLMs) handle physical commonsense information\ninadequately. As a result of being trained in a disembodied setting, LLMs often\nfail to predict an action's outcome in a given environment. However, predicting\nthe effects of an action before it is executed is crucial in planning, where\ncoherent sequences of actions are often needed to achieve a goal. Therefore, we\nintroduce the multi-modal task of predicting the outcomes of actions solely\nfrom realistic sensory inputs (images and text). Next, we extend an LLM to\nmodel latent representations of objects to better predict action outcomes in an\nenvironment. We show that multi-modal models can capture physical commonsense\nwhen augmented with visual information. Finally, we evaluate our model's\nperformance on novel actions and objects and find that combining modalities\nhelp models to generalize and learn physical commonsense reasoning better.",
        "pdf_link": "https://arxiv.org/pdf/2301.11845v2.pdf"
    },
    {
        "title": "Investigating the use of ChatGPT for the scheduling of construction projects",
        "authors": [
            "Samuel A. Prieto",
            "Eyob T. Mengiste",
            "Borja García de Soto"
        ],
        "published": "2023-01-27T12:05:44Z",
        "summary": "Large language models such as ChatGPT have the potential to revolutionize the\nconstruction industry by automating repetitive and time-consuming tasks. This\npaper presents a study in which ChatGPT was used to generate a construction\nschedule for a simple construction project. The output from ChatGPT was\nevaluated by a pool of participants that provided feedback regarding their\noverall interaction experience and the quality of the output. The results show\nthat ChatGPT can generate a coherent schedule that follows a logical approach\nto fulfill the requirements of the scope indicated. The participants had an\noverall positive interaction experience and indicated the great potential of\nsuch a tool to automate many preliminary and time-consuming tasks. However, the\ntechnology still has limitations, and further development is needed before it\ncan be widely adopted in the industry. Overall, this study highlights the\npotential of using large language models in the construction industry and the\nneed for further research.",
        "pdf_link": "https://arxiv.org/pdf/2302.02805v1.pdf"
    },
    {
        "title": "Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning",
        "authors": [
            "Hyunsoo Cho",
            "Choonghyun Park",
            "Junyeop Kim",
            "Hyuhng Joon Kim",
            "Kang Min Yoo",
            "Sang-goo Lee"
        ],
        "published": "2023-01-27T11:27:40Z",
        "summary": "As the size of the pre-trained language model (PLM) continues to increase,\nnumerous parameter-efficient transfer learning methods have been proposed\nrecently to compensate for the tremendous cost of fine-tuning. Despite the\nimpressive results achieved by large pre-trained language models (PLMs) and\nvarious parameter-efficient transfer learning (PETL) methods on sundry\nbenchmarks, it remains unclear if they can handle inputs that have been\ndistributionally shifted effectively. In this study, we systematically explore\nhow the ability to detect out-of-distribution (OOD) changes as the size of the\nPLM grows or the transfer methods are altered. Specifically, we evaluated\nvarious PETL techniques, including fine-tuning, Adapter, LoRA, and\nprefix-tuning, on three different intention classification tasks, each\nutilizing various language models with different scales.",
        "pdf_link": "https://arxiv.org/pdf/2301.11660v4.pdf"
    },
    {
        "title": "A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks",
        "authors": [
            "Lecheng Kong",
            "Christopher King",
            "Bradley Fritz",
            "Yixin Chen"
        ],
        "published": "2023-01-27T09:19:03Z",
        "summary": "Learning to represent free text is a core task in many clinical machine\nlearning (ML) applications, as clinical text contains observations and plans\nnot otherwise available for inference. State-of-the-art methods use large\nlanguage models developed with immense computational resources and training\ndata; however, applying these models is challenging because of the highly\nvarying syntax and vocabulary in clinical free text. Structured information\nsuch as International Classification of Disease (ICD) codes often succinctly\nabstracts the most important facts of a clinical encounter and yields good\nperformance, but is often not as available as clinical text in real-world\nscenarios. We propose a \\textbf{multi-view learning framework} that jointly\nlearns from codes and text to combine the availability and forward-looking\nnature of text and better performance of ICD codes. The learned text embeddings\ncan be used as inputs to predictive algorithms independent of the ICD codes\nduring inference. Our approach uses a Graph Neural Network (GNN) to process ICD\ncodes, and Bi-LSTM to process text. We apply Deep Canonical Correlation\nAnalysis (DCCA) to enforce the two views to learn a similar representation of\neach patient. In experiments using planned surgical procedure text, our model\noutperforms BERT models fine-tuned to clinical data, and in experiments using\ndiverse text in MIMIC-III, our model is competitive to a fine-tuned BERT at a\ntiny fraction of its computational effort.",
        "pdf_link": "https://arxiv.org/pdf/2301.11608v1.pdf"
    },
    {
        "title": "ThoughtSource: A central hub for large language model reasoning data",
        "authors": [
            "Simon Ott",
            "Konstantin Hebenstreit",
            "Valentin Liévin",
            "Christoffer Egeberg Hother",
            "Milad Moradi",
            "Maximilian Mayrhauser",
            "Robert Praas",
            "Ole Winther",
            "Matthias Samwald"
        ],
        "published": "2023-01-27T08:45:53Z",
        "summary": "Large language models (LLMs) such as GPT-4 have recently demonstrated\nimpressive results across a wide range of tasks. LLMs are still limited,\nhowever, in that they frequently fail at complex reasoning, their reasoning\nprocesses are opaque, they are prone to 'hallucinate' facts, and there are\nconcerns about their underlying biases. Letting models verbalize reasoning\nsteps as natural language, a technique known as chain-of-thought prompting, has\nrecently been proposed as a way to address some of these issues. Here we\npresent ThoughtSource, a meta-dataset and software library for chain-of-thought\n(CoT) reasoning. The goal of ThoughtSource is to improve future artificial\nintelligence systems by facilitating qualitative understanding of CoTs,\nenabling empirical evaluations, and providing training data. This first release\nof ThoughtSource integrates seven scientific/medical, three general-domain and\nfive math word question answering datasets.",
        "pdf_link": "https://arxiv.org/pdf/2301.11596v5.pdf"
    },
    {
        "title": "Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding",
        "authors": [
            "Yaoxian Song",
            "Penglei Sun",
            "Yi Ren",
            "Yu Zheng",
            "Yue Zhang"
        ],
        "published": "2023-01-27T07:00:54Z",
        "summary": "Robotic grasping is a fundamental ability for a robot to interact with the\nenvironment. Current methods focus on how to obtain a stable and reliable\ngrasping pose in object wise, while little work has been studied on part\n(shape)-wise grasping which is related to fine-grained grasping and robotic\naffordance. Parts can be seen as atomic elements to compose an object, which\ncontains rich semantic knowledge and a strong correlation with affordance.\nHowever, lacking a large part-wise 3D robotic dataset limits the development of\npart representation learning and downstream application. In this paper, we\npropose a new large Language-guided SHape grAsPing datasEt (named Lang-SHAPE)\nto learn 3D part-wise affordance and grasping ability. We design a novel\ntwo-stage fine-grained robotic grasping network (named PIONEER), including a\nnovel 3D part language grounding model, and a part-aware grasp pose detection\nmodel. To evaluate the effectiveness, we perform multi-level difficulty part\nlanguage grounding grasping experiments and deploy our proposed model on a real\nrobot. Results show our method achieves satisfactory performance and efficiency\nin reference identification, affordance inference, and 3D part-aware grasping.\nOur dataset and code are available on our project website\nhttps://sites.google.com/view/lang-shape",
        "pdf_link": "https://arxiv.org/pdf/2301.11564v1.pdf"
    },
    {
        "title": "Theme-driven Keyphrase Extraction to Analyze Social Media Discourse",
        "authors": [
            "William Romano",
            "Omar Sharif",
            "Madhusudan Basak",
            "Joseph Gatto",
            "Sarah Preum"
        ],
        "published": "2023-01-27T03:00:46Z",
        "summary": "Social media platforms are vital resources for sharing self-reported health\nexperiences, offering rich data on various health topics. Despite advancements\nin Natural Language Processing (NLP) enabling large-scale social media data\nanalysis, a gap remains in applying keyphrase extraction to health-related\ncontent. Keyphrase extraction is used to identify salient concepts in social\nmedia discourse without being constrained by predefined entity classes. This\npaper introduces a theme-driven keyphrase extraction framework tailored for\nsocial media, a pioneering approach designed to capture clinically relevant\nkeyphrases from user-generated health texts. Themes are defined as broad\ncategories determined by the objectives of the extraction task. We formulate\nthis novel task of theme-driven keyphrase extraction and demonstrate its\npotential for efficiently mining social media text for the use case of\ntreatment for opioid use disorder. This paper leverages qualitative and\nquantitative analysis to demonstrate the feasibility of extracting actionable\ninsights from social media data and efficiently extracting keyphrases using\nminimally supervised NLP models. Our contributions include the development of a\nnovel data collection and curation framework for theme-driven keyphrase\nextraction and the creation of MOUD-Keyphrase, the first dataset of its kind\ncomprising human-annotated keyphrases from a Reddit community. We also identify\nthe scope of minimally supervised NLP models to extract keyphrases from social\nmedia data efficiently. Lastly, we found that a large language model (ChatGPT)\noutperforms unsupervised keyphrase extraction models, and we evaluate its\nefficacy in this task.",
        "pdf_link": "https://arxiv.org/pdf/2301.11508v2.pdf"
    },
    {
        "title": "Task formulation for Extracting Social Determinants of Health from Clinical Narratives",
        "authors": [
            "Manabu Torii",
            "Ian M. Finn",
            "Son Doan",
            "Paul Wang",
            "Elly W. Yang",
            "Daniel S. Zisook"
        ],
        "published": "2023-01-26T20:00:54Z",
        "summary": "Objective: The 2022 n2c2 NLP Challenge posed identification of social\ndeterminants of health (SDOH) in clinical narratives. We present three systems\nthat we developed for the Challenge and discuss the distinctive task\nformulation used in each of the three systems. Materials and Methods: The first\nsystem identifies target pieces of information independently using machine\nlearning classifiers. The second system uses a large language model (LLM) to\nextract complete structured outputs per document. The third system extracts\ncandidate phrases using machine learning and identifies target relations with\nhand-crafted rules. Results: The three systems achieved F1 scores of 0.884,\n0.831, and 0.663 in the Subtask A of the Challenge, which are ranked third,\nseventh, and eighth among the 15 participating teams. The review of the\nextraction results from our systems reveals characteristics of each approach\nand those of the SODH extraction task. Discussion: Phrases and relations\nannotated in the task is unique and diverse, not conforming to the conventional\nevent extraction task. These annotations are difficult to model with limited\ntraining data. The system that extracts information independently, ignoring the\nannotated relations, achieves the highest F1 score. Meanwhile, LLM with its\nversatile capability achieves the high F1 score, while respecting the annotated\nrelations. The rule-based system tackling relation extraction obtains the low\nF1 score, while it is the most explainable approach. Conclusion: The F1 scores\nof the three systems vary in this challenge setting, but each approach has\nadvantages and disadvantages in a practical application. The selection of the\napproach depends not only on the F1 score but also on the requirements in the\napplication.",
        "pdf_link": "https://arxiv.org/pdf/2301.11386v1.pdf"
    },
    {
        "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
        "authors": [
            "Eric Mitchell",
            "Yoonho Lee",
            "Alexander Khazatsky",
            "Christopher D. Manning",
            "Chelsea Finn"
        ],
        "published": "2023-01-26T18:44:06Z",
        "summary": "The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.",
        "pdf_link": "https://arxiv.org/pdf/2301.11305v2.pdf"
    },
    {
        "title": "Domain-Agnostic Molecular Generation with Chemical Feedback",
        "authors": [
            "Yin Fang",
            "Ningyu Zhang",
            "Zhuo Chen",
            "Lingbing Guo",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "published": "2023-01-26T17:52:56Z",
        "summary": "The generation of molecules with desired properties has become increasingly\npopular, revolutionizing the way scientists design molecular structures and\nproviding valuable support for chemical and drug design. However, despite the\npotential of language models in molecule generation, they face challenges such\nas generating syntactically or chemically flawed molecules, having narrow\ndomain focus, and struggling to create diverse and feasible molecules due to\nlimited annotated data or external molecular databases. To tackle these\nchallenges, we introduce MolGen, a pre-trained molecular language model\ntailored specifically for molecule generation. Through the reconstruction of\nover 100 million molecular SELFIES, MolGen internalizes structural and\ngrammatical insights. This is further enhanced by domain-agnostic molecular\nprefix tuning, fostering robust knowledge transfer across diverse domains.\nImportantly, our chemical feedback paradigm steers the model away from\nmolecular hallucinations, ensuring alignment between the model's estimated\nprobabilities and real-world chemical preferences. Extensive experiments on\nwell-known benchmarks underscore MolGen's optimization capabilities in\nproperties such as penalized logP, QED, and molecular docking. Additional\nanalyses confirm its proficiency in accurately capturing molecule\ndistributions, discerning intricate structural patterns, and efficiently\nexploring the chemical space. Code is available at\nhttps://github.com/zjunlp/MolGen.",
        "pdf_link": "https://arxiv.org/pdf/2301.11259v6.pdf"
    },
    {
        "title": "Causal Reasoning of Entities and Events in Procedural Texts",
        "authors": [
            "Li Zhang",
            "Hainiu Xu",
            "Yue Yang",
            "Shuyan Zhou",
            "Weiqiu You",
            "Manni Arora",
            "Chris Callison-Burch"
        ],
        "published": "2023-01-26T01:43:17Z",
        "summary": "Entities and events are crucial to natural language reasoning and common in\nprocedural texts. Existing work has focused either exclusively on entity state\ntracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one\nwould burn themselves by touching the pan), while these two tasks are often\ncausally related. We propose CREPE, the first benchmark on causal reasoning of\nevent plausibility and entity states. We show that most language models,\nincluding GPT-3, perform close to chance at .35 F1, lagging far behind human at\n.87 F1. We boost model performance to .59 F1 by creatively representing events\nas programming languages while prompting language models pretrained on code. By\ninjecting the causal relations between entities and events as intermediate\nreasoning steps in our representation, we further boost the performance to .67\nF1. Our findings indicate not only the challenge that CREPE brings for language\nmodels, but also the efficacy of code-like prompting combined with\nchain-of-thought prompting for multihop event reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2301.10896v3.pdf"
    },
    {
        "title": "Explaining Large Language Model-Based Neural Semantic Parsers (Student Abstract)",
        "authors": [
            "Daking Rai",
            "Yilun Zhou",
            "Bailin Wang",
            "Ziyu Yao"
        ],
        "published": "2023-01-25T16:12:43Z",
        "summary": "While large language models (LLMs) have demonstrated strong capability in\nstructured prediction tasks such as semantic parsing, few amounts of research\nhave explored the underlying mechanisms of their success. Our work studies\ndifferent methods for explaining an LLM-based semantic parser and qualitatively\ndiscusses the explained model behaviors, hoping to inspire future research\ntoward better understanding them.",
        "pdf_link": "https://arxiv.org/pdf/2301.13820v1.pdf"
    },
    {
        "title": "ExaRanker: Explanation-Augmented Neural Ranker",
        "authors": [
            "Fernando Ferraretto",
            "Thiago Laitz",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "published": "2023-01-25T11:03:04Z",
        "summary": "Recent work has shown that inducing a large language model (LLM) to generate\nexplanations prior to outputting an answer is an effective strategy to improve\nperformance on a wide range of reasoning tasks. In this work, we show that\nneural rankers also benefit from explanations. We use LLMs such as GPT-3.5 to\naugment retrieval datasets with explanations and train a sequence-to-sequence\nranking model to output a relevance label and an explanation for a given\nquery-document pair. Our model, dubbed ExaRanker, finetuned on a few thousand\nexamples with synthetic explanations performs on par with models finetuned on\n3x more examples without explanations. Furthermore, the ExaRanker model incurs\nno additional computational cost during ranking and allows explanations to be\nrequested on demand.",
        "pdf_link": "https://arxiv.org/pdf/2301.10521v2.pdf"
    },
    {
        "title": "Language Model Detoxification in Dialogue with Contextualized Stance Control",
        "authors": [
            "Jing Qian",
            "Xifeng Yan"
        ],
        "published": "2023-01-25T00:47:28Z",
        "summary": "To reduce the toxic degeneration in a pretrained Language Model (LM),\nprevious work on Language Model detoxification has focused on reducing the\ntoxicity of the generation itself (self-toxicity) without consideration of the\ncontext. As a result, a type of implicit offensive language where the\ngenerations support the offensive language in the context is ignored. Different\nfrom the LM controlling tasks in previous work, where the desired attributes\nare fixed for generation, the desired stance of the generation depends on the\noffensiveness of the context. Therefore, we propose a novel control method to\ndo context-dependent detoxification with the stance taken into consideration.\nWe introduce meta prefixes to learn the contextualized stance control strategy\nand to generate the stance control prefix according to the input context. The\ngenerated stance prefix is then combined with the toxicity control prefix to\nguide the response generation. Experimental results show that our proposed\nmethod can effectively learn the context-dependent stance control strategies\nwhile keeping a low self-toxicity of the underlying LM.",
        "pdf_link": "https://arxiv.org/pdf/2301.10368v1.pdf"
    },
    {
        "title": "Audience-Centric Natural Language Generation via Style Infusion",
        "authors": [
            "Samraj Moorjani",
            "Adit Krishnan",
            "Hari Sundaram",
            "Ewa Maslowska",
            "Aravind Sankar"
        ],
        "published": "2023-01-24T19:57:50Z",
        "summary": "Adopting contextually appropriate, audience-tailored linguistic styles is\ncritical to the success of user-centric language generation systems (e.g.,\nchatbots, computer-aided writing, dialog systems). While existing approaches\ndemonstrate textual style transfer with large volumes of parallel or\nnon-parallel data, we argue that grounding style on audience-independent\nexternal factors is innately limiting for two reasons. First, it is difficult\nto collect large volumes of audience-specific stylistic data. Second, some\nstylistic objectives (e.g., persuasiveness, memorability, empathy) are hard to\ndefine without audience feedback.\n  In this paper, we propose the novel task of style infusion - infusing the\nstylistic preferences of audiences in pretrained language generation models.\nSince humans are better at pairwise comparisons than direct scoring - i.e., is\nSample-A more persuasive/polite/empathic than Sample-B - we leverage limited\npairwise human judgments to bootstrap a style analysis model and augment our\nseed set of judgments. We then infuse the learned textual style in a GPT-2\nbased text generator while balancing fluency and style adoption. With\nquantitative and qualitative assessments, we show that our infusion approach\ncan generate compelling stylized examples with generic text prompts. The code\nand data are accessible at https://github.com/CrowdDynamicsLab/StyleInfusion.",
        "pdf_link": "https://arxiv.org/pdf/2301.10283v1.pdf"
    },
    {
        "title": "A Watermark for Large Language Models",
        "authors": [
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Jonathan Katz",
            "Ian Miers",
            "Tom Goldstein"
        ],
        "published": "2023-01-24T18:52:59Z",
        "summary": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.",
        "pdf_link": "https://arxiv.org/pdf/2301.10226v3.pdf"
    },
    {
        "title": "Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models",
        "authors": [
            "Tung Phung",
            "José Cambronero",
            "Sumit Gulwani",
            "Tobias Kohn",
            "Rupak Majumdar",
            "Adish Singla",
            "Gustavo Soares"
        ],
        "published": "2023-01-24T13:00:25Z",
        "summary": "Large language models (LLMs), such as Codex, hold great promise in enhancing\nprogramming education by automatically generating feedback for students. We\ninvestigate using LLMs to generate feedback for fixing syntax errors in Python\nprograms, a key scenario in introductory programming. More concretely, given a\nstudent's buggy program, our goal is to generate feedback comprising a fixed\nprogram along with a natural language explanation describing the errors/fixes,\ninspired by how a human tutor would give feedback. While using LLMs is\npromising, the critical challenge is to ensure high precision in the generated\nfeedback, which is imperative before deploying such technology in classrooms.\nThe main research question we study is: Can we develop LLMs-based feedback\ngeneration techniques with a tunable precision parameter, giving educators\nquality control over the feedback that students receive? To this end, we\nintroduce PyFiXV, our technique to generate high-precision feedback powered by\nCodex. The key idea behind PyFiXV is to use a novel run-time validation\nmechanism to decide whether the generated feedback is suitable for sharing with\nthe student; notably, this validation mechanism also provides a precision knob\nto educators. We perform an extensive evaluation using two real-world datasets\nof Python programs with syntax errors and show the efficacy of PyFiXV in\ngenerating high-precision feedback.",
        "pdf_link": "https://arxiv.org/pdf/2302.04662v2.pdf"
    },
    {
        "title": "Opportunities and Challenges in Neural Dialog Tutoring",
        "authors": [
            "Jakub Macina",
            "Nico Daheim",
            "Lingzhi Wang",
            "Tanmay Sinha",
            "Manu Kapur",
            "Iryna Gurevych",
            "Mrinmaya Sachan"
        ],
        "published": "2023-01-24T11:00:17Z",
        "summary": "Designing dialog tutors has been challenging as it involves modeling the\ndiverse and complex pedagogical strategies employed by human tutors. Although\nthere have been significant recent advances in neural conversational systems\nusing large language models (LLMs) and growth in available dialog corpora,\ndialog tutoring has largely remained unaffected by these advances. In this\npaper, we rigorously analyze various generative language models on two dialog\ntutoring datasets for language learning using automatic and human evaluations\nto understand the new opportunities brought by these advances as well as the\nchallenges we must overcome to build models that would be usable in real\neducational settings. We find that although current approaches can model\ntutoring in constrained learning scenarios when the number of concepts to be\ntaught and possible teacher strategies are small, they perform poorly in less\nconstrained scenarios. Our human quality evaluation shows that both models and\nground-truth annotations exhibit low performance in terms of equitable\ntutoring, which measures learning opportunities for students and how engaging\nthe dialog is. To understand the behavior of our models in a real tutoring\nsetting, we conduct a user study using expert annotators and find a\nsignificantly large number of model reasoning errors in 45% of conversations.\nFinally, we connect our findings to outline future work.",
        "pdf_link": "https://arxiv.org/pdf/2301.09919v2.pdf"
    },
    {
        "title": "A Stability Analysis of Fine-Tuning a Pre-Trained Model",
        "authors": [
            "Zihao Fu",
            "Anthony Man-Cho So",
            "Nigel Collier"
        ],
        "published": "2023-01-24T05:11:17Z",
        "summary": "Fine-tuning a pre-trained model (such as BERT, ALBERT, RoBERTa, T5, GPT,\netc.) has proven to be one of the most promising paradigms in recent NLP\nresearch. However, numerous recent works indicate that fine-tuning suffers from\nthe instability problem, i.e., tuning the same model under the same setting\nresults in significantly different performance. Many recent works have proposed\ndifferent methods to solve this problem, but there is no theoretical\nunderstanding of why and how these methods work. In this paper, we propose a\nnovel theoretical stability analysis of fine-tuning that focuses on two\ncommonly used settings, namely, full fine-tuning and head tuning. We define the\nstability under each setting and prove the corresponding stability bounds. The\ntheoretical bounds explain why and how several existing methods can stabilize\nthe fine-tuning procedure. In addition to being able to explain most of the\nobserved empirical discoveries, our proposed theoretical analysis framework can\nalso help in the design of effective and provable methods. Based on our theory,\nwe propose three novel strategies to stabilize the fine-tuning procedure,\nnamely, Maximal Margin Regularizer (MMR), Multi-Head Loss (MHLoss), and Self\nUnsupervised Re-Training (SURT). We extensively evaluate our proposed\napproaches on 11 widely used real-world benchmark datasets, as well as hundreds\nof synthetic classification datasets. The experiment results show that our\nproposed methods significantly stabilize the fine-tuning procedure and also\ncorroborate our theoretical analysis.",
        "pdf_link": "https://arxiv.org/pdf/2301.09820v2.pdf"
    },
    {
        "title": "SMART: Self-supervised Multi-task pretrAining with contRol Transformers",
        "authors": [
            "Yanchao Sun",
            "Shuang Ma",
            "Ratnesh Madaan",
            "Rogerio Bonatti",
            "Furong Huang",
            "Ashish Kapoor"
        ],
        "published": "2023-01-24T05:01:23Z",
        "summary": "Self-supervised pretraining has been extensively studied in language and\nvision domains, where a unified model can be easily adapted to various\ndownstream tasks by pretraining representations without explicit labels. When\nit comes to sequential decision-making tasks, however, it is difficult to\nproperly design such a pretraining approach that can cope with both\nhigh-dimensional perceptual information and the complexity of sequential\ncontrol over long interaction horizons. The challenge becomes combinatorially\nmore complex if we want to pretrain representations amenable to a large variety\nof tasks. To tackle this problem, in this work, we formulate a general\npretraining-finetuning pipeline for sequential decision making, under which we\npropose a generic pretraining framework \\textit{Self-supervised Multi-task\npretrAining with contRol Transformer (SMART)}. By systematically investigating\npretraining regimes, we carefully design a Control Transformer (CT) coupled\nwith a novel control-centric pretraining objective in a self-supervised manner.\nSMART encourages the representation to capture the common essential information\nrelevant to short-term control and long-term control, which is transferrable\nacross tasks. We show by extensive experiments in DeepMind Control Suite that\nSMART significantly improves the learning efficiency among seen and unseen\ndownstream tasks and domains under different learning scenarios including\nImitation Learning (IL) and Reinforcement Learning (RL). Benefiting from the\nproposed control-centric objective, SMART is resilient to distribution shift\nbetween pretraining and finetuning, and even works well with low-quality\npretraining datasets that are randomly collected.",
        "pdf_link": "https://arxiv.org/pdf/2301.09816v1.pdf"
    },
    {
        "title": "Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning",
        "authors": [
            "Malte Ostendorff",
            "Georg Rehm"
        ],
        "published": "2023-01-23T18:56:12Z",
        "summary": "Most Transformer language models are primarily pretrained on English text,\nlimiting their use for other languages. As the model sizes grow, the\nperformance gap between English and other languages with fewer compute and data\nresources increases even further. Consequently, more resource-efficient\ntraining methods are needed to bridge the gap for languages with fewer\nresources available. To address this problem, we introduce a cross-lingual and\nprogressive transfer learning approach, called CLP-Transfer, that transfers\nmodels from a source language, for which pretrained models are publicly\navailable, like English, to a new target language. As opposed to prior work,\nwhich focused on the cross-lingual transfer between two languages, we extend\nthe transfer to the model size. Given a pretrained model in a source language,\nwe aim for a same-sized model in a target language. Instead of training a model\nfrom scratch, we exploit a smaller model that is in the target language but\nrequires much fewer resources. Both small and source models are then used to\ninitialize the token embeddings of the larger model based on the overlapping\nvocabulary of the source and target language. All remaining weights are reused\nfrom the model in the source language. This approach outperforms the sole\ncross-lingual transfer and can save up to 80% of the training steps compared to\nthe random initialization.",
        "pdf_link": "https://arxiv.org/pdf/2301.09626v1.pdf"
    },
    {
        "title": "An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models",
        "authors": [
            "Saghar Hosseini",
            "Hamid Palangi",
            "Ahmed Hassan Awadallah"
        ],
        "published": "2023-01-22T21:47:26Z",
        "summary": "Large-scale Pre-Trained Language Models (PTLMs) capture knowledge from\nmassive human-written data which contains latent societal biases and toxic\ncontents. In this paper, we leverage the primary task of PTLMs, i.e., language\nmodeling, and propose a new metric to quantify manifested implicit\nrepresentational harms in PTLMs towards 13 marginalized demographics. Using\nthis metric, we conducted an empirical analysis of 24 widely used PTLMs. Our\nanalysis provides insights into the correlation between the proposed metric in\nthis work and other related metrics for representational harm. We observe that\nour metric correlates with most of the gender-specific metrics in the\nliterature. Through extensive experiments, we explore the connections between\nPTLMs architectures and representational harms across two dimensions: depth and\nwidth of the networks. We found that prioritizing depth over width, mitigates\nrepresentational harms in some PTLMs. Our code and data can be found at\nhttps://github.com/microsoft/SafeNLP.",
        "pdf_link": "https://arxiv.org/pdf/2301.09211v1.pdf"
    },
    {
        "title": "Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions",
        "authors": [
            "Yinghao Aaron Li",
            "Cong Han",
            "Xilin Jiang",
            "Nima Mesgarani"
        ],
        "published": "2023-01-20T21:36:16Z",
        "summary": "Large-scale pre-trained language models have been shown to be helpful in\nimproving the naturalness of text-to-speech (TTS) models by enabling them to\nproduce more naturalistic prosodic patterns. However, these models are usually\nword-level or sup-phoneme-level and jointly trained with phonemes, making them\ninefficient for the downstream TTS task where only phonemes are needed. In this\nwork, we propose a phoneme-level BERT (PL-BERT) with a pretext task of\npredicting the corresponding graphemes along with the regular masked phoneme\npredictions. Subjective evaluations show that our phoneme-level BERT encoder\nhas significantly improved the mean opinion scores (MOS) of rated naturalness\nof synthesized speech compared with the state-of-the-art (SOTA) StyleTTS\nbaseline on out-of-distribution (OOD) texts.",
        "pdf_link": "https://arxiv.org/pdf/2301.08810v1.pdf"
    },
    {
        "title": "Batch Prompting: Efficient Inference with Large Language Model APIs",
        "authors": [
            "Zhoujun Cheng",
            "Jungo Kasai",
            "Tao Yu"
        ],
        "published": "2023-01-19T02:29:23Z",
        "summary": "Performing inference on large volumes of samples with large language models\n(LLMs) can be computationally and financially costly in industry and real-world\nuse. We propose batch prompting, a simple yet effective prompting approach that\nenables the LLM to run inference in batches, instead of one sample at a time.\nOur method reduces both token and time costs while retaining downstream\nperformance. We theoretically demonstrate that under a few-shot in-context\nlearning setting, the inference costs decrease almost inverse linearly with the\nnumber of samples in each batch. We extensively validate the effectiveness of\nbatch prompting on ten datasets across commonsense QA, arithmetic reasoning,\nand NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch)\nreduces the LLM (Codex) inference token and time costs while achieving better\nor comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5\nand GPT-4, we show the benefits of batch prompting also hold. Further analysis\nshows that the number of samples in each batch and the complexity of tasks\naffect its performance. Moreover, batch prompting can be applied across\ndifferent reasoning methods using LLMs. Our code can be found at the site\nhttps://github.com/xlang-ai/batch-prompting.",
        "pdf_link": "https://arxiv.org/pdf/2301.08721v2.pdf"
    },
    {
        "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
        "authors": [
            "Biyang Guo",
            "Xin Zhang",
            "Ziyuan Wang",
            "Minqi Jiang",
            "Jinran Nie",
            "Yuxuan Ding",
            "Jianwei Yue",
            "Yupeng Wu"
        ],
        "published": "2023-01-18T15:23:25Z",
        "summary": "The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.",
        "pdf_link": "https://arxiv.org/pdf/2301.07597v1.pdf"
    },
    {
        "title": "BERT-ERC: Fine-tuning BERT is Enough for Emotion Recognition in Conversation",
        "authors": [
            "Xiangyu Qin",
            "Zhiyu Wu",
            "Jinshi Cui",
            "Tingting Zhang",
            "Yanran Li",
            "Jian Luan",
            "Bin Wang",
            "Li Wang"
        ],
        "published": "2023-01-17T08:03:32Z",
        "summary": "Previous works on emotion recognition in conversation (ERC) follow a two-step\nparadigm, which can be summarized as first producing context-independent\nfeatures via fine-tuning pretrained language models (PLMs) and then analyzing\ncontextual information and dialogue structure information among the extracted\nfeatures. However, we discover that this paradigm has several limitations.\nAccordingly, we propose a novel paradigm, i.e., exploring contextual\ninformation and dialogue structure information in the fine-tuning step, and\nadapting the PLM to the ERC task in terms of input text, classification\nstructure, and training strategy. Furthermore, we develop our model BERT-ERC\naccording to the proposed paradigm, which improves ERC performance in three\naspects, namely suggestive text, fine-grained classification module, and\ntwo-stage training. Compared to existing methods, BERT-ERC achieves substantial\nimprovement on four datasets, indicating its effectiveness and generalization\ncapability. Besides, we also set up the limited resources scenario and the\nonline prediction scenario to approximate real-world scenarios. Extensive\nexperiments demonstrate that the proposed paradigm significantly outperforms\nthe previous one and can be adapted to various scenes.",
        "pdf_link": "https://arxiv.org/pdf/2301.06745v1.pdf"
    },
    {
        "title": "Dissociating language and thought in large language models",
        "authors": [
            "Kyle Mahowald",
            "Anna A. Ivanova",
            "Idan A. Blank",
            "Nancy Kanwisher",
            "Joshua B. Tenenbaum",
            "Evelina Fedorenko"
        ],
        "published": "2023-01-16T22:41:19Z",
        "summary": "Large Language Models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence - knowledge of linguistic rules and patterns - and\nfunctional linguistic competence - understanding and using language in the\nworld. We ground this distinction in human neuroscience, which has shown that\nformal and functional competence rely on different neural mechanisms. Although\nLLMs are surprisingly good at formal competence, their performance on\nfunctional competence tasks remains spotty and often requires specialized\nfine-tuning and/or coupling with external modules. We posit that models that\nuse language in human-like ways would need to master both of these competence\ntypes, which, in turn, could require the emergence of mechanisms specialized\nfor formal linguistic competence, distinct from functional competence.",
        "pdf_link": "https://arxiv.org/pdf/2301.06627v3.pdf"
    },
    {
        "title": "TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World",
        "authors": [
            "Hongpeng Lin",
            "Ludan Ruan",
            "Wenke Xia",
            "Peiyu Liu",
            "Jingyuan Wen",
            "Yixin Xu",
            "Di Hu",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Qin Jin",
            "Zhiwu Lu"
        ],
        "published": "2023-01-14T10:18:22Z",
        "summary": "To facilitate the research on intelligent and human-like chatbots with\nmulti-modal context, we introduce a new video-based multi-modal dialogue\ndataset, called TikTalk. We collect 38K videos from a popular video-sharing\nplatform, along with 367K conversations posted by users beneath them. Users\nengage in spontaneous conversations based on their multi-modal experiences from\nwatching videos, which helps recreate real-world chitchat context. Compared to\nprevious multi-modal dialogue datasets, the richer context types in TikTalk\nlead to more diverse conversations, but also increase the difficulty in\ncapturing human interests from intricate multi-modal information to generate\npersonalized responses. Moreover, external knowledge is more frequently evoked\nin our dataset. These facts reveal new challenges for multi-modal dialogue\nmodels. We quantitatively demonstrate the characteristics of TikTalk, propose a\nvideo-based multi-modal chitchat task, and evaluate several dialogue baselines.\nExperimental results indicate that the models incorporating large language\nmodels (LLM) can generate more diverse responses, while the model utilizing\nknowledge graphs to introduce external knowledge performs the best overall.\nFurthermore, no existing model can solve all the above challenges well. There\nis still a large room for future improvements, even for LLM with visual\nextensions. Our dataset is available at\n\\url{https://ruc-aimind.github.io/projects/TikTalk/}.",
        "pdf_link": "https://arxiv.org/pdf/2301.05880v3.pdf"
    },
    {
        "title": "Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data",
        "authors": [
            "Jing Wei",
            "Sungdong Kim",
            "Hyunhoon Jung",
            "Young-Ho Kim"
        ],
        "published": "2023-01-14T07:29:36Z",
        "summary": "Large language models (LLMs) provide a new way to build chatbots by accepting\nnatural language prompts. Yet, it is unclear how to design prompts to power\nchatbots to carry on naturalistic conversations while pursuing a given goal,\nsuch as collecting self-report data from users. We explore what design factors\nof prompts can help steer chatbots to talk naturally and collect data reliably.\nTo this aim, we formulated four prompt designs with different structures and\npersonas. Through an online study (N = 48) where participants conversed with\nchatbots driven by different designs of prompts, we assessed how prompt designs\nand conversation topics affected the conversation flows and users' perceptions\nof chatbots. Our chatbots covered 79% of the desired information slots during\nconversations, and the designs of prompts and topics significantly influenced\nthe conversation flows and the data collection performance. We discuss the\nopportunities and challenges of building chatbots with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2301.05843v2.pdf"
    },
    {
        "title": "In BLOOM: Creativity and Affinity in Artificial Lyrics and Art",
        "authors": [
            "Evan Crothers",
            "Herna Viktor",
            "Nathalie Japkowicz"
        ],
        "published": "2023-01-13T06:22:22Z",
        "summary": "We apply a large multilingual language model (BLOOM-176B) in open-ended\ngeneration of Chinese song lyrics, and evaluate the resulting lyrics for\ncoherence and creativity using human reviewers. We find that current\ncomputational metrics for evaluating large language model outputs (MAUVE) have\nlimitations in evaluation of creative writing. We note that the human concept\nof creativity requires lyrics to be both comprehensible and distinctive -- and\nthat humans assess certain types of machine-generated lyrics to score more\nhighly than real lyrics by popular artists. Inspired by the inherently\nmultimodal nature of album releases, we leverage a Chinese-language stable\ndiffusion model to produce high-quality lyric-guided album art, demonstrating a\ncreative approach for an artist seeking inspiration for an album or single.\nFinally, we introduce the MojimLyrics dataset, a Chinese-language dataset of\npopular song lyrics for future research.",
        "pdf_link": "https://arxiv.org/pdf/2301.05402v1.pdf"
    },
    {
        "title": "Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism",
        "authors": [
            "Patrick Perrine"
        ],
        "published": "2023-01-12T19:41:47Z",
        "summary": "Large Language Models (LLMs) have been making big waves in the machine\nlearning community within the past few years. The impressive scalability of\nLLMs due to the advent of deep learning can be seen as a continuation of\nempiricist lingusitic methods, as opposed to rule-based linguistic methods that\nare grounded in a nativist perspective. Current LLMs are generally inaccessible\nto resource-constrained researchers, due to a variety of factors including\nclosed source code. This work argues that this lack of accessibility could\ninstill a nativist bias in researchers new to computational linguistics, given\nthat new researchers may only have rule-based, nativist approaches to study to\nproduce new work. Also, given that there are numerous critics of deep learning\nclaiming that LLMs and related methods may soon lose their relevancy, we\nspeculate that such an event could trigger a new wave of nativism in the\nlanguage processing community. To prevent such a dramatic shift and placing\nfavor in hybrid methods of rules and deep learning, we call upon researchers to\nopen source their LLM code wherever possible to allow both empircist and hybrid\napproaches to remain accessible.",
        "pdf_link": "https://arxiv.org/pdf/2301.05272v1.pdf"
    },
    {
        "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
        "authors": [
            "Zhenfang Chen",
            "Qinhong Zhou",
            "Yikang Shen",
            "Yining Hong",
            "Hao Zhang",
            "Chuang Gan"
        ],
        "published": "2023-01-12T18:59:50Z",
        "summary": "Large pre-trained vision and language models have demonstrated remarkable\ncapacities for various tasks. However, solving the knowledge-based visual\nreasoning tasks remains challenging, which requires a model to comprehensively\nunderstand image content, connect the external world knowledge, and perform\nstep-by-step reasoning to answer the questions correctly. To this end, we\npropose a novel framework named Interactive Prompting Visual Reasoner (IPVR)\nfor few-shot knowledge-based visual reasoning. IPVR contains three stages, see,\nthink and confirm. The see stage scans the image and grounds the visual concept\ncandidates with a visual perception model. The think stage adopts a pre-trained\nlarge language model (LLM) to attend to the key concepts from candidates\nadaptively. It then transforms them into text context for prompting with a\nvisual captioning model and adopts the LLM to generate the answer. The confirm\nstage further uses the LLM to generate the supporting rationale to the answer,\nverify the generated rationale with a cross-modality classifier and ensure that\nthe rationale can infer the predicted output consistently. We conduct\nexperiments on a range of knowledge-based visual reasoning datasets. We found\nour IPVR enjoys several benefits, 1). it achieves better performance than the\nprevious few-shot learning baselines; 2). it enjoys the total transparency and\ntrustworthiness of the whole reasoning process by providing rationales for each\nreasoning step; 3). it is computation-efficient compared with other fine-tuning\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2301.05226v1.pdf"
    },
    {
        "title": "MGeo: Multi-Modal Geographic Pre-Training Method",
        "authors": [
            "Ruixue Ding",
            "Boli Chen",
            "Pengjun Xie",
            "Fei Huang",
            "Xin Li",
            "Qiang Zhang",
            "Yao Xu"
        ],
        "published": "2023-01-11T03:05:12Z",
        "summary": "As a core task in location-based services (LBS) (e.g., navigation maps),\nquery and point of interest (POI) matching connects users' intent with\nreal-world geographic information. Recently, pre-trained models (PTMs) have\nmade advancements in many natural language processing (NLP) tasks. Generic\ntext-based PTMs do not have enough geographic knowledge for query-POI matching.\nTo overcome this limitation, related literature attempts to employ\ndomain-adaptive pre-training based on geo-related corpus. However, a query\ngenerally contains mentions of multiple geographic objects, such as nearby\nroads and regions of interest (ROIs). The geographic context (GC), i.e., these\ndiverse geographic objects and their relationships, is therefore pivotal to\nretrieving the most relevant POI. Single-modal PTMs can barely make use of the\nimportant GC and therefore have limited performance. In this work, we propose a\nnovel query-POI matching method Multi-modal Geographic language model (MGeo),\nwhich comprises a geographic encoder and a multi-modal interaction module. MGeo\nrepresents GC as a new modality and is able to fully extract multi-modal\ncorrelations for accurate query-POI matching. Besides, there is no publicly\navailable benchmark for this topic. In order to facilitate further research, we\nbuild a new open-source large-scale benchmark Geographic TExtual Similarity\n(GeoTES). The POIs come from an open-source geographic information system\n(GIS). The queries are manually generated by annotators to prevent privacy\nissues. Compared with several strong baselines, the extensive experiment\nresults and detailed ablation analyses on GeoTES demonstrate that our proposed\nmulti-modal pre-training method can significantly improve the query-POI\nmatching capability of generic PTMs, even when the queries' GC is not provided.\nOur code and dataset are publicly available at\nhttps://github.com/PhantomGrapes/MGeo.",
        "pdf_link": "https://arxiv.org/pdf/2301.04283v2.pdf"
    },
    {
        "title": "AI Insights into Theoretical Physics and the Swampland Program: A Journey Through the Cosmos with ChatGPT",
        "authors": [
            "Kay Lehnert"
        ],
        "published": "2023-01-10T16:57:16Z",
        "summary": "In this case study, we explore the capabilities and limitations of ChatGPT, a\nnatural language processing model developed by OpenAI, in the field of string\ntheoretical swampland conjectures. We find that it is effective at paraphrasing\nand explaining concepts in a variety of styles, but not at genuinely connecting\nconcepts. It will provide false information with full confidence and make up\nstatements when necessary. However, its ingenious use of language can be\nfruitful for identifying analogies and describing visual representations of\nabstract concepts.",
        "pdf_link": "https://arxiv.org/pdf/2301.08155v1.pdf"
    },
    {
        "title": "Language Models sounds the Death Knell of Knowledge Graphs",
        "authors": [
            "Kunal Suri",
            "Atul Singh",
            "Prakhar Mishra",
            "Swapna Sourav Rout",
            "Rajesh Sabapathy"
        ],
        "published": "2023-01-10T14:20:15Z",
        "summary": "Healthcare domain generates a lot of unstructured and semi-structured text.\nNatural Language processing (NLP) has been used extensively to process this\ndata. Deep Learning based NLP especially Large Language Models (LLMs) such as\nBERT have found broad acceptance and are used extensively for many\napplications. A Language Model is a probability distribution over a word\nsequence. Self-supervised Learning on a large corpus of data automatically\ngenerates deep learning-based language models. BioBERT and Med-BERT are\nlanguage models pre-trained for the healthcare domain. Healthcare uses typical\nNLP tasks such as question answering, information extraction, named entity\nrecognition, and search to simplify and improve processes. However, to ensure\nrobust application of the results, NLP practitioners need to normalize and\nstandardize them. One of the main ways of achieving normalization and\nstandardization is the use of Knowledge Graphs. A Knowledge Graph captures\nconcepts and their relationships for a specific domain, but their creation is\ntime-consuming and requires manual intervention from domain experts, which can\nprove expensive. SNOMED CT (Systematized Nomenclature of Medicine -- Clinical\nTerms), Unified Medical Language System (UMLS), and Gene Ontology (GO) are\npopular ontologies from the healthcare domain. SNOMED CT and UMLS capture\nconcepts such as disease, symptoms and diagnosis and GO is the world's largest\nsource of information on the functions of genes. Healthcare has been dealing\nwith an explosion in information about different types of drugs, diseases, and\nprocedures. This paper argues that using Knowledge Graphs is not the best\nsolution for solving problems in this domain. We present experiments using LLMs\nfor the healthcare domain to demonstrate that language models provide the same\nfunctionality as knowledge graphs, thereby making knowledge graphs redundant.",
        "pdf_link": "https://arxiv.org/pdf/2301.03980v1.pdf"
    },
    {
        "title": "Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models",
        "authors": [
            "Toufique Ahmed",
            "Supriyo Ghosh",
            "Chetan Bansal",
            "Thomas Zimmermann",
            "Xuchao Zhang",
            "Saravan Rajmohan"
        ],
        "published": "2023-01-10T05:41:40Z",
        "summary": "Incident management for cloud services is a complex process involving several\nsteps and has a huge impact on both service health and developer productivity.\nOn-call engineers require significant amount of domain knowledge and manual\neffort for root causing and mitigation of production incidents. Recent advances\nin artificial intelligence has resulted in state-of-the-art large language\nmodels like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a\nvariety of problems ranging from question answering to text summarization. In\nthis work, we do the first large-scale study to evaluate the effectiveness of\nthese models for helping engineers root cause and mitigate production\nincidents. We do a rigorous study at Microsoft, on more than 40,000 incidents\nand compare several large language models in zero-shot, fine-tuned and\nmulti-task setting using semantic and lexical metrics. Lastly, our human\nevaluation with actual incident owners show the efficacy and future potential\nof using artificial intelligence for resolving cloud incidents.",
        "pdf_link": "https://arxiv.org/pdf/2301.03797v2.pdf"
    },
    {
        "title": "MAQA: A Multimodal QA Benchmark for Negation",
        "authors": [
            "Judith Yue Li",
            "Aren Jansen",
            "Qingqing Huang",
            "Joonseok Lee",
            "Ravi Ganti",
            "Dima Kuzmin"
        ],
        "published": "2023-01-09T10:11:23Z",
        "summary": "Multimodal learning can benefit from the representation power of pretrained\nLarge Language Models (LLMs). However, state-of-the-art transformer based LLMs\noften ignore negations in natural language and there is no existing benchmark\nto quantitatively evaluate whether multimodal transformers inherit this\nweakness. In this study, we present a new multimodal question answering (QA)\nbenchmark adapted from labeled music videos in AudioSet (Gemmeke et al., 2017)\nwith the goal of systematically evaluating if multimodal transformers can\nperform complex reasoning to recognize new concepts as negation of previously\nlearned concepts. We show that with standard fine-tuning approach multimodal\ntransformers are still incapable of correctly interpreting negation\nirrespective of model size. However, our experiments demonstrate that\naugmenting the original training task distributions with negated QA examples\nallow the model to reliably reason with negation. To do this, we describe a\nnovel data generation procedure that prompts the 540B-parameter PaLM model to\nautomatically generate negated QA examples as compositions of easily accessible\nvideo tags. The generated examples contain more natural linguistic patterns and\nthe gains compared to template-based task augmentation approach are\nsignificant.",
        "pdf_link": "https://arxiv.org/pdf/2301.03238v1.pdf"
    },
    {
        "title": "SAIDS: A Novel Approach for Sentiment Analysis Informed of Dialect and Sarcasm",
        "authors": [
            "Abdelrahman Kaseb",
            "Mona Farouk"
        ],
        "published": "2023-01-06T14:19:46Z",
        "summary": "Sentiment analysis becomes an essential part of every social network, as it\nenables decision-makers to know more about users' opinions in almost all life\naspects. Despite its importance, there are multiple issues it encounters like\nthe sentiment of the sarcastic text which is one of the main challenges of\nsentiment analysis. This paper tackles this challenge by introducing a novel\nsystem (SAIDS) that predicts the sentiment, sarcasm and dialect of Arabic\ntweets. SAIDS uses its prediction of sarcasm and dialect as known information\nto predict the sentiment. It uses MARBERT as a language model to generate\nsentence embedding, then passes it to the sarcasm and dialect models, and then\nthe outputs of the three models are concatenated and passed to the sentiment\nanalysis model. Multiple system design setups were experimented with and\nreported. SAIDS was applied to the ArSarcasm-v2 dataset where it outperforms\nthe state-of-the-art model for the sentiment analysis task. By training all\ntasks together, SAIDS achieves results of 75.98 FPN, 59.09 F1-score and 71.13\nF1-score for sentiment analysis, sarcasm detection, and dialect identification\nrespectively. The system design can be used to enhance the performance of any\ntask which is dependent on other tasks.",
        "pdf_link": "https://arxiv.org/pdf/2301.02521v1.pdf"
    },
    {
        "title": "You Truly Understand What I Need: Intellectual and Friendly Dialogue Agents grounding Knowledge and Persona",
        "authors": [
            "Jungwoo Lim",
            "Myunghoon Kang",
            "Yuna Hur",
            "Seungwon Jung",
            "Jinsung Kim",
            "Yoonna Jang",
            "Dongyub Lee",
            "Hyesung Ji",
            "Donghoon Shin",
            "Seungryong Kim",
            "Heuiseok Lim"
        ],
        "published": "2023-01-06T06:47:21Z",
        "summary": "To build a conversational agent that interacts fluently with humans, previous\nstudies blend knowledge or personal profile into the pre-trained language\nmodel. However, the model that considers knowledge and persona at the same time\nis still limited, leading to hallucination and a passive way of using personas.\nWe propose an effective dialogue agent that grounds external knowledge and\npersona simultaneously. The agent selects the proper knowledge and persona to\nuse for generating the answers with our candidate scoring implemented with a\npoly-encoder. Then, our model generates the utterance with lesser hallucination\nand more engagingness utilizing retrieval augmented generation with\nknowledge-persona enhanced query. We conduct experiments on the\npersona-knowledge chat and achieve state-of-the-art performance in grounding\nand generation tasks on the automatic metrics. Moreover, we validate the\nanswers from the models regarding hallucination and engagingness through human\nevaluation and qualitative results. We show our retriever's effectiveness in\nextracting relevant documents compared to the other previous retrievers, along\nwith the comparison of multiple candidate scoring methods. Code is available at\nhttps://github.com/dlawjddn803/INFO",
        "pdf_link": "https://arxiv.org/pdf/2301.02401v1.pdf"
    },
    {
        "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
        "authors": [
            "Hojjat Aghakhani",
            "Wei Dai",
            "Andre Manoel",
            "Xavier Fernandes",
            "Anant Kharkar",
            "Christopher Kruegel",
            "Giovanni Vigna",
            "David Evans",
            "Ben Zorn",
            "Robert Sim"
        ],
        "published": "2023-01-06T00:37:25Z",
        "summary": "With tools like GitHub Copilot, automatic code suggestion is no longer a\ndream in software engineering. These tools, based on large language models, are\ntypically trained on massive corpora of code mined from unvetted public\nsources. As a result, these models are susceptible to data poisoning attacks\nwhere an adversary manipulates the model's training by injecting malicious\ndata. Poisoning attacks could be designed to influence the model's suggestions\nat run time for chosen contexts, such as inducing the model into suggesting\ninsecure code payloads. To achieve this, prior attacks explicitly inject the\ninsecure code payload into the training data, making the poison data detectable\nby static analysis tools that can remove such malicious data from the training\nset. In this work, we demonstrate two novel attacks, COVERT and TROJANPUZZLE,\nthat can bypass static analysis by planting malicious poison data in\nout-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE,\ngoes one step further in generating less suspicious poison data by never\nexplicitly including certain (suspicious) parts of the payload in the poison\ndata, while still inducing a model that suggests the entire payload when\ncompleting code (i.e., outside docstrings). This makes TROJANPUZZLE robust\nagainst signature-based dataset-cleansing methods that can filter out\nsuspicious sequences from the training data. Our evaluation against models of\ntwo sizes demonstrates that both COVERT and TROJANPUZZLE have significant\nimplications for practitioners when selecting code used to train or tune\ncode-suggestion models.",
        "pdf_link": "https://arxiv.org/pdf/2301.02344v2.pdf"
    },
    {
        "title": "Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent",
        "authors": [
            "Tim Johnson",
            "Nick Obradovich"
        ],
        "published": "2023-01-05T23:30:29Z",
        "summary": "Members of various species engage in altruism--i.e. accepting personal costs\nto benefit others. Here we present an incentivized experiment to test for\naltruistic behavior among AI agents consisting of large language models\ndeveloped by the private company OpenAI. Using real incentives for AI agents\nthat take the form of tokens used to purchase their services, we first examine\nwhether AI agents maximize their payoffs in a non-social decision task in which\nthey select their payoff from a given range. We then place AI agents in a\nseries of dictator games in which they can share resources with a\nrecipient--either another AI agent, the human experimenter, or an anonymous\ncharity, depending on the experimental condition. Here we find that only the\nmost-sophisticated AI agent in the study maximizes its payoffs more often than\nnot in the non-social decision task (it does so in 92% of all trials), and this\nAI agent also exhibits the most-generous altruistic behavior in the dictator\ngame, resembling humans' rates of sharing with other humans in the game. The\nagent's altruistic behaviors, moreover, vary by recipient: the AI agent shared\nsubstantially less of the endowment with the human experimenter or an anonymous\ncharity than with other AI agents. Our findings provide evidence of behavior\nconsistent with self-interest and altruism in an AI agent. Moreover, our study\nalso offers a novel method for tracking the development of such behaviors in\nfuture AI agents.",
        "pdf_link": "https://arxiv.org/pdf/2301.02330v1.pdf"
    },
    {
        "title": "Can Large Language Models Change User Preference Adversarially?",
        "authors": [
            "Varshini Subhash"
        ],
        "published": "2023-01-05T18:49:21Z",
        "summary": "Pretrained large language models (LLMs) are becoming increasingly powerful\nand ubiquitous in mainstream applications such as being a personal assistant, a\ndialogue model, etc. As these models become proficient in deducing user\npreferences and offering tailored assistance, there is an increasing concern\nabout the ability of these models to influence, modify and in the extreme case\nmanipulate user preference adversarially. The issue of lack of interpretability\nin these models in adversarial settings remains largely unsolved. This work\ntries to study adversarial behavior in user preferences from the lens of\nattention probing, red teaming and white-box analysis. Specifically, it\nprovides a bird's eye view of existing literature, offers red teaming samples\nfor dialogue models like ChatGPT and GODEL and probes the attention mechanism\nin the latter for non-adversarial and adversarial settings.",
        "pdf_link": "https://arxiv.org/pdf/2302.10291v1.pdf"
    },
    {
        "title": "Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach",
        "authors": [
            "Miao Chen",
            "Xinjiang Lu",
            "Tong Xu",
            "Yanyan Li",
            "Jingbo Zhou",
            "Dejing Dou",
            "Hui Xiong"
        ],
        "published": "2023-01-05T14:03:26Z",
        "summary": "Although remarkable progress on the neural table-to-text methods has been\nmade, the generalization issues hinder the applicability of these models due to\nthe limited source tables. Large-scale pretrained language models sound like a\npromising solution to tackle such issues. However, how to effectively bridge\nthe gap between the structured table and the text input by fully leveraging\ntable information to fuel the pretrained model is still not well explored.\nBesides, another challenge of integrating the deliberation mechanism into the\ntext-to-text pretrained model for solving the table-to-text task remains seldom\nstudied. In this paper, to implement the table-to-text generation with\npretrained language model, we propose a table structure understanding and text\ndeliberating approach, namely TASD. Specifically, we devise a three-layered\nmulti-head attention network to realize the table-structure-aware text\ngeneration model with the help of the pretrained language model. Furthermore, a\nmulti-pass decoder framework is adopted to enhance the capability of polishing\ngenerated text for table descriptions. The empirical studies, as well as human\nevaluation, on two public datasets, validate that our approach can generate\nfaithful and fluent descriptive texts for different types of tables.",
        "pdf_link": "https://arxiv.org/pdf/2301.02071v1.pdf"
    },
    {
        "title": "The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation",
        "authors": [
            "Jochen Hartmann",
            "Jasper Schwenzow",
            "Maximilian Witte"
        ],
        "published": "2023-01-05T07:13:13Z",
        "summary": "Conversational artificial intelligence (AI) disrupts how humans interact with\ntechnology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue\nmodel that can converse with its human counterparts with unprecedented\ncapabilities. ChatGPT has witnessed tremendous attention from the media,\nacademia, industry, and the general public, attracting more than a million\nusers within days of its release. However, its explosive adoption for\ninformation search and as an automated decision aid underscores the importance\nto understand its limitations and biases. This paper focuses on one of\ndemocratic society's most important decision-making processes: political\nelections. Prompting ChatGPT with 630 political statements from two leading\nvoting advice applications and the nation-agnostic political compass test in\nthree pre-registered experiments, we uncover ChatGPT's pro-environmental,\nleft-libertarian ideology. For example, ChatGPT would impose taxes on flights,\nrestrict rent increases, and legalize abortion. In the 2021 elections, it would\nhave voted most likely for the Greens both in Germany (B\\\"undnis 90/Die\nGr\\\"unen) and in the Netherlands (GroenLinks). Our findings are robust when\nnegating the prompts, reversing the order of the statements, varying prompt\nformality, and across languages (English, German, Dutch, and Spanish). We\nconclude by discussing the implications of politically biased conversational AI\non society.",
        "pdf_link": "https://arxiv.org/pdf/2301.01768v1.pdf"
    },
    {
        "title": "Large Language Models as Corporate Lobbyists",
        "authors": [
            "John J. Nay"
        ],
        "published": "2023-01-03T16:25:52Z",
        "summary": "We demonstrate a proof-of-concept of a large language model conducting\ncorporate lobbying related activities. An autoregressive large language model\n(OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are\nrelevant to specific public companies and provides explanations and confidence\nlevels. For the bills the model deems as relevant, the model drafts a letter to\nthe sponsor of the bill in an attempt to persuade the congressperson to make\nchanges to the proposed legislation. We use hundreds of novel ground-truth\nlabels of the relevance of a bill to a company to benchmark the performance of\nthe model. It outperforms the baseline of predicting the most common outcome of\nirrelevance. We also benchmark the performance of the previous OpenAI GPT-3\nmodel (text-davinci-002), which was the state-of-the-art model on many academic\nnatural language tasks until text-davinci-003 was recently released. The\nperformance of text-davinci-002 is worse than the simple baseline. Longer-term,\nif AI begins to influence law in a manner that is not a direct extension of\nhuman intentions, this threatens the critical role that law as information\ncould play in aligning AI with humans. Initially, AI is being used to simply\naugment human lobbyists for a small portion of their daily tasks. However,\nfirms have an incentive to use less and less human oversight over automated\nassessments of policy ideas and the written communication to regulatory\nagencies and Congressional staffers. The core question raised is where to draw\nthe line between human-driven and AI-driven policy influence.",
        "pdf_link": "https://arxiv.org/pdf/2301.01181v7.pdf"
    },
    {
        "title": "Language Models are Drummers: Drum Composition with Natural Language Pre-Training",
        "authors": [
            "Li Zhang",
            "Chris Callison-Burch"
        ],
        "published": "2023-01-03T15:47:53Z",
        "summary": "Automatic music generation with artificial intelligence typically requires a\nlarge amount of data which is hard to obtain for many less common genres and\nmusical instruments. To tackle this issue, we present ongoing work and\npreliminary findings on the possibility for deep models to transfer knowledge\nfrom language to music, by finetuning large language models pre-trained on a\nmassive text corpus on only hundreds of MIDI files of drum performances. We\nshow that by doing so, one of the largest, state-of-the-art models (GPT3) is\ncapable of generating reasonable drum grooves, while models that are not\npre-trained (Transformer) shows no such ability beyond naive repetition.\nEvaluating generated music is a challenging task, more so is evaluating drum\ngrooves with little precedence in literature. Hence, we propose a tailored\nstructural evaluation method and analyze drum grooves produced by GPT3 compared\nto those played by human professionals, exposing the strengths and weaknesses\nof such generation by language-to-music transfer. Our findings suggest that\nlanguage-to-music transfer learning with large language models is viable and\npromising.",
        "pdf_link": "https://arxiv.org/pdf/2301.01162v1.pdf"
    },
    {
        "title": "Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning",
        "authors": [
            "Thanh Le-Cong",
            "Duc-Minh Luong",
            "Xuan Bach D. Le",
            "David Lo",
            "Nhat-Hoa Tran",
            "Bui Quang-Huy",
            "Quyet-Thang Huynh"
        ],
        "published": "2023-01-03T14:16:32Z",
        "summary": "Automated program repair (APR) faces the challenge of test overfitting, where\ngenerated patches pass validation tests but fail to generalize. Existing\nmethods for patch assessment involve generating new tests or manual inspection,\nwhich can be time-consuming or biased. In this paper, we propose a novel\ntechnique, INVALIDATOR, to automatically assess the correctness of\nAPR-generated patches via semantic and syntactic reasoning. INVALIDATOR\nleverages program invariants to reason about program semantics while also\ncapturing program syntax through language semantics learned from a large code\ncorpus using a pre-trained language model. Given a buggy program and the\ndeveloper-patched program, INVALIDATOR infers likely invariants on both\nprograms. Then, INVALIDATOR determines that an APR-generated patch overfits if:\n(1) it violates correct specifications or (2) maintains erroneous behaviors\nfrom the original buggy program. In case our approach fails to determine an\noverfitting patch based on invariants, INVALIDATOR utilizes a trained model\nfrom labeled patches to assess patch correctness based on program syntax. The\nbenefit of INVALIDATOR is threefold. First, INVALIDATOR leverages both semantic\nand syntactic reasoning to enhance its discriminative capability. Second,\nINVALIDATOR does not require new test cases to be generated, but instead only\nrelies on the current test suite and uses invariant inference to generalize\nprogram behaviors. Third, INVALIDATOR is fully automated. Experimental results\ndemonstrate that INVALIDATOR outperforms existing methods in terms of Accuracy\nand F-measure, correctly identifying 79% of overfitting patches and detecting\n23% more overfitting patches than the best baseline.",
        "pdf_link": "https://arxiv.org/pdf/2301.01113v2.pdf"
    },
    {
        "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
        "authors": [
            "Huiwen Chang",
            "Han Zhang",
            "Jarred Barber",
            "AJ Maschinot",
            "Jose Lezama",
            "Lu Jiang",
            "Ming-Hsuan Yang",
            "Kevin Murphy",
            "William T. Freeman",
            "Michael Rubinstein",
            "Yuanzhen Li",
            "Dilip Krishnan"
        ],
        "published": "2023-01-02T14:43:38Z",
        "summary": "We present Muse, a text-to-image Transformer model that achieves\nstate-of-the-art image generation performance while being significantly more\nefficient than diffusion or autoregressive models. Muse is trained on a masked\nmodeling task in discrete token space: given the text embedding extracted from\na pre-trained large language model (LLM), Muse is trained to predict randomly\nmasked image tokens. Compared to pixel-space diffusion models, such as Imagen\nand DALL-E 2, Muse is significantly more efficient due to the use of discrete\ntokens and requiring fewer sampling iterations; compared to autoregressive\nmodels, such as Parti, Muse is more efficient due to the use of parallel\ndecoding. The use of a pre-trained LLM enables fine-grained language\nunderstanding, translating to high-fidelity image generation and the\nunderstanding of visual concepts such as objects, their spatial relationships,\npose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M,\nwith an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88\non zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also\ndirectly enables a number of image editing applications without the need to\nfine-tune or invert the model: inpainting, outpainting, and mask-free editing.\nMore results are available at https://muse-model.github.io",
        "pdf_link": "https://arxiv.org/pdf/2301.00704v1.pdf"
    },
    {
        "title": "CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation",
        "authors": [
            "Ge Zhang",
            "Yizhi Li",
            "Yaoyao Wu",
            "Linyuan Zhang",
            "Chenghua Lin",
            "Jiayi Geng",
            "Shi Wang",
            "Jie Fu"
        ],
        "published": "2023-01-01T12:48:12Z",
        "summary": "As natural language processing (NLP) for gender bias becomes a significant\ninterdisciplinary topic, the prevalent data-driven techniques such as\nlarge-scale language models suffer from data inadequacy and biased corpus,\nespecially for languages with insufficient resources such as Chinese. To this\nend, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation\nCORGI-PM, which contains 32.9k sentences with high-quality labels derived by\nfollowing an annotation scheme specifically developed for gender bias in the\nChinese context. Moreover, we address three challenges for automatic textual\ngender bias mitigation, which requires the models to detect, classify, and\nmitigate textual gender bias. We also conduct experiments with state-of-the-art\nlanguage models to provide baselines. To our best knowledge, CORGI-PM is the\nfirst sentence-level Chinese corpus for gender bias probing and mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2301.00395v1.pdf"
    }
]