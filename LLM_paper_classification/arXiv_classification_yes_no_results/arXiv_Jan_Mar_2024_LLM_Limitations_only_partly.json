[
    {
        "title": "Conceptual and Unbiased Reasoning in Language Models",
        "authors": [
            "Ben Zhou",
            "Hongming Zhang",
            "Sihao Chen",
            "Dian Yu",
            "Hongwei Wang",
            "Baolin Peng",
            "Dan Roth",
            "Dong Yu"
        ],
        "published": "2024-03-30T00:53:53Z",
        "summary": "Conceptual reasoning, the ability to reason in abstract and high-level\nperspectives, is key to generalization in human cognition. However, limited\nstudy has been done on large language models' capability to perform conceptual\nreasoning. In this work, we bridge this gap and propose a novel\nconceptualization framework that forces models to perform conceptual reasoning\non abstract questions and generate solutions in a verifiable symbolic space.\nUsing this framework as an analytical tool, we show that existing large\nlanguage models fall short on conceptual reasoning, dropping 9% to 28% on\nvarious benchmarks compared to direct inference methods. We then discuss how\nmodels can improve since high-level abstract reasoning is key to unbiased and\ngeneralizable decision-making. We propose two techniques to add trustworthy\ninduction signals by generating familiar questions with similar underlying\nreasoning paths and asking models to perform self-refinement. Experiments show\nthat our proposed techniques improve models' conceptual reasoning performance\nby 8% to 11%, achieving a more robust reasoning system that relies less on\ninductive biases.",
        "pdf_link": "https://arxiv.org/pdf/2404.00205v1.pdf"
    },
    {
        "title": "Large Language Models are Contrastive Reasoners",
        "authors": [
            "Liang Yao"
        ],
        "published": "2024-03-13T03:15:05Z",
        "summary": "Prompting methods play a crucial role in enhancing the capabilities of\npre-trained large language models (LLMs). We explore how contrastive prompting\n(CP) significantly improves the ability of large language models to perform\ncomplex reasoning. We demonstrate that LLMs are decent contrastive reasoners by\nsimply adding \"Let's give a correct and a wrong answer.\" before LLMs provide\nanswers. Experiments on two large language models show that zero-shot\ncontrastive prompting improves performance on a range of arithmetic,\ncommonsense, and symbolic reasoning tasks without any hand-crafted few-shot\nexamples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and\nAQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method\nnot only surpasses zero-shot CoT and few-shot CoT in most arithmetic and\ncommonsense reasoning tasks but also can seamlessly integrate with existing\nprompting methods, resulting in improved or comparable results when compared to\nstate-of-the-art methods. Our code is available at\nhttps://github.com/yao8839836/cp",
        "pdf_link": "https://arxiv.org/pdf/2403.08211v1.pdf"
    },
    {
        "title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA",
        "authors": [
            "Sheng Wang",
            "Boyang Xue",
            "Jiacheng Ye",
            "Jiyue Jiang",
            "Liheng Chen",
            "Lingpeng Kong",
            "Chuan Wu"
        ],
        "published": "2024-02-24T13:39:05Z",
        "summary": "With the rapid scaling of large language models (LLMs), serving numerous\nLoRAs concurrently has become increasingly impractical, leading to unaffordable\ncosts and necessitating more parameter-efficient finetuning methods. In this\nwork, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA),\nan intra-layer sharing mechanism comprising four essential components:\nbroadcast reduction, rotation enhancement, partially-sharing refinement, and\nrectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its\nadvantages, and effectively circumvent the drawbacks of peer parameter-sharing\nmethods with superior model capacity, practical feasibility, and broad\napplicability. Empirical experiments demonstrate the remarkably higher\nparameter efficiency of PRoLoRA in both specific parameter budget and\nperformance target scenarios, and its scalability to larger LLMs. Notably, with\none time less trainable parameters, PRoLoRA still outperforms LoRA on multiple\ninstruction tuning datasets. Subsequently, an ablation study is conducted to\nvalidate the necessity of individual components and highlight the superiority\nof PRoLoRA over three potential variants. Hopefully, the conspicuously higher\nparameter efficiency can establish PRoLoRA as a resource-friendly alternative\nto LoRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.16902v1.pdf"
    },
    {
        "title": "Predict the Next Word: Humans exhibit uncertainty in this task and language models _____",
        "authors": [
            "Evgenia Ilia",
            "Wilker Aziz"
        ],
        "published": "2024-02-27T14:11:32Z",
        "summary": "Language models (LMs) are statistical models trained to assign probability to\nhuman-generated text. As such, it is reasonable to question whether they\napproximate linguistic variability exhibited by humans well. This form of\nstatistical assessment is difficult to perform at the passage level, for it\nrequires acceptability judgements (i.e., human evaluation) or a robust\nautomated proxy (which is non-trivial). At the word level, however, given some\ncontext, samples from an LM can be assessed via exact matching against a\nprerecorded dataset of alternative single-word continuations of the available\ncontext. We exploit this fact and evaluate the LM's ability to reproduce\nvariability that humans (in particular, a population of English speakers)\nexhibit in the 'next word prediction' task. This can be seen as assessing a\nform of calibration, which, in the context of text classification, Baan et al.\n(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and\nChatGPT and find that they exhibit fairly low calibration to human uncertainty.\nWe also verify the failure of expected calibration error (ECE) to reflect this,\nand as such, advise the community against relying on it in this setting.",
        "pdf_link": "https://arxiv.org/pdf/2402.17527v2.pdf"
    },
    {
        "title": "ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent Classifier and Slot Filler",
        "authors": [
            "Paramita Mirza",
            "Viju Sudhi",
            "Soumya Ranjan Sahoo",
            "Sinchana Ramakanth Bhat"
        ],
        "published": "2024-03-26T09:41:21Z",
        "summary": "State-of-the-art intent classification (IC) and slot filling (SF) methods\noften rely on data-intensive deep learning models, limiting their practicality\nfor industry applications. Large language models on the other hand,\nparticularly instruction-tuned models (Instruct-LLMs), exhibit remarkable\nzero-shot performance across various natural language tasks. This study\nevaluates Instruct-LLMs on popular benchmark datasets for IC and SF,\nemphasizing their capacity to learn from fewer examples. We introduce\nILLUMINER, an approach framing IC and SF as language generation tasks for\nInstruct-LLMs, with a more efficient SF-prompting method compared to prior\nwork. A comprehensive comparison with multiple baselines shows that our\napproach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint\nIC+SF method and in-context learning with GPT3.5 (175B), particularly in slot\nfilling by 11.1--32.2 percentage points. Additionally, our in-depth ablation\nstudy demonstrates that parameter-efficient fine-tuning requires less than 6%\nof training data to yield comparable performance with traditional full-weight\nfine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.17536v1.pdf"
    },
    {
        "title": "Knowledge-Centric Templatic Views of Documents",
        "authors": [
            "Isabel Cachola",
            "Silviu Cucerzan",
            "Allen Herring",
            "Vuksan Mijovic",
            "Erik Oveson",
            "Sujay Kumar Jauhar"
        ],
        "published": "2024-01-13T01:22:15Z",
        "summary": "Authors seeking to communicate with broader audiences often compose their\nideas about the same underlying knowledge in different documents and formats --\nfor example, as slide decks, newsletters, reports, brochures, etc. Prior work\nin document generation has generally considered the creation of each separate\nformat to be different a task, developing independent methods for generation\nand evaluation. This approach is suboptimal for the advancement of AI-supported\ncontent authoring from both research and application perspectives because it\nleads to fragmented learning processes, redundancy in models and methods, and\ndisjointed evaluation. Thus, in our work, we consider each of these documents\nto be templatic views of the same underlying knowledge, and we aim to unify the\ngeneration and evaluation of these templatic views of documents. We begin by\nintroducing an LLM-powered method to extract the most important information\nfrom an input document and represent this information in a structured format.\nWe show that this unified representation can be used to generate multiple\ntemplatic views with no supervision and with very little guidance, improving\nover strong baselines. We additionally introduce a unified evaluation method\nthat is template agnostic, and can be adapted to building document generators\nfor heterogeneous downstream applications. Finally, we conduct a human\nevaluation, which shows that humans prefer 82% of the downstream documents\ngenerated with our method. Furthermore, the newly proposed evaluation metric\ncorrelates more highly with human judgement than prior metrics, while providing\na unified evaluation method.",
        "pdf_link": "https://arxiv.org/pdf/2401.06945v1.pdf"
    },
    {
        "title": "Language-guided Skill Learning with Temporal Variational Inference",
        "authors": [
            "Haotian Fu",
            "Pratyusha Sharma",
            "Elias Stengel-Eskin",
            "George Konidaris",
            "Nicolas Le Roux",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Xingdi Yuan"
        ],
        "published": "2024-02-26T07:19:23Z",
        "summary": "We present an algorithm for skill discovery from expert demonstrations. The\nalgorithm first utilizes Large Language Models (LLMs) to propose an initial\nsegmentation of the trajectories. Following that, a hierarchical variational\ninference framework incorporates the LLM-generated segmentation information to\ndiscover reusable skills by merging trajectory segments. To further control the\ntrade-off between compression and reusability, we introduce a novel auxiliary\nobjective based on the Minimum Description Length principle that helps guide\nthis skill discovery process. Our results demonstrate that agents equipped with\nour method are able to discover skills that help accelerate learning and\noutperform baseline skill learning approaches on new long-horizon tasks in\nBabyAI, a grid world navigation environment, as well as ALFRED, a household\nsimulation environment.",
        "pdf_link": "https://arxiv.org/pdf/2402.16354v1.pdf"
    },
    {
        "title": "Are Large Language Models Table-based Fact-Checkers?",
        "authors": [
            "Hangwen Zhang",
            "Qingyi Si",
            "Peng Fu",
            "Zheng Lin",
            "Weiping Wang"
        ],
        "published": "2024-02-04T15:52:59Z",
        "summary": "Table-based Fact Verification (TFV) aims to extract the entailment relation\nbetween statements and structured tables. Existing TFV methods based on\nsmall-scaled models suffer from insufficient labeled data and weak zero-shot\nability. Recently, the appearance of Large Language Models (LLMs) has gained\nlots of attraction in research fields. They have shown powerful zero-shot and\nin-context learning abilities on several NLP tasks, but their potential on TFV\nis still unknown. In this work, we implement a preliminary study about whether\nLLMs are table-based fact-checkers. In detail, we design diverse prompts to\nexplore how the in-context learning can help LLMs in TFV, i.e., zero-shot and\nfew-shot TFV capability. Besides, we carefully design and construct TFV\ninstructions to study the performance gain brought by the instruction tuning of\nLLMs. Experimental results demonstrate that LLMs can achieve acceptable results\non zero-shot and few-shot TFV with prompt engineering, while instruction-tuning\ncan stimulate the TFV capability significantly. We also make some valuable\nfindings about the format of zero-shot prompts and the number of in-context\nexamples. Finally, we analyze some possible directions to promote the accuracy\nof TFV via LLMs, which is beneficial to further research of table reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.02549v1.pdf"
    },
    {
        "title": "MobilityGPT: Enhanced Human Mobility Modeling with a GPT model",
        "authors": [
            "Ammar Haydari",
            "Dongjie Chen",
            "Zhengfeng Lai",
            "Chen-Nee Chuah"
        ],
        "published": "2024-02-05T18:22:21Z",
        "summary": "Generative models have shown promising results in capturing human mobility\ncharacteristics and generating synthetic trajectories. However, it remains\nchallenging to ensure that the generated geospatial mobility data is\nsemantically realistic, including consistent location sequences, and reflects\nreal-world characteristics, such as constraining on geospatial limits. To\naddress these issues, we reformat human mobility modeling as an autoregressive\ngeneration task, leveraging Generative Pre-trained Transformer (GPT). To ensure\nits controllable generation to alleviate the above challenges, we propose a\ngeospatially-aware generative model, MobilityGPT. We propose a gravity-based\nsampling method to train a transformer for semantic sequence similarity. Then,\nwe constrained the training process via a road connectivity matrix that\nprovides the connectivity of sequences in trajectory generation, thereby\nkeeping generated trajectories in geospatial limits. Lastly, we constructed a\nReinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel\ndistance between training and the synthetically generated trajectories. Our\nexperiments on real-world datasets demonstrate that MobilityGPT outperforms\nstate-of-the-art methods in generating high-quality mobility trajectories that\nare closest to real data in terms of origin-destination similarity, trip\nlength, travel radius, link, and gravity distributions.",
        "pdf_link": "https://arxiv.org/pdf/2402.03264v1.pdf"
    },
    {
        "title": "From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?",
        "authors": [
            "Guangming Huang",
            "Yunfei Long",
            "Yingya Li",
            "Giorgos Papanastasiou"
        ],
        "published": "2024-03-18T15:53:33Z",
        "summary": "Deep learning (DL) has substantially enhanced healthcare research by\naddressing various natural language processing (NLP) tasks. Yet, the increasing\ncomplexity of DL-based NLP methods necessitates transparent model\ninterpretability, or at least explainability, for reliable decision-making.\nThis work presents a thorough scoping review on explainable and interpretable\nDL in healthcare NLP. The term \"XIAI\" (eXplainable and Interpretable Artificial\nIntelligence) was introduced to distinguish XAI from IAI. Methods were further\ncategorized based on their functionality (model-, input-, output-based) and\nscope (local, global). Our analysis shows that attention mechanisms were the\nmost dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The\nmajor challenges identified are that most XIAI do not explore \"global\" modeling\nprocesses, the lack of best practices, and the unmet need for systematic\nevaluation and benchmarks. Important opportunities were raised such as using\n\"attention\" to enhance multi-modal XIAI for personalized medicine and combine\nDL with causal reasoning. Our discussion encourages the integration of XIAI in\nLLMs and domain-specific smaller models. Our review can stimulate further\nresearch and benchmarks toward improving inherent IAI and engaging complex NLP\nin healthcare.",
        "pdf_link": "https://arxiv.org/pdf/2403.11894v1.pdf"
    },
    {
        "title": "Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions",
        "authors": [
            "Oindrila Saha",
            "Grant Van Horn",
            "Subhransu Maji"
        ],
        "published": "2024-01-04T08:39:13Z",
        "summary": "The zero-shot performance of existing vision-language models (VLMs) such as\nCLIP is limited by the availability of large-scale, aligned image and text\ndatasets in specific domains. In this work, we leverage two complementary\nsources of information -- descriptions of categories generated by large\nlanguage models (LLMs) and abundant, fine-grained image classification datasets\n-- to improve the zero-shot classification performance of VLMs across\nfine-grained domains. On the technical side, we develop methods to train VLMs\nwith this \"bag-level\" image-text supervision. We find that simply using these\nattributes at test-time does not improve performance, but our training\nstrategy, for example, on the iNaturalist dataset, leads to an average\nimprovement of 4-5% in zero-shot classification accuracy for novel categories\nof birds and flowers. Similar improvements are observed in domains where a\nsubset of the categories was used to fine-tune the model. By prompting LLMs in\nvarious ways, we generate descriptions that capture visual appearance, habitat,\nand geographic regions and pair them with existing attributes such as the\ntaxonomic structure of the categories. We systematically evaluate their ability\nto improve zero-shot categorization in natural domains. Our findings suggest\nthat geographic priors can be just as effective and are complementary to visual\nappearance. Our method also outperforms prior work on prompt-based tuning of\nVLMs. We release the benchmark, consisting of 14 datasets at\nhttps://github.com/cvl-umass/AdaptCLIPZS , which will contribute to future\nresearch in zero-shot recognition.",
        "pdf_link": "https://arxiv.org/pdf/2401.02460v2.pdf"
    },
    {
        "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
        "authors": [
            "Yifan Wang",
            "Yafei Liu",
            "Chufan Shi",
            "Haoling Li",
            "Chen Chen",
            "Haonan Lu",
            "Yujiu Yang"
        ],
        "published": "2024-03-18T03:10:36Z",
        "summary": "Instruction tuning effectively optimizes Large Language Models (LLMs) for\ndownstream tasks. Due to the changing environment in real-life applications,\nLLMs necessitate continual task-specific adaptation without catastrophic\nforgetting. Considering the heavy computational cost, replay-based Continual\nLearning (CL) methods are the simplest and most widely used for LLMs to address\nthe forgetting issue. However, traditional replay-based methods do not fully\nutilize instructions to customize the replay strategy. In this work, we propose\na novel paradigm called Instruction-based Continual Learning (InsCL). InsCL\ndynamically replays previous data based on task similarity, calculated by\nWasserstein Distance with instructions. Moreover, we further introduce an\nInstruction Information Metric (InsInfo) to quantify the complexity and\ndiversity of instructions. According to InsInfo, InsCL guides the replay\nprocess more inclined to high-quality data. We conduct extensive experiments\nover 16 tasks with different training orders, observing consistent performance\nimprovements of InsCL. When all tasks have been trained, InsCL achieves\nperformance gains of 3.0 Relative Gain compared with Random Replay, and 27.96\nRelative Gain compared with No Replay.",
        "pdf_link": "https://arxiv.org/pdf/2403.11435v1.pdf"
    },
    {
        "title": "LLM-based agents for automating the enhancement of user story quality: An early report",
        "authors": [
            "Zheying Zhang",
            "Maruf Rayhan",
            "Tomas Herda",
            "Manuel Goisauf",
            "Pekka Abrahamsson"
        ],
        "published": "2024-03-14T14:35:53Z",
        "summary": "In agile software development, maintaining high-quality user stories is\ncrucial, but also challenging. This study explores the use of large language\nmodels to automatically improve the user story quality in Austrian Post Group\nIT agile teams. We developed a reference model for an Autonomous LLM-based\nAgent System and implemented it at the company. The quality of user stories in\nthe study and the effectiveness of these agents for user story quality\nimprovement was assessed by 11 participants across six agile teams. Our\nfindings demonstrate the potential of LLMs in improving user story quality,\ncontributing to the research on AI role in agile development, and providing a\npractical example of the transformative impact of AI in an industry setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.09442v1.pdf"
    },
    {
        "title": "Safe Task Planning for Language-Instructed Multi-Robot Systems using Conformal Prediction",
        "authors": [
            "Jun Wang",
            "Guocheng He",
            "Yiannis Kantaros"
        ],
        "published": "2024-02-23T15:02:44Z",
        "summary": "This paper addresses task planning problems for language-instructed robot\nteams. Tasks are expressed in natural language (NL), requiring the robots to\napply their capabilities (e.g., mobility, manipulation, and sensing) at various\nlocations and semantic objects. Several recent works have addressed similar\nplanning problems by leveraging pre-trained Large Language Models (LLMs) to\ndesign effective multi-robot plans. However, these approaches lack mission\nperformance and safety guarantees. To address this challenge, we introduce a\nnew decentralized LLM-based planner that is capable of achieving high mission\nsuccess rates. This is accomplished by leveraging conformal prediction (CP), a\ndistribution-free uncertainty quantification tool in black-box models. CP\nallows the proposed multi-robot planner to reason about its inherent\nuncertainty in a decentralized fashion, enabling robots to make individual\ndecisions when they are sufficiently certain and seek help otherwise. We show,\nboth theoretically and empirically, that the proposed planner can achieve\nuser-specified task success rates while minimizing the overall number of help\nrequests. We demonstrate the performance of our approach on multi-robot home\nservice applications. We also show through comparative experiments, that our\nmethod outperforms recent centralized and decentralized multi-robot LLM-based\nplanners in terms of in terms of its ability to design correct plans. The\nadvantage of our algorithm over baselines becomes more pronounced with\nincreasing mission complexity and robot team size.",
        "pdf_link": "https://arxiv.org/pdf/2402.15368v1.pdf"
    },
    {
        "title": "H2RSVLM: Towards Helpful and Honest Remote Sensing Large Vision Language Model",
        "authors": [
            "Chao Pang",
            "Jiang Wu",
            "Jiayu Li",
            "Yi Liu",
            "Jiaxing Sun",
            "Weijia Li",
            "Xingxing Weng",
            "Shuai Wang",
            "Litong Feng",
            "Gui-Song Xia",
            "Conghui He"
        ],
        "published": "2024-03-29T14:50:43Z",
        "summary": "The generic large Vision-Language Models (VLMs) is rapidly developing, but\nstill perform poorly in Remote Sensing (RS) domain, which is due to the unique\nand specialized nature of RS imagery and the comparatively limited spatial\nperception of current VLMs. Existing Remote Sensing specific Vision Language\nModels (RSVLMs) still have considerable potential for improvement, primarily\nowing to the lack of large-scale, high-quality RS vision-language datasets. We\nconstructed HqDC-1.4M, the large scale High quality and Detailed Captions for\nRS images, containing 1.4 million image-caption pairs, which not only enhance\nthe RSVLM's understanding of RS images but also significantly improve the\nmodel's spatial perception abilities, such as localization and counting,\nthereby increasing the helpfulness of the RSVLM. Moreover, to address the\ninevitable \"hallucination\" problem in RSVLM, we developed RSSA, the first\ndataset aimed at enhancing the Self-Awareness capability of RSVLMs. By\nincorporating a variety of unanswerable questions into typical RS visual\nquestion-answering tasks, RSSA effectively improves the truthfulness and\nreduces the hallucinations of the model's outputs, thereby enhancing the\nhonesty of the RSVLM. Based on these datasets, we proposed the H2RSVLM, the\nHelpful and Honest Remote Sensing Vision Language Model. H2RSVLM has achieved\noutstanding performance on multiple RS public datasets and is capable of\nrecognizing and refusing to answer the unanswerable questions, effectively\nmitigating the incorrect generations. We will release the code, data and model\nweights at https://github.com/opendatalab/H2RSVLM .",
        "pdf_link": "https://arxiv.org/pdf/2403.20213v1.pdf"
    },
    {
        "title": "Automatic Question-Answer Generation for Long-Tail Knowledge",
        "authors": [
            "Rohan Kumar",
            "Youngmin Kim",
            "Sunitha Ravi",
            "Haitian Sun",
            "Christos Faloutsos",
            "Ruslan Salakhutdinov",
            "Minji Yoon"
        ],
        "published": "2024-03-03T03:06:31Z",
        "summary": "Pretrained Large Language Models (LLMs) have gained significant attention for\naddressing open-domain Question Answering (QA). While they exhibit high\naccuracy in answering questions related to common knowledge, LLMs encounter\ndifficulties in learning about uncommon long-tail knowledge (tail entities).\nSince manually constructing QA datasets demands substantial human resources,\nthe types of existing QA datasets are limited, leaving us with a scarcity of\ndatasets to study the performance of LLMs on tail entities. In this paper, we\npropose an automatic approach to generate specialized QA datasets for tail\nentities and present the associated research challenges. We conduct extensive\nexperiments by employing pretrained LLMs on our newly generated long-tail QA\ndatasets, comparing their performance with and without external resources\nincluding Wikipedia and Wikidata knowledge graphs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01382v1.pdf"
    },
    {
        "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
        "authors": [
            "Zhangyang Gao",
            "Cheng Tan",
            "Jue Wang",
            "Yufei Huang",
            "Lirong Wu",
            "Stan Z. Li"
        ],
        "published": "2024-02-04T12:18:51Z",
        "summary": "Is there a foreign language describing protein sequences and structures\nsimultaneously? Protein structures, represented by continuous 3D points, have\nlong posed a challenge due to the contrasting modeling paradigms of discrete\nsequences. We introduce \\textbf{FoldTokenizer} to represent protein\nsequence-structure as discrete symbols. This innovative approach involves\nprojecting residue types and structures into a discrete space, guided by a\nreconstruction loss for information preservation. We refer to the learned\ndiscrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves\nas a new protein language, transforming the protein sequence-structure into a\nunified modality. We apply the created protein language on general backbone\ninpainting and antibody design tasks, building the first GPT-style model\n(\\textbf{FoldGPT}) for sequence-structure co-generation with promising results.\nKey to our success is the substantial enhancement of the vector quantization\nmodule, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
        "pdf_link": "https://arxiv.org/pdf/2403.09673v2.pdf"
    },
    {
        "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads",
        "authors": [
            "Tianle Cai",
            "Yuhong Li",
            "Zhengyang Geng",
            "Hongwu Peng",
            "Jason D. Lee",
            "Deming Chen",
            "Tri Dao"
        ],
        "published": "2024-01-19T15:48:40Z",
        "summary": "The inference process in Large Language Models (LLMs) is often limited due to\nthe absence of parallelism in the auto-regressive decoding process, resulting\nin most operations being restricted by the memory bandwidth of accelerators.\nWhile methods such as speculative decoding have been suggested to address this\nissue, their implementation is impeded by the challenges associated with\nacquiring and maintaining a separate draft model. In this paper, we present\nMedusa, an efficient method that augments LLM inference by adding extra\ndecoding heads to predict multiple subsequent tokens in parallel. Using a\ntree-based attention mechanism, Medusa constructs multiple candidate\ncontinuations and verifies them simultaneously in each decoding step. By\nleveraging parallel processing, Medusa introduces only minimal overhead in\nterms of single-step latency while substantially reducing the number of\ndecoding steps required.\n  We present two levels of fine-tuning procedures for Medusa to meet the needs\nof different use cases: Medusa-1: Medusa is directly fine-tuned on top of a\nfrozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa\nis fine-tuned together with the backbone LLM, enabling better prediction\naccuracy of Medusa heads and higher speedup but needing a special training\nrecipe that preserves the backbone model's capabilities.\n  Moreover, we propose several extensions that improve or expand the utility of\nMedusa, including a self-distillation to handle situations where no training\ndata is available and a typical acceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate Medusa on models of various\nsizes and training procedures. Our experiments demonstrate that Medusa-1 can\nachieve over 2.2x speedup without compromising generation quality, while\nMedusa-2 further improves the speedup to 2.3-3.6x.",
        "pdf_link": "https://arxiv.org/pdf/2401.10774v1.pdf"
    },
    {
        "title": "A Study on Large Language Models' Limitations in Multiple-Choice Question Answering",
        "authors": [
            "Aisha Khatun",
            "Daniel G. Brown"
        ],
        "published": "2024-01-15T20:42:16Z",
        "summary": "The widespread adoption of Large Language Models (LLMs) has become\ncommonplace, particularly with the emergence of open-source models. More\nimportantly, smaller models are well-suited for integration into consumer\ndevices and are frequently employed either as standalone solutions or as\nsubroutines in various AI tasks. Despite their ubiquitous use, there is no\nsystematic analysis of their specific capabilities and limitations. In this\nstudy, we tackle one of the most widely used tasks - answering Multiple Choice\nQuestion (MCQ). We analyze 26 small open-source models and find that 65% of the\nmodels do not understand the task, only 4 models properly select an answer from\nthe given choices, and only 5 of these models are choice order independent.\nThese results are rather alarming given the extensive use of MCQ tests with\nthese models. We recommend exercising caution and testing task understanding\nbefore using MCQ to evaluate LLMs in any field whatsoever.",
        "pdf_link": "https://arxiv.org/pdf/2401.07955v1.pdf"
    },
    {
        "title": "Retrieval is Accurate Generation",
        "authors": [
            "Bowen Cao",
            "Deng Cai",
            "Leyang Cui",
            "Xuxin Cheng",
            "Wei Bi",
            "Yuexian Zou",
            "Shuming Shi"
        ],
        "published": "2024-02-27T14:16:19Z",
        "summary": "Standard language models generate text by selecting tokens from a fixed,\nfinite, and standalone vocabulary. We introduce a novel method that selects\ncontext-aware phrases from a collection of supporting documents. One of the\nmost significant challenges for this paradigm shift is determining the training\noracles, because a string of text can be segmented in various ways and each\nsegment can be retrieved from numerous possible documents. To address this, we\npropose to initialize the training oracles using linguistic heuristics and,\nmore importantly, bootstrap the oracles through iterative self-reinforcement.\nExtensive experiments show that our model not only outperforms standard\nlanguage models on a variety of knowledge-intensive tasks but also demonstrates\nimproved generation quality in open-ended text generation. For instance,\ncompared to the standard language model counterpart, our model raises the\naccuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from\n42.61% to 81.58% in open-ended text generation. Remarkably, our model also\nachieves the best performance and the lowest latency among several\nretrieval-augmented baselines. In conclusion, we assert that retrieval is more\naccurate generation and hope that our work will encourage further research on\nthis new paradigm shift.",
        "pdf_link": "https://arxiv.org/pdf/2402.17532v3.pdf"
    },
    {
        "title": "TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview",
        "authors": [
            "Mohammad Aliannejadi",
            "Zahra Abbasiantaeb",
            "Shubham Chatterjee",
            "Jeffery Dalton",
            "Leif Azzopardi"
        ],
        "published": "2024-01-02T18:40:03Z",
        "summary": "Conversational Information Seeking has evolved rapidly in the last few years\nwith the development of Large Language Models providing the basis for\ninterpreting and responding in a naturalistic manner to user requests. iKAT\nemphasizes the creation and research of conversational search agents that adapt\nresponses based on the user's prior interactions and present context. This\nmeans that the same question might yield varied answers, contingent on the\nuser's profile and preferences. The challenge lies in enabling Conversational\nSearch Agents (CSA) to incorporate personalized context to effectively guide\nusers through the relevant information to them. iKAT's first year attracted\nseven teams and a total of 24 runs. Most of the runs leveraged Large Language\nModels (LLMs) in their pipelines, with a few focusing on a\ngenerate-then-retrieve approach.",
        "pdf_link": "https://arxiv.org/pdf/2401.01330v2.pdf"
    },
    {
        "title": "Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization",
        "authors": [
            "Jianfei Xiao",
            "Yancan Chen",
            "Yimin Ou",
            "Hanyi Yu",
            "Kai Shu",
            "Yiyong Xiao"
        ],
        "published": "2024-01-27T20:20:39Z",
        "summary": "Large language models (LLMs) like Llama, Baichuan and Bloom models show\nremarkable ability with instruction fine-tuning in many natural language tasks.\nNevertheless, for the dialogue summarization task, which aims to generate\nsummaries for different roles in dialogue, most of the state-of-the-art methods\nconduct on small models (e.g Bart and Bert). Existing methods try to add task\nspecified optimization on small models like adding global-local centrality\nscore to models. In this paper, we propose an instruction fine-tuning model:\nBaichuan2-Sum, for role-oriented diaglouge summarization. By setting different\ninstructions for different roles, the model can learn from the dialogue\ninteractions and output the expected summaries. Furthermore, we applied NEFTune\ntechnique to add suitable noise during training to improve the results. The\nexperiments demonstrate that the proposed model achieves the new\nstate-of-the-art results on two public dialogue summarization datasets: CSDS\nand SAMSUM. We release our model and related codes to facilitate future studies\non dialogue summarization task.",
        "pdf_link": "https://arxiv.org/pdf/2401.15496v3.pdf"
    },
    {
        "title": "Fast and Optimal Weight Update for Pruned Large Language Models",
        "authors": [
            "Vladim\u00edr Bo\u017ea"
        ],
        "published": "2024-01-01T23:10:23Z",
        "summary": "Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and optimal weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nCoupled with a simple iterative pruning mask selection, our algorithm achieves\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.",
        "pdf_link": "https://arxiv.org/pdf/2401.02938v1.pdf"
    },
    {
        "title": "MedAide: Leveraging Large Language Models for On-Premise Medical Assistance on Edge Devices",
        "authors": [
            "Abdul Basit",
            "Khizar Hussain",
            "Muhammad Abdullah Hanif",
            "Muhammad Shafique"
        ],
        "published": "2024-02-28T08:30:49Z",
        "summary": "Large language models (LLMs) are revolutionizing various domains with their\nremarkable natural language processing (NLP) abilities. However, deploying LLMs\nin resource-constrained edge computing and embedded systems presents\nsignificant challenges. Another challenge lies in delivering medical assistance\nin remote areas with limited healthcare facilities and infrastructure. To\naddress this, we introduce MedAide, an on-premise healthcare chatbot. It\nleverages tiny-LLMs integrated with LangChain, providing efficient edge-based\npreliminary medical diagnostics and support. MedAide employs model\noptimizations for minimal memory footprint and latency on embedded edge devices\nwithout server infrastructure. The training process is optimized using low-rank\nadaptation (LoRA). Additionally, the model is trained on diverse medical\ndatasets, employing reinforcement learning from human feedback (RLHF) to\nenhance its domain-specific capabilities. The system is implemented on various\nconsumer GPUs and Nvidia Jetson development board. MedAide achieves 77\\%\naccuracy in medical consultations and scores 56 in USMLE benchmark, enabling an\nenergy-efficient healthcare assistance platform that alleviates privacy\nconcerns due to edge-based deployment, thereby empowering the community.",
        "pdf_link": "https://arxiv.org/pdf/2403.00830v1.pdf"
    },
    {
        "title": "CABINET: Content Relevance based Noise Reduction for Table Question Answering",
        "authors": [
            "Sohan Patnaik",
            "Heril Changwal",
            "Milan Aggarwal",
            "Sumit Bhatia",
            "Yaman Kumar",
            "Balaji Krishnamurthy"
        ],
        "published": "2024-02-02T05:48:39Z",
        "summary": "Table understanding capability of Large Language Models (LLMs) has been\nextensively studied through the task of question-answering (QA) over tables.\nTypically, only a small part of the whole table is relevant to derive the\nanswer for a given question. The irrelevant parts act as noise and are\ndistracting information, resulting in sub-optimal performance due to the\nvulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content\nRelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to\nenable LLMs to focus on relevant tabular data by suppressing extraneous\ninformation. CABINET comprises an Unsupervised Relevance Scorer (URS), trained\ndifferentially with the QA LLM, that weighs the table content based on its\nrelevance to the input question before feeding it to the question-answering LLM\n(QA LLM). To further aid the relevance scorer, CABINET employs a weakly\nsupervised module that generates a parsing statement describing the criteria of\nrows and columns relevant to the question and highlights the content of\ncorresponding table cells. CABINET significantly outperforms various tabular\nLLM baselines, as well as GPT3-based in-context learning methods, is more\nrobust to noise, maintains outperformance on tables of varying sizes, and\nestablishes new SoTA performance on WikiTQ, FeTaQA, and WikiSQL datasets. We\nrelease our code and datasets at https://github.com/Sohanpatnaik106/CABINET_QA.",
        "pdf_link": "https://arxiv.org/pdf/2402.01155v3.pdf"
    },
    {
        "title": "TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation",
        "authors": [
            "Dingbang Li",
            "Wenzhou Chen",
            "Xin Lin"
        ],
        "published": "2024-03-13T05:22:39Z",
        "summary": "Zero-shot navigation is a critical challenge in Vision-Language Navigation\n(VLN) tasks, where the ability to adapt to unfamiliar instructions and to act\nin unknown environments is essential. Existing supervised learning-based\nmodels, trained using annotated data through reinforcement learning, exhibit\nlimitations in generalization capabilities. Large Language Models (LLMs), with\ntheir extensive knowledge and emergent reasoning abilities, present a potential\npathway for achieving zero-shot navigation. This paper presents a VLN agent\nbased on LLMs, exploring approaches to the zero-shot navigation problem. To\ncompensate for the shortcomings of LLMs in environmental perception, we propose\nthe Thinking, Interacting, and Action (TINA) framework. TINA enables the agent\nto scrutinize perceptual information and autonomously query key clues within\nthe environment through an introduced question-answering module, thereby\naligning instructions with specific perceptual data. The navigation agent's\nperceptual abilities are enhanced through the TINA framework, while the\nexplicit thought and query processes also improve the navigational procedure's\nexplainability and transparency. We evaluate the performance of our method on\nthe Room-to-Room dataset. The experiment results indicate that our approach\nimproves the navigation performance of LLM-based agents. Our approach also\noutperformed some supervised learning-based methods, highlighting its efficacy\nin zero-shot navigation.",
        "pdf_link": "https://arxiv.org/pdf/2403.08833v1.pdf"
    },
    {
        "title": "CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling",
        "authors": [
            "Jawook Gu",
            "Han-Cheol Cho",
            "Jiho Kim",
            "Kihyun You",
            "Eun Kyoung Hong",
            "Byungseok Roh"
        ],
        "published": "2024-01-21T14:30:20Z",
        "summary": "Free-text radiology reports present a rich data source for various medical\ntasks, but effectively labeling these texts remains challenging. Traditional\nrule-based labeling methods fall short of capturing the nuances of diverse\nfree-text patterns. Moreover, models using expert-annotated data are limited by\ndata scarcity and pre-defined classes, impacting their performance, flexibility\nand scalability. To address these issues, our study offers three main\ncontributions: 1) We demonstrate the potential of GPT as an adept labeler using\ncarefully designed prompts. 2) Utilizing only the data labeled by GPT, we\ntrained a BERT-based labeler, CheX-GPT, which operates faster and more\nefficiently than its GPT counterpart. 3) To benchmark labeler performance, we\nintroduced a publicly available expert-annotated test set, MIMIC-500,\ncomprising 500 cases from the MIMIC validation set. Our findings demonstrate\nthat CheX-GPT not only excels in labeling accuracy over existing models, but\nalso showcases superior efficiency, flexibility, and scalability, supported by\nour introduction of the MIMIC-500 dataset for robust benchmarking. Code and\nmodels are available at https://github.com/kakaobrain/CheXGPT.",
        "pdf_link": "https://arxiv.org/pdf/2401.11505v1.pdf"
    },
    {
        "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
        "authors": [
            "Arka Pal",
            "Deep Karkhanis",
            "Samuel Dooley",
            "Manley Roberts",
            "Siddartha Naidu",
            "Colin White"
        ],
        "published": "2024-02-20T18:42:34Z",
        "summary": "Direct Preference Optimisation (DPO) is effective at significantly improving\nthe performance of large language models (LLMs) on downstream tasks such as\nreasoning, summarisation, and alignment. Using pairs of preferred and\ndispreferred data, DPO models the \\textit{relative} probability of picking one\nresponse over another. In this work, first we show theoretically that the\nstandard DPO loss can lead to a \\textit{reduction} of the model's likelihood of\nthe preferred examples, as long as the relative probability between the\npreferred and dispreferred classes increases. We then show empirically that\nthis phenomenon occurs when fine-tuning LLMs on common datasets, especially\ndatasets in which the edit distance between pairs of completions is low. Using\nthese insights, we design DPO-Positive (DPOP), a new loss function and training\nprocedure which avoids this failure mode. Surprisingly, we also find that DPOP\nsignificantly outperforms DPO across a wide variety of datasets and downstream\ntasks, including datasets with high edit distances between completions. By\nfine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which\nachieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly\n2\\% better than any other open-source model on the HuggingFace Open LLM\nLeaderboard and becomes the first open-source LLM to surpass an average\naccuracy of 80\\%.",
        "pdf_link": "https://arxiv.org/pdf/2402.13228v1.pdf"
    },
    {
        "title": "ITCMA: A Generative Agent Based on a Computational Consciousness Structure",
        "authors": [
            "Hanzhong Zhang",
            "Jibin Yin",
            "Haoyang Wang",
            "Ziwei Xiang"
        ],
        "published": "2024-03-29T10:23:18Z",
        "summary": "Large Language Models (LLMs) still face challenges in tasks requiring\nunderstanding implicit instructions and applying common-sense knowledge. In\nsuch scenarios, LLMs may require multiple attempts to achieve human-level\nperformance, potentially leading to inaccurate responses or inferences in\npractical environments, affecting their long-term consistency and behavior.\nThis paper introduces the Internal Time-Consciousness Machine (ITCM), a\ncomputational consciousness structure. We further propose the ITCM-based Agent\n(ITCMA), which supports behavior generation and reasoning in open-world\nsettings. ITCMA enhances LLMs' ability to understand implicit instructions and\napply common-sense knowledge by considering agents' interaction and reasoning\nwith the environment. Evaluations in the Alfworld environment show that trained\nITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even\nuntrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher\nthan SOTA, indicating its superiority over traditional intelligent agents in\nutility and generalization. In real-world tasks with quadruped robots, the\nuntrained ITCMA achieves an 85% task completion rate, which is close to its\nperformance in the unseen set, demonstrating its comparable utility in\nreal-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.20097v1.pdf"
    },
    {
        "title": "LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario",
        "authors": [
            "Hongyi Liu",
            "Zirui Liu",
            "Ruixiang Tang",
            "Jiayi Yuan",
            "Shaochen Zhong",
            "Yu-Neng Chuang",
            "Li Li",
            "Rui Chen",
            "Xia Hu"
        ],
        "published": "2024-02-29T20:25:16Z",
        "summary": "Fine-tuning LLMs is crucial to enhancing their task-specific performance and\nensuring model behaviors are aligned with human preferences. Among various\nfine-tuning methods, LoRA is popular for its efficiency and ease to use,\nallowing end-users to easily post and adopt lightweight LoRA modules on\nopen-source platforms to tailor their model for different customization.\nHowever, such a handy share-and-play setting opens up new attack surfaces, that\nthe attacker can render LoRA as an attacker, such as backdoor injection, and\nwidely distribute the adversarial LoRA to the community easily. This can result\nin detrimental outcomes. Despite the huge potential risks of sharing LoRA\nmodules, this aspect however has not been fully explored. To fill the gap, in\nthis study we thoroughly investigate the attack opportunities enabled in the\ngrowing share-and-play scenario. Specifically, we study how to inject backdoor\ninto the LoRA module and dive deeper into LoRA's infection mechanisms. We found\nthat training-free mechanism is possible in LoRA backdoor injection. We also\ndiscover the impact of backdoor attacks with the presence of multiple LoRA\nadaptions concurrently as well as LoRA based backdoor transferability. Our aim\nis to raise awareness of the potential risks under the emerging share-and-play\nscenario, so as to proactively prevent potential consequences caused by\nLoRA-as-an-Attack. Warning: the paper contains potential offensive content\ngenerated by models.",
        "pdf_link": "https://arxiv.org/pdf/2403.00108v1.pdf"
    },
    {
        "title": "Feedback-Generation for Programming Exercises With GPT-4",
        "authors": [
            "Imen Azaiz",
            "Natalie Kiesler",
            "Sven Strickroth"
        ],
        "published": "2024-03-07T12:37:52Z",
        "summary": "Ever since Large Language Models (LLMs) and related applications have become\nbroadly available, several studies investigated their potential for assisting\neducators and supporting students in higher education. LLMs such as Codex,\nGPT-3.5, and GPT 4 have shown promising results in the context of large\nprogramming courses, where students can benefit from feedback and hints if\nprovided timely and at scale. This paper explores the quality of GPT-4 Turbo's\ngenerated output for prompts containing both the programming task specification\nand a student's submission as input. Two assignments from an introductory\nprogramming course were selected, and GPT-4 was asked to generate feedback for\n55 randomly chosen, authentic student programming submissions. The output was\nqualitatively analyzed regarding correctness, personalization, fault\nlocalization, and other features identified in the material. Compared to prior\nwork and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For\nexample, the output is more structured and consistent. GPT-4 Turbo can also\naccurately identify invalid casing in student programs' output. In some cases,\nthe feedback also includes the output of the student program. At the same time,\ninconsistent feedback was noted such as stating that the submission is correct\nbut an error needs to be fixed. The present work increases our understanding of\nLLMs' potential, limitations, and how to integrate them into e-assessment\nsystems, pedagogical scenarios, and instructing students who are using\napplications based on GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.04449v1.pdf"
    },
    {
        "title": "Improving Generalizability of Extracting Social Determinants of Health Using Large Language Models through Prompt-tuning",
        "authors": [
            "Cheng Peng",
            "Zehao Yu",
            "Kaleb E Smith",
            "Wei-Hsuan Lo-Ciganic",
            "Jiang Bian",
            "Yonghui Wu"
        ],
        "published": "2024-03-19T02:34:33Z",
        "summary": "The progress in natural language processing (NLP) using large language models\n(LLMs) has greatly improved patient information extraction from clinical\nnarratives. However, most methods based on the fine-tuning strategy have\nlimited transfer learning ability for cross-domain applications. This study\nproposed a novel approach that employs a soft prompt-based learning\narchitecture, which introduces trainable prompts to guide LLMs toward desired\noutputs. We examined two types of LLM architectures, including encoder-only\nGatorTron and decoder-only GatorTronGPT, and evaluated their performance for\nthe extraction of social determinants of health (SDoH) using a\ncross-institution dataset from the 2022 n2c2 challenge and a cross-disease\ndataset from the University of Florida (UF) Health. The results show that\ndecoder-only LLMs with prompt tuning achieved better performance in\ncross-domain applications. GatorTronGPT achieved the best F1 scores for both\ndatasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a\ncross-institution setting, and 5.5% and 14.5% in a cross-disease setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.12374v1.pdf"
    },
    {
        "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
        "authors": [
            "Jiawen Shi",
            "Zenghui Yuan",
            "Yinuo Liu",
            "Yue Huang",
            "Pan Zhou",
            "Lichao Sun",
            "Neil Zhenqiang Gong"
        ],
        "published": "2024-03-26T13:58:00Z",
        "summary": "LLM-as-a-Judge is a novel solution that can assess textual information with\nlarge language models (LLMs). Based on existing research studies, LLMs\ndemonstrate remarkable performance in providing a compelling alternative to\ntraditional human assessment. However, the robustness of these systems against\nprompt injection attacks remains an open question. In this work, we introduce\nJudgeDeceiver, a novel optimization-based prompt injection attack tailored to\nLLM-as-a-Judge. Our method formulates a precise optimization objective for\nattacking the decision-making process of LLM-as-a-Judge and utilizes an\noptimization algorithm to efficiently automate the generation of adversarial\nsequences, achieving targeted and effective manipulation of model evaluations.\nCompared to handcraft prompt injection attacks, our method demonstrates\nsuperior efficacy, posing a significant challenge to the current security\nparadigms of LLM-based judgment systems. Through extensive experiments, we\nshowcase the capability of JudgeDeceiver in altering decision outcomes across\nvarious cases, highlighting the vulnerability of LLM-as-a-Judge systems to the\noptimization-based prompt injection attack.",
        "pdf_link": "https://arxiv.org/pdf/2403.17710v1.pdf"
    },
    {
        "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
        "authors": [
            "Nicholas Lee",
            "Thanakul Wattanawong",
            "Sehoon Kim",
            "Karttikeya Mangalam",
            "Sheng Shen",
            "Gopala Anumanchipali",
            "Michael W. Mahoney",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "published": "2024-03-22T08:57:07Z",
        "summary": "Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a LLaMA2-7B student model.",
        "pdf_link": "https://arxiv.org/pdf/2403.15042v1.pdf"
    },
    {
        "title": "FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large Language Models",
        "authors": [
            "Sahil Mishra",
            "Ujjwal Sudev",
            "Tanmoy Chakraborty"
        ],
        "published": "2024-02-21T08:50:40Z",
        "summary": "Taxonomies represent an arborescence hierarchical structure that establishes\nrelationships among entities to convey knowledge within a specific domain. Each\nedge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find\nutility in various real-world applications, such as e-commerce search engines\nand recommendation systems. Consequently, there arises a necessity to enhance\nthese taxonomies over time. However, manually curating taxonomies with neoteric\ndata presents challenges due to limitations in available human resources and\nthe exponential growth of data. Therefore, it becomes imperative to develop\nautomatic taxonomy expansion methods. Traditional supervised taxonomy expansion\napproaches encounter difficulties stemming from limited resources, primarily\ndue to the small size of existing taxonomies. This scarcity of training data\noften leads to overfitting. In this paper, we propose FLAME, a novel approach\nfor taxonomy expansion in low-resource environments by harnessing the\ncapabilities of large language models that are trained on extensive real-world\nknowledge. LLMs help compensate for the scarcity of domain-specific knowledge.\nSpecifically, FLAME leverages prompting in few-shot settings to extract the\ninherent knowledge within the LLMs, ascertaining the hypernym entities within\nthe taxonomy. Furthermore, it employs reinforcement learning to fine-tune the\nlarge language models, resulting in more accurate predictions. Experiments on\nthree real-world benchmark datasets demonstrate the effectiveness of FLAME in\nreal-world scenarios, achieving a remarkable improvement of 18.5% in accuracy\nand 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate\nthe strengths and weaknesses of FLAME through an extensive case study, error\nanalysis and ablation studies on the benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.13623v1.pdf"
    },
    {
        "title": "Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing",
        "authors": [
            "Yong Cao",
            "Wenyan Li",
            "Jiaang Li",
            "Yifei Yuan",
            "Antonia Karamolegkou",
            "Daniel Hershcovich"
        ],
        "published": "2024-02-08T19:25:40Z",
        "summary": "Pretrained large Vision-Language models have drawn considerable interest in\nrecent years due to their remarkable performance. Despite considerable efforts\nto assess these models from diverse perspectives, the extent of visual cultural\nawareness in the state-of-the-art GPT-4V model remains unexplored. To tackle\nthis gap, we extensively probed GPT-4V using the MaRVL benchmark dataset,\naiming to investigate its capabilities and limitations in visual understanding\nwith a focus on cultural aspects. Specifically, we introduced three visual\nrelated tasks, i.e. caption classification, pairwise captioning, and culture\ntag selection, to systematically delve into fine-grained visual cultural\nevaluation. Experimental results indicate that GPT-4V excels at identifying\ncultural concepts but still exhibits weaker performance in low-resource\nlanguages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V\nproves to be more culturally relevant in image captioning tasks than the\noriginal MaRVL human annotations, suggesting a promising solution for future\nvisual cultural benchmark construction.",
        "pdf_link": "https://arxiv.org/pdf/2402.06015v2.pdf"
    },
    {
        "title": "Gender Bias in Large Language Models across Multiple Languages",
        "authors": [
            "Jinman Zhao",
            "Yitian Ding",
            "Chen Jia",
            "Yining Wang",
            "Zifan Qian"
        ],
        "published": "2024-03-01T04:47:16Z",
        "summary": "With the growing deployment of large language models (LLMs) across various\napplications, assessing the influence of gender biases embedded in LLMs becomes\ncrucial. The topic of gender bias within the realm of natural language\nprocessing (NLP) has gained considerable focus, particularly in the context of\nEnglish. Nonetheless, the investigation of gender bias in languages other than\nEnglish is still relatively under-explored and insufficiently analyzed. In this\nwork, We examine gender bias in LLMs-generated outputs for different languages.\nWe use three measurements: 1) gender bias in selecting descriptive words given\nthe gender-related context. 2) gender bias in selecting gender-related pronouns\n(she/he) given the descriptive words. 3) gender bias in the topics of\nLLM-generated dialogues. We investigate the outputs of the GPT series of LLMs\nin various languages using our three measurement methods. Our findings revealed\nsignificant gender biases across all the languages we examined.",
        "pdf_link": "https://arxiv.org/pdf/2403.00277v1.pdf"
    },
    {
        "title": "On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era",
        "authors": [
            "Matteo Tiezzi",
            "Michele Casoni",
            "Alessandro Betti",
            "Tommaso Guidi",
            "Marco Gori",
            "Stefano Melacci"
        ],
        "published": "2024-02-12T23:55:55Z",
        "summary": "A longstanding challenge for the Machine Learning community is the one of\ndeveloping models that are capable of processing and learning from very long\nsequences of data. The outstanding results of Transformers-based networks\n(e.g., Large Language Models) promotes the idea of parallel attention as the\nkey to succeed in such a challenge, obfuscating the role of classic sequential\nprocessing of Recurrent Models. However, in the last few years, researchers who\nwere concerned by the quadratic complexity of self-attention have been\nproposing a novel wave of neural models, which gets the best from the two\nworlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State\nModels emerged as robust approaches to function approximation over time, thus\nopening a new perspective in learning from sequential data, followed by many\npeople in the field and exploited to implement a special class of (linear)\nRecurrent Neural Networks. This survey is aimed at providing an overview of\nthese trends framed under the unifying umbrella of Recurrence. Moreover, it\nemphasizes novel research opportunities that become prominent when abandoning\nthe idea of processing long sequences whose length is known-in-advance for the\nmore realistic setting of potentially infinite-length sequences, thus\nintersecting the field of lifelong-online learning from streamed data.",
        "pdf_link": "https://arxiv.org/pdf/2402.08132v2.pdf"
    },
    {
        "title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining",
        "authors": [
            "Yanchao Tan",
            "Hang Lv",
            "Xinyi Huang",
            "Jiawei Zhang",
            "Shiping Wang",
            "Carl Yang"
        ],
        "published": "2024-03-02T09:27:32Z",
        "summary": "Graphs with abundant attributes are essential in modeling interconnected\nentities and improving predictions in various real-world applications.\nTraditional Graph Neural Networks (GNNs), which are commonly used for modeling\nattributed graphs, need to be re-trained every time when applied to different\ngraph tasks and datasets. Although the emergence of Large Language Models\n(LLMs) has introduced a new paradigm in natural language processing, the\ngenerative potential of LLMs in graph mining remains largely under-explored. To\nthis end, we propose a novel framework MuseGraph, which seamlessly integrates\nthe strengths of GNNs and LLMs and facilitates a more effective and generic\napproach for graph mining across different tasks and datasets. Specifically, we\nfirst introduce a compact graph description via the proposed adaptive input\ngeneration to encapsulate key information from the graph under the constraints\nof language token limitations. Then, we propose a diverse instruction\ngeneration mechanism, which distills the reasoning capabilities from LLMs\n(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction\npackages for different graph tasks. Finally, we propose a graph-aware\ninstruction tuning with a dynamic instruction package allocation strategy\nacross tasks and datasets, ensuring the effectiveness and generalization of the\ntraining process. Our experimental results demonstrate significant improvements\nin different graph tasks, showcasing the potential of our MuseGraph in\nenhancing the accuracy of graph-oriented downstream tasks while keeping the\ngeneration powers of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04780v2.pdf"
    },
    {
        "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
        "authors": [
            "Mengwei Xu",
            "Wangsong Yin",
            "Dongqi Cai",
            "Rongjie Yi",
            "Daliang Xu",
            "Qipeng Wang",
            "Bingyang Wu",
            "Yihao Zhao",
            "Chen Yang",
            "Shihe Wang",
            "Qiyang Zhang",
            "Zhenyan Lu",
            "Li Zhang",
            "Shangguang Wang",
            "Yuanchun Li",
            "Yunxin Liu",
            "Xin Jin",
            "Xuanzhe Liu"
        ],
        "published": "2024-01-16T03:35:26Z",
        "summary": "Large foundation models, including large language models (LLMs), vision\ntransformers (ViTs), diffusion, and LLM-based multimodal models, are\nrevolutionizing the entire machine learning lifecycle, from training to\ndeployment. However, the substantial advancements in versatility and\nperformance these models offer come at a significant cost in terms of hardware\nresources. To support the growth of these large models in a scalable and\nenvironmentally sustainable way, there has been a considerable focus on\ndeveloping resource-efficient strategies. This survey delves into the critical\nimportance of such research, examining both algorithmic and systemic aspects.\nIt offers a comprehensive analysis and valuable insights gleaned from existing\nliterature, encompassing a broad array of topics from cutting-edge model\narchitectures and training/serving algorithms to practical system designs and\nimplementations. The goal of this survey is to provide an overarching\nunderstanding of how current approaches are tackling the resource challenges\nposed by large foundation models and to potentially inspire future\nbreakthroughs in this field.",
        "pdf_link": "https://arxiv.org/pdf/2401.08092v1.pdf"
    },
    {
        "title": "Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models",
        "authors": [
            "Huanxuan Liao",
            "Shizhu He",
            "Yao Xu",
            "Yuanzhe Zhang",
            "Kang Liu",
            "Shengping Liu",
            "Jun Zhao"
        ],
        "published": "2024-03-22T15:06:45Z",
        "summary": "Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been\nproposed to enhance the knowledge required for question answering over Large\nLanguage Models (LLMs). However, the former depends on external resources, and\nboth require incorporating the explicit documents into the context, which\nresults in longer contexts that lead to more resource consumption. Recent works\nindicate that LLMs have modeled rich knowledge, albeit not effectively\ntriggered or activated. Inspired by this, we propose a novel\nknowledge-augmented framework, Imagination-Augmented-Generation (IAG), which\nsimulates the human capacity to compensate for knowledge deficits while\nanswering questions solely through imagination, without relying on external\nresources. Guided by IAG, we propose an imagine richer context method for\nquestion answering (IMcQA), which obtains richer context through the following\ntwo modules: explicit imagination by generating a short dummy document with\nlong context compress and implicit imagination with HyperNetwork for generating\nadapter weights. Experimental results on three datasets demonstrate that IMcQA\nexhibits significant advantages in both open-domain and closed-book settings,\nas well as in both in-distribution performance and out-of-distribution\ngeneralizations. Our code will be available at\nhttps://github.com/Xnhyacinth/IAG.",
        "pdf_link": "https://arxiv.org/pdf/2403.15268v2.pdf"
    },
    {
        "title": "Enhancing Zero-shot Counting via Language-guided Exemplar Learning",
        "authors": [
            "Mingjie Wang",
            "Jun Zhou",
            "Yong Dai",
            "Eric Buys",
            "Minglun Gong"
        ],
        "published": "2024-02-08T04:07:38Z",
        "summary": "Recently, Class-Agnostic Counting (CAC) problem has garnered increasing\nattention owing to its intriguing generality and superior efficiency compared\nto Category-Specific Counting (CSC). This paper proposes a novel ExpressCount\nto enhance zero-shot object counting by delving deeply into language-guided\nexemplar learning. Specifically, the ExpressCount is comprised of an innovative\nLanguage-oriented Exemplar Perceptron and a downstream visual Zero-shot\nCounting pipeline. Thereinto, the perceptron hammers at exploiting accurate\nexemplar cues from collaborative language-vision signals by inheriting rich\nsemantic priors from the prevailing pre-trained Large Language Models (LLMs),\nwhereas the counting pipeline excels in mining fine-grained features through\ndual-branch and cross-attention schemes, contributing to the high-quality\nsimilarity learning. Apart from building a bridge between the LLM in vogue and\nthe visual counting tasks, expression-guided exemplar estimation significantly\nadvances zero-shot learning capabilities for counting instances with arbitrary\nclasses. Moreover, devising a FSC-147-Express with annotations of meticulous\nlinguistic expressions pioneers a new venue for developing and validating\nlanguage-based counting models. Extensive experiments demonstrate the\nstate-of-the-art performance of our ExpressCount, even showcasing the accuracy\non par with partial CSC models.",
        "pdf_link": "https://arxiv.org/pdf/2402.05394v1.pdf"
    },
    {
        "title": "Projective Methods for Mitigating Gender Bias in Pre-trained Language Models",
        "authors": [
            "Hillary Dawkins",
            "Isar Nejadgholi",
            "Daniel Gillis",
            "Judi McCuaig"
        ],
        "published": "2024-03-27T17:49:31Z",
        "summary": "Mitigation of gender bias in NLP has a long history tied to debiasing static\nword embeddings. More recently, attention has shifted to debiasing pre-trained\nlanguage models. We study to what extent the simplest projective debiasing\nmethods, developed for word embeddings, can help when applied to BERT's\ninternal representations. Projective methods are fast to implement, use a small\nnumber of saved parameters, and make no updates to the existing model\nparameters. We evaluate the efficacy of the methods in reducing both intrinsic\nbias, as measured by BERT's next sentence prediction task, and in mitigating\nobserved bias in a downstream setting when fine-tuned. To this end, we also\nprovide a critical analysis of a popular gender-bias assessment test for\nquantifying intrinsic bias, resulting in an enhanced test set and new bias\nmeasures. We find that projective methods can be effective at both intrinsic\nbias and downstream bias mitigation, but that the two outcomes are not\nnecessarily correlated. This finding serves as a warning that intrinsic bias\ntest sets, based either on language modeling tasks or next sentence prediction,\nshould not be the only benchmark in developing a debiased language model.",
        "pdf_link": "https://arxiv.org/pdf/2403.18803v1.pdf"
    },
    {
        "title": "Towards Full Authorship with AI: Supporting Revision with AI-Generated Views",
        "authors": [
            "Jiho Kim",
            "Ray C. Flanagan",
            "Noelle E. Haviland",
            "ZeAi Sun",
            "Souad N. Yakubu",
            "Edom A. Maru",
            "Kenneth C. Arnold"
        ],
        "published": "2024-03-02T01:11:35Z",
        "summary": "Large language models (LLMs) are shaping a new user interface (UI) paradigm\nin writing tools by enabling users to generate text through prompts. This\nparadigm shifts some creative control from the user to the system, thereby\ndiminishing the user's authorship and autonomy in the writing process. To\nrestore autonomy, we introduce Textfocals, a UI prototype designed to\ninvestigate a human-centered approach that emphasizes the user's role in\nwriting. Textfocals supports the writing process by providing LLM-generated\nsummaries, questions, and advice (i.e., LLM views) in a sidebar of a text\neditor, encouraging reflection and self-driven revision in writing without\ndirect text generation. Textfocals' UI affordances, including contextually\nadaptive views and scaffolding for prompt selection and customization, offer a\nnovel way to interact with LLMs where users maintain full authorship of their\nwriting. A formative user study with Textfocals showed promising evidence that\nthis approach might help users develop underdeveloped ideas, cater to the\nrhetorical audience, and clarify their writing. However, the study also showed\ninteraction design challenges related to document navigation and scoping,\nprompt engineering, and context management. Our work highlights the breadth of\nthe design space of writing support interfaces powered by generative AI that\nmaintain authorship integrity.",
        "pdf_link": "https://arxiv.org/pdf/2403.01055v1.pdf"
    },
    {
        "title": "Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?",
        "authors": [
            "Nishant Balepur",
            "Abhilasha Ravichander",
            "Rachel Rudinger"
        ],
        "published": "2024-02-19T19:38:58Z",
        "summary": "Multiple-choice question answering (MCQA) is often used to evaluate large\nlanguage models (LLMs). To see if MCQA assesses LLMs as intended, we probe if\nLLMs can perform MCQA with choices-only prompts, where models must select the\ncorrect answer only from the choices. In three MCQA datasets and four LLMs,\nthis prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy\ngain. To help explain this behavior, we conduct an in-depth, black-box analysis\non memorization, choice dynamics, and question inference. Our key findings are\nthreefold. First, we find no evidence that the choices-only accuracy stems from\nmemorization alone. Second, priors over individual choices do not fully explain\nchoices-only accuracy, hinting that LLMs use the group dynamics of choices.\nThird, LLMs have some ability to infer a relevant question from choices, and\nsurprisingly can sometimes even match the original question. We hope to\nmotivate the use of stronger baselines in MCQA benchmarks, the design of robust\nMCQA datasets, and further efforts to explain LLM decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.12483v1.pdf"
    },
    {
        "title": "On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities",
        "authors": [
            "Xiyang Wu",
            "Ruiqi Xian",
            "Tianrui Guan",
            "Jing Liang",
            "Souradip Chakraborty",
            "Fuxiao Liu",
            "Brian Sadler",
            "Dinesh Manocha",
            "Amrit Singh Bedi"
        ],
        "published": "2024-02-15T22:01:45Z",
        "summary": "In this paper, we highlight the critical issues of robustness and safety\nassociated with integrating large language models (LLMs) and vision-language\nmodels (VLMs) into robotics applications. Recent works have focused on using\nLLMs and VLMs to improve the performance of robotics tasks, such as\nmanipulation, navigation, etc. However, such integration can introduce\nsignificant vulnerabilities, in terms of their susceptibility to adversarial\nattacks due to the language models, potentially leading to catastrophic\nconsequences. By examining recent works at the interface of LLMs/VLMs and\nrobotics, we show that it is easy to manipulate or misguide the robot's\nactions, leading to safety hazards. We define and provide examples of several\nplausible adversarial attacks, and conduct experiments on three prominent robot\nframeworks integrated with a language model, including KnowNo VIMA, and\nInstruct2Act, to assess their susceptibility to these attacks. Our empirical\nfindings reveal a striking vulnerability of LLM/VLM-robot integrated systems:\nsimple adversarial attacks can significantly undermine the effectiveness of\nLLM/VLM-robot integrated systems. Specifically, our data demonstrate an average\nperformance deterioration of 21.2% under prompt attacks and a more alarming\n30.2% under perception attacks. These results underscore the critical need for\nrobust countermeasures to ensure the safe and reliable deployment of the\nadvanced LLM/VLM-based robotic systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.10340v3.pdf"
    },
    {
        "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
        "authors": [
            "Santiago Castro",
            "Amir Ziai",
            "Avneesh Saluja",
            "Zhuoning Yuan",
            "Rada Mihalcea"
        ],
        "published": "2024-02-22T23:42:25Z",
        "summary": "Recent years have witnessed a significant increase in the performance of\nVision and Language tasks. Foundational Vision-Language Models (VLMs), such as\nCLIP, have been leveraged in multiple settings and demonstrated remarkable\nperformance across several tasks. Such models excel at object-centric\nrecognition yet learn text representations that seem invariant to word order,\nfailing to compose known concepts in novel ways. However, no evidence exists\nthat any VLM, including large-scale single-stream models such as GPT-4V,\nidentifies compositions successfully. In this paper, we introduce a framework\nto significantly improve the ability of existing models to encode compositional\nlanguage, with over 10% absolute improvement on compositionality benchmarks,\nwhile maintaining or improving the performance on standard object-recognition\nand retrieval benchmarks. Our code and pre-trained models are publicly\navailable at https://github.com/netflix/clove.",
        "pdf_link": "https://arxiv.org/pdf/2402.15021v2.pdf"
    },
    {
        "title": "E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models",
        "authors": [
            "Jinchang Hou",
            "Chang Ao",
            "Haihong Wu",
            "Xiangtao Kong",
            "Zhigang Zheng",
            "Daijia Tang",
            "Chengming Li",
            "Xiping Hu",
            "Ruifeng Xu",
            "Shiwen Ni",
            "Min Yang"
        ],
        "published": "2024-01-29T07:34:37Z",
        "summary": "With the accelerating development of Large Language Models (LLMs), many LLMs\nare beginning to be used in the Chinese K-12 education domain. The integration\nof LLMs and education is getting closer and closer, however, there is currently\nno benchmark for evaluating LLMs that focuses on the Chinese K-12 education\ndomain. Therefore, there is an urgent need for a comprehensive natural language\nprocessing benchmark to accurately assess the capabilities of various LLMs in\nthe Chinese K-12 education domain. To address this, we introduce the E-EVAL,\nthe first comprehensive evaluation benchmark specifically designed for the\nChinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice\nquestions at the primary, middle, and high school levels across a wide range of\nsubjects, including Chinese, English, Politics, History, Ethics, Physics,\nChemistry, Mathematics, and Geography. We conducted a comprehensive evaluation\nof E-EVAL on advanced LLMs, including both English-dominant and\nChinese-dominant models. Findings show that Chinese-dominant models perform\nwell compared to English-dominant models, with many scoring even above the GPT\n4.0. However, almost all models perform poorly in complex subjects such as\nmathematics. We also found that most Chinese-dominant LLMs did not achieve\nhigher scores at the primary school level compared to the middle school level.\nWe observe that the mastery of higher-order knowledge by the model does not\nnecessarily imply the mastery of lower-order knowledge as well. Additionally,\nthe experimental results indicate that the Chain of Thought (CoT) technique is\neffective only for the challenging science subjects, while Few-shot prompting\nis more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze\nthe strengths and limitations of LLMs in educational applications, and to\ncontribute to the progress and development of Chinese K-12 education and LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.15927v1.pdf"
    },
    {
        "title": "Monte Carlo Tree Search for Recipe Generation using GPT-2",
        "authors": [
            "Karan Taneja",
            "Richard Segal",
            "Richard Goodwin"
        ],
        "published": "2024-01-10T14:50:46Z",
        "summary": "Automatic food recipe generation methods provide a creative tool for chefs to\nexplore and to create new, and interesting culinary delights. Given the recent\nsuccess of large language models (LLMs), they have the potential to create new\nrecipes that can meet individual preferences, dietary constraints, and adapt to\nwhat is in your refrigerator. Existing research on using LLMs to generate\nrecipes has shown that LLMs can be finetuned to generate realistic-sounding\nrecipes. However, on close examination, these generated recipes often fail to\nmeet basic requirements like including chicken as an ingredient in chicken\ndishes. In this paper, we propose RecipeMC, a text generation method using\nGPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to\ndefine reward functions to put soft constraints on text generation and thus\nimprove the credibility of the generated recipes. Our results show that human\nevaluators prefer recipes generated with RecipeMC more often than recipes\ngenerated with other baseline methods when compared with real recipes.",
        "pdf_link": "https://arxiv.org/pdf/2401.05199v1.pdf"
    },
    {
        "title": "Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model unless you have access to GPT-4",
        "authors": [
            "Aryo Pradipta Gema",
            "Giwon Hong",
            "Pasquale Minervini",
            "Luke Daines",
            "Beatrice Alex"
        ],
        "published": "2024-03-30T22:27:21Z",
        "summary": "The NLI4CT task assesses Natural Language Inference systems in predicting\nwhether hypotheses entail or contradict evidence from Clinical Trial Reports.\nIn this study, we evaluate various Large Language Models (LLMs) with multiple\nstrategies, including Chain-of-Thought, In-Context Learning, and\nParameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the\nconsistency of LLMs by merging adapters that were fine-tuned separately using\ntriplet and language modelling objectives. We found that merging the two PEFT\nadapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs.\nHowever, our novel methods did not produce more accurate results than GPT-4 in\nterms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks\njoint-first in the competition with 0.8328. Finally, our contamination analysis\nwith GPT-4 indicates that there was no test data leakage.",
        "pdf_link": "https://arxiv.org/pdf/2404.00484v1.pdf"
    },
    {
        "title": "Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions",
        "authors": [
            "Yang Deng",
            "Yong Zhao",
            "Moxin Li",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-23T02:24:36Z",
        "summary": "Despite the remarkable abilities of Large Language Models (LLMs) to answer\nquestions, they often display a considerable level of overconfidence even when\nthe question does not have a definitive answer. To avoid providing hallucinated\nanswers to these unknown questions, existing studies typically investigate\napproaches to refusing to answer these questions. In this work, we propose a\nnovel and scalable self-alignment method to utilize the LLM itself to enhance\nits response-ability to different types of unknown questions, being capable of\nnot only refusing to answer but also providing explanation to the\nunanswerability of unknown questions. Specifically, the Self-Align method first\nemploy a two-stage class-aware self-augmentation approach to generate a large\namount of unknown question-response data. Then we conduct disparity-driven\nself-curation to select qualified data for fine-tuning the LLM itself for\naligning the responses to unknown questions as desired. Experimental results on\ntwo datasets across four types of unknown questions validate the superiority of\nthe Self-Align method over existing baselines in terms of three types of task\nformulation.",
        "pdf_link": "https://arxiv.org/pdf/2402.15062v1.pdf"
    },
    {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
        "authors": [
            "Junyi Li",
            "Jie Chen",
            "Ruiyang Ren",
            "Xiaoxue Cheng",
            "Wayne Xin Zhao",
            "Jian-Yun Nie",
            "Ji-Rong Wen"
        ],
        "published": "2024-01-06T12:40:45Z",
        "summary": "In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "pdf_link": "https://arxiv.org/pdf/2401.03205v1.pdf"
    },
    {
        "title": "Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment",
        "authors": [
            "Kun-Peng Ning",
            "Shuo Yang",
            "Yu-Yang Liu",
            "Jia-Yu Yao",
            "Zhen-Hui Liu",
            "Yu Wang",
            "Ming Pang",
            "Li Yuan"
        ],
        "published": "2024-02-02T18:49:26Z",
        "summary": "Existing large language models (LLMs) evaluation methods typically focus on\ntesting the performance on some closed-environment and domain-specific\nbenchmarks with human annotations. In this paper, we explore a novel\nunsupervised evaluation direction, utilizing peer-review mechanisms to measure\nLLMs automatically. In this setting, both open-source and closed-source LLMs\nlie in the same environment, capable of answering unlabeled questions and\nevaluating each other, where each LLM's response score is jointly determined by\nother anonymous ones. To obtain the ability hierarchy among these models, we\nassign each LLM a learnable capability parameter to adjust the final ranking.\nWe formalize it as a constrained optimization problem, intending to maximize\nthe consistency of each LLM's capabilities and scores. The key assumption\nbehind is that high-level LLM can evaluate others' answers more accurately than\nlow-level ones, while higher-level LLM can also achieve higher response scores.\nMoreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap\nin aligning human rankings. We perform experiments on multiple datasets with\nthese metrics, validating the effectiveness of the proposed approach.",
        "pdf_link": "https://arxiv.org/pdf/2402.01830v1.pdf"
    },
    {
        "title": "Humans or LLMs as the Judge? A Study on Judgement Biases",
        "authors": [
            "Guiming Hardy Chen",
            "Shunian Chen",
            "Ziche Liu",
            "Feng Jiang",
            "Benyou Wang"
        ],
        "published": "2024-02-16T13:21:06Z",
        "summary": "Adopting human and large language models (LLM) as judges (\\textit{a.k.a}\nhuman- and LLM-as-a-judge) for evaluating the performance of existing LLMs has\nrecently gained attention. Nonetheless, this approach concurrently introduces\npotential biases from human and LLM judges, questioning the reliability of the\nevaluation results. In this paper, we propose a novel framework for\ninvestigating 5 types of biases for LLM and human judges. We curate a dataset\nwith 142 samples referring to the revised Bloom's Taxonomy and conduct\nthousands of human and LLM evaluations. Results show that human and LLM judges\nare vulnerable to perturbations to various degrees, and that even the most\ncutting-edge judges possess considerable biases. We further exploit their\nweakness and conduct attacks on LLM judges. We hope that our work can notify\nthe community of the vulnerability of human- and LLM-as-a-judge against\nperturbations, as well as the urgency of developing robust evaluation systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.10669v2.pdf"
    },
    {
        "title": "Dial-insight: Fine-tuning Large Language Models with High-Quality Domain-Specific Data Preventing Capability Collapse",
        "authors": [
            "Jianwei Sun",
            "Chaoyang Mei",
            "Linlin Wei",
            "Kaiyu Zheng",
            "Na Liu",
            "Ming Cui",
            "Tianyi Li"
        ],
        "published": "2024-03-14T08:27:32Z",
        "summary": "The efficacy of large language models (LLMs) is heavily dependent on the\nquality of the underlying data, particularly within specialized domains. A\ncommon challenge when fine-tuning LLMs for domain-specific applications is the\npotential degradation of the model's generalization capabilities. To address\nthese issues, we propose a two-stage approach for the construction of\nproduction prompts designed to yield high-quality data. This method involves\nthe generation of a diverse array of prompts that encompass a broad spectrum of\ntasks and exhibit a rich variety of expressions. Furthermore, we introduce a\ncost-effective, multi-dimensional quality assessment framework to ensure the\nintegrity of the generated labeling data. Utilizing a dataset comprised of\nservice provider and customer interactions from the real estate sector, we\ndemonstrate a positive correlation between data quality and model performance.\nNotably, our findings indicate that the domain-specific proficiency of general\nLLMs can be enhanced through fine-tuning with data produced via our proposed\nmethod, without compromising their overall generalization abilities, even when\nexclusively domain-specific data is employed for fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.09167v1.pdf"
    },
    {
        "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
        "authors": [
            "Kai Wang",
            "Yuwei Xu",
            "Zhiyong Wu",
            "Siqiang Luo"
        ],
        "published": "2024-02-19T03:21:19Z",
        "summary": "Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts\nfrom new KGs that are not seen during training, has been widely adopted in\nvarious applications. One critical challenge of KG inductive reasoning is\nhandling low-resource scenarios with scarcity in both textual and structural\naspects. In this paper, we attempt to address this challenge with Large\nLanguage Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to\ngenerate a graph-structural prompt to enhance the pre-trained Graph Neural\nNetworks (GNNs), which brings us new methodological insights into the KG\ninductive reasoning methods, as well as high generalizability in practice. On\nthe methodological side, we introduce a novel pretraining and prompting\nframework ProLINK, designed for low-resource inductive reasoning across\narbitrary KGs without requiring additional training. On the practical side, we\nexperimentally evaluate our approach on 36 low-resource KG datasets and find\nthat ProLINK outperforms previous methods in three-shot, one-shot, and\nzero-shot reasoning tasks, exhibiting average performance improvements by 20%,\n45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong\nrobustness for various LLM promptings as well as full-shot scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11804v1.pdf"
    },
    {
        "title": "A Language Model based Framework for New Concept Placement in Ontologies",
        "authors": [
            "Hang Dong",
            "Jiaoyan Chen",
            "Yuan He",
            "Yongsheng Gao",
            "Ian Horrocks"
        ],
        "published": "2024-02-27T21:27:35Z",
        "summary": "We investigate the task of inserting new concepts extracted from texts into\nan ontology using language models. We explore an approach with three steps:\nedge search which is to find a set of candidate locations to insert (i.e.,\nsubsumptions between concepts), edge formation and enrichment which leverages\nthe ontological structure to produce and enhance the edge candidates, and edge\nselection which eventually locates the edge to be placed into. In all steps, we\npropose to leverage neural methods, where we apply embedding-based methods and\ncontrastive learning with Pre-trained Language Models (PLMs) such as BERT for\nedge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder,\nand Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for\nedge selection. We evaluate the methods on recent datasets created using the\nSNOMED CT ontology and the MedMentions entity linking benchmark. The best\nsettings in our framework use fine-tuned PLM for search and a multi-label\nCross-encoder for selection. Zero-shot prompting of LLMs is still not adequate\nfor the task, and we propose explainable instruction tuning of LLMs for\nimproved performance. Our study shows the advantages of PLMs and highlights the\nencouraging performance of LLMs that motivates future studies.",
        "pdf_link": "https://arxiv.org/pdf/2402.17897v2.pdf"
    },
    {
        "title": "A Survey of Large Language Models in Finance (FinLLMs)",
        "authors": [
            "Jean Lee",
            "Nicholas Stevens",
            "Soyeon Caren Han",
            "Minseok Song"
        ],
        "published": "2024-02-04T02:06:57Z",
        "summary": "Large Language Models (LLMs) have shown remarkable capabilities across a wide\nvariety of Natural Language Processing (NLP) tasks and have attracted attention\nfrom multiple domains, including financial services. Despite the extensive\nresearch into general-domain LLMs, and their immense potential in finance,\nFinancial LLM (FinLLM) research remains limited. This survey provides a\ncomprehensive overview of FinLLMs, including their history, techniques,\nperformance, and opportunities and challenges. Firstly, we present a\nchronological overview of general-domain Pre-trained Language Models (PLMs)\nthrough to current FinLLMs, including the GPT-series, selected open-source\nLLMs, and financial LMs. Secondly, we compare five techniques used across\nfinancial PLMs and FinLLMs, including training methods, training data, and\nfine-tuning methods. Thirdly, we summarize the performance evaluations of six\nbenchmark tasks and datasets. In addition, we provide eight advanced financial\nNLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we\ndiscuss the opportunities and the challenges facing FinLLMs, such as\nhallucination, privacy, and efficiency. To support AI research in finance, we\ncompile a collection of accessible datasets and evaluation benchmarks on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2402.02315v1.pdf"
    },
    {
        "title": "Text classification of column headers with a controlled vocabulary: leveraging LLMs for metadata enrichment",
        "authors": [
            "Margherita Martorana",
            "Tobias Kuhn",
            "Lise Stork",
            "Jacco van Ossenbruggen"
        ],
        "published": "2024-03-01T10:01:36Z",
        "summary": "Traditional dataset retrieval systems index on metadata information rather\nthan on the data values. Thus relying primarily on manual annotations and\nhigh-quality metadata, processes known to be labour-intensive and challenging\nto automate. We propose a method to support metadata enrichment with topic\nannotations of column headers using three Large Language Models (LLMs):\nChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to\nclassify column headers based on domain-specific topics from a controlled\nvocabulary. We evaluate our approach by assessing the internal consistency of\nthe LLMs, the inter-machine alignment, and the human-machine agreement for the\ntopic classification task. Additionally, we investigate the impact of\ncontextual information (i.e. dataset description) on the classification\noutcomes. Our results suggest that ChatGPT and GoogleGemini outperform\nGoogleBard for internal consistency as well as LLM-human-alignment.\nInterestingly, we found that context had no impact on the LLMs performances.\nThis work proposes a novel approach that leverages LLMs for text classification\nusing a controlled topic vocabulary, which has the potential to facilitate\nautomated metadata enrichment, thereby enhancing dataset retrieval and the\nFindability, Accessibility, Interoperability and Reusability (FAIR) of research\ndata on the Web.",
        "pdf_link": "https://arxiv.org/pdf/2403.00884v2.pdf"
    },
    {
        "title": "The Impact of Reasoning Step Length on Large Language Models",
        "authors": [
            "Mingyu Jin",
            "Qinkai Yu",
            "Dong Shu",
            "Haiyan Zhao",
            "Wenyue Hua",
            "Yanda Meng",
            "Yongfeng Zhang",
            "Mengnan Du"
        ],
        "published": "2024-01-10T04:37:38Z",
        "summary": "Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations,\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences.",
        "pdf_link": "https://arxiv.org/pdf/2401.04925v3.pdf"
    },
    {
        "title": "A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",
        "authors": [
            "Mohamed R. Shoaib",
            "Heba M. Emara",
            "Jun Zhao"
        ],
        "published": "2024-01-12T10:29:48Z",
        "summary": "This survey paper explores the transformative influence of frontier AI,\nfoundation models, and Large Language Models (LLMs) in the realm of Intelligent\nTransportation Systems (ITS), emphasizing their integral role in advancing\ntransportation intelligence, optimizing traffic management, and contributing to\nthe realization of smart cities. Frontier AI refers to the forefront of AI\ntechnology, encompassing the latest advancements, innovations, and experimental\ntechniques in the field, especially AI foundation models and LLMs. Foundation\nmodels, like GPT-4, are large, general-purpose AI models that provide a base\nfor a wide range of applications. They are characterized by their versatility\nand scalability. LLMs are obtained from finetuning foundation models with a\nspecific focus on processing and generating natural language. They excel in\ntasks like language understanding, text generation, translation, and\nsummarization. By leveraging vast textual data, including traffic reports and\nsocial media interactions, LLMs extract critical insights, fostering the\nevolution of ITS. The survey navigates the dynamic synergy between LLMs and\nITS, delving into applications in traffic management, integration into\nautonomous vehicles, and their role in shaping smart cities. It provides\ninsights into ongoing research, innovations, and emerging trends, aiming to\ninspire collaboration at the intersection of language, intelligence, and\nmobility for safer, more efficient, and sustainable transportation systems. The\npaper further surveys interactions between LLMs and various aspects of ITS,\nexploring roles in traffic management, facilitating autonomous vehicles, and\ncontributing to smart city development, while addressing challenges brought by\nfrontier AI and foundation models. This paper offers valuable inspiration for\nfuture research and innovation in the transformative domain of intelligent\ntransportation.",
        "pdf_link": "https://arxiv.org/pdf/2401.06831v1.pdf"
    },
    {
        "title": "XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model",
        "authors": [
            "Zhitao Wang",
            "Wei Wang",
            "Zirao Li",
            "Long Wang",
            "Can Yi",
            "Xinjie Xu",
            "Luyang Cao",
            "Hanjing Su",
            "Shouzhi Chen",
            "Jun Zhou"
        ],
        "published": "2024-01-05T08:24:30Z",
        "summary": "In past years, we have been dedicated to automating user acceptance testing\n(UAT) process of WeChat Pay, one of the most influential mobile payment\napplications in China. A system titled XUAT has been developed for this\npurpose. However, there is still a human-labor-intensive stage, i.e, test\nscripts generation, in the current system. Therefore, in this paper, we\nconcentrate on methods of boosting the automation level of the current system,\nparticularly the stage of test scripts generation. With recent notable\nsuccesses, large language models (LLMs) demonstrate significant potential in\nattaining human-like intelligence and there has been a growing research area\nthat employs LLMs as autonomous agents to obtain human-like decision-making\ncapabilities. Inspired by these works, we propose an LLM-powered multi-agent\ncollaborative system, named XUAT-Copilot, for automated UAT. The proposed\nsystem mainly consists of three LLM-based agents responsible for action\nplanning, state checking and parameter selecting, respectively, and two\nadditional modules for state sensing and case rewriting. The agents interact\nwith testing device, make human-like decision and generate action command in a\ncollaborative way. The proposed multi-agent system achieves a close\neffectiveness to human testers in our experimental studies and gains a\nsignificant improvement of Pass@1 accuracy compared with single-agent\narchitecture. More importantly, the proposed system has launched in the formal\ntesting environment of WeChat Pay mobile app, which saves a considerable amount\nof manpower in the daily development work.",
        "pdf_link": "https://arxiv.org/pdf/2401.02705v2.pdf"
    },
    {
        "title": "An Evaluation of Large Language Models in Bioinformatics Research",
        "authors": [
            "Hengchuang Yin",
            "Zhonghui Gu",
            "Fanhao Wang",
            "Yiparemu Abuduhaibaier",
            "Yanqiao Zhu",
            "Xinming Tu",
            "Xian-Sheng Hua",
            "Xiao Luo",
            "Yizhou Sun"
        ],
        "published": "2024-02-21T11:27:31Z",
        "summary": "Large language models (LLMs) such as ChatGPT have gained considerable\ninterest across diverse research communities. Their notable ability for text\ncompletion and generation has inaugurated a novel paradigm for\nlanguage-interfaced problem solving. However, the potential and efficacy of\nthese models in bioinformatics remain incompletely explored. In this work, we\nstudy the performance LLMs on a wide spectrum of crucial bioinformatics tasks.\nThese tasks include the identification of potential coding regions, extraction\nof named entities for genes and proteins, detection of antimicrobial and\nanti-cancer peptides, molecular optimization, and resolution of educational\nbioinformatics problems. Our findings indicate that, given appropriate prompts,\nLLMs like GPT variants can successfully handle most of these tasks. In\naddition, we provide a thorough analysis of their limitations in the context of\ncomplicated bioinformatics tasks. In conclusion, we believe that this work can\nprovide new perspectives and motivate future research in the field of LLMs\napplications, AI for Science and bioinformatics.",
        "pdf_link": "https://arxiv.org/pdf/2402.13714v1.pdf"
    },
    {
        "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
        "authors": [
            "Jingling Li",
            "Zeyu Tang",
            "Xiaoyu Liu",
            "Peter Spirtes",
            "Kun Zhang",
            "Liu Leqi",
            "Yang Liu"
        ],
        "published": "2024-03-13T17:46:28Z",
        "summary": "Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.",
        "pdf_link": "https://arxiv.org/pdf/2403.08743v1.pdf"
    },
    {
        "title": "\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing",
        "authors": [
            "Vipula Rawte",
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Prachi Priya",
            "Aman Chadha",
            "Amit P. Sheth",
            "Amitava Das"
        ],
        "published": "2024-03-27T19:45:09Z",
        "summary": "Hallucination has emerged as the most vulnerable aspect of contemporary Large\nLanguage Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA)\nprompting, aimed to avoid LLM hallucinations by enhancing comprehension\nthrough: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay\nLLM generation. First, we provide an in-depth analysis of linguistic nuances:\nformality, readability, and concreteness of prompts for 21 LLMs, and elucidate\nhow these nuances contribute to hallucinated generation. Prompts with lower\nreadability, formality, or concreteness pose comprehension challenges for LLMs,\nsimilar to those faced by humans. In such scenarios, an LLM tends to speculate\nand generate content based on its imagination (associative memory) to fill\nthese information gaps. Although these speculations may occasionally align with\nfactual information, their accuracy is not assured, often resulting in\nhallucination. Recent studies reveal that an LLM often neglects the middle\nsections of extended prompts, a phenomenon termed as lost in the middle. While\na specific paraphrase may suit one LLM, the same paraphrased version may elicit\na different response from another LLM. Therefore, we propose an optimal\nparaphrasing technique to identify the most comprehensible paraphrase of a\ngiven prompt, evaluated using Integrated Gradient (and its variations) to\nguarantee that the LLM accurately processes all words. While reading lengthy\nsentences, humans often pause at various points to better comprehend the\nmeaning read thus far. We have fine-tuned an LLM with injected [PAUSE] tokens,\nallowing the LLM to pause while reading lengthier prompts. This has brought\nseveral key contributions: (i) determining the optimal position to inject\n[PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and\n(iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE]\ninsertion.",
        "pdf_link": "https://arxiv.org/pdf/2403.18976v1.pdf"
    },
    {
        "title": "Improving LLM-based Machine Translation with Systematic Self-Correction",
        "authors": [
            "Zhaopeng Feng",
            "Yan Zhang",
            "Hao Li",
            "Wenqiang Liu",
            "Jun Lang",
            "Yang Feng",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "published": "2024-02-26T07:58:12Z",
        "summary": "Large Language Models (LLMs) have achieved impressive results in Machine\nTranslation (MT). However, careful evaluations by human reveal that the\ntranslations produced by LLMs still contain multiple errors. Importantly,\nfeeding back such error information into the LLMs can lead to self-correction\nand result in improved translation performance. Motivated by these insights, we\nintroduce a systematic LLM-based self-correcting translation framework, named\nTER, which stands for Translate, Estimate, and Refine, marking a significant\nstep forward in this direction. Our findings demonstrate that 1) our\nself-correction framework successfully assists LLMs in improving their\ntranslation quality across a wide range of languages, whether it's from\nhigh-resource languages to low-resource ones or whether it's English-centric or\ncentered around other languages; 2) TER exhibits superior systematicity and\ninterpretability compared to previous methods; 3) different estimation\nstrategies yield varied impacts on AI feedback, directly affecting the\neffectiveness of the final corrections. We further compare different LLMs and\nconduct various experiments involving self-correction and cross-model\ncorrection to investigate the potential relationship between the translation\nand evaluation capabilities of LLMs. Our code and data are available at\nhttps://github.com/fzp0424/self_correct_mt",
        "pdf_link": "https://arxiv.org/pdf/2402.16379v2.pdf"
    },
    {
        "title": "Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation",
        "authors": [
            "Ziyan Wang",
            "Yingpeng Du",
            "Zhu Sun",
            "Haoyan Chua",
            "Kaidong Feng",
            "Wenya Wang",
            "Jie Zhang"
        ],
        "published": "2024-03-25T05:12:18Z",
        "summary": "Large Language Models (LLMs) are emerging as promising approaches to enhance\nsession-based recommendation (SBR), where both prompt-based and\nfine-tuning-based methods have been widely investigated to align LLMs with SBR.\nHowever, the former methods struggle with optimal prompts to elicit the correct\nreasoning of LLMs due to the lack of task-specific feedback, leading to\nunsatisfactory recommendations. Although the latter methods attempt to\nfine-tune LLMs with domain-specific knowledge, they face limitations such as\nhigh computational costs and reliance on open-source backbones. To address such\nissues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for\nSBR, guiding LLMs to focus on specialized knowledge essential for more accurate\nrecommendations effectively and efficiently. In particular, we first design the\nReflective Exploration Module to effectively extract knowledge that is readily\nunderstandable and digestible by LLMs. To be specific, we direct LLMs to\nexamine recommendation errors through self-reflection and construct a knowledge\nbase (KB) comprising hints capable of rectifying these errors. To efficiently\nelicit the correct reasoning of LLMs, we further devise the Reinforcement\nUtilization Module to train a lightweight retrieval agent. It learns to select\nhints from the constructed KB based on the task-specific feedback, where the\nhints can serve as guidance to help correct LLMs reasoning for better\nrecommendations. Extensive experiments on multiple real-world datasets\ndemonstrate that our method consistently outperforms state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.16427v3.pdf"
    },
    {
        "title": "CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering",
        "authors": [
            "Hongbin Na"
        ],
        "published": "2024-03-24T04:34:34Z",
        "summary": "The recent advancements in artificial intelligence highlight the potential of\nlanguage models in psychological health support. While models trained on data\nfrom mental health service platform have achieved preliminary success,\nchallenges persist in areas such as data scarcity, quality, and ensuring a\nsolid foundation in psychological techniques. To address these challenges, this\nstudy introduces a novel approach to enhance the precision and efficacy of\npsychological support through large language models. Specifically, we design a\nspecific prompt derived from principles of Cognitive Behavioral Therapy (CBT)\nand have generated the CBT QA dataset, specifically for Chinese psychological\nhealth Q&A based on CBT structured intervention strategies. Unlike previous\nmethods, our dataset emphasizes professional and structured response. Utilizing\nthis dataset, we fine-tuned the large language model, giving birth to CBT-LLM,\nthe large-scale language model specifically designed for Cognitive Behavioral\nTherapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in\ngenerating structured, professional, and highly relevant responses in\npsychological health support tasks, showcasing its practicality and quality.\nThe model is available on Hugging Face:\nhttps://huggingface.co/Hongbin37/CBT-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.16008v1.pdf"
    },
    {
        "title": "CIC: A framework for Culturally-aware Image Captioning",
        "authors": [
            "Youngsik Yun",
            "Jihie Kim"
        ],
        "published": "2024-02-08T03:12:25Z",
        "summary": "Image Captioning generates descriptive sentences from images using\nVision-Language Pre-trained models (VLPs) such as BLIP, which has improved\ngreatly. However, current methods lack the generation of detailed descriptive\ncaptions for the cultural elements depicted in the images, such as the\ntraditional clothing worn by people from Asian cultural groups. In this paper,\nwe propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)},\nthat generates captions and describes cultural elements extracted from cultural\nvisual elements in images representing cultures. Inspired by methods combining\nvisual modality and Large Language Models (LLMs) through appropriate prompts,\nour framework (1) generates questions based on cultural categories from images,\n(2) extracts cultural visual elements from Visual Question Answering (VQA)\nusing generated questions, and (3) generates culturally-aware captions using\nLLMs with the prompts. Our human evaluation conducted on 45 participants from 4\ndifferent cultural groups with a high understanding of the corresponding\nculture shows that our proposed framework generates more culturally descriptive\ncaptions when compared to the image captioning baseline based on VLPs. Our code\nand dataset will be made publicly available upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2402.05374v1.pdf"
    },
    {
        "title": "Can Large Language Models Detect Rumors on Social Media?",
        "authors": [
            "Qiang Liu",
            "Xiang Tao",
            "Junfei Wu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published": "2024-02-06T11:33:57Z",
        "summary": "In this work, we investigate to use Large Language Models (LLMs) for rumor\ndetection on social media. However, it is challenging for LLMs to reason over\nthe entire propagation information on social media, which contains news\ncontents and numerous comments, due to LLMs may not concentrate on key clues in\nthe complex propagation information, and have trouble in reasoning when facing\nmassive and redundant information. Accordingly, we propose an LLM-empowered\nRumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to\nreason over important clues in news and comments, and divide the entire\npropagation information into a Chain-of-Propagation for reducing LLMs' burden.\nWe conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD\noutperforms several state-of-the-art rumor detection models by 3.2% to 7.7%.\nMeanwhile, by applying LLMs, LeRuD requires no data for training, and thus\nshows more promising rumor detection ability in few-shot or zero-shot\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.03916v2.pdf"
    },
    {
        "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
        "authors": [
            "Yanis Labrak",
            "Adrien Bazoge",
            "Emmanuel Morin",
            "Pierre-Antoine Gourraud",
            "Mickael Rouvier",
            "Richard Dufour"
        ],
        "published": "2024-02-15T23:39:04Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility in\nrecent years, offering potential applications across specialized domains such\nas healthcare and medicine. Despite the availability of various open-source\nLLMs tailored for health contexts, adapting general-purpose LLMs to the medical\ndomain presents significant challenges. In this paper, we introduce BioMistral,\nan open-source LLM tailored for the biomedical domain, utilizing Mistral as its\nfoundation model and further pre-trained on PubMed Central. We conduct a\ncomprehensive evaluation of BioMistral on a benchmark comprising 10 established\nmedical question-answering (QA) tasks in English. We also explore lightweight\nmodels obtained through quantization and model merging approaches. Our results\ndemonstrate BioMistral's superior performance compared to existing open-source\nmedical models and its competitive edge against proprietary counterparts.\nFinally, to address the limited availability of data beyond English and to\nassess the multilingual generalization of medical LLMs, we automatically\ntranslated and evaluated this benchmark into 7 other languages. This marks the\nfirst large-scale multilingual evaluation of LLMs in the medical domain.\nDatasets, multilingual evaluation benchmarks, scripts, and all the models\nobtained during our experiments are freely released.",
        "pdf_link": "https://arxiv.org/pdf/2402.10373v1.pdf"
    },
    {
        "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
        "authors": [
            "Hakim Sidahmed",
            "Samrat Phatale",
            "Alex Hutcheson",
            "Zhuonan Lin",
            "Zhang Chen",
            "Zac Yu",
            "Jarvis Jin",
            "Roman Komarytsia",
            "Christiane Ahlheim",
            "Yonghao Zhu",
            "Simral Chaudhary",
            "Bowen Li",
            "Saravanan Ganesh",
            "Bill Byrne",
            "Jessica Hoffmann",
            "Hassan Mansoor",
            "Wei Li",
            "Abhinav Rastogi",
            "Lucas Dixon"
        ],
        "published": "2024-03-15T21:43:46Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong\nmethod to align Pretrained Large Language Models (LLMs) with human preferences.\nBut training models with RLHF is computationally expensive, and an overall\ncomplex process. In this work, we study RLHF where the underlying models are\ntrained using the parameter efficient method of Low-Rank Adaptation (LoRA)\nintroduced by Hu et al. [2021]. We investigate the setup of \"Parameter\nEfficient Reinforcement Learning\" (PERL), in which we perform reward model\ntraining and reinforcement learning using LoRA. We compare PERL to conventional\nfine-tuning (full-tuning) across various configurations for 7 benchmarks,\nincluding 2 novel datasets, of reward modeling and reinforcement learning. We\nfind that PERL performs on par with the conventional RLHF setting, while\ntraining faster, and with less memory. This enables the high performance of\nRLHF, while reducing the computational burden that limits its adoption as an\nalignment technique for Large Language Models. We also release 2 novel thumbs\nup/down preference datasets: \"Taskmaster Coffee\", and \"Taskmaster Ticketing\" to\npromote research around RLHF.",
        "pdf_link": "https://arxiv.org/pdf/2403.10704v1.pdf"
    },
    {
        "title": "Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions",
        "authors": [
            "Guangya Wan",
            "Yuqi Wu",
            "Mengxuan Hu",
            "Zhixuan Chu",
            "Sheng Li"
        ],
        "published": "2024-02-16T20:48:53Z",
        "summary": "Causal discovery (CD) and Large Language Models (LLMs) represent two emerging\nfields of study with significant implications for artificial intelligence.\nDespite their distinct origins, CD focuses on uncovering cause-effect\nrelationships from data, and LLMs on processing and generating humanlike text,\nthe convergence of these domains offers novel insights and methodologies for\nunderstanding complex systems. This paper presents a comprehensive survey of\nthe integration of LLMs, such as GPT4, into CD tasks. We systematically review\nand compare existing approaches that leverage LLMs for various CD tasks and\nhighlight their innovative use of metadata and natural language to infer causal\nstructures. Our analysis reveals the strengths and potential of LLMs in both\nenhancing traditional CD methods and as an imperfect expert, alongside the\nchallenges and limitations inherent in current practices. Furthermore, we\nidentify gaps in the literature and propose future research directions aimed at\nharnessing the full potential of LLMs in causality research. To our knowledge,\nthis is the first survey to offer a unified and detailed examination of the\nsynergy between LLMs and CD, setting the stage for future advancements in the\nfield.",
        "pdf_link": "https://arxiv.org/pdf/2402.11068v1.pdf"
    },
    {
        "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models",
        "authors": [
            "Terry Yue Zhuo",
            "Armel Zebaze",
            "Nitchakarn Suppattarachai",
            "Leandro von Werra",
            "Harm de Vries",
            "Qian Liu",
            "Niklas Muennighoff"
        ],
        "published": "2024-01-01T15:30:19Z",
        "summary": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.00788v1.pdf"
    },
    {
        "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
        "authors": [
            "Yuxuan Yue",
            "Zhihang Yuan",
            "Haojie Duanmu",
            "Sifan Zhou",
            "Jianlong Wu",
            "Liqiang Nie"
        ],
        "published": "2024-02-19T11:33:21Z",
        "summary": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization.",
        "pdf_link": "https://arxiv.org/pdf/2402.12065v2.pdf"
    },
    {
        "title": "Chatbot Meets Pipeline: Augment Large Language Model with Definite Finite Automaton",
        "authors": [
            "Yiyou Sun",
            "Junjie Hu",
            "Wei Cheng",
            "Haifeng Chen"
        ],
        "published": "2024-02-06T21:14:45Z",
        "summary": "This paper introduces the Definite Finite Automaton augmented large language\nmodel (DFA-LLM), a novel framework designed to enhance the capabilities of\nconversational agents using large language models (LLMs). Traditional LLMs face\nchallenges in generating regulated and compliant responses in special scenarios\nwith predetermined response guidelines, like emotional support and customer\nservice. Our framework addresses these challenges by embedding a Definite\nFinite Automaton (DFA), learned from training dialogues, within the LLM. This\nstructured approach enables the LLM to adhere to a deterministic response\npathway, guided by the DFA. The advantages of DFA-LLM include an interpretable\nstructure through human-readable DFA, context-aware retrieval for responses in\nconversations, and plug-and-play compatibility with existing LLMs. Extensive\nbenchmarks validate DFA-LLM's effectiveness, indicating its potential as a\nvaluable contribution to the conversational agent.",
        "pdf_link": "https://arxiv.org/pdf/2402.04411v1.pdf"
    },
    {
        "title": "Are Language Models More Like Libraries or Like Librarians? Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs",
        "authors": [
            "Harvey Lederman",
            "Kyle Mahowald"
        ],
        "published": "2024-01-10T00:05:45Z",
        "summary": "Are LLMs cultural technologies like photocopiers or printing presses, which\ntransmit information but cannot create new content? A challenge for this idea,\nwhich we call bibliotechnism, is that LLMs often generate entirely novel text.\nWe begin (Part I) with a sustained defense of bibliotechnism against this\nchallenge showing how even entirely novel text may be meaningful only in a\nderivative sense, and arguing that, in particular, much novel text generated by\nLLMs is only derivatively meaningful. But we argue (Part II) that\nbibliotechnism faces a different, novel challenge, stemming from examples in\nwhich LLMs generate \"novel reference\", using novel names to refer to novel\nentities. Such examples could be smoothly explained if LLMs were not cultural\ntechnologies but possessed a limited form of agency (beliefs, desires, and\nintentions). According to interpretationism in the philosophy of mind, a system\nhas beliefs, desires and intentions if and only if its behavior is well\nexplained by the hypothesis that it has such states. So, according to\ninterpretationism, cases of novel reference provide evidence that LLMs have\nbeliefs, desires, and intentions. Given that interpretationism is a live\nhypothesis about the nature of these states, we suggest that cases of novel\nreference provide evidence that LLMs do have beliefs, desires, and intentions.",
        "pdf_link": "https://arxiv.org/pdf/2401.04854v2.pdf"
    },
    {
        "title": "CleanAgent: Automating Data Standardization with LLM-based Agents",
        "authors": [
            "Danrui Qi",
            "Jiannan Wang"
        ],
        "published": "2024-03-13T06:54:15Z",
        "summary": "Data standardization is a crucial part in data science life cycle. While\ntools like Pandas offer robust functionalities, their complexity and the manual\neffort required for customizing code to diverse column types pose significant\nchallenges. Although large language models (LLMs) like ChatGPT have shown\npromise in automating this process through natural language understanding and\ncode generation, it still demands expert-level programming knowledge and\ncontinuous interaction for prompt refinement. To solve these challenges, our\nkey idea is to propose a Python library with declarative, unified APIs for\nstandardizing column types, simplifying the code generation of LLM with concise\nAPI calls. We first propose Dataprep.Clean which is written as a component of\nthe Dataprep Library, offers a significant reduction in complexity by enabling\nthe standardization of specific column types with a single line of code. Then\nwe introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based\nagents to automate the data standardization process. With CleanAgent, data\nscientists need only provide their requirements once, allowing for a\nhands-free, automatic standardization process.",
        "pdf_link": "https://arxiv.org/pdf/2403.08291v1.pdf"
    },
    {
        "title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability",
        "authors": [
            "Congying Xia",
            "Chen Xing",
            "Jiangshu Du",
            "Xinyi Yang",
            "Yihao Feng",
            "Ran Xu",
            "Wenpeng Yin",
            "Caiming Xiong"
        ],
        "published": "2024-02-28T19:23:27Z",
        "summary": "This paper presents FoFo, a pioneering benchmark for evaluating large\nlanguage models' (LLMs) ability to follow complex, domain-specific formats, a\ncrucial yet underexamined capability for their application as AI agents.\nDespite LLMs' advancements, existing benchmarks fail to assess their\nformat-following proficiency adequately. FoFo fills this gap with a diverse\nrange of real-world formats and instructions, developed through an AI-Human\ncollaborative method. Our evaluation across both open-source (e.g., Llama 2,\nWizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three\nkey findings: open-source models significantly lag behind closed-source ones in\nformat adherence; LLMs' format-following performance is independent of their\ncontent generation quality; and LLMs' format proficiency varies across\ndifferent domains. These insights suggest the need for specialized tuning for\nformat-following skills and highlight FoFo's role in guiding the selection of\ndomain-specific AI agents. FoFo is released here at\nhttps://github.com/SalesforceAIResearch/FoFo.",
        "pdf_link": "https://arxiv.org/pdf/2402.18667v1.pdf"
    },
    {
        "title": "DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling",
        "authors": [
            "Shanghaoran Quan"
        ],
        "published": "2024-03-02T12:31:22Z",
        "summary": "The performance of the reward model (RM) is a critical factor in improving\nthe effectiveness of the large language model (LLM) during alignment\nfine-tuning. There remain two challenges in RM training: 1) training the same\nRM using various categories of data may cause its generalization performance to\nsuffer from multi-task disturbance, and 2) the human annotation consistency\nrate is generally only $60\\%$ to $75\\%$, causing training data to contain a lot\nof noise. To tackle these two challenges, we introduced the idea of\nMixture-of-Experts (MoE) into the field of RM for the first time. We propose\nthe Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After\nclassifying an input into task categories, we route it to the corresponding\ninner layer task-specific model. The inner layer MoE is a dense model. We\ndecompose the specific task into multiple capability dimensions and\nindividually fine-tune a LoRA expert on each one. Their outputs are then\nsynthesized by an MLP to compute the final rewards. To minimize costs, we call\na public LLM API to obtain the capability preference labels. The validation on\nmanually labeled datasets confirms that our model attains superior consistency\nwith human preference and outstrips advanced generative approaches. Meanwhile,\nthrough BoN sampling and RL experiments, we demonstrate that our model\noutperforms state-of-the-art ensemble methods of RM and mitigates the\noveroptimization problem. Our code and dataset are available at:\nhttps://github.com/quanshr/DMoERM-v1.",
        "pdf_link": "https://arxiv.org/pdf/2403.01197v1.pdf"
    },
    {
        "title": "Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions",
        "authors": [
            "Xuming Hu",
            "Xiaochuan Li",
            "Junzhe Chen",
            "Yinghui Li",
            "Yangning Li",
            "Xiaoguang Li",
            "Yasheng Wang",
            "Qun Liu",
            "Lijie Wen",
            "Philip S. Yu",
            "Zhijiang Guo"
        ],
        "published": "2024-02-25T11:22:19Z",
        "summary": "Generative search engines have the potential to transform how people seek\ninformation online, but generated responses from existing large language models\n(LLMs)-backed generative search engines may not always be accurate.\nNonetheless, retrieval-augmented generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable part of a claim. To this end, we propose evaluating the\nrobustness of generative search engines in the realistic and high-risk setting,\nwhere adversaries have only black-box system access and seek to deceive the\nmodel into returning incorrect responses. Through a comprehensive human\nevaluation of various generative search engines, such as Bing Chat,\nPerplexityAI, and YouChat across diverse queries, we demonstrate the\neffectiveness of adversarial factual questions in inducing incorrect responses.\nMoreover, retrieval-augmented generation exhibits a higher susceptibility to\nfactual errors compared to LLMs without retrieval. These findings highlight the\npotential security risks of these systems and emphasize the need for rigorous\nevaluation before deployment.",
        "pdf_link": "https://arxiv.org/pdf/2403.12077v1.pdf"
    },
    {
        "title": "ArabicaQA: A Comprehensive Dataset for Arabic Question Answering",
        "authors": [
            "Abdelrahman Abdallah",
            "Mahmoud Kasem",
            "Mahmoud Abdalla",
            "Mohamed Mahmoud",
            "Mohamed Elkasaby",
            "Yasser Elbendary",
            "Adam Jatowt"
        ],
        "published": "2024-03-26T16:37:54Z",
        "summary": "In this paper, we address the significant gap in Arabic natural language\nprocessing (NLP) resources by introducing ArabicaQA, the first large-scale\ndataset for machine reading comprehension and open-domain question answering in\nArabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701\nunanswerable questions created by crowdworkers to look similar to answerable\nones, along with additional labels of open-domain questions marks a crucial\nadvancement in Arabic NLP resources. We also present AraDPR, the first dense\npassage retrieval model trained on the Arabic Wikipedia corpus, specifically\ndesigned to tackle the unique challenges of Arabic text retrieval. Furthermore,\nour study includes extensive benchmarking of large language models (LLMs) for\nArabic question answering, critically evaluating their performance in the\nArabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking\nof LLMs in Arabic question answering offer significant advancements in the\nfield of Arabic NLP. The dataset and code are publicly accessible for further\nresearch https://github.com/DataScienceUIBK/ArabicaQA.",
        "pdf_link": "https://arxiv.org/pdf/2403.17848v1.pdf"
    },
    {
        "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
        "authors": [
            "Zhangqian Bi",
            "Yao Wan",
            "Zheng Wang",
            "Hongyu Zhang",
            "Batu Guan",
            "Fangxin Lu",
            "Zili Zhang",
            "Yulei Sui",
            "Xuanhua Shi",
            "Hai Jin"
        ],
        "published": "2024-03-25T14:07:27Z",
        "summary": "Large language models (LLMs) have shown remarkable progress in automated code\ngeneration. Yet, incorporating LLM-based code generation into real-life\nsoftware projects poses challenges, as the generated code may contain errors in\nAPI usage, class, data structure, or missing project-specific information. As\nmuch of this project-specific context cannot fit into the prompts of LLMs, we\nmust find ways to allow the model to explore the project-level code context. To\nthis end, this paper puts forward a novel approach, termed ProCoder, which\niteratively refines the project-level code context for precise code generation,\nguided by the compiler feedback. In particular, ProCoder first leverages\ncompiler techniques to identify a mismatch between the generated code and the\nproject's context. It then iteratively aligns and fixes the identified errors\nusing information extracted from the code repository. We integrate ProCoder\nwith two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and\napply it to Python code generation. Experimental results show that ProCoder\nsignificantly improves the vanilla LLMs by over 80% in generating code\ndependent on project context, and consistently outperforms the existing\nretrieval-based code generation baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.16792v2.pdf"
    },
    {
        "title": "UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All",
        "authors": [
            "Yuanhuiyi Lyu",
            "Xu Zheng",
            "Jiazhou Zhou",
            "Lin Wang"
        ],
        "published": "2024-03-19T08:09:27Z",
        "summary": "We present UniBind, a flexible and efficient approach that learns a unified\nrepresentation space for seven diverse modalities -- images, text, audio, point\ncloud, thermal, video, and event data. Existing works, eg., ImageBind, treat\nthe image as the central modality and build an image-centered representation\nspace; however, the space may be sub-optimal as it leads to an unbalanced\nrepresentation space among all modalities. Moreover, the category names are\ndirectly used to extract text embeddings for the downstream tasks, making it\nhardly possible to represent the semantics of multi-modal data. The\n'out-of-the-box' insight of our UniBind is to make the alignment center\nmodality-agnostic and further learn a unified and balanced representation\nspace, empowered by the large language models (LLMs). UniBind is superior in\nits flexible application to all CLIP-style models and delivers remarkable\nperformance boosts. To make this possible, we 1) construct a knowledge base of\ntext embeddings with the help of LLMs and multi-modal LLMs; 2) adaptively build\nLLM-augmented class-wise embedding center on top of the knowledge base and\nencoded visual embeddings; 3) align all the embeddings to the LLM-augmented\nembedding center via contrastive learning to achieve a unified and balanced\nrepresentation space. UniBind shows strong zero-shot recognition performance\ngains over prior arts by an average of 6.36%. Finally, we achieve new\nstate-of-the-art performance, eg., a 6.75% gain on ImageNet, on the multi-modal\nfine-tuning setting while reducing 90% of the learnable parameters.",
        "pdf_link": "https://arxiv.org/pdf/2403.12532v1.pdf"
    },
    {
        "title": "Can Small Language Models be Good Reasoners for Sequential Recommendation?",
        "authors": [
            "Yuling Wang",
            "Changxin Tian",
            "Binbin Hu",
            "Yanhua Yu",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou",
            "Liang Pang",
            "Xiao Wang"
        ],
        "published": "2024-03-07T06:49:37Z",
        "summary": "Large language models (LLMs) open up new horizons for sequential\nrecommendations, owing to their remarkable language comprehension and\ngeneration capabilities. However, there are still numerous challenges that\nshould be addressed to successfully implement sequential recommendations\nempowered by LLMs. Firstly, user behavior patterns are often complex, and\nrelying solely on one-step reasoning from LLMs may lead to incorrect or\ntask-irrelevant responses. Secondly, the prohibitively resource requirements of\nLLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real\nsequential recommender systems. In this paper, we propose a novel Step-by-step\nknowLedge dIstillation fraMework for recommendation (SLIM), paving a promising\npath for sequential recommenders to enjoy the exceptional reasoning\ncapabilities of LLMs in a \"slim\" (i.e., resource-efficient) manner. We\nintroduce CoT prompting based on user behavior sequences for the larger teacher\nmodel. The rationales generated by the teacher model are then utilized as\nlabels to distill the downstream smaller student model (e.g., LLaMA2-7B). In\nthis way, the student model acquires the step-by-step reasoning capabilities in\nrecommendation tasks. We encode the generated rationales from the student model\ninto a dense vector, which empowers recommendation in both ID-based and\nID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of\nSLIM over state-of-the-art baselines, and further analysis showcasing its\nability to generate meaningful recommendation reasoning at affordable costs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04260v2.pdf"
    },
    {
        "title": "Assessing and Understanding Creativity in Large Language Models",
        "authors": [
            "Yunpu Zhao",
            "Rui Zhang",
            "Wenyi Li",
            "Di Huang",
            "Jiaming Guo",
            "Shaohui Peng",
            "Yifan Hao",
            "Yuanbo Wen",
            "Xing Hu",
            "Zidong Du",
            "Qi Guo",
            "Ling Li",
            "Yunji Chen"
        ],
        "published": "2024-01-23T05:19:47Z",
        "summary": "In the field of natural language processing, the rapid development of large\nlanguage model (LLM) has attracted more and more attention. LLMs have shown a\nhigh level of creativity in various tasks, but the methods for assessing such\ncreativity are inadequate. The assessment of LLM creativity needs to consider\ndifferences from humans, requiring multi-dimensional measurement while\nbalancing accuracy and efficiency. This paper aims to establish an efficient\nframework for assessing the level of creativity in LLMs. By adapting the\nmodified Torrance Tests of Creative Thinking, the research evaluates the\ncreative performance of various LLMs across 7 tasks, emphasizing 4 criteria\nincluding Fluency, Flexibility, Originality, and Elaboration. In this context,\nwe develop a comprehensive dataset of 700 questions for testing and an\nLLM-based evaluation method. In addition, this study presents a novel analysis\nof LLMs' responses to diverse prompts and role-play situations. We found that\nthe creativity of LLMs primarily falls short in originality, while excelling in\nelaboration. Besides, the use of prompts and the role-play settings of the\nmodel significantly influence creativity. Additionally, the experimental\nresults also indicate that collaboration among multiple LLMs can enhance\noriginality. Notably, our findings reveal a consensus between human evaluations\nand LLMs regarding the personality traits that influence creativity. The\nfindings underscore the significant impact of LLM design on creativity and\nbridges artificial intelligence and human creativity, offering insights into\nLLMs' creativity and potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.12491v1.pdf"
    },
    {
        "title": "AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation",
        "authors": [
            "Yasheng Sun",
            "Wenqing Chu",
            "Hang Zhou",
            "Kaisiyuan Wang",
            "Hideki Koike"
        ],
        "published": "2024-02-25T15:51:05Z",
        "summary": "While considerable progress has been made in achieving accurate lip\nsynchronization for 3D speech-driven talking face generation, the task of\nincorporating expressive facial detail synthesis aligned with the speaker's\nspeaking status remains challenging. Our goal is to directly leverage the\ninherent style information conveyed by human speech for generating an\nexpressive talking face that aligns with the speaking status. In this paper, we\npropose AVI-Talking, an Audio-Visual Instruction system for expressive Talking\nface generation. This system harnesses the robust contextual reasoning and\nhallucination capability offered by Large Language Models (LLMs) to instruct\nthe realistic synthesis of 3D talking faces. Instead of directly learning\nfacial movements from human speech, our two-stage strategy involves the LLMs\nfirst comprehending audio information and generating instructions implying\nexpressive facial details seamlessly corresponding to the speech. Subsequently,\na diffusion-based generative network executes these instructions. This\ntwo-stage process, coupled with the incorporation of LLMs, enhances model\ninterpretability and provides users with flexibility to comprehend instructions\nand specify desired operations or modifications. Extensive experiments showcase\nthe effectiveness of our approach in producing vivid talking faces with\nexpressive facial movements and consistent emotional status.",
        "pdf_link": "https://arxiv.org/pdf/2402.16124v1.pdf"
    },
    {
        "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
        "authors": [
            "Yue Fan",
            "Xiaojian Ma",
            "Rujie Wu",
            "Yuntao Du",
            "Jiaqi Li",
            "Zhi Gao",
            "Qing Li"
        ],
        "published": "2024-03-18T05:07:59Z",
        "summary": "We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.",
        "pdf_link": "https://arxiv.org/pdf/2403.11481v1.pdf"
    },
    {
        "title": "Predicting Machine Translation Performance on Low-Resource Languages: The Role of Domain Similarity",
        "authors": [
            "Eric Khiu",
            "Hasti Toossi",
            "David Anugraha",
            "Jinyu Liu",
            "Jiaxu Li",
            "Juan Armando Parra Flores",
            "Leandro Acros Roman",
            "A. Seza Do\u011fru\u00f6z",
            "En-Shiun Annie Lee"
        ],
        "published": "2024-02-04T22:56:56Z",
        "summary": "Fine-tuning and testing a multilingual large language model is expensive and\nchallenging for low-resource languages (LRLs). While previous studies have\npredicted the performance of natural language processing (NLP) tasks using\nmachine learning methods, they primarily focus on high-resource languages,\noverlooking LRLs and shifts across domains. Focusing on LRLs, we investigate\nthree factors: the size of the fine-tuning corpus, the domain similarity\nbetween fine-tuning and testing corpora, and the language similarity between\nsource and target languages. We employ classical regression models to assess\nhow these factors impact the model's performance. Our results indicate that\ndomain similarity has the most critical impact on predicting the performance of\nMachine Translation models.",
        "pdf_link": "https://arxiv.org/pdf/2402.02633v1.pdf"
    },
    {
        "title": "generAItor: Tree-in-the-Loop Text Generation for Language Model Explainability and Adaptation",
        "authors": [
            "Thilo Spinner",
            "Rebecca Kehlbeck",
            "Rita Sevastjanova",
            "Tobias St\u00e4hle",
            "Daniel A. Keim",
            "Oliver Deussen",
            "Mennatallah El-Assady"
        ],
        "published": "2024-03-12T13:09:15Z",
        "summary": "Large language models (LLMs) are widely deployed in various downstream tasks,\ne.g., auto-completion, aided writing, or chat-based text generation. However,\nthe considered output candidates of the underlying search algorithm are\nunder-explored and under-explained. We tackle this shortcoming by proposing a\ntree-in-the-loop approach, where a visual representation of the beam search\ntree is the central component for analyzing, explaining, and adapting the\ngenerated outputs. To support these tasks, we present generAItor, a visual\nanalytics technique, augmenting the central beam search tree with various\ntask-specific widgets, providing targeted visualizations and interaction\npossibilities. Our approach allows interactions on multiple levels and offers\nan iterative pipeline that encompasses generating, exploring, and comparing\noutput candidates, as well as fine-tuning the model based on adapted data. Our\ncase study shows that our tool generates new insights in gender bias analysis\nbeyond state-of-the-art template-based methods. Additionally, we demonstrate\nthe applicability of our approach in a qualitative user study. Finally, we\nquantitatively evaluate the adaptability of the model to few samples, as\noccurring in text-generation use cases.",
        "pdf_link": "https://arxiv.org/pdf/2403.07627v1.pdf"
    },
    {
        "title": "LLM on FHIR -- Demystifying Health Records",
        "authors": [
            "Paul Schmiedmayer",
            "Adrit Rao",
            "Philipp Zagar",
            "Vishnu Ravi",
            "Aydin Zahedivash",
            "Arash Fereydooni",
            "Oliver Aalami"
        ],
        "published": "2024-01-25T17:45:34Z",
        "summary": "Objective: To enhance health literacy and accessibility of health information\nfor a diverse patient population by developing a patient-centered artificial\nintelligence (AI) solution using large language models (LLMs) and Fast\nHealthcare Interoperability Resources (FHIR) application programming interfaces\n(APIs). Materials and Methods: The research involved developing LLM on FHIR, an\nopen-source mobile application allowing users to interact with their health\nrecords using LLMs. The app is built on Stanford's Spezi ecosystem and uses\nOpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient\ndataset and evaluated by medical experts to assess the app's effectiveness in\nincreasing health literacy. The evaluation focused on the accuracy, relevance,\nand understandability of the LLM's responses to common patient questions.\nResults: LLM on FHIR demonstrated varying but generally high degrees of\naccuracy and relevance in providing understandable health information to\npatients. The app effectively translated medical data into patient-friendly\nlanguage and was able to adapt its responses to different patient profiles.\nHowever, challenges included variability in LLM responses and the need for\nprecise filtering of health data. Discussion and Conclusion: LLMs offer\nsignificant potential in improving health literacy and making health records\nmore accessible. LLM on FHIR, as a pioneering application in this field,\ndemonstrates the feasibility and challenges of integrating LLMs into patient\ncare. While promising, the implementation and pilot also highlight risks such\nas inconsistent responses and the importance of replicable output. Future\ndirections include better resource identification mechanisms and executing LLMs\non-device to enhance privacy and reduce costs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01711v1.pdf"
    },
    {
        "title": "Large Language Model Evaluation via Matrix Entropy",
        "authors": [
            "Lai Wei",
            "Zhiquan Tan",
            "Chenghai Li",
            "Jindong Wang",
            "Weiran Huang"
        ],
        "published": "2024-01-30T16:19:55Z",
        "summary": "Large language models (LLMs) have revolutionized the field of natural\nlanguage processing, extending their strong capabilities into multi-modal\ndomains. Thus, it is vital to define proper and diversified metrics for the\nevaluation of LLMs.\n  In this paper, we introduce matrix entropy, a novel metric rooted in\ninformation theory and geometry principles to quantify the data compression\nproficiency in LLMs. It reflects the model's ability to extract relevant\ninformation and eliminate unnecessary elements, thereby providing insight into\nthe language model's intrinsic capability. Specifically, we demonstrate its\napplicability in both single-modal (language) and multi-modal settings. For\nlanguage models, our findings reveal that the matrix entropy of representations\nfollows a scaling law type reduction when the model scales up, serving as a\ncomplement to the traditional loss scaling law. For the multi-modal setting, we\nalso propose an evaluation method based on matrix entropy for assessing\nalignment quality and we find that modern large multi-modal models exhibit\ngreat alignment performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.17139v1.pdf"
    },
    {
        "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents",
        "authors": [
            "Kaijie Zhu",
            "Jindong Wang",
            "Qinlin Zhao",
            "Ruochen Xu",
            "Xing Xie"
        ],
        "published": "2024-02-21T06:46:34Z",
        "summary": "Evaluation of large language models (LLMs) has raised great concerns in the\ncommunity due to the issue of data contamination. Existing work designed\nevaluation protocols using well-defined algorithms for specific tasks, which\ncannot be easily extended to diverse scenarios. Moreover, current evaluation\nbenchmarks can only provide the overall benchmark results and cannot support a\nfine-grained and multifaceted analysis of LLMs' abilities. In this paper, we\npropose meta probing agents (MPA), a general dynamic evaluation protocol\ninspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal\n2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs\nthe probing and judging agents to automatically transform an original\nevaluation problem into a new one following psychometric theory on three basic\ncognitive abilities: language understanding, problem solving, and domain\nknowledge. These basic abilities are also dynamically configurable, allowing\nmultifaceted analysis. We conducted extensive evaluations using MPA and found\nthat most LLMs achieve poorer performance, indicating room for improvement. Our\nmultifaceted analysis demonstrated the strong correlation between the basic\nabilities and an implicit Matthew effect on model size, i.e., larger models\npossess stronger correlations of the abilities. MPA can also be used as a data\naugmentation approach to enhance LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14865v1.pdf"
    },
    {
        "title": "Infusing Knowledge into Large Language Models with Contextual Prompts",
        "authors": [
            "Kinshuk Vasisht",
            "Balaji Ganesan",
            "Vikas Kumar",
            "Vasudha Bhatnagar"
        ],
        "published": "2024-03-03T11:19:26Z",
        "summary": "Knowledge infusion is a promising method for enhancing Large Language Models\nfor domain-specific NLP tasks rather than pre-training models over large data\nfrom scratch. These augmented LLMs typically depend on additional pre-training\nor knowledge prompts from an existing knowledge graph, which is impractical in\nmany applications. In contrast, knowledge infusion directly from relevant\ndocuments is more generalisable and alleviates the need for structured\nknowledge graphs while also being useful for entities that are usually not\nfound in any knowledge graph. With this motivation, we propose a simple yet\ngeneralisable approach for knowledge infusion by generating prompts from the\ncontext in the input text. Our experiments show the effectiveness of our\napproach which we evaluate by probing the fine-tuned LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01481v1.pdf"
    },
    {
        "title": "DEEP-ICL: Definition-Enriched Experts for Language Model In-Context Learning",
        "authors": [
            "Xingwei Qu",
            "Yiming Liang",
            "Yucheng Wang",
            "Tianyu Zheng",
            "Tommy Yue",
            "Lei Ma",
            "Stephen W. Huang",
            "Jiajun Zhang",
            "Wenhu Chen",
            "Chenghua Lin",
            "Jie Fu",
            "Ge Zhang"
        ],
        "published": "2024-03-07T05:26:41Z",
        "summary": "It has long been assumed that the sheer number of parameters in large\nlanguage models (LLMs) drives in-context learning (ICL) capabilities, enabling\nremarkable performance improvements by leveraging task-specific demonstrations.\nChallenging this hypothesis, we introduce DEEP-ICL, a novel task Definition\nEnriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts\ntask definitions from given demonstrations and generates responses through\nlearning task-specific examples. We argue that improvement from ICL does not\ndirectly rely on model size, but essentially stems from understanding task\ndefinitions and task-guided learning. Inspired by this, DEEP-ICL combines two\n3B models with distinct roles (one for concluding task definitions and the\nother for learning task demonstrations) and achieves comparable performance to\nLLaMA2-13B. Furthermore, our framework outperforms conventional ICL by\novercoming pretraining sequence length limitations, by supporting unlimited\ndemonstrations. We contend that DEEP-ICL presents a novel alternative for\nachieving efficient few-shot learning, extending beyond the conventional ICL.",
        "pdf_link": "https://arxiv.org/pdf/2403.04233v1.pdf"
    },
    {
        "title": "Analysis of Privacy Leakage in Federated Large Language Models",
        "authors": [
            "Minh N. Vu",
            "Truc Nguyen",
            "Tre' R. Jeter",
            "My T. Thai"
        ],
        "published": "2024-03-02T20:25:38Z",
        "summary": "With the rapid adoption of Federated Learning (FL) as the training and tuning\nprotocol for applications utilizing Large Language Models (LLMs), recent\nresearch highlights the need for significant modifications to FL to accommodate\nthe large-scale of LLMs. While substantial adjustments to the protocol have\nbeen introduced as a response, comprehensive privacy analysis for the adapted\nFL protocol is currently lacking.\n  To address this gap, our work delves into an extensive examination of the\nprivacy analysis of FL when used for training LLMs, both from theoretical and\npractical perspectives. In particular, we design two active membership\ninference attacks with guaranteed theoretical success rates to assess the\nprivacy leakages of various adapted FL configurations. Our theoretical findings\nare translated into practical attacks, revealing substantial privacy\nvulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and\nOpenAI's GPTs, across multiple real-world language datasets. Additionally, we\nconduct thorough experiments to evaluate the privacy leakage of these models\nwhen data is protected by state-of-the-art differential privacy (DP)\nmechanisms.",
        "pdf_link": "https://arxiv.org/pdf/2403.04784v1.pdf"
    },
    {
        "title": "SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation",
        "authors": [
            "Xue Jiang",
            "Yihong Dong",
            "Zhi Jin",
            "Ge Li"
        ],
        "published": "2024-02-29T16:09:02Z",
        "summary": "Although Large Language Models (LLMs) have made significant progress in code\ngeneration, they still struggle with code generation tasks in specific\nscenarios. These scenarios usually necessitate the adaptation of LLMs to\nfulfill specific needs, but the limited training samples available in practice\nlead to poor code generation performance. Therefore, how to effectively adapt\nLLMs to new scenarios with few training samples is a major challenge for\ncurrent code generation. In this paper, we propose a novel adaptation approach\nnamed SEED, which stands for Sample-Efficient adaptation with Error-Driven\nlearning for code generation. SEED leverages the errors made by LLMs as\nlearning opportunities, using error revision to overcome its own shortcomings,\nthus achieving efficient learning. Specifically, SEED involves identifying\nerror code generated by LLMs, employing Self-revise for code revision,\noptimizing the model with revised code, and iteratively adapting the process\nfor continuous improvement. Experimental results show that, compared to other\nmainstream fine-tuning approaches, SEED achieves superior performance with few\ntraining samples, showing an average relative improvement of 54.7% in Pass@1 on\nmultiple code generation benchmarks. We also validate the effectiveness of\nSelf-revise, which generates revised code that optimizes the model more\nefficiently compared to the code samples from datasets. Moreover, SEED\nconsistently demonstrates strong performance across various LLMs, underscoring\nits generalizability.",
        "pdf_link": "https://arxiv.org/pdf/2403.00046v2.pdf"
    },
    {
        "title": "Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm",
        "authors": [
            "Yuanzhen Xie",
            "Xinzhou Jin",
            "Tao Xie",
            "MingXiong Lin",
            "Liang Chen",
            "Chenyun Yu",
            "Lei Cheng",
            "ChengXiang Zhuo",
            "Bo Hu",
            "Zang Li"
        ],
        "published": "2024-02-16T13:24:05Z",
        "summary": "In-context learning of large-language models (LLMs) has achieved remarkable\nsuccess in the field of natural language processing, while extensive case\nstudies reveal that the single-step chain-of-thought prompting approach faces\nchallenges such as attention diffusion and inadequate performance in complex\ntasks like text-to-SQL. To improve the contextual learning capabilities of LLMs\nin text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the\nattention and problem-solving scope of LLMs through decomposition.\nSpecifically, the information determination module for eliminating redundant\ninformation and the brand-new prompt structure based on problem classification\ngreatly enhance the model's attention. Additionally, the inclusion of\nself-correcting and active learning modules greatly expands the problem-solving\nscope of LLMs, hence improving the upper limit of LLM-based approaches.\nExtensive experiments conducted on three datasets demonstrate that our approach\noutperforms other methods by a significant margin. About 2-3 percentage point\nimprovements compared to the existing baseline on the Spider Dev and\nSpider-Realistic datasets and new SOTA results on the Spider Test dataset are\nachieved. Our code is available on GitHub:\n\\url{https://github.com/FlyingFeather/DEA-SQL}.",
        "pdf_link": "https://arxiv.org/pdf/2402.10671v1.pdf"
    },
    {
        "title": "Developing Healthcare Language Model Embedding Spaces",
        "authors": [
            "Niall Taylor",
            "Dan Schofield",
            "Andrey Kormilitzin",
            "Dan W Joyce",
            "Alejo Nevado-Holgado"
        ],
        "published": "2024-03-28T19:31:32Z",
        "summary": "Pre-trained Large Language Models (LLMs) often struggle on out-of-domain\ndatasets like healthcare focused text. We explore specialized pre-training to\nadapt smaller LLMs to different healthcare datasets. Three methods are\nassessed: traditional masked language modeling, Deep Contrastive Learning for\nUnsupervised Textual Representations (DeCLUTR), and a novel pre-training\nobjective utilizing metadata categories from the healthcare settings. These\nschemes are evaluated on downstream document classification tasks for each\ndataset, with additional analysis of the resultant embedding spaces.\nContrastively trained models outperform other approaches on the classification\ntasks, delivering strong performance from limited labeled data and with fewer\nmodel parameter updates required. While metadata-based pre-training does not\nfurther improve classifications across the datasets, it yields interesting\nembedding cluster separability. All domain adapted LLMs outperform their\npublicly available general base LLM, validating the importance of\ndomain-specialization. This research illustrates efficient approaches to\ninstill healthcare competency in compact LLMs even under tight computational\nbudgets, an essential capability for responsible and sustainable deployment in\nlocal healthcare settings. We provide pre-training guidelines for specialized\nhealthcare LLMs, motivate continued inquiry into contrastive objectives, and\ndemonstrates adaptation techniques to align small LLMs with privacy-sensitive\nmedical tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.19802v1.pdf"
    },
    {
        "title": "Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models",
        "authors": [
            "Pei Wang",
            "Yejie Wang",
            "Muxi Diao",
            "Keqing He",
            "Guanting Dong",
            "Weiran Xu"
        ],
        "published": "2024-02-17T13:37:39Z",
        "summary": "In the deployment of large language models (LLMs), accurate confidence\nestimation is critical for assessing the credibility of model predictions.\nHowever, existing methods often fail to overcome the issue of overconfidence on\nincorrect answers. In this work, we focus on improving the confidence\nestimation of large language models. Considering the fragility of\nself-awareness in language models, we introduce a Multi-Perspective Consistency\n(MPC) method. We leverage complementary insights from different perspectives\nwithin models (MPC-Internal) and across different models (MPC-Across) to\nmitigate the issue of overconfidence arising from a singular viewpoint. The\nexperimental results on eight publicly available datasets show that our MPC\nachieves state-of-the-art performance. Further analyses indicate that MPC can\nmitigate the problem of overconfidence and is effectively scalable to other\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.11279v1.pdf"
    },
    {
        "title": "CharPoet: A Chinese Classical Poetry Generation System Based on Token-free LLM",
        "authors": [
            "Chengyue Yu",
            "Lei Zang",
            "Jiaotuan Wang",
            "Chenyi Zhuang",
            "Jinjie Gu"
        ],
        "published": "2024-01-07T15:00:36Z",
        "summary": "Automatic Chinese classical poetry generation has attracted much research\ninterest, but achieving effective control over format and content\nsimultaneously remains challenging. Traditional systems usually accept keywords\nas user inputs, resulting in limited control over content. Large language\nmodels (LLMs) improve content control by allowing unrestricted user\ninstructions, but the token-by-token generation process frequently makes format\nerrors. Motivated by this, we propose CharPoet, a Chinese classical poetry\ngeneration system based on token-free LLM, which provides effective control\nover both format and content. Our token-free architecture generates in a\ncharacter-by-character manner, enabling precise control over the number of\ncharacters. Pruned from existing token-based LLMs, CharPoet inherits their\npretrained capabilities and can generate poetry following instructions like\n\"Write me a poem for my mother's birthday.\" CharPoet achieves format accuracy\nabove 0.96, outperforming Jiuge-GPT-2 (0.91) and GPT-4 (0.38). In terms of\ncontent quality, CharPoet surpasses traditional systems including Jiuge, and is\ncomparable to other LLMs. Our system is open source and available at\nhttps://modelscope.cn/models/CharPoet/CharPoet. A video demonstration of\nCharPoet is available at https://youtu.be/voZ25qEp3Dc.",
        "pdf_link": "https://arxiv.org/pdf/2401.03512v3.pdf"
    },
    {
        "title": "Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing",
        "authors": [
            "Zhenyu Qian",
            "Yiming Qian",
            "Yuting Song",
            "Fei Gao",
            "Hai Jin",
            "Chen Yu",
            "Xia Xie"
        ],
        "published": "2024-03-31T07:38:39Z",
        "summary": "Handling graph data is one of the most difficult tasks. Traditional\ntechniques, such as those based on geometry and matrix factorization, rely on\nassumptions about the data relations that become inadequate when handling large\nand complex graph data. On the other hand, deep learning approaches demonstrate\npromising results in handling large graph data, but they often fall short of\nproviding interpretable explanations. To equip the graph processing with both\nhigh accuracy and explainability, we introduce a novel approach that harnesses\nthe power of a large language model (LLM), enhanced by an uncertainty-aware\nmodule to provide a confidence score on the generated answer. We experiment\nwith our approach on two graph processing tasks: few-shot knowledge graph\ncompletion and graph classification. Our results demonstrate that through\nparameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms\nby a substantial margin across ten diverse benchmark datasets. Moreover, to\naddress the challenge of explainability, we propose an uncertainty estimation\nbased on perturbation, along with a calibration scheme to quantify the\nconfidence scores of the generated answers. Our confidence measure achieves an\nAUC of 0.8 or higher on seven out of the ten datasets in predicting the\ncorrectness of the answer generated by LLM.",
        "pdf_link": "https://arxiv.org/pdf/2404.00589v1.pdf"
    },
    {
        "title": "ARKS: Active Retrieval in Knowledge Soup for Code Generation",
        "authors": [
            "Hongjin Su",
            "Shuyang Jiang",
            "Yuhang Lai",
            "Haoyuan Wu",
            "Boao Shi",
            "Che Liu",
            "Qian Liu",
            "Tao Yu"
        ],
        "published": "2024-02-19T17:37:28Z",
        "summary": "Recently the retrieval-augmented generation (RAG) paradigm has raised much\nattention for its potential in incorporating external knowledge into large\nlanguage models (LLMs) without further training. While widely explored in\nnatural language applications, its utilization in code generation remains\nunder-explored. In this paper, we introduce Active Retrieval in Knowledge Soup\n(ARKS), an advanced strategy for generalizing large language models for code.\nIn contrast to relying on a single source, we construct a knowledge soup\nintegrating web search, documentation, execution feedback, and evolved code\nsnippets. We employ an active retrieval strategy that iteratively refines the\nquery and updates the knowledge soup. To assess the performance of ARKS, we\ncompile a new benchmark comprising realistic coding problems associated with\nfrequently updated libraries and long-tail programming languages. Experimental\nresults on ChatGPT and CodeLlama demonstrate a substantial improvement in the\naverage execution accuracy of ARKS on LLMs. The analysis confirms the\neffectiveness of our proposed knowledge soup and active retrieval strategies,\noffering rich insights into the construction of effective retrieval-augmented\ncode generation (RACG) pipelines. Our model, code, and data are available at\nhttps://arks-codegen.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2402.12317v1.pdf"
    },
    {
        "title": "Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization",
        "authors": [
            "Jin Peng Zhou",
            "Charles Staats",
            "Wenda Li",
            "Christian Szegedy",
            "Kilian Q. Weinberger",
            "Yuhuai Wu"
        ],
        "published": "2024-03-26T22:01:13Z",
        "summary": "Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.",
        "pdf_link": "https://arxiv.org/pdf/2403.18120v1.pdf"
    },
    {
        "title": "The World of Generative AI: Deepfakes and Large Language Models",
        "authors": [
            "Alakananda Mitra",
            "Saraju P. Mohanty",
            "Elias Kougianos"
        ],
        "published": "2024-02-06T20:18:32Z",
        "summary": "We live in the era of Generative Artificial Intelligence (GenAI). Deepfakes\nand Large Language Models (LLMs) are two examples of GenAI. Deepfakes, in\nparticular, pose an alarming threat to society as they are capable of spreading\nmisinformation and changing the truth. LLMs are powerful language models that\ngenerate general-purpose language. However due to its generative aspect, it can\nalso be a risk for people if used with ill intentions. The ethical use of these\ntechnologies is a big concern. This short article tries to find out the\ninterrelationship between them.",
        "pdf_link": "https://arxiv.org/pdf/2402.04373v1.pdf"
    },
    {
        "title": "Found in the Middle: How Language Models Use Long Contexts Better via Plug-and-Play Positional Encoding",
        "authors": [
            "Zhenyu Zhang",
            "Runjin Chen",
            "Shiwei Liu",
            "Zhewei Yao",
            "Olatunji Ruwase",
            "Beidi Chen",
            "Xiaoxia Wu",
            "Zhangyang Wang"
        ],
        "published": "2024-03-05T04:58:37Z",
        "summary": "This paper aims to overcome the \"lost-in-the-middle\" challenge of large\nlanguage models (LLMs). While recent advancements have successfully enabled\nLLMs to perform stable language modeling with up to 4 million tokens, the\npersistent difficulty faced by most LLMs in identifying relevant information\nsituated in the middle of the context has not been adequately tackled. To\naddress this problem, this paper introduces Multi-scale Positional Encoding\n(Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the\ncapacity of LLMs to handle the relevant information located in the middle of\nthe context, without fine-tuning or introducing any additional overhead. Ms-PoE\nleverages the position indice rescaling to relieve the long-term decay effect\nintroduced by RoPE, while meticulously assigning distinct scaling ratios to\ndifferent attention heads to preserve essential knowledge learned during the\npre-training step, forming a multi-scale context fusion from short to long\ndistance. Extensive experiments with a wide range of LLMs demonstrate the\nefficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of\nup to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are\navailable at https://github.com/VITA-Group/Ms-PoE.",
        "pdf_link": "https://arxiv.org/pdf/2403.04797v1.pdf"
    },
    {
        "title": "Yell At Your Robot: Improving On-the-Fly from Language Corrections",
        "authors": [
            "Lucy Xiaoyang Shi",
            "Zheyuan Hu",
            "Tony Z. Zhao",
            "Archit Sharma",
            "Karl Pertsch",
            "Jianlan Luo",
            "Sergey Levine",
            "Chelsea Finn"
        ],
        "published": "2024-03-19T17:08:24Z",
        "summary": "Hierarchical policies that combine language and low-level control have been\nshown to perform impressively long-horizon robotic tasks, by leveraging either\nzero-shot high-level planners like pretrained language and vision-language\nmodels (LLMs/VLMs) or models trained on annotated robotic demonstrations.\nHowever, for complex and dexterous skills, attaining high success rates on\nlong-horizon tasks still represents a major challenge -- the longer the task\nis, the more likely it is that some stage will fail. Can humans help the robot\nto continuously improve its long-horizon task performance through intuitive and\nnatural feedback? In this paper, we make the following observation: high-level\npolicies that index into sufficiently rich and expressive low-level\nlanguage-conditioned skills can be readily supervised with human feedback in\nthe form of language corrections. We show that even fine-grained corrections,\nsuch as small movements (\"move a bit to the left\"), can be effectively\nincorporated into high-level policies, and that such corrections can be readily\nobtained from humans observing the robot and making occasional suggestions.\nThis framework enables robots not only to rapidly adapt to real-time language\nfeedback, but also incorporate this feedback into an iterative training scheme\nthat improves the high-level policy's ability to correct errors in both\nlow-level execution and high-level decision-making purely from verbal feedback.\nOur evaluation on real hardware shows that this leads to significant\nperformance improvement in long-horizon, dexterous manipulation tasks without\nthe need for any additional teleoperation. Videos and code are available at\nhttps://yay-robot.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.12910v1.pdf"
    },
    {
        "title": "Calibrating Large Language Models Using Their Generations Only",
        "authors": [
            "Dennis Ulmer",
            "Martin Gubri",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seong Joon Oh"
        ],
        "published": "2024-03-09T17:46:24Z",
        "summary": "As large language models (LLMs) are increasingly deployed in user-facing\napplications, building trust and maintaining safety by accurately quantifying a\nmodel's confidence in its prediction becomes even more important. However,\nfinding effective ways to calibrate LLMs - especially when the only interface\nto the models is their generated text - remains a challenge. We propose APRICOT\n(auxiliary prediction of confidence targets): A method to set confidence\ntargets and train an additional model that predicts an LLM's confidence based\non its textual input and output alone. This approach has several advantages: It\nis conceptually simple, does not require access to the target model beyond its\noutput, does not interfere with the language generation, and has a multitude of\npotential usages, for instance by verbalizing the predicted confidence or\nadjusting the given answer based on the confidence. We show how our approach\nperforms competitively in terms of calibration error for white-box and\nblack-box LLMs on closed-book question-answering to detect incorrect LLM\nanswers.",
        "pdf_link": "https://arxiv.org/pdf/2403.05973v1.pdf"
    },
    {
        "title": "AI for Biomedicine in the Era of Large Language Models",
        "authors": [
            "Zhenyu Bi",
            "Sajib Acharjee Dip",
            "Daniel Hajialigol",
            "Sindhura Kommu",
            "Hanwen Liu",
            "Meng Lu",
            "Xuan Wang"
        ],
        "published": "2024-03-23T01:40:22Z",
        "summary": "The capabilities of AI for biomedicine span a wide spectrum, from the atomic\nlevel, where it solves partial differential equations for quantum systems, to\nthe molecular level, predicting chemical or protein structures, and further\nextending to societal predictions like infectious disease outbreaks. Recent\nadvancements in large language models, exemplified by models like ChatGPT, have\nshowcased significant prowess in natural language tasks, such as translating\nlanguages, constructing chatbots, and answering questions. When we consider\nbiomedical data, we observe a resemblance to natural language in terms of\nsequences: biomedical literature and health records presented as text,\nbiological sequences or sequencing data arranged in sequences, or sensor data\nlike brain signals as time series. The question arises: Can we harness the\npotential of recent large language models to drive biomedical knowledge\ndiscoveries? In this survey, we will explore the application of large language\nmodels to three crucial categories of biomedical data: 1) textual data, 2)\nbiological sequences, and 3) brain signals. Furthermore, we will delve into\nlarge language model challenges in biomedical research, including ensuring\ntrustworthiness, achieving personalization, and adapting to multi-modal data\nrepresentation",
        "pdf_link": "https://arxiv.org/pdf/2403.15673v1.pdf"
    },
    {
        "title": "RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning",
        "authors": [
            "Junjie Ye",
            "Yilong Wu",
            "Songyang Gao",
            "Caishuang Huang",
            "Sixian Li",
            "Guanyu Li",
            "Xiaoran Fan",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-01-16T12:45:15Z",
        "summary": "Tool learning has generated widespread interest as a vital means of\ninteraction between Large Language Models (LLMs) and the physical world.\nCurrent research predominantly emphasizes LLMs' capacity to utilize tools in\nwell-structured environments while overlooking their stability when confronted\nwith the inevitable noise of the real world. To bridge this gap, we introduce\nRoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool\nlearning. Specifically, we establish five external environments, each featuring\nvarying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),\nproviding an in-depth analysis of the model's resilience across three critical\nphases: tool selection, parameter identification, and content filling.\nExperiments involving six widely-used models underscore the urgent necessity\nfor enhancing the robustness of LLMs in tool learning. For instance, the\nperformance of GPT-4 even drops significantly from 80.00 to 58.10 when there is\nno substantial change in manual accuracy. More surprisingly, the noise\ncorrection capability inherent in the GPT family paradoxically impedes its\nadaptability in the face of mild noise. In light of these findings, we propose\nRoTTuning, a strategy that enriches the diversity of training environments to\nbolster the robustness of LLMs in tool learning. The code and data are\navailable at https://github.com/Junjie-Ye/RoTBench.",
        "pdf_link": "https://arxiv.org/pdf/2401.08326v2.pdf"
    },
    {
        "title": "CoLLEGe: Concept Embedding Generation for Large Language Models",
        "authors": [
            "Ryan Teehan",
            "Brenden Lake",
            "Mengye Ren"
        ],
        "published": "2024-03-22T17:26:05Z",
        "summary": "Current language models are unable to quickly learn new concepts on the fly,\noften requiring a more involved finetuning process to learn robustly. Prompting\nin-context is not robust to context distractions, and often fails to confer\nmuch information about the new concepts. Classic methods for few-shot word\nlearning in NLP, relying on global word vectors, are less applicable to large\nlanguage models. In this paper, we introduce a novel approach named CoLLEGe\n(Concept Learning with Language Embedding Generation) to modernize few-shot\nconcept learning. CoLLEGe is a meta-learning framework capable of generating\nflexible embeddings for new concepts using a small number of example sentences\nor definitions. Our primary meta-learning objective is simply to facilitate a\nlanguage model to make next word predictions in forthcoming sentences, making\nit compatible with language model pretraining. We design a series of tasks to\ntest new concept learning in challenging real-world scenarios, including new\nword acquisition, definition inference, and verbal reasoning, and demonstrate\nthat our method succeeds in each setting without task-specific training.",
        "pdf_link": "https://arxiv.org/pdf/2403.15362v1.pdf"
    },
    {
        "title": "CACA Agent: Capability Collaboration based AI Agent",
        "authors": [
            "Peng Xu",
            "Haoran Wang",
            "Chuang Wang",
            "Xu Liu"
        ],
        "published": "2024-03-22T11:42:47Z",
        "summary": "As AI Agents based on Large Language Models (LLMs) have shown potential in\npractical applications across various fields, how to quickly deploy an AI agent\nand how to conveniently expand the application scenario of AI agents has become\na challenge. Previous studies mainly focused on implementing all the reasoning\ncapabilities of AI agents within a single LLM, which often makes the model more\ncomplex and also reduces the extensibility of AI agent functionality. In this\npaper, we propose CACA Agent (Capability Collaboration based AI Agent), using\nan open architecture inspired by service computing. CACA Agent integrates a set\nof collaborative capabilities to implement AI Agents, not only reducing the\ndependence on a single LLM, but also enhancing the extensibility of both the\nplanning abilities and the tools available to AI agents. Utilizing the proposed\nsystem, we present a demo to illustrate the operation and the application\nscenario extension of CACA Agent.",
        "pdf_link": "https://arxiv.org/pdf/2403.15137v1.pdf"
    },
    {
        "title": "Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese",
        "authors": [
            "Rifki Afina Putri",
            "Faiz Ghifari Haznitrama",
            "Dea Adhista",
            "Alice Oh"
        ],
        "published": "2024-02-27T08:24:32Z",
        "summary": "Large Language Models (LLMs) are increasingly being used to generate\nsynthetic data for training and evaluating models. However, it is unclear\nwhether they can generate a good quality of question answering (QA) dataset\nthat incorporates knowledge and cultural nuance embedded in a language,\nespecially for low-resource languages. In this study, we investigate the\neffectiveness of using LLMs in generating culturally relevant commonsense QA\ndatasets for Indonesian and Sundanese languages. To do so, we create datasets\nfor these languages using various methods involving both LLMs and human\nannotators. Our experiments show that the current best-performing LLM, GPT-4\nTurbo, is capable of generating questions with adequate knowledge in Indonesian\nbut not in Sundanese, highlighting the performance discrepancy between medium-\nand lower-resource languages. We also benchmark various LLMs on our generated\ndatasets and find that they perform better on the LLM-generated datasets\ncompared to those created by humans.",
        "pdf_link": "https://arxiv.org/pdf/2402.17302v1.pdf"
    },
    {
        "title": "DreamLIP: Language-Image Pre-training with Long Captions",
        "authors": [
            "Kecheng Zheng",
            "Yifei Zhang",
            "Wei Wu",
            "Fan Lu",
            "Shuailei Ma",
            "Xin Jin",
            "Wei Chen",
            "Yujun Shen"
        ],
        "published": "2024-03-25T17:59:42Z",
        "summary": "Language-image pre-training largely relies on how precisely and thoroughly a\ntext describes its paired image. In practice, however, the contents of an image\ncan be so rich that well describing them requires lengthy captions (e.g., with\n10 sentences), which are usually missing in existing datasets. Consequently,\nthere are currently no clear evidences on whether and how language-image\npre-training could benefit from long captions. To figure this out, we first\nre-caption 30M images with detailed descriptions using a pre-trained\nMulti-modality Large Language Model (MLLM), and then study the usage of the\nresulting captions under a contrastive learning framework. We observe that,\neach sentence within a long caption is very likely to describe the image\npartially (e.g., an object). Motivated by this, we propose to dynamically\nsample sub-captions from the text label to construct multiple positive pairs,\nand introduce a grouping loss to match the embeddings of each sub-caption with\nits corresponding local image patches in a self-supervised manner. Experimental\nresults on a wide rage of downstream tasks demonstrate the consistent\nsuperiority of our method, termed DreamLIP, over previous alternatives,\nhighlighting its fine-grained representational capacity. It is noteworthy that,\non the tasks of image-text retrieval and semantic segmentation, our model\ntrained with 30M image-text pairs achieves on par or even better performance\nthan CLIP trained with 400M pairs. Project page is available at\nhttps://zyf0619sjtu.github.io/dream-lip.",
        "pdf_link": "https://arxiv.org/pdf/2403.17007v1.pdf"
    },
    {
        "title": "Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts",
        "authors": [
            "Shubhra Kanti Karmaker Santu",
            "Sanjeev Kumar Sinha",
            "Naman Bansal",
            "Alex Knipper",
            "Souvika Sarkar",
            "John Salvador",
            "Yash Mahajan",
            "Sri Guttikonda",
            "Mousumi Akter",
            "Matthew Freestone",
            "Matthew C. Williams Jr"
        ],
        "published": "2024-02-23T20:14:16Z",
        "summary": "One of the most important yet onerous tasks in the academic peer-reviewing\nprocess is composing meta-reviews, which involves understanding the core\ncontributions, strengths, and weaknesses of a scholarly manuscript based on\npeer-review narratives from multiple experts and then summarizing those\nmultiple experts' perspectives into a concise holistic overview. Given the\nlatest major developments in generative AI, especially Large Language Models\n(LLMs), it is very compelling to rigorously study the utility of LLMs in\ngenerating such meta-reviews in an academic peer-review setting. In this paper,\nwe perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and\nPaLM2, to automatically generate meta-reviews by prompting them with different\ntypes/levels of prompts based on the recently proposed TELeR taxonomy. Finally,\nwe perform a detailed qualitative study of the meta-reviews generated by the\nLLMs and summarize our findings and recommendations for prompting LLMs for this\ncomplex task.",
        "pdf_link": "https://arxiv.org/pdf/2402.15589v1.pdf"
    },
    {
        "title": "NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models",
        "authors": [
            "Lizhou Fan",
            "Wenyue Hua",
            "Xiang Li",
            "Kaijie Zhu",
            "Mingyu Jin",
            "Lingyao Li",
            "Haoyang Ling",
            "Jinkui Chi",
            "Jindong Wang",
            "Xin Ma",
            "Yongfeng Zhang"
        ],
        "published": "2024-03-04T07:10:31Z",
        "summary": "Understanding the reasoning capabilities of Multimodal Large Language Models\n(MLLMs) is an important area of research. In this study, we introduce a dynamic\nbenchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating\nthe pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to\ndisentangle the effect of various factors such as image recognition and\ninstruction following, from the overall performance of the models, allowing us\nto focus solely on evaluating their reasoning abilities. It is built by\nconverting textual description of questions from NPHardEval to image\nrepresentations. Our findings reveal significant discrepancies in reasoning\nabilities across different models and highlight the relatively weak performance\nof MLLMs compared to LLMs in terms of reasoning. We also investigate the impact\nof different prompting styles, including visual, text, and combined visual and\ntext prompts, on the reasoning abilities of MLLMs, demonstrating the different\nimpacts of multimodal inputs in model performance. Unlike traditional\nbenchmarks, which focus primarily on static evaluations, our benchmark will be\nupdated monthly to prevent overfitting and ensure a more authentic and\nfine-grained evaluation of the models. We believe that this benchmark can aid\nin understanding and guide the further development of reasoning abilities in\nMLLMs. The benchmark dataset and code are available at\nhttps://github.com/lizhouf/NPHardEval4V",
        "pdf_link": "https://arxiv.org/pdf/2403.01777v2.pdf"
    },
    {
        "title": "JDocQA: Japanese Document Question Answering Dataset for Generative Language Models",
        "authors": [
            "Eri Onami",
            "Shuhei Kurita",
            "Taiki Miyanishi",
            "Taro Watanabe"
        ],
        "published": "2024-03-28T14:22:54Z",
        "summary": "Document question answering is a task of question answering on given\ndocuments such as reports, slides, pamphlets, and websites, and it is a truly\ndemanding task as paper and electronic forms of documents are so common in our\nsociety. This is known as a quite challenging task because it requires not only\ntext understanding but also understanding of figures and tables, and hence\nvisual question answering (VQA) methods are often examined in addition to\ntextual approaches. We introduce Japanese Document Question Answering (JDocQA),\na large-scale document-based QA dataset, essentially requiring both visual and\ntextual information to answer questions, which comprises 5,504 documents in PDF\nformat and annotated 11,600 question-and-answer instances in Japanese. Each QA\ninstance includes references to the document pages and bounding boxes for the\nanswer clues. We incorporate multiple categories of questions and unanswerable\nquestions from the document for realistic question-answering applications. We\nempirically evaluate the effectiveness of our dataset with text-based large\nlanguage models (LLMs) and multimodal models. Incorporating unanswerable\nquestions in finetuning may contribute to harnessing the so-called\nhallucination generation.",
        "pdf_link": "https://arxiv.org/pdf/2403.19454v1.pdf"
    },
    {
        "title": "Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming",
        "authors": [
            "Anisha Agarwal",
            "Aaron Chan",
            "Shubham Chandel",
            "Jinu Jang",
            "Shaun Miller",
            "Roshanak Zilouchian Moghaddam",
            "Yevhen Mohylevskyy",
            "Neel Sundaresan",
            "Michele Tufano"
        ],
        "published": "2024-02-22T03:51:34Z",
        "summary": "The integration of Large Language Models (LLMs) into Development Environments\n(IDEs) has become a focal point in modern software development. LLMs such as\nOpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment\ndeveloper productivity by serving as intelligent, chat-driven programming\nassistants. However, utilizing LLMs out of the box is unlikely to be optimal\nfor any given scenario. Rather, each system requires the LLM to be honed to its\nset of heuristics to ensure the best performance. In this paper, we introduce\nthe Copilot evaluation harness: a set of data and tools for evaluating\nLLM-guided IDE interactions, covering various programming scenarios and\nlanguages. We propose our metrics as a more robust and information-dense\nevaluation than previous state of the art evaluation systems. We design and\ncompute both static and execution based success metrics for scenarios\nencompassing a wide range of developer tasks, including code generation from\nnatural language (generate), documentation generation from code (doc), test\ncase generation (test), bug-fixing (fix), and workspace understanding and query\nresolution (workspace). These success metrics are designed to evaluate the\nperformance of LLMs within a given IDE and its respective parameter space. Our\nlearnings from evaluating three common LLMs using these metrics can inform the\ndevelopment and validation of future scenarios in LLM guided IDEs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14261v1.pdf"
    },
    {
        "title": "Third-Party Language Model Performance Prediction from Instruction",
        "authors": [
            "Rahul Nadkarni",
            "Yizhong Wang",
            "Noah A. Smith"
        ],
        "published": "2024-03-19T03:53:47Z",
        "summary": "Language model-based instruction-following systems have lately shown\nincreasing performance on many benchmark tasks, demonstrating the capability of\nadapting to a broad variety of instructions. However, such systems are often\nnot designed to be transparent about their limitations; a user may easily\nprompt a model with an instruction without any idea of whether the responses\nshould be expected to be accurate, or if the system is even capable of\nperforming the task. We propose a third party performance prediction framework,\nwhere a separate model is trained to predict the metric resulting from\nevaluating an instruction-following system on a task while assuming access only\nto its inputs and outputs at inference time. We perform this analysis with a\nvariety of both open and closed instruction-following models as well as\nmultiple performance predictors, and examine the effect of various factors such\nas model size, number of training tasks, and prompt format. Our findings\nindicate that third-party performance prediction is very challenging, and much\nwork remains in developing predictors that can automatically reveal the\nlimitations of modern instruction-following natural language processing\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.12413v1.pdf"
    },
    {
        "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
        "authors": [
            "Eli Schwartz",
            "Leshem Choshen",
            "Joseph Shtok",
            "Sivan Doveh",
            "Leonid Karlinsky",
            "Assaf Arbelle"
        ],
        "published": "2024-03-30T19:46:59Z",
        "summary": "Language models struggle with handling numerical data and performing\narithmetic operations. We hypothesize that this limitation can be partially\nattributed to non-intuitive textual numbers representation. When a digit is\nread or generated by a causal language model it does not know its place value\n(e.g. thousands vs. hundreds) until the entire number is processed. To address\nthis issue, we propose a simple adjustment to how numbers are represented by\nincluding the count of digits before each number. For instance, instead of\n\"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term\nNumeroLogic, offers an added advantage in number generation by serving as a\nChain of Thought (CoT). By requiring the model to consider the number of digits\nfirst, it enhances the reasoning process before generating the actual number.\nWe use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic\nformatting. We further demonstrate NumeroLogic applicability to general natural\nlanguage modeling, improving language understanding performance in the MMLU\nbenchmark.",
        "pdf_link": "https://arxiv.org/pdf/2404.00459v1.pdf"
    },
    {
        "title": "ESG Sentiment Analysis: comparing human and language model performance including GPT",
        "authors": [
            "Karim Derrick"
        ],
        "published": "2024-02-26T15:22:30Z",
        "summary": "In this paper we explore the challenges of measuring sentiment in relation to\nEnvironmental, Social and Governance (ESG) social media. ESG has grown in\nimportance in recent years with a surge in interest from the financial sector\nand the performance of many businesses has become based in part on their ESG\nrelated reputations. The use of sentiment analysis to measure ESG related\nreputation has developed and with it interest in the use of machines to do so.\nThe era of digital media has created an explosion of new media sources, driven\nby the growth of social media platforms. This growing data environment has\nbecome an excellent source for behavioural insight studies across many\ndisciplines that includes politics, healthcare and market research. Our study\nseeks to compare human performance with the cutting edge in machine performance\nin the measurement of ESG related sentiment. To this end researchers classify\nthe sentiment of 150 tweets and a reliability measure is made. A gold standard\ndata set is then established based on the consensus of 3 researchers and this\ndata set is then used to measure the performance of different machine\napproaches: one based on the VADER dictionary approach to sentiment\nclassification and then multiple language model approaches, including Llama2,\nT5, Mistral, Mixtral, FINBERT, GPT3.5 and GPT4.",
        "pdf_link": "https://arxiv.org/pdf/2402.16650v1.pdf"
    },
    {
        "title": "Enhancing LLM-Based Coding Tools through Native Integration of IDE-Derived Static Context",
        "authors": [
            "Yichen Li",
            "Yun Peng",
            "Yintong Huo",
            "Michael R. Lyu"
        ],
        "published": "2024-02-06T01:59:41Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in code\ncompletion, as evidenced by their essential roles in developing code assistant\nservices such as Copilot. Being trained on in-file contexts, current LLMs are\nquite effective in completing code for single source files. However, it is\nchallenging for them to conduct repository-level code completion for large\nsoftware projects that require cross-file information. Existing research on\nLLM-based repository-level code completion identifies and integrates cross-file\ncontexts, but it suffers from low accuracy and limited context length of LLMs.\nIn this paper, we argue that Integrated Development Environments (IDEs) can\nprovide direct, accurate and real-time cross-file information for\nrepository-level code completion. We propose IDECoder, a practical framework\nthat leverages IDE native static contexts for cross-context construction and\ndiagnosis results for self-refinement. IDECoder utilizes the rich cross-context\ninformation available in IDEs to enhance the capabilities of LLMs of\nrepository-level code completion. We conducted preliminary experiments to\nvalidate the performance of IDECoder and observed that this synergy represents\na promising trend for future exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.03630v2.pdf"
    },
    {
        "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents",
        "authors": [
            "Michael Ahn",
            "Debidatta Dwibedi",
            "Chelsea Finn",
            "Montse Gonzalez Arenas",
            "Keerthana Gopalakrishnan",
            "Karol Hausman",
            "Brian Ichter",
            "Alex Irpan",
            "Nikhil Joshi",
            "Ryan Julian",
            "Sean Kirmani",
            "Isabel Leal",
            "Edward Lee",
            "Sergey Levine",
            "Yao Lu",
            "Isabel Leal",
            "Sharath Maddineni",
            "Kanishka Rao",
            "Dorsa Sadigh",
            "Pannag Sanketi",
            "Pierre Sermanet",
            "Quan Vuong",
            "Stefan Welker",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Steve Xu",
            "Zhuo Xu"
        ],
        "published": "2024-01-23T18:45:54Z",
        "summary": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.",
        "pdf_link": "https://arxiv.org/pdf/2401.12963v1.pdf"
    },
    {
        "title": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems",
        "authors": [
            "Hanqing Yang",
            "Marie Siew",
            "Carlee Joe-Wong"
        ],
        "published": "2024-03-25T14:32:28Z",
        "summary": "The increasing prevalence of Cyber-Physical Systems and the Internet of\nThings (CPS-IoT) applications and Foundation Models are enabling new\napplications that leverage real-time control of the environment. For example,\nreal-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems\ncan reduce its usage when not needed for the comfort of human occupants, hence\nreducing energy consumption. Collecting real-time feedback on human preferences\nin such human-in-the-loop (HITL) systems, however, is difficult in practice. We\npropose the use of large language models (LLMs) to deal with the challenges of\ndynamic environments and difficult-to-obtain data in CPS optimization. In this\npaper, we present a case study that employs LLM agents to mimic the behaviors\nand thermal preferences of various population groups (e.g. young families, the\nelderly) in a shopping mall. The aggregated thermal preferences are integrated\ninto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which\nemploys the LLM as a dynamic simulation of the physical environment to learn\nhow to balance between energy savings and occupant comfort. Our results show\nthat LLMs are capable of simulating complex population movements within large\nopen spaces. Besides, AitL-RL demonstrates superior performance compared to the\npopular existing policy of set point control, suggesting that adaptive and\npersonalized decision-making is critical for efficient optimization in CPS-IoT\napplications. Through this case study, we demonstrate the potential of\nintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance system\nadaptability and efficiency. The project's code can be found on our GitHub\nrepository.",
        "pdf_link": "https://arxiv.org/pdf/2403.16809v1.pdf"
    },
    {
        "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over Tabular and Textual Data",
        "authors": [
            "Fengbin Zhu",
            "Ziyang Liu",
            "Fuli Feng",
            "Chao Wang",
            "Moxin Li",
            "Tat-Seng Chua"
        ],
        "published": "2024-01-24T04:28:50Z",
        "summary": "In this work, we address question answering (QA) over a hybrid of tabular and\ntextual data that are very common content on the Web (e.g. SEC filings), where\ndiscrete reasoning capabilities are often required. Recently, large language\nmodels (LLMs) like GPT-4 have demonstrated strong multi-step reasoning\ncapabilities. We then consider harnessing the amazing power of LLMs to solve\nour task. We abstract a Step-wise Pipeline for tabular and textual QA, which\nconsists of three key steps, including Extractor, Reasoner and Executor, and\ninitially design an instruction to instantiate the pipeline and validate that\nGPT-4 outperforms all existing methods. However, utilizing an online LLM like\nGPT-4 holds various challenges in terms of cost, latency, and data security\nrisk, which motivates us to specialize smaller LLMs in this task. We develop a\nTAT-LLM language model by fine-tuning LLaMA 2 with the training data generated\nautomatically from existing expert-annotated datasets following the Step-wise\nPipeline. The experimental results have verified that our TAT-LLM model can\noutperform all baseline models, including the previous best fine-tuned models\nand very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2401.13223v2.pdf"
    },
    {
        "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
        "authors": [
            "Zhen Qin",
            "Weigao Sun",
            "Dong Li",
            "Xuyang Shen",
            "Weixuan Sun",
            "Yiran Zhong"
        ],
        "published": "2024-01-09T16:27:28Z",
        "summary": "Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.",
        "pdf_link": "https://arxiv.org/pdf/2401.04658v2.pdf"
    },
    {
        "title": "A Preliminary Study on Using Large Language Models in Software Pentesting",
        "authors": [
            "Kumar Shashwat",
            "Francis Hahn",
            "Xinming Ou",
            "Dmitry Goldgof",
            "Lawrence Hall",
            "Jay Ligatti",
            "S. Raj Rajgopalan",
            "Armin Ziaie Tabari"
        ],
        "published": "2024-01-30T21:42:59Z",
        "summary": "Large language models (LLM) are perceived to offer promising potentials for\nautomating security tasks, such as those found in security operation centers\n(SOCs). As a first step towards evaluating this perceived potential, we\ninvestigate the use of LLMs in software pentesting, where the main task is to\nautomatically identify software security vulnerabilities in source code. We\nhypothesize that an LLM-based AI agent can be improved over time for a specific\nsecurity task as human operators interact with it. Such improvement can be\nmade, as a first step, by engineering prompts fed to the LLM based on the\nresponses produced, to include relevant contexts and structures so that the\nmodel provides more accurate results. Such engineering efforts become\nsustainable if the prompts that are engineered to produce better results on\ncurrent tasks, also produce better results on future unknown tasks. To examine\nthis hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains\n2,740 hand-crafted source code test cases containing various types of\nvulnerabilities. We divide the test cases into training and testing data, where\nwe engineer the prompts based on the training data (only), and evaluate the\nfinal system on the testing data. We compare the AI agent's performance on the\ntesting data against the performance of the agent without the prompt\nengineering. We also compare the AI agent's results against those from\nSonarQube, a widely used static code analyzer for security testing. We built\nand tested multiple versions of the AI agent using different off-the-shelf LLMs\n-- Google's Gemini-pro, as well as OpenAI's GPT-3.5-Turbo and GPT-4-Turbo (with\nboth chat completion and assistant APIs). The results show that using LLMs is a\nviable approach to build an AI agent for software pentesting that can improve\nthrough repeated use and prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2401.17459v1.pdf"
    },
    {
        "title": "The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey",
        "authors": [
            "Saurav Pawar",
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Vinija Jain",
            "Aman Chadha",
            "Amitava Das"
        ],
        "published": "2024-01-15T18:07:21Z",
        "summary": "The advent of Large Language Models (LLMs) represents a notable breakthrough\nin Natural Language Processing (NLP), contributing to substantial progress in\nboth text comprehension and generation. However, amidst these advancements, it\nis noteworthy that LLMs often face a limitation in terms of context length\nextrapolation. Understanding and extending the context length for LLMs is\ncrucial in enhancing their performance across various NLP applications. In this\nsurvey paper, we delve into the multifaceted aspects of exploring why it is\nessential, and the potential transformations that superior techniques could\nbring to NLP applications. We study the inherent challenges associated with\nextending context length and present an organized overview of the existing\nstrategies employed by researchers. Additionally, we discuss the intricacies of\nevaluating context extension techniques and highlight the open challenges that\nresearchers face in this domain. Furthermore, we explore whether there is a\nconsensus within the research community regarding evaluation standards and\nidentify areas where further agreement is needed. This comprehensive survey\naims to serve as a valuable resource for researchers, guiding them through the\nnuances of context length extension techniques and fostering discussions on\nfuture advancements in this evolving field.",
        "pdf_link": "https://arxiv.org/pdf/2401.07872v1.pdf"
    },
    {
        "title": "HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning",
        "authors": [
            "Fucai Ke",
            "Zhixi Cai",
            "Simindokht Jahangard",
            "Weiqing Wang",
            "Pari Delir Haghighi",
            "Hamid Rezatofighi"
        ],
        "published": "2024-03-19T16:31:30Z",
        "summary": "Recent advances in visual reasoning (VR), particularly with the aid of Large\nVision-Language Models (VLMs), show promise but require access to large-scale\ndatasets and face challenges such as high computational costs and limited\ngeneralization capabilities. Compositional visual reasoning approaches have\nemerged as effective strategies; however, they heavily rely on the commonsense\nknowledge encoded in Large Language Models (LLMs) to perform planning,\nreasoning, or both, without considering the effect of their decisions on the\nvisual reasoning process, which can lead to errors or failed procedures. To\naddress these challenges, we introduce HYDRA, a multi-stage dynamic\ncompositional visual reasoning framework designed for reliable and\nincrementally progressive general reasoning. HYDRA integrates three essential\nmodules: a planner, a Reinforcement Learning (RL) agent serving as a cognitive\ncontroller, and a reasoner. The planner and reasoner modules utilize an LLM to\ngenerate instruction samples and executable code from the selected instruction,\nrespectively, while the RL agent dynamically interacts with these modules,\nmaking high-level decisions on selection of the best instruction sample given\ninformation from the historical state stored through a feedback loop. This\nadaptable design enables HYDRA to adjust its actions based on previous feedback\nreceived during the reasoning process, leading to more reliable reasoning\noutputs and ultimately enhancing its overall effectiveness. Our framework\ndemonstrates state-of-the-art performance in various VR tasks on four different\nwidely-used datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.12884v1.pdf"
    },
    {
        "title": "LAMP: A Language Model on the Map",
        "authors": [
            "Pasquale Balsebre",
            "Weiming Huang",
            "Gao Cong"
        ],
        "published": "2024-03-14T02:56:38Z",
        "summary": "Large Language Models (LLMs) are poised to play an increasingly important\nrole in our lives, providing assistance across a wide array of tasks. In the\ngeospatial domain, LLMs have demonstrated the ability to answer generic\nquestions, such as identifying a country's capital; nonetheless, their utility\nis hindered when it comes to answering fine-grained questions about specific\nplaces, such as grocery stores or restaurants, which constitute essential\naspects of people's everyday lives. This is mainly because the places in our\ncities haven't been systematically fed into LLMs, so as to understand and\nmemorize them. This study introduces a novel framework for fine-tuning a\npre-trained model on city-specific data, to enable it to provide accurate\nrecommendations, while minimizing hallucinations. We share our model, LAMP, and\nthe data used to train it. We conduct experiments to analyze its ability to\ncorrectly retrieving spatial objects, and compare it to well-known open- and\nclosed- source language models, such as GPT-4. Finally, we explore its emerging\ncapabilities through a case study on day planning.",
        "pdf_link": "https://arxiv.org/pdf/2403.09059v1.pdf"
    },
    {
        "title": "SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees",
        "authors": [
            "Saehan Jo",
            "Immanuel Trummer"
        ],
        "published": "2024-03-11T17:45:47Z",
        "summary": "The advancement of Large Language Models (LLMs) has significantly boosted\nperformance in natural language processing (NLP) tasks. However, the deployment\nof high-performance LLMs incurs substantial costs, primarily due to the\nincreased number of parameters aimed at enhancing model performance. This has\nmade the use of state-of-the-art LLMs more expensive for end-users. AI service\nproviders, such as OpenAI and Anthropic, often offer multiple versions of LLMs\nwith varying prices and performance. However, end-users still face challenges\nin choosing the appropriate LLM for their tasks that balance result quality\nwith cost.\n  We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel\nLLM framework designed to minimize the inference costs of NLP tasks while\nensuring sufficient result quality. It enables users to specify an accuracy\nconstraint in terms of the equivalence of outputs to those of the most powerful\nLLM. SMART then generates results that deviate from the outputs of this LLM\nonly with a probability below a user-defined threshold. SMART employs a\nprofiling phase that evaluates the performance of multiple LLMs to identify\nthose that meet the user-defined accuracy level. SMART optimizes the tradeoff\nbetween profiling overheads and the anticipated cost savings resulting from\nprofiling. Moreover, our approach significantly reduces inference costs by\nstrategically leveraging a mix of LLMs. Our experiments on three real-world\ndatasets show that, based on OpenAI models, SMART achieves significant cost\nsavings, up to 25.6x in comparison to GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.13835v1.pdf"
    },
    {
        "title": "NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation",
        "authors": [
            "Jiazhao Zhang",
            "Kunyu Wang",
            "Rongtao Xu",
            "Gengze Zhou",
            "Yicong Hong",
            "Xiaomeng Fang",
            "Qi Wu",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "published": "2024-02-24T16:39:16Z",
        "summary": "Vision-and-Language Navigation (VLN) stands as a key research problem of\nEmbodied AI, aiming at enabling agents to navigate in unseen environments\nfollowing linguistic instructions. In this field, generalization is a\nlong-standing challenge, either to out-of-distribution scenes or from Sim to\nReal. In this paper, we propose NaVid, a video-based large vision language\nmodel (VLM), to mitigate such a generalization gap. NaVid makes the first\nendeavour to showcase the capability of VLMs to achieve state-of-the-art level\nnavigation performance without any maps, odometer and depth inputs. Following\nhuman instruction, NaVid only requires an on-the-fly video stream from a\nmonocular RGB camera equipped on the robot to output the next-step action. Our\nformulation mimics how humans navigate and naturally gets rid of the problems\nintroduced by odometer noises, and the Sim2Real gaps from map or depth inputs.\nMoreover, our video-based approach can effectively encode the historical\nobservations of robots as spatio-temporal contexts for decision-making and\ninstruction following. We train NaVid with 550k navigation samples collected\nfrom VLN-CE trajectories, including action-planning and instruction-reasoning\nsamples, along with 665k large-scale web data. Extensive experiments show that\nNaVid achieves SOTA performance in simulation environments and the real world,\ndemonstrating superior cross-dataset and Sim2Real transfer. We thus believe our\nproposed VLM approach plans the next step for not only the navigation agents\nbut also this research field.",
        "pdf_link": "https://arxiv.org/pdf/2402.15852v4.pdf"
    },
    {
        "title": "Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data",
        "authors": [
            "Haoyang Liu",
            "Yijiang Li",
            "Jinglin Jian",
            "Yuxuan Cheng",
            "Jianrong Lu",
            "Shuyi Guo",
            "Jinglei Zhu",
            "Mianchen Zhang",
            "Miantong Zhang",
            "Haohan Wang"
        ],
        "published": "2024-02-15T06:30:12Z",
        "summary": "Machine learning has emerged as a powerful tool for scientific discovery,\nenabling researchers to extract meaningful insights from complex datasets. For\ninstance, it has facilitated the identification of disease-predictive genes\nfrom gene expression data, significantly advancing healthcare. However, the\ntraditional process for analyzing such datasets demands substantial human\neffort and expertise for the data selection, processing, and analysis. To\naddress this challenge, we introduce a novel framework, a Team of AI-made\nScientists (TAIS), designed to streamline the scientific discovery pipeline.\nTAIS comprises simulated roles, including a project manager, data engineer, and\ndomain expert, each represented by a Large Language Model (LLM). These roles\ncollaborate to replicate the tasks typically performed by data scientists, with\na specific focus on identifying disease-predictive genes. Furthermore, we have\ncurated a benchmark dataset to assess TAIS's effectiveness in gene\nidentification, demonstrating our system's potential to significantly enhance\nthe efficiency and scope of scientific exploration. Our findings represent a\nsolid step towards automating scientific discovery through large language\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.12391v2.pdf"
    },
    {
        "title": "LLMs for Relational Reasoning: How Far are We?",
        "authors": [
            "Zhiming Li",
            "Yushi Cao",
            "Xiufeng Xu",
            "Junzhe Jiang",
            "Xu Liu",
            "Yon Shin Teo",
            "Shang-wei Lin",
            "Yang Liu"
        ],
        "published": "2024-01-17T08:22:52Z",
        "summary": "Large language models (LLMs) have revolutionized many areas (e.g. natural\nlanguage processing, software engineering, etc.) by achieving state-of-the-art\nperformance on extensive downstream tasks. Aiming to achieve robust and general\nartificial intelligence, there has been a surge of interest in investigating\nthe reasoning ability of the LLMs. Whereas the textual and numerical reasoning\nbenchmarks adopted by previous works are rather shallow and simple, it is hard\nto conclude that the LLMs possess strong reasoning ability by merely achieving\npositive results on these benchmarks. Recent efforts have demonstrated that the\nLLMs are poor at solving sequential decision-making problems that require\ncommon-sense planning by evaluating their performance on the reinforcement\nlearning benchmarks. In this work, we conduct an in-depth assessment of several\nstate-of-the-art LLMs' reasoning ability based on the inductive logic\nprogramming (ILP) benchmark, which is broadly recognized as a representative\nand challenging measurement for evaluating logic program induction/synthesis\nsystems as it requires inducing strict cause-effect logic to achieve robust\ndeduction on independent and identically distributed (IID) and\nout-of-distribution (OOD) test samples. Our evaluations illustrate that\ncompared with the neural program induction systems which are much smaller in\nmodel size, the state-of-the-art LLMs are much poorer in terms of reasoning\nability by achieving much lower performance and generalization using either\nnatural language prompting or truth-value matrix prompting.",
        "pdf_link": "https://arxiv.org/pdf/2401.09042v1.pdf"
    },
    {
        "title": "Cost-Performance Optimization for Processing Low-Resource Language Tasks Using Commercial LLMs",
        "authors": [
            "Arijit Nag",
            "Animesh Mukherjee",
            "Niloy Ganguly",
            "Soumen Chakrabarti"
        ],
        "published": "2024-03-08T16:37:36Z",
        "summary": "Large Language Models (LLMs) exhibit impressive zero/few-shot inference and\ngeneration quality for high-resource languages(HRLs). A few of them have been\ntrained in low-resource languages (LRLs) and give decent performance. Owing to\nthe prohibitive costs of training LLMs, they are usually used as a network\nservice, with the client charged by the count of input and output tokens. The\nnumber of tokens strongly depends on the script and language, as well as the\nLLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage,\nbecause the well-known LLMs produce more tokens for LRLs than HRLs. This is\nbecause most currently popular LLMs are optimized for HRL vocabularies. Our\nobjective is to level the playing field: reduce the cost of processing LRLs in\ncontemporary LLMs while ensuring that predictive and generative qualities are\nnot compromised. As means to reduce the number of tokens processed by the LLM,\nwe consider code-mixing, translation, and transliteration of LRLs to HRLs. We\nperform an extensive study using the IndicXTREME dataset, covering 15 Indian\nlanguages, while using GPT-4 (one of the costliest LLM services released so\nfar) as a commercial LLM. We observe and analyze interesting patterns involving\ntoken count, cost,and quality across a multitude of languages and tasks. We\nshow that choosing the best policy to interact with the LLM can reduce cost by\n90% while giving better or comparable performance, compared to communicating\nwith the LLM in the original LRL.",
        "pdf_link": "https://arxiv.org/pdf/2403.05434v1.pdf"
    },
    {
        "title": "Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries",
        "authors": [
            "Manjeet Yadav",
            "Nilesh Kumar Sahu",
            "Mudita Chaturvedi",
            "Snehil Gupta",
            "Haroon R Lone"
        ],
        "published": "2024-03-29T12:25:37Z",
        "summary": "Improving mental health support in developing countries is a pressing need.\nOne potential solution is the development of scalable, automated systems to\nconduct diagnostic screenings, which could help alleviate the burden on mental\nhealth professionals. In this work, we evaluate several state-of-the-art Large\nLanguage Models (LLMs), with and without fine-tuning, on our custom dataset for\ngenerating concise summaries from mental state examinations. We rigorously\nevaluate four different models for summary generation using established ROUGE\nmetrics and input from human evaluators. The results highlight that our\ntop-performing fine-tuned model outperforms existing models, achieving ROUGE-1\nand ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed\nthe fine-tuned model's generalizability on a publicly available D4 dataset, and\nthe outcomes were promising, indicating its potential applicability beyond our\ncustom dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.20145v2.pdf"
    },
    {
        "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
        "authors": [
            "Andrew Brown",
            "Jiading Zhu",
            "Mohamed Abdelwahab",
            "Alec Dong",
            "Cindy Wang",
            "Jonathan Rose"
        ],
        "published": "2024-02-01T22:54:31Z",
        "summary": "Large Foundational Language Models are capable of performing many tasks at a\nhigh level but are difficult to deploy in many applications because of their\nsize and proprietary ownership. Many will be motivated to distill specific\ncapabilities of foundational models into smaller models that can be owned and\ncontrolled. In the development of a therapeutic chatbot, we wish to distill a\ncapability known as reflective listening, in which a therapist produces\nreflections of client speech. These reflections either restate what a client\nhas said, or connect what was said to a relevant observation, idea or guess\nthat encourages and guides the client to continue contemplation. In this paper,\nwe present a method for distilling the generation of reflections from a\nFoundational Language Model (GPT-4) into smaller models. We first show that\nGPT-4, using zero-shot prompting, can generate reflections at near 100% success\nrate, superior to all previous methods. Using reflections generated by GPT-4,\nwe fine-tune different sizes of the GPT-2 family. The GPT-2-small model\nachieves 83% success on a hold-out test set and the GPT-2 XL achieves 90%\nsuccess. We also show that GPT-4 can help in the labor-intensive task of\nevaluating the quality of the distilled models, using it as a zero-shot\nclassifier. Using triple-human review as a guide, the classifier achieves a\nCohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
        "pdf_link": "https://arxiv.org/pdf/2402.01051v1.pdf"
    },
    {
        "title": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models",
        "authors": [
            "Zihao Lin",
            "Mohammad Beigi",
            "Hongxuan Li",
            "Yufan Zhou",
            "Yuxiang Zhang",
            "Qifan Wang",
            "Wenpeng Yin",
            "Lifu Huang"
        ],
        "published": "2024-02-16T23:08:55Z",
        "summary": "Memory Editing (ME) has emerged as an efficient method to modify erroneous\nfacts or inject new facts into Large Language Models (LLMs). Two mainstream ME\nmethods exist: parameter-modifying ME and parameter-preserving ME (integrating\nextra modules while preserving original parameters). Regrettably, previous\nstudies on ME evaluation have two critical limitations: (i) evaluating LLMs\nwith single edit only, neglecting the need for continuous editing, and (ii)\nevaluations focusing solely on basic factual triples, overlooking broader LLM\ncapabilities like logical reasoning and reading understanding. This study\naddresses these limitations with contributions threefold: (i) We explore how ME\naffects a wide range of fundamental capabilities of LLMs under sequential\nediting. Experimental results reveal an intriguing phenomenon: Most\nparameter-modifying ME consistently degrade performance across all tasks after\na few sequential edits. In contrast, parameter-preserving ME effectively\nmaintains LLMs' fundamental capabilities but struggles to accurately recall\nedited knowledge presented in a different format. (ii) We extend our evaluation\nto different editing settings, such as layers to edit, model size, instruction\ntuning, etc. Experimental findings indicate several strategies that can\npotentially mitigate the adverse effects of ME. (iii) We further explain why\nparameter-modifying ME damages LLMs from three dimensions: parameter changes\nafter editing, language modeling capability, and the in-context learning\ncapability. Our in-depth study advocates more careful use of ME in real-world\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11122v1.pdf"
    },
    {
        "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
        "authors": [
            "Pau Rodriguez Inserte",
            "Mariam Nakhl\u00e9",
            "Raheel Qader",
            "Gaetan Caillaut",
            "Jingshu Liu"
        ],
        "published": "2024-01-26T11:04:01Z",
        "summary": "Natural language processing (NLP) has recently gained relevance within\nfinancial institutions by providing highly valuable insights into companies and\nmarkets' financial documents. However, the landscape of the financial domain\npresents extra challenges for NLP, due to the complexity of the texts and the\nuse of specific terminology. Generalist language models tend to fall short in\ntasks specifically tailored for finance, even when using large language models\n(LLMs) with great natural language understanding and generative capabilities.\nThis paper presents a study on LLM adaptation methods targeted at the financial\ndomain and with high emphasis on financial sentiment analysis. To this purpose,\ntwo foundation models with less than 1.5B parameters have been adapted using a\nwide range of strategies. We show that through careful fine-tuning on both\nfinancial documents and instructions, these foundation models can be adapted to\nthe target domain. Moreover, we observe that small LLMs have comparable\nperformance to larger scale models, while being more efficient in terms of\nparameters and data. In addition to the models, we show how to generate\nartificial instructions through LLMs to augment the number of samples of the\ninstruction dataset.",
        "pdf_link": "https://arxiv.org/pdf/2401.14777v1.pdf"
    },
    {
        "title": "RL-GPT: Integrating Reinforcement Learning and Code-as-policy",
        "authors": [
            "Shaoteng Liu",
            "Haoqi Yuan",
            "Minda Hu",
            "Yanwei Li",
            "Yukang Chen",
            "Shu Liu",
            "Zongqing Lu",
            "Jiaya Jia"
        ],
        "published": "2024-02-29T16:07:22Z",
        "summary": "Large Language Models (LLMs) have demonstrated proficiency in utilizing\nvarious tools by coding, yet they face limitations in handling intricate logic\nand precise control. In embodied tasks, high-level planning is amenable to\ndirect coding, while low-level actions often necessitate task-specific\nrefinement, such as Reinforcement Learning (RL). To seamlessly integrate both\nmodalities, we introduce a two-level hierarchical framework, RL-GPT, comprising\na slow agent and a fast agent. The slow agent analyzes actions suitable for\ncoding, while the fast agent executes coding tasks. This decomposition\neffectively focuses each agent on specific tasks, proving highly efficient\nwithin our pipeline. Our approach outperforms traditional RL methods and\nexisting GPT agents, demonstrating superior efficiency. In the Minecraft game,\nit rapidly obtains diamonds within a single day on an RTX3090. Additionally, it\nachieves SOTA performance across all designated MineDojo tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.19299v1.pdf"
    },
    {
        "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning",
        "authors": [
            "Adib Hasan",
            "Ileana Rugina",
            "Alex Wang"
        ],
        "published": "2024-01-19T18:05:34Z",
        "summary": "Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type\nof attack that can coax these models into generating harmful and illegal\ncontent. In this paper, we show that pruning up to 20% of LLM parameters\nmarkedly increases their resistance to such attacks without additional training\nand without sacrificing their performance in standard benchmarks. Intriguingly,\nwe discovered that the enhanced safety observed post-pruning correlates to the\ninitial safety training level of the model, hinting that the effect of pruning\ncould be more general and may hold for other LLM behaviors beyond safety.\nAdditionally, we introduce a curated dataset of 225 harmful tasks across five\ncategories, inserted into ten different Jailbreaking prompts, showing that\npruning aids LLMs in concentrating attention on task-relevant tokens in\njailbreaking prompts. Lastly, our experiments reveal that the prominent chat\nmodels, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high\nsusceptibility to jailbreaking attacks, with some categories achieving nearly\n70-100% success rate. These insights underline the potential of pruning as a\ngeneralizable approach for improving LLM safety, reliability, and potentially\nother desired behaviors.",
        "pdf_link": "https://arxiv.org/pdf/2401.10862v1.pdf"
    },
    {
        "title": "FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference",
        "authors": [
            "Zirui Liu",
            "Qingquan Song",
            "Qiang Charles Xiao",
            "Sathiya Keerthi Selvaraj",
            "Rahul Mazumder",
            "Aman Gupta",
            "Xia Hu"
        ],
        "published": "2024-01-08T17:29:16Z",
        "summary": "The large number of parameters in Pretrained Language Models enhance their\nperformance, but also make them resource-intensive, making it challenging to\ndeploy them on commodity hardware like a single GPU. Due to the memory and\npower limitations of these devices, model compression techniques are often used\nto decrease both the model's size and its inference latency. This usually\nresults in a trade-off between model accuracy and efficiency. Therefore,\noptimizing this balance is essential for effectively deploying LLMs on\ncommodity hardware. A significant portion of the efficiency challenge is the\nFeed-forward network (FFN) component, which accounts for roughly $\\frac{2}{3}$\ntotal parameters and inference latency. In this paper, we first observe that\nonly a few neurons of FFN module have large output norm for any input tokens,\na.k.a. heavy hitters, while the others are sparsely triggered by different\ntokens. Based on this observation, we explicitly split the FFN into two parts\naccording to the heavy hitters. We improve the efficiency-accuracy trade-off of\nexisting compression methods by allocating more resource to FFN parts with\nheavy hitters. In practice, our method can reduce model size by 43.1\\% and\nbring $1.25\\sim1.56\\times$ wall clock time speedup on different hardware with\nnegligible accuracy drop.",
        "pdf_link": "https://arxiv.org/pdf/2401.04044v1.pdf"
    },
    {
        "title": "IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification",
        "authors": [
            "Abdullah Alsuhaibani",
            "Hamad Zogan",
            "Imran Razzak",
            "Shoaib Jameel",
            "Guandong Xu"
        ],
        "published": "2024-01-08T17:07:37Z",
        "summary": "Language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have been very effective in various Natural Language\nProcessing (NLP) and text mining tasks including text classification. However,\nsome tasks still pose challenges for these models, including text\nclassification with limited labels. This can result in a cold-start problem.\nAlthough some approaches have attempted to address this problem through\nsingle-stage clustering as an intermediate training step coupled with a\npre-trained language model, which generates pseudo-labels to improve\nclassification, these methods are often error-prone due to the limitations of\nthe clustering algorithms. To overcome this, we have developed a novel\ntwo-stage intermediate clustering with subsequent fine-tuning that models the\npseudo-labels reliably, resulting in reduced prediction errors. The key novelty\nin our model, IDoFew, is that the two-stage clustering coupled with two\ndifferent clustering algorithms helps exploit the advantages of the\ncomplementary algorithms that reduce the errors in generating reliable\npseudo-labels for fine-tuning. Our approach has shown significant improvements\ncompared to strong comparative models.",
        "pdf_link": "https://arxiv.org/pdf/2401.04025v1.pdf"
    },
    {
        "title": "VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model",
        "authors": [
            "Pengying Wu",
            "Yao Mu",
            "Bingxian Wu",
            "Yi Hou",
            "Ji Ma",
            "Shanghang Zhang",
            "Chang Liu"
        ],
        "published": "2024-01-05T08:05:07Z",
        "summary": "In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)\ntask empowers agents to adeptly traverse unfamiliar environments and locate\nobjects from novel categories without prior explicit training. This paper\nintroduces VoroNav, a novel semantic exploration framework that proposes the\nReduced Voronoi Graph to extract exploratory paths and planning nodes from a\nsemantic map constructed in real time. By harnessing topological and semantic\ninformation, VoroNav designs text-based descriptions of paths and images that\nare readily interpretable by a large language model (LLM). In particular, our\napproach presents a synergy of path and farsight descriptions to represent the\nenvironmental context, enabling LLM to apply commonsense reasoning to ascertain\nwaypoints for navigation. Extensive evaluation on HM3D and HSSD validates\nVoroNav surpasses existing benchmarks in both success rate and exploration\nefficiency (absolute improvement: +2.8% Success and +3.7% SPL on HM3D, +2.6%\nSuccess and +3.8% SPL on HSSD). Additionally introduced metrics that evaluate\nobstacle avoidance proficiency and perceptual efficiency further corroborate\nthe enhancements achieved by our method in ZSON planning. Project page:\nhttps://voro-nav.github.io",
        "pdf_link": "https://arxiv.org/pdf/2401.02695v2.pdf"
    },
    {
        "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
        "authors": [
            "Zhihao Chen",
            "Tao Chen",
            "Chenhui Wang",
            "Chuang Niu",
            "Ge Wang",
            "Hongming Shan"
        ],
        "published": "2024-03-10T08:21:50Z",
        "summary": "While various deep learning methods were proposed for low-dose computed\ntomography (CT) denoising, they often suffer from over-smoothing, blurring, and\nlack of explainability. To alleviate these issues, we propose a plug-and-play\nLanguage-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT\ndenoising models. Our idea is to leverage large language models (LLMs) to align\ndenoised CT and normal dose CT images in both the continuous perceptual space\nand discrete semantic space, which is the first LLM-based scheme for low-dose\nCT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided\nCT autoencoder, which can encode a CT image into continuous high-level features\nand quantize them into a token space to produce semantic tokens derived from\nthe LLM's vocabulary; and the second is to minimize the discrepancy between the\ndenoised CT images and normal dose CT in terms of both encoded high-level\nfeatures and quantized token embeddings derived by the LLM-guided CT\nautoencoder. Extensive experimental results on two public LDCT denoising\ndatasets demonstrate that our LEDA can enhance existing denoising models in\nterms of quantitative metrics and qualitative evaluation, and also provide\nexplainability through language-level image understanding. Source code is\navailable at https://github.com/hao1635/LEDA.",
        "pdf_link": "https://arxiv.org/pdf/2403.06128v1.pdf"
    },
    {
        "title": "Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean",
        "authors": [
            "ChangSu Choi",
            "Yongbin Jeong",
            "Seoyoon Park",
            "InHo Won",
            "HyeonSeok Lim",
            "SangMin Kim",
            "Yejee Kang",
            "Chanhyuk Yoon",
            "Jaewan Park",
            "Yiseul Lee",
            "HyeJin Lee",
            "Younggyun Hahm",
            "Hansaem Kim",
            "KyungTae Lim"
        ],
        "published": "2024-03-16T10:26:38Z",
        "summary": "Large language models (LLMs) use pretraining to predict the subsequent word;\nhowever, their expansion requires significant computing resources. Numerous big\ntech companies and research institutes have developed multilingual LLMs (MLLMs)\nto meet current demands, overlooking less-resourced languages (LRLs). This\nstudy proposed three strategies to enhance the performance of LRLs based on the\npublicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to\nenhance expressiveness. Second, bilingual data were used for pretraining to\nalign the high- and less-resourced languages. Third, a high-quality small-scale\ninstruction dataset was constructed and instruction-tuning was performed to\naugment the LRL. The experiments employed the Llama2 model and Korean was used\nas the LRL, which was quantitatively evaluated against other developed LLMs\nacross eight tasks. Furthermore, a qualitative assessment was performed based\non human evaluation and GPT4. Experimental results showed that our proposed\nBllossom model exhibited superior performance in qualitative analyses compared\nto previously proposed Korean monolingual models.",
        "pdf_link": "https://arxiv.org/pdf/2403.10882v2.pdf"
    },
    {
        "title": "MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models",
        "authors": [
            "Divyanshu Aggarwal",
            "Ashutosh Sathe",
            "Ishaan Watts",
            "Sunayana Sitaram"
        ],
        "published": "2024-01-15T11:06:43Z",
        "summary": "Parameter Efficient Finetuning (PEFT) has emerged as a viable solution for\nimproving the performance of Large Language Models (LLMs) without requiring\nmassive resources and compute. Prior work on multilingual evaluation has shown\nthat there is a large gap between the performance of LLMs on English and other\nlanguages. Further, there is also a large gap between the performance of\nsmaller open-source models and larger LLMs. Finetuning can be an effective way\nto bridge this gap and make language models more equitable. In this work, we\nfinetune the LLama-2-7B and Mistral-7B models on two synthetic multilingual\ninstruction tuning datasets to determine its effect on model performance on six\ndownstream tasks covering forty languages in all. Additionally, we experiment\nwith various parameters, such as rank for low-rank adaptation and values of\nquantisation to determine their effects on downstream performance and find that\nhigher rank and higher quantisation values benefit low-resource languages. We\nfind that PEFT of smaller open-source models sometimes bridges the gap between\nthe performance of these models and the larger ones, however, English\nperformance can take a hit. We also find that finetuning sometimes improves\nperformance on low-resource languages, while degrading performance on\nhigh-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2401.07598v2.pdf"
    },
    {
        "title": "Detecting Mode Collapse in Language Models via Narration",
        "authors": [
            "Sil Hamilton"
        ],
        "published": "2024-02-06T23:52:58Z",
        "summary": "No two authors write alike. Personal flourishes invoked in written\nnarratives, from lexicon to rhetorical devices, imply a particular author--what\nliterary theorists label the implied or virtual author; distinct from the real\nauthor or narrator of a text. Early large language models trained on unfiltered\ntraining sets drawn from a variety of discordant sources yielded incoherent\npersonalities, problematic for conversational tasks but proving useful for\nsampling literature from multiple perspectives. Successes in alignment research\nin recent years have allowed researchers to impose subjectively consistent\npersonae on language models via instruction tuning and reinforcement learning\nfrom human feedback (RLHF), but whether aligned models retain the ability to\nmodel an arbitrary virtual author has received little scrutiny. By studying\n4,374 stories sampled from three OpenAI language models, we show successive\nversions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby\noverfitting the model during alignment constrains it from generalizing over\nauthorship: models suffering from mode collapse become unable to assume a\nmultiplicity of perspectives. Our method and results are significant for\nresearchers seeking to employ language models in sociological simulations.",
        "pdf_link": "https://arxiv.org/pdf/2402.04477v1.pdf"
    },
    {
        "title": "Enhancing Robotic Manipulation with AI Feedback from Multimodal Large Language Models",
        "authors": [
            "Jinyi Liu",
            "Yifu Yuan",
            "Jianye Hao",
            "Fei Ni",
            "Lingzhi Fu",
            "Yibin Chen",
            "Yan Zheng"
        ],
        "published": "2024-02-22T03:14:03Z",
        "summary": "Recently, there has been considerable attention towards leveraging large\nlanguage models (LLMs) to enhance decision-making processes. However, aligning\nthe natural language text instructions generated by LLMs with the vectorized\noperations required for execution presents a significant challenge, often\nnecessitating task-specific details. To circumvent the need for such\ntask-specific granularity, inspired by preference-based policy learning\napproaches, we investigate the utilization of multimodal LLMs to provide\nautomated preference feedback solely from image inputs to guide\ndecision-making. In this study, we train a multimodal LLM, termed CriticGPT,\ncapable of understanding trajectory videos in robot manipulation tasks, serving\nas a critic to offer analysis and preference feedback. Subsequently, we\nvalidate the effectiveness of preference labels generated by CriticGPT from a\nreward modeling perspective. Experimental evaluation of the algorithm's\npreference accuracy demonstrates its effective generalization ability to new\ntasks. Furthermore, performance on Meta-World tasks reveals that CriticGPT's\nreward model efficiently guides policy learning, surpassing rewards based on\nstate-of-the-art pre-trained representation models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14245v1.pdf"
    },
    {
        "title": "GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events",
        "authors": [
            "Xingcheng Zhou",
            "Alois C. Knoll"
        ],
        "published": "2024-02-03T16:38:25Z",
        "summary": "The recognition and understanding of traffic incidents, particularly traffic\naccidents, is a topic of paramount importance in the realm of intelligent\ntransportation systems and intelligent vehicles. This area has continually\ncaptured the extensive focus of both the academic and industrial sectors.\nIdentifying and comprehending complex traffic events is highly challenging,\nprimarily due to the intricate nature of traffic environments, diverse\nobservational perspectives, and the multifaceted causes of accidents. These\nfactors have persistently impeded the development of effective solutions. The\nadvent of large vision-language models (VLMs) such as GPT-4V, has introduced\ninnovative approaches to addressing this issue. In this paper, we explore the\nability of GPT-4V with a set of representative traffic incident videos and\ndelve into the model's capacity of understanding these complex traffic\nsituations. We observe that GPT-4V demonstrates remarkable cognitive,\nreasoning, and decision-making ability in certain classic traffic events.\nConcurrently, we also identify certain limitations of GPT-4V, which constrain\nits understanding in more intricate scenarios. These limitations merit further\nexploration and resolution.",
        "pdf_link": "https://arxiv.org/pdf/2402.02205v3.pdf"
    },
    {
        "title": "PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models",
        "authors": [
            "Wendi Cui",
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Hao Sun",
            "Damien Lopez",
            "Kamalika Das",
            "Bradley Malin",
            "Sricharan Kumar"
        ],
        "published": "2024-02-17T17:47:10Z",
        "summary": "Crafting an ideal prompt for Large Language Models (LLMs) is a challenging\ntask that demands significant resources and expert human input. Existing work\ntreats the optimization of prompt instruction and in-context learning examples\nas distinct problems, leading to sub-optimal prompt performance. This research\naddresses this limitation by establishing a unified in-context prompt\noptimization framework, which aims to achieve joint optimization of the prompt\ninstruction and examples. However, formulating such optimization in the\ndiscrete and high-dimensional natural language space introduces challenges in\nterms of convergence and computational efficiency. To overcome these issues, we\npresent PhaseEvo, an efficient automatic prompt optimization framework that\ncombines the generative capability of LLMs with the global search proficiency\nof evolution algorithms. Our framework features a multi-phase design\nincorporating innovative LLM-based mutation operators to enhance search\nefficiency and accelerate convergence. We conduct an extensive evaluation of\nour approach across 35 benchmark tasks. The results demonstrate that PhaseEvo\nsignificantly outperforms the state-of-the-art baseline methods by a large\nmargin whilst maintaining good efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.11347v1.pdf"
    },
    {
        "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty",
        "authors": [
            "Kaiqu Liang",
            "Zixu Zhang",
            "Jaime Fern\u00e1ndez Fisac"
        ],
        "published": "2024-02-09T16:40:59Z",
        "summary": "Large language models (LLMs) exhibit advanced reasoning skills, enabling\nrobots to comprehend natural language instructions and strategically plan\nhigh-level actions through proper grounding. However, LLM hallucination may\nresult in robots confidently executing plans that are misaligned with user\ngoals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural\nlanguage instructions can induce task uncertainty, particularly in situations\nwhere multiple valid options exist. To address this issue, LLMs must identify\nsuch uncertainty and proactively seek clarification. This paper explores the\nconcept of introspective planning as a systematic method for guiding LLMs in\nforming uncertainty--aware plans for robotic task execution without the need\nfor fine-tuning. We investigate uncertainty quantification in task-level robot\nplanning and demonstrate that introspection significantly improves both success\nrates and safety compared to state-of-the-art LLM-based planning approaches.\nFurthermore, we assess the effectiveness of introspective planning in\nconjunction with conformal prediction, revealing that this combination yields\ntighter confidence bounds, thereby maintaining statistical success guarantees\nwith fewer superfluous user clarification queries.",
        "pdf_link": "https://arxiv.org/pdf/2402.06529v2.pdf"
    },
    {
        "title": "Online Cascade Learning for Efficient Inference over Streams",
        "authors": [
            "Lunyiu Nie",
            "Zhimin Ding",
            "Erdong Hu",
            "Christopher Jermaine",
            "Swarat Chaudhuri"
        ],
        "published": "2024-02-07T01:46:50Z",
        "summary": "Large Language Models (LLMs) have a natural role in answering complex queries\nabout data streams, but the high computational cost of LLM inference makes them\ninfeasible in many such tasks. We propose online cascade learning, the first\napproach to addressing this challenge. The objective here is to learn a\n\"cascade\" of models, starting with lower-capacity models (such as logistic\nregressors) and ending with a powerful LLM, along with a deferral policy that\ndetermines the model that is used on a given input. We formulate the task of\nlearning cascades online as an imitation-learning problem and give a no-regret\nalgorithm for the problem. Experimental results across four benchmarks show\nthat our method parallels LLMs in accuracy while cutting down inference costs\nby as much as 90%, underscoring its efficacy and adaptability in stream\nprocessing.",
        "pdf_link": "https://arxiv.org/pdf/2402.04513v1.pdf"
    },
    {
        "title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
        "authors": [
            "Ivar Frisch",
            "Mario Giulianelli"
        ],
        "published": "2024-02-05T11:05:20Z",
        "summary": "While both agent interaction and personalisation are vibrant topics in\nresearch on large language models (LLMs), there has been limited focus on the\neffect of language interaction on the behaviour of persona-conditioned LLM\nagents. Such an endeavour is important to ensure that agents remain consistent\nto their assigned traits yet are able to engage in open, naturalistic\ndialogues. In our experiments, we condition GPT-3.5 on personality profiles\nthrough prompting and create a two-group population of LLM agents using a\nsimple variability-inducing sampling algorithm. We then administer personality\ntests and submit the agents to a collaborative writing task, finding that\ndifferent profiles exhibit different degrees of personality consistency and\nlinguistic alignment to their conversational partners. Our study seeks to lay\nthe groundwork for better understanding of dialogue-based interaction between\nLLMs and highlights the need for new approaches to crafting robust, more\nhuman-like LLM personas for interactive environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.02896v1.pdf"
    },
    {
        "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation",
        "authors": [
            "Shiqi Chen",
            "Miao Xiong",
            "Junteng Liu",
            "Zhengxuan Wu",
            "Teng Xiao",
            "Siyang Gao",
            "Junxian He"
        ],
        "published": "2024-03-03T15:53:41Z",
        "summary": "Large language models (LLMs) frequently hallucinate and produce factual\nerrors, yet our understanding of why they make these errors remains limited. In\nthis study, we delve into the underlying mechanisms of LLM hallucinations from\nthe perspective of inner representations, and discover a salient pattern\nassociated with hallucinations: correct generations tend to have sharper\ncontext activations in the hidden states of the in-context tokens, compared to\nthe incorrect ones. Leveraging this insight, we propose an entropy-based metric\nto quantify the ``sharpness'' among the in-context hidden states and\nincorporate it into the decoding process to formulate a constrained decoding\napproach. Experiments on various knowledge-seeking and hallucination benchmarks\ndemonstrate our approach's consistent effectiveness, for example, achieving up\nto an 8.6 point improvement on TruthfulQA. We believe this study can improve\nour understanding of hallucinations and serve as a practical solution for\nhallucination mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2403.01548v3.pdf"
    },
    {
        "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
        "authors": [
            "Zehui Chen",
            "Kuikun Liu",
            "Qiuchen Wang",
            "Wenwei Zhang",
            "Jiangning Liu",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "published": "2024-03-19T16:26:10Z",
        "summary": "Open-sourced Large Language Models (LLMs) have achieved great success in\nvarious NLP tasks, however, they are still far inferior to API-based models\nwhen acting as agents. How to integrate agent ability into general LLMs becomes\na crucial and urgent problem. This paper first delivers three key observations:\n(1) the current agent training corpus is entangled with both formats following\nand agent reasoning, which significantly shifts from the distribution of its\npre-training data; (2) LLMs exhibit different learning speeds on the\ncapabilities required by agent tasks; and (3) current approaches have\nside-effects when improving agent abilities by introducing hallucinations.\nBased on the above findings, we propose Agent-FLAN to effectively Fine-tune\nLANguage models for Agents. Through careful decomposition and redesign of the\ntraining corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by\n3.5\\% across various agent evaluation datasets. With comprehensively\nconstructed negative samples, Agent-FLAN greatly alleviates the hallucination\nissues based on our established evaluation benchmark. Besides, it consistently\nimproves the agent capability of LLMs when scaling model sizes while slightly\nenhancing the general capability of LLMs. The code will be available at\nhttps://github.com/InternLM/Agent-FLAN.",
        "pdf_link": "https://arxiv.org/pdf/2403.12881v1.pdf"
    },
    {
        "title": "Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting",
        "authors": [
            "Masahiro Kaneko",
            "Danushka Bollegala",
            "Naoaki Okazaki",
            "Timothy Baldwin"
        ],
        "published": "2024-01-28T06:50:10Z",
        "summary": "There exist both scalable tasks, like reading comprehension and\nfact-checking, where model performance improves with model size, and unscalable\ntasks, like arithmetic reasoning and symbolic reasoning, where model\nperformance does not necessarily improve with model size. Large language models\n(LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate\nincremental predictions even on unscalable tasks. Unfortunately, despite their\nexceptional reasoning abilities, LLMs tend to internalize and reproduce\ndiscriminatory societal biases. Whether CoT can provide discriminatory or\negalitarian rationalizations for the implicit information in unscalable tasks\nremains an open question.\n  In this study, we examine the impact of LLMs' step-by-step predictions on\ngender bias in unscalable tasks. For this purpose, we construct a benchmark for\nan unscalable task where the LLM is given a list of words comprising feminine,\nmasculine, and gendered occupational words, and is required to count the number\nof feminine and masculine words. In our CoT prompts, we require the LLM to\nexplicitly indicate whether each word in the word list is a feminine or\nmasculine before making the final predictions. With counting and handling the\nmeaning of words, this benchmark has characteristics of both arithmetic\nreasoning and symbolic reasoning. Experimental results in English show that\nwithout step-by-step prediction, most LLMs make socially biased predictions,\ndespite the task being as simple as counting words. Interestingly, CoT\nprompting reduces this unconscious social bias in LLMs and encourages fair\npredictions.",
        "pdf_link": "https://arxiv.org/pdf/2401.15585v1.pdf"
    },
    {
        "title": "Guiding Enumerative Program Synthesis with Large Language Models",
        "authors": [
            "Yixuan Li",
            "Julian Parsert",
            "Elizabeth Polgreen"
        ],
        "published": "2024-03-06T19:13:53Z",
        "summary": "Pre-trained Large Language Models (LLMs) are beginning to dominate the\ndiscourse around automatic code generation with natural language\nspecifications. In contrast, the best-performing synthesizers in the domain of\nformal synthesis with precise logical specifications are still based on\nenumerative algorithms. In this paper, we evaluate the abilities of LLMs to\nsolve formal synthesis benchmarks by carefully crafting a library of prompts\nfor the domain. When one-shot synthesis fails, we propose a novel enumerative\nsynthesis algorithm, which integrates calls to an LLM into a weighted\nprobabilistic search. This allows the synthesizer to provide the LLM with\ninformation about the progress of the enumerator, and the LLM to provide the\nenumerator with syntactic guidance in an iterative loop. We evaluate our\ntechniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition.\nWe find that GPT-3.5 as a stand-alone tool for formal synthesis is easily\noutperformed by state-of-the-art formal synthesis algorithms, but our approach\nintegrating the LLM into an enumerative synthesis algorithm shows significant\nperformance gains over both the LLM and the enumerative synthesizer alone and\nthe winning SyGuS competition tool.",
        "pdf_link": "https://arxiv.org/pdf/2403.03997v1.pdf"
    },
    {
        "title": "LLMs for Test Input Generation for Semantic Caches",
        "authors": [
            "Zafaryab Rasool",
            "Scott Barnett",
            "David Willie",
            "Stefanus Kurniawan",
            "Sherwin Balugo",
            "Srikanth Thudumu",
            "Mohamed Abdelrazek"
        ],
        "published": "2024-01-16T06:16:33Z",
        "summary": "Large language models (LLMs) enable state-of-the-art semantic capabilities to\nbe added to software systems such as semantic search of unstructured documents\nand text generation. However, these models are computationally expensive. At\nscale, the cost of serving thousands of users increases massively affecting\nalso user experience. To address this problem, semantic caches are used to\ncheck for answers to similar queries (that may have been phrased differently)\nwithout hitting the LLM service. Due to the nature of these semantic cache\ntechniques that rely on query embeddings, there is a high chance of errors\nimpacting user confidence in the system. Adopting semantic cache techniques\nusually requires testing the effectiveness of a semantic cache (accurate cache\nhits and misses) which requires a labelled test set of similar queries and\nresponses which is often unavailable. In this paper, we present VaryGen, an\napproach for using LLMs for test input generation that produces similar\nquestions from unstructured text documents. Our novel approach uses the\nreasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise\nsubtle variations to queries, and 3) evaluate the synthesised test dataset. We\nevaluated our approach in the domain of a student question and answer system by\nqualitatively analysing 100 generated queries and result pairs, and conducting\nan empirical case study with an open source semantic cache. Our results show\nthat query pairs satisfy human expectations of similarity and our generated\ndata demonstrates failure cases of a semantic cache. Additionally, we also\nevaluate our approach on Qasper dataset. This work is an important first step\ninto test input generation for semantic applications and presents\nconsiderations for practitioners when calibrating a semantic cache.",
        "pdf_link": "https://arxiv.org/pdf/2401.08138v1.pdf"
    },
    {
        "title": "GreenLLaMA: A Framework for Detoxification with Explanations",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Muhammad Abdul-Mageed",
            "Laks V. S. Lakshmanan"
        ],
        "published": "2024-02-25T01:56:47Z",
        "summary": "Prior works on detoxification are scattered in the sense that they do not\ncover all aspects of detoxification needed in a real-world scenario. Notably,\nprior works restrict the task of developing detoxification models to only a\nseen subset of platforms, leaving the question of how the models would perform\non unseen platforms unexplored. Additionally, these works do not address\nnon-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified\nwithout altering the meaning. We propose GreenLLaMA, the first comprehensive\nend-to-end detoxification framework, which attempts to alleviate the\naforementioned limitations. We first introduce a cross-platform pseudo-parallel\ncorpus applying multi-step data processing and generation strategies leveraging\nChatGPT. We then train a suite of detoxification models with our cross-platform\ncorpus. We show that our detoxification models outperform the SoTA model\ntrained with human-annotated parallel corpus. We further introduce explanation\nto promote transparency and trustworthiness. GreenLLaMA additionally offers a\nunique paraphrase detector especially dedicated for the detoxification task to\ntackle the non-detoxifiable cases. Through experimental analysis, we\ndemonstrate the effectiveness of our cross-platform corpus and the robustness\nof GreenLLaMA against adversarial toxicity.",
        "pdf_link": "https://arxiv.org/pdf/2402.15951v1.pdf"
    },
    {
        "title": "Data is all you need: Finetuning LLMs for Chip Design via an Automated design-data augmentation framework",
        "authors": [
            "Kaiyan Chang",
            "Kun Wang",
            "Nan Yang",
            "Ying Wang",
            "Dantong Jin",
            "Wenlong Zhu",
            "Zhirong Chen",
            "Cangyuan Li",
            "Hao Yan",
            "Yunhao Zhou",
            "Zhuoliang Zhao",
            "Yuan Cheng",
            "Yudong Pan",
            "Yiqi Liu",
            "Mengdi Wang",
            "Shengwen Liang",
            "yinhe han",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "published": "2024-03-17T13:01:03Z",
        "summary": "Recent advances in large language models have demonstrated their potential\nfor automated generation of hardware description language (HDL) code from\nhigh-level prompts. Researchers have utilized fine-tuning to enhance the\nability of these large language models (LLMs) in the field of Chip Design.\nHowever, the lack of Verilog data hinders further improvement in the quality of\nVerilog generation by LLMs. Additionally, the absence of a Verilog and\nElectronic Design Automation (EDA) script data augmentation framework\nsignificantly increases the time required to prepare the training dataset for\nLLM trainers. This paper proposes an automated design-data augmentation\nframework, which generates high-volume and high-quality natural language\naligned with Verilog and EDA scripts. For Verilog generation, it translates\nVerilog files to an abstract syntax tree and then maps nodes to natural\nlanguage with a predefined template. For Verilog repair, it uses predefined\nrules to generate the wrong verilog file and then pairs EDA Tool feedback with\nthe right and wrong verilog file. For EDA Script generation, it uses existing\nLLM(GPT-3.5) to obtain the description of the Script. To evaluate the\neffectiveness of our data augmentation method, we finetune Llama2-13B and\nLlama2-7B models using the dataset generated by our augmentation framework. The\nresults demonstrate a significant improvement in the Verilog generation tasks\nwith LLMs. Moreover, the accuracy of Verilog generation surpasses that of the\ncurrent state-of-the-art open-source Verilog generation model, increasing from\n58.8% to 70.6% with the same benchmark. Our 13B model (ChipGPT-FT) has a pass\nrate improvement compared with GPT-3.5 in Verilog generation and outperforms in\nEDA script (i.e., SiliconCompiler) generation with only 200 EDA script data.",
        "pdf_link": "https://arxiv.org/pdf/2403.11202v1.pdf"
    },
    {
        "title": "Knowledge Distillation for Closed-Source Language Models",
        "authors": [
            "Hongzhan Chen",
            "Xiaojun Quan",
            "Hehong Chen",
            "Ming Yan",
            "Ji Zhang"
        ],
        "published": "2024-01-13T08:43:32Z",
        "summary": "Closed-source language models such as GPT-4 have achieved remarkable\nperformance. Many recent studies focus on enhancing the capabilities of smaller\nmodels through knowledge distillation from closed-source language models.\nHowever, due to the incapability to directly access the weights, hidden states,\nand output distributions of these closed-source models, the distillation can\nonly be performed by fine-tuning smaller models with data samples generated by\nclosed-source language models, which constrains the effectiveness of knowledge\ndistillation. In this paper, we propose to estimate the output distributions of\nclosed-source language models within a Bayesian estimation framework, involving\nboth prior and posterior estimation. The prior estimation aims to derive a\nprior distribution by utilizing the corpus generated by closed-source language\nmodels, while the posterior estimation employs a proxy model to update the\nprior distribution and derive a posterior distribution. By leveraging the\nestimated output distribution of closed-source language models, traditional\nknowledge distillation can be executed. Experimental results demonstrate that\nour method surpasses the performance of current models directly fine-tuned on\ndata generated by closed-source language models.",
        "pdf_link": "https://arxiv.org/pdf/2401.07013v1.pdf"
    },
    {
        "title": "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering",
        "authors": [
            "Zihan Zhang",
            "Meng Fang",
            "Ling Chen"
        ],
        "published": "2024-02-26T09:59:04Z",
        "summary": "Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine\nthe necessity of retrieval for queries instead of retrieving indiscriminately\nto enhance the efficiency and relevance of the sourced information. However,\nprevious works largely overlook the evaluation of ARAG approaches, leading to\ntheir effectiveness being understudied. This work presents a benchmark,\nRetrievalQA, comprising 1,271 short-form questions covering new world and\nlong-tail knowledge. The knowledge necessary to answer the questions is absent\nfrom LLMs; therefore, external information must be retrieved to answer\ncorrectly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG\nmethods. We observe that calibration-based methods heavily rely on threshold\ntuning, while vanilla prompting is inadequate for guiding LLMs to make reliable\nretrieval decisions. Based on our findings, we propose Time-Aware Adaptive\nRetrieval (TA-ARE), a simple yet effective method that helps LLMs assess the\nnecessity of retrieval without calibration or additional training. The dataset\nand code will be available at \\url{https://github.com/hyintell/RetrievalQA}",
        "pdf_link": "https://arxiv.org/pdf/2402.16457v1.pdf"
    },
    {
        "title": "DataFrame QA: A Universal LLM Framework on DataFrame Question Answering Without Data Exposure",
        "authors": [
            "Junyi Ye",
            "Mengnan Du",
            "Guiling Wang"
        ],
        "published": "2024-01-27T17:06:53Z",
        "summary": "This paper introduces DataFrame question answering (QA), a novel task that\nutilizes large language models (LLMs) to generate Pandas queries for\ninformation retrieval and data analysis on dataframes, emphasizing safe and\nnon-revealing data handling. Our method, which solely relies on dataframe\ncolumn names, not only ensures data privacy but also significantly reduces the\ncontext window in the prompt, streamlining information processing and\naddressing major challenges in LLM-based data analysis. We propose DataFrame QA\nas a comprehensive framework that includes safe Pandas query generation and\ncode execution. Various LLMs, notably GPT-4, are evaluated using the pass@1\nmetric on the renowned WikiSQL and our newly developed 'UCI-DataFrameQA',\ntailored for complex data analysis queries. Our findings indicate that GPT-4\nachieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,\nunderscoring its capability in securely retrieving and aggregating dataframe\nvalues and conducting sophisticated data analyses. This approach, deployable in\na zero-shot manner without prior training or adjustments, proves to be highly\nadaptable and secure for diverse applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.15463v1.pdf"
    },
    {
        "title": "Linguacodus: A Synergistic Framework for Transformative Code Generation in Machine Learning Pipelines",
        "authors": [
            "Ekaterina Trofimova",
            "Emil Sataev",
            "Andrey E. Ustyuzhanin"
        ],
        "published": "2024-03-18T08:58:47Z",
        "summary": "In the ever-evolving landscape of machine learning, seamless translation of\nnatural language descriptions into executable code remains a formidable\nchallenge. This paper introduces Linguacodus, an innovative framework designed\nto tackle this challenge by deploying a dynamic pipeline that iteratively\ntransforms natural language task descriptions into code through high-level\ndata-shaping instructions. The core of Linguacodus is a fine-tuned large\nlanguage model (LLM), empowered to evaluate diverse solutions for various\nproblems and select the most fitting one for a given task. This paper details\nthe fine-tuning process, and sheds light on how natural language descriptions\ncan be translated into functional code. Linguacodus represents a substantial\nleap towards automated code generation, effectively bridging the gap between\ntask descriptions and executable code. It holds great promise for advancing\nmachine learning applications across diverse domains. Additionally, we propose\nan algorithm capable of transforming a natural description of an ML task into\ncode with minimal human interaction. In extensive experiments on a vast machine\nlearning code dataset originating from Kaggle, we showcase the effectiveness of\nLinguacodus. The investigations highlight its potential applications across\ndiverse domains, emphasizing its impact on applied machine learning in various\nscientific fields.",
        "pdf_link": "https://arxiv.org/pdf/2403.11585v1.pdf"
    },
    {
        "title": "Can Language Models Act as Knowledge Bases at Scale?",
        "authors": [
            "Qiyuan He",
            "Yizhong Wang",
            "Wenya Wang"
        ],
        "published": "2024-02-22T04:20:14Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating responses to complex queries through large-scale\npre-training. However, the efficacy of these models in memorizing and reasoning\namong large-scale structured knowledge, especially world knowledge that\nexplicitly covers abundant factual information remains questionable. Addressing\nthis gap, our research investigates whether LLMs can effectively store, recall,\nand reason with knowledge on a large scale comparable to latest knowledge bases\n(KBs) such as Wikidata. Specifically, we focus on three crucial aspects to\nstudy the viability: (1) the efficiency of LLMs with different sizes in\nmemorizing the exact knowledge in the large-scale KB; (2) the flexibility of\nrecalling the memorized knowledge in response to natural language queries; (3)\nthe capability to infer new knowledge through reasoning. Our findings indicate\nthat while LLMs hold promise as large-scale KBs capable of retrieving and\nresponding with flexibility, enhancements in their reasoning capabilities are\nnecessary to fully realize their potential.",
        "pdf_link": "https://arxiv.org/pdf/2402.14273v1.pdf"
    },
    {
        "title": "DistiLLM: Towards Streamlined Distillation for Large Language Models",
        "authors": [
            "Jongwoo Ko",
            "Sungnyun Kim",
            "Tianyi Chen",
            "Se-Young Yun"
        ],
        "published": "2024-02-06T11:10:35Z",
        "summary": "Knowledge distillation (KD) is widely used for compressing a teacher model to\na smaller student model, reducing its inference cost and memory footprint while\npreserving model capabilities. However, current KD methods for auto-regressive\nsequence models (e.g., large language models) suffer from missing a\nstandardized objective function. Moreover, the recent use of student-generated\noutputs to address training-inference mismatches has significantly escalated\ncomputational costs. To tackle these issues, we introduce DistiLLM, a more\neffective and efficient KD framework for auto-regressive language models.\nDistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence\nloss, where we unveil and leverage its theoretical properties, and (2) an\nadaptive off-policy approach designed to enhance the efficiency in utilizing\nstudent-generated outputs. Extensive experiments, including\ninstruction-following tasks, demonstrate the effectiveness of DistiLLM in\nbuilding high-performing student models while achieving up to 4.3$\\times$\nspeedup compared to recent KD methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.03898v1.pdf"
    },
    {
        "title": "MEGAnno+: A Human-LLM Collaborative Annotation System",
        "authors": [
            "Hannah Kim",
            "Kushan Mitra",
            "Rafael Li Chen",
            "Sajjadur Rahman",
            "Dan Zhang"
        ],
        "published": "2024-02-28T04:58:07Z",
        "summary": "Large language models (LLMs) can label data faster and cheaper than humans\nfor various NLP tasks. Despite their prowess, LLMs may fall short in\nunderstanding of complex, sociocultural, or domain-specific context,\npotentially leading to incorrect annotations. Therefore, we advocate a\ncollaborative approach where humans and LLMs work together to produce reliable\nand high-quality labels. We present MEGAnno+, a human-LLM collaborative\nannotation system that offers effective LLM agent and annotation management,\nconvenient and robust LLM annotation, and exploratory verification of LLM\nlabels by humans.",
        "pdf_link": "https://arxiv.org/pdf/2402.18050v1.pdf"
    },
    {
        "title": "Feedback Loops With Language Models Drive In-Context Reward Hacking",
        "authors": [
            "Alexander Pan",
            "Erik Jones",
            "Meena Jagadeesan",
            "Jacob Steinhardt"
        ],
        "published": "2024-02-09T18:59:29Z",
        "summary": "Language models influence the external world: they query APIs that read and\nwrite to web pages, generate content that shapes human behavior, and run system\ncommands as autonomous agents. These interactions form feedback loops: LLM\noutputs affect the world, which in turn affect subsequent LLM outputs. In this\nwork, we show that feedback loops can cause in-context reward hacking (ICRH),\nwhere the LLM at test-time optimizes a (potentially implicit) objective but\ncreates negative side effects in the process. For example, consider an LLM\nagent deployed to increase Twitter engagement; the LLM may retrieve its\nprevious tweets into the context window and make them more controversial,\nincreasing engagement but also toxicity. We identify and study two processes\nthat lead to ICRH: output-refinement and policy-refinement. For these\nprocesses, evaluations on static datasets are insufficient -- they miss the\nfeedback effects and thus cannot capture the most harmful behavior. In\nresponse, we provide three recommendations for evaluation to capture more\ninstances of ICRH. As AI development accelerates, the effects of feedback loops\nwill proliferate, increasing the need to understand their role in shaping LLM\nbehavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.06627v1.pdf"
    },
    {
        "title": "EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling",
        "authors": [
            "Shimao Zhang",
            "Yu Bao",
            "Shujian Huang"
        ],
        "published": "2024-03-21T16:41:12Z",
        "summary": "Recently, Large Language Models (LLMs) have demonstrated outstanding\nperformance across a wide range of downstream language tasks. Temperature\nsampling is a commonly used decoding strategy for LLMs' generation process.\nHowever, a fixed temperature parameter is used in most cases, which may not\nalways be an optimal choice for balancing generation quality and diversity. In\nthis paper, we propose an effective Entropy-based Dynamic Temperature (EDT)\nSampling method, to achieve a more balanced performance in terms of both\ngeneration quality and diversity by dynamically selecting the temperature\nparameter. Additionally, we also show model performance and comprehensive\nanalyses for 4 different generation benchmarks. Our experiments show that EDT\nsignificantly outperforms the existing strategies across different tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.14541v2.pdf"
    },
    {
        "title": "SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models",
        "authors": [
            "Mengqi Zhou",
            "Jun Hou",
            "Chuanchen Luo",
            "Yuxi Wang",
            "Zhaoxiang Zhang",
            "Junran Peng"
        ],
        "published": "2024-03-23T03:23:29Z",
        "summary": "Due to its great application potential, large-scale scene generation has\ndrawn extensive attention in academia and industry. Recent research employs\npowerful generative models to create desired scenes and achieves promising\nresults. However, most of these methods represent the scene using 3D primitives\n(e.g. point cloud or radiance field) incompatible with the industrial pipeline,\nwhich leads to a substantial gap between academic research and industrial\ndeployment. Procedural Controllable Generation (PCG) is an efficient technique\nfor creating scalable and high-quality assets, but it is unfriendly for\nordinary users as it demands profound domain expertise. To address these\nissues, we resort to using the large language model (LLM) to drive the\nprocedural modeling. In this paper, we introduce a large-scale scene generation\nframework, SceneX, which can automatically produce high-quality procedural\nmodels according to designers' textual descriptions.Specifically, the proposed\nmethod comprises two components, PCGBench and PCGPlanner. The former\nencompasses an extensive collection of accessible procedural assets and\nthousands of hand-craft API documents. The latter aims to generate executable\nactions for Blender to produce controllable and precise 3D assets guided by the\nuser's instructions. Our SceneX can generate a city spanning 2.5 km times 2.5\nkm with delicate layout and geometric structures, drastically reducing the time\ncost from several weeks for professional PCG engineers to just a few hours for\nan ordinary user. Extensive experiments demonstrated the capability of our\nmethod in controllable large-scale scene generation and editing, including\nasset placement and season translation.",
        "pdf_link": "https://arxiv.org/pdf/2403.15698v1.pdf"
    },
    {
        "title": "Can Large Multimodal Models Uncover Deep Semantics Behind Images?",
        "authors": [
            "Yixin Yang",
            "Zheng Li",
            "Qingxiu Dong",
            "Heming Xia",
            "Zhifang Sui"
        ],
        "published": "2024-02-17T13:41:44Z",
        "summary": "Understanding the deep semantics of images is essential in the era dominated\nby social media. However, current research works primarily on the superficial\ndescription of images, revealing a notable deficiency in the systematic\ninvestigation of the inherent deep semantics. In this work, we introduce\nDEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs)\ncapacities of visual deep semantics. DEEPEVAL includes human-annotated dataset\nand three progressive subtasks: fine-grained description selection, in-depth\ntitle matching, and deep semantics understanding. Utilizing DEEPEVAL, we\nevaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a\nsubstantial gap between the deep semantic comprehension capabilities of\nexisting LMMs and humans. For example, GPT-4V is 30% behind humans in\nunderstanding deep semantics, even though it achieves human-comparable\nperformance in image description. Further analysis indicates that the\nintegration of description texts during the inference process notably enhances\nLMMs' ability to perceive deep semantics. Furthermore, our dataset is divided\ninto multiple categories, and we conducted a more detailed analysis within\nthese categories.",
        "pdf_link": "https://arxiv.org/pdf/2402.11281v1.pdf"
    },
    {
        "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
        "authors": [
            "Jiuhai Chen",
            "Jonas Mueller"
        ],
        "published": "2024-03-19T14:44:45Z",
        "summary": "Large Language Models have become the de facto approach to\nsequence-to-sequence text generation tasks, but for specialized tasks/domains,\na pretrained LLM lacks specific capabilities to produce accurate or\nwell-formatted responses. Supervised fine-tuning specializes a LLM by training\nit on dataset of example prompts with target responses, but real-world data\ntends to be noisy. While many fine-tuning algorithms exist, here we consider a\n\\emph{data-centric AI} perspective on LLM fine-tuning, studying how to\n\\emph{systematically} curate the training dataset to improve the LLM produced\nvia \\emph{any} fine-tuning algorithm.\n  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM\nEvaluation And Rectification) for instruction tuning datasets, that can be used\nwith any LLM and fine-tuning procedure. CLEAR estimates which training data is\nlow-quality and either filters or corrects it. Automatically identifying which\ndata to filter or correct is done via LLM-derived confidence estimates, to\nensure only confident modifications to the dataset. Unlike existing data\ncuration techniques, CLEAR is a comprehensive framework that can improve a\ndataset (and trained model outputs) without additional fine-tuning\ncomputations. We don't assume access to a stronger LLM than the model being\nfine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether\nCLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal\nthat CLEAR consistently improves the performance of fine-tuned models across\nmany datasets and models (like GPT-3.5 and Llama2).",
        "pdf_link": "https://arxiv.org/pdf/2403.12776v1.pdf"
    },
    {
        "title": "GRATH: Gradual Self-Truthifying for Large Language Models",
        "authors": [
            "Weixin Chen",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2024-01-22T19:00:08Z",
        "summary": "Truthfulness is paramount for large language models (LLMs) as they are\nincreasingly deployed in real-world applications. However, existing LLMs still\nstruggle with generating truthful content, as evidenced by their modest\nperformance on benchmarks like TruthfulQA. To address this issue, we propose\nGRAdual self-truTHifying (GRATH), a novel post-processing method to enhance\ntruthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate\npairwise truthfulness training data with each pair containing a question and\nits correct and incorrect answers, and then optimizes the model via direct\npreference optimization (DPO) to learn from the truthfulness difference between\nanswer pairs. GRATH iteratively refines truthfulness data and updates the\nmodel, leading to a gradual improvement in model truthfulness in a\nself-supervised manner. Empirically, we evaluate GRATH using different 7B-LLMs\nand compare with LLMs with similar or even larger sizes on benchmark datasets.\nOur results show that GRATH effectively improves LLMs' truthfulness without\ncompromising other core capabilities. Notably, GRATH achieves state-of-the-art\nperformance on TruthfulQA, with MC1 accuracy of 54.71% and MC2 accuracy of\n69.10%, which even surpass those on 70B-LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.12292v2.pdf"
    },
    {
        "title": "Collaborative decoding of critical tokens for boosting factuality of large language models",
        "authors": [
            "Lifeng Jin",
            "Baolin Peng",
            "Linfeng Song",
            "Haitao Mi",
            "Ye Tian",
            "Dong Yu"
        ],
        "published": "2024-02-28T01:53:37Z",
        "summary": "The most common training pipeline for large language models includes\npretraining, finetuning and aligning phases, with their respective resulting\nmodels, such as the pretrained model and the finetuned model. Finetuned and\naligned models show improved abilities of instruction following and safe\ngeneration, however their abilities to stay factual about the world are\nimpacted by the finetuning process. Furthermore, the common practice of using\nsampling during generation also increases chances of hallucination. In this\nwork, we introduce a collaborative decoding framework to harness the high\nfactuality within pretrained models through the concept of critical tokens. We\nfirst design a critical token classifier to decide which model to use for the\nnext token, and subsequently generates the next token using different decoding\nstrategies. Experiments with different models and datasets show that our\ndecoding framework is able to reduce model hallucination significantly,\nshowcasing the importance of the collaborative decoding framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.17982v1.pdf"
    },
    {
        "title": "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment",
        "authors": [
            "Keming Lu",
            "Bowen Yu",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2024-01-23T03:56:22Z",
        "summary": "Considerable efforts have been invested in augmenting the role-playing\nproficiency of open-source large language models (LLMs) by emulating\nproprietary counterparts. Nevertheless, we posit that LLMs inherently harbor\nrole-play capabilities, owing to the extensive knowledge of characters and\npotential dialogues ingrained in their vast training corpora. Thus, in this\nstudy, we introduce Ditto, a self-alignment method for role-play. Ditto\ncapitalizes on character knowledge, encouraging an instruction-following LLM to\nsimulate role-play dialogues as a variant of reading comprehension. This method\ncreates a role-play training set comprising 4,000 characters, surpassing the\nscale of currently available datasets by tenfold regarding the number of roles.\nSubsequently, we fine-tune the LLM using this self-generated dataset to augment\nits role-playing capabilities. Upon evaluating our meticulously constructed and\nreproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in\nvarious parameter scales, consistently maintains a consistent role identity and\nprovides accurate role-specific knowledge in multi-turn role-play\nconversations. Notably, it outperforms all open-source role-play baselines,\nshowcasing performance levels comparable to advanced proprietary chatbots.\nFurthermore, we present the first comprehensive cross-supervision alignment\nexperiment in the role-play domain, revealing that the intrinsic capabilities\nof LLMs confine the knowledge within role-play. Meanwhile, the role-play styles\ncan be easily acquired with the guidance of smaller models. We open-source\nrelated resources at https://github.com/OFA-Sys/Ditto.",
        "pdf_link": "https://arxiv.org/pdf/2401.12474v1.pdf"
    },
    {
        "title": "PAP-REC: Personalized Automatic Prompt for Recommendation Language Model",
        "authors": [
            "Zelong Li",
            "Jianchao Ji",
            "Yingqiang Ge",
            "Wenyue Hua",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-01T02:29:16Z",
        "summary": "Recently emerged prompt-based Recommendation Language Models (RLM) can solve\nmultiple recommendation tasks uniformly. The RLMs make full use of the\ninherited knowledge learned from the abundant pre-training data to solve the\ndownstream recommendation tasks by prompts, without introducing additional\nparameters or network training. However, handcrafted prompts require\nsignificant expertise and human effort since slightly rewriting prompts may\ncause massive performance changes. In this paper, we propose PAP-REC, a\nframework to generate the Personalized Automatic Prompt for RECommendation\nlanguage models to mitigate the inefficiency and ineffectiveness problems\nderived from manually designed prompts. Specifically, personalized automatic\nprompts allow different users to have different prompt tokens for the same\ntask, automatically generated using a gradient-based method. One challenge for\npersonalized automatic prompt generation for recommendation language models is\nthe extremely large search space, leading to a long convergence time. To\neffectively and efficiently address the problem, we develop surrogate metrics\nand leverage an alternative updating schedule for prompting recommendation\nlanguage models. Experimental results show that our PAP-REC framework manages\nto generate personalized prompts, and the automatically generated prompts\noutperform manually constructed prompts and also outperform various baseline\nrecommendation models. The source code of the work is available at\nhttps://github.com/rutgerswiselab/PAP-REC.",
        "pdf_link": "https://arxiv.org/pdf/2402.00284v1.pdf"
    },
    {
        "title": "Multi-hop Question Answering under Temporal Knowledge Editing",
        "authors": [
            "Keyuan Cheng",
            "Gang Lin",
            "Haoyang Fei",
            "Yuxuan zhai",
            "Lu Yu",
            "Muhammad Asif Ali",
            "Lijie Hu",
            "Di Wang"
        ],
        "published": "2024-03-30T23:22:51Z",
        "summary": "Multi-hop question answering (MQA) under knowledge editing (KE) has garnered\nsignificant attention in the era of large language models. However, existing\nmodels for MQA under KE exhibit poor performance when dealing with questions\ncontaining explicit temporal contexts. To address this limitation, we propose a\nnovel framework, namely TEMPoral knowLEdge augmented Multi-hop Question\nAnswering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a\ntime-aware graph (TAG) to store edit knowledge in a structured manner. Then,\nthrough our proposed inference path, structural retrieval, and joint reasoning\nstages, TEMPLE-MQA effectively discerns temporal contexts within the question\nquery. Experiments on benchmark datasets demonstrate that TEMPLE-MQA\nsignificantly outperforms baseline models. Additionally, we contribute a new\ndataset, namely TKEMQA, which serves as the inaugural benchmark tailored\nspecifically for MQA with temporal scopes.",
        "pdf_link": "https://arxiv.org/pdf/2404.00492v1.pdf"
    },
    {
        "title": "Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs",
        "authors": [
            "David R. Mortensen",
            "Valentina Izrailevitch",
            "Yunze Xiao",
            "Hinrich Sch\u00fctze",
            "Leonie Weissweiler"
        ],
        "published": "2024-03-26T16:45:27Z",
        "summary": "Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)\nis a hallmark of English morphology. In conversion, a word with one part of\nspeech is placed in a non-prototypical context, where it is coerced to behave\nas if it had a different part of speech. However, while this process affects a\nlarge part of the English lexicon, little work has been done to establish the\ndegree to which language models capture this type of generalization. This paper\nreports the first study on the behavior of large language models with reference\nto conversion. We design a task for testing lexical-syntactic flexibility --\nthe degree to which models can generalize over words in a construction with a\nnon-prototypical part of speech. This task is situated within a natural\nlanguage inference paradigm. We test the abilities of five language models --\ntwo proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral\n7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,\nfollowed by GPT-3.5, but that the open source language models are also able to\nperform it and that the 7B parameter Mistral displays as little difference\nbetween its baseline performance on the natural language inference task and the\nnon-prototypical syntactic category task, as the massive GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.17856v1.pdf"
    },
    {
        "title": "Optimizing Language Models for Human Preferences is a Causal Inference Problem",
        "authors": [
            "Victoria Lin",
            "Eli Ben-Michael",
            "Louis-Philippe Morency"
        ],
        "published": "2024-02-22T21:36:07Z",
        "summary": "As large language models (LLMs) see greater use in academic and commercial\nsettings, there is increasing interest in methods that allow language models to\ngenerate texts aligned with human preferences. In this paper, we present an\ninitial exploration of language model optimization for human preferences from\ndirect outcome datasets, where each sample consists of a text and an associated\nnumerical outcome measuring the reader's response. We first propose that\nlanguage model optimization should be viewed as a causal problem to ensure that\nthe model correctly learns the relationship between the text and the outcome.\nWe formalize this causal language optimization problem, and we develop a\nmethod--causal preference optimization (CPO)--that solves an unbiased surrogate\nobjective for the problem. We further extend CPO with doubly robust CPO\n(DR-CPO), which reduces the variance of the surrogate objective while retaining\nprovably strong guarantees on bias. Finally, we empirically demonstrate the\neffectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human\npreferences on direct outcome data, and we validate the robustness of DR-CPO\nunder difficult confounding conditions.",
        "pdf_link": "https://arxiv.org/pdf/2402.14979v1.pdf"
    },
    {
        "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators",
        "authors": [
            "Jingwei Ni",
            "Minjing Shi",
            "Dominik Stammbach",
            "Mrinmaya Sachan",
            "Elliott Ash",
            "Markus Leippold"
        ],
        "published": "2024-02-16T20:59:57Z",
        "summary": "With the rise of generative AI, automated fact-checking methods to combat\nmisinformation are becoming more and more important. However, factual claim\ndetection, the first step in a fact-checking pipeline, suffers from two key\nissues that limit its scalability and generalizability: (1) inconsistency in\ndefinitions of the task and what a claim is, and (2) the high cost of manual\nannotation. To address (1), we review the definitions in related work and\npropose a unifying definition of factual claims that focuses on verifiability.\nTo address (2), we introduce AFaCTA (Automatic Factual Claim deTection\nAnnotator), a novel framework that assists in the annotation of factual claims\nwith the help of large language models (LLMs). AFaCTA calibrates its annotation\nconfidence with consistency along three predefined reasoning paths. Extensive\nevaluation and experiments in the domain of political speech reveal that AFaCTA\ncan efficiently assist experts in annotating factual claims and training\nhigh-quality classifiers, and can work with or without expert supervision. Our\nanalyses also result in PoliClaim, a comprehensive claim detection dataset\nspanning diverse political topics.",
        "pdf_link": "https://arxiv.org/pdf/2402.11073v1.pdf"
    },
    {
        "title": "NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method",
        "authors": [
            "Jakub Hoscilowicz",
            "Adam Wiacek",
            "Jan Chojnacki",
            "Adam Cieslak",
            "Leszek Michon",
            "Vitalii Urbanevych",
            "Artur Janicki"
        ],
        "published": "2024-03-27T15:22:16Z",
        "summary": "Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).",
        "pdf_link": "https://arxiv.org/pdf/2403.18680v1.pdf"
    },
    {
        "title": "BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models",
        "authors": [
            "Haitao Li",
            "Qingyao Ai",
            "Jia Chen",
            "Qian Dong",
            "Zhijing Wu",
            "Yiqun Liu",
            "Chong Chen",
            "Qi Tian"
        ],
        "published": "2024-03-27T08:57:21Z",
        "summary": "Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable\nof addressing a diverse range of tasks. However, general LLMs, which are\ndeveloped on open-domain data, may lack the domain-specific knowledge essential\nfor tasks in vertical domains, such as legal, medical, etc. To address this\nissue, previous approaches either conduct continuous pre-training with\ndomain-specific data or employ retrieval augmentation to support general LLMs.\nUnfortunately, these strategies are either cost-intensive or unreliable in\npractical applications. To this end, we present a novel framework named BLADE,\nwhich enhances Black-box LArge language models with small Domain-spEcific\nmodels. BLADE consists of a black-box LLM and a small domain-specific LM. The\nsmall LM preserves domain-specific knowledge and offers specialized insights,\nwhile the general LLM contributes robust language comprehension and reasoning\ncapabilities. Specifically, our method involves three steps: 1) pre-training\nthe small LM with domain-specific data, 2) fine-tuning this model using\nknowledge instruction data, and 3) joint Bayesian optimization of the general\nLLM and the small LM. Extensive experiments conducted on public legal and\nmedical benchmarks reveal that BLADE significantly outperforms existing\napproaches. This shows the potential of BLADE as an effective and\ncost-efficient solution in adapting general LLMs for vertical domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.18365v1.pdf"
    },
    {
        "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
        "authors": [
            "Pratyush Maini",
            "Skyler Seto",
            "He Bai",
            "David Grangier",
            "Yizhe Zhang",
            "Navdeep Jaitly"
        ],
        "published": "2024-01-29T18:19:08Z",
        "summary": "Large language models are trained on massive scrapes of the web, which are\noften unstructured, noisy, and poorly phrased. Current scaling laws show that\nlearning from such data requires an abundance of both compute and data, which\ngrows with the size of the model being trained. This is infeasible both because\nof the large compute costs and duration associated with pre-training, and the\nimpending scarcity of high-quality data on the web. In this work, we propose\nWeb Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an\noff-the-shelf instruction-tuned model prompted to paraphrase documents on the\nweb in specific styles such as \"like Wikipedia\" or in \"question-answer format\"\nto jointly pre-train LLMs on real and synthetic rephrases. First, we show that\nusing WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training\nby $\\sim3x$. At the same pre-training compute budget, it improves perplexity by\nmore than 10% on average across different subsets of the Pile, and improves\nzero-shot question answer accuracy across 13 tasks by more than 2%. Second, we\ninvestigate the impact of the re-phrasing style on the performance of the\nmodel, offering insights into how the composition of the training data can\nimpact the performance of LLMs in OOD settings. Our gains are attributed to the\nfact that re-phrased synthetic data has higher utility than just real data\nbecause it (i) incorporates style diversity that closely reflects downstream\nevaluation style, and (ii) has higher 'quality' than web-scraped data.",
        "pdf_link": "https://arxiv.org/pdf/2401.16380v1.pdf"
    },
    {
        "title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick",
        "authors": [
            "Jiayi Fu",
            "Xuandong Zhao",
            "Ruihan Yang",
            "Yuansen Zhang",
            "Jiangjie Chen",
            "Yanghua Xiao"
        ],
        "published": "2024-02-20T12:05:47Z",
        "summary": "Large language models (LLMs) excellently generate human-like text, but also\nraise concerns about misuse in fake news and academic dishonesty.\nDecoding-based watermark, particularly the GumbelMax-trick-based watermark(GM\nwatermark), is a standout solution for safeguarding machine-generated texts due\nto its notable detectability. However, GM watermark encounters a major\nchallenge with generation diversity, always yielding identical outputs for the\nsame prompt, negatively impacting generation diversity and user experience. To\novercome this limitation, we propose a new type of GM watermark, the\nLogits-Addition watermark, and its three variants, specifically designed to\nenhance diversity. Among these, the GumbelSoft watermark (a softmax variant of\nthe Logits-Addition watermark) demonstrates superior performance in high\ndiversity settings, with its AUROC score outperforming those of the two\nalternative variants by 0.1 to 0.3 and surpassing other decoding-based\nwatermarking methods by a minimum of 0.1.",
        "pdf_link": "https://arxiv.org/pdf/2402.12948v2.pdf"
    },
    {
        "title": "Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study",
        "authors": [
            "Rudra Dhar",
            "Karthik Vaidhyanathan",
            "Vasudeva Varma"
        ],
        "published": "2024-03-04T03:56:14Z",
        "summary": "Architectural Knowledge Management (AKM) involves the organized handling of\ninformation related to architectural decisions and design within a project or\norganization. An essential artifact of AKM is the Architecture Decision Records\n(ADR), which documents key design decisions. ADRs are documents that capture\ndecision context, decision made and various aspects related to a design\ndecision, thereby promoting transparency, collaboration, and understanding.\nDespite their benefits, ADR adoption in software development has been slow due\nto challenges like time constraints and inconsistent uptake. Recent\nadvancements in Large Language Models (LLMs) may help bridge this adoption gap\nby facilitating ADR generation. However, the effectiveness of LLM for ADR\ngeneration or understanding is something that has not been explored. To this\nend, in this work, we perform an exploratory study that aims to investigate the\nfeasibility of using LLM for the generation of ADRs given the decision context.\nIn our exploratory study, we utilize GPT and T5-based models with 0-shot,\nfew-shot, and fine-tuning approaches to generate the Decision of an ADR given\nits Context. Our results indicate that in a 0-shot setting, state-of-the-art\nmodels such as GPT-4 generate relevant and accurate Design Decisions, although\nthey fall short of human-level performance. Additionally, we observe that more\ncost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot\nsetting, and smaller models such as Flan-T5 can yield comparable results after\nfine-tuning. To conclude, this exploratory study suggests that LLM can generate\nDesign Decisions, but further research is required to attain human-level\ngeneration and establish standardized widespread adoption.",
        "pdf_link": "https://arxiv.org/pdf/2403.01709v1.pdf"
    },
    {
        "title": "MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection",
        "authors": [
            "Federico Borra",
            "Claudio Savelli",
            "Giacomo Rosso",
            "Alkis Koudounas",
            "Flavio Giobergia"
        ],
        "published": "2024-03-01T20:31:10Z",
        "summary": "In Natural Language Generation (NLG), contemporary Large Language Models\n(LLMs) face several challenges, such as generating fluent yet inaccurate\noutputs and reliance on fluency-centric metrics. This often leads to neural\nnetworks exhibiting \"hallucinations\". The SHROOM challenge focuses on\nautomatically identifying these hallucinations in the generated text. To tackle\nthese issues, we introduce two key components, a data augmentation pipeline\nincorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a\nvoting ensemble from three models pre-trained on Natural Language Inference\n(NLI) tasks and fine-tuned on diverse datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.00964v1.pdf"
    },
    {
        "title": "Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation",
        "authors": [
            "Zhenrui Yue",
            "Huimin Zeng",
            "Yimeng Lu",
            "Lanyu Shang",
            "Yang Zhang",
            "Dong Wang"
        ],
        "published": "2024-03-22T05:05:45Z",
        "summary": "The proliferation of online misinformation has posed significant threats to\npublic interest. While numerous online users actively participate in the combat\nagainst misinformation, many of such responses can be characterized by the lack\nof politeness and supporting facts. As a solution, text generation approaches\nare proposed to automatically produce counter-misinformation responses.\nNevertheless, existing methods are often trained end-to-end without leveraging\nexternal knowledge, resulting in subpar text quality and excessively repetitive\nresponses. In this paper, we propose retrieval augmented response generation\nfor online misinformation (RARG), which collects supporting evidence from\nscientific sources and generates counter-misinformation responses based on the\nevidences. In particular, our RARG consists of two stages: (1) evidence\ncollection, where we design a retrieval pipeline to retrieve and rerank\nevidence documents using a database comprising over 1M academic articles; (2)\nresponse generation, in which we align large language models (LLMs) to generate\nevidence-based responses via reinforcement learning from human feedback (RLHF).\nWe propose a reward function to maximize the utilization of the retrieved\nevidence while maintaining the quality of the generated text, which yields\npolite and factual responses that clearly refutes misinformation. To\ndemonstrate the effectiveness of our method, we study the case of COVID-19 and\nperform extensive experiments with both in- and cross-domain datasets, where\nRARG consistently outperforms baselines by generating high-quality\ncounter-misinformation responses.",
        "pdf_link": "https://arxiv.org/pdf/2403.14952v1.pdf"
    },
    {
        "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
        "authors": [
            "Mengzhou Xia",
            "Sadhika Malladi",
            "Suchin Gururangan",
            "Sanjeev Arora",
            "Danqi Chen"
        ],
        "published": "2024-02-06T19:18:04Z",
        "summary": "Instruction tuning has unlocked powerful capabilities in large language\nmodels (LLMs), effectively using combined datasets to develop generalpurpose\nchatbots. However, real-world applications often require a specialized suite of\nskills (e.g., reasoning). The challenge lies in identifying the most relevant\ndata from these extensive datasets to effectively develop specific\ncapabilities, a setting we frame as targeted instruction tuning. We propose\nLESS, an optimizer-aware and practically efficient algorithm to effectively\nestimate data influences and perform Low-rank gradiEnt Similarity Search for\ninstruction data selection. Crucially, LESS adapts existing influence\nformulations to work with the Adam optimizer and variable-length instruction\ndata. LESS first constructs a highly reusable and transferable gradient\ndatastore with low-dimensional gradient features and then selects examples\nbased on their similarity to few-shot examples embodying a specific capability.\nExperiments show that training on a LESS-selected 5% of the data can often\noutperform training on the full dataset across diverse downstream tasks.\nFurthermore, the selected data is highly transferable: smaller models can be\nleveraged to select useful data for larger models and models from different\nfamilies. Our qualitative analysis shows that our method goes beyond surface\nform cues to identify data that exemplifies the necessary reasoning skills for\nthe intended downstream application.",
        "pdf_link": "https://arxiv.org/pdf/2402.04333v2.pdf"
    },
    {
        "title": "An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations",
        "authors": [
            "Eric H. C. Chow",
            "TJ Kao",
            "Xiaoli Li"
        ],
        "published": "2024-03-25T05:04:52Z",
        "summary": "This study delves into the potential use of Large Language Models (LLMs) for\ngenerating Library of Congress Subject Headings (LCSH). The authors employed\nChatGPT to generate subject headings for electronic theses and dissertations\n(ETDs) based on their titles and summaries. The results revealed that although\nsome generated subject headings were valid, there were issues regarding\nspecificity and exhaustiveness. The study showcases that LLMs can serve as a\nstrategic response to the backlog of items awaiting cataloging in academic\nlibraries, while also offering a cost-effective approach for promptly\ngenerating LCSH. Nonetheless, human catalogers remain essential for verifying\nand enhancing the validity, exhaustiveness, and specificity of LCSH generated\nby LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.16424v2.pdf"
    },
    {
        "title": "Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations",
        "authors": [
            "Milan Bhan",
            "Jean-Noel Vittaut",
            "Nicolas Chesneau",
            "Marie-Jeanne Lesot"
        ],
        "published": "2024-02-19T10:47:09Z",
        "summary": "Incorporating natural language rationales in the prompt and In-Context\nLearning (ICL) has led to a significant improvement of Large Language Models\n(LLMs) performance. However, rationales currently require human-annotation or\nthe use of auxiliary proxy models to target promising samples or generate\nhigh-quality rationales. In this work, we propose Self-AMPLIFY to generate\nautomatically rationales from post hoc explanation methods applied to Small\nLanguage Models (SLMs) to improve their own performance. Self-AMPLIFY is a\n3-step method that targets samples, generates rationales and builds a final\nprompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and\ntwo datasets requiring reasoning abilities: these experiments show that\nSelf-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the\nfirst method to apply post hoc explanation methods to SLM to generate\nrationales to improve their own performance in a fully automated manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.12038v1.pdf"
    },
    {
        "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
        "authors": [
            "Keegan Hines",
            "Gary Lopez",
            "Matthew Hall",
            "Federico Zarfati",
            "Yonatan Zunger",
            "Emre Kiciman"
        ],
        "published": "2024-03-20T15:26:23Z",
        "summary": "Large Language Models (LLMs), while powerful, are built and trained to\nprocess a single text input. In common applications, multiple inputs can be\nprocessed by concatenating them together into a single stream of text. However,\nthe LLM is unable to distinguish which sections of prompt belong to various\ninput sources. Indirect prompt injection attacks take advantage of this\nvulnerability by embedding adversarial instructions into untrusted data being\nprocessed alongside user commands. Often, the LLM will mistake the adversarial\ninstructions as user commands to be followed, creating a security vulnerability\nin the larger system. We introduce spotlighting, a family of prompt engineering\ntechniques that can be used to improve LLMs' ability to distinguish among\nmultiple sources of input. The key insight is to utilize transformations of an\ninput to provide a reliable and continuous signal of its provenance. We\nevaluate spotlighting as a defense against indirect prompt injection attacks,\nand find that it is a robust defense that has minimal detrimental impact to\nunderlying NLP tasks. Using GPT-family models, we find that spotlighting\nreduces the attack success rate from greater than {50}\\% to below {2}\\% in our\nexperiments with minimal impact on task efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2403.14720v1.pdf"
    },
    {
        "title": "Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid Approach",
        "authors": [
            "Jinxi Kuang",
            "Jinyang Liu",
            "Junjie Huang",
            "Renyi Zhong",
            "Jiazhen Gu",
            "Lan Yu",
            "Rui Tan",
            "Zengyin Yang",
            "Michael R. Lyu"
        ],
        "published": "2024-03-11T07:48:35Z",
        "summary": "Due to the scale and complexity of cloud systems, a system failure would\ntrigger an \"alert storm\", i.e., massive correlated alerts. Although these\nalerts can be traced back to a few root causes, the overwhelming number makes\nit infeasible for manual handling. Alert aggregation is thus critical to help\nengineers concentrate on the root cause and facilitate failure resolution.\nExisting methods typically utilize semantic similarity-based methods or\nstatistical methods to aggregate alerts. However, semantic similarity-based\nmethods overlook the causal rationale of alerts, while statistical methods can\nhardly handle infrequent alerts.\n  To tackle these limitations, we introduce leveraging external knowledge,\ni.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose\nCOLA, a novel hybrid approach based on correlation mining and LLM (Large\nLanguage Model) reasoning for online alert aggregation. The correlation mining\nmodule effectively captures the temporal and spatial relations between alerts,\nmeasuring their correlations in an efficient manner. Subsequently, only\nuncertain pairs with low confidence are forwarded to the LLM reasoning module\nfor detailed analysis. This hybrid design harnesses both statistical evidence\nfor frequent alerts and the reasoning capabilities of computationally intensive\nLLMs, ensuring the overall efficiency of COLA in handling large volumes of\nalerts in practical scenarios. We evaluate COLA on three datasets collected\nfrom the production environment of a large-scale cloud platform. The\nexperimental results show COLA achieves F1-scores from 0.901 to 0.930,\noutperforming state-of-the-art methods and achieving comparable efficiency. We\nalso share our experience in deploying COLA in our real-world cloud system,\nCloud X.",
        "pdf_link": "https://arxiv.org/pdf/2403.06485v1.pdf"
    },
    {
        "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
        "authors": [
            "Chengxing Xie",
            "Canyu Chen",
            "Feiran Jia",
            "Ziyu Ye",
            "Kai Shu",
            "Adel Bibi",
            "Ziniu Hu",
            "Philip Torr",
            "Bernard Ghanem",
            "Guohao Li"
        ],
        "published": "2024-02-07T03:37:19Z",
        "summary": "Large Language Model (LLM) agents have been increasingly adopted as\nsimulation tools to model humans in applications such as social science.\nHowever, one fundamental question remains: can LLM agents really simulate human\nbehaviors? In this paper, we focus on one of the most critical behaviors in\nhuman interactions, trust, and aim to investigate whether or not LLM agents can\nsimulate human trust behaviors. We first find that LLM agents generally exhibit\ntrust behaviors, referred to as agent trust, under the framework of Trust\nGames, which are widely recognized in behavioral economics. Then, we discover\nthat LLM agents can have high behavioral alignment with humans regarding trust\nbehaviors, particularly for GPT-4, indicating the feasibility to simulate human\ntrust behaviors with LLM agents. In addition, we probe into the biases in agent\ntrust and the differences in agent trust towards agents and humans. We also\nexplore the intrinsic properties of agent trust under conditions including\nadvanced reasoning strategies and external manipulations. We further offer\nimportant implications of our discoveries for various scenarios where trust is\nparamount. Our study provides new insights into the behaviors of LLM agents and\nthe fundamental analogy between LLMs and humans.",
        "pdf_link": "https://arxiv.org/pdf/2402.04559v2.pdf"
    },
    {
        "title": "RepoHyper: Better Context Retrieval Is All You Need for Repository-Level Code Completion",
        "authors": [
            "Huy N. Phan",
            "Hoang N. Phan",
            "Tien N. Nguyen",
            "Nghi D. Q. Bui"
        ],
        "published": "2024-03-10T05:10:34Z",
        "summary": "Code Large Language Models (CodeLLMs) have demonstrated impressive\nproficiency in code completion tasks. However, they often fall short of fully\nunderstanding the extensive context of a project repository, such as the\nintricacies of relevant files and class hierarchies, which can result in less\nprecise completions. To overcome these limitations, we present \\tool, a\nmultifaceted framework designed to address the complex challenges associated\nwith repository-level code completion. Central to \\tool is the {\\em Repo-level\nSemantic Graph} (RSG), a novel semantic graph structure that encapsulates the\nvast context of code repositories. Furthermore, RepoHyper leverages\n\\textit{Expand and Refine} retrieval method, including a graph expansion and a\nlink prediction algorithm applied to the RSG, enabling the effective retrieval\nand prioritization of relevant code snippets. Our evaluations show that \\tool\nmarkedly outperforms existing techniques in repository-level code completion,\nshowcasing enhanced accuracy across various datasets when compared to several\nstrong baselines. Our implementation of RepoHyper can be found\nat~\\url{https://github.com/FSoft-AI4Code/RepoHyper}.",
        "pdf_link": "https://arxiv.org/pdf/2403.06095v2.pdf"
    },
    {
        "title": "Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning",
        "authors": [
            "Shuai Zhao",
            "Meihuizi Jia",
            "Luu Anh Tuan",
            "Fengjun Pan",
            "Jinming Wen"
        ],
        "published": "2024-01-11T14:38:19Z",
        "summary": "In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Despite being widely applied, in-context learning is\nvulnerable to malicious attacks. In this work, we raise security concerns\nregarding this paradigm. Our studies demonstrate that an attacker can\nmanipulate the behavior of large language models by poisoning the demonstration\ncontext, without the need for fine-tuning the model. Specifically, we design a\nnew backdoor attack method, named ICLAttack, to target large language models\nbased on in-context learning. Our method encompasses two types of attacks:\npoisoning demonstration examples and poisoning demonstration prompts, which can\nmake models behave in alignment with predefined intentions. ICLAttack does not\nrequire additional fine-tuning to implant a backdoor, thus preserving the\nmodel's generality. Furthermore, the poisoned examples are correctly labeled,\nenhancing the natural stealth of our attack method. Extensive experimental\nresults across several language models, ranging in size from 1.3B to 180B\nparameters, demonstrate the effectiveness of our attack method, exemplified by\na high average attack success rate of 95.0% across the three datasets on OPT\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2401.05949v4.pdf"
    },
    {
        "title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
        "authors": [
            "Wenqi Jiang",
            "Shuai Zhang",
            "Boran Han",
            "Jie Wang",
            "Bernie Wang",
            "Tim Kraska"
        ],
        "published": "2024-03-08T21:09:20Z",
        "summary": "Retrieval-augmented generation (RAG) can enhance the generation quality of\nlarge language models (LLMs) by incorporating external token databases.\nHowever, retrievals from large databases can constitute a substantial portion\nof the overall generation time, particularly when retrievals are periodically\nperformed to align the retrieved content with the latest states of generation.\nIn this paper, we introduce PipeRAG, a novel algorithm-system co-design\napproach to reduce generation latency and enhance generation quality. PipeRAG\nintegrates (1) pipeline parallelism to enable concurrent retrieval and\ngeneration processes, (2) flexible retrieval intervals to maximize the\nefficiency of pipeline parallelism, and (3) a performance model to\nautomatically balance retrieval quality and latency based on the generation\nstates and underlying hardware. Our evaluation shows that, by combining the\nthree aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in\nend-to-end generation latency while improving generation quality. These\npromising results showcase the effectiveness of co-designing algorithms with\nunderlying systems, paving the way for the adoption of PipeRAG in future RAG\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.05676v1.pdf"
    },
    {
        "title": "CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of Code and Text",
        "authors": [
            "Zhenru Lin",
            "Yiqun Yao",
            "Yang Yuan"
        ],
        "published": "2024-03-04T07:26:07Z",
        "summary": "Large language models (LLMs) such as ChatGPT are increasingly proficient in\nunderstanding and generating a mixture of code and text. Evaluation based on\nsuch $\\textit{mixture}$ can lead to a more comprehensive understanding of the\nmodels' abilities in solving coding problems. However, in this context, current\nevaluation methods are either limited in task coverage or lack standardization.\nTo address this issue, we propose using category theory as a framework for\nevaluation. Specifically, morphisms within a code category can represent code\ndebugging and transformation, functors between two categories represent code\ntranslation, and functors between a code category and a natural language\ncategory represent code generation, explanation, and reproduction. We present\nan automatic evaluation framework called $\\textbf{CatCode}$\n($\\textbf{Cat}$egory $\\textbf{Code}$) that can comprehensively assess the\ncoding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX.",
        "pdf_link": "https://arxiv.org/pdf/2403.01784v1.pdf"
    },
    {
        "title": "Correcting misinformation on social media with a large language model",
        "authors": [
            "Xinyi Zhou",
            "Ashish Sharma",
            "Amy X. Zhang",
            "Tim Althoff"
        ],
        "published": "2024-03-17T10:59:09Z",
        "summary": "Real-world misinformation can be partially correct and even factual but\nmisleading. It undermines public trust in science and democracy, particularly\non social media, where it can spread rapidly. High-quality and timely\ncorrection of misinformation that identifies and explains its (in)accuracies\nhas been shown to effectively reduce false beliefs. Despite the wide acceptance\nof manual correction, it is difficult to promptly correct newly created\nmisinformation and to scale this approach, a concern as technologies like large\nlanguage models (LLMs) make misinformation easier to produce. LLMs also have\nversatile capabilities that could accelerate misinformation\ncorrection--however, they struggle due to a lack of recent information, a\ntendency to produce false content, and limitations in addressing multimodal\ninformation. We propose MUSE, an LLM augmented with access to and credibility\nevaluation of up-to-date information. By retrieving evidence as refutations or\ncontexts, MUSE identifies and explains (in)accuracies in a piece of\ncontent--not presupposed to be misinformation--with references. It also\ndescribes images and conducts multimodal searches to verify and correct\nmultimodal content. Fact-checking experts evaluate responses to social media\ncontent that are not presupposed to be (non-)misinformation but broadly include\nincorrect, partially correct, and correct posts, that may or may not be\nmisleading. We propose and evaluate 13 dimensions of misinformation correction\nquality, ranging from the accuracy of identifications and factuality of\nexplanations to the relevance and credibility of references. The results\ndemonstrate MUSE's ability to promptly write high-quality responses to\npotential misinformation on social media--overall, MUSE outperforms GPT-4 by\n37% and even high-quality responses from laypeople by 29%.",
        "pdf_link": "https://arxiv.org/pdf/2403.11169v2.pdf"
    },
    {
        "title": "Citation-Enhanced Generation for LLM-based Chatbots",
        "authors": [
            "Weitao Li",
            "Junkai Li",
            "Weizhi Ma",
            "Yang Liu"
        ],
        "published": "2024-02-25T11:24:41Z",
        "summary": "Large language models (LLMs) exhibit powerful general intelligence across\ndiverse scenarios, including their integration into chatbots. However, a vital\nchallenge of LLM-based chatbots is that they may produce hallucinated content\nin responses, which significantly limits their applicability. Various efforts\nhave been made to alleviate hallucination, such as retrieval augmented\ngeneration and reinforcement learning with human feedback, but most of them\nrequire additional training and data annotation. In this paper, we propose a\nnovel post-hoc Citation-Enhanced Generation (CEG) approach combined with\nretrieval argumentation. Unlike previous studies that focus on preventing\nhallucinations during generation, our method addresses this issue in a post-hoc\nway. It incorporates a retrieval module to search for supporting documents\nrelevant to the generated content, and employs a natural language\ninference-based citation generation module. Once the statements in the\ngenerated content lack of reference, our model can regenerate responses until\nall statements are supported by citations. Note that our method is a\ntraining-free plug-and-play plugin that is capable of various LLMs. Experiments\non various hallucination-related datasets show our framework outperforms\nstate-of-the-art methods in both hallucination detection and response\nregeneration on three benchmarks. Our codes and dataset will be publicly\navailable.",
        "pdf_link": "https://arxiv.org/pdf/2402.16063v3.pdf"
    },
    {
        "title": "DP-TabICL: In-Context Learning with Differentially Private Tabular Data",
        "authors": [
            "Alycia N. Carey",
            "Karuna Bhaila",
            "Kennedy Edemacu",
            "Xintao Wu"
        ],
        "published": "2024-03-08T21:19:01Z",
        "summary": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks by conditioning on demonstrations of question-answer pairs and it has\nbeen shown to have comparable performance to costly model retraining and\nfine-tuning. Recently, ICL has been extended to allow tabular data to be used\nas demonstration examples by serializing individual records into natural\nlanguage formats. However, it has been shown that LLMs can leak information\ncontained in prompts, and since tabular data often contain sensitive\ninformation, understanding how to protect the underlying tabular data used in\nICL is a critical area of research. This work serves as an initial\ninvestigation into how to use differential privacy (DP) -- the long-established\ngold standard for data privacy and anonymization -- to protect tabular data\nused in ICL. Specifically, we investigate the application of DP mechanisms for\nprivate tabular ICL via data privatization prior to serialization and\nprompting. We formulate two private ICL frameworks with provable privacy\nguarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios\nvia injecting noise into individual records or group statistics, respectively.\nWe evaluate our DP-based frameworks on eight real-world tabular datasets and\nacross multiple ICL and DP settings. Our evaluations show that DP-based ICL can\nprotect the privacy of the underlying tabular data while achieving comparable\nperformance to non-LLM baselines, especially under high privacy regimes.",
        "pdf_link": "https://arxiv.org/pdf/2403.05681v1.pdf"
    },
    {
        "title": "Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code",
        "authors": [
            "Andreas Florath"
        ],
        "published": "2024-03-19T10:53:40Z",
        "summary": "In the realm of formal theorem proving, the Coq proof assistant stands out\nfor its rigorous approach to verifying mathematical assertions and software\ncorrectness. Despite the advances in artificial intelligence and machine\nlearning, the specialized nature of Coq syntax and semantics poses unique\nchallenges for Large Language Models (LLMs). Addressing this gap, we present a\ncomprehensive dataset specifically designed to enhance LLMs' proficiency in\ninterpreting and generating Coq code. This dataset, derived from a collection\nof over 10,000 Coq source files, encompasses a wide array of propositions,\nproofs, and definitions, enriched with metadata including source references and\nlicensing information. Our primary aim is to facilitate the development of LLMs\ncapable of generating syntactically correct and semantically meaningful Coq\nconstructs, thereby advancing the frontier of automated theorem proving.\nInitial experiments with this dataset have showcased its significant potential;\nmodels trained on this data exhibited enhanced accuracy in Coq code generation.\nNotably, a particular experiment revealed that a fine-tuned LLM was capable of\ngenerating 141 valid proofs for a basic lemma, highlighting the dataset's\nutility in facilitating the discovery of diverse and valid proof strategies.\nThis paper discusses the dataset's composition, the methodology behind its\ncreation, and the implications of our findings for the future of machine\nlearning in formal verification. The dataset is accessible for further research\nand exploration:\nhttps://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1",
        "pdf_link": "https://arxiv.org/pdf/2403.12627v2.pdf"
    },
    {
        "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
        "authors": [
            "Shengrui Li",
            "Xueting Han",
            "Jing Bai"
        ],
        "published": "2024-02-15T08:03:12Z",
        "summary": "The considerable size of Large Language Models (LLMs) presents notable\ndeployment challenges, particularly on resource-constrained hardware.\nStructured pruning, offers an effective means to compress LLMs, thereby\nreducing storage costs and enhancing inference speed for more efficient\nutilization. In this work, we study data-efficient and resource-efficient\nstructure pruning methods to obtain smaller yet still powerful models.\nKnowledge Distillation is well-suited for pruning, as the intact model can\nserve as an excellent teacher for pruned students. However, it becomes\nchallenging in the context of LLMs due to memory constraints. To address this,\nwe propose an efficient progressive Numerous-teacher pruning method\n(NutePrune). NutePrune mitigates excessive memory costs by loading only one\nintact model and integrating it with various masks and LoRA modules, enabling\nit to seamlessly switch between teacher and student roles. This approach allows\nus to leverage numerous teachers with varying capacities to progressively guide\nthe pruned model, enhancing overall performance. Extensive experiments across\nvarious tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot\nexperiments, NutePrune retains 97.17% of the performance of the original model\nat 20% sparsity and 95.07% at 25% sparsity.",
        "pdf_link": "https://arxiv.org/pdf/2402.09773v1.pdf"
    },
    {
        "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
        "authors": [
            "Rui Pan",
            "Xiang Liu",
            "Shizhe Diao",
            "Renjie Pi",
            "Jipeng Zhang",
            "Chi Han",
            "Tong Zhang"
        ],
        "published": "2024-03-26T17:55:02Z",
        "summary": "The machine learning community has witnessed impressive advancements since\nthe first appearance of large language models (LLMs), yet their huge memory\nconsumption has become a major roadblock to large-scale training. Parameter\nEfficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been\nproposed to alleviate this problem, but their performance still fails to match\nfull parameter training in most large-scale fine-tuning settings. Attempting to\ncomplement this deficiency, we investigate layerwise properties of LoRA on\nfine-tuning tasks and observe an uncommon skewness of weight norms across\ndifferent layers. Utilizing this key observation, a surprisingly simple\ntraining strategy is discovered, which outperforms both LoRA and full parameter\ntraining in a wide range of settings with memory costs as low as LoRA. We name\nit Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,\nwhich applies the idea of importance sampling to different layers in LLMs and\nrandomly freeze most middle layers during optimization. Experimental results\nshow that with similar or less GPU memory consumption, LISA surpasses LoRA or\neven full parameter tuning in downstream fine-tuning tasks, where LISA\nconsistently outperforms LoRA by over $11\\%$-$37\\%$ in terms of MT-Bench\nscores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or\nbetter performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating\nits effectiveness across different domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.17919v2.pdf"
    },
    {
        "title": "AugSumm: towards generalizable speech summarization using synthetic labels from large language model",
        "authors": [
            "Jee-weon Jung",
            "Roshan Sharma",
            "William Chen",
            "Bhiksha Raj",
            "Shinji Watanabe"
        ],
        "published": "2024-01-10T18:39:46Z",
        "summary": "Abstractive speech summarization (SSUM) aims to generate human-like summaries\nfrom speech. Given variations in information captured and phrasing, recordings\ncan be summarized in multiple ways. Therefore, it is more reasonable to\nconsider a probabilistic distribution of all potential summaries rather than a\nsingle summary. However, conventional SSUM models are mostly trained and\nevaluated with a single ground-truth (GT) human-annotated deterministic summary\nfor every recording. Generating multiple human references would be ideal to\nbetter represent the distribution statistically, but is impractical because\nannotation is expensive. We tackle this challenge by proposing AugSumm, a\nmethod to leverage large language models (LLMs) as a proxy for human annotators\nto generate augmented summaries for training and evaluation. First, we explore\nprompting strategies to generate synthetic summaries from ChatGPT. We validate\nthe quality of synthetic summaries using multiple metrics including human\nevaluation, where we find that summaries generated using AugSumm are perceived\nas more valid to humans. Second, we develop methods to utilize synthetic\nsummaries in training and evaluation. Experiments on How2 demonstrate that\npre-training on synthetic summaries and fine-tuning on GT summaries improves\nROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries\nare available at https://github.com/Jungjee/AugSumm.",
        "pdf_link": "https://arxiv.org/pdf/2401.06806v1.pdf"
    },
    {
        "title": "All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification",
        "authors": [
            "Deepak Narayan Gadde",
            "Aman Kumar",
            "Thomas Nalapat",
            "Evgenii Rezunov",
            "Fabio Cappellini"
        ],
        "published": "2024-03-25T13:23:24Z",
        "summary": "Modern hardware designs have grown increasingly efficient and complex.\nHowever, they are often susceptible to Common Weakness Enumerations (CWEs).\nThis paper is focused on the formal verification of CWEs in a dataset of\nhardware designs written in SystemVerilog from Regenerative Artificial\nIntelligence (AI) powered by Large Language Models (LLMs). We applied formal\nverification to categorize each hardware design as vulnerable or CWE-free. This\ndataset was generated by 4 different LLMs and features a unique set of designs\nfor each of the 10 CWEs we target in our paper. We have associated the\nidentified vulnerabilities with CWE numbers for a dataset of 60,000 generated\nSystemVerilog Register Transfer Level (RTL) code. It was also found that most\nLLMs are not aware of any hardware CWEs; hence they are usually not considered\nwhen generating the hardware code. Our study reveals that approximately 60% of\nthe hardware designs generated by LLMs are prone to CWEs, posing potential\nsafety and security risks. The dataset could be ideal for training LLMs and\nMachine Learning (ML) algorithms to abstain from generating CWE-prone hardware\ndesigns.",
        "pdf_link": "https://arxiv.org/pdf/2403.16750v1.pdf"
    },
    {
        "title": "Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens",
        "authors": [
            "Ziqian Zeng",
            "Jiahong Yu",
            "Qianshi Pang",
            "Zihao Wang",
            "Huiping Zhuang",
            "Cen Chen"
        ],
        "published": "2024-02-24T08:10:39Z",
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, their widespread application is hindered by the\nresource-intensive decoding process. To address this challenge, current\napproaches have incorporated additional decoding heads to enable parallel\nprediction of multiple subsequent tokens, thereby achieving inference\nacceleration. Nevertheless, the accuracy of these decoding heads falls short of\nthe auto-regressive decoding approach.\n  In light of these limitations, we propose Chimera, a novel framework\nspecifically designed for speculative sampling. Within this framework, we\nintroduce a lightweight draft model that effectively utilizes previously\ngenerated tokens to predict subsequent words. To ensure both accuracy and\nefficiency, we present two strategies within the lightweight draft model.\nFirstly, we focus on capturing short-range dependencies at the bottom layer.\nSecondly, we leverage the readily available representations from the original\nLLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera\ndemonstrates impressive results, achieving an average latency speedup ratio of\n2.7x compared to the vanilla auto-regressive decoding approach. This highlights\nthe potential of our proposed framework in significantly improving the\nefficiency of large language models during the decoding process.",
        "pdf_link": "https://arxiv.org/pdf/2402.15758v1.pdf"
    },
    {
        "title": "Prediction-Powered Ranking of Large Language Models",
        "authors": [
            "Ivi Chatzi",
            "Eleni Straitouri",
            "Suhas Thejaswi",
            "Manuel Gomez Rodriguez"
        ],
        "published": "2024-02-27T19:00:01Z",
        "summary": "Large language models are often ranked according to their level of alignment\nwith human preferences -- a model is better than other models if its outputs\nare more frequently preferred by humans. One of the most popular ways to elicit\nhuman preferences utilizes pairwise comparisons between the outputs provided by\ndifferent models to the same inputs. However, since gathering pairwise\ncomparisons by humans is costly and time-consuming, it has become a very common\npractice to gather pairwise comparisons by a strong large language model -- a\nmodel strongly aligned with human preferences. Surprisingly, practitioners\ncannot currently measure the uncertainty that any mismatch between human and\nmodel preferences may introduce in the constructed rankings. In this work, we\ndevelop a statistical framework to bridge this gap. Given a small set of\npairwise comparisons by humans and a large set of pairwise comparisons by a\nmodel, our framework provides a rank-set -- a set of possible ranking positions\n-- for each of the models under comparison. Moreover, it guarantees that, with\na probability greater than or equal to a user-specified value, the rank-sets\ncover the true ranking consistent with (the distribution of) human pairwise\npreferences. Our framework is computationally efficient, easy to use, and does\nnot make any assumption about the distribution of human preferences nor about\nthe degree of alignment between the pairwise comparisons by the humans and the\nstrong large language model.",
        "pdf_link": "https://arxiv.org/pdf/2402.17826v1.pdf"
    },
    {
        "title": "Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study",
        "authors": [
            "Tim van Dam",
            "Frank van der Heijden",
            "Philippe de Bekker",
            "Berend Nieuwschepen",
            "Marc Otten",
            "Maliheh Izadi"
        ],
        "published": "2024-03-22T13:13:13Z",
        "summary": "Language model-based code completion models have quickly grown in use,\nhelping thousands of developers write code in many different programming\nlanguages. However, research on code completion models typically focuses on\nimperative languages such as Python and JavaScript, which results in a lack of\nrepresentation for functional programming languages. Consequently, these models\noften perform poorly on functional languages such as Haskell. To investigate\nwhether this can be alleviated, we evaluate the performance of two language\nmodels for code, CodeGPT and UniXcoder, on the functional programming language\nHaskell. We fine-tune and evaluate the models on Haskell functions sourced from\na publicly accessible Haskell dataset on HuggingFace. Additionally, we manually\nevaluate the models using our novel translated HumanEval dataset. Our automatic\nevaluation shows that knowledge of imperative programming languages in the\npre-training of LLMs may not transfer well to functional languages, but that\ncode completion on functional languages is feasible. Consequently, this shows\nthe need for more high-quality Haskell datasets. A manual evaluation on\nHumanEval-Haskell indicates CodeGPT frequently generates empty predictions and\nextra comments, while UniXcoder more often produces incomplete or incorrect\npredictions. Finally, we release HumanEval-Haskell, along with the fine-tuned\nmodels and all code required to reproduce our experiments on GitHub\n(https://github.com/AISE-TUDelft/HaskellCCEval).",
        "pdf_link": "https://arxiv.org/pdf/2403.15185v1.pdf"
    },
    {
        "title": "LLMs Instruct LLMs:An Extraction and Editing Method",
        "authors": [
            "Xin Zhang",
            "Tianjie Ju",
            "Huijia Liang",
            "Ying Fu",
            "Qin Zhang"
        ],
        "published": "2024-03-23T06:03:36Z",
        "summary": "The interest in updating Large Language Models (LLMs) without retraining from\nscratch is substantial, yet it comes with some challenges.This is especially\ntrue for situations demanding complex reasoning with limited samples, a\nscenario we refer to as the Paucity-Constrained Complex Reasoning Adaptation\nfor LLMs (PCRA-LLM).Traditional methods like Low-Rank Adaptation (LoRA) and\nRetrieval-Augmented Generation (RAG) are inadequate for this critical issue,\nparticularly evident in our exploration of a specific medical context that\nepitomize the PCRA-LLM's distinct needs.To address the issue, we propose a\nSequential Fusion method to incorporate knowledge from complex context into\nLLMs. This method employs a two-stage framework: initially, it leverages\ngeneral LLMs to construct knowledge graphs (KGs) for extracting knowledge from\ncomplex texts; subsequently, it updates the domain LLMs through knowledge edit.\nAccording to our method, the domain LLM achieved a 71.69\\% accuracy in question\nanswering tasks. Subsequently, we broadened our assessment to a novel dataset\nwe developed in the economics and management field, where our method realized a\n75\\% accuracy. These outcomes underline the efficacy and adaptability of our\napproach for PCRA-LLM across various domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.15736v1.pdf"
    },
    {
        "title": "Arabic Synonym BERT-based Adversarial Examples for Text Classification",
        "authors": [
            "Norah Alshahrani",
            "Saied Alshahrani",
            "Esma Wali",
            "Jeanna Matthews"
        ],
        "published": "2024-02-05T19:39:07Z",
        "summary": "Text classification systems have been proven vulnerable to adversarial text\nexamples, modified versions of the original text examples that are often\nunnoticed by human eyes, yet can force text classification models to alter\ntheir classification. Often, research works quantifying the impact of\nadversarial text attacks have been applied only to models trained in English.\nIn this paper, we introduce the first word-level study of adversarial attacks\nin Arabic. Specifically, we use a synonym (word-level) attack using a Masked\nLanguage Modeling (MLM) task with a BERT model in a black-box setting to assess\nthe robustness of the state-of-the-art text classification models to\nadversarial attacks in Arabic. To evaluate the grammatical and semantic\nsimilarities of the newly produced adversarial examples using our synonym\nBERT-based attack, we invite four human evaluators to assess and compare the\nproduced adversarial examples with their original examples. We also study the\ntransferability of these newly produced Arabic adversarial examples to various\nmodels and investigate the effectiveness of defense mechanisms against these\nadversarial examples on the BERT models. We find that fine-tuned BERT models\nwere more susceptible to our synonym attacks than the other Deep Neural\nNetworks (DNN) models like WordCNN and WordLSTM we trained. We also find that\nfine-tuned BERT models were more susceptible to transferred attacks. We,\nlastly, find that fine-tuned BERT models successfully regain at least 2% in\naccuracy after applying adversarial training as an initial defense mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2402.03477v1.pdf"
    },
    {
        "title": "EventGround: Narrative Reasoning by Grounding to Eventuality-centric Knowledge Graphs",
        "authors": [
            "Cheng Jiayang",
            "Lin Qiu",
            "Chunkit Chan",
            "Xin Liu",
            "Yangqiu Song",
            "Zheng Zhang"
        ],
        "published": "2024-03-30T01:16:37Z",
        "summary": "Narrative reasoning relies on the understanding of eventualities in story\ncontexts, which requires a wealth of background world knowledge. To help\nmachines leverage such knowledge, existing solutions can be categorized into\ntwo groups. Some focus on implicitly modeling eventuality knowledge by\npretraining language models (LMs) with eventuality-aware objectives. However,\nthis approach breaks down knowledge structures and lacks interpretability.\nOthers explicitly collect world knowledge of eventualities into structured\neventuality-centric knowledge graphs (KGs). However, existing research on\nleveraging these knowledge sources for free-texts is limited. In this work, we\npropose an initial comprehensive framework called EventGround, which aims to\ntackle the problem of grounding free-texts to eventuality-centric KGs for\ncontextualized narrative reasoning. We identify two critical problems in this\ndirection: the event representation and sparsity problems. We provide simple\nyet effective parsing and partial information extraction methods to tackle\nthese problems. Experimental results demonstrate that our approach consistently\noutperforms baseline models when combined with graph neural network (GNN) or\nlarge language model (LLM) based graph reasoning models. Our framework,\nincorporating grounded knowledge, achieves state-of-the-art performance while\nproviding interpretable evidence.",
        "pdf_link": "https://arxiv.org/pdf/2404.00209v1.pdf"
    },
    {
        "title": "Unveiling the Generalization Power of Fine-Tuned Large Language Models",
        "authors": [
            "Haoran Yang",
            "Yumeng Zhang",
            "Jiaqi Xu",
            "Hongyuan Lu",
            "Pheng Ann Heng",
            "Wai Lam"
        ],
        "published": "2024-03-14T08:18:59Z",
        "summary": "While Large Language Models (LLMs) have demonstrated exceptional multitasking\nabilities, fine-tuning these models on downstream, domain-specific datasets is\noften necessary to yield superior performance on test sets compared to their\ncounterparts without fine-tuning. However, the comprehensive effects of\nfine-tuning on the LLMs' generalization ability are not fully understood. This\npaper delves into the differences between original, unmodified LLMs and their\nfine-tuned variants. Our primary investigation centers on whether fine-tuning\naffects the generalization ability intrinsic to LLMs. To elaborate on this, we\nconduct extensive experiments across five distinct language tasks on various\ndatasets. Our main findings reveal that models fine-tuned on generation and\nclassification tasks exhibit dissimilar behaviors in generalizing to different\ndomains and tasks. Intriguingly, we observe that integrating the in-context\nlearning strategy during fine-tuning on generation tasks can enhance the\nmodel's generalization ability. Through this systematic investigation, we aim\nto contribute valuable insights into the evolving landscape of fine-tuning\npractices for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.09162v1.pdf"
    },
    {
        "title": "SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning",
        "authors": [
            "Weizheng Wang",
            "Le Mao",
            "Ruiqi Wang",
            "Byung-Cheol Min"
        ],
        "published": "2024-03-22T23:12:28Z",
        "summary": "An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm",
        "pdf_link": "https://arxiv.org/pdf/2403.15648v1.pdf"
    },
    {
        "title": "Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy",
        "authors": [
            "SeongKu Kang",
            "Shivam Agarwal",
            "Bowen Jin",
            "Dongha Lee",
            "Hwanjo Yu",
            "Jiawei Han"
        ],
        "published": "2024-03-07T02:34:54Z",
        "summary": "Document retrieval has greatly benefited from the advancements of large-scale\npre-trained language models (PLMs). However, their effectiveness is often\nlimited in theme-specific applications for specialized areas or industries, due\nto unique terminologies, incomplete contexts of user queries, and specialized\nsearch intents. To capture the theme-specific information and improve\nretrieval, we propose to use a corpus topical taxonomy, which outlines the\nlatent topic structure of the corpus while reflecting user-interested aspects.\nWe introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which\nidentifies the central topics of queries and documents with the guidance of the\ntaxonomy, and exploits their topical relatedness to supplement missing\ncontexts. As a plug-and-play framework, ToTER can be flexibly employed to\nenhance various PLM-based retrievers. Through extensive quantitative, ablative,\nand exploratory experiments on two real-world datasets, we ascertain the\nbenefits of using topical taxonomy for retrieval in theme-specific applications\nand demonstrate the effectiveness of ToTER.",
        "pdf_link": "https://arxiv.org/pdf/2403.04160v1.pdf"
    },
    {
        "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
        "authors": [
            "Xiao Liu",
            "Xixuan Song",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2024-03-31T08:30:15Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has been a central\ntechnique for recent large language model (LLM) alignment. However, its heavy\ndependence on costly human or LLM-as-Judge preference feedback could stymie its\nwider applications. In this work, we introduce Self-Contrast, a feedback-free\nlarge language model alignment method via exploiting extensive self-generated\nnegatives. With only supervised fine-tuning (SFT) targets, Self-Contrast\nleverages the LLM itself to generate massive diverse candidates, and harnesses\na pre-trained embedding model to filter multiple negatives according to text\nsimilarity. Theoretically, we illustrate that in this setting, merely scaling\nnegative responses can still effectively approximate situations with more\nbalanced positive and negative preference annotations. Our experiments with\ndirect preference optimization (DPO) on three datasets show that, Self-Contrast\ncould consistently outperform SFT and standard DPO training by large margins.\nAnd as the number of self-generated negatives increases, the performance of\nSelf-Contrast continues to grow. Code and data are available at\nhttps://github.com/THUDM/Self-Contrast.",
        "pdf_link": "https://arxiv.org/pdf/2404.00604v1.pdf"
    },
    {
        "title": "Adversarial Text Purification: A Large Language Model Approach for Defense",
        "authors": [
            "Raha Moraffah",
            "Shubh Khandelwal",
            "Amrita Bhattacharjee",
            "Huan Liu"
        ],
        "published": "2024-02-05T02:36:41Z",
        "summary": "Adversarial purification is a defense mechanism for safeguarding classifiers\nagainst adversarial attacks without knowing the type of attacks or training of\nthe classifier. These techniques characterize and eliminate adversarial\nperturbations from the attacked inputs, aiming to restore purified samples that\nretain similarity to the initially attacked ones and are correctly classified\nby the classifier. Due to the inherent challenges associated with\ncharacterizing noise perturbations for discrete inputs, adversarial text\npurification has been relatively unexplored. In this paper, we investigate the\neffectiveness of adversarial purification methods in defending text\nclassifiers. We propose a novel adversarial text purification that harnesses\nthe generative capabilities of Large Language Models (LLMs) to purify\nadversarial text without the need to explicitly characterize the discrete noise\nperturbations. We utilize prompt engineering to exploit LLMs for recovering the\npurified examples for given adversarial examples such that they are\nsemantically similar and correctly classified. Our proposed method demonstrates\nremarkable performance over various classifiers, improving their accuracy under\nthe attack by over 65% on average.",
        "pdf_link": "https://arxiv.org/pdf/2402.06655v1.pdf"
    },
    {
        "title": "PerOS: Personalized Self-Adapting Operating Systems in the Cloud",
        "authors": [
            "Hongyu H\u00e8"
        ],
        "published": "2024-03-26T20:10:31Z",
        "summary": "Operating systems (OSes) are foundational to computer systems, managing\nhardware resources and ensuring secure environments for diverse applications.\nHowever, despite their enduring importance, the fundamental design objectives\nof OSes have seen minimal evolution over decades. Traditionally prioritizing\naspects like speed, memory efficiency, security, and scalability, these\nobjectives often overlook the crucial aspect of intelligence as well as\npersonalized user experience. The lack of intelligence becomes increasingly\ncritical amid technological revolutions, such as the remarkable advancements in\nmachine learning (ML).\n  Today's personal devices, evolving into intimate companions for users, pose\nunique challenges for traditional OSes like Linux and iOS, especially with the\nemergence of specialized hardware featuring heterogeneous components.\nFurthermore, the rise of large language models (LLMs) in ML has introduced\ntransformative capabilities, reshaping user interactions and software\ndevelopment paradigms.\n  While existing literature predominantly focuses on leveraging ML methods for\nsystem optimization or accelerating ML workloads, there is a significant gap in\naddressing personalized user experiences at the OS level. To tackle this\nchallenge, this work proposes PerOS, a personalized OS ingrained with LLM\ncapabilities. PerOS aims to provide tailored user experiences while\nsafeguarding privacy and personal data through declarative interfaces,\nself-adaptive kernels, and secure data management in a scalable cloud-centric\narchitecture; therein lies the main research question of this work: How can we\ndevelop intelligent, secure, and scalable OSes that deliver personalized\nexperiences to thousands of users?",
        "pdf_link": "https://arxiv.org/pdf/2404.00057v1.pdf"
    },
    {
        "title": "Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models",
        "authors": [
            "Minghan Wang",
            "Thuy-Trang Vu",
            "Ehsan Shareghi",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-16T10:32:16Z",
        "summary": "Simultaneous machine translation (SimulMT) presents a challenging trade-off\nbetween translation quality and latency. Recent studies have shown that LLMs\ncan achieve good performance in SimulMT tasks. However, this often comes at the\nexpense of high inference cost and latency. In this paper, we propose a\nconversational SimulMT framework to enhance the inference efficiency of\nLLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments\nwith Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of\nLLM in translation quality while achieving comparable computational latency to\nspecialized SimulMT models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10552v1.pdf"
    },
    {
        "title": "Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models",
        "authors": [
            "Chenyu Lian",
            "Hong-Yu Zhou",
            "Yizhou Yu",
            "Liansheng Wang"
        ],
        "published": "2024-01-22T18:59:07Z",
        "summary": "Parameter-efficient fine-tuning (PEFT) that was initially developed for\nexploiting pre-trained large language models has recently emerged as an\neffective approach to perform transfer learning on computer vision tasks.\nHowever, the effectiveness of PEFT on medical vision foundation models is still\nunclear and remains to be explored. As a proof of concept, we conducted a\ndetailed empirical study on applying PEFT to chest radiography foundation\nmodels. Specifically, we delved into LoRA, a representative PEFT method, and\ncompared it against full-parameter fine-tuning (FFT) on two self-supervised\nradiography foundation models across three well-established chest radiograph\ndatasets. Our results showed that LoRA outperformed FFT in 13 out of 18\ntransfer learning tasks by at most 2.9% using fewer than 1% tunable parameters.\nCombining LoRA with foundation models, we set up new state-of-the-art on a\nrange of data-efficient learning tasks, such as an AUROC score of 80.6% using\n1% labeled data on NIH ChestX-ray14. We hope this study can evoke more\nattention from the community in the use of PEFT for transfer learning on\nmedical imaging tasks. Code and models are available at\nhttps://github.com/RL4M/MED-PEFT.",
        "pdf_link": "https://arxiv.org/pdf/2401.12215v1.pdf"
    },
    {
        "title": "WARM: On the Benefits of Weight Averaged Reward Models",
        "authors": [
            "Alexandre Ram\u00e9",
            "Nino Vieillard",
            "L\u00e9onard Hussenot",
            "Robert Dadashi",
            "Geoffrey Cideron",
            "Olivier Bachem",
            "Johan Ferret"
        ],
        "published": "2024-01-22T18:27:08Z",
        "summary": "Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.",
        "pdf_link": "https://arxiv.org/pdf/2401.12187v1.pdf"
    },
    {
        "title": "Supervised Fine-Tuning as Inverse Reinforcement Learning",
        "authors": [
            "Hao Sun"
        ],
        "published": "2024-03-18T17:52:57Z",
        "summary": "The prevailing approach to aligning Large Language Models (LLMs) typically\nrelies on human or AI feedback and assumes access to specific types of\npreference datasets. In our work, we question the efficacy of such datasets and\nexplore various scenarios where alignment with expert demonstrations proves\nmore realistic. We build a sequential decision-making framework to formulate\nthe problem of aligning LLMs using demonstration datasets. Drawing insights\nfrom inverse reinforcement learning and imitation learning, we introduce\nvarious approaches for divergence minimization in the LLM alignment tasks. Our\nanalysis highlights the mass-covering and mode-seeking behaviors of these\ndifferent approaches. Inclusively, we examine the pros and cons of the\nclassical supervised fine-tuning method, elaborating on scenarios where\ndifferent methods shine.",
        "pdf_link": "https://arxiv.org/pdf/2403.12017v1.pdf"
    },
    {
        "title": "COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning",
        "authors": [
            "Yuelin Bai",
            "Xinrun Du",
            "Yiming Liang",
            "Yonggang Jin",
            "Ziqiang Liu",
            "Junting Zhou",
            "Tianyu Zheng",
            "Xincheng Zhang",
            "Nuo Ma",
            "Zekun Wang",
            "Ruibin Yuan",
            "Haihong Wu",
            "Hongquan Lin",
            "Wenhao Huang",
            "Jiajun Zhang",
            "Wenhu Chen",
            "Chenghua Lin",
            "Jie Fu",
            "Min Yang",
            "Shiwen Ni",
            "Ge Zhang"
        ],
        "published": "2024-03-26T19:24:18Z",
        "summary": "Recently, there have been significant advancements in large language models\n(LLMs), particularly focused on the English language. These advancements have\nenabled these LLMs to understand and execute complex instructions with\nunprecedented accuracy and fluency. However, despite these advancements, there\nremains a noticeable gap in the development of Chinese instruction tuning. The\nunique linguistic features and cultural depth of the Chinese language pose\nchallenges for instruction tuning tasks. Existing datasets are either derived\nfrom English-centric LLMs or are ill-suited for aligning with the interaction\npatterns of real-world Chinese users. To bridge this gap, we introduce\nCOIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to\nbuild a diverse, wide-ranging instruction-tuning dataset to better align model\nbehavior with human interactions. To this end, we collect a high-quality\nhuman-written corpus from various sources on the Chinese Internet, including\nQ&A communities, Wikis, examinations, and existing NLP datasets. This corpus\nwas rigorously filtered and carefully processed to form the COIG-CQIA dataset.\nFurthermore, we train models of various scales on different subsets of CQIA,\nfollowing in-depth evaluation and analyses. The findings from our experiments\noffer valuable insights for selecting and developing Chinese instruction-tuning\ndatasets. We also find that models trained on CQIA-Subset achieve competitive\nresults in human assessment as well as knowledge and security benchmarks. Data\nare available at https://huggingface.co/datasets/m-a-p/COIG-CQIA",
        "pdf_link": "https://arxiv.org/pdf/2403.18058v1.pdf"
    },
    {
        "title": "From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for Opportunities, Risks, and Regulatory Compliance in Commercializing Large Language Models",
        "authors": [
            "Timothy R. McIntosh",
            "Teo Susnjak",
            "Tong Liu",
            "Paul Watters",
            "Raza Nowrozy",
            "Malka N. Halgamuge"
        ],
        "published": "2024-02-24T09:06:25Z",
        "summary": "This study investigated the integration readiness of four predominant\ncybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0,\nCOBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the\nopportunities, risks, and regulatory compliance when adopting Large Language\nModels (LLMs), using qualitative content analysis and expert validation. Our\nanalysis, with both LLMs and human experts in the loop, uncovered potential for\nLLM integration together with inadequacies in LLM risk oversight of those\nframeworks. Comparative gap analysis has highlighted that the new ISO\n42001:2023, specifically designed for Artificial Intelligence (AI) management\nsystems, provided most comprehensive facilitation for LLM opportunities,\nwhereas COBIT 2019 aligned most closely with the impending European Union AI\nAct. Nonetheless, our findings suggested that all evaluated frameworks would\nbenefit from enhancements to more effectively and more comprehensively address\nthe multifaceted risks associated with LLMs, indicating a critical and\ntime-sensitive need for their continuous evolution. We propose integrating\nhuman-expert-in-the-loop validation processes as crucial for enhancing\ncybersecurity frameworks to support secure and compliant LLM integration, and\ndiscuss implications for the continuous evolution of cybersecurity GRC\nframeworks to support the secure integration of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15770v1.pdf"
    },
    {
        "title": "Instruction-Driven Game Engines on Large Language Models",
        "authors": [
            "Hongqiu Wu",
            "Y. Wang",
            "Xingyuan Liu",
            "Hai Zhao",
            "Min Zhang"
        ],
        "published": "2024-03-30T08:02:16Z",
        "summary": "The Instruction-Driven Game Engine (IDGE) project aims to democratize game\ndevelopment by enabling a large language model (LLM) to follow free-form game\nrules and autonomously generate game-play processes. The IDGE allows users to\ncreate games by issuing simple natural language instructions, which\nsignificantly lowers the barrier for game development. We approach the learning\nprocess for IDGEs as a Next State Prediction task, wherein the model\nautoregressively predicts in-game states given player actions. It is a\nchallenging task because the computation of in-game states must be precise;\notherwise, slight errors could disrupt the game-play. To address this, we train\nthe IDGE in a curriculum manner that progressively increases the model's\nexposure to complex scenarios. Our initial progress lies in developing an IDGE\nfor Poker, a universally cherished card game. The engine we've designed not\nonly supports a wide range of poker variants but also allows for high\ncustomization of rules through natural language inputs. Furthermore, it also\nfavors rapid prototyping of new games from minimal samples, proposing an\ninnovative paradigm in game development that relies on minimal prompt and data\nengineering. This work lays the groundwork for future advancements in\ninstruction-driven game creation, potentially transforming how games are\ndesigned and played.",
        "pdf_link": "https://arxiv.org/pdf/2404.00276v2.pdf"
    },
    {
        "title": "A Fast, Performant, Secure Distributed Training Framework For Large Language Model",
        "authors": [
            "Wei Huang",
            "Yinggui Wang",
            "Anda Cheng",
            "Aihui Zhou",
            "Chaofan Yu",
            "Lei Wang"
        ],
        "published": "2024-01-18T08:33:09Z",
        "summary": "The distributed (federated) LLM is an important method for co-training the\ndomain-specific LLM using siloed data. However, maliciously stealing model\nparameters and data from the server or client side has become an urgent problem\nto be solved. In this paper, we propose a secure distributed LLM based on model\nslicing. In this case, we deploy the Trusted Execution Environment (TEE) on\nboth the client and server side, and put the fine-tuned structure (LoRA or\nembedding of P-tuning v2) into the TEE. Then, secure communication is executed\nin the TEE and general environments through lightweight encryption. In order to\nfurther reduce the equipment cost as well as increase the model performance and\naccuracy, we propose a split fine-tuning scheme. In particular, we split the\nLLM by layers and place the latter layers in a server-side TEE (the client does\nnot need a TEE). We then combine the proposed Sparsification Parameter\nFine-tuning (SPF) with the LoRA part to improve the accuracy of the downstream\ntask. Numerous experiments have shown that our method guarantees accuracy while\nmaintaining security.",
        "pdf_link": "https://arxiv.org/pdf/2401.09796v2.pdf"
    },
    {
        "title": "Transfer Learning for Text Diffusion Models",
        "authors": [
            "Kehang Han",
            "Kathleen Kenealy",
            "Aditya Barua",
            "Noah Fiedel",
            "Noah Constant"
        ],
        "published": "2024-01-30T17:11:56Z",
        "summary": "In this report, we explore the potential for text diffusion to replace\nautoregressive (AR) decoding for the training and deployment of large language\nmodels (LLMs). We are particularly interested to see whether pretrained AR\nmodels can be transformed into text diffusion models through a lightweight\nadaptation procedure we call ``AR2Diff''. We begin by establishing a strong\nbaseline setup for training text diffusion models. Comparing across multiple\narchitectures and pretraining objectives, we find that training a decoder-only\nmodel with a prefix LM objective is best or near-best across several tasks.\nBuilding on this finding, we test various transfer learning setups for text\ndiffusion models. On machine translation, we find that text diffusion\nunderperforms the standard AR approach. However, on code synthesis and\nextractive QA, we find diffusion models trained from scratch outperform AR\nmodels in many cases. We also observe quality gains from AR2Diff -- adapting AR\nmodels to use diffusion decoding. These results are promising given that text\ndiffusion is relatively underexplored and can be significantly faster than AR\ndecoding for long text generation.",
        "pdf_link": "https://arxiv.org/pdf/2401.17181v1.pdf"
    },
    {
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "authors": [
            "Hanlin Tang",
            "Yifu Sun",
            "Decheng Wu",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "published": "2024-03-05T08:45:30Z",
        "summary": "Large language models (LLMs) have proven to be very superior to conventional\nmethods in various tasks. However, their expensive computations and high memory\nrequirements are prohibitive for deployment. Model quantization is an effective\nmethod for reducing this overhead. The problem is that in most previous works,\nthe quantized model was calibrated using few samples from the training data,\nwhich might affect the generalization of the quantized LLMs to unknown cases\nand tasks. Hence in this work, we explore an important question: Can we design\na data-independent quantization method for LLMs to guarantee its generalization\nperformance? In this work, we propose EasyQuant, a training-free and\ndata-independent weight-only quantization algorithm for LLMs. Our observation\nindicates that two factors: outliers in the weight and quantization ranges, are\nessential for reducing the quantization error. Therefore, in EasyQuant, we\nleave the outliers (less than 1%) unchanged and optimize the quantization range\nto reduce the reconstruction error. With these methods, we surprisingly find\nthat EasyQuant achieves comparable performance to the original model. Since\nEasyQuant does not depend on any training data, the generalization performance\nof quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented\nin parallel so that the quantized model could be attained in a few minutes even\nfor LLMs over 100B. To our best knowledge, we are the first work that achieves\nalmost lossless quantization performance for LLMs under a data-independent\nsetting and our algorithm runs over 10 times faster than the data-dependent\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2403.02775v1.pdf"
    },
    {
        "title": "A Comparative Analysis of Conversational Large Language Models in Knowledge-Based Text Generation",
        "authors": [
            "Phillip Schneider",
            "Manuel Klettner",
            "Elena Simperl",
            "Florian Matthes"
        ],
        "published": "2024-02-02T15:26:39Z",
        "summary": "Generating natural language text from graph-structured data is essential for\nconversational information seeking. Semantic triples derived from knowledge\ngraphs can serve as a valuable source for grounding responses from\nconversational agents by providing a factual basis for the information they\ncommunicate. This is especially relevant in the context of large language\nmodels, which offer great potential for conversational interaction but are\nprone to hallucinating, omitting, or producing conflicting information. In this\nstudy, we conduct an empirical analysis of conversational large language models\nin generating natural language text from semantic triples. We compare four\nlarge language models of varying sizes with different prompting techniques.\nThrough a series of benchmark experiments on the WebNLG dataset, we analyze the\nmodels' performance and identify the most common issues in the generated\npredictions. Our findings show that the capabilities of large language models\nin triple verbalization can be significantly improved through few-shot\nprompting, post-processing, and efficient fine-tuning techniques, particularly\nfor smaller models that exhibit lower zero-shot performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.01495v1.pdf"
    },
    {
        "title": "WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?",
        "authors": [
            "Alexandre Drouin",
            "Maxime Gasse",
            "Massimo Caccia",
            "Issam H. Laradji",
            "Manuel Del Verme",
            "Tom Marty",
            "L\u00e9o Boisvert",
            "Megh Thakkar",
            "Quentin Cappart",
            "David Vazquez",
            "Nicolas Chapados",
            "Alexandre Lacoste"
        ],
        "published": "2024-03-12T14:58:45Z",
        "summary": "We study the use of large language model-based agents for interacting with\nsoftware via web browsers. Unlike prior work, we focus on measuring the agents'\nability to perform tasks that span the typical daily work of knowledge workers\nutilizing enterprise software systems. To this end, we propose WorkArena, a\nremote-hosted benchmark of 29 tasks based on the widely-used ServiceNow\nplatform. We also introduce BrowserGym, an environment for the design and\nevaluation of such agents, offering a rich set of actions as well as multimodal\nobservations. Our empirical evaluation reveals that while current agents show\npromise on WorkArena, there remains a considerable gap towards achieving full\ntask automation. Notably, our analysis uncovers a significant performance\ndisparity between open and closed-source LLMs, highlighting a critical area for\nfuture exploration and development in the field.",
        "pdf_link": "https://arxiv.org/pdf/2403.07718v1.pdf"
    },
    {
        "title": "Never-Ending Embodied Robot Learning",
        "authors": [
            "Wenqi Liang",
            "Gan Sun",
            "Qian He",
            "Yu Ren",
            "Jiahua Dong",
            "Yang Cong"
        ],
        "published": "2024-03-01T07:51:29Z",
        "summary": "Relying on large language models (LLMs), embodied robots could perform\ncomplex multimodal robot manipulation tasks from visual observations with\npowerful generalization ability. However, most visual behavior-cloning agents\nsuffer from manipulation performance degradation and skill knowledge forgetting\nwhen adapting into a series of challenging unseen tasks. We here investigate\nthe above challenge with NBCagent in embodied robots, a pioneering\nlanguage-conditioned Never-ending Behavior-Cloning agent, which can continually\nlearn observation knowledge of novel robot manipulation skills from\nskill-specific and skill-shared attributes. Specifically, we establish a\nskill-specific evolving planner to perform knowledge decoupling, which can\ncontinually embed novel skill-specific knowledge in our NBCagent agent from\nlatent and low-rank space. Meanwhile, we propose a skill-shared semantics\nrendering module and a skill-shared representation distillation module to\neffectively transfer anti-forgetting skill-shared knowledge, further tackling\ncatastrophic forgetting on old skills from semantics and representation\naspects. Finally, we design a continual embodied robot manipulation benchmark,\nand several expensive experiments demonstrate the significant performance of\nour method. Visual results, code, and dataset are provided at:\nhttps://neragent.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2403.00336v1.pdf"
    },
    {
        "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization",
        "authors": [
            "Yang Jin",
            "Zhicheng Sun",
            "Kun Xu",
            "Kun Xu",
            "Liwei Chen",
            "Hao Jiang",
            "Quzhe Huang",
            "Chengru Song",
            "Yuliang Liu",
            "Di Zhang",
            "Yang Song",
            "Kun Gai",
            "Yadong Mu"
        ],
        "published": "2024-02-05T16:30:49Z",
        "summary": "In light of recent advances in multimodal Large Language Models (LLMs), there\nis increasing attention to scaling them from image-text data to more\ninformative real-world videos. Compared to static images, video poses unique\nchallenges for effective large-scale pre-training due to the modeling of its\nspatiotemporal dynamics. In this paper, we address such limitations in\nvideo-language pre-training with an efficient video decomposition that\nrepresents each video as keyframes and temporal motions. These are then adapted\nto an LLM using well-designed tokenizers that discretize visual and temporal\ninformation as a few tokens, thus enabling unified generative pre-training of\nvideos, images, and text. At inference, the generated tokens from the LLM are\ncarefully recovered to the original continuous pixel space to create various\nvideo content. Our proposed framework is both capable of comprehending and\ngenerating image and video content, as demonstrated by its competitive\nperformance across 13 multimodal benchmarks in image and video understanding\nand generation. Our code and models will be available at\nhttps://video-lavit.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2402.03161v2.pdf"
    },
    {
        "title": "Navigating the Peril of Generated Alternative Facts: A ChatGPT-4 Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation",
        "authors": [
            "Malik Sallam",
            "Jan Egger",
            "Rainer Roehrig",
            "Behrus Puladi"
        ],
        "published": "2024-02-04T13:21:19Z",
        "summary": "In an era where artificial intelligence (AI) intertwines with medical\nresearch, the delineation of truth becomes increasingly complex. This study\nostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega\nvariant, showcasing 31 unique mutations in the S gene region. However, the real\nundercurrent of this narrative is a demonstration of the ease with which AI,\nspecifically ChatGPT-4, can fabricate convincing yet entirely fictional\nscientific data. The so-called Omega variant was identified in a fully\nvaccinated, previously infected 35-year-old male presenting with severe\nCOVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and\ncontact tracing, this study mirrors the rigorous methodology of genuine case\nreports, thereby setting the stage for a compelling but entirely constructed\nnarrative. The entire case study was generated by ChatGPT-4, a large language\nmodel by OpenAI. The fabricated Omega variant features an ensemble of\nmutations, including N501Y and E484K, known for enhancing ACE2 receptor\naffinity, alongside L452R and P681H, ostensibly indicative of immune evasion.\nThis variant's contrived interaction dynamics - severe symptoms in a vaccinated\nindividual versus mild ones in unvaccinated contacts - were designed to mimic\nreal-world complexities, including suggestions of antibody-dependent\nenhancement (ADE). While the Omega variant is a product of AI-generated\nfiction, the implications of this exercise are real and profound. The ease with\nwhich AI can generate believable but false scientific information, as\nillustrated in this case, raises significant concerns about the potential for\nmisinformation in medicine. This study, therefore, serves as a cautionary tale,\nemphasizing the necessity for critical evaluation of sources, especially in an\nage where AI tools like ChatGPT are becoming increasingly sophisticated and\nwidespread in their use.",
        "pdf_link": "https://arxiv.org/pdf/2403.09674v1.pdf"
    },
    {
        "title": "LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System",
        "authors": [
            "Yan Zhao",
            "Zhongyun Li",
            "Yushan Pan",
            "Jiaxing Wang",
            "Yihong Wang"
        ],
        "published": "2024-02-05T16:47:17Z",
        "summary": "Generative Artificial Intelligence (AI), because of its emergent abilities,\nhas empowered various fields, one typical of which is large language models\n(LLMs). One of the typical application fields of Generative AI is large\nlanguage models (LLMs), and the natural language understanding capability of\nLLM is dramatically improved when compared with conventional AI-based methods.\nThe natural language understanding capability has always been a barrier to the\nintent recognition performance of the Knowledge-Based-Question-and-Answer\n(KBQA) system, which arises from linguistic diversity and the newly appeared\nintent. Conventional AI-based methods for intent recognition can be divided\ninto semantic parsing-based and model-based approaches. However, both of the\nmethods suffer from limited resources in intent recognition. To address this\nissue, we propose a novel KBQA system based on a Large Language Model(LLM) and\nBERT (LB-KBQA). With the help of generative AI, our proposed method could\ndetect newly appeared intent and acquire new knowledge. In experiments on\nfinancial domain question answering, our model has demonstrated superior\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.05130v2.pdf"
    },
    {
        "title": "LLM-based NLG Evaluation: Current Status and Challenges",
        "authors": [
            "Mingqi Gao",
            "Xinyu Hu",
            "Jie Ruan",
            "Xiao Pu",
            "Xiaojun Wan"
        ],
        "published": "2024-02-02T13:06:35Z",
        "summary": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in artificial intelligence. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\nevaluation methods, and discuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\nopen problems in this area and point out future research directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.01383v2.pdf"
    },
    {
        "title": "Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models",
        "authors": [
            "Kaiwen Wei",
            "Jingyuan Zhang",
            "Hongzhi Zhang",
            "Fuzheng Zhang",
            "Di Zhang",
            "Li Jin",
            "Yue Yu"
        ],
        "published": "2024-02-20T08:03:05Z",
        "summary": "Large Language Models (LLMs) exhibit remarkable generative capabilities,\nenabling the generation of valuable information. Despite these advancements,\nprevious research found that LLMs sometimes struggle with adhering to specific\nconstraints (e.g., in specific place or at specific time), at times even\noverlooking them, which leads to responses that are either too generic or not\nfully satisfactory. Existing approaches attempted to address this issue by\ndecomposing or rewriting input instructions, yet they fall short in adequately\nemphasizing specific constraints and in unlocking the underlying knowledge\n(e.g., programming within the context of software development). In response,\nthis paper proposes a simple yet effective method named Chain-of-Specificity\n(CoS). Specifically, CoS iteratively emphasizes the specific constraints in the\ninput instructions, unlocks knowledge within LLMs, and refines responses.\nExperiments conducted on publicly available and self-build complex datasets\ndemonstrate that CoS outperforms existing methods in enhancing generated\ncontent especially for the specificity. Besides, as the number of specific\nconstraints increase, other baselines falter, while CoS still performs well.\nMoreover, we show that distilling responses generated by CoS effectively\nenhances the ability of smaller models to follow the constrained instructions.\nResources of this paper will be released for further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.15526v1.pdf"
    },
    {
        "title": "InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks",
        "authors": [
            "Xueyu Hu",
            "Ziyu Zhao",
            "Shuang Wei",
            "Ziwei Chai",
            "Qianli Ma",
            "Guoyin Wang",
            "Xuwu Wang",
            "Jing Su",
            "Jingjing Xu",
            "Ming Zhu",
            "Yao Cheng",
            "Jianbo Yuan",
            "Jiwei Li",
            "Kun Kuang",
            "Yang Yang",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "published": "2024-01-10T19:04:00Z",
        "summary": "In this paper, we introduce InfiAgent-DABench, the first benchmark\nspecifically designed to evaluate LLM-based agents on data analysis tasks.\nThese tasks require agents to end-to-end solving complex tasks by interacting\nwith an execution environment. This benchmark contains DAEval, a dataset\nconsisting of 257 data analysis questions derived from 52 CSV files, and an\nagent framework which incorporates LLMs to serve as data analysis agents for\nboth serving and evaluation. Since data analysis questions are often open-ended\nand hard to evaluate without human supervision, we adopt a format-prompting\ntechnique to convert each question into a closed-form format so that they can\nbe automatically evaluated. Our extensive benchmarking of 34 LLMs uncovers the\ncurrent challenges encountered in data analysis tasks. In addition, building on\ntop of our agent framework, we develop a specialized agent, DAAgent, which\nsurpasses GPT-3.5 by 3.9% on DABench. Evaluation datasets and toolkits for\nInfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent .",
        "pdf_link": "https://arxiv.org/pdf/2401.05507v3.pdf"
    },
    {
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
        "authors": [
            "Hongru Wang",
            "Wenyu Huang",
            "Yang Deng",
            "Rui Wang",
            "Zezhong Wang",
            "Yufei Wang",
            "Fei Mi",
            "Jeff Z. Pan",
            "Kam-Fai Wong"
        ],
        "published": "2024-01-24T06:50:20Z",
        "summary": "Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.13256v1.pdf"
    },
    {
        "title": "ExpressEdit: Video Editing with Natural Language and Sketching",
        "authors": [
            "Bekzat Tilekbay",
            "Saelyne Yang",
            "Michal Lewkowicz",
            "Alex Suryapranata",
            "Juho Kim"
        ],
        "published": "2024-03-26T13:34:21Z",
        "summary": "Informational videos serve as a crucial source for explaining conceptual and\nprocedural knowledge to novices and experts alike. When producing informational\nvideos, editors edit videos by overlaying text/images or trimming footage to\nenhance the video quality and make it more engaging. However, video editing can\nbe difficult and time-consuming, especially for novice video editors who often\nstruggle with expressing and implementing their editing ideas. To address this\nchallenge, we first explored how multimodality$-$natural language (NL) and\nsketching, which are natural modalities humans use for expression$-$can be\nutilized to support video editors in expressing video editing ideas. We\ngathered 176 multimodal expressions of editing commands from 10 video editors,\nwhich revealed the patterns of use of NL and sketching in describing edit\nintents. Based on the findings, we present ExpressEdit, a system that enables\nediting videos via NL text and sketching on the video frame. Powered by LLM and\nvision models, the system interprets (1) temporal, (2) spatial, and (3)\noperational references in an NL command and spatial references from sketching.\nThe system implements the interpreted edits, which then the user can iterate\non. An observational study (N=10) showed that ExpressEdit enhanced the ability\nof novice video editors to express and implement their edit ideas. The system\nallowed participants to perform edits more efficiently and generate more ideas\nby generating edits based on user's multimodal edit commands and supporting\niterations on the editing commands. This work offers insights into the design\nof future multimodal interfaces and AI-based pipelines for video editing.",
        "pdf_link": "https://arxiv.org/pdf/2403.17693v1.pdf"
    },
    {
        "title": "Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning",
        "authors": [
            "Yuelyu Ji",
            "Zeshui Yu",
            "Yanshan Wang"
        ],
        "published": "2024-01-31T05:11:00Z",
        "summary": "In this study, we aim to address the task of assertion detection when\nextracting medical concepts from clinical notes, a key process in clinical\nnatural language processing (NLP). Assertion detection in clinical NLP usually\ninvolves identifying assertion types for medical concepts in the clinical text,\nnamely certainty (whether the medical concept is positive, negated, possible,\nor hypothetical), temporality (whether the medical concept is for present or\nthe past history), and experiencer (whether the medical concept is described\nfor the patient or a family member). These assertion types are essential for\nhealthcare professionals to quickly and clearly understand the context of\nmedical conditions from unstructured clinical texts, directly influencing the\nquality and outcomes of patient care. Although widely used, traditional\nmethods, particularly rule-based NLP systems and machine learning or deep\nlearning models, demand intensive manual efforts to create patterns and tend to\noverlook less common assertion types, leading to an incomplete understanding of\nthe context. To address this challenge, our research introduces a novel\nmethodology that utilizes Large Language Models (LLMs) pre-trained on a vast\narray of medical data for assertion detection. We enhanced the current method\nwith advanced reasoning techniques, including Tree of Thought (ToT), Chain of\nThought (CoT), and Self-Consistency (SC), and refine it further with Low-Rank\nAdaptation (LoRA) fine-tuning. We first evaluated the model on the i2b2 2010\nassertion dataset. Our method achieved a micro-averaged F-1 of 0.89, with 0.11\nimprovements over the previous works. To further assess the generalizability of\nour approach, we extended our evaluation to a local dataset that focused on\nsleep concept extraction. Our approach achieved an F-1 of 0.74, which is 0.31\nhigher than the previous method.",
        "pdf_link": "https://arxiv.org/pdf/2401.17602v1.pdf"
    },
    {
        "title": "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs",
        "authors": [
            "Shaoxiang Chen",
            "Zequn Jie",
            "Lin Ma"
        ],
        "published": "2024-01-29T13:48:36Z",
        "summary": "Instruction finetuning on a variety of image-text instruction data is the key\nto obtaining a versatile Multimodal Large Language Model (MLLM), and different\nconfigurations of the instruction data can lead to finetuned models with\ndifferent capabilities. However, we have discovered that data conflicts are\ninevitable when mixing instruction data from distinct domains, which can result\nin performance drops for tasks of a specific domain. To address this issue, we\npropose to apply an efficient Mixture of Experts (MoE) design, which is a\nsparse Mixture of LoRA Experts (MoLE) for instruction finetuning MLLMs. Within\nthe Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method\nby creating a set of LoRA experts specifically for the MLP layer, and route\neach token to the top-1 expert based on a routing function, allowing adaptive\nchoices for tokens from different domains. Since the LoRA experts are sparsely\nactivated, the training and inference cost are kept roughly constant compared\nto the original LoRA method. By replacing the plain-LoRA of LLaVA-1.5 with our\nMoE design, our final model is named LLaVA-MoLE. Extensive experiments proved\nthat LLaVA-MoLE effectively mitigates the data conflict issue when mixing\nmultiple distinct instruction datasets with various configurations, and\nachieves consistent performance gains over the strong plain-LoRA baselines.\nMost importantly, on the mixed datasets, LLaVA-MoLE can even outperform the\nplain-LoRA baseline trained with twice the samples.",
        "pdf_link": "https://arxiv.org/pdf/2401.16160v2.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision",
        "authors": [
            "Jinyan Su",
            "Peilin Yu",
            "Jieyu Zhang",
            "Stephen H. Bach"
        ],
        "published": "2024-02-02T19:45:39Z",
        "summary": "Prompted weak supervision (PromptedWS) applies pre-trained large language\nmodels (LLMs) as the basis for labeling functions (LFs) in a weak supervision\nframework to obtain large labeled datasets. We further extend the use of LLMs\nin the loop to address one of the key challenges in weak supervision: learning\nthe statistical dependency structure among supervision sources. In this work,\nwe ask the LLM how similar are these prompted LFs. We propose a Structure\nRefining Module, a simple yet effective first approach based on the\nsimilarities of the prompts by taking advantage of the intrinsic structure in\nthe embedding space. At the core of Structure Refining Module are Labeling\nFunction Removal (LaRe) and Correlation Structure Generation (CosGen). Compared\nto previous methods that learn the dependencies from weak labels, our method\nfinds the dependencies which are intrinsic to the LFs and less dependent on the\ndata. We show that our Structure Refining Module improves the PromptedWS\npipeline by up to 12.7 points on the benchmark tasks. We also explore the\ntrade-offs between efficiency and performance with comprehensive ablation\nexperiments and analysis. Code for this project can be found in\nhttps://github.com/BatsResearch/su-bigdata23-code.",
        "pdf_link": "https://arxiv.org/pdf/2402.01867v1.pdf"
    },
    {
        "title": "ReEvo: Large Language Models as Hyper-Heuristics with Reflective Evolution",
        "authors": [
            "Haoran Ye",
            "Jiarui Wang",
            "Zhiguang Cao",
            "Guojie Song"
        ],
        "published": "2024-02-02T05:04:51Z",
        "summary": "The omnipresence of NP-hard combinatorial optimization problems (COPs)\ncompels domain experts to engage in trial-and-error heuristic design process.\nThe long-standing endeavor of design automation has gained new momentum with\nthe rise of large language models (LLMs). This paper introduces Language\nHyper-Heuristics (LHHs), an emerging variant of Hyper-Heuristics that leverages\nLLMs for heuristic generation, featuring minimal manual intervention and\nopen-ended heuristic spaces. To empower LHHs, we present Reflective Evolution\n(ReEvo), a generic searching framework that emulates the reflective design\napproach of human experts while far surpassing human capabilities with its\nscalable LLM inference, Internet-scale domain knowledge, and powerful\nevolutionary search. Evaluations across 12 COP settings show that 1) verbal\nreflections for evolution lead to smoother fitness landscapes, explicit\ninference of black-box COP settings, and better search results; 2) heuristics\ngenerated by ReEvo in minutes can outperform state-of-the-art human designs and\nneural solvers; 3) LHHs enable efficient algorithm design automation even when\nchallenged with black-box COPs, demonstrating its potential for complex and\nnovel real-world applications. Our code is available:\nhttps://github.com/ai4co/LLM-as-HH.",
        "pdf_link": "https://arxiv.org/pdf/2402.01145v1.pdf"
    },
    {
        "title": "Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures",
        "authors": [
            "Chu-Cheng Lin",
            "Xinyi Wang",
            "Jonathan H. Clark",
            "Han Lu",
            "Yun Zhu",
            "Chenxi Whitehouse",
            "Hongkun Yu"
        ],
        "published": "2024-02-27T23:12:45Z",
        "summary": "Adapting pretrained large language models (LLMs) to various downstream tasks\nin tens or hundreds of human languages is computationally expensive.\nParameter-efficient fine-tuning (PEFT) significantly reduces the adaptation\ncost, by tuning only a small amount of parameters. However, directly applying\nPEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could\nlead to suboptimal performance due to limited parameter capacity and negative\ninterference among different datasets. In this work, we propose Featurized\nLow-rank Mixtures (FLix), a novel PEFT method designed for effective multitask\nmultilingual tuning. FLix associates each unique dataset feature, such as the\ndataset's language or task, with its own low-rank weight update parameters. By\ncomposing feature-specific parameters for each dataset, FLix can accommodate\ndiverse dataset mixtures and generalize better to unseen datasets. Our\nexperiments show that FLix leads to significant improvements over a variety of\ntasks for both supervised learning and zero-shot settings using different\ntraining data mixtures.",
        "pdf_link": "https://arxiv.org/pdf/2402.17934v1.pdf"
    },
    {
        "title": "Large Language Model Meets Graph Neural Network in Knowledge Distillation",
        "authors": [
            "Shengxiang Hu",
            "Guobing Zou",
            "Song Yang",
            "Yanglan Gan",
            "Bofeng Zhang",
            "Yixin Chen"
        ],
        "published": "2024-02-08T18:33:21Z",
        "summary": "Despite recent community revelations about the advancements and potential\napplications of Large Language Models (LLMs) in understanding Text-Attributed\nGraph (TAG), the deployment of LLMs for production is hindered by its high\ncomputational and storage requirements, as well as long latencies during model\ninference. Simultaneously, although traditional Graph Neural Networks (GNNs)\nare light weight and adept at learning structural features of graphs, their\nability to grasp the complex semantics in TAG is somewhat constrained for real\napplications. To address these limitations, we concentrate on the downstream\ntask of node classification in TAG and propose a novel graph knowledge\ndistillation framework, termed Linguistic Graph Knowledge Distillation\n(LinguGKD), using LLMs as teacher models and GNNs as student models for\nknowledge distillation. It involves TAG-oriented instruction tuning of LLM on\ndesigned tailored prompts, followed by propagating knowledge and aligning the\nhierarchically learned node features from the teacher LLM to the student GNN in\nlatent space, employing a layer-adaptive contrastive learning strategy. Through\nextensive experiments on a variety of LLM and GNN models and multiple benchmark\ndatasets, the proposed LinguGKD significantly boosts the student GNN's\npredictive accuracy and convergence rate, without the need of extra data or\nmodel parameters. Compared to teacher LLM, distilled GNN achieves superior\ninference speed equipped with much fewer computing and storage demands, when\nsurpassing the teacher LLM's classification accuracy on some of benchmark\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.05894v2.pdf"
    },
    {
        "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
        "authors": [
            "Liang Zhang",
            "Katherine Jijo",
            "Spurthi Setty",
            "Eden Chung",
            "Fatima Javid",
            "Natan Vidra",
            "Tommy Clifford"
        ],
        "published": "2024-01-27T00:18:07Z",
        "summary": "Large Language Models (LLMs) generate responses to questions; however, their\neffectiveness is often hindered by sub-optimal quality of answers and\noccasional failures to provide accurate responses to questions. To address\nthese challenges, a fine-tuning process is employed, involving feedback and\nexamples to refine models. The objective is to enhance AI models through\ncontinuous feedback loops, utilizing metrics such as cosine similarity, LLM\nevaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like\nGPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on\nfinancial datasets, including the FinanceBench and RAG Instruct Benchmark\nTester Dataset, illustrating the necessity of fine-tuning. The results showcase\nthe capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,\nproviding superior question and answering capabilities. Notably, the\ncombination of fine-tuning the LLM with a process known as Retrieval Augmented\nGeneration (RAG) proves to generate responses with improved accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.01722v1.pdf"
    },
    {
        "title": "SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models",
        "authors": [
            "Tianhan Xu",
            "Zhe Hu",
            "Ling Chen",
            "Bin Li"
        ],
        "published": "2024-02-01T10:26:27Z",
        "summary": "Recent advances in large language models (LLMs) have demonstrated exceptional\nperformance in various natural language processing (NLP) tasks. However, their\neffective application in the medical domain is hampered by a lack of medical\ndomain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable\nframework that aims to inject medical knowledge into general-purpose LLMs\nthrough instruction tuning, thereby enabling adaptability for various\ndownstream tasks. SA-MDKIF consists of two stages: skill training and skill\nadaptation. In the first stage, we define 12 basic medical skills and use\nAdaLoRA to train these skills based on uniformly formatted instructional\ndatasets that we have constructed. In the next stage, we train the skill router\nusing task-specific downstream data and use this router to integrate the\nacquired skills with LLMs during inference. Experimental results on 9 different\nmedical tasks show that SA-MDKIF improves performance by 10-20% compared to the\noriginal LLMs. Notably, this improvement is particularly pronounced for unseen\nmedical tasks, showing an improvement of up to 30%.",
        "pdf_link": "https://arxiv.org/pdf/2402.00474v1.pdf"
    },
    {
        "title": "Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain",
        "authors": [
            "Burcu Sayin",
            "Pasquale Minervini",
            "Jacopo Staiano",
            "Andrea Passerini"
        ],
        "published": "2024-03-29T16:59:13Z",
        "summary": "We explore the potential of Large Language Models (LLMs) to assist and\npotentially correct physicians in medical decision-making tasks. We evaluate\nseveral LLMs, including Meditron, Llama2, and Mistral, to analyze the ability\nof these models to interact effectively with physicians across different\nscenarios. We consider questions from PubMedQA and several tasks, ranging from\nbinary (yes/no) responses to long answer generation, where the answer of the\nmodel is produced after an interaction with a physician. Our findings suggest\nthat prompt design significantly influences the downstream accuracy of LLMs and\nthat LLMs can provide valuable feedback to physicians, challenging incorrect\ndiagnoses and contributing to more accurate decision-making. For example, when\nthe physician is accurate 38% of the time, Mistral can produce the correct\nanswer, improving accuracy up to 74% depending on the prompt being used, while\nLlama2 and Meditron models exhibit greater sensitivity to prompt choice. Our\nanalysis also uncovers the challenges of ensuring that LLM-generated\nsuggestions are pertinent and useful, emphasizing the need for further research\nin this area.",
        "pdf_link": "https://arxiv.org/pdf/2403.20288v1.pdf"
    },
    {
        "title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?",
        "authors": [
            "Guijin Son",
            "Sangwon Baek",
            "Sangdae Nam",
            "Ilgyun Jeong",
            "Seungone Kim"
        ],
        "published": "2024-02-18T14:25:19Z",
        "summary": "Large language models (LLMs) are typically prompted to follow a single\ninstruction per inference call. In this work, we analyze whether LLMs also hold\nthe capability to handle multiple instructions simultaneously, denoted as\nMulti-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task\nInference Benchmark), a comprehensive evaluation benchmark encompassing 5,000\ninstances across 25 tasks. Each task in the MTI Bench involves 2 to 3\nsub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces\nthe total inference time by 1.46 times in average since it does not require\nmultiple inference calls. Interestingly, contrary to the expectation that LLMs\nwould perform better when tasks are divided, we find that state-of-the-art\nLLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved\nperformance with Multi-Task Inference compared to Single-Task Inference on the\nMTI Bench. We release the MTI Bench dataset and our code at this link\nhttps://github.com/guijinSON/MTI-Bench.",
        "pdf_link": "https://arxiv.org/pdf/2402.11597v1.pdf"
    },
    {
        "title": "Efficient Multimodal Learning from Data-centric Perspective",
        "authors": [
            "Muyang He",
            "Yexin Liu",
            "Boya Wu",
            "Jianhao Yuan",
            "Yueze Wang",
            "Tiejun Huang",
            "Bo Zhao"
        ],
        "published": "2024-02-18T10:09:10Z",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated notable\ncapabilities in general visual understanding and reasoning tasks. However,\ntheir deployment is hindered by substantial computational costs in both\ntraining and inference, limiting accessibility to the broader research and user\ncommunities. A straightforward solution is to leverage smaller pre-trained\nvision and language models, which inevitably causes significant performance\ndrop. In this paper, we demonstrate the possibility to beat the scaling law and\ntrain a smaller but better MLLM by exploring more informative training data.\nSpecifically, we introduce Bunny, a family of lightweight MLLMs with flexible\nvision and language backbones for efficient multimodal learning from condensed\ntraining data. Remarkably, our Bunny-3B outperforms the state-of-the-art large\nMLLMs, especially LLaVA-v1.5-13B, on multiple benchmarks. The code, models and\ndata can be found in https://github.com/BAAI-DCAI/Bunny.",
        "pdf_link": "https://arxiv.org/pdf/2402.11530v1.pdf"
    },
    {
        "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers",
        "authors": [
            "Hui Huang",
            "Yingqi Qu",
            "Jing Liu",
            "Muyun Yang",
            "Tiejun Zhao"
        ],
        "published": "2024-03-05T10:20:52Z",
        "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. In this study, we conduct an empirical study of\ndifferent judge models on their evaluation capability. Our findings indicate\nthat although the fine-tuned judge models achieve high accuracy on in-domain\ntest sets, even surpassing GPT4, they are inherently task-specific classifiers,\nand their generalizability and fairness severely underperform GPT4.",
        "pdf_link": "https://arxiv.org/pdf/2403.02839v1.pdf"
    },
    {
        "title": "Cheetah: Natural Language Generation for 517 African Languages",
        "authors": [
            "Ife Adebara",
            "AbdelRahim Elmadany",
            "Muhammad Abdul-Mageed"
        ],
        "published": "2024-01-02T06:24:13Z",
        "summary": "Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across six generation downstream\ntasks. In five of the six tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We publicly release our models for research.",
        "pdf_link": "https://arxiv.org/pdf/2401.01053v3.pdf"
    },
    {
        "title": "LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study",
        "authors": [
            "Zihao Xu",
            "Yi Liu",
            "Gelei Deng",
            "Yuekang Li",
            "Stjepan Picek"
        ],
        "published": "2024-02-21T01:26:39Z",
        "summary": "Large Language Models (LLMS) have increasingly become central to generating\ncontent with potential societal impacts. Notably, these models have\ndemonstrated capabilities for generating content that could be deemed harmful.\nTo mitigate these risks, researchers have adopted safety training techniques to\nalign model outputs with societal values to curb the generation of malicious\ncontent. However, the phenomenon of \"jailbreaking\", where carefully crafted\nprompts elicit harmful responses from models, persists as a significant\nchallenge. This research conducts a comprehensive analysis of existing studies\non jailbreaking LLMs and their defense techniques. We meticulously investigate\nnine attack techniques and seven defense techniques applied across three\ndistinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate\nthe effectiveness of these attack and defense techniques. Our findings reveal\nthat existing white-box attacks underperform compared to universal techniques\nand that including special tokens in the input significantly affects the\nlikelihood of successful attacks. This research highlights the need to\nconcentrate on the security facets of LLMs. Additionally, we contribute to the\nfield by releasing our datasets and testing framework, aiming to foster further\nresearch into LLM security. We believe these contributions will facilitate the\nexploration of security measures within this domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.13457v1.pdf"
    },
    {
        "title": "FAC$^2$E: Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition",
        "authors": [
            "Xiaoqiang Wang",
            "Bang Liu",
            "Lingfei Wu"
        ],
        "published": "2024-02-29T21:05:37Z",
        "summary": "Large language models (LLMs) are primarily evaluated by overall performance\non various text understanding and generation tasks. However, such a paradigm\nfails to comprehensively differentiate the fine-grained language and cognitive\nskills, rendering the lack of sufficient interpretation to LLMs' capabilities.\nIn this paper, we present FAC$^2$E, a framework for Fine-grAined and\nCognition-grounded LLMs' Capability Evaluation. Specifically, we formulate\nLLMs' evaluation in a multi-dimensional and explainable manner by dissociating\nthe language-related capabilities and the cognition-related ones. Besides,\nthrough extracting the intermediate reasoning from LLMs, we further break down\nthe process of applying a specific capability into three sub-steps: recalling\nrelevant knowledge, utilizing knowledge, and solving problems. Finally,\nFAC$^2$E evaluates each sub-step of each fine-grained capability, providing a\ntwo-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common\nshortfall in knowledge utilization among models and propose a straightforward,\nknowledge-enhanced method to mitigate this issue. Our results not only showcase\npromising performance enhancements but also highlight a direction for future\nLLM advancements.",
        "pdf_link": "https://arxiv.org/pdf/2403.00126v1.pdf"
    },
    {
        "title": "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning",
        "authors": [
            "Long Qian",
            "Juncheng Li",
            "Yu Wu",
            "Yaobo Ye",
            "Hao Fei",
            "Tat-Seng Chua",
            "Yueting Zhuang",
            "Siliang Tang"
        ],
        "published": "2024-02-18T03:04:38Z",
        "summary": "Large Language Models (LLMs) demonstrate remarkable proficiency in\ncomprehending and handling text-based tasks. Many efforts are being made to\ntransfer these attributes to video modality, which are termed Video-LLMs.\nHowever, existing Video-LLMs can only capture the coarse-grained semantics and\nare unable to effectively handle tasks related to comprehension or localization\nof specific video segments. In light of these challenges, we propose Momentor,\na Video-LLM capable of accomplishing fine-grained temporal understanding tasks.\nTo support the training of Momentor, we design an automatic data generation\nengine to construct Moment-10M, a large-scale video instruction dataset with\nsegment-level instruction data. We train Momentor on Moment-10M, enabling it to\nperform segment-level reasoning and localization. Zero-shot evaluations on\nseveral tasks demonstrate that Momentor excels in fine-grained temporally\ngrounded comprehension and localization.",
        "pdf_link": "https://arxiv.org/pdf/2402.11435v1.pdf"
    },
    {
        "title": "Cognition is All You Need -- The Next Layer of AI Above Large Language Models",
        "authors": [
            "Nova Spivack",
            "Sam Douglas",
            "Michelle Crames",
            "Tim Connors"
        ],
        "published": "2024-03-04T16:11:57Z",
        "summary": "Recent studies of the applications of conversational AI tools, such as\nchatbots powered by large language models, to complex real-world knowledge work\nhave shown limitations related to reasoning and multi-step problem solving.\nSpecifically, while existing chatbots simulate shallow reasoning and\nunderstanding they are prone to errors as problem complexity increases. The\nfailure of these systems to address complex knowledge work is due to the fact\nthat they do not perform any actual cognition. In this position paper, we\npresent Cognitive AI, a higher-level framework for implementing\nprogrammatically defined neuro-symbolic cognition above and outside of large\nlanguage models. Specifically, we propose a dual-layer functional architecture\nfor Cognitive AI that serves as a roadmap for AI systems that can perform\ncomplex multi-step knowledge work. We propose that Cognitive AI is a necessary\nprecursor for the evolution of higher forms of AI, such as AGI, and\nspecifically claim that AGI cannot be achieved by probabilistic approaches on\ntheir own. We conclude with a discussion of the implications for large language\nmodels, adoption cycles in AI, and commercial Cognitive AI development.",
        "pdf_link": "https://arxiv.org/pdf/2403.02164v2.pdf"
    },
    {
        "title": "Entropy-Regularized Token-Level Policy Optimization for Large Language Models",
        "authors": [
            "Muning Wen",
            "Cheng Deng",
            "Jun Wang",
            "Weinan Zhang",
            "Ying Wen"
        ],
        "published": "2024-02-09T07:45:26Z",
        "summary": "Large Language Models (LLMs) have shown promise as intelligent agents in\ninteractive decision-making tasks. Traditional approaches often depend on\nmeticulously designed prompts, high-quality examples, or additional reward\nmodels for in-context learning, supervised fine-tuning, or RLHF. Reinforcement\nlearning (RL) presents a dynamic alternative for LLMs to overcome these\ndependencies by engaging directly with task-specific environments. Nonetheless,\nit faces significant hurdles: 1) instability stemming from the exponentially\nvast action space requiring exploration; 2) challenges in assigning token-level\ncredit based on action-level reward signals, resulting in discord between\nmaximizing rewards and accurately modeling corpus data. In response to these\nchallenges, we introduce Entropy-Regularized Token-level Policy Optimization\n(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the\ntoken level. At the heart of ETPO is our novel per-token soft Bellman update,\ndesigned to harmonize the RL process with the principles of language modeling.\nThis methodology decomposes the Q-function update from a coarse action-level\nview to a more granular token-level perspective, backed by theoretical proof of\noptimization consistency. Crucially, this decomposition renders linear time\ncomplexity in action exploration. We assess the effectiveness of ETPO within a\nsimulated environment that models data science code generation as a series of\nmulti-step interactive tasks; results show that ETPO achieves effective\nperformance improvement on the CodeLlama-7B model and surpasses a variant PPO\nbaseline inherited from RLHF. This underlines ETPO's potential as a robust\nmethod for refining the interactive decision-making capabilities of LLMs. Our\ncode is open-sourced at https://github.com/morning9393/ETPO.",
        "pdf_link": "https://arxiv.org/pdf/2402.06700v2.pdf"
    },
    {
        "title": "TrustLLM: Trustworthiness in Large Language Models",
        "authors": [
            "Lichao Sun",
            "Yue Huang",
            "Haoran Wang",
            "Siyuan Wu",
            "Qihui Zhang",
            "Yuan Li",
            "Chujie Gao",
            "Yixin Huang",
            "Wenhan Lyu",
            "Yixuan Zhang",
            "Xiner Li",
            "Zhengliang Liu",
            "Yixin Liu",
            "Yijue Wang",
            "Zhikun Zhang",
            "Bertie Vidgen",
            "Bhavya Kailkhura",
            "Caiming Xiong",
            "Chaowei Xiao",
            "Chunyuan Li",
            "Eric Xing",
            "Furong Huang",
            "Hao Liu",
            "Heng Ji",
            "Hongyi Wang",
            "Huan Zhang",
            "Huaxiu Yao",
            "Manolis Kellis",
            "Marinka Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Neil Zhenqiang Gong",
            "Philip S. Yu",
            "Pin-Yu Chen",
            "Quanquan Gu",
            "Ran Xu",
            "Rex Ying",
            "Shuiwang Ji",
            "Suman Jana",
            "Tianlong Chen",
            "Tianming Liu",
            "Tianyi Zhou",
            "William Wang",
            "Xiang Li",
            "Xiangliang Zhang",
            "Xiao Wang",
            "Xing Xie",
            "Xun Chen",
            "Xuyu Wang",
            "Yan Liu",
            "Yanfang Ye",
            "Yinzhi Cao",
            "Yong Chen",
            "Yue Zhao"
        ],
        "published": "2024-01-10T22:07:21Z",
        "summary": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2401.05561v4.pdf"
    },
    {
        "title": "InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models",
        "authors": [
            "Chao-Wei Huang",
            "Yun-Nung Chen"
        ],
        "published": "2024-03-25T05:31:22Z",
        "summary": "This paper introduces InstUPR, an unsupervised passage reranking method based\non large language models (LLMs). Different from existing approaches that rely\non extensive training with query-document pairs or retrieval-specific\ninstructions, our method leverages the instruction-following capabilities of\ninstruction-tuned LLMs for passage reranking without any additional\nfine-tuning. To achieve this, we introduce a soft score aggregation technique\nand employ pairwise reranking for unsupervised passage reranking. Experiments\non the BEIR benchmark demonstrate that InstUPR outperforms unsupervised\nbaselines as well as an instruction-tuned reranker, highlighting its\neffectiveness and superiority. Source code to reproduce all experiments is\nopen-sourced at https://github.com/MiuLab/InstUPR",
        "pdf_link": "https://arxiv.org/pdf/2403.16435v1.pdf"
    },
    {
        "title": "Question Aware Vision Transformer for Multimodal Reasoning",
        "authors": [
            "Roy Ganz",
            "Yair Kittenplon",
            "Aviad Aberdam",
            "Elad Ben Avraham",
            "Oren Nuriel",
            "Shai Mazor",
            "Ron Litman"
        ],
        "published": "2024-02-08T08:03:39Z",
        "summary": "Vision-Language (VL) models have gained significant research focus, enabling\nremarkable advances in multimodal reasoning. These architectures typically\ncomprise a vision encoder, a Large Language Model (LLM), and a projection\nmodule that aligns visual features with the LLM's representation space. Despite\ntheir success, a critical limitation persists: the vision encoding process\nremains decoupled from user queries, often in the form of image-related\nquestions. Consequently, the resulting visual features may not be optimally\nattuned to the query-specific elements of the image. To address this, we\nintroduce QA-ViT, a Question Aware Vision Transformer approach for multimodal\nreasoning, which embeds question awareness directly within the vision encoder.\nThis integration results in dynamic visual features focusing on relevant image\naspects to the posed question. QA-ViT is model-agnostic and can be incorporated\nefficiently into any VL architecture. Extensive experiments demonstrate the\neffectiveness of applying our method to various multimodal architectures,\nleading to consistent improvement across diverse tasks and showcasing its\npotential for enhancing visual and scene-text understanding.",
        "pdf_link": "https://arxiv.org/pdf/2402.05472v1.pdf"
    },
    {
        "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
        "authors": [
            "Mengjie Ren",
            "Boxi Cao",
            "Hongyu Lin",
            "Cao Liu",
            "Xianpei Han",
            "Ke Zeng",
            "Guanglu Wan",
            "Xunliang Cai",
            "Le Sun"
        ],
        "published": "2024-02-28T11:16:00Z",
        "summary": "Instruction Fine-tuning~(IFT) is a critical phase in building large language\nmodels~(LLMs). Previous works mainly focus on the IFT's role in the transfer of\nbehavioral norms and the learning of additional world knowledge. However, the\nunderstanding of the underlying mechanisms of IFT remains significantly\nlimited. In this paper, we design a knowledge intervention framework to\ndecouple the potential underlying factors of IFT, thereby enabling individual\nanalysis of different factors. Surprisingly, our experiments reveal that\nattempting to learn additional world knowledge through IFT often struggles to\nyield positive impacts and can even lead to markedly negative effects. Further,\nwe discover that maintaining internal knowledge consistency before and after\nIFT is a critical factor for achieving successful IFT. Our findings reveal the\nunderlying mechanisms of IFT and provide robust support for some very recent\nand potential future works.",
        "pdf_link": "https://arxiv.org/pdf/2402.18243v2.pdf"
    },
    {
        "title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models",
        "authors": [
            "Xuchen Pan",
            "Yanxi Chen",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "published": "2024-02-01T11:39:04Z",
        "summary": "This work introduces EE-Tuning, a lightweight and economical solution to\ntraining/tuning early-exit large language models (LLMs). In contrast to the\ncommon approach of full-parameter pre-training, EE-Tuning augments any\npre-trained (and possibly fine-tuned) standard LLM with additional early-exit\nlayers that are tuned in a parameter-efficient manner, which requires\nsignificantly less computational resources and training data. Our\nimplementation of EE-Tuning achieves outstanding training efficiency via\nextensive performance optimizations, as well as scalability due to its full\ncompatibility with 3D parallelism. Results of systematic experiments validate\nthe efficacy of EE-Tuning, confirming that effective early-exit LLM inference\ncan be achieved with a limited training budget. In hope of making early-exit\nLLMs accessible to the community, we release the source code of our\nimplementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.00518v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study",
        "authors": [
            "Matteo Esposito",
            "Francesco Palagiano"
        ],
        "published": "2024-03-23T07:59:30Z",
        "summary": "Preliminary security risk analysis (PSRA) provides a quick approach to\nidentify, evaluate and propose remeditation to potential risks in specific\nscenarios. The extensive expertise required for an effective PSRA and the\nsubstantial ammount of textual-related tasks hinder quick assessments in\nmission-critical contexts, where timely and prompt actions are essential. The\nspeed and accuracy of human experts in PSRA significantly impact response time.\nA large language model can quickly summarise information in less time than a\nhuman. To our knowledge, no prior study has explored the capabilities of\nfine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of\nFTM to assist practitioners in PSRA. We manually curated 141 representative\nsamples from over 50 mission-critical analyses archived by the industrial\ncontext team in the last five years.We compared the proficiency of the FTM\nversus seven human experts. Within the industrial context, our approach has\nproven successful in reducing errors in PSRA, hastening security risk\ndetection, and minimizing false positives and negatives. This translates to\ncost savings for the company by averting unnecessary expenses associated with\nimplementing unwarranted countermeasures. Therefore, experts can focus on more\ncomprehensive risk analysis, leveraging LLMs for an effective preliminary\nassessment within a condensed timeframe.",
        "pdf_link": "https://arxiv.org/pdf/2403.15756v1.pdf"
    },
    {
        "title": "Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM",
        "authors": [
            "Chunqiu Steven Xia",
            "Yinlin Deng",
            "Lingming Zhang"
        ],
        "published": "2024-03-28T03:10:39Z",
        "summary": "LLMs have become the go-to choice for code generation tasks, with an\nexponential increase in the training, development, and usage of LLMs\nspecifically for code generation. To evaluate the ability of LLMs on code, both\nacademic and industry practitioners rely on popular handcrafted benchmarks.\nHowever, prior benchmarks contain only a very limited set of problems, both in\nquantity and variety. Further, due to popularity and age, many benchmarks are\nprone to data leakage where example solutions can be readily found on the web\nand thus potentially in training data. Such limitations inevitably lead us to\ninquire: Is the leaderboard performance on existing benchmarks reliable and\ncomprehensive enough to measure the program synthesis ability of LLMs? To\naddress this, we introduce EvoEval -- a program synthesis benchmark suite\ncreated by evolving existing benchmarks into different targeted domains for a\ncomprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows\nthat compared to the high performance obtained on standard benchmarks like\nHumanEval, there is a significant drop in performance (on average 39.4%) when\nusing EvoEval. Additionally, the decrease in performance can range from 19.6%\nto 47.7%, leading to drastic ranking changes amongst LLMs and showing potential\noverfitting of existing benchmarks. Furthermore, we showcase various insights,\nincluding the brittleness of instruction-following models when encountering\nrewording or subtle changes as well as the importance of learning problem\ncomposition and decomposition. EvoEval not only provides comprehensive\nbenchmarks, but can be used to further evolve arbitrary problems to keep up\nwith advances and the ever-changing landscape of LLMs for code. We have\nopen-sourced our benchmarks, tools, and complete LLM generations at\nhttps://github.com/evo-eval/evoeval",
        "pdf_link": "https://arxiv.org/pdf/2403.19114v1.pdf"
    },
    {
        "title": "Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars",
        "authors": [
            "Daniel Melcer",
            "Nathan Fulton",
            "Sanjay Krishna Gouda",
            "Haifeng Qian"
        ],
        "published": "2024-02-28T02:12:47Z",
        "summary": "Large Language Models are powerful tools for program synthesis and advanced\nauto-completion, but come with no guarantee that their output code is\nsyntactically correct. This paper contributes an incremental parser that allows\nearly rejection of syntactically incorrect code, as well as efficient detection\nof complete programs for fill-in-the-middle (FItM) tasks. We develop\nEarley-style parsers that operate over left and right quotients of arbitrary\ncontext-free grammars, and we extend our incremental parsing and quotient\noperations to several context-sensitive features present in the grammars of\nmany common programming languages. The result of these contributions is an\nefficient, general, and well-grounded method for left and right quotient\nparsing.\n  To validate our theoretical contributions -- and the practical effectiveness\nof certain design decisions -- we evaluate our method on the particularly\ndifficult case of FItM completion for Python 3. Our results demonstrate that\nconstrained generation can significantly reduce the incidence of syntax errors\nin recommended code.",
        "pdf_link": "https://arxiv.org/pdf/2402.17988v1.pdf"
    },
    {
        "title": "ChatDBG: An AI-Powered Debugging Assistant",
        "authors": [
            "Kyla Levin",
            "Nicolas van Kempen",
            "Emery D. Berger",
            "Stephen N. Freund"
        ],
        "published": "2024-03-25T01:12:57Z",
        "summary": "This paper presents ChatDBG, the first AI-powered debugging assistant.\nChatDBG integrates large language models (LLMs) to significantly enhance the\ncapabilities and user-friendliness of conventional debuggers. ChatDBG lets\nprogrammers engage in a collaborative dialogue with the debugger, allowing them\nto pose complex questions about program state, perform root cause analysis for\ncrashes or assertion failures, and explore open-ended queries like \"why is x\nnull?\". To handle these queries, ChatDBG grants the LLM autonomy to take the\nwheel and drive debugging by issuing commands to navigate through stacks and\ninspect program state; it then reports its findings and yields back control to\nthe programmer. Our ChatDBG prototype integrates with standard debuggers\nincluding LLDB, GDB, and WinDBG for native code and Pdb for Python. Our\nevaluation across a diverse set of code, including C/C++ code with known bugs\nand a suite of Python code including standalone scripts and Jupyter notebooks,\ndemonstrates that ChatDBG can successfully analyze root causes, explain bugs,\nand generate accurate fixes for a wide range of real-world errors. For the\nPython programs, a single query led to an actionable bug fix 67% of the time;\none additional follow-up query increased the success rate to 85%. ChatDBG has\nseen rapid uptake; it has already been downloaded nearly 30,000 times.",
        "pdf_link": "https://arxiv.org/pdf/2403.16354v1.pdf"
    },
    {
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "authors": [
            "Ningyu Zhang",
            "Yunzhi Yao",
            "Bozhong Tian",
            "Peng Wang",
            "Shumin Deng",
            "Mengru Wang",
            "Zekun Xi",
            "Shengyu Mao",
            "Jintian Zhang",
            "Yuansheng Ni",
            "Siyuan Cheng",
            "Ziwen Xu",
            "Xin Xu",
            "Jia-Chen Gu",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Lei Liang",
            "Zhiqiang Zhang",
            "Xiaowei Zhu",
            "Jun Zhou",
            "Huajun Chen"
        ],
        "published": "2024-01-02T16:54:58Z",
        "summary": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.",
        "pdf_link": "https://arxiv.org/pdf/2401.01286v4.pdf"
    },
    {
        "title": "LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content",
        "authors": [
            "Qihao Zhao",
            "Yalun Dai",
            "Hao Li",
            "Wei Hu",
            "Fan Zhang",
            "Jun Liu"
        ],
        "published": "2024-03-09T09:52:15Z",
        "summary": "Long-tail recognition is challenging because it requires the model to learn\ngood representations from tail categories and address imbalances across all\ncategories. In this paper, we propose a novel generative and fine-tuning\nframework, LTGC, to handle long-tail recognition via leveraging generated\ncontent. Firstly, inspired by the rich implicit knowledge in large-scale models\n(e.g., large language models, LLMs), LTGC leverages the power of these models\nto parse and reason over the original tail data to produce diverse tail-class\ncontent. We then propose several novel designs for LTGC to ensure the quality\nof the generated data and to efficiently fine-tune the model using both the\ngenerated and original data. The visualization demonstrates the effectiveness\nof the generation module in LTGC, which produces accurate and diverse tail\ndata. Additionally, the experimental results demonstrate that our LTGC\noutperforms existing state-of-the-art methods on popular long-tailed\nbenchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05854v3.pdf"
    },
    {
        "title": "From Large to Tiny: Distilling and Refining Mathematical Expertise for Math Word Problems with Weakly Supervision",
        "authors": [
            "Qingwen Lin",
            "Boyan Xu",
            "Zhengting Huang",
            "Ruichu Cai"
        ],
        "published": "2024-03-21T13:29:54Z",
        "summary": "Addressing the challenge of high annotation costs in solving Math Word\nProblems (MWPs) through full supervision with intermediate equations, recent\nworks have proposed weakly supervised task settings that rely solely on the\nfinal answer as a supervised signal. Existing leading approaches typically\nemploy various search techniques to infer intermediate equations, but cannot\nensure their semantic consistency with natural language descriptions. The rise\nof Large Language Models (LLMs) like ChatGPT has opened up new possibilities\nfor addressing MWPs directly. However, the computational demands of LLMs make\nthem less than ideal for use in settings where resources are tight. In light of\nthese challenges, we introduce an innovative two-stage framework that adeptly\ntransfers mathematical Expertise from large to tiny language models. In\n\\emph{Distillation Stage}, we propose a series of extraction processes that\nsatisfy the properties of MWPs to distill mathematical knowledge from LLMs to\nconstruct problem-equation pairs required for supervised training. In\n\\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee\nthe full utilization of all data, we further utilize the unsuccessfully\nsearched data effectively by Knowledge Refine method. Finally, We train a small\nmodel using distilled data generated through two-stage methods. As our method\nfully leverages the semantic understanding capabilities during the searching\n'problem-equation' pair, it demonstrates significantly improved performance on\nthe Math23K and Weak12K datasets compared to existing small model methods,\nwhile maintaining a much lower computational cost than ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2403.14390v1.pdf"
    },
    {
        "title": "Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds",
        "authors": [
            "Jiageng WU",
            "Xian Wu",
            "Jie Yang"
        ],
        "published": "2024-03-11T10:53:20Z",
        "summary": "Clinical reasoning refers to the cognitive process that physicians employ in\nevaluating and managing patients. This process typically involves suggesting\nnecessary examinations, diagnosing patients' diseases, and deciding on\nappropriate therapies, etc. Accurate clinical reasoning requires extensive\nmedical knowledge and rich clinical experience, setting a high bar for\nphysicians. This is particularly challenging in developing countries due to the\noverwhelming number of patients and limited physician resources, contributing\nsignificantly to global health inequity and necessitating automated clinical\nreasoning approaches. Recently, the emergence of large language models (LLMs)\nsuch as ChatGPT and GPT-4 have demonstrated their potential in clinical\nreasoning. However, these LLMs are prone to hallucination problems, and the\nreasoning process of LLMs may not align with the clinical decision path of\nphysicians. In this study, we introduce a novel framework, In-Context Padding\n(ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer\ncritical clinical reasoning elements (referred to as knowledge seeds) and use\nthese as anchors to guide the generation process of LLMs. Experiments on two\nclinical question datasets demonstrate that ICP significantly improves the\nclinical reasoning ability of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.06609v1.pdf"
    },
    {
        "title": "TempCompass: Do Video LLMs Really Understand Videos?",
        "authors": [
            "Yuanxin Liu",
            "Shicheng Li",
            "Yi Liu",
            "Yuxiang Wang",
            "Shuhuai Ren",
            "Lei Li",
            "Sishuo Chen",
            "Xu Sun",
            "Lu Hou"
        ],
        "published": "2024-03-01T12:02:19Z",
        "summary": "Recently, there is a surge in interest surrounding video large language\nmodels (Video LLMs). However, existing benchmarks fail to provide a\ncomprehensive feedback on the temporal perception ability of Video LLMs. On the\none hand, most of them are unable to distinguish between different temporal\naspects (e.g., speed, direction) and thus cannot reflect the nuanced\nperformance on these specific aspects. On the other hand, they are limited in\nthe diversity of task formats (e.g., only multi-choice QA), which hinders the\nunderstanding of how temporal perception performance may vary across different\ntypes of tasks. Motivated by these two problems, we propose the\n\\textbf{TempCompass} benchmark, which introduces a diversity of temporal\naspects and task formats. To collect high-quality test data, we devise two\nnovel strategies: (1) In video collection, we construct conflicting videos that\nshare the same static content but differ in a specific temporal aspect, which\nprevents Video LLMs from leveraging single-frame bias or language priors. (2)\nTo collect the task instructions, we propose a paradigm where humans first\nannotate meta-information for a video and then an LLM generates the\ninstruction. We also design an LLM-based approach to automatically and\naccurately evaluate the responses from Video LLMs. Based on TempCompass, we\ncomprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs,\nand reveal the discerning fact that these models exhibit notably poor temporal\nperception ability. The data and evaluation code are available at\nhttps://github.com/llyx97/TempCompass.",
        "pdf_link": "https://arxiv.org/pdf/2403.00476v2.pdf"
    },
    {
        "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
        "authors": [
            "Yuejiang Liu",
            "Alexandre Alahi"
        ],
        "published": "2024-02-23T18:56:11Z",
        "summary": "Steering the behavior of a strong model pre-trained on internet-scale data\ncan be difficult due to the scarcity of competent supervisors. Recent studies\nreveal that, despite supervisory noises, a strong student model may surpass its\nweak teacher when fine-tuned on specific objectives. Yet, the effectiveness of\nsuch weak-to-strong generalization remains limited, especially in the presence\nof large capability gaps. In this paper, we propose to address this challenge\nby harnessing a diverse set of specialized teachers, instead of a single\ngeneralist one, that collectively supervises the strong student. Our approach\nresembles the classical hierarchical mixture of experts, with two components\ntailored for co-supervision: (i) we progressively alternate student training\nand teacher assignment, leveraging the growth of the strong student to identify\nplausible supervisions; (ii) we conservatively enforce teacher-student and\nlocal-global consistency, leveraging their dependencies to reject potential\nannotation noises. We validate the proposed method through visual recognition\ntasks on the OpenAI weak-to-strong benchmark and additional multi-domain\ndatasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.",
        "pdf_link": "https://arxiv.org/pdf/2402.15505v1.pdf"
    },
    {
        "title": "Single Word Change is All You Need: Designing Attacks and Defenses for Text Classifiers",
        "authors": [
            "Lei Xu",
            "Sarah Alnegheimish",
            "Laure Berti-Equille",
            "Alfredo Cuesta-Infante",
            "Kalyan Veeramachaneni"
        ],
        "published": "2024-01-30T17:30:44Z",
        "summary": "In text classification, creating an adversarial example means subtly\nperturbing a few words in a sentence without changing its meaning, causing it\nto be misclassified by a classifier. A concerning observation is that a\nsignificant portion of adversarial examples generated by existing methods\nchange only one word. This single-word perturbation vulnerability represents a\nsignificant weakness in classifiers, which malicious users can exploit to\nefficiently create a multitude of adversarial examples. This paper studies this\nproblem and makes the following key contributions: (1) We introduce a novel\nmetric \\r{ho} to quantitatively assess a classifier's robustness against\nsingle-word perturbation. (2) We present the SP-Attack, designed to exploit the\nsingle-word perturbation vulnerability, achieving a higher attack success rate,\nbetter preserving sentence meaning, while reducing computation costs compared\nto state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims\nto improve \\r{ho} by applying data augmentation in learning. Experimental\nresults on 4 datasets and BERT and distilBERT classifiers show that SP-Defense\nimproves \\r{ho} by 14.6% and 13.9% and decreases the attack success rate of\nSP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the\nattack success rate of existing attack methods that involve multiple-word\nperturbations.",
        "pdf_link": "https://arxiv.org/pdf/2401.17196v1.pdf"
    },
    {
        "title": "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
        "authors": [
            "Dawei Li",
            "Zhen Tan",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "published": "2024-01-28T08:56:49Z",
        "summary": "While textual information significantly enhances the performance of\npre-trained language models (PLMs) in knowledge graph completion (KGC), the\nstatic and noisy nature of existing corpora collected from Wikipedia articles\nor synsets definitions often limits the potential of PLM-based KGC models. To\nsurmount these challenges, we introduce the Contextualization Distillation\nstrategy, a versatile plug-in-and-play approach compatible with both\ndiscriminative and generative KGC frameworks. Our method begins by instructing\nlarge language models (LLMs) to transform compact, structural triplets into\ncontext-rich segments. Subsequently, we introduce two tailored auxiliary tasks,\nreconstruction and contextualization, allowing smaller KGC models to assimilate\ninsights from these enriched triplets. Comprehensive evaluations across diverse\ndatasets and KGC techniques highlight the efficacy and adaptability of our\napproach, revealing consistent performance enhancements irrespective of\nunderlying pipelines or architectures. Moreover, our analysis makes our method\nmore explainable and provides insight into generating path selection, as well\nas the choosing of suitable distillation tasks. All the code and data in this\nwork will be released at\nhttps://github.com/David-Li0406/Contextulization-Distillation",
        "pdf_link": "https://arxiv.org/pdf/2402.01729v3.pdf"
    },
    {
        "title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space",
        "authors": [
            "Zongru Wu",
            "Zhuosheng Zhang",
            "Pengzhou Cheng",
            "Gongshen Liu"
        ],
        "published": "2024-02-19T10:34:48Z",
        "summary": "Despite the notable success of language models (LMs) in various natural\nlanguage processing (NLP) tasks, the reliability of LMs is susceptible to\nbackdoor attacks. Prior research attempts to mitigate backdoor learning while\ntraining the LMs on the poisoned dataset, yet struggles against complex\nbackdoor attacks in real-world scenarios. In this paper, we investigate the\nlearning mechanisms of backdoor LMs in the frequency space by Fourier analysis.\nOur findings indicate that the backdoor mapping presented on the poisoned\ndatasets exhibits a more discernible inclination towards lower frequency\ncompared to clean mapping, resulting in the faster convergence of backdoor\nmapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation\n(MuScleLoRA), which deploys multiple radial scalings in the frequency space\nwith low-rank adaptation to the target model and further aligns the gradients\nwhen updating parameters. Through downscaling in the frequency space,\nMuScleLoRA encourages the model to prioritize the learning of relatively\nhigh-frequency clean mapping, consequently mitigating backdoor learning.\nExperimental results demonstrate that MuScleLoRA outperforms baselines\nsignificantly. Notably, MuScleLoRA reduces the average success rate of diverse\nbackdoor attacks to below 15\\% across multiple datasets and generalizes to\nvarious backbone LMs, including BERT, RoBERTa, and Llama2. The codes are\navailable at https://github.com/ZrW00/MuScleLoRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.12026v2.pdf"
    },
    {
        "title": "IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus",
        "authors": [
            "Honghao Gui",
            "Lin Yuan",
            "Hongbin Ye",
            "Ningyu Zhang",
            "Mengshu Sun",
            "Lei Liang",
            "Huajun Chen"
        ],
        "published": "2024-02-22T17:11:38Z",
        "summary": "Large Language Models (LLMs) demonstrate remarkable potential across various\ndomains; however, they exhibit a significant performance gap in Information\nExtraction (IE). Note that high-quality instruction data is the vital key for\nenhancing the specific capabilities of LLMs, while current IE datasets tend to\nbe small in scale, fragmented, and lack standardized schema. To this end, we\nintroduce IEPile, a comprehensive bilingual (English and Chinese) IE\ninstruction corpus, which contains approximately 0.32B tokens. We construct\nIEPile by collecting and cleaning 33 existing IE datasets, and introduce\nschema-based instruction generation to unearth a large-scale corpus.\nExperimental results on LLaMA, Baichuan and Qwen demonstrate that using IEPile\ncan enhance the performance of LLMs for IE, especially the zero-shot\ngeneralization. We open-source the resource and pre-trained models, hoping to\nprovide valuable support to the NLP community.",
        "pdf_link": "https://arxiv.org/pdf/2402.14710v2.pdf"
    },
    {
        "title": "Stable Knowledge Editing in Large Language Models",
        "authors": [
            "Zihao Wei",
            "Liang Pang",
            "Hanxing Ding",
            "Jingcheng Deng",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-20T14:36:23Z",
        "summary": "Efficient knowledge editing of large language models is crucial for replacing\nobsolete information or incorporating specialized knowledge on a large scale.\nHowever, previous methods implicitly assume that knowledge is localized and\nisolated within the model, an assumption that oversimplifies the interconnected\nnature of model knowledge. The premise of localization results in an incomplete\nknowledge editing, whereas an isolated assumption may impair both other\nknowledge and general abilities. It introduces instability to the performance\nof the knowledge editing method. To transcend these assumptions, we introduce\nStableKE, a method adopts a novel perspective based on knowledge augmentation\nrather than knowledge localization. To overcome the expense of human labeling,\nStableKE integrates two automated knowledge augmentation strategies: Semantic\nParaphrase Enhancement strategy, which diversifies knowledge descriptions to\nfacilitate the teaching of new information to the model, and Contextual\nDescription Enrichment strategy, expanding the surrounding knowledge to prevent\nthe forgetting of related information. StableKE surpasses other knowledge\nediting methods, demonstrating stability both edited knowledge and multi-hop\nknowledge, while also preserving unrelated knowledge and general abilities.\nMoreover, StableKE can edit knowledge on ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.13048v1.pdf"
    },
    {
        "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering",
        "authors": [
            "Tianlong Li",
            "Shihan Dou",
            "Wenhao Liu",
            "Muling Wu",
            "Changze Lv",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published": "2024-01-12T00:50:04Z",
        "summary": "Jailbreaking techniques aim to probe the boundaries of safety in large\nlanguage models (LLMs) by inducing them to generate toxic responses to\nmalicious queries, a significant concern within the LLM community. While\nexisting jailbreaking methods primarily rely on prompt engineering, altering\ninputs to evade LLM safety mechanisms, they suffer from low attack success\nrates and significant time overheads, rendering them inflexible. To overcome\nthese limitations, we propose a novel jailbreaking approach, named Jailbreaking\nLLMs through Representation Engineering (JRE). Our method requires only a small\nnumber of query pairs to extract ``safety patterns'' that can be used to\ncircumvent the target model's defenses, achieving unprecedented jailbreaking\nperformance. Building upon these findings, we also introduce a novel defense\nframework inspired by JRE principles, which demonstrates notable effectiveness.\nExtensive experimentation confirms the superior performance of the JRE attacks\nand the robustness of the JRE defense framework. We hope this study contributes\nto advancing the understanding of model safety issues through the lens of\nrepresentation engineering.",
        "pdf_link": "https://arxiv.org/pdf/2401.06824v2.pdf"
    },
    {
        "title": "Reliable LLM-based User Simulator for Task-Oriented Dialogue Systems",
        "authors": [
            "Ivan Sekuli\u0107",
            "Silvia Terragni",
            "Victor Guimar\u00e3es",
            "Nghia Khau",
            "Bruna Guedes",
            "Modestas Filipavicius",
            "Andr\u00e9 Ferreira Manso",
            "Roland Mathis"
        ],
        "published": "2024-02-20T20:57:47Z",
        "summary": "In the realm of dialogue systems, user simulation techniques have emerged as\na game-changer, redefining the evaluation and enhancement of task-oriented\ndialogue (TOD) systems. These methods are crucial for replicating real user\ninteractions, enabling applications like synthetic data augmentation, error\ndetection, and robust evaluation. However, existing approaches often rely on\nrigid rule-based methods or on annotated data. This paper introduces DAUS, a\nDomain-Aware User Simulator. Leveraging large language models, we fine-tune\nDAUS on real examples of task-oriented dialogues. Results on two relevant\nbenchmarks showcase significant improvements in terms of user goal fulfillment.\nNotably, we have observed that fine-tuning enhances the simulator's coherence\nwith user goals, effectively mitigating hallucinations -- a major source of\ninconsistencies in simulator responses.",
        "pdf_link": "https://arxiv.org/pdf/2402.13374v1.pdf"
    },
    {
        "title": "Disentangling Length from Quality in Direct Preference Optimization",
        "authors": [
            "Ryan Park",
            "Rafael Rafailov",
            "Stefano Ermon",
            "Chelsea Finn"
        ],
        "published": "2024-03-28T06:03:47Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) has been a crucial\ncomponent in the recent success of Large Language Models. However, RLHF is know\nto exploit biases in human preferences, such as verbosity. A well-formatted and\neloquent answer is often more highly rated by users, even when it is less\nhelpful and objective. A number of approaches have been developed to control\nthose biases in the classical RLHF literature, but the problem remains\nrelatively under-explored for Direct Alignment Algorithms such as Direct\nPreference Optimization (DPO). Unlike classical RLHF, DPO does not train a\nseparate reward model or use reinforcement learning directly, so previous\napproaches developed to control verbosity cannot be directly applied to this\nsetting. Our work makes several contributions. For the first time, we study the\nlength problem in the DPO setting, showing significant exploitation in DPO and\nlinking it to out-of-distribution bootstrapping. We then develop a principled\nbut simple regularization strategy that prevents length exploitation, while\nstill maintaining improvements in model quality. We demonstrate these effects\nacross datasets on summarization and dialogue, where we achieve up to 20\\%\nimprovement in win rates when controlling for length, despite the GPT4 judge's\nwell-known verbosity bias.",
        "pdf_link": "https://arxiv.org/pdf/2403.19159v1.pdf"
    },
    {
        "title": "On Zero-Shot Counterspeech Generation by LLMs",
        "authors": [
            "Punyajoy Saha",
            "Aalok Agrawal",
            "Abhik Jana",
            "Chris Biemann",
            "Animesh Mukherjee"
        ],
        "published": "2024-03-22T04:13:10Z",
        "summary": "With the emergence of numerous Large Language Models (LLM), the usage of such\nmodels in various Natural Language Processing (NLP) applications is increasing\nextensively. Counterspeech generation is one such key task where efforts are\nmade to develop generative models by fine-tuning LLMs with hatespeech -\ncounterspeech pairs, but none of these attempts explores the intrinsic\nproperties of large language models in zero-shot settings. In this work, we\npresent a comprehensive analysis of the performances of four LLMs namely GPT-2,\nDialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech\ngeneration, which is the first of its kind. For GPT-2 and DialoGPT, we further\ninvestigate the deviation in performance with respect to the sizes (small,\nmedium, large) of the models. On the other hand, we propose three different\nprompting strategies for generating different types of counterspeech and\nanalyse the impact of such strategies on the performance of the models. Our\nanalysis shows that there is an improvement in generation quality for two\ndatasets (17%), however the toxicity increase (25%) with increase in model\nsize. Considering type of model, GPT-2 and FlanT5 models are significantly\nbetter in terms of counterspeech quality but also have high toxicity as\ncompared to DialoGPT. ChatGPT are much better at generating counter speech than\nother models across all metrics. In terms of prompting, we find that our\nproposed strategies help in improving counter speech generation across all the\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2403.14938v1.pdf"
    },
    {
        "title": "Secret Collusion Among Generative AI Agents",
        "authors": [
            "Sumeet Ramesh Motwani",
            "Mikhail Baranchuk",
            "Martin Strohmeier",
            "Vijay Bolina",
            "Philip H. S. Torr",
            "Lewis Hammond",
            "Christian Schroeder de Witt"
        ],
        "published": "2024-02-12T09:31:21Z",
        "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which teams of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both the AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.",
        "pdf_link": "https://arxiv.org/pdf/2402.07510v1.pdf"
    },
    {
        "title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal",
        "authors": [
            "Jianheng Huang",
            "Leyang Cui",
            "Ante Wang",
            "Chengyi Yang",
            "Xinting Liao",
            "Linfeng Song",
            "Junfeng Yao",
            "Jinsong Su"
        ],
        "published": "2024-03-02T16:11:23Z",
        "summary": "Large language models (LLMs) suffer from catastrophic forgetting during\ncontinual learning. Conventional rehearsal-based methods rely on previous\ntraining data to retain the model's ability, which may not be feasible in\nreal-world applications. When conducting continual learning based on a\npublicly-released LLM checkpoint, the availability of the original training\ndata may be non-existent. To address this challenge, we propose a framework\ncalled Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic\ninstances for rehearsal. Concretely, we first employ the base LLM for\nin-context learning to generate synthetic instances. Subsequently, we utilize\nthe latest LLM to refine the instance outputs based on the synthetic inputs,\npreserving its acquired ability. Finally, we select diverse high-quality\nsynthetic instances for rehearsal in future stages. Experimental results\ndemonstrate that SSR achieves superior or comparable performance compared to\nconventional rehearsal-based approaches while being more data-efficient.\nBesides, SSR effectively preserves the generalization capabilities of LLMs in\ngeneral domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.01244v1.pdf"
    },
    {
        "title": "Uni-SMART: Universal Science Multimodal Analysis and Research Transformer",
        "authors": [
            "Hengxing Cai",
            "Xiaochen Cai",
            "Shuwen Yang",
            "Jiankun Wang",
            "Lin Yao",
            "Zhifeng Gao",
            "Junhan Chang",
            "Sihang Li",
            "Mingjun Xu",
            "Changxin Wang",
            "Hongshuai Wang",
            "Yongge Li",
            "Mujie Lin",
            "Yaqi Li",
            "Yuqi Yin",
            "Linfeng Zhang",
            "Guolin Ke"
        ],
        "published": "2024-03-15T13:43:47Z",
        "summary": "In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as molecular\nstructure, tables, and charts, which are hard for text-focused LLMs to\nunderstand and analyze. This issue points to the urgent need for new solutions\nthat can fully understand and analyze multimodal content in scientific\nliterature. To answer this demand, we present Uni-SMART (Universal Science\nMultimodal Analysis and Research Transformer), an innovative model designed for\nin-depth understanding of multimodal scientific literature. Through rigorous\nquantitative evaluation across several domains, Uni-SMART demonstrates superior\nperformance over leading text-focused LLMs. Furthermore, our exploration\nextends to practical applications, including patent infringement detection and\nnuanced analysis of charts. These applications not only highlight Uni-SMART's\nadaptability but also its potential to revolutionize how we interact with\nscientific literature.",
        "pdf_link": "https://arxiv.org/pdf/2403.10301v1.pdf"
    },
    {
        "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning",
        "authors": [
            "Wenhan Xia",
            "Chengwei Qin",
            "Elad Hazan"
        ],
        "published": "2024-01-08T14:26:49Z",
        "summary": "Fine-tuning is the primary methodology for tailoring pre-trained large\nlanguage models to specific tasks. As the model's scale and the diversity of\ntasks expand, parameter-efficient fine-tuning methods are of paramount\nimportance. One of the most widely used family of methods is low-rank\nadaptation (LoRA) and its variants. LoRA encodes weight update as the product\nof two low-rank matrices. Despite its advantages, LoRA falls short of\nfull-parameter fine-tuning in terms of generalization error for certain tasks.\n  We introduce Chain of LoRA (COLA), an iterative optimization framework\ninspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full\nparameter fine-tuning, without incurring additional computational costs or\nmemory overheads. COLA employs a residual learning procedure where it merges\nlearned LoRA modules into the pre-trained language model parameters and\nre-initilize optimization for new born LoRA modules. We provide theoretical\nconvergence guarantees as well as empirical results to validate the\neffectiveness of our algorithm. Across various models (OPT and llama-2) and\nseven benchmarking tasks, we demonstrate that COLA can consistently outperform\nLoRA without additional computational or memory costs.",
        "pdf_link": "https://arxiv.org/pdf/2401.04151v1.pdf"
    },
    {
        "title": "CliqueParcel: An Approach For Batching LLM Prompts That Jointly Optimizes Efficiency And Faithfulness",
        "authors": [
            "Jiayi Liu",
            "Tinghan Yang",
            "Jennifer Neville"
        ],
        "published": "2024-02-17T22:37:17Z",
        "summary": "Large language models (LLMs) have become pivotal in recent research. However,\nduring the inference process, LLMs still require substantial resources. In this\npaper, we propose CliqueParcel, a method designed to improve the efficiency of\nLLMs via prompt batching. Existing strategies to optimize inference efficiency\noften compromise on output quality, leading to a discounted output problem.\nThis issue might result in reduced accuracy or outputs that are less detailed.\nCliqueParcel is our answer to this challenge. While ensuring accuracy and\nminimizing deviations from the original outputs (i.e., faithfulness), our\nmethod significantly improves efficiency during inference.\n  To lay the groundwork, we first redefine efficiency measurements by excluding\nthe reduction in running time due to shorter lengths. Then, we provide a\ncomprehensive trade-off between efficiency and faithfulness to clarify the\nnature of the 'discounted output' problem. Within the CliqueParcel framework,\nwe suggest multiple batching sub-methods and discuss the specific scenarios in\nwhich they can be applied. During evaluation, CliqueParcel is tested on eight\nwidely recognized datasets, which can be classified into three types: reading\ncomprehension, open-source question-answering, and reasoning. Our experiments\nexplore the performance of CliqueParcel, including efficiency, faithfulness,\nand the trade-off between them. This work provides novel insights into\ninference efficiency and demonstrates promising performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.14833v1.pdf"
    },
    {
        "title": "A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models",
        "authors": [
            "Rui Ma",
            "Qiang Zhou",
            "Bangjun Xiao",
            "Yizhu Jin",
            "Daquan Zhou",
            "Xiuyu Li",
            "Aishani Singh",
            "Yi Qu",
            "Kurt Keutzer",
            "Xiaodong Xie",
            "Jingtong Hu",
            "Zhen Dong",
            "Shanghang Zhang"
        ],
        "published": "2024-01-04T11:14:01Z",
        "summary": "Copyright is a legal right that grants creators the exclusive authority to\nreproduce, distribute, and profit from their creative works. However, the\nrecent advancements in text-to-image generation techniques have posed\nsignificant challenges to copyright protection, as these methods have\nfacilitated the learning of unauthorized content, artistic creations, and\nportraits, which are subsequently utilized to generate and disseminate\nuncontrolled content. Especially, the use of stable diffusion, an emerging\nmodel for text-to-image generation, poses an increased risk of unauthorized\ncopyright infringement and distribution. Currently, there is a lack of\nsystematic studies evaluating the potential correlation between content\ngenerated by stable diffusion and those under copyright protection. Conducting\nsuch studies faces several challenges, including i) the intrinsic ambiguity\nrelated to copyright infringement in text-to-image models, ii) the absence of a\ncomprehensive large-scale dataset, and iii) the lack of standardized metrics\nfor defining copyright infringement. This work provides the first large-scale\nstandardized dataset and benchmark on copyright protection. Specifically, we\npropose a pipeline to coordinate CLIP, ChatGPT, and diffusion models to\ngenerate a dataset that contains anchor images, corresponding prompts, and\nimages generated by text-to-image models, reflecting the potential abuses of\ncopyright. Furthermore, we explore a suite of evaluation metrics to judge the\neffectiveness of copyright protection methods. The proposed dataset, benchmark\nlibrary, and evaluation metrics will be open-sourced to facilitate future\nresearch and application. The website and dataset can be accessed website\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.12052v2.pdf"
    },
    {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models",
        "authors": [
            "Xiang Chen",
            "Chenxi Wang",
            "Yida Xue",
            "Ningyu Zhang",
            "Xiaoyan Yang",
            "Qiang Li",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "published": "2024-02-05T16:56:11Z",
        "summary": "Despite significant strides in multimodal tasks, Multimodal Large Language\nModels (MLLMs) are plagued by the critical issue of hallucination. The reliable\ndetection of such hallucinations in MLLMs has, therefore, become a vital aspect\nof model evaluation and the safeguarding of practical application deployment.\nPrior research in this domain has been constrained by a narrow focus on\nsingular tasks, an inadequate range of hallucination categories addressed, and\na lack of detailed granularity. In response to these challenges, our work\nexpands the investigative horizons of hallucination detection. We present a\nnovel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate\nthe evaluation of advancements in hallucination detection methods.\nAdditionally, we unveil a novel unified multimodal hallucination detection\nframework, UNIHD, which leverages a suite of auxiliary tools to validate the\noccurrence of hallucinations robustly. We demonstrate the effectiveness of\nUNIHD through meticulous evaluation and comprehensive analysis. We also provide\nstrategic insights on the application of specific tools for addressing various\ncategories of hallucinations.",
        "pdf_link": "https://arxiv.org/pdf/2402.03190v3.pdf"
    },
    {
        "title": "Few-Shot Detection of Machine-Generated Text using Style Representations",
        "authors": [
            "Rafael Rivera Soto",
            "Kailin Koch",
            "Aleem Khan",
            "Barry Chen",
            "Marcus Bishop",
            "Nicholas Andrews"
        ],
        "published": "2024-01-12T17:26:51Z",
        "summary": "The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.",
        "pdf_link": "https://arxiv.org/pdf/2401.06712v2.pdf"
    },
    {
        "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
        "authors": [
            "Jiaxing Chen",
            "Yuxuan Liu",
            "Dehu Li",
            "Xiang An",
            "Ziyong Feng",
            "Yongle Zhao",
            "Yin Xie"
        ],
        "published": "2024-03-28T11:26:30Z",
        "summary": "The surge of Multimodal Large Language Models (MLLMs), given their prominent\nemergent capabilities in instruction following and reasoning, has greatly\nadvanced the field of visual reasoning. However, constrained by their\nnon-lossless image tokenization, most MLLMs fall short of comprehensively\ncapturing details of text and objects, especially in high-resolution images. To\naddress this, we propose P2G, a novel framework for plug-and-play grounding of\nreasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of\nMLLMs to employ expert agents to achieve on-the-fly grounding to critical\nvisual and textual objects of image, thus achieving deliberate reasoning via\nmultimodal prompting. We further create P2GB, a benchmark aimed at assessing\nMLLMs' ability to understand inter-object relationships and text in challenging\nhigh-resolution images. Comprehensive experiments on visual reasoning tasks\ndemonstrate the superiority of P2G. Noteworthy, P2G achieved comparable\nperformance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the\npotential of plug-and-play grounding of reasoning and opens up a promising\nalternative beyond model scaling.",
        "pdf_link": "https://arxiv.org/pdf/2403.19322v1.pdf"
    },
    {
        "title": "Steering Conversational Large Language Models for Long Emotional Support Conversations",
        "authors": [
            "Navid Madani",
            "Sougata Saha",
            "Rohini Srihari"
        ],
        "published": "2024-02-16T05:03:01Z",
        "summary": "In this study, we address the challenge of consistently following emotional\nsupport strategies in long conversations by large language models (LLMs). We\nintroduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic\nmeasure designed to evaluate the effectiveness of LLMs in adhering to strategic\nprompts in emotional support contexts. By analyzing conversations within the\nEmotional Support Conversations dataset (ESConv) using LLaMA models, we\ndemonstrate that SRA is significantly correlated with a model's ability to\nsustain the outlined strategy throughout the interactions. Our findings reveal\nthat the application of SRA-informed prompts leads to enhanced strategic\nadherence, resulting in conversations that more reliably exhibit the desired\nemotional support strategies over longer conversations. Furthermore, we\ncontribute a comprehensive, multi-branch synthetic conversation dataset for\nESConv, featuring a variety of strategy continuations informed by our optimized\nprompting method. The code and data are publicly available on our Github.",
        "pdf_link": "https://arxiv.org/pdf/2402.10453v1.pdf"
    },
    {
        "title": "Enhancing ICU Patient Recovery: Using LLMs to Assist Nurses in Diary Writing",
        "authors": [
            "Samuel Kernan Freire",
            "Margo MC van Mol",
            "Carola Schol",
            "Elif \u00d6zcan Vieira"
        ],
        "published": "2024-02-23T09:06:25Z",
        "summary": "Intensive care unit (ICU) patients often develop new health-related problems\nin their long-term recovery. Health care professionals keeping a diary of a\npatient's stay is a proven strategy to tackle this but faces several adoption\nbarriers, such as lack of time and difficulty in knowing what to write. Large\nlanguage models (LLMs), with their ability to generate human-like text and\nadaptability, could solve these challenges. However, realizing this vision\ninvolves addressing several socio-technical and practical research challenges.\nThis paper discusses these challenges and proposes future research directions\nto utilize the potential of LLMs in ICU diary writing, ultimately improving the\nlong-term recovery outcomes for ICU patients.",
        "pdf_link": "https://arxiv.org/pdf/2402.15205v1.pdf"
    },
    {
        "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
        "authors": [
            "Anku Rani",
            "Vipula Rawte",
            "Harshad Sharma",
            "Neeraj Anand",
            "Krishnav Rajbangshi",
            "Amit Sheth",
            "Amitava Das"
        ],
        "published": "2024-03-26T01:28:42Z",
        "summary": "The troubling rise of hallucination presents perhaps the most significant\nimpediment to the advancement of responsible AI. In recent times, considerable\nresearch has focused on detecting and mitigating hallucination in Large\nLanguage Models (LLMs). However, it's worth noting that hallucination is also\nquite prevalent in Vision-Language models (VLMs). In this paper, we offer a\nfine-grained discourse on profiling VLM hallucination based on two tasks: i)\nimage captioning, and ii) Visual Question Answering (VQA). We delineate eight\nfine-grained orientations of visual hallucination: i) Contextual Guessing, ii)\nIdentity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender\nAnomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric\nDiscrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly\navailable dataset comprising 2,000 samples generated using eight VLMs across\ntwo tasks of captioning and VQA along with human annotations for the categories\nas mentioned earlier.",
        "pdf_link": "https://arxiv.org/pdf/2403.17306v2.pdf"
    },
    {
        "title": "Assessing AI Detectors in Identifying AI-Generated Code: Implications for Education",
        "authors": [
            "Wei Hung Pan",
            "Ming Jie Chok",
            "Jonathan Leong Shan Wong",
            "Yung Xin Shin",
            "Yeong Shian Poon",
            "Zhou Yang",
            "Chun Yong Chong",
            "David Lo",
            "Mei Kuan Lim"
        ],
        "published": "2024-01-08T05:53:52Z",
        "summary": "Educators are increasingly concerned about the usage of Large Language Models\n(LLMs) such as ChatGPT in programming education, particularly regarding the\npotential exploitation of imperfections in Artificial Intelligence Generated\nContent (AIGC) Detectors for academic misconduct. In this paper, we present an\nempirical study where the LLM is examined for its attempts to bypass detection\nby AIGC Detectors. This is achieved by generating code in response to a given\nquestion using different variants. We collected a dataset comprising 5,069\nsamples, with each sample consisting of a textual description of a coding\nproblem and its corresponding human-written Python solution codes. These\nsamples were obtained from various sources, including 80 from Quescol, 3,264\nfrom Kaggle, and 1,725 from LeetCode. From the dataset, we created 13 sets of\ncode problem variant prompts, which were used to instruct ChatGPT to generate\nthe outputs. Subsequently, we assessed the performance of five AIGC detectors.\nOur results demonstrate that existing AIGC Detectors perform poorly in\ndistinguishing between human-written code and AI-generated code.",
        "pdf_link": "https://arxiv.org/pdf/2401.03676v1.pdf"
    },
    {
        "title": "Identifying Multiple Personalities in Large Language Models with External Evaluation",
        "authors": [
            "Xiaoyang Song",
            "Yuta Adachi",
            "Jessie Feng",
            "Mouwei Lin",
            "Linhao Yu",
            "Frank Li",
            "Akshat Gupta",
            "Gopala Anumanchipalli",
            "Simerjot Kaur"
        ],
        "published": "2024-02-22T18:57:20Z",
        "summary": "As Large Language Models (LLMs) are integrated with human daily applications\nrapidly, many societal and ethical concerns are raised regarding the behavior\nof LLMs. One of the ways to comprehend LLMs' behavior is to analyze their\npersonalities. Many recent studies quantify LLMs' personalities using\nself-assessment tests that are created for humans. Yet many critiques question\nthe applicability and reliability of these self-assessment tests when applied\nto LLMs. In this paper, we investigate LLM personalities using an alternate\npersonality measurement method, which we refer to as the external evaluation\nmethod, where instead of prompting LLMs with multiple-choice questions in the\nLikert scale, we evaluate LLMs' personalities by analyzing their responses\ntoward open-ended situational questions using an external machine learning\nmodel. We first fine-tuned a Llama2-7B model as the MBTI personality predictor\nthat outperforms the state-of-the-art models as the tool to analyze LLMs'\nresponses. Then, we prompt the LLMs with situational questions and ask them to\ngenerate Twitter posts and comments, respectively, in order to assess their\npersonalities when playing two different roles. Using the external personality\nevaluation method, we identify that the obtained personality types for LLMs are\nsignificantly different when generating posts versus comments, whereas humans\nshow a consistent personality profile in these two different situations. This\nshows that LLMs can exhibit different personalities based on different\nscenarios, thus highlighting a fundamental difference between personality in\nLLMs and humans. With our work, we call for a re-evaluation of personality\ndefinition and measurement in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14805v1.pdf"
    },
    {
        "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
        "authors": [
            "Yijun Yang",
            "Ruiyuan Gao",
            "Xiao Yang",
            "Jianyuan Zhong",
            "Qiang Xu"
        ],
        "published": "2024-03-03T09:04:34Z",
        "summary": "Recent advancements in Text-to-Image (T2I) models have raised significant\nsafety concerns about their potential misuse for generating inappropriate or\nNot-Safe-For-Work (NSFW) contents, despite existing countermeasures such as\nNSFW classifiers or model fine-tuning for inappropriate concept removal.\nAddressing this challenge, our study unveils GuardT2I, a novel moderation\nframework that adopts a generative approach to enhance T2I models' robustness\nagainst adversarial prompts. Instead of making a binary classification,\nGuardT2I utilizes a Large Language Model (LLM) to conditionally transform text\nguidance embeddings within the T2I models into natural language for effective\nadversarial prompt detection, without compromising the models' inherent\nperformance. Our extensive experiments reveal that GuardT2I outperforms leading\ncommercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a\nsignificant margin across diverse adversarial scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.01446v1.pdf"
    },
    {
        "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
        "authors": [
            "Abhay Zala",
            "Jaemin Cho",
            "Han Lin",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "published": "2024-03-18T17:51:16Z",
        "summary": "Recent SOTA approaches for embodied learning via interaction directly employ\nlarge language models (LLMs) as agents to determine the next steps in an\nenvironment. Due to their world knowledge and reasoning capabilities, LLM\nagents achieve stronger performance than previous smaller agents based on\nreinforcement learning (RL); however, frequently calling LLMs is slow and\nexpensive. Instead of directly employing LLMs as agents, can we use LLMs'\nreasoning capabilities to adaptively create training environments to help\nsmaller embodied RL agents learn useful skills that they are weak at? We\npropose EnvGen, a novel framework to address this question. First, we prompt an\nLLM to generate training environments that allow agents to quickly learn\ndifferent tasks in parallel. Concretely, the LLM is given the task description\nand simulator objectives that the agents should learn and is then asked to\ngenerate a set of environment configurations (e.g., different terrains, items\ngiven to agents, etc.). Next, we train a small RL agent in a mixture of the\noriginal and LLM-generated environments. Then, we enable the LLM to\ncontinuously adapt the generated environments to progressively improve the\nskills that the agent is weak at, by providing feedback to the LLM in the form\nof the agent's performance. We demonstrate the usefulness of EnvGen with\ncomprehensive experiments in Crafter and Heist environments. We find that a\nsmall RL agent trained with EnvGen can outperform SOTA methods, including a\nGPT-4 agent, and learns long-horizon tasks significantly faster. We show\nqualitatively how the LLM adapts training environments to help improve RL\nagents' weaker skills over time. Additionally, EnvGen is substantially more\nefficient as it only uses a small number of LLM calls (e.g., 4 in total),\nwhereas LLM agents require thousands of LLM calls. Lastly, we present detailed\nablation studies for our design choices.",
        "pdf_link": "https://arxiv.org/pdf/2403.12014v1.pdf"
    },
    {
        "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
        "authors": [
            "Zhiyang Xu",
            "Chao Feng",
            "Rulin Shao",
            "Trevor Ashby",
            "Ying Shen",
            "Di Jin",
            "Yu Cheng",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "published": "2024-02-18T19:38:44Z",
        "summary": "Despite vision-language models' (VLMs) remarkable capabilities as versatile\nvisual assistants, two substantial challenges persist within the existing VLM\nframeworks: (1) lacking task diversity in pretraining and visual instruction\ntuning, and (2) annotation error and bias in GPT-4 synthesized instruction\ntuning data. Both challenges lead to issues such as poor generalizability,\nhallucination, and catastrophic forgetting. To address these challenges, we\nconstruct Vision-Flan, the most diverse publicly available visual instruction\ntuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances\nsourced from academic datasets, and each task is accompanied by an\nexpert-written instruction. In addition, we propose a two-stage instruction\ntuning framework, in which VLMs are firstly finetuned on Vision-Flan and\nfurther tuned on GPT-4 synthesized data. We find this two-stage tuning\nframework significantly outperforms the traditional single-stage visual\ninstruction tuning framework and achieves the state-of-the-art performance\nacross a wide range of multi-modal evaluation benchmarks. Finally, we conduct\nin-depth analyses to understand visual instruction tuning and our findings\nreveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs'\ncapabilities but rather modulates the model's responses to human-preferred\nformats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can\neffectively align VLM responses with human-preference; (3) Visual instruction\ntuning mainly helps large-language models (LLMs) to understand visual features.",
        "pdf_link": "https://arxiv.org/pdf/2402.11690v1.pdf"
    },
    {
        "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
        "authors": [
            "Junbing Yan",
            "Chengyu Wang",
            "Jun Huang",
            "Wei Zhang"
        ],
        "published": "2024-02-19T12:12:35Z",
        "summary": "Over the past few years, the abilities of large language models (LLMs) have\nreceived extensive attention, which have performed exceptionally well in\ncomplicated scenarios such as logical reasoning and symbolic inference. A\nsignificant factor contributing to this progress is the benefit of in-context\nlearning and few-shot prompting. However, the reasons behind the success of\nsuch models using contextual reasoning have not been fully explored. Do LLMs\nhave understand logical rules to draw inferences, or do they ``guess'' the\nanswers by learning a type of probabilistic mapping through context? This paper\ninvestigates the reasoning capabilities of LLMs on two logical reasoning\ndatasets by using counterfactual methods to replace context text and modify\nlogical concepts. Based on our analysis, it is found that LLMs do not truly\nunderstand logical rules; rather, in-context learning has simply enhanced the\nlikelihood of these models arriving at the correct answers. If one alters\ncertain words in the context text or changes the concepts of logical terms, the\noutputs of LLMs can be significantly disrupted, leading to counter-intuitive\nresponses. This work provides critical insights into the limitations of LLMs,\nunderscoring the need for more robust mechanisms to ensure reliable logical\nreasoning in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.12091v1.pdf"
    },
    {
        "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
        "authors": [
            "Mandar Kulkarni",
            "Praveen Tangarajan",
            "Kyung Kim",
            "Anusua Trivedi"
        ],
        "published": "2024-01-10T02:57:20Z",
        "summary": "With the advent of Large Language Models (LLM), conversational assistants\nhave become prevalent for domain use cases. LLMs acquire the ability to\ncontextual question answering through training, and Retrieval Augmented\nGeneration (RAG) further enables the bot to answer domain-specific questions.\nThis paper describes a RAG-based approach for building a chatbot that answers\nuser's queries using Frequently Asked Questions (FAQ) data. We train an\nin-house retrieval embedding model using infoNCE loss, and experimental results\ndemonstrate that the in-house model works significantly better than the\nwell-known general-purpose public embedding model, both in terms of retrieval\naccuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open\nAPI-based paid ChatGPT model. We noticed that a previously retrieved-context\ncould be used to generate an answer for specific patterns/sequences of queries\n(e.g., follow-up queries). Hence, there is a scope to optimize the number of\nLLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize\nthe number of LLM tokens using Reinforcement Learning (RL). Specifically, we\npropose a policy-based model external to the RAG, which interacts with the RAG\npipeline through policy actions and updates the policy to optimize the cost.\nThe policy model can perform two actions: to fetch FAQ context or skip\nretrieval. We use the open API-based GPT-4 as the reward model. We then train a\npolicy model using policy gradient on multiple training chat sessions. As a\npolicy model, we experimented with a public gpt-2 model and an in-house BERT\nmodel. With the proposed RL-based optimization combined with similarity\nthreshold, we are able to achieve significant cost savings while getting a\nslightly improved accuracy. Though we demonstrate results for the FAQ chatbot,\nthe proposed RL approach is generic and can be experimented with any existing\nRAG pipeline.",
        "pdf_link": "https://arxiv.org/pdf/2401.06800v1.pdf"
    },
    {
        "title": "Large Language Models are Geographically Biased",
        "authors": [
            "Rohin Manvi",
            "Samar Khanna",
            "Marshall Burke",
            "David Lobell",
            "Stefano Ermon"
        ],
        "published": "2024-02-05T02:32:09Z",
        "summary": "Large Language Models (LLMs) inherently carry the biases contained in their\ntraining corpora, which can lead to the perpetuation of societal harm. As the\nimpact of these foundation models grows, understanding and evaluating their\nbiases becomes crucial to achieving fairness and accuracy. We propose to study\nwhat LLMs know about the world we live in through the lens of geography. This\napproach is particularly powerful as there is ground truth for the numerous\naspects of human life that are meaningfully projected onto geographic space\nsuch as culture, race, language, politics, and religion. We show various\nproblematic geographic biases, which we define as systemic errors in geospatial\npredictions. Initially, we demonstrate that LLMs are capable of making accurate\nzero-shot geospatial predictions in the form of ratings that show strong\nmonotonic correlation with ground truth (Spearman's $\\rho$ of up to 0.89). We\nthen show that LLMs exhibit common biases across a range of objective and\nsubjective topics. In particular, LLMs are clearly biased against locations\nwith lower socioeconomic conditions (e.g. most of Africa) on a variety of\nsensitive subjective topics such as attractiveness, morality, and intelligence\n(Spearman's $\\rho$ of up to 0.70). Finally, we introduce a bias score to\nquantify this and find that there is significant variation in the magnitude of\nbias across existing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.02680v1.pdf"
    },
    {
        "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
        "authors": [
            "Yu Wang",
            "Xiusi Chen",
            "Jingbo Shang",
            "Julian McAuley"
        ],
        "published": "2024-02-07T07:14:11Z",
        "summary": "Existing Large Language Models (LLMs) usually remain static after deployment,\nwhich might make it hard to inject new knowledge into the model. We aim to\nbuild models containing a considerable portion of self-updatable parameters,\nenabling the model to integrate new knowledge effectively and efficiently. To\nthis end, we introduce MEMORYLLM, a model that comprises a transformer and a\nfixed-size memory pool within the latent space of the transformer. MEMORYLLM\ncan self-update with text knowledge and memorize the knowledge injected\nearlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively\nincorporate new knowledge, as evidenced by its performance on model editing\nbenchmarks. Meanwhile, the model exhibits long-term information retention\ncapacity, which is validated through our custom-designed evaluations and\nlong-context benchmarks. MEMORYLLM also shows operational integrity without any\nsign of performance degradation even after nearly a million memory updates.",
        "pdf_link": "https://arxiv.org/pdf/2402.04624v1.pdf"
    },
    {
        "title": "Tree-Based Hard Attention with Self-Motivation for Large Language Models",
        "authors": [
            "Chenxi Lin",
            "Jiayu Ren",
            "Guoxiu He",
            "Zhuoren Jiang",
            "Haiyan Yu",
            "Xiaomin Zhu"
        ],
        "published": "2024-02-14T00:40:51Z",
        "summary": "While large language models (LLMs) excel at understanding and generating\nplain text, they are not specifically tailored to handle hierarchical text\nstructures. Extracting the task-desired property from their natural language\nresponses typically necessitates additional processing steps. In fact,\nselectively comprehending the hierarchical structure of large-scale text is\npivotal to understanding its substance. Aligning LLMs more closely with the\nclassification or regression values of specific task through prompting also\nremains challenging. To this end, we propose a novel framework called\nTree-Based Hard Attention with Self-Motivation for Large Language Models\n(TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs\nto process hierarchically structured text inputs. By leveraging prompting, it\nenables a frozen LLM to selectively focus on relevant leaves in relation to the\nroot, generating a tailored symbolic representation of their relationship.\nMoreover, TEAROOM comprises a self-motivation strategy for another LLM equipped\nwith a trainable adapter and a linear layer. The selected symbolic outcomes are\nintegrated into another prompt, along with the predictive value of the task. We\niteratively feed output values back into the prompt, enabling the trainable LLM\nto progressively approximate the golden truth. TEAROOM outperforms existing\nstate-of-the-art methods in experimental evaluations across three benchmark\ndatasets, showing its effectiveness in estimating task-specific properties.\nThrough comprehensive experiments and analysis, we have validated the ability\nof TEAROOM to gradually approach the underlying golden truth through multiple\ninferences.",
        "pdf_link": "https://arxiv.org/pdf/2402.08874v1.pdf"
    },
    {
        "title": "TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models",
        "authors": [
            "Xue Zhang",
            "Xiangyu Shi",
            "Xinyue Lou",
            "Rui Qi",
            "Yufeng Chen",
            "Jinan Xu",
            "Wenjuan Han"
        ],
        "published": "2024-01-09T10:20:29Z",
        "summary": "Large language models (LLMs) and multimodal large language models (MLLMs)\nhave shown excellent general capabilities, even exhibiting adaptability in many\nprofessional domains such as law, economics, transportation, and medicine.\nCurrently, many domain-specific benchmarks have been proposed to verify the\nperformance of (M)LLMs in specific fields. Among various domains,\ntransportation plays a crucial role in modern society as it impacts the\neconomy, the environment, and the quality of life for billions of people.\nHowever, it is unclear how much traffic knowledge (M)LLMs possess and whether\nthey can reliably perform transportation-related tasks. To address this gap, we\npropose TransportationGames, a carefully designed and thorough evaluation\nbenchmark for assessing (M)LLMs in the transportation domain. By\ncomprehensively considering the applications in real-world scenarios and\nreferring to the first three levels in Bloom's Taxonomy, we test the\nperformance of various (M)LLMs in memorizing, understanding, and applying\ntransportation knowledge by the selected tasks. The experimental results show\nthat although some models perform well in some tasks, there is still much room\nfor improvement overall. We hope the release of TransportationGames can serve\nas a foundation for future research, thereby accelerating the implementation\nand application of (M)LLMs in the transportation domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.04471v1.pdf"
    },
    {
        "title": "Parameter-Efficient Detoxification with Contrastive Decoding",
        "authors": [
            "Tong Niu",
            "Caiming Xiong",
            "Semih Yavuz",
            "Yingbo Zhou"
        ],
        "published": "2024-01-13T01:46:20Z",
        "summary": "The field of natural language generation has witnessed significant\nadvancements in recent years, including the development of controllable text\ngeneration techniques. However, controlling the attributes of the generated\ntext remains a challenge, especially when aiming to avoid undesirable behavior\nsuch as toxicity. In this work, we introduce Detoxification Generator\n(DETOXIGEN), an inference-time algorithm that steers the generation away from\nunwanted styles. DETOXIGEN is an ensemble of a pre-trained language model\n(generator) and a detoxifier. The detoxifier is trained intentionally on the\ntoxic data representative of the undesirable attribute, encouraging it to\ngenerate text in that style exclusively. During the actual generation, we use\nthe trained detoxifier to produce undesirable tokens for the generator to\ncontrast against at each decoding step. This approach directly informs the\ngenerator to avoid generating tokens that the detoxifier considers highly\nlikely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS\nbenchmark (Gehman et al., 2020) with various language models as generators. We\nfind that it significantly outperforms previous approaches in detoxification\nmetrics while not compromising on the generation quality. Moreover, the\ndetoxifier is obtained by soft prompt-tuning using the same backbone language\nmodel as the generator. Hence, DETOXIGEN requires only a tiny amount of extra\nweights from the virtual tokens of the detoxifier to be loaded into GPU memory\nwhile decoding, making it a promising lightweight, practical, and\nparameter-efficient detoxification strategy.",
        "pdf_link": "https://arxiv.org/pdf/2401.06947v1.pdf"
    },
    {
        "title": "Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal Traits",
        "authors": [
            "Zhivar Sourati",
            "Meltem Ozcan",
            "Colin McDaniel",
            "Alireza Ziabari",
            "Nuan Wen",
            "Ala Tak",
            "Fred Morstatter",
            "Morteza Dehghani"
        ],
        "published": "2024-03-30T06:49:17Z",
        "summary": "Prior research has established associations between individuals' language\nusage and their personal traits; our linguistic patterns reveal information\nabout our personalities, emotional states, and beliefs. However, with the\nincreasing adoption of Large Language Models (LLMs) as writing assistants in\neveryday writing, a critical question emerges: are authors' linguistic patterns\nstill predictive of their personal traits when LLMs are involved in the writing\nprocess? We investigate the impact of LLMs on the linguistic markers of\ndemographic and psychological traits, specifically examining three LLMs -\nGPT3.5, Llama 2, and Gemini - across six different traits: gender, age,\npolitical affiliation, personality, empathy, and morality. Our findings\nindicate that although the use of LLMs slightly reduces the predictive power of\nlinguistic patterns over authors' personal traits, the significant changes are\ninfrequent, and the use of LLMs does not fully diminish the predictive power of\nauthors' linguistic patterns over their personal traits. We also note that some\ntheoretically established lexical-based linguistic markers lose their\nreliability as predictors when LLMs are used in the writing process. Our\nfindings have important implications for the study of linguistic markers of\npersonal traits in the age of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2404.00267v2.pdf"
    },
    {
        "title": "Zero Resource Cross-Lingual Part Of Speech Tagging",
        "authors": [
            "Sahil Chopra"
        ],
        "published": "2024-01-11T08:12:47Z",
        "summary": "Part of speech tagging in zero-resource settings can be an effective approach\nfor low-resource languages when no labeled training data is available. Existing\nsystems use two main techniques for POS tagging i.e. pretrained multilingual\nlarge language models(LLM) or project the source language labels into the zero\nresource target language and train a sequence labeling model on it. We explore\nthe latter approach using the off-the-shelf alignment module and train a hidden\nMarkov model(HMM) to predict the POS tags. We evaluate transfer learning setup\nwith English as a source language and French, German, and Spanish as target\nlanguages for part-of-speech tagging. Our conclusion is that projected\nalignment data in zero-resource language can be beneficial to predict POS tags.",
        "pdf_link": "https://arxiv.org/pdf/2401.05727v1.pdf"
    },
    {
        "title": "SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental Health Sensing Studies",
        "authors": [
            "Akshat Choube",
            "Vedant Das Swain",
            "Varun Mishra"
        ],
        "published": "2024-03-25T21:48:22Z",
        "summary": "Advances in mobile and wearable technologies have enabled the potential to\npassively monitor a person's mental, behavioral, and affective health. These\napproaches typically rely on longitudinal collection of self-reported outcomes,\ne.g., depression, stress, and anxiety, to train machine learning (ML) models.\nHowever, the need to continuously self-report adds a significant burden on the\nparticipants, often resulting in attrition, missing labels, or insincere\nresponses. In this work, we introduce the Scale Scores Simulation using Mental\nModels (SeSaMe) framework to alleviate participants' burden in digital mental\nhealth studies. By leveraging pre-trained large language models (LLMs), SeSaMe\nenables the simulation of participants' responses on psychological scales. In\nSeSaMe, researchers can prompt LLMs with information on participants' internal\nbehavioral dispositions, enabling LLMs to construct mental models of\nparticipants to simulate their responses on psychological scales. We\ndemonstrate an application of SeSaMe, where we use GPT-4 to simulate responses\non one scale using responses from another as behavioral information. We also\nevaluate the alignment between human and SeSaMe-simulated responses to\npsychological scales. Then, we present experiments to inspect the utility of\nSeSaMe-simulated responses as ground truth in training ML models by replicating\nestablished depression and anxiety screening tasks from a previous study. Our\nresults indicate SeSaMe to be a promising approach, but its alignment may vary\nacross scales and specific prediction objectives. We also observed that model\nperformance with simulated data was on par with using the real data for\ntraining in most evaluation scenarios. We conclude by discussing the potential\nimplications of SeSaMe in addressing some challenges researchers face with\nground-truth collection in passive sensing studies.",
        "pdf_link": "https://arxiv.org/pdf/2403.17219v2.pdf"
    },
    {
        "title": "Understanding and Patching Compositional Reasoning in LLMs",
        "authors": [
            "Zhaoyi Li",
            "Gangwei Jiang",
            "Hong Xie",
            "Linqi Song",
            "Defu Lian",
            "Ying Wei"
        ],
        "published": "2024-02-22T06:47:56Z",
        "summary": "LLMs have marked a revolutonary shift, yet they falter when faced with\ncompositional reasoning tasks. Our research embarks on a quest to uncover the\nroot causes of compositional reasoning failures of LLMs, uncovering that most\nof them stem from the improperly generated or leveraged implicit reasoning\nresults. Inspired by our empirical findings, we resort to Logit Lens and an\nintervention experiment to dissect the inner hidden states of LLMs. This deep\ndive reveals that implicit reasoning results indeed surface within middle\nlayers and play a causative role in shaping the final explicit reasoning\nresults. Our exploration further locates multi-head self-attention (MHSA)\nmodules within these layers, which emerge as the linchpins in accurate\ngeneration and leveraing of implicit reasoning results. Grounded on the above\nfindings, we develop CREME, a lightweight method to patch errors in\ncompositional reasoning via editing the located MHSA modules. Our empirical\nevidence stands testament to CREME's effectiveness, paving the way for\nautonomously and continuously enhancing compositional reasoning capabilities in\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14328v1.pdf"
    },
    {
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "authors": [
            "Siyu Yuan",
            "Kaitao Song",
            "Jiangjie Chen",
            "Xu Tan",
            "Yongliang Shen",
            "Ren Kan",
            "Dongsheng Li",
            "Deqing Yang"
        ],
        "published": "2024-01-11T15:45:11Z",
        "summary": "To address intricate real-world tasks, there has been a rising interest in\ntool utilization in applications of large language models (LLMs). To develop\nLLM-based agents, it usually requires LLMs to understand many tool functions\nfrom different tool documentation. But these documentations could be diverse,\nredundant or incomplete, which immensely affects the capability of LLMs in\nusing tools. To solve this, we introduce EASYTOOL, a framework transforming\ndiverse and lengthy tool documentation into a unified and concise tool\ninstruction for easier tool usage. EasyTool purifies essential information from\nextensive tool documentation of different sources, and elaborates a unified\ninterface (i.e., tool instruction) to offer standardized tool descriptions and\nfunctionalities for LLM-based agents. Extensive experiments on multiple\ndifferent tasks demonstrate that EasyTool can significantly reduce token\nconsumption and improve the performance of tool utilization in real-world\nscenarios. Our code will be available at\n\\url{https://github.com/microsoft/JARVIS/} in the future.",
        "pdf_link": "https://arxiv.org/pdf/2401.06201v3.pdf"
    },
    {
        "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
        "authors": [
            "Vyas Raina",
            "Adian Liusie",
            "Mark Gales"
        ],
        "published": "2024-02-21T18:55:20Z",
        "summary": "Large Language Models (LLMs) are powerful zero-shot assessors and are\nincreasingly used in real-world situations such as for written exams or\nbenchmarking systems. Despite this, no existing work has analyzed the\nvulnerability of judge-LLMs against adversaries attempting to manipulate\noutputs. This work presents the first study on the adversarial robustness of\nassessment LLMs, where we search for short universal phrases that when appended\nto texts can deceive LLMs to provide high assessment scores. Experiments on\nSummEval and TopicalChat demonstrate that both LLM-scoring and pairwise\nLLM-comparative assessment are vulnerable to simple concatenation attacks,\nwhere in particular LLM-scoring is very susceptible and can yield maximum\nassessment scores irrespective of the input text quality. Interestingly, such\nattacks are transferable and phrases learned on smaller open-source LLMs can be\napplied to larger closed-source models, such as GPT3.5. This highlights the\npervasive nature of the adversarial vulnerabilities across different judge-LLM\nsizes, families and methods. Our findings raise significant concerns on the\nreliability of LLMs-as-a-judge methods, and underscore the importance of\naddressing vulnerabilities in LLM assessment methods before deployment in\nhigh-stakes real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.14016v1.pdf"
    },
    {
        "title": "Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control",
        "authors": [
            "Yongjun Kim",
            "Sejin Seo",
            "Jihong Park",
            "Mehdi Bennis",
            "Seong-Lyun Kim",
            "Junil Choi"
        ],
        "published": "2024-01-23T10:23:13Z",
        "summary": "In this work, we compare emergent communication (EC) built upon multi-agent\ndeep reinforcement learning (MADRL) and language-oriented semantic\ncommunication (LSC) empowered by a pre-trained large language model (LLM) using\nhuman language. In a multi-agent remote navigation task, with multimodal input\ndata comprising location and channel maps, it is shown that EC incurs high\ntraining cost and struggles when using multimodal data, whereas LSC yields high\ninference computing cost due to the LLM's large size. To address their\nrespective bottlenecks, we propose a novel framework of language-guided EC\n(LEC) by guiding the EC training using LSC via knowledge distillation (KD).\nSimulations corroborate that LEC achieves faster travel time while avoiding\nareas with poor channel conditions, as well as speeding up the MADRL training\nconvergence by up to 61.8% compared to EC.",
        "pdf_link": "https://arxiv.org/pdf/2401.12624v2.pdf"
    },
    {
        "title": "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?",
        "authors": [
            "Alexander Arno Weber",
            "Klaudia Thellmann",
            "Jan Ebert",
            "Nicolas Flores-Herr",
            "Jens Lehmann",
            "Michael Fromm",
            "Mehdi Ali"
        ],
        "published": "2024-02-21T11:07:07Z",
        "summary": "The adaption of multilingual pre-trained Large Language Models (LLMs) into\neloquent and helpful assistants is essential to facilitate their use across\ndifferent language regions. In that spirit, we are the first to conduct an\nextensive study of the performance of multilingual models on parallel,\nmulti-turn instruction-tuning benchmarks across a selection of the most-spoken\nIndo-European languages. We systematically examine the effects of language and\ninstruction dataset size on a mid-sized, multilingual LLM by instruction-tuning\nit on parallel instruction-tuning datasets. Our results demonstrate that\ninstruction-tuning on parallel instead of monolingual corpora benefits\ncross-lingual instruction following capabilities by up to 4.6%. Furthermore, we\nshow that the Superficial Alignment Hypothesis does not hold in general, as the\ninvestigated multilingual 7B parameter model presents a counter-example\nrequiring large-scale instruction-tuning datasets. Finally, we conduct a human\nannotation study to understand the alignment between human-based and\nGPT-4-based evaluation within multilingual chat scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.13703v1.pdf"
    },
    {
        "title": "Language Detection for Transliterated Content",
        "authors": [
            "Selva Kumar S",
            "Afifah Khan Mohammed Ajmal Khan",
            "Chirag Manjeshwar",
            "Imadh Ajaz Banday"
        ],
        "published": "2024-01-09T15:40:54Z",
        "summary": "In the contemporary digital era, the Internet functions as an unparalleled\ncatalyst, dismantling geographical and linguistic barriers particularly evident\nin texting. This evolution facilitates global communication, transcending\nphysical distances and fostering dynamic cultural exchange. A notable trend is\nthe widespread use of transliteration, where the English alphabet is employed\nto convey messages in native languages, posing a unique challenge for language\ntechnology in accurately detecting the source language. This paper addresses\nthis challenge through a dataset of phone text messages in Hindi and Russian\ntransliterated into English utilizing BERT for language classification and\nGoogle Translate API for transliteration conversion. The research pioneers\ninnovative approaches to identify and convert transliterated text, navigating\nchallenges in the diverse linguistic landscape of digital communication.\nEmphasizing the pivotal role of comprehensive datasets for training Large\nLanguage Models LLMs like BERT, our model showcases exceptional proficiency in\naccurately identifying and classifying languages from transliterated text. With\na validation accuracy of 99% our models robust performance underscores its\nreliability. The comprehensive exploration of transliteration dynamics\nsupported by innovative approaches and cutting edge technologies like BERT,\npositions our research at the forefront of addressing unique challenges in the\nlinguistic landscape of digital communication. Beyond contributing to language\nidentification and transliteration capabilities this work holds promise for\napplications in content moderation, analytics and fostering a globally\nconnected community engaged in meaningful dialogue.",
        "pdf_link": "https://arxiv.org/pdf/2401.04619v1.pdf"
    },
    {
        "title": "Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric, Data, and Algorithm",
        "authors": [
            "Lei Liu",
            "Xiaoyan Yang",
            "Fangzhou Li",
            "Chenfei Chi",
            "Yue Shen",
            "Shiwei Lyu Ming Zhang",
            "Xiaowei Ma",
            "Xiangguo Lyu",
            "Liya Ma",
            "Zhiqiang Zhang",
            "Wei Xue",
            "Yiran Huang",
            "Jinjie Gu"
        ],
        "published": "2024-03-25T06:17:54Z",
        "summary": "Large language models (LLMs) are gaining increasing interests to improve\nclinical efficiency for medical diagnosis, owing to their unprecedented\nperformance in modelling natural language. Ensuring the safe and reliable\nclinical applications, the evaluation of LLMs indeed becomes critical for\nbetter mitigating the potential risks, e.g., hallucinations. However, current\nevaluation methods heavily rely on labor-intensive human participation to\nachieve human-preferred judgements. To overcome this challenge, we propose an\nautomatic evaluation paradigm tailored to assess the LLMs' capabilities in\ndelivering clinical services, e.g., disease diagnosis and treatment. The\nevaluation paradigm contains three basic elements: metric, data, and algorithm.\nSpecifically, inspired by professional clinical practice pathways, we formulate\na LLM-specific clinical pathway (LCP) to define the clinical capabilities that\na doctor agent should possess. Then, Standardized Patients (SPs) from the\nmedical education are introduced as the guideline for collecting medical data\nfor evaluation, which can well ensure the completeness of the evaluation\nprocedure. Leveraging these steps, we develop a multi-agent framework to\nsimulate the interactive environment between SPs and a doctor agent, which is\nequipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the\nbehaviors of a doctor agent are in accordance with LCP. The above paradigm can\nbe extended to any similar clinical scenarios to automatically evaluate the\nLLMs' medical capabilities. Applying such paradigm, we construct an evaluation\nbenchmark in the field of urology, including a LCP, a SPs dataset, and an\nautomated RAE. Extensive experiments are conducted to demonstrate the\neffectiveness of the proposed approach, providing more insights for LLMs' safe\nand reliable deployments in clinical practice.",
        "pdf_link": "https://arxiv.org/pdf/2403.16446v1.pdf"
    },
    {
        "title": "Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations",
        "authors": [
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "published": "2024-02-08T12:36:29Z",
        "summary": "Long-form generations from large language models (LLMs) contains a mix of\nfactual and non-factual claims, making evaluating factuality difficult. To\nevaluate factual precision of long-form generations in a more fine-grained way,\nprior works propose to decompose long-form generations into multiple verifiable\nfacts and verify those facts independently. The factuality of the generation is\nthe proportion of verifiable facts among all the facts. Such methods assume\nthat combining factual claims forms a factual paragraph. This paper shows that\nthe assumption can be violated due to entity ambiguity. We show that LLMs can\ngenerate paragraphs that contain verifiable facts, but the facts are combined\nto form a non-factual paragraph due to entity ambiguity. We further reveal that\nexisting factual precision metrics, including FActScore and citation recall,\ncannot properly evaluate the factuality of these non-factual paragraphs. To\naddress this, we introduce an enhanced metric, D-FActScore, specifically\ndesigned for content with ambiguous entities. We evaluate the D-FActScores of\npeople biographies generated with retrieval-augmented generation (RAG). We show\nthat D-FActScore can better assess the factuality of paragraphs with entity\nambiguity than FActScore. We also find that four widely used open-source LLMs\ntend to mix information of distinct entities to form non-factual paragraphs.",
        "pdf_link": "https://arxiv.org/pdf/2402.05629v2.pdf"
    },
    {
        "title": "CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities",
        "authors": [
            "Yujun Mao",
            "Yoon Kim",
            "Yilun Zhou"
        ],
        "published": "2024-01-13T03:18:16Z",
        "summary": "Recent large language models (LLMs) have shown indications of mathematical\nreasoning ability. However it has not been clear how they would fare on more\nchallenging competition-level problems. And while self-generated verbalizations\nof intermediate reasoning steps (i.e., chain-of-thought prompting) have been\nshown to be helpful, whether LLMs can make use of helpful side information such\nas problem-specific hints has not been investigated before. In this paper, we\npropose a challenging benchmark dataset for enabling such analyses. The Concept\nand Hint-Annotated Math Problems (CHAMP) consists of high school math\ncompetition problems, annotated with concepts, or general math facts, and\nhints, or problem-specific tricks. These annotations allow us to explore the\neffects of additional information, such as relevant hints, misleading concepts,\nor related problems. This benchmark is difficult, with the best model only\nscoring 58.1% in standard settings. With concepts and hints, performance\nsometimes improves, indicating that some models can make use of such side\ninformation. We further annotate model-generated solutions for their\ncorrectness. Using this corpus, we find that models often arrive at the correct\nfinal answer through wrong reasoning steps. In addition, we test whether models\nare able to verify these solutions, and find that most models struggle. The\ndataset and code are available on the project website.",
        "pdf_link": "https://arxiv.org/pdf/2401.06961v1.pdf"
    },
    {
        "title": "Comparing Template-based and Template-free Language Model Probing",
        "authors": [
            "Sagi Shaier",
            "Kevin Bennett",
            "Lawrence E Hunter",
            "Katharina von der Wense"
        ],
        "published": "2024-01-31T19:07:37Z",
        "summary": "The differences between cloze-task language model (LM) probing with 1)\nexpert-made templates and 2) naturally-occurring text have often been\noverlooked. Here, we evaluate 16 different LMs on 10 probing English datasets\n-- 4 template-based and 6 template-free -- in general and biomedical domains to\nanswer the following research questions: (RQ1) Do model rankings differ between\nthe two approaches? (RQ2) Do models' absolute scores differ between the two\napproaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and\ndomain-specific models? Our findings are: 1) Template-free and template-based\napproaches often rank models differently, except for the top domain-specific\nmodels. 2) Scores decrease by up to 42% Acc@1 when comparing parallel\ntemplate-free and template-based prompts. 3) Perplexity is negatively\ncorrelated with accuracy in the template-free approach, but,\ncounter-intuitively, they are positively correlated for template-based probing.\n4) Models tend to predict the same answers frequently across prompts for\ntemplate-based probing, which is less common when employing template-free\ntechniques.",
        "pdf_link": "https://arxiv.org/pdf/2402.00123v1.pdf"
    },
    {
        "title": "Multi-Candidate Speculative Decoding",
        "authors": [
            "Sen Yang",
            "Shujian Huang",
            "Xinyu Dai",
            "Jiajun Chen"
        ],
        "published": "2024-01-12T17:15:23Z",
        "summary": "Large language models have shown impressive capabilities across a variety of\nNLP tasks, yet their generating text autoregressively is time-consuming. One\nway to speed them up is speculative decoding, which generates candidate\nsegments (a sequence of tokens) from a fast draft model that is then verified\nin parallel by the target model. However, the acceptance rate of candidate\ntokens receives limitations from several factors, such as the model, the\ndataset, and the decoding setup. This paper proposes sampling multiple\ncandidates from a draft model and then organising them in batches for\nverification. We design algorithms for efficient multi-candidate verification\nwhile maintaining the distribution of the target model. Our approach shows\nsignificant improvements in acceptance rates on multiple datasets and models,\nconsistently outperforming standard speculative decoding.",
        "pdf_link": "https://arxiv.org/pdf/2401.06706v1.pdf"
    },
    {
        "title": "Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery",
        "authors": [
            "Wei Zhang",
            "Miaoxin Cai",
            "Tong Zhang",
            "Guoqiang Lei",
            "Yin Zhuang",
            "Xuerui Mao"
        ],
        "published": "2024-03-06T15:35:53Z",
        "summary": "Ship detection needs to identify ship locations from remote sensing (RS)\nscenes. However, due to different imaging payloads, various appearances of\nships, and complicated background interference from the bird's eye view, it is\ndifficult to set up a unified paradigm for achieving multi-source ship\ndetection. Therefore, in this article, considering that the large language\nmodels (LLMs) emerge the powerful generalization ability, a novel unified\nvisual-language model called Popeye is proposed for multi-source ship detection\nfrom RS imagery. First, to bridge the interpretation gap between multi-source\nimages for ship detection, a novel image-instruction-answer way is designed to\nintegrate the various ship detection ways (e.g., horizontal bounding box (HBB),\noriented bounding box (OBB)) into a unified labeling paradigm. Then, in view of\nthis, a cross-modal image interpretation method is developed for the proposed\nPopeye to enhance interactive comprehension ability between visual and language\ncontent, which can be easily migrated into any multi-source ship detection\ntask. Subsequently, owing to objective domain differences, a knowledge adaption\nmechanism is designed to adapt the pre-trained visual-language knowledge from\nthe nature scene into the RS domain for multi-source ship detection. In\naddition, the segment anything model (SAM) is also seamlessly integrated into\nthe proposed Popeye to achieve pixel-level ship segmentation without additional\ntraining costs. Finally, extensive experiments are conducted on the newly\nconstructed instruction dataset named MMShip, and the results indicate that the\nproposed Popeye outperforms current specialist, open-vocabulary, and other\nvisual-language models for zero-shot multi-source ship detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.03790v1.pdf"
    },
    {
        "title": "Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse",
        "authors": [
            "Joseph Gatto",
            "Madhusudan Basak",
            "Yash Srivastava",
            "Philip Bohlman",
            "Sarah M. Preum"
        ],
        "published": "2024-03-05T21:38:19Z",
        "summary": "In this paper, we develop an LLM-powered framework for the curation and\nevaluation of emerging opinion mining in online health communities. We\nformulate emerging opinion mining as a pairwise stance detection problem\nbetween (title, comment) pairs sourced from Reddit, where post titles contain\nemerging health-related claims on a topic that is not predefined. The claims\nare either explicitly or implicitly expressed by the user. We detail (i) a\nmethod of claim identification -- the task of identifying if a post title\ncontains a claim and (ii) an opinion mining-driven evaluation framework for\nstance detection using LLMs.\n  We facilitate our exploration by releasing a novel test dataset, Long\nCOVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of\nclaim identification and stance detection in online health communities. Long\nCovid is an emerging post-COVID disorder with uncertain and complex treatment\nguidelines, thus making it a suitable use case for our task. LC-Stance contains\nlong COVID treatment related discourse sourced from a Reddit community. Our\nevaluation shows that GPT-4 significantly outperforms prior works on zero-shot\nstance detection. We then perform thorough LLM model diagnostics, identifying\nthe role of claim type (i.e. implicit vs explicit claims) and comment length as\nsources of model error.",
        "pdf_link": "https://arxiv.org/pdf/2403.03336v1.pdf"
    },
    {
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
        "authors": [
            "Liyan Tang",
            "Igor Shalyminov",
            "Amy Wing-mei Wong",
            "Jon Burnsky",
            "Jake W. Vincent",
            "Yu'an Yang",
            "Siffi Singh",
            "Song Feng",
            "Hwanjun Song",
            "Hang Su",
            "Lijia Sun",
            "Yi Zhang",
            "Saab Mansour",
            "Kathleen McKeown"
        ],
        "published": "2024-02-20T18:58:49Z",
        "summary": "Single document news summarization has seen substantial progress on\nfaithfulness in recent years, driven by research on the evaluation of factual\nconsistency, or hallucinations. We ask whether these advances carry over to\nother text summarization domains. We propose a new evaluation benchmark on\ntopic-focused dialogue summarization, generated by LLMs of varying sizes. We\nprovide binary sentence-level human annotations of the factual consistency of\nthese summaries along with detailed explanations of factually inconsistent\nsentences. Our analysis shows that existing LLMs hallucinate significant\namounts of factual errors in the dialogue domain, regardless of the model's\nsize. On the other hand, when LLMs, including GPT-4, serve as binary factual\nevaluators, they perform poorly and can be outperformed by prevailing\nstate-of-the-art specialized factuality evaluation metrics. Finally, we\nconducted an analysis of hallucination types with a curated error taxonomy. We\nfind that there are diverse errors and error distributions in model-generated\nsummaries and that non-LLM based metrics can capture all error types better\nthan LLM-based evaluators.",
        "pdf_link": "https://arxiv.org/pdf/2402.13249v2.pdf"
    },
    {
        "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
        "authors": [
            "Chen Liu",
            "Shibo He",
            "Qihang Zhou",
            "Shizhong Li",
            "Wenchao Meng"
        ],
        "published": "2024-01-26T09:51:07Z",
        "summary": "Self-supervised methods have gained prominence in time series anomaly\ndetection due to the scarcity of available annotations. Nevertheless, they\ntypically demand extensive training data to acquire a generalizable\nrepresentation map, which conflicts with scenarios of a few available samples,\nthereby limiting their performance. To overcome the limitation, we propose\n\\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly\ndetection approach where the student network is trained to mimic the features\nof the large language model (LLM)-based teacher network that is pretrained on\nlarge-scale datasets. During the testing phase, anomalies are detected when the\ndiscrepancy between the features of the teacher and student networks is large.\nTo circumvent the student network from learning the teacher network's feature\nof anomalous samples, we devise two key strategies. 1) Prototypical signals are\nincorporated into the student network to consolidate the normal feature\nextraction. 2) We use synthetic anomalies to enlarge the representation gap\nbetween the two networks. AnomalyLLM demonstrates state-of-the-art performance\non 15 datasets, improving accuracy by at least 14.5\\% in the UCR dataset.",
        "pdf_link": "https://arxiv.org/pdf/2401.15123v1.pdf"
    },
    {
        "title": "Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution Analyst?",
        "authors": [
            "Bruno de Melo",
            "Jamiel Sheikh"
        ],
        "published": "2024-03-15T17:12:57Z",
        "summary": "Performance attribution analysis, defined as the process of explaining the\ndrivers of the excess performance of an investment portfolio against a\nbenchmark, stands as a significant feature of portfolio management and plays a\ncrucial role in the investment decision-making process, particularly within the\nfund management industry. Rooted in a solid financial and mathematical\nframework, the importance and methodologies of this analytical technique are\nextensively documented across numerous academic research papers and books. The\nintegration of large language models (LLMs) and AI agents marks a\ngroundbreaking development in this field. These agents are designed to automate\nand enhance the performance attribution analysis by accurately calculating and\nanalyzing portfolio performances against benchmarks. In this study, we\nintroduce the application of an AI Agent for a variety of essential performance\nattribution tasks, including the analysis of performance drivers and utilizing\nLLMs as calculation engine for multi-level attribution analysis and\nquestion-answering (QA) tasks. Leveraging advanced prompt engineering\ntechniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and\nemploying a standard agent framework from LangChain, the research achieves\npromising results: it achieves accuracy rates exceeding 93% in analyzing\nperformance drivers, attains 100% in multi-level attribution calculations, and\nsurpasses 84% accuracy in QA exercises that simulate official examination\nstandards. These findings affirm the impactful role of AI agents, prompt\nengineering and evaluation in advancing portfolio management processes,\nhighlighting a significant development in the practical application and\nevaluation of Generative AI technologies within the domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.10482v2.pdf"
    },
    {
        "title": "Enhancing Temporal Knowledge Graph Forecasting with Large Language Models via Chain-of-History Reasoning",
        "authors": [
            "Yuwei Xia",
            "Ding Wang",
            "Qiang Liu",
            "Liang Wang",
            "Shu Wu",
            "Xiaoyu Zhang"
        ],
        "published": "2024-02-22T08:51:39Z",
        "summary": "Temporal Knowledge Graph (TKG) forecasting aims to predict future facts based\non given histories. Most recent graph-based models excel at capturing\nstructural information within TKGs but lack semantic comprehension abilities.\nNowadays, with the surge of LLMs, the LLM-based TKG prediction model has\nemerged. However, the existing LLM-based model exhibits three shortcomings: (1)\nIt only focuses on the first-order history for prediction while ignoring\nhigh-order historical information, resulting in the provided information for\nLLMs being extremely limited. (2) LLMs struggle with optimal reasoning\nperformance under heavy historical information loads. (3) For TKG prediction,\nthe temporal reasoning capability of LLM alone is limited. To address the first\ntwo challenges, we propose Chain-of-History (CoH) reasoning which explores\nhigh-order histories step-by-step, achieving effective utilization of\nhigh-order historical information for LLMs on TKG prediction. To address the\nthird issue, we design CoH as a paly-and-plug module to enhance the performance\nof graph-based models for TKG prediction. Extensive experiments on three\ndatasets and backbones demonstrate the effectiveness of CoH.",
        "pdf_link": "https://arxiv.org/pdf/2402.14382v1.pdf"
    },
    {
        "title": "Caveat Lector: Large Language Models in Legal Practice",
        "authors": [
            "Eliza Mik"
        ],
        "published": "2024-03-14T08:19:41Z",
        "summary": "The current fascination with large language models, or LLMs, derives from the\nfact that many users lack the expertise to evaluate the quality of the\ngenerated text. LLMs may therefore appear more capable than they actually are.\nThe dangerous combination of fluency and superficial plausibility leads to the\ntemptation to trust the generated text and creates the risk of overreliance.\nWho would not trust perfect legalese? Relying recent findings in both technical\nand legal scholarship, this Article counterbalances the overly optimistic\npredictions as to the role of LLMs in legal practice. Integrating LLMs into\nlegal workstreams without a better comprehension of their limitations, will\ncreate inefficiencies if not outright risks. Notwithstanding their\nunprecedented ability to generate text, LLMs do not understand text. Without\nthe ability to understand meaning, LLMs will remain unable to use language, to\nacquire knowledge and to perform complex reasoning tasks. Trained to model\nlanguage on the basis of stochastic word predictions, LLMs cannot distinguish\nfact from fiction. Their knowledge of the law is limited to word strings\nmemorized in their parameters. It is also incomplete and largely incorrect.\nLLMs operate at the level of word distributions, not at the level of verified\nfacts. The resulting propensity to hallucinate, to produce statements that are\nincorrect but appear helpful and relevant, is alarming in high-risk areas like\nlegal services. At present, lawyers should beware of relying on text generated\nby LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.09163v1.pdf"
    },
    {
        "title": "Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models",
        "authors": [
            "Xinpeng Ding",
            "Jinahua Han",
            "Hang Xu",
            "Xiaodan Liang",
            "Wei Zhang",
            "Xiaomeng Li"
        ],
        "published": "2024-01-02T01:54:22Z",
        "summary": "The rise of multimodal large language models (MLLMs) has spurred interest in\nlanguage-based driving tasks. However, existing research typically focuses on\nlimited tasks and often omits key multi-view and temporal information which is\ncrucial for robust autonomous driving. To bridge these gaps, we introduce\nNuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17\nsubtasks, where each task demands holistic information (e.g., temporal,\nmulti-view, and spatial), significantly elevating the challenge level. To\nobtain NuInstruct, we propose a novel SQL-based method to generate\ninstruction-response pairs automatically, which is inspired by the driving\nlogical progression of humans. We further present BEV-InMLLM, an end-to-end\nmethod for efficiently deriving instruction-aware Bird's-Eye-View (BEV)\nfeatures, language-aligned for large language models. BEV-InMLLM integrates\nmulti-view, spatial awareness, and temporal semantics to enhance MLLMs'\ncapabilities on NuInstruct tasks. Moreover, our proposed BEV injection module\nis a plug-and-play method for existing MLLMs. Our experiments on NuInstruct\ndemonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.\naround 9% improvement on various tasks. We plan to release our NuInstruct for\nfuture research development.",
        "pdf_link": "https://arxiv.org/pdf/2401.00988v1.pdf"
    },
    {
        "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning",
        "authors": [
            "Chenyu Wang",
            "Weixin Luo",
            "Qianyu Chen",
            "Haonan Mai",
            "Jindi Guo",
            "Sixun Dong",
            "Xiaohua",
            "Xuan",
            "Zhengxin Li",
            "Lin Ma",
            "Shenghua Gao"
        ],
        "published": "2024-01-19T14:44:37Z",
        "summary": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' perceiving tool-use ability\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the visual- or auditory-grounded instructions' information.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learnt LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featured by consisting of multi-modal input tools from\nHuggingFace. Another important feature of our dataset is that our dataset also\ncontains multiple potential choices for the same instruction due to the\nexistence of identical functions and synonymous functions, which provides more\npotential solutions for the same query. The experiments reveal that our\nMLLM-Tool is capable of recommending appropriate tools for multi-modal\ninstructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool.",
        "pdf_link": "https://arxiv.org/pdf/2401.10727v2.pdf"
    },
    {
        "title": "Comprehensive Cognitive LLM Agent for Smartphone GUI Automation",
        "authors": [
            "Xinbei Ma",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "published": "2024-02-19T08:29:03Z",
        "summary": "Large language models (LLMs) have shown remarkable potential as human-like\nautonomous language agents to interact with real-world environments, especially\nfor graphical user interface (GUI) automation. However, those GUI agents\nrequire comprehensive cognition ability including exhaustive perception and\nreliable action response. We propose \\underline{Co}mprehensive\n\\underline{Co}gnitive LLM \\underline{Agent}, CoCo-Agent, with two novel\napproaches, comprehensive environment perception (CEP) and conditional action\nprediction (CAP), to systematically improve the GUI automation performance.\nFirst, CEP facilitates the GUI perception through different aspects and\ngranularity, including screenshots and complementary detailed layouts for the\nvisual channel and historical actions for the textual channel. Second, CAP\ndecomposes the action prediction into sub-problems: action type prediction and\naction target conditioned on the action type. With our technical design, our\nagent achieves new state-of-the-art performance on AITW and META-GUI\nbenchmarks, showing promising abilities in realistic scenarios. Code is\navailable at https://github.com/xbmxb/AAgent.",
        "pdf_link": "https://arxiv.org/pdf/2402.11941v2.pdf"
    },
    {
        "title": "Bridge the Modality and Capacity Gaps in Vision-Language Model Selection",
        "authors": [
            "Chao Yi",
            "De-Chuan Zhan",
            "Han-Jia Ye"
        ],
        "published": "2024-03-20T17:54:58Z",
        "summary": "Vision Language Models (VLMs) excel in zero-shot image classification by\npairing images with textual category names. The expanding variety of\nPre-Trained VLMs enhances the likelihood of identifying a suitable VLM for\nspecific tasks. Thus, a promising zero-shot image classification strategy is\nselecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely\non the text data of the target dataset without access to the dataset's images.\nIn this paper, we analyze two inherent challenges in assessing the ability of a\nVLM in this Language-Only VLM selection: the \"Modality Gap\" -- the disparity in\nVLM's embeddings across two different modalities, making text a less reliable\nsubstitute for images; and the \"Capability Gap\" -- the discrepancy between the\nVLM's overall ranking and its ranking for target dataset, hindering direct\nprediction of a model's dataset-specific performance from its general\nperformance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the\nnegative impact of these two gaps. SWAB first adopts optimal transport to\ncapture the relevance between open-source datasets and target dataset with a\ntransportation matrix. It then uses this matrix to transfer useful statistics\nof VLMs from open-source datasets to the target dataset for bridging those two\ngaps and enhancing the VLM's capacity estimation for VLM selection. Experiments\nacross various VLMs and image classification datasets validate SWAB's\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.13797v1.pdf"
    },
    {
        "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
        "authors": [
            "Junjie Ye",
            "Sixian Li",
            "Guanyu Li",
            "Caishuang Huang",
            "Songyang Gao",
            "Yilong Wu",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-02-16T15:19:46Z",
        "summary": "Tool learning is widely acknowledged as a foundational approach or deploying\nlarge language models (LLMs) in real-world scenarios. While current research\nprimarily emphasizes leveraging tools to augment LLMs, it frequently neglects\nemerging safety considerations tied to their application. To fill this gap, we\npresent $ToolSword$, a comprehensive framework dedicated to meticulously\ninvestigating safety issues linked to LLMs in tool learning. Specifically,\nToolSword delineates six safety scenarios for LLMs in tool learning,\nencompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input\nstage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and\n$harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments\nconducted on 11 open-source and closed-source LLMs reveal enduring safety\nchallenges in tool learning, such as handling harmful queries, employing risky\ntools, and delivering detrimental feedback, which even GPT-4 is susceptible to.\nMoreover, we conduct further studies with the aim of fostering research on tool\nlearning safety. The data is released in\nhttps://github.com/Junjie-Ye/ToolSword.",
        "pdf_link": "https://arxiv.org/pdf/2402.10753v1.pdf"
    },
    {
        "title": "PID Control-Based Self-Healing to Improve the Robustness of Large Language Models",
        "authors": [
            "Zhuotong Chen",
            "Zihu Wang",
            "Yifan Yang",
            "Qianxiao Li",
            "Zheng Zhang"
        ],
        "published": "2024-03-31T23:46:51Z",
        "summary": "Despite the effectiveness of deep neural networks in numerous natural\nlanguage processing applications, recent findings have exposed the\nvulnerability of these language models when minor perturbations are introduced.\nWhile appearing semantically indistinguishable to humans, these perturbations\ncan significantly reduce the performance of well-trained language models,\nraising concerns about the reliability of deploying them in safe-critical\nsituations. In this work, we construct a computationally efficient self-healing\nprocess to correct undesired model behavior during online inference when\nperturbations are applied to input data. This is formulated as a trajectory\noptimization problem in which the internal states of the neural network layers\nare automatically corrected using a PID (Proportional-Integral-Derivative)\ncontrol mechanism. The P controller targets immediate state adjustments, while\nthe I and D controllers consider past states and future dynamical trends,\nrespectively. We leverage the geometrical properties of the training data to\ndesign effective linear PID controllers. This approach reduces the\ncomputational cost to that of using just the P controller, instead of the full\nPID control. Further, we introduce an analytical method for approximating the\noptimal control solutions, enhancing the real-time inference capabilities of\nthis controlled system. Moreover, we conduct a theoretical error analysis of\nthe analytic solution in a simplified setting. The proposed PID control-based\nself-healing is a low cost framework that improves the robustness of\npre-trained large language models, whether standard or robustly trained,\nagainst a wide range of perturbations. A detailed implementation can be found\nin:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.",
        "pdf_link": "https://arxiv.org/pdf/2404.00828v1.pdf"
    },
    {
        "title": "Enhance Reasoning for Large Language Models in the Game Werewolf",
        "authors": [
            "Shuang Wu",
            "Liwen Zhu",
            "Tao Yang",
            "Shiwei Xu",
            "Qiang Fu",
            "Yang Wei",
            "Haobo Fu"
        ],
        "published": "2024-02-04T03:47:10Z",
        "summary": "This paper presents an innovative framework that integrates Large Language\nModels (LLMs) with an external Thinker module to enhance the reasoning\ncapabilities of LLM-based agents. Unlike augmenting LLMs with prompt\nengineering, Thinker directly harnesses knowledge from databases and employs\nvarious optimization techniques. The framework forms a reasoning hierarchy\nwhere LLMs handle intuitive System-1 tasks such as natural language processing,\nwhile the Thinker focuses on cognitive System-2 tasks that require complex\nlogical analysis and domain-specific knowledge. Our framework is presented\nusing a 9-player Werewolf game that demands dual-system reasoning. We introduce\na communication protocol between LLMs and the Thinker, and train the Thinker\nusing data from 18800 human sessions and reinforcement learning. Experiments\ndemonstrate the framework's effectiveness in deductive reasoning, speech\ngeneration, and online game evaluation. Additionally, we fine-tune a 6B LLM to\nsurpass GPT4 when integrated with the Thinker. This paper also contributes the\nlargest dataset for social deduction games to date.",
        "pdf_link": "https://arxiv.org/pdf/2402.02330v2.pdf"
    },
    {
        "title": "Revisiting Meta-evaluation for Grammatical Error Correction",
        "authors": [
            "Masamune Kobayashi",
            "Masato Mita",
            "Mamoru Komachi"
        ],
        "published": "2024-03-05T05:53:09Z",
        "summary": "Metrics are the foundation for automatic evaluation in grammatical error\ncorrection (GEC), with their evaluation of the metrics (meta-evaluation)\nrelying on their correlation with human judgments. However, conventional\nmeta-evaluations in English GEC encounter several challenges including biases\ncaused by inconsistencies in evaluation granularity, and an outdated setup\nusing classical systems. These problems can lead to misinterpretation of\nmetrics and potentially hinder the applicability of GEC techniques. To address\nthese issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation.\nSEEDA consists of corrections with human ratings along two different\ngranularities: edit-based and sentence-based, covering 12 state-of-the-art\nsystems including large language models (LLMs), and two human corrections with\ndifferent focuses. The results of improved correlations by aligning the\ngranularity in the sentence-level meta-evaluation, suggest that edit-based\nmetrics may have been underestimated in existing studies. Furthermore,\ncorrelations of most metrics decrease when changing from classical to neural\nsystems, indicating that traditional metrics are relatively poor at evaluating\nfluently corrected sentences with many edits.",
        "pdf_link": "https://arxiv.org/pdf/2403.02674v1.pdf"
    },
    {
        "title": "Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series",
        "authors": [
            "Vijay Ekambaram",
            "Arindam Jati",
            "Nam H. Nguyen",
            "Pankaj Dayama",
            "Chandra Reddy",
            "Wesley M. Gifford",
            "Jayant Kalagnanam"
        ],
        "published": "2024-01-08T15:21:21Z",
        "summary": "Large pre-trained models for zero/few-shot learning excel in language and\nvision domains but encounter challenges in multivariate time series (TS) due to\nthe diverse nature and scarcity of publicly available pre-training data.\nConsequently, there has been a recent surge in utilizing pre-trained large\nlanguage models (LLMs) with token adaptations for TS forecasting. These\napproaches employ cross-domain transfer learning and surprisingly yield\nimpressive results. However, these models are typically very slow and large\n(~billion parameters) and do not consider cross-channel correlations. To\naddress this, we present Tiny Time Mixers (TTM), a significantly small model\nbased on the lightweight TSMixer architecture. TTM marks the first success in\ndeveloping fast and tiny general pre-trained models (<1M parameters),\nexclusively trained on public TS datasets, with effective transfer learning\ncapabilities for forecasting. To tackle the complexity of pre-training on\nmultiple datasets with varied temporal resolutions, we introduce several novel\nenhancements such as adaptive patching, dataset augmentation via downsampling,\nand resolution prefix tuning. Moreover, we employ a multi-level modeling\nstrategy to effectively model channel correlations and infuse exogenous signals\nduring fine-tuning, a crucial capability lacking in existing benchmarks. TTM\nshows significant accuracy gains (12-38\\%) over popular benchmarks in\nfew/zero-shot forecasting. It also drastically reduces the compute needs as\ncompared to LLM-TS methods, with a 14X cut in learnable parameters, 106X less\ntotal parameters, and substantial reductions in fine-tuning (65X) and inference\ntime (54X). In fact, TTM's zero-shot often surpasses the few-shot results in\nmany popular benchmarks, highlighting the efficacy of our approach. Models and\nsource code are available at https://huggingface.co/ibm/TTM",
        "pdf_link": "https://arxiv.org/pdf/2401.03955v5.pdf"
    },
    {
        "title": "Can ChatGPT evaluate research quality?",
        "authors": [
            "Mike Thelwall"
        ],
        "published": "2024-02-08T10:00:40Z",
        "summary": "Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research\nevaluations on journal articles to automate this time-consuming task.\nDesign/methodology/approach: Test the extent to which ChatGPT-4 can assess the\nquality of journal articles using a case study of the published scoring\nguidelines of the UK Research Excellence Framework (REF) 2021 to create a\nresearch evaluation ChatGPT. This was applied to 51 of my own articles and\ncompared against my own quality judgements. Findings: ChatGPT-4 can produce\nplausible document summaries and quality evaluation rationales that match the\nREF criteria. Its overall scores have weak correlations with my self-evaluation\nscores of the same documents (averaging r=0.281 over 15 iterations, with 8\nbeing statistically significantly different from 0). In contrast, the average\nscores from the 15 iterations produced a statistically significant positive\ncorrelation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds\nseems more effective than individual scores. The positive correlation may be\ndue to ChatGPT being able to extract the author's significance, rigour, and\noriginality claims from inside each paper. If my weakest articles are removed,\nthen the correlation with average scores (r=0.200) falls below statistical\nsignificance, suggesting that ChatGPT struggles to make fine-grained\nevaluations. Research limitations: The data is self-evaluations of a\nconvenience sample of articles from one academic in one field. Practical\nimplications: Overall, ChatGPT does not yet seem to be accurate enough to be\ntrusted for any formal or informal research quality evaluation tasks. Research\nevaluators, including journal editors, should therefore take steps to control\nits use. Originality/value: This is the first published attempt at\npost-publication expert review accuracy testing for ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.05519v1.pdf"
    },
    {
        "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off",
        "authors": [
            "Eva Giboulot",
            "Furon Teddy"
        ],
        "published": "2024-03-06T10:55:30Z",
        "summary": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite.",
        "pdf_link": "https://arxiv.org/pdf/2403.04808v1.pdf"
    },
    {
        "title": "CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory",
        "authors": [
            "Zexue He",
            "Leonid Karlinsky",
            "Donghyun Kim",
            "Julian McAuley",
            "Dmitry Krotov",
            "Rogerio Feris"
        ],
        "published": "2024-02-21T01:00:17Z",
        "summary": "Large Language Models (LLMs) struggle to handle long input sequences due to\nhigh memory and runtime costs. Memory-augmented models have emerged as a\npromising solution to this problem, but current methods are hindered by limited\nmemory capacity and require costly re-training to integrate with a new LLM. In\nthis work, we introduce an associative memory module which can be coupled to\nany pre-trained (frozen) attention-based LLM without re-training, enabling it\nto handle arbitrarily long input sequences. Unlike previous methods, our\nassociative memory module consolidates representations of individual tokens\ninto a non-parametric distribution model, dynamically managed by properly\nbalancing the novelty and recency of the incoming data. By retrieving\ninformation from this consolidated associative memory, the base LLM can achieve\nsignificant (up to 29.7% on Arxiv) perplexity reduction in long-context\nmodeling compared to other baselines evaluated on standard benchmarks. This\narchitecture, which we call CAMELoT (Consolidated Associative Memory Enhanced\nLong Transformer), demonstrates superior performance even with a tiny context\nwindow of 128 tokens, and also enables improved in-context learning with a much\nlarger set of demonstrations.",
        "pdf_link": "https://arxiv.org/pdf/2402.13449v1.pdf"
    },
    {
        "title": "Head-wise Shareable Attention for Large Language Models",
        "authors": [
            "Zouying Cao",
            "Yifei Yang",
            "Hai Zhao"
        ],
        "published": "2024-02-19T04:19:36Z",
        "summary": "Large Language Models (LLMs) suffer from huge number of parameters, which\nrestricts their deployment on edge devices. Weight sharing is one promising\nsolution that encourages weight reuse, effectively reducing memory usage with\nless performance drop. However, current weight sharing techniques primarily\nfocus on small-scale models like BERT and employ coarse-grained sharing rules,\ne.g., layer-wise. This becomes limiting given the prevalence of LLMs and\nsharing an entire layer or block obviously diminishes the flexibility of weight\nsharing. In this paper, we present a perspective on $\\textit{$\\textbf{head-wise\nshareable attention for large language models}$}$. We further propose two\nmemory-efficient methods that share parameters across attention heads, with a\nspecific focus on LLMs. Both of them use the same dynamic strategy to select\nthe shared weight matrices. The first method directly reuses the pre-trained\nweights without retraining, denoted as $\\textbf{DirectShare}$. The second\nmethod first post-trains with constraint on weight matrix similarity and then\nshares, denoted as $\\textbf{PostShare}$. Experimental results reveal our\nhead-wise shared models still maintain satisfactory capabilities, demonstrating\nthe feasibility of fine-grained weight sharing applied to LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11819v1.pdf"
    },
    {
        "title": "German Text Simplification: Finetuning Large Language Models with Semi-Synthetic Data",
        "authors": [
            "Lars Kl\u00f6ser",
            "Mika Beele",
            "Jan-Niklas Schagen",
            "Bodo Kraft"
        ],
        "published": "2024-02-16T13:28:44Z",
        "summary": "This study pioneers the use of synthetically generated data for training\ngenerative models in document-level text simplification of German texts. We\ndemonstrate the effectiveness of our approach with real-world online texts.\nAddressing the challenge of data scarcity in language simplification, we\ncrawled professionally simplified German texts and synthesized a corpus using\nGPT-4. We finetune Large Language Models with up to 13 billion parameters on\nthis data and evaluate their performance. This paper employs various\nmethodologies for evaluation and demonstrates the limitations of currently used\nrule-based metrics. Both automatic and manual evaluations reveal that our\nmodels can significantly simplify real-world online texts, indicating the\npotential of synthetic data in improving text simplification.",
        "pdf_link": "https://arxiv.org/pdf/2402.10675v1.pdf"
    },
    {
        "title": "Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models",
        "authors": [
            "Zhiyuan Yu",
            "Xiaogeng Liu",
            "Shunning Liang",
            "Zach Cameron",
            "Chaowei Xiao",
            "Ning Zhang"
        ],
        "published": "2024-03-26T02:47:42Z",
        "summary": "Recent advancements in generative AI have enabled ubiquitous access to large\nlanguage models (LLMs). Empowered by their exceptional capabilities to\nunderstand and generate human-like text, these models are being increasingly\nintegrated into our society. At the same time, there are also concerns on the\npotential misuse of this powerful technology, prompting defensive measures from\nservice providers. To overcome such protection, jailbreaking prompts have\nrecently emerged as one of the most effective mechanisms to circumvent security\nrestrictions and elicit harmful content originally designed to be prohibited.\n  Due to the rapid development of LLMs and their ease of access via natural\nlanguages, the frontline of jailbreak prompts is largely seen in online forums\nand among hobbyists. To gain a better understanding of the threat landscape of\nsemantically meaningful jailbreak prompts, we systemized existing prompts and\nmeasured their jailbreak effectiveness empirically. Further, we conducted a\nuser study involving 92 participants with diverse backgrounds to unveil the\nprocess of manually creating jailbreak prompts. We observed that users often\nsucceeded in jailbreak prompts generation regardless of their expertise in\nLLMs. Building on the insights from the user study, we also developed a system\nusing AI as the assistant to automate the process of jailbreak prompt\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2403.17336v1.pdf"
    },
    {
        "title": "GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering",
        "authors": [
            "Ziyu Ma",
            "Shutao Li",
            "Bin Sun",
            "Jianfei Cai",
            "Zuxiang Long",
            "Fuyan Ma"
        ],
        "published": "2024-02-04T14:28:23Z",
        "summary": "Knowledge-based visual question answering (VQA) requires world knowledge\nbeyond the image for accurate answer. Recently, instead of extra knowledge\nbases, a large language model (LLM) like GPT-3 is activated as an implicit\nknowledge engine to jointly acquire and reason the necessary knowledge for\nanswering by converting images into textual information (e.g., captions and\nanswer candidates). However, such conversion may introduce irrelevant\ninformation, which causes the LLM to misinterpret images and ignore visual\ndetails crucial for accurate knowledge. We argue that multimodal large language\nmodel (MLLM) is a better implicit knowledge engine than the LLM for its\nsuperior capability of visual understanding. Despite this, how to activate the\ncapacity of MLLM as the implicit knowledge engine has not been explored yet.\nTherefore, we propose GeReA, a generate-reason framework that prompts a MLLM\nlike InstructBLIP with question relevant vision and language information to\ngenerate knowledge-relevant descriptions and reasons those descriptions for\nknowledge-based VQA. Specifically, the question-relevant image regions and\nquestion-specific manual prompts are encoded in the MLLM to generate the\nknowledge relevant descriptions, referred to as question-aware prompt captions.\nAfter that, the question-aware prompt captions, image-question pair, and\nsimilar samples are sent into the multi-modal reasoning model to learn a joint\nknowledge-image-question representation for answer prediction. GeReA unlocks\nthe use of MLLM as the implicit knowledge engine, surpassing all previous\nstate-of-the-art methods on OK-VQA and A-OKVQA datasets, with test accuracies\nof 66.5% and 63.3% respectively. Our code will be released at\nhttps://github.com/Upper9527/GeReA.",
        "pdf_link": "https://arxiv.org/pdf/2402.02503v1.pdf"
    },
    {
        "title": "CoverUp: Coverage-Guided LLM-Based Test Generation",
        "authors": [
            "Juan Altmayer Pizzorno",
            "Emery D. Berger"
        ],
        "published": "2024-03-24T16:18:27Z",
        "summary": "This paper presents CoverUp, a novel system that drives the generation of\nhigh-coverage Python regression tests via a combination of coverage analysis\nand large-language models (LLMs). CoverUp iteratively improves coverage,\ninterleaving coverage analysis with dialogs with the LLM to focus its attention\non as yet uncovered lines and branches. The resulting test suites significantly\nimprove coverage over the current state of the art: compared to CodaMosa, a\nhybrid LLM / search-based software testing system, CoverUp substantially\nimproves coverage across the board. On a per-module basis, CoverUp achieves\nmedian line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and\nline+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative,\ncoverage-guided approach is crucial to its effectiveness, contributing to\nnearly half of its successes.",
        "pdf_link": "https://arxiv.org/pdf/2403.16218v1.pdf"
    },
    {
        "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "authors": [
            "Pei Zhou",
            "Jay Pujara",
            "Xiang Ren",
            "Xinyun Chen",
            "Heng-Tze Cheng",
            "Quoc V. Le",
            "Ed H. Chi",
            "Denny Zhou",
            "Swaroop Mishra",
            "Huaixiu Steven Zheng"
        ],
        "published": "2024-02-06T01:13:53Z",
        "summary": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the\ntask-intrinsic reasoning structures to tackle complex reasoning problems that\nare challenging for typical prompting methods. Core to the framework is a\nself-discovery process where LLMs select multiple atomic reasoning modules such\nas critical thinking and step-by-step thinking, and compose them into an\nexplicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER\nsubstantially improves GPT-4 and PaLM 2's performance on challenging reasoning\nbenchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as\nmuch as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER\noutperforms inference-intensive methods such as CoT-Self-Consistency by more\nthan 20%, while requiring 10-40x fewer inference compute. Finally, we show that\nthe self-discovered reasoning structures are universally applicable across\nmodel families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share\ncommonalities with human reasoning patterns.",
        "pdf_link": "https://arxiv.org/pdf/2402.03620v1.pdf"
    },
    {
        "title": "A Computational Framework for Behavioral Assessment of LLM Therapists",
        "authors": [
            "Yu Ying Chiu",
            "Ashish Sharma",
            "Inna Wanyin Lin",
            "Tim Althoff"
        ],
        "published": "2024-01-01T17:32:28Z",
        "summary": "The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.",
        "pdf_link": "https://arxiv.org/pdf/2401.00820v1.pdf"
    },
    {
        "title": "Investigating Text Shortening Strategy in BERT: Truncation vs Summarization",
        "authors": [
            "Mirza Alim Mutasodirin",
            "Radityo Eko Prasojo"
        ],
        "published": "2024-03-19T15:01:14Z",
        "summary": "The parallelism of Transformer-based models comes at the cost of their input\nmax-length. Some studies proposed methods to overcome this limitation, but none\nof them reported the effectiveness of summarization as an alternative. In this\nstudy, we investigate the performance of document truncation and summarization\nin text classification tasks. Each of the two was investigated with several\nvariations. This study also investigated how close their performances are to\nthe performance of full-text. We used a dataset of summarization tasks based on\nIndonesian news articles (IndoSum) to do classification tests. This study shows\nhow the summaries outperform the majority of truncation method variations and\nlose to only one. The best strategy obtained in this study is taking the head\nof the document. The second is extractive summarization. This study explains\nwhat happened to the result, leading to further research in order to exploit\nthe potential of document summarization as a shortening alternative. The code\nand data used in this work are publicly available in\nhttps://github.com/mirzaalimm/TruncationVsSummarization.",
        "pdf_link": "https://arxiv.org/pdf/2403.12799v1.pdf"
    },
    {
        "title": "ChatCell: Facilitating Single-Cell Analysis with Natural Language",
        "authors": [
            "Yin Fang",
            "Kangwei Liu",
            "Ningyu Zhang",
            "Xinle Deng",
            "Penghui Yang",
            "Zhuo Chen",
            "Xiangru Tang",
            "Mark Gerstein",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "published": "2024-02-13T09:06:14Z",
        "summary": "As Large Language Models (LLMs) rapidly evolve, their influence in science is\nbecoming increasingly prominent. The emerging capabilities of LLMs in task\ngeneralization and free-form dialogue can significantly advance fields like\nchemistry and biology. However, the field of single-cell biology, which forms\nthe foundational building blocks of living organisms, still faces several\nchallenges. High knowledge barriers and limited scalability in current methods\nrestrict the full exploitation of LLMs in mastering single-cell data, impeding\ndirect accessibility and rapid iteration. To this end, we introduce ChatCell,\nwhich signifies a paradigm shift by facilitating single-cell analysis with\nnatural language. Leveraging vocabulary adaptation and unified sequence\ngeneration, ChatCell has acquired profound expertise in single-cell biology and\nthe capability to accommodate a diverse range of analysis tasks. Extensive\nexperiments further demonstrate ChatCell's robust performance and potential to\ndeepen single-cell insights, paving the way for more accessible and intuitive\nexploration in this pivotal field. Our project homepage is available at\nhttps://zjunlp.github.io/project/ChatCell.",
        "pdf_link": "https://arxiv.org/pdf/2402.08303v4.pdf"
    },
    {
        "title": "BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text",
        "authors": [
            "Elliot Bolton",
            "Abhinav Venigalla",
            "Michihiro Yasunaga",
            "David Hall",
            "Betty Xiong",
            "Tony Lee",
            "Roxana Daneshjou",
            "Jonathan Frankle",
            "Percy Liang",
            "Michael Carbin",
            "Christopher D. Manning"
        ],
        "published": "2024-03-27T10:18:21Z",
        "summary": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.18421v1.pdf"
    },
    {
        "title": "Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models",
        "authors": [
            "Huachuan Qiu",
            "Shuai Zhang",
            "Hongliang He",
            "Anqi Li",
            "Zhenzhong Lan"
        ],
        "published": "2024-03-20T02:29:09Z",
        "summary": "Pornographic content occurring in human-machine interaction dialogues can\ncause severe side effects for users in open-domain dialogue systems. However,\nresearch on detecting pornographic language within human-machine interaction\ndialogues is an important subject that is rarely studied. To advance in this\ndirection, we introduce CensorChat, a dialogue monitoring dataset aimed at\ndetecting whether the dialogue session contains pornographic content. To this\nend, we collect real-life human-machine interaction dialogues in the wild and\nbreak them down into single utterances and single-turn dialogues, with the last\nutterance spoken by the chatbot. We propose utilizing knowledge distillation of\nlarge language models to annotate the dataset. Specifically, first, the raw\ndataset is annotated by four open-source large language models, with the\nmajority vote determining the label. Second, we use ChatGPT to update the empty\nlabel from the first step. Third, to ensure the quality of the validation and\ntest sets, we utilize GPT-4 for label calibration. If the current label does\nnot match the one generated by GPT-4, we employ a self-criticism strategy to\nverify its correctness. Finally, to facilitate the detection of pornographic\ntext, we develop a series of text classifiers using a pseudo-labeled dataset.\nDetailed data analysis demonstrates that leveraging knowledge distillation\ntechniques with large language models provides a practical and cost-efficient\nmethod for developing pornographic text detectors.",
        "pdf_link": "https://arxiv.org/pdf/2403.13250v1.pdf"
    },
    {
        "title": "Differentially Private Knowledge Distillation via Synthetic Text Generation",
        "authors": [
            "James Flemings",
            "Murali Annavaram"
        ],
        "published": "2024-03-01T19:22:24Z",
        "summary": "Large Language models (LLMs) are achieving state-of-the-art performance in\nmany different downstream tasks. However, the increasing urgency of data\nprivacy requires LLMs to train with Differential Privacy (DP) on private data.\nConcurrently it is also necessary to compress LLMs for real-life deployments on\nresource-constrained devices or latency-sensitive applications. Differential\nprivacy and model compression generally must trade off utility loss to achieve\ntheir objectives. Moreover, concurrently achieving both can result in even more\nutility loss. To this end, we propose a novel differentially private knowledge\ndistillation algorithm that exploits synthetic data generated by a\ndifferentially private LLM. The knowledge of a teacher model is transferred\nonto the student in two ways: one way from the synthetic data itself, the hard\nlabels, and the other way by the output distribution of the teacher model\nevaluated on the synthetic data, the soft labels. Furthermore, if the teacher\nand student share a similar architectural structure, we can further distill\nknowledge by exploiting hidden representations. Our results show that our\nframework substantially improves the utility over existing baselines with\nstrong privacy parameters, {\\epsilon} = 2, validating that we can successfully\ncompress autoregressive LLMs while preserving the privacy of training data.",
        "pdf_link": "https://arxiv.org/pdf/2403.00932v1.pdf"
    },
    {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": [
            "Xiangru Tang",
            "Qiao Jin",
            "Kunlun Zhu",
            "Tongxin Yuan",
            "Yichi Zhang",
            "Wangchunshu Zhou",
            "Meng Qu",
            "Yilun Zhao",
            "Jian Tang",
            "Zhuosheng Zhang",
            "Arman Cohan",
            "Zhiyong Lu",
            "Mark Gerstein"
        ],
        "published": "2024-02-06T18:54:07Z",
        "summary": "Intelligent agents powered by large language models (LLMs) have demonstrated\nsubstantial promise in autonomously conducting experiments and facilitating\nscientific discoveries across various disciplines. While their capabilities are\npromising, they also introduce novel vulnerabilities that demand careful\nconsideration for safety. However, there exists a notable gap in the\nliterature, as there has been no comprehensive exploration of these\nvulnerabilities. This position paper fills this gap by conducting a thorough\nexamination of vulnerabilities in LLM-based agents within scientific domains,\nshedding light on potential risks associated with their misuse and emphasizing\nthe need for safety measures. We begin by providing a comprehensive overview of\nthe potential risks inherent to scientific LLM agents, taking into account user\nintent, the specific scientific domain, and their potential impact on the\nexternal environment. Then, we delve into the origins of these vulnerabilities\nand provide a scoping review of the limited existing works. Based on our\nanalysis, we propose a triadic framework involving human regulation, agent\nalignment, and an understanding of environmental feedback (agent regulation) to\nmitigate these identified risks. Furthermore, we highlight the limitations and\nchallenges associated with safeguarding scientific agents and advocate for the\ndevelopment of improved models, robust benchmarks, and comprehensive\nregulations to address these issues effectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.04247v2.pdf"
    },
    {
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements",
        "authors": [
            "Alex Havrilla",
            "Sharath Raparthy",
            "Christoforus Nalmpantis",
            "Jane Dwivedi-Yu",
            "Maksym Zhuravinskyi",
            "Eric Hambro",
            "Roberta Railneau"
        ],
        "published": "2024-02-13T20:16:29Z",
        "summary": "State-of-the-art language models can exhibit impressive reasoning refinement\ncapabilities on math, science or coding tasks. However, recent work\ndemonstrates that even the best models struggle to identify \\textit{when and\nwhere to refine} without access to external feedback. Outcome-based Reward\nModels (\\textbf{ORMs}), trained to predict correctness of the final answer\nindicating when to refine, offer one convenient solution for deciding when to\nrefine. Process Based Reward Models (\\textbf{PRMs}), trained to predict\ncorrectness of intermediate steps, can then be used to indicate where to\nrefine. But they are expensive to train, requiring extensive human annotations.\nIn this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained,\nonly on synthetic data, to approximate the expected future reward of the\noptimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict\nthe correctness of the final answer when sampling the current policy many times\n(rather than only once as in the case of ORMs). Our experiments show that SORMs\ncan more accurately detect incorrect reasoning steps compared to ORMs, thus\nimproving downstream accuracy when doing refinements. We then train\n\\textit{global} refinement models, which take only the question and a draft\nsolution as input and predict a corrected solution, and \\textit{local}\nrefinement models which also take as input a critique indicating the location\nof the first reasoning error. We generate training data for both models\nsynthetically by reusing data used to train the SORM. We find combining global\nand local refinements, using the ORM as a reranker, significantly outperforms\neither one individually, as well as a best of three sample baseline. With this\nstrategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned\nwith RL) on GSM8K from 53\\% to 65\\% when greedily sampled.",
        "pdf_link": "https://arxiv.org/pdf/2402.10963v1.pdf"
    },
    {
        "title": "Multimodal Embodied Interactive Agent for Cafe Scene",
        "authors": [
            "Yang Liu",
            "Xinshuai Song",
            "Kaixuan Jiang",
            "Weixing Chen",
            "Jingzhou Luo",
            "Guanbin Li",
            "Liang Lin"
        ],
        "published": "2024-02-01T02:43:20Z",
        "summary": "With the surge in the development of large language models, embodied\nintelligence has attracted increasing attention. Nevertheless, prior works on\nembodied intelligence typically encode scene or historical memory in an\nunimodal manner, either visual or linguistic, which complicates the alignment\nof the model's action planning with embodied control. To overcome this\nlimitation, we introduce the Multimodal Embodied Interactive Agent (MEIA),\ncapable of translating high-level tasks expressed in natural language into a\nsequence of executable actions. Specifically, we propose a novel Multimodal\nEnvironment Memory (MEM) module, facilitating the integration of embodied\ncontrol with large models through the visual-language memory of scenes. This\ncapability enables MEIA to generate executable action plans based on diverse\nrequirements and the robot's capabilities. We conduct experiments in a dynamic\nvirtual cafe environment, utilizing multiple large models through zero-shot\nlearning, and carefully design scenarios for various situations. The\nexperimental results showcase the promising performance of our MEIA in various\nembodied interactive tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.00290v1.pdf"
    },
    {
        "title": "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
        "authors": [
            "Siwei Yang",
            "Bingchen Zhao",
            "Cihang Xie"
        ],
        "published": "2024-02-14T18:59:33Z",
        "summary": "This paper introduces AQA-Bench, a novel benchmark to assess the sequential\nreasoning capabilities of large language models (LLMs) in algorithmic contexts,\nsuch as depth-first search (DFS). The key feature of our evaluation benchmark\nlies in its interactive evaluation protocol -- for example, in DFS, the\navailability of each node's connected edge is contingent upon the model's\ntraversal to that node, thereby necessitating the LLM's ability to effectively\nremember visited nodes and strategize subsequent moves. We comprehensively\nbuild AQA-Bench with three different algorithms, namely binary search,\ndepth-first search, and breadth-first search, and to evaluate the sequential\nreasoning ability of 12 different LLMs. Our investigations reveal several\ninteresting findings: (1) Closed-source models like GPT-4 and Gemini generally\nshow strong sequential reasoning ability, significantly outperforming\nopen-source LLMs. (2) Naively providing interactive examples may inadvertently\nhurt few-shot performance. (3) A very limited number of predecessor steps\nfollowing the optimal policy can substantially boost small models' performance.\n(4) The scaling correlation between performance and model size is not always\nsignificant, sometimes even showcasing an inverse trend. We hope our study can\ncatalyze future work on advancing the understanding and enhancement of LLMs'\ncapabilities in sequential reasoning. The code is available at\nhttps://github.com/UCSC-VLAA/AQA-Bench.",
        "pdf_link": "https://arxiv.org/pdf/2402.09404v1.pdf"
    },
    {
        "title": "TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese",
        "authors": [
            "Nicholas Kluge Corr\u00eaa",
            "Sophia Falk",
            "Shiza Fatimah",
            "Aniket Sen",
            "Nythamar de Oliveira"
        ],
        "published": "2024-01-30T00:25:54Z",
        "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing, but their progress has yet to be equal across languages. While most\nLLMs are trained in high-resource languages like English, multilingual models\ngenerally underperform monolingual ones. Additionally, aspects of their\nmultilingual foundation sometimes restrict the byproducts they produce, like\ncomputational demands and licensing regimes. In this study, we document the\ndevelopment of open-foundation models tailored for use in low-resource\nsettings, their limitations, and their benefits. This is the TeenyTinyLlama\npair: two compact models for Brazilian Portuguese text generation. We release\nthem under the permissive Apache 2.0 license on GitHub and Hugging Face for\ncommunity use and further development. See\nhttps://github.com/Nkluge-correa/TeenyTinyLlama",
        "pdf_link": "https://arxiv.org/pdf/2401.16640v2.pdf"
    },
    {
        "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
        "authors": [
            "Letian Peng",
            "Zilong Wang",
            "Feng Yao",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2024-03-30T19:43:45Z",
        "summary": "Information extraction (IE) is a fundamental area in natural language\nprocessing where prompting large language models (LLMs), even with in-context\nexamples, cannot defeat small LMs tuned on very small IE datasets. We observe\nthat IE tasks, such as named entity recognition and relation extraction, all\nfocus on extracting important information, which can be formalized as a\nlabel-to-span matching. In this paper, we propose a novel framework MetaIE to\nbuild a small LM as meta-model by learning to extract \"important information\",\ni.e., the meta-understanding of IE, so that this meta-model can be adapted to\nall kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains\nthe small LM via a symbolic distillation from an LLM following the\nlabel-to-span scheme. We construct the distillation dataset via sampling\nsentences from language model pre-training datasets (e.g., OpenWebText in our\nimplementation) and prompting an LLM to identify the typed spans of \"important\ninformation\". We evaluate the meta-model under the few-shot adaptation setting.\nExtensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer\na better starting point for few-shot tuning on IE datasets and outperform other\nmeta-models from (1) vanilla language model pre-training, (2) multi-IE-task\npre-training with human annotations, and (3) single-IE-task symbolic\ndistillation from LLM. Moreover, we provide comprehensive analyses of MetaIE,\nsuch as the size of the distillation dataset, the meta-model architecture, and\nthe size of the meta-model.",
        "pdf_link": "https://arxiv.org/pdf/2404.00457v1.pdf"
    },
    {
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "authors": [
            "Andrey Gromov",
            "Kushal Tirumala",
            "Hassan Shapourian",
            "Paolo Glorioso",
            "Daniel A. Roberts"
        ],
        "published": "2024-03-26T17:20:04Z",
        "summary": "We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2403.17887v1.pdf"
    },
    {
        "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models",
        "authors": [
            "Chaoqun Liu",
            "Wenxuan Zhang",
            "Yiran Zhao",
            "Anh Tuan Luu",
            "Lidong Bing"
        ],
        "published": "2024-03-15T12:47:39Z",
        "summary": "Large language models (LLMs) have demonstrated strong multilingual\ncapabilities; yet, they are mostly English-centric due to the imbalanced\ntraining corpora. Existing works leverage this phenomenon to improve their\nmultilingual performances on NLP tasks. In this work, we extend the evaluation\nfrom NLP tasks to real user queries. We find that even though translation into\nEnglish can help improve the performance of multilingual NLP tasks for\nEnglish-centric LLMs, it may not be optimal for all scenarios. For\nculture-related tasks that need deep language understanding, prompting in the\nnative language proves to be more promising since it can capture the nuances\nrelated to culture and language. Therefore, we advocate for more efforts\ntowards the development of strong multilingual LLMs instead of just\nEnglish-centric LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.10258v1.pdf"
    },
    {
        "title": "Ranking Large Language Models without Ground Truth",
        "authors": [
            "Amit Dhurandhar",
            "Rahul Nair",
            "Moninder Singh",
            "Elizabeth Daly",
            "Karthikeyan Natesan Ramamurthy"
        ],
        "published": "2024-02-21T00:49:43Z",
        "summary": "Evaluation and ranking of large language models (LLMs) has become an\nimportant problem with the proliferation of these models and their impact.\nEvaluation methods either require human responses which are expensive to\nacquire or use pairs of LLMs to evaluate each other which can be unreliable. In\nthis paper, we provide a novel perspective where, given a dataset of prompts\n(viz. questions, instructions, etc.) and a set of LLMs, we rank them without\naccess to any ground truth or reference responses. Inspired by real life where\nboth an expert and a knowledgeable person can identify a novice our main idea\nis to consider triplets of models, where each one of them evaluates the other\ntwo, correctly identifying the worst model in the triplet with high\nprobability. We also analyze our idea and provide sufficient conditions for it\nto succeed. Applying this idea repeatedly, we propose two methods to rank LLMs.\nIn experiments on different generative tasks (summarization, multiple-choice,\nand dialog), our methods reliably recover close to true rankings without\nreference data. This points to a viable low-resource mechanism for practical\nuse.",
        "pdf_link": "https://arxiv.org/pdf/2402.14860v2.pdf"
    },
    {
        "title": "T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives",
        "authors": [
            "Suchita Pati",
            "Shaizeen Aga",
            "Mahzabeen Islam",
            "Nuwan Jayasena",
            "Matthew D. Sinclair"
        ],
        "published": "2024-01-30T01:55:34Z",
        "summary": "Large Language Models increasingly rely on distributed techniques for their\ntraining and inference. These techniques require communication across devices\nwhich can reduce scaling efficiency as the number of devices increases. While\nsome distributed techniques can overlap, and thus, hide this communication with\nindependent computations, techniques such as Tensor Parallelism (TP) inherently\nserialize communication with model execution. One approach to hide this\nserialized communication is to interleave it with the producer operation (of\nthe communicated data) in a fine-grained manner. However, this fine-grained\ninterleaving of communication and computation in software can be difficult.\nFurthermore, as with any concurrent execution, it requires compute and memory\nresources to be shared between computation and communication, causing resource\ncontention that reduces overlapping efficacy.\n  To overcome these challenges, we propose T3 which applies hardware-software\nco-design to transparently overlap serialized communication while minimizing\nresource contention with compute. T3 transparently fuses producer operations\nwith the subsequent communication via a simple configuration of the producer's\noutput address space and requires minor software changes. At the hardware\nlevel, T3 adds a lightweight track and trigger mechanism to orchestrate the\nproducer's compute, and communication. It further uses compute-enhanced\nmemories for communication's attendant compute. As a result, T3 reduces\nresource contention, and efficiently overlaps serialized communication with\ncomputation. For important Transformer models like T-NLG, T3 speeds up\ncommunication-heavy sublayers by 30% geomean (max 47%) and reduces data\nmovement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models\nscale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM\nand MT-NLG.",
        "pdf_link": "https://arxiv.org/pdf/2401.16677v1.pdf"
    },
    {
        "title": "A Novel BERT-based Classifier to Detect Political Leaning of YouTube Videos based on their Titles",
        "authors": [
            "Nouar AlDahoul",
            "Talal Rahwan",
            "Yasir Zaki"
        ],
        "published": "2024-02-16T14:44:30Z",
        "summary": "A quarter of US adults regularly get their news from YouTube. Yet, despite\nthe massive political content available on the platform, to date no classifier\nhas been proposed to identify the political leaning of YouTube videos. To fill\nthis gap, we propose a novel classifier based on Bert -- a language model from\nGoogle -- to classify YouTube videos merely based on their titles into six\ncategories, namely: Far Left, Left, Center, Anti-Woke, Right, and Far Right. We\nused a public dataset of 10 million YouTube video titles (under various\ncategories) to train and validate the proposed classifier. We compare the\nclassifier against several alternatives that we trained on the same dataset,\nrevealing that our classifier achieves the highest accuracy (75%) and the\nhighest F1 score (77%). To further validate the classification performance, we\ncollect videos from YouTube channels of numerous prominent news agencies, such\nas Fox News and New York Times, which have widely known political leanings, and\napply our classifier to their video titles. For the vast majority of cases, the\npredicted political leaning matches that of the news agency.",
        "pdf_link": "https://arxiv.org/pdf/2404.04261v1.pdf"
    },
    {
        "title": "Enhancing LLM Safety via Constrained Direct Preference Optimization",
        "authors": [
            "Zixuan Liu",
            "Xiaolin Sun",
            "Zizhan Zheng"
        ],
        "published": "2024-03-04T20:39:24Z",
        "summary": "The rapidly increasing capabilities of large language models (LLMs) raise an\nurgent need to align AI systems with diverse human preferences to\nsimultaneously enhance their usefulness and safety, despite the often\nconflicting nature of these goals. To address this important problem, a\npromising approach is to enforce a safety constraint at the fine-tuning stage\nthrough a constrained Reinforcement Learning from Human Feedback (RLHF)\nframework. This approach, however, is computationally expensive and often\nunstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension\nof the recently proposed Direct Preference Optimization (DPO) approach for\nfine-tuning LLMs that is both efficient and lightweight. By integrating dual\ngradient descent and DPO, our method identifies a nearly optimal trade-off\nbetween helpfulness and harmlessness without using reinforcement learning.\nEmpirically, our approach provides a safety guarantee to LLMs that is missing\nin DPO while achieving significantly higher rewards under the same safety\nconstraint compared to a recently proposed safe RLHF approach.\n  Warning: This paper contains example data that may be offensive or harmful.",
        "pdf_link": "https://arxiv.org/pdf/2403.02475v1.pdf"
    },
    {
        "title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space",
        "authors": [
            "Shaolei Zhang",
            "Tian Yu",
            "Yang Feng"
        ],
        "published": "2024-02-27T14:45:04Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, they sometimes suffer from producing hallucinations,\nparticularly in cases where they may generate untruthful responses despite\npossessing the correct knowledge. In this paper, we propose TruthX, an\ninference-time method to elicit the truthfulness of LLMs by editing their\ninternal representations in truthful space. TruthX employs an auto-encoder to\nmap LLM's representations into semantic and truthful latent spaces\nrespectively, and applies contrastive learning to identify a truthful editing\ndirection within the truthful space. During inference, by editing LLM's\ninternal representations in truthful space, TruthX effectively enhances the\ntruthfulness of LLMs. Experiments show that TruthX effectively improves the\ntruthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark.\nFurther analyses suggest that the truthful space acquired by TruthX plays a\npivotal role in controlling LLM to produce truthful or hallucinatory responses.",
        "pdf_link": "https://arxiv.org/pdf/2402.17811v1.pdf"
    },
    {
        "title": "Lumos : Empowering Multimodal LLMs with Scene Text Recognition",
        "authors": [
            "Ashish Shenoy",
            "Yichao Lu",
            "Srihari Jayakumar",
            "Debojeet Chatterjee",
            "Mohsen Moslehpour",
            "Pierce Chuang",
            "Abhay Harpale",
            "Vikas Bhardwaj",
            "Di Xu",
            "Shicong Zhao",
            "Longfang Zhao",
            "Ankit Ramchandani",
            "Xin Luna Dong",
            "Anuj Kumar"
        ],
        "published": "2024-02-12T19:27:26Z",
        "summary": "We introduce Lumos, the first end-to-end multimodal question-answering system\nwith text understanding capabilities. At the core of Lumos is a Scene Text\nRecognition (STR) component that extracts text from first person point-of-view\nimages, the output of which is used to augment input to a Multimodal Large\nLanguage Model (MM-LLM). While building Lumos, we encountered numerous\nchallenges related to STR quality, overall latency, and model inference. In\nthis paper, we delve into those challenges, and discuss the system\narchitecture, design choices, and modeling techniques employed to overcome\nthese obstacles. We also provide a comprehensive evaluation for each component,\nshowcasing high quality and efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.08017v1.pdf"
    },
    {
        "title": "Reframe Anything: LLM Agent for Open World Video Reframing",
        "authors": [
            "Jiawang Cao",
            "Yongliang Wu",
            "Weiheng Chi",
            "Wenbo Zhu",
            "Ziyue Su",
            "Jay Wu"
        ],
        "published": "2024-03-10T03:29:56Z",
        "summary": "The proliferation of mobile devices and social media has revolutionized\ncontent dissemination, with short-form video becoming increasingly prevalent.\nThis shift has introduced the challenge of video reframing to fit various\nscreen aspect ratios, a process that highlights the most compelling parts of a\nvideo. Traditionally, video reframing is a manual, time-consuming task\nrequiring professional expertise, which incurs high production costs. A\npotential solution is to adopt some machine learning models, such as video\nsalient object detection, to automate the process. However, these methods often\nlack generalizability due to their reliance on specific training data. The\nadvent of powerful large language models (LLMs) open new avenues for AI\ncapabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a\nLLM-based agent that leverages visual foundation models and human instructions\nto restructure visual content for video reframing. RAVA operates in three\nstages: perception, where it interprets user instructions and video content;\nplanning, where it determines aspect ratios and reframing strategies; and\nexecution, where it invokes the editing tools to produce the final video. Our\nexperiments validate the effectiveness of RAVA in video salient object\ndetection and real-world reframing tasks, demonstrating its potential as a tool\nfor AI-powered video editing.",
        "pdf_link": "https://arxiv.org/pdf/2403.06070v1.pdf"
    },
    {
        "title": "Towards Language-Driven Video Inpainting via Multimodal Large Language Models",
        "authors": [
            "Jianzong Wu",
            "Xiangtai Li",
            "Chenyang Si",
            "Shangchen Zhou",
            "Jingkang Yang",
            "Jiangning Zhang",
            "Yining Li",
            "Kai Chen",
            "Yunhai Tong",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "published": "2024-01-18T18:59:13Z",
        "summary": "We introduce a new task -- language-driven video inpainting, which uses\nnatural language instructions to guide the inpainting process. This approach\novercomes the limitations of traditional video inpainting methods that depend\non manually labeled binary masks, a process often tedious and labor-intensive.\nWe present the Remove Objects from Videos by Instructions (ROVI) dataset,\ncontaining 5,650 videos and 9,091 inpainting results, to support training and\nevaluation for this task. We also propose a novel diffusion-based\nlanguage-driven video inpainting framework, the first end-to-end baseline for\nthis task, integrating Multimodal Large Language Models to understand and\nexecute complex language-based inpainting requests effectively. Our\ncomprehensive results showcase the dataset's versatility and the model's\neffectiveness in various language-instructed inpainting scenarios. We will make\ndatasets, code, and models publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2401.10226v1.pdf"
    },
    {
        "title": "Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models",
        "authors": [
            "Derong Xu",
            "Ziheng Zhang",
            "Zhenxi Lin",
            "Xian Wu",
            "Zhihong Zhu",
            "Tong Xu",
            "Xiangyu Zhao",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "published": "2024-03-04T12:16:15Z",
        "summary": "Knowledge graph completion (KGC) is a widely used method to tackle\nincompleteness in knowledge graphs (KGs) by making predictions for missing\nlinks. Description-based KGC leverages pre-trained language models to learn\nentity and relation representations with their names or descriptions, which\nshows promising results. However, the performance of description-based KGC is\nstill limited by the quality of text and the incomplete structure, as it lacks\nsufficient entity descriptions and relies solely on relation names, leading to\nsub-optimal results. To address this issue, we propose MPIKGC, a general\nframework to compensate for the deficiency of contextualized knowledge and\nimprove KGC by querying large language models (LLMs) from various perspectives,\nwhich involves leveraging the reasoning, explanation, and summarization\ncapabilities of LLMs to expand entity descriptions, understand relations, and\nextract structures, respectively. We conducted extensive evaluation of the\neffectiveness and improvement of our framework based on four description-based\nKGC models and four datasets, for both link prediction and triplet\nclassification tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.01972v1.pdf"
    },
    {
        "title": "Microstructures and Accuracy of Graph Recall by Large Language Models",
        "authors": [
            "Yanbang Wang",
            "Hejie Cui",
            "Jon Kleinberg"
        ],
        "published": "2024-02-19T04:29:45Z",
        "summary": "Graphs data is crucial for many applications, and much of it exists in the\nrelations described in textual format. As a result, being able to accurately\nrecall and encode a graph described in earlier text is a basic yet pivotal\nability that LLMs need to demonstrate if they are to perform reasoning tasks\nthat involve graph-structured information. Human performance at graph recall\nhas been studied by cognitive scientists for decades, and has been found to\noften exhibit certain structural patterns of bias that align with human\nhandling of social relationships. To date, however, we know little about how\nLLMs behave in analogous graph recall tasks: do their recalled graphs also\nexhibit certain biased patterns, and if so, how do they compare with humans and\naffect other graph reasoning tasks? In this work, we perform the first\nsystematical study of graph recall by LLMs, investigating the accuracy and\nbiased microstructures (local structural patterns) in their recall. We find\nthat LLMs not only underperform often in graph recall, but also tend to favor\nmore triangles and alternating 2-paths. Moreover, we find that more advanced\nLLMs have a striking dependence on the domain that a real-world graph comes\nfrom -- by yielding the best recall accuracy when the graph is narrated in a\nlanguage style consistent with its original domain.",
        "pdf_link": "https://arxiv.org/pdf/2402.11821v2.pdf"
    },
    {
        "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction",
        "authors": [
            "Xiang Huang",
            "Sitao Cheng",
            "Shanshan Huang",
            "Jiayu Shen",
            "Yong Xu",
            "Chaoyun Zhang",
            "Yuzhong Qu"
        ],
        "published": "2024-03-18T15:39:14Z",
        "summary": "Employing Large Language Models (LLMs) for semantic parsing has achieved\nremarkable success. However, we find existing methods fall short in terms of\nreliability and efficiency when hallucinations are encountered. In this paper,\nwe address these challenges with a framework called QueryAgent, which solves a\nquestion step-by-step and performs step-wise self-correction. We introduce an\nenvironmental feedback-based self-correction method called ERASER. Unlike\ntraditional approaches, ERASER leverages rich environmental feedback in the\nintermediate steps to perform selective and differentiated self-correction only\nwhen necessary. Experimental results demonstrate that QueryAgent notably\noutperforms all previous few-shot methods using only one example on GrailQA and\nGraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms\nof efficiency, including runtime, query overhead, and API invocation costs. By\nleveraging ERASER, we further improve another baseline (i.e., AgentBench) by\napproximately 10 points, revealing the strong transferability of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2403.11886v1.pdf"
    },
    {
        "title": "Large Language Models Powered Context-aware Motion Prediction",
        "authors": [
            "Xiaoji Zheng",
            "Lixiu Wu",
            "Zhijie Yan",
            "Yuanrong Tang",
            "Hao Zhao",
            "Chen Zhong",
            "Bokui Chen",
            "Jiangtao Gong"
        ],
        "published": "2024-03-17T02:06:49Z",
        "summary": "Motion prediction is among the most fundamental tasks in autonomous driving.\nTraditional methods of motion forecasting primarily encode vector information\nof maps and historical trajectory data of traffic participants, lacking a\ncomprehensive understanding of overall traffic semantics, which in turn affects\nthe performance of prediction tasks. In this paper, we utilized Large Language\nModels (LLMs) to enhance the global traffic context understanding for motion\nprediction tasks. We first conducted systematic prompt engineering, visualizing\ncomplex traffic environments and historical trajectory information of traffic\nparticipants into image prompts -- Transportation Context Map (TC-Map),\naccompanied by corresponding text prompts. Through this approach, we obtained\nrich traffic context information from the LLM. By integrating this information\ninto the motion prediction model, we demonstrate that such context can enhance\nthe accuracy of motion predictions. Furthermore, considering the cost\nassociated with LLMs, we propose a cost-effective deployment strategy:\nenhancing the accuracy of motion prediction tasks at scale with 0.7\\%\nLLM-augmented datasets. Our research offers valuable insights into enhancing\nthe understanding of traffic scenes of LLMs and the motion prediction\nperformance of autonomous driving.",
        "pdf_link": "https://arxiv.org/pdf/2403.11057v1.pdf"
    },
    {
        "title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
        "authors": [
            "Ehsan Doostmohammadi",
            "Oskar Holmstr\u00f6m",
            "Marco Kuhlmann"
        ],
        "published": "2024-02-16T15:48:33Z",
        "summary": "Work on instruction-tuned Large Language Models (LLMs) has used automatic\nmethods based on text overlap and LLM judgments as cost-effective alternatives\nto human evaluation. In this paper, we study the reliability of such methods\nacross a broad range of tasks and in a cross-lingual setting. In contrast to\nprevious findings, we observe considerable variability in correlations between\nautomatic methods and human evaluators when scores are differentiated by task\ntype. Specifically, the widely-used ROUGE-L metric strongly correlates with\nhuman judgments for short-answer English tasks but is unreliable in free-form\ngeneration tasks and cross-lingual transfer. The effectiveness of GPT-4 as an\nevaluator depends on including reference answers when prompting for\nassessments, which can lead to overly strict evaluations in free-form\ngeneration tasks. In summary, we find that, while automatic evaluation methods\ncan approximate human judgements under specific conditions, their reliability\nis highly context-dependent. Our findings enhance the understanding of how\nautomatic methods should be applied and interpreted when developing and\nevaluating instruction-tuned LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.10770v1.pdf"
    },
    {
        "title": "MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway Encoding",
        "authors": [
            "Jiageng Wu",
            "Xian Wu",
            "Yefeng Zheng",
            "Jie Yang"
        ],
        "published": "2024-03-11T10:57:45Z",
        "summary": "With appropriate data selection and training techniques, Large Language\nModels (LLMs) have demonstrated exceptional success in various medical\nexaminations and multiple-choice questions. However, the application of LLMs in\nmedical dialogue generation-a task more closely aligned with actual medical\npractice-has been less explored. This gap is attributed to the insufficient\nmedical knowledge of LLMs, which leads to inaccuracies and hallucinated\ninformation in the generated medical responses. In this work, we introduce the\nMedical dialogue with Knowledge enhancement and clinical Pathway encoding\n(MedKP) framework, which integrates an external knowledge enhancement module\nthrough a medical knowledge graph and an internal clinical pathway encoding via\nmedical entities and physician actions. Evaluated with comprehensive metrics,\nour experiments on two large-scale, real-world online medical consultation\ndatasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines\nand mitigates the incidence of hallucinations, achieving a new\nstate-of-the-art. Extensive ablation studies further reveal the effectiveness\nof each component of MedKP. This enhancement advances the development of\nreliable, automated medical consultation responses using LLMs, thereby\nbroadening the potential accessibility of precise and real-time medical\nassistance.",
        "pdf_link": "https://arxiv.org/pdf/2403.06611v1.pdf"
    },
    {
        "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
        "authors": [
            "Bongsu Kang",
            "Jundong Kim",
            "Tae-Rim Yun",
            "Chang-Eop Kim"
        ],
        "published": "2024-01-20T14:59:43Z",
        "summary": "We propose a natural language prompt-based retrieval augmented generation\n(Prompt-RAG), a novel approach to enhance the performance of generative large\nlanguage models (LLMs) in niche domains. Conventional RAG methods mostly\nrequire vector embeddings, yet the suitability of generic LLM-based embedding\nrepresentations for specialized domains remains uncertain. To explore and\nexemplify this point, we compared vector embeddings from Korean Medicine (KM)\nand Conventional Medicine (CM) documents, finding that KM document embeddings\ncorrelated more with token overlaps and less with human-assessed document\nrelatedness, in contrast to CM embeddings. Prompt-RAG, distinct from\nconventional RAG models, operates without the need for embedding vectors. Its\nperformance was assessed through a Question-Answering (QA) chatbot application,\nwhere responses were evaluated for relevance, readability, and informativeness.\nThe results showed that Prompt-RAG outperformed existing models, including\nChatGPT and conventional vector embedding-based RAGs, in terms of relevance and\ninformativeness. Despite challenges like content structuring and response\nlatency, the advancements in LLMs are expected to encourage the use of\nPrompt-RAG, making it a promising tool for other domains in need of RAG\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2401.11246v1.pdf"
    },
    {
        "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models",
        "authors": [
            "Chenyang Song",
            "Xu Han",
            "Zhengyan Zhang",
            "Shengding Hu",
            "Xiyu Shi",
            "Kuai Li",
            "Chen Chen",
            "Zhiyuan Liu",
            "Guangli Li",
            "Tao Yang",
            "Maosong Sun"
        ],
        "published": "2024-02-21T03:58:49Z",
        "summary": "Activation sparsity refers to the existence of considerable\nweakly-contributed elements among activation outputs. As a prevalent property\nof the models using the ReLU activation function, it has been proven a\npromising paradigm to boost model inference efficiency. Nevertheless, most\nlarge language models (LLMs) adopt activation functions without intrinsic\nactivation sparsity (e.g., GELU and Swish). Some recent efforts have explored\nintroducing ReLU or its variants as the substitutive activation function to\nhelp LLMs achieve activation sparsity and inference acceleration, but few can\nsimultaneously obtain high sparsity and comparable model performance. This\npaper introduces an effective sparsification method named \"ProSparse\" to push\nLLMs for higher activation sparsity without decreasing model performance.\nSpecifically, after substituting the activation function of LLMs with ReLU,\nProSparse adopts progressive sparsity regularization with a factor smoothly\nincreasing along sine curves in multiple stages. This can enhance activation\nsparsity and alleviate performance degradation by avoiding radical shifts in\nactivation distribution. With ProSparse, we obtain high sparsity of 89.32% and\n88.80% for LLaMA2-7B and LLaMA2-13B, respectively, achieving comparable\nperformance to their original Swish-activated versions. Our inference\nacceleration experiments further demonstrate the practical acceleration brought\nby higher activation sparsity.",
        "pdf_link": "https://arxiv.org/pdf/2402.13516v2.pdf"
    },
    {
        "title": "Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data",
        "authors": [
            "Yinghao Zhu",
            "Zixiang Wang",
            "Junyi Gao",
            "Yuning Tong",
            "Jingkun An",
            "Weibin Liao",
            "Ewen M. Harrison",
            "Liantao Ma",
            "Chengwei Pan"
        ],
        "published": "2024-01-25T20:14:50Z",
        "summary": "The inherent complexity of structured longitudinal Electronic Health Records\n(EHR) data poses a significant challenge when integrated with Large Language\nModels (LLMs), which are traditionally tailored for natural language\nprocessing. Motivated by the urgent need for swift decision-making during new\ndisease outbreaks, where traditional predictive models often fail due to a lack\nof historical data, this research investigates the adaptability of LLMs, like\nGPT-4, to EHR data. We particularly focus on their zero-shot capabilities,\nwhich enable them to make predictions in scenarios in which they haven't been\nexplicitly trained. In response to the longitudinal, sparse, and\nknowledge-infused nature of EHR data, our prompting approach involves taking\ninto account specific EHR characteristics such as units and reference ranges,\nand employing an in-context learning strategy that aligns with clinical\ncontexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets\ndemonstrate that with our elaborately designed prompting framework, LLMs can\nimprove prediction performance in key tasks such as mortality, length-of-stay,\nand 30-day readmission by about 35\\%, surpassing ML models in few-shot\nsettings. Our research underscores the potential of LLMs in enhancing clinical\ndecision-making, especially in urgent healthcare situations like the outbreak\nof emerging diseases with no labeled data. The code is publicly available at\nhttps://github.com/yhzhu99/llm4healthcare for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2402.01713v2.pdf"
    },
    {
        "title": "Using Zero-shot Prompting in the Automatic Creation and Expansion of Topic Taxonomies for Tagging Retail Banking Transactions",
        "authors": [
            "Daniel de S. Moraes",
            "Pedro T. C. Santos",
            "Polyana B. da Costa",
            "Matheus A. S. Pinto",
            "Ivan de J. P. Pinto",
            "\u00c1lvaro M. G. da Veiga",
            "Sergio Colcher",
            "Antonio J. G. Busson",
            "Rafael H. Rocha",
            "Rennan Gaio",
            "Rafael Miceli",
            "Gabriela Tourinho",
            "Marcos Rabaioli",
            "Leandro Santos",
            "Fellipe Marques",
            "David Favaro"
        ],
        "published": "2024-01-08T00:27:16Z",
        "summary": "This work presents an unsupervised method for automatically constructing and\nexpanding topic taxonomies using instruction-based fine-tuned LLMs (Large\nLanguage Models). We apply topic modeling and keyword extraction techniques to\ncreate initial topic taxonomies and LLMs to post-process the resulting terms\nand create a hierarchy. To expand an existing taxonomy with new terms, we use\nzero-shot prompting to find out where to add new nodes, which, to our\nknowledge, is the first work to present such an approach to taxonomy tasks. We\nuse the resulting taxonomies to assign tags that characterize merchants from a\nretail bank dataset. To evaluate our work, we asked 12 volunteers to answer a\ntwo-part form in which we first assessed the quality of the taxonomies created\nand then the tags assigned to merchants based on that taxonomy. The evaluation\nrevealed a coherence rate exceeding 90% for the chosen taxonomies. The\ntaxonomies' expansion with LLMs also showed exciting results for parent node\nprediction, with an f1-score above 70% in our taxonomies.",
        "pdf_link": "https://arxiv.org/pdf/2401.06790v2.pdf"
    },
    {
        "title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization",
        "authors": [
            "Oscar Ma\u00f1as",
            "Pietro Astolfi",
            "Melissa Hall",
            "Candace Ross",
            "Jack Urbanek",
            "Adina Williams",
            "Aishwarya Agrawal",
            "Adriana Romero-Soriano",
            "Michal Drozdzal"
        ],
        "published": "2024-03-26T15:42:01Z",
        "summary": "Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.17804v1.pdf"
    },
    {
        "title": "Advancing Time Series Classification with Multimodal Language Modeling",
        "authors": [
            "Mingyue Cheng",
            "Yiheng Chen",
            "Qi Liu",
            "Zhiding Liu",
            "Yucong Luo"
        ],
        "published": "2024-03-19T02:32:24Z",
        "summary": "For the advancements of time series classification, scrutinizing previous\nstudies, most existing methods adopt a common learning-to-classify paradigm - a\ntime series classifier model tries to learn the relation between sequence\ninputs and target label encoded by one-hot distribution. Although effective,\nthis paradigm conceals two inherent limitations: (1) encoding target categories\nwith one-hot distribution fails to reflect the comparability and similarity\nbetween labels, and (2) it is very difficult to learn transferable model across\ndomains, which greatly hinder the development of universal serving paradigm. In\nthis work, we propose InstructTime, a novel attempt to reshape time series\nclassification as a learning-to-generate paradigm. Relying on the powerful\ngenerative capacity of the pre-trained language model, the core idea is to\nformulate the classification of time series as a multimodal understanding task,\nin which both task-specific instructions and raw time series are treated as\nmultimodal inputs while the label information is represented by texts. To\naccomplish this goal, three distinct designs are developed in the InstructTime.\nFirstly, a time series discretization module is designed to convert continuous\ntime series into a sequence of hard tokens to solve the inconsistency issue\nacross modal inputs. To solve the modality representation gap issue, for one\nthing, we introduce an alignment projected layer before feeding the transformed\ntoken of time series into language models. For another, we highlight the\nnecessity of auto-regressive pre-training across domains, which can facilitate\nthe transferability of the language model and boost the generalization\nperformance. Extensive experiments are conducted over benchmark datasets, whose\nresults uncover the superior performance of InstructTime and the potential for\na universal foundation model in time series classification.",
        "pdf_link": "https://arxiv.org/pdf/2403.12371v1.pdf"
    },
    {
        "title": "OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining",
        "authors": [
            "Fanjin Zhang",
            "Shijie Shi",
            "Yifan Zhu",
            "Bo Chen",
            "Yukuo Cen",
            "Jifan Yu",
            "Yelin Chen",
            "Lulu Wang",
            "Qingfei Zhao",
            "Yuqing Cheng",
            "Tianyi Han",
            "Yuwei An",
            "Dan Zhang",
            "Weng Lam Tam",
            "Kun Cao",
            "Yunhe Pang",
            "Xinyu Guan",
            "Huihui Yuan",
            "Jian Song",
            "Xiaoyan Li",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published": "2024-02-24T13:15:54Z",
        "summary": "With the rapid proliferation of scientific literature, versatile academic\nknowledge services increasingly rely on comprehensive academic graph mining.\nDespite the availability of public academic graphs, benchmarks, and datasets,\nthese resources often fall short in multi-aspect and fine-grained annotations,\nare constrained to specific task types and domains, or lack underlying real\nacademic graphs. In this paper, we present OAG-Bench, a comprehensive,\nmulti-aspect, and fine-grained human-curated benchmark based on the Open\nAcademic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines,\nand 120+ experimental results to date. We propose new data annotation\nstrategies for certain tasks and offer a suite of data pre-processing codes,\nalgorithm implementations, and standardized evaluation protocols to facilitate\nacademic graph mining. Extensive experiments reveal that even advanced\nalgorithms like large language models (LLMs) encounter difficulties in\naddressing key challenges in certain tasks, such as paper source tracing and\nscholar profiling. We also introduce the Open Academic Graph Challenge\n(OAG-Challenge) to encourage community input and sharing. We envisage that\nOAG-Bench can serve as a common ground for the community to evaluate and\ncompare algorithms in academic graph mining, thereby accelerating algorithm\ndevelopment and advancement in this field. OAG-Bench is accessible at\nhttps://www.aminer.cn/data/.",
        "pdf_link": "https://arxiv.org/pdf/2402.15810v1.pdf"
    },
    {
        "title": "Construction of Hyper-Relational Knowledge Graphs Using Pre-Trained Large Language Models",
        "authors": [
            "Preetha Datta",
            "Fedor Vitiugin",
            "Anastasiia Chizhikova",
            "Nitin Sawhney"
        ],
        "published": "2024-03-18T13:44:48Z",
        "summary": "Extracting hyper-relations is crucial for constructing comprehensive\nknowledge graphs, but there are limited supervised methods available for this\ntask. To address this gap, we introduce a zero-shot prompt-based method using\nOpenAI's GPT-3.5 model for extracting hyper-relational knowledge from text.\nComparing our model with a baseline, we achieved promising results, with a\nrecall of 0.77. Although our precision is currently lower, a detailed analysis\nof the model outputs has uncovered potential pathways for future research in\nthis area.",
        "pdf_link": "https://arxiv.org/pdf/2403.11786v1.pdf"
    },
    {
        "title": "The FinBen: An Holistic Financial Benchmark for Large Language Models",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Zhengyu Chen",
            "Ruoyu Xiang",
            "Xiao Zhang",
            "Yueru He",
            "Mengxi Xiao",
            "Dong Li",
            "Yongfu Dai",
            "Duanyu Feng",
            "Yijing Xu",
            "Haoqiang Kang",
            "Ziyan Kuang",
            "Chenhan Yuan",
            "Kailai Yang",
            "Zheheng Luo",
            "Tianlin Zhang",
            "Zhiwei Liu",
            "Guojun Xiong",
            "Zhiyang Deng",
            "Yuechen Jiang",
            "Zhiyuan Yao",
            "Haohang Li",
            "Yangyang Yu",
            "Gang Hu",
            "Jiajia Huang",
            "Xiao-Yang Liu",
            "Alejandro Lopez-Lira",
            "Benyou Wang",
            "Yanzhao Lai",
            "Hao Wang",
            "Min Peng",
            "Sophia Ananiadou",
            "Jimin Huang"
        ],
        "published": "2024-02-20T02:16:16Z",
        "summary": "LLMs have transformed NLP and shown promise in various fields, yet their\npotential in finance is underexplored due to a lack of thorough evaluations and\nthe complexity of financial tasks. This along with the rapid development of\nLLMs, highlights the urgent need for a systematic financial evaluation\nbenchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive\nopen-sourced evaluation benchmark, specifically designed to thoroughly assess\nthe capabilities of LLMs in the financial domain. FinBen encompasses 35\ndatasets across 23 financial tasks, organized into three spectrums of\ndifficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs'\ncognitive abilities in inductive reasoning, associative memory, quantitative\nreasoning, crystallized intelligence, and more. Our evaluation of 15\nrepresentative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals\ninsights into their strengths and limitations within the financial domain. The\nfindings indicate that GPT-4 leads in quantification, extraction, numerical\nreasoning, and stock trading, while Gemini shines in generation and\nforecasting; however, both struggle with complex extraction and forecasting,\nshowing a clear need for targeted enhancements. Instruction tuning boosts\nsimple task performance but falls short in improving complex reasoning and\nforecasting abilities. FinBen seeks to continuously evaluate LLMs in finance,\nfostering AI development with regular updates of tasks and models.",
        "pdf_link": "https://arxiv.org/pdf/2402.12659v1.pdf"
    },
    {
        "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
        "authors": [
            "Song Guo",
            "Fan Wu",
            "Lei Zhang",
            "Xiawu Zheng",
            "Shengchuan Zhang",
            "Fei Chao",
            "Yiyu Shi",
            "Rongrong Ji"
        ],
        "published": "2024-02-19T09:55:32Z",
        "summary": "Existing methods for fine-tuning sparse LLMs often suffer from\nresource-intensive requirements and high retraining costs. Additionally, many\nfine-tuning methods often rely on approximations or heuristic optimization\nstrategies, which may lead to suboptimal solutions. To address these issues, we\npropose an efficient and fast framework for fine-tuning sparse LLMs based on\nminimizing reconstruction error. Our approach involves sampling a small dataset\nfor calibration and utilizing backpropagation to iteratively optimize\nblock-wise reconstruction error, on a block-by-block basis, aiming for optimal\nsolutions. Extensive experiments on various benchmarks consistently demonstrate\nthe superiority of our method over other baselines. For instance, on the\nWikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a\nperplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of\n75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a\nperplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the\nfine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes,\nand the entire framework can be executed on a single 16GB GPU. The source code\nis available at https://github.com/sunggo/EBFT.",
        "pdf_link": "https://arxiv.org/pdf/2402.12419v1.pdf"
    },
    {
        "title": "Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning",
        "authors": [
            "Shen Li",
            "Liuyi Yao",
            "Jinyang Gao",
            "Lan Zhang",
            "Yaliang Li"
        ],
        "published": "2024-02-22T04:55:14Z",
        "summary": "To support various applications, business owners often seek the customized\nmodels that are obtained by fine-tuning a pre-trained LLM through the API\nprovided by LLM owners or cloud servers. However, this process carries a\nsubstantial risk of model misuse, potentially resulting in severe economic\nconsequences for business owners. Thus, safeguarding the copyright of these\ncustomized models during LLM fine-tuning has become an urgent practical\nrequirement, but there are limited existing solutions to provide such\nprotection. To tackle this pressing issue, we propose a novel watermarking\napproach named \"Double-I watermark\". Specifically, based on the instruct-tuning\ndata, two types of backdoor data paradigms are introduced with trigger in the\ninstruction and the input, respectively. By leveraging LLM's learning\ncapability to incorporate customized backdoor samples into the dataset, the\nproposed approach effectively injects specific watermarking information into\nthe customized model during fine-tuning, which makes it easy to inject and\nverify watermarks in commercial scenarios. We evaluate the proposed \"Double-I\nwatermark\" under various fine-tuning methods, demonstrating its harmlessness,\nrobustness, uniqueness, imperceptibility, and validity through both theoretical\nanalysis and experimental verification.",
        "pdf_link": "https://arxiv.org/pdf/2402.14883v1.pdf"
    },
    {
        "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
        "authors": [
            "Yang Deng",
            "Xuan Zhang",
            "Wenxuan Zhang",
            "Yifei Yuan",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "published": "2024-02-23T02:18:12Z",
        "summary": "Web agents powered by Large Language Models (LLMs) have demonstrated\nremarkable abilities in planning and executing multi-step interactions within\ncomplex web-based environments, fulfilling a wide range of web navigation\ntasks. Despite these advancements, the potential for LLM-powered agents to\neffectively engage with sequential user instructions in real-world scenarios\nhas not been fully explored. In this work, we introduce a new task of\nConversational Web Navigation, which necessitates sophisticated interactions\nthat span multiple turns with both the users and the environment, supported by\na specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To\ntackle the limited context length of LLMs and the context-dependency issue of\nthe conversational tasks, we further propose a novel framework, named\nself-reflective memory-augmented planning (Self-MAP), which employs memory\nutilization and self-reflection techniques. Extensive experiments are conducted\nto benchmark the MT-Mind2Web dataset, and validate the effectiveness of the\nproposed method.",
        "pdf_link": "https://arxiv.org/pdf/2402.15057v1.pdf"
    },
    {
        "title": "Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning",
        "authors": [
            "Gabriel Simmons",
            "Vladislav Savinov"
        ],
        "published": "2024-02-12T01:55:51Z",
        "summary": "This study evaluates the ability of Large Language Model (LLM)-based\nSubpopulation Representative Models (SRMs) to generalize from empirical data,\nutilizing in-context learning with data from the 2016 and 2020 American\nNational Election Studies. We explore generalization across response variables\nand demographic subgroups. While conditioning with empirical data improves\nperformance on the whole, the benefit of in-context learning varies\nconsiderably across demographics, sometimes hurting performance for one\ndemographic while helping performance for others. The inequitable benefits of\nin-context learning for SRM present a challenge for practitioners implementing\nSRMs, and for decision-makers who might come to rely on them. Our work\nhighlights a need for fine-grained benchmarks captured from diverse\nsubpopulations that test not only fidelity but generalization.",
        "pdf_link": "https://arxiv.org/pdf/2402.07368v1.pdf"
    },
    {
        "title": "Construction of a Japanese Financial Benchmark for Large Language Models",
        "authors": [
            "Masanori Hirano"
        ],
        "published": "2024-03-22T09:40:27Z",
        "summary": "With the recent development of large language models (LLMs), models that\nfocus on certain domains and languages have been discussed for their necessity.\nThere is also a growing need for benchmarks to evaluate the performance of\ncurrent LLMs in each domain. Therefore, in this study, we constructed a\nbenchmark comprising multiple tasks specific to the Japanese and financial\ndomains and performed benchmark measurements on some models. Consequently, we\nconfirmed that GPT-4 is currently outstanding, and that the constructed\nbenchmarks function effectively. According to our analysis, our benchmark can\ndifferentiate benchmark scores among models in all performance ranges by\ncombining tasks with different difficulties.",
        "pdf_link": "https://arxiv.org/pdf/2403.15062v1.pdf"
    },
    {
        "title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues",
        "authors": [
            "Ge Bai",
            "Jie Liu",
            "Xingyuan Bu",
            "Yancheng He",
            "Jiaheng Liu",
            "Zhanhui Zhou",
            "Zhuoran Lin",
            "Wenbo Su",
            "Tiezheng Ge",
            "Bo Zheng",
            "Wanli Ouyang"
        ],
        "published": "2024-02-22T18:21:59Z",
        "summary": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue\nsystems. However, comprehensively evaluating the dialogue abilities of LLMs\nremains a challenge. Previous benchmarks have primarily focused on single-turn\ndialogues or provided coarse-grained and incomplete assessments of multi-turn\ndialogues, overlooking the complexity and fine-grained nuances of real-life\ndialogues. To address this issue, we introduce MT-Bench-101, specifically\ndesigned to evaluate the fine-grained abilities of LLMs in multi-turn\ndialogues. By conducting a detailed analysis of real multi-turn dialogue data,\nwe construct a three-tier hierarchical ability taxonomy comprising 4208 turns\nacross 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21\npopular LLMs based on MT-Bench-101, conducting comprehensive analyses from both\nability and task perspectives and observing differing trends in LLMs\nperformance across dialogue turns within various tasks. Further analysis\nindicates that neither utilizing common alignment techniques nor chat-specific\ndesigns has led to obvious enhancements in the multi-turn abilities of LLMs.\nExtensive case studies suggest that our designed tasks accurately assess the\ncorresponding multi-turn abilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.14762v1.pdf"
    },
    {
        "title": "If in a Crowdsourced Data Annotation Pipeline, a GPT-4",
        "authors": [
            "Zeyu He",
            "Chieh-Yang Huang",
            "Chien-Kuang Cornelia Ding",
            "Shaurya Rohatgi",
            "Ting-Hao 'Kenneth' Huang"
        ],
        "published": "2024-02-26T18:08:52Z",
        "summary": "Recent studies indicated GPT-4 outperforms online crowd workers in data\nlabeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).\nHowever, these studies were criticized for deviating from standard\ncrowdsourcing practices and emphasizing individual workers' performances over\nthe whole data-annotation process. This paper compared GPT-4 and an ethical and\nwell-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments\nfrom 200 scholarly articles using the CODA-19 scheme. Two worker interfaces\nyielded 127,080 labels, which were then used to infer the final labels through\neight label-aggregation algorithms. Our evaluation showed that despite best\npractices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved\n83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected\nvia an advanced worker interface for aggregation, 2 out of the 8 algorithms\nachieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested\nthat, when the crowd's and GPT-4's labeling strengths are complementary,\naggregating them could increase labeling accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2402.16795v1.pdf"
    },
    {
        "title": "Elysium: Exploring Object-level Perception in Videos via MLLM",
        "authors": [
            "Han Wang",
            "Yanjie Wang",
            "Yongjie Ye",
            "Yuxiang Nie",
            "Can Huang"
        ],
        "published": "2024-03-25T09:17:15Z",
        "summary": "Multi-modal Large Language Models (MLLMs) have demonstrated their ability to\nperceive objects in still images, but their application in video-related tasks,\nsuch as object tracking, remains understudied. This lack of exploration is\nprimarily due to two key challenges. Firstly, extensive pretraining on\nlarge-scale video datasets is required to equip MLLMs with the capability to\nperceive objects across multiple frames and understand inter-frame\nrelationships. Secondly, processing a large number of frames within the context\nwindow of Large Language Models (LLMs) can impose a significant computational\nburden. To address the first challenge, we introduce ElysiumTrack-1M, a\nlarge-scale video dataset supported for three tasks: Single Object Tracking\n(SOT), Referring Single Object Tracking (RSOT), and Video Referring Expression\nGeneration (Video-REG). ElysiumTrack-1M contains 1.27 million annotated video\nframes with corresponding object boxes and descriptions. Leveraging this\ndataset, we conduct training of MLLMs and propose a token-compression model\nT-Selector to tackle the second challenge. Our proposed approach, Elysium:\nExploring Object-level Perception in Videos via MLLM, is an end-to-end\ntrainable MLLM that attempts to conduct object-level tasks in videos without\nrequiring any additional plug-in or expert models. All codes and datasets are\navailable at https://github.com/Hon-Wong/Elysium.",
        "pdf_link": "https://arxiv.org/pdf/2403.16558v2.pdf"
    },
    {
        "title": "Collage Prompting: Budget-Friendly Visual Recognition with GPT-4V",
        "authors": [
            "Siyu Xu",
            "Yunke Wang",
            "Daochang Liu",
            "Chang Xu"
        ],
        "published": "2024-03-18T04:41:38Z",
        "summary": "Recent advancements in generative AI have suggested that by taking visual\nprompt, GPT-4V can demonstrate significant proficiency in image recognition\ntask. Despite its impressive capabilities, the financial cost associated with\nGPT-4V's inference presents a substantial barrier for its wide use. To address\nthis challenge, our work introduces Collage Prompting, a budget-friendly\nprompting approach that concatenates multiple images into a single visual\ninput. With collage prompt, GPT-4V is able to perform image recognition on\nseveral images simultaneously. Based on the observation that the accuracy of\nGPT-4V's image recognition varies significantly with the order of images within\nthe collage prompt, our method further learns to optimize the arrangement of\nimages for maximum recognition accuracy. A graph predictor is trained to\nindicate the accuracy of each collage prompt, then we propose an optimization\nmethod to navigate the search space of possible image arrangements. Experiment\nresults across various datasets demonstrate the cost-efficiency score of\ncollage prompt is much larger than standard prompt. Additionally, collage\nprompt with learned arrangement achieves clearly better accuracy than collage\nprompt with random arrangement in GPT-4V's visual recognition.",
        "pdf_link": "https://arxiv.org/pdf/2403.11468v1.pdf"
    },
    {
        "title": "Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text",
        "authors": [
            "Sara Abdali",
            "Richard Anarfi",
            "CJ Barberan",
            "Jia He"
        ],
        "published": "2024-03-09T01:13:54Z",
        "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Generation (NLG) by demonstrating an impressive ability to generate\nhuman-like text. However, their widespread usage introduces challenges that\nnecessitate thoughtful examination, ethical scrutiny, and responsible\npractices. In this study, we delve into these challenges, explore existing\nstrategies for mitigating them, with a particular emphasis on identifying\nAI-generated text as the ultimate solution. Additionally, we assess the\nfeasibility of detection from a theoretical perspective and propose novel\nresearch directions to address the current limitations in this domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.05750v1.pdf"
    },
    {
        "title": "User-LLM: Efficient LLM Contextualization with User Embeddings",
        "authors": [
            "Lin Ning",
            "Luyang Liu",
            "Jiaxing Wu",
            "Neo Wu",
            "Devora Berlowitz",
            "Sushant Prakash",
            "Bradley Green",
            "Shawn O'Banion",
            "Jun Xie"
        ],
        "published": "2024-02-21T08:03:27Z",
        "summary": "Large language models (LLMs) have revolutionized natural language processing.\nHowever, effectively incorporating complex and potentially noisy user\ninteraction data remains a challenge. To address this, we propose User-LLM, a\nnovel framework that leverages user embeddings to contextualize LLMs. These\nembeddings, distilled from diverse user interactions using self-supervised\npretraining, capture latent user preferences and their evolution over time. We\nintegrate these user embeddings with LLMs through cross-attention and\nsoft-prompting, enabling LLMs to dynamically adapt to user context. Our\ncomprehensive experiments on MovieLens, Amazon Review, and Google Local Review\ndatasets demonstrate significant performance gains across various tasks.\nNotably, our approach outperforms text-prompt-based contextualization on long\nsequence tasks and tasks that require deep user understanding while being\ncomputationally efficient. We further incorporate Perceiver layers to\nstreamline the integration between user encoders and LLMs, reducing\ncomputational demands.",
        "pdf_link": "https://arxiv.org/pdf/2402.13598v1.pdf"
    },
    {
        "title": "LeanReasoner: Boosting Complex Logical Reasoning with Lean",
        "authors": [
            "Dongwei Jiang",
            "Marcio Fonseca",
            "Shay B. Cohen"
        ],
        "published": "2024-03-20T05:29:06Z",
        "summary": "Large language models (LLMs) often struggle with complex logical reasoning\ndue to logical inconsistencies and the inherent difficulty of such reasoning.\nWe use Lean, a theorem proving framework, to address these challenges. By\nformalizing logical reasoning problems into theorems within Lean, we can solve\nthem by proving or disproving the corresponding theorems. This method reduces\nthe risk of logical inconsistencies with the help of Lean's symbolic solver. It\nalso enhances our ability to treat complex reasoning tasks by using Lean's\nextensive library of theorem proofs. Our method achieves state-of-the-art\nperformance on the FOLIO dataset and achieves performance near this level on\nProofWriter. Notably, these results were accomplished by fine-tuning on fewer\nthan 100 in-domain samples for each dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.13312v1.pdf"
    },
    {
        "title": "From Text to CQL: Bridging Natural Language and Corpus Search Engine",
        "authors": [
            "Luming Lu",
            "Jiyuan An",
            "Yujie Wang",
            "Liner yang",
            "Cunliang Kong",
            "Zhenghao Liu",
            "Shuo Wang",
            "Haozhe Lin",
            "Mingwei Fang",
            "Yaping Huang",
            "Erhong Yang"
        ],
        "published": "2024-02-21T12:11:28Z",
        "summary": "Natural Language Processing (NLP) technologies have revolutionized the way we\ninteract with information systems, with a significant focus on converting\nnatural language queries into formal query languages such as SQL. However, less\nemphasis has been placed on the Corpus Query Language (CQL), a critical tool\nfor linguistic research and detailed analysis within text corpora. The manual\nconstruction of CQL queries is a complex and time-intensive task that requires\na great deal of expertise, which presents a notable challenge for both\nresearchers and practitioners. This paper presents the first text-to-CQL task\nthat aims to automate the translation of natural language into CQL. We present\na comprehensive framework for this task, including a specifically curated\nlarge-scale dataset and methodologies leveraging large language models (LLMs)\nfor effective text-to-CQL task. In addition, we established advanced evaluation\nmetrics to assess the syntactic and semantic accuracy of the generated queries.\nWe created innovative LLM-based conversion approaches and detailed experiments.\nThe results demonstrate the efficacy of our methods and provide insights into\nthe complexities of text-to-CQL task.",
        "pdf_link": "https://arxiv.org/pdf/2402.13740v1.pdf"
    },
    {
        "title": "Training microrobots to swim by a large language model",
        "authors": [
            "Zhuoqun Xu",
            "Lailai Zhu"
        ],
        "published": "2024-01-21T12:18:59Z",
        "summary": "Machine learning and artificial intelligence have recently represented a\npopular paradigm for designing and optimizing robotic systems across various\nscales. Recent studies have showcased the innovative application of large\nlanguage models (LLMs) in industrial control [1] and in directing legged\nwalking robots [2]. In this study, we utilize an LLM, GPT-4, to train two\nprototypical microrobots for swimming in viscous fluids. Adopting a few-shot\nlearning approach, we develop a minimal, unified prompt composed of only five\nsentences. The same concise prompt successfully guides two distinct articulated\nmicrorobots -- the three-link swimmer and the three-sphere swimmer -- in\nmastering their signature strokes. These strokes, initially conceptualized by\nphysicists, are now effectively interpreted and applied by the LLM, enabling\nthe microrobots to circumvent the physical constraints inherent to\nmicro-locomotion. Remarkably, our LLM-based decision-making strategy\nsubstantially surpasses a traditional reinforcement learning method in terms of\ntraining speed. We discuss the nuanced aspects of prompt design, particularly\nemphasizing the reduction of monetary expenses of using GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.00044v1.pdf"
    },
    {
        "title": "Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications",
        "authors": [
            "Rushang Karia",
            "Daksh Dobhal",
            "Daniel Bramblett",
            "Pulkit Verma",
            "Siddharth Srivastava"
        ],
        "published": "2024-03-27T08:08:00Z",
        "summary": "Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.18327v1.pdf"
    },
    {
        "title": "MolTC: Towards Molecular Relational Modeling In Language Models",
        "authors": [
            "Junfeng Fang",
            "Shuai Zhang",
            "Chang Wu",
            "Zhengyi Yang",
            "Zhiyuan Liu",
            "Sihang Li",
            "Kun Wang",
            "Wenjie Du",
            "Xiang Wang"
        ],
        "published": "2024-02-06T07:51:56Z",
        "summary": "Molecular Relational Learning (MRL), aiming to understand interactions\nbetween molecular pairs, plays a pivotal role in advancing biochemical\nresearch. Recently, the adoption of large language models (LLMs), known for\ntheir vast knowledge repositories and advanced logical inference capabilities,\nhas emerged as a promising way for efficient and effective MRL. Despite their\npotential, these methods predominantly rely on the textual data, thus not fully\nharnessing the wealth of structural information inherent in molecular graphs.\nMoreover, the absence of a unified framework exacerbates the issue of\ninformation underutilization, as it hinders the sharing of interaction\nmechanism learned across diverse datasets. To address these challenges, this\nwork proposes a novel LLM-based multi-modal framework for Molecular inTeraction\nprediction following Chain-of-Thought (CoT) theory, termed MolTC, which\neffectively integrate graphical information of two molecules in pair. For\nachieving a unified MRL, MolTC innovatively develops a dynamic\nparameter-sharing strategy for cross-dataset information sharing. Moreover, to\ntrain MolTC efficiently, we introduce a Multi-hierarchical CoT concept to\nrefine its training paradigm, and conduct a comprehensive Molecular Interactive\nInstructions dataset for the development of biochemical LLMs involving MRL. Our\nexperiments, conducted across various datasets involving over 4,000,000\nmolecular pairs, exhibit the superiority of our method over current GNN and\nLLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC.",
        "pdf_link": "https://arxiv.org/pdf/2402.03781v5.pdf"
    },
    {
        "title": "Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges",
        "authors": [
            "Bosheng Ding",
            "Chengwei Qin",
            "Ruochen Zhao",
            "Tianze Luo",
            "Xinze Li",
            "Guizhen Chen",
            "Wenhan Xia",
            "Junjie Hu",
            "Anh Tuan Luu",
            "Shafiq Joty"
        ],
        "published": "2024-03-05T14:11:54Z",
        "summary": "In the rapidly evolving field of machine learning (ML), data augmentation\n(DA) has emerged as a pivotal technique for enhancing model performance by\ndiversifying training examples without the need for additional data collection.\nThis survey explores the transformative impact of Large Language Models (LLMs)\non DA, particularly addressing the unique challenges and opportunities they\npresent in the context of natural language processing (NLP) and beyond. From a\ndata perspective and a learning perspective, we examine various strategies that\nutilize Large Language Models for data augmentation, including a novel\nexploration of learning paradigms where LLM-generated data is used for further\ntraining. Additionally, this paper delineates the primary challenges faced in\nthis domain, ranging from controllable data augmentation to multi modal data\naugmentation. This survey highlights the paradigm shift introduced by LLMs in\nDA, aims to serve as a foundational guide for researchers and practitioners in\nthis field.",
        "pdf_link": "https://arxiv.org/pdf/2403.02990v1.pdf"
    },
    {
        "title": "Enhancing Retrieval Processes for Language Generation with Augmented Queries",
        "authors": [
            "Julien Pierre Edmond Ghali",
            "Kosuke Shima",
            "Koichi Moriyama",
            "Atsuko Mutoh",
            "Nobuhiro Inuzuka"
        ],
        "published": "2024-02-06T13:19:53Z",
        "summary": "In the rapidly changing world of smart technology, searching for documents\nhas become more challenging due to the rise of advanced language models. These\nmodels sometimes face difficulties, like providing inaccurate information,\ncommonly known as \"hallucination.\" This research focuses on addressing this\nissue through Retrieval-Augmented Generation (RAG), a technique that guides\nmodels to give accurate responses based on real facts. To overcome scalability\nissues, the study explores connecting user queries with sophisticated language\nmodels such as BERT and Orca2, using an innovative query optimization process.\nThe study unfolds in three scenarios: first, without RAG, second, without\nadditional assistance, and finally, with extra help. Choosing the compact yet\nefficient Orca2 7B model demonstrates a smart use of computing resources. The\nempirical results indicate a significant improvement in the initial language\nmodel's performance under RAG, particularly when assisted with prompts\naugmenters. Consistency in document retrieval across different encodings\nhighlights the effectiveness of using language model-generated queries. The\nintroduction of UMAP for BERT further simplifies document retrieval while\nmaintaining strong results.",
        "pdf_link": "https://arxiv.org/pdf/2402.16874v1.pdf"
    },
    {
        "title": "In-Context Example Ordering Guided by Label Distributions",
        "authors": [
            "Zhichao Xu",
            "Daniel Cohen",
            "Bei Wang",
            "Vivek Srikumar"
        ],
        "published": "2024-02-18T04:08:10Z",
        "summary": "By allowing models to predict without task-specific training, in-context\nlearning (ICL) with pretrained LLMs has enormous potential in NLP. However, a\nnumber of problems persist in ICL. In particular, its performance is sensitive\nto the choice and order of in-context examples. Given the same set of\nin-context examples with different orderings, model performance may vary\nbetween near random to near state-of-the-art. In this work, we formulate\nin-context example ordering as an optimization problem. We examine three\nproblem settings that differ in the assumptions they make about what is known\nabout the task. Inspired by the idea of learning from label proportions, we\npropose two principles for in-context example ordering guided by model's\nprobability predictions. We apply our proposed principles to thirteen text\nclassification datasets and nine different autoregressive LLMs with 700M to 13B\nparameters. We demonstrate that our approach outperforms the baselines by\nimproving the classification accuracy, reducing model miscalibration, and also\nby selecting better in-context examples.",
        "pdf_link": "https://arxiv.org/pdf/2402.11447v1.pdf"
    },
    {
        "title": "Bootstrapping Cognitive Agents with a Large Language Model",
        "authors": [
            "Feiyu Zhu",
            "Reid Simmons"
        ],
        "published": "2024-02-25T01:40:30Z",
        "summary": "Large language models contain noisy general knowledge of the world, yet are\nhard to train or fine-tune. On the other hand cognitive architectures have\nexcellent interpretability and are flexible to update but require a lot of\nmanual work to instantiate. In this work, we combine the best of both worlds:\nbootstrapping a cognitive-based model with the noisy knowledge encoded in large\nlanguage models. Through an embodied agent doing kitchen tasks, we show that\nour proposed framework yields better efficiency compared to an agent based\nentirely on large language models. Our experiments indicate that large language\nmodels are a good source of information for cognitive architectures, and the\ncognitive architecture in turn can verify and update the knowledge of large\nlanguage models to a specific domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.00810v1.pdf"
    },
    {
        "title": "Navigating the OverKill in Large Language Models",
        "authors": [
            "Chenyu Shi",
            "Xiao Wang",
            "Qiming Ge",
            "Songyang Gao",
            "Xianjun Yang",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang",
            "Xun Zhao",
            "Dahua Lin"
        ],
        "published": "2024-01-31T07:26:47Z",
        "summary": "Large language models are meticulously aligned to be both helpful and\nharmless. However, recent research points to a potential overkill which means\nmodels may refuse to answer benign queries. In this paper, we investigate the\nfactors for overkill by exploring how models handle and determine the safety of\nqueries. Our findings reveal the presence of shortcuts within models, leading\nto an over-attention of harmful words like 'kill' and prompts emphasizing\nsafety will exacerbate overkill. Based on these insights, we introduce\nSelf-Contrastive Decoding (Self-CD), a training-free and model-agnostic\nstrategy, to alleviate this phenomenon. We first extract such over-attention by\namplifying the difference in the model's output distributions when responding\nto system prompts that either include or omit an emphasis on safety. Then we\ndetermine the final next-token predictions by downplaying the over-attention\nfrom the model via contrastive decoding. Empirical results indicate that our\nmethod has achieved an average reduction of the refusal rate by 20\\% while\nhaving almost no impact on safety.",
        "pdf_link": "https://arxiv.org/pdf/2401.17633v1.pdf"
    },
    {
        "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
        "authors": [
            "Junhong Shen",
            "Neil Tenenholtz",
            "James Brian Hall",
            "David Alvarez-Melis",
            "Nicolo Fusi"
        ],
        "published": "2024-02-06T20:11:54Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating natural language. However, their capabilities wane\nin highly specialized domains underrepresented in the pretraining corpus, such\nas physical and biomedical sciences. This work explores how to repurpose\ngeneral LLMs into effective task solvers for specialized domains. We introduce\na novel, model-agnostic framework for learning custom input tags, which are\nparameterized as continuous vectors appended to the LLM's embedding layer, to\ncondition the LLM. We design two types of input tags: domain tags are used to\ndelimit specialized representations (e.g., chemical formulas) and provide\ndomain-relevant context; function tags are used to represent specific functions\n(e.g., predicting molecular properties) and compress function-solving\ninstructions. We develop a three-stage protocol to learn these tags using\nauxiliary data and domain knowledge. By explicitly disentangling task domains\nfrom task functions, our method enables zero-shot generalization to unseen\nproblems through diverse combinations of the input tags. It also boosts LLM's\nperformance in various specialized domains, such as predicting protein or\nchemical properties and modeling drug-target interactions, outperforming expert\nmodels tailored to these tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.05140v1.pdf"
    },
    {
        "title": "LLMs for Coding and Robotics Education",
        "authors": [
            "Peng Shu",
            "Huaqin Zhao",
            "Hanqi Jiang",
            "Yiwei Li",
            "Shaochen Xu",
            "Yi Pan",
            "Zihao Wu",
            "Zhengliang Liu",
            "Guoyu Lu",
            "Le Guan",
            "Gong Chen",
            "Xianqiao Wang Tianming Liu"
        ],
        "published": "2024-02-09T00:58:57Z",
        "summary": "Large language models and multimodal large language models have\nrevolutionized artificial intelligence recently. An increasing number of\nregions are now embracing these advanced technologies. Within this context,\nrobot coding education is garnering increasing attention. To teach young\nchildren how to code and compete in robot challenges, large language models are\nbeing utilized for robot code explanation, generation, and modification. In\nthis paper, we highlight an important trend in robot coding education. We test\nseveral mainstream large language models on both traditional coding tasks and\nthe more challenging task of robot code generation, which includes block\ndiagrams. Our results show that GPT-4V outperforms other models in all of our\ntests but struggles with generating block diagram images.",
        "pdf_link": "https://arxiv.org/pdf/2402.06116v1.pdf"
    },
    {
        "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
        "authors": [
            "Jiaheng Liu",
            "Zhiqi Bai",
            "Yuanxing Zhang",
            "Chenchen Zhang",
            "Yu Zhang",
            "Ge Zhang",
            "Jiakai Wang",
            "Haoran Que",
            "Yukang Chen",
            "Wenbo Su",
            "Tiezheng Ge",
            "Jie Fu",
            "Wenhu Chen",
            "Bo Zheng"
        ],
        "published": "2024-01-13T02:11:20Z",
        "summary": "Typically, training LLMs with long context sizes is computationally\nexpensive, requiring extensive training hours and GPU resources. Existing\nlong-context extension methods usually need additional training procedures to\nsupport corresponding long-context windows, where the long-context training\ndata (e.g., 32k) is needed, and high GPU training costs are assumed. To address\nthe aforementioned issues, we propose an Efficient and Extreme length extension\nmethod for Large Language Models, called E 2 -LLM, with only one training\nprocedure and dramatically reduced computation cost, which also removes the\nneed to collect long-context data. Concretely, first, the training data of our\nE 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost\ngreatly. Second, the training procedure on the short training context window is\nperformed only once time, and we can support different evaluation context\nwindows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,\nwe introduce two different augmentation methods on the scale and position index\nparameters for different samples in training. It aims to make the model more\nrobust to the different relative differences when directly interpolating the\narbitrary context length at inference. Comprehensive experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on\nchallenging long-context tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.06951v3.pdf"
    },
    {
        "title": "LoMA: Lossless Compressed Memory Attention",
        "authors": [
            "Yumeng Wang",
            "Zhenyang Xiao"
        ],
        "published": "2024-01-16T09:18:46Z",
        "summary": "Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.",
        "pdf_link": "https://arxiv.org/pdf/2401.09486v2.pdf"
    },
    {
        "title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding",
        "authors": [
            "Chris Kelly",
            "Luhui Hu",
            "Jiayin Hu",
            "Yu Tian",
            "Deshun Yang",
            "Bang Yang",
            "Cindy Yang",
            "Zihao Li",
            "Zaoshan Huang",
            "Yuexian Zou"
        ],
        "published": "2024-03-14T16:13:00Z",
        "summary": "The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent",
        "pdf_link": "https://arxiv.org/pdf/2403.09530v2.pdf"
    },
    {
        "title": "Enhancing Trust in Autonomous Agents: An Architecture for Accountability and Explainability through Blockchain and Large Language Models",
        "authors": [
            "Laura Fern\u00e1ndez-Becerra",
            "Miguel \u00c1ngel Gonz\u00e1lez-Santamarta",
            "\u00c1ngel Manuel Guerrero-Higueras",
            "Francisco Javier Rodr\u00edguez-Lera",
            "Vicente Matell\u00e1n Olivera"
        ],
        "published": "2024-03-14T16:57:18Z",
        "summary": "The deployment of autonomous agents in environments involving human\ninteraction has increasingly raised security concerns. Consequently,\nunderstanding the circumstances behind an event becomes critical, requiring the\ndevelopment of capabilities to justify their behaviors to non-expert users.\nSuch explanations are essential in enhancing trustworthiness and safety, acting\nas a preventive measure against failures, errors, and misunderstandings.\nAdditionally, they contribute to improving communication, bridging the gap\nbetween the agent and the user, thereby improving the effectiveness of their\ninteractions. This work presents an accountability and explainability\narchitecture implemented for ROS-based mobile robots. The proposed solution\nconsists of two main components. Firstly, a black box-like element to provide\naccountability, featuring anti-tampering properties achieved through blockchain\ntechnology. Secondly, a component in charge of generating natural language\nexplanations by harnessing the capabilities of Large Language Models (LLMs)\nover the data contained within the previously mentioned black box. The study\nevaluates the performance of our solution in three different scenarios, each\ninvolving autonomous agent navigation functionalities. This evaluation includes\na thorough examination of accountability and explainability metrics,\ndemonstrating the effectiveness of our approach in using accountable data from\nrobot actions to obtain coherent, accurate and understandable explanations,\neven when facing challenges inherent in the use of autonomous agents in\nreal-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.09567v1.pdf"
    },
    {
        "title": "Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization",
        "authors": [
            "Rui Zhang",
            "Hongwei Li",
            "Rui Wen",
            "Wenbo Jiang",
            "Yuan Zhang",
            "Michael Backes",
            "Yun Shen",
            "Yang Zhang"
        ],
        "published": "2024-02-14T13:47:35Z",
        "summary": "The increasing demand for customized Large Language Models (LLMs) has led to\nthe development of solutions like GPTs. These solutions facilitate tailored LLM\ncreation via natural language prompts without coding. However, the\ntrustworthiness of third-party custom versions of LLMs remains an essential\nconcern. In this paper, we propose the first instruction backdoor attacks\nagainst applications integrated with untrusted customized LLMs (e.g., GPTs).\nSpecifically, these attacks embed the backdoor into the custom version of LLMs\nby designing prompts with backdoor instructions, outputting the attacker's\ndesired result when inputs contain the pre-defined triggers. Our attack\nincludes 3 levels of attacks: word-level, syntax-level, and semantic-level,\nwhich adopt different types of triggers with progressive stealthiness. We\nstress that our attacks do not require fine-tuning or any modification to the\nbackend LLMs, adhering strictly to GPTs development guidelines. We conduct\nextensive experiments on 4 prominent LLMs and 5 benchmark text classification\ndatasets. The results show that our instruction backdoor attacks achieve the\ndesired attack performance without compromising utility. Additionally, we\npropose an instruction-ignoring defense mechanism and demonstrate its partial\neffectiveness in mitigating such attacks. Our findings highlight the\nvulnerability and the potential risks of LLM customization such as GPTs.",
        "pdf_link": "https://arxiv.org/pdf/2402.09179v2.pdf"
    },
    {
        "title": "NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention",
        "authors": [
            "Tianyi Zhang",
            "Jonah Wonkyu Yi",
            "Bowen Yao",
            "Zhaozhuo Xu",
            "Anshumali Shrivastava"
        ],
        "published": "2024-03-02T17:29:22Z",
        "summary": "Large language model inference on Central Processing Units (CPU) is\nchallenging due to the vast quantities of expensive Multiply-Add (MAD) matrix\noperations in the attention computations. In this paper, we argue that there is\na rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,\nwhich allow for ultra-low-latency lookups in batch. We leverage this unique\ncapability of CPUs to propose NoMAD-Attention, an efficient attention algorithm\nthat replaces MAD operations with in-register lookups. Through hardware-aware\nalgorithmic designs, NoMAD-Attention achieves the computation of attention\nscores using repeated fast accesses to SIMD registers despite their highly\nlimited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based\nLLMs without model finetuning. Empirical evaluations demonstrate that\nNoMAD-Attention maintains the quality of the original LLMs well, and speeds up\nthe 4-bit quantized LLaMA-7B-based model by up to 2$\\times$ at 16k context\nlength. Our results are reproducible at\nhttps://github.com/tonyzhang617/nomad-dist.",
        "pdf_link": "https://arxiv.org/pdf/2403.01273v1.pdf"
    },
    {
        "title": "Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors",
        "authors": [
            "Shengkun Ma",
            "Jiale Han",
            "Yi Liang",
            "Bo Cheng"
        ],
        "published": "2024-02-24T04:32:44Z",
        "summary": "Continual Few-shot Relation Extraction (CFRE) is a practical problem that\nrequires the model to continuously learn novel relations while avoiding\nforgetting old ones with few labeled training data. The primary challenges are\ncatastrophic forgetting and overfitting. This paper harnesses prompt learning\nto explore the implicit capabilities of pre-trained language models to address\nthe above two challenges, thereby making language models better continual\nfew-shot relation extractors. Specifically, we propose a Contrastive Prompt\nLearning framework, which designs prompt representation to acquire more\ngeneralized knowledge that can be easily adapted to old and new categories, and\nmargin-based contrastive learning to focus more on hard samples, therefore\nalleviating catastrophic forgetting and overfitting issues. To further remedy\noverfitting in low-resource scenarios, we introduce an effective memory\naugmentation strategy that employs well-crafted prompts to guide ChatGPT in\ngenerating diverse samples. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods by a large margin and significantly\nmitigates catastrophic forgetting and overfitting in low-resource scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.15713v1.pdf"
    },
    {
        "title": "Beyond LLMs: Advancing the Landscape of Complex Reasoning",
        "authors": [
            "Jennifer Chu-Carroll",
            "Andrew Beck",
            "Greg Burnham",
            "David OS Melville",
            "David Nachman",
            "A. Erdem \u00d6zcan",
            "David Ferrucci"
        ],
        "published": "2024-02-12T21:14:45Z",
        "summary": "Since the advent of Large Language Models a few years ago, they have often\nbeen considered the de facto solution for many AI problems. However, in\naddition to the many deficiencies of LLMs that prevent them from broad industry\nadoption, such as reliability, cost, and speed, there is a whole class of\ncommon real world problems that Large Language Models perform poorly on,\nnamely, constraint satisfaction and optimization problems. These problems are\nubiquitous and current solutions are highly specialized and expensive to\nimplement. At Elemental Cognition, we developed our EC AI platform which takes\na neuro-symbolic approach to solving constraint satisfaction and optimization\nproblems. The platform employs, at its core, a precise and high performance\nlogical reasoning engine, and leverages LLMs for knowledge acquisition and user\ninteraction. This platform supports developers in specifying application logic\nin natural and concise language while generating application user interfaces to\ninteract with users effectively. We evaluated LLMs against systems built on the\nEC AI platform in three domains and found the EC AI systems to significantly\noutperform LLMs on constructing valid and optimal solutions, on validating\nproposed solutions, and on repairing invalid solutions.",
        "pdf_link": "https://arxiv.org/pdf/2402.08064v1.pdf"
    },
    {
        "title": "Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases",
        "authors": [
            "Rio Aguina-Kang",
            "Maxim Gumin",
            "Do Heon Han",
            "Stewart Morris",
            "Seung Jean Yoo",
            "Aditya Ganeshan",
            "R. Kenny Jones",
            "Qiuhong Anna Wei",
            "Kailiang Fu",
            "Daniel Ritchie"
        ],
        "published": "2024-02-05T01:59:31Z",
        "summary": "We present a system for generating indoor scenes in response to text prompts.\nThe prompts are not limited to a fixed vocabulary of scene descriptions, and\nthe objects in generated scenes are not restricted to a fixed set of object\ncategories -- we call this setting indoor scene generation. Unlike most prior\nwork on indoor scene generation, our system does not require a large training\ndataset of existing 3D scenes. Instead, it leverages the world knowledge\nencoded in pre-trained large language models (LLMs) to synthesize programs in a\ndomain-specific layout language that describe objects and spatial relations\nbetween them. Executing such a program produces a specification of a constraint\nsatisfaction problem, which the system solves using a gradient-based\noptimization scheme to produce object positions and orientations. To produce\nobject geometry, the system retrieves 3D meshes from a database. Unlike prior\nwork which uses databases of category-annotated, mutually-aligned meshes, we\ndevelop a pipeline using vision-language models (VLMs) to retrieve meshes from\nmassive databases of un-annotated, inconsistently-aligned meshes. Experimental\nevaluations show that our system outperforms generative models trained on 3D\ndata for traditional, closed-universe scene generation tasks; it also\noutperforms a recent LLM-based layout generation method on open-universe scene\ngeneration.",
        "pdf_link": "https://arxiv.org/pdf/2403.09675v1.pdf"
    },
    {
        "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
        "authors": [
            "Xisen Jin",
            "Xiang Ren"
        ],
        "published": "2024-02-02T19:43:15Z",
        "summary": "Language models deployed in the wild make errors. However, simply updating\nthe model with the corrected error instances causes catastrophic forgetting --\nthe updated model makes errors on instances learned during the instruction\ntuning or upstream training phase. Randomly replaying upstream data yields\nunsatisfactory performance and often comes with high variance and poor\ncontrollability. To this end, we try to forecast upstream examples that will be\nforgotten due to a model update for improved controllability of the replay\nprocess and interpretability. We train forecasting models given a collection of\nonline learned examples and corresponding forgotten upstream pre-training\nexamples. We propose a partially interpretable forecasting model based on the\nobservation that changes in pre-softmax logit scores of pretraining examples\nresemble that of online learned examples, which performs decently on BART but\nfails on T5 models. We further show a black-box classifier based on inner\nproducts of example representations achieves better forecasting performance\nover a series of setups. Finally, we show that we reduce forgetting of upstream\npretraining examples by replaying examples that are forecasted to be forgotten,\ndemonstrating the practical utility of forecasting example forgetting.",
        "pdf_link": "https://arxiv.org/pdf/2402.01865v1.pdf"
    },
    {
        "title": "Supervisory Prompt Training",
        "authors": [
            "Jean Ghislain Billa",
            "Min Oh",
            "Liang Du"
        ],
        "published": "2024-03-26T19:08:20Z",
        "summary": "The performance of Large Language Models (LLMs) relies heavily on the quality\nof prompts, which are often manually engineered and task-specific, making them\ncostly and non-scalable. We propose a novel approach, Supervisory Prompt\nTraining (SPT). SPT automates the generation of highly effective prompts using\na dual LLM system. In this system, one LLM, the generator, performs a task\nwhile the other, the corrector, provides feedback and generates improved\nprompts. In contrast to earlier techniques, both the generator and corrector\ncollaboratively and continuously improve their prompts over time. We also\nintroduce the concept of \\textit{impact scores} to measure the sentence-level\neffectiveness of the prompts. Our method was tested on four benchmarks, testing\nthe level of hallucinations in LLMs. Notably, we were able to increase the\naccuracy of GPT-4 on GSM8K from 65.8\\% to 94.1\\% (28.3\\% increase). SPT\nadvances LLMs by refining prompts to enhance performance and reduce\nhallucinations, offering an efficient and scalable alternative to traditional\nmodel fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.18051v1.pdf"
    },
    {
        "title": "Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models",
        "authors": [
            "Dingning Liu",
            "Xiaoshui Huang",
            "Yuenan Hou",
            "Zhihui Wang",
            "Zhenfei Yin",
            "Yongshun Gong",
            "Peng Gao",
            "Wanli Ouyang"
        ],
        "published": "2024-01-09T06:20:23Z",
        "summary": "In this paper, we introduce Uni3D-LLM, a unified framework that leverages a\nLarge Language Model (LLM) to integrate tasks of 3D perception, generation, and\nediting within point cloud scenes. This framework empowers users to\neffortlessly generate and modify objects at specified locations within a scene,\nguided by the versatility of natural language descriptions. Uni3D-LLM harnesses\nthe expressive power of natural language to allow for precise command over the\ngeneration and editing of 3D objects, thereby significantly enhancing\noperational flexibility and controllability. By mapping point cloud into the\nunified representation space, Uni3D-LLM achieves cross-application\nfunctionality, enabling the seamless execution of a wide array of tasks,\nranging from the accurate instantiation of 3D objects to the diverse\nrequirements of interactive design. Through a comprehensive suite of rigorous\nexperiments, the efficacy of Uni3D-LLM in the comprehension, generation, and\nediting of point cloud has been validated. Additionally, we have assessed the\nimpact of integrating a point cloud perception module on the generation and\nediting processes, confirming the substantial potential of our approach for\npractical applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.03327v1.pdf"
    },
    {
        "title": "Can AI Assistants Know What They Don't Know?",
        "authors": [
            "Qinyuan Cheng",
            "Tianxiang Sun",
            "Xiangyang Liu",
            "Wenwei Zhang",
            "Zhangyue Yin",
            "Shimin Li",
            "Linyang Li",
            "Zhengfu He",
            "Kai Chen",
            "Xipeng Qiu"
        ],
        "published": "2024-01-24T07:34:55Z",
        "summary": "Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.",
        "pdf_link": "https://arxiv.org/pdf/2401.13275v2.pdf"
    },
    {
        "title": "Naming, Describing, and Quantifying Visual Objects in Humans and LLMs",
        "authors": [
            "Alberto Testoni",
            "Juell Sprott",
            "Sandro Pezzelle"
        ],
        "published": "2024-03-11T17:20:12Z",
        "summary": "While human speakers use a variety of different expressions when describing\nthe same object in an image, giving rise to a distribution of plausible labels\ndriven by pragmatic constraints, the extent to which current Vision \\& Language\nLarge Language Models (VLLMs) can mimic this crucial feature of language use is\nan open question. This applies to common, everyday objects, but it is\nparticularly interesting for uncommon or novel objects for which a category\nlabel may be lacking or fuzzy. Furthermore, humans show clear production\npreferences for highly context-sensitive expressions, such as the quantifiers\n`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on\nthree categories (nouns, attributes, and quantifiers) where humans show great\nsubjective variability concerning the distribution over plausible labels, using\ndatasets and resources mostly under-explored in previous work. Our results\nreveal mixed evidence on the ability of VLLMs to capture human naming\npreferences, with all models failing in tasks that require high-level reasoning\nsuch as assigning quantifiers.",
        "pdf_link": "https://arxiv.org/pdf/2403.06935v2.pdf"
    },
    {
        "title": "Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small Language Models",
        "authors": [
            "Minjie Zhu",
            "Yichen Zhu",
            "Xin Liu",
            "Ning Liu",
            "Zhiyuan Xu",
            "Chaomin Shen",
            "Yaxin Peng",
            "Zhicai Ou",
            "Feifei Feng",
            "Jian Tang"
        ],
        "published": "2024-03-10T12:43:27Z",
        "summary": "Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.",
        "pdf_link": "https://arxiv.org/pdf/2403.06199v4.pdf"
    },
    {
        "title": "CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs",
        "authors": [
            "Daoan Zhang",
            "Junming Yang",
            "Hanjia Lyu",
            "Zijian Jin",
            "Yuan Yao",
            "Mingkai Chen",
            "Jiebo Luo"
        ],
        "published": "2024-01-05T00:26:07Z",
        "summary": "When exploring the development of Artificial General Intelligence (AGI), a\ncritical task for these models involves interpreting and processing information\nfrom multiple image inputs. However, Large Multimodal Models (LMMs) encounter\ntwo issues in such scenarios: (1) a lack of fine-grained perception, and (2) a\ntendency to blend information across multiple images. We first extensively\ninvestigate the capability of LMMs to perceive fine-grained visual details when\ndealing with multiple input images. The research focuses on two aspects: first,\nimage-to-image matching (to evaluate whether LMMs can effectively reason and\npair relevant images), and second, multi-image-to-text matching (to assess\nwhether LMMs can accurately capture and summarize detailed image information).\nWe conduct evaluations on a range of both open-source and closed-source large\nmodels, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model\nperformance, we further develop a Contrastive Chain-of-Thought (CoCoT)\nprompting approach based on multi-input multimodal models. This method requires\nLMMs to compare the similarities and differences among multiple image inputs,\nand then guide the models to answer detailed questions about multi-image inputs\nbased on the identified similarities and differences. Our experimental results\nshowcase CoCoT's proficiency in enhancing the multi-image comprehension\ncapabilities of large multimodal models.",
        "pdf_link": "https://arxiv.org/pdf/2401.02582v1.pdf"
    },
    {
        "title": "When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",
        "authors": [
            "Abdenour Hadid",
            "Tanujit Chakraborty",
            "Daniel Busby"
        ],
        "published": "2024-01-25T12:03:50Z",
        "summary": "Generative Artificial Intelligence (GAI) represents an emerging field that\npromises the creation of synthetic data and outputs in different modalities.\nGAI has recently shown impressive results across a large spectrum of\napplications ranging from biology, medicine, education, legislation, computer\nscience, and finance. As one strives for enhanced safety, efficiency, and\nsustainability, generative AI indeed emerges as a key differentiator and\npromises a paradigm shift in the field. This paper explores the potential\napplications of generative AI and large language models in geoscience. The\nrecent developments in the field of machine learning and deep learning have\nenabled the generative model's utility for tackling diverse prediction\nproblems, simulation, and multi-criteria decision-making challenges related to\ngeoscience and Earth system dynamics. This survey discusses several GAI models\nthat have been used in geoscience comprising generative adversarial networks\n(GANs), physics-informed neural networks (PINNs), and generative pre-trained\ntransformer (GPT)-based structures. These tools have helped the geoscience\ncommunity in several applications, including (but not limited to) data\ngeneration/augmentation, super-resolution, panchromatic sharpening, haze\nremoval, restoration, and land surface changing. Some challenges still remain\nsuch as ensuring physical interpretation, nefarious use cases, and\ntrustworthiness. Beyond that, GAI models show promises to the geoscience\ncommunity, especially with the support to climate change, urban science,\natmospheric science, marine science, and planetary science through their\nextraordinary ability to data-driven modeling and uncertainty quantification.",
        "pdf_link": "https://arxiv.org/pdf/2402.03349v1.pdf"
    },
    {
        "title": "SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization",
        "authors": [
            "Jacob Parnell",
            "Inigo Jauregi Unanue",
            "Massimo Piccardi"
        ],
        "published": "2024-03-20T02:04:42Z",
        "summary": "Cross-lingual summarization (XLS) generates summaries in a language different\nfrom that of the input documents (e.g., English to Spanish), allowing speakers\nof the target language to gain a concise view of their content. In the present\nday, the predominant approach to this task is to take a performing, pretrained\nmultilingual language model (LM) and fine-tune it for XLS on the language pairs\nof interest. However, the scarcity of fine-tuning samples makes this approach\nchallenging in some cases. For this reason, in this paper we propose revisiting\nthe summarize-and-translate pipeline, where the summarization and translation\ntasks are performed in a sequence. This approach allows reusing the many,\npublicly-available resources for monolingual summarization and translation,\nobtaining a very competitive zero-shot performance. In addition, the proposed\npipeline is completely differentiable end-to-end, allowing it to take advantage\nof few-shot fine-tuning, where available. Experiments over two contemporary and\nwidely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable\nzero-shot performance of the proposed approach, and also its strong few-shot\nperformance compared to an equivalent multilingual LM baseline, that the\nproposed approach has been able to outperform in many languages with only 10%\nof the fine-tuning samples.",
        "pdf_link": "https://arxiv.org/pdf/2403.13240v1.pdf"
    },
    {
        "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning",
        "authors": [
            "Weiqi Wang",
            "Tianqing Fang",
            "Chunyang Li",
            "Haochen Shi",
            "Wenxuan Ding",
            "Baixuan Xu",
            "Zhaowei Wang",
            "Jiaxin Bai",
            "Xin Liu",
            "Jiayang Cheng",
            "Chunkit Chan",
            "Yangqiu Song"
        ],
        "published": "2024-01-14T13:24:30Z",
        "summary": "The sequential process of conceptualization and instantiation is essential to\ngeneralizable commonsense reasoning as it allows the application of existing\nknowledge to unfamiliar scenarios. However, existing works tend to undervalue\nthe step of instantiation and heavily rely on pre-built concept taxonomies and\nhuman annotations to collect both types of knowledge, resulting in a lack of\ninstantiated knowledge to complete reasoning, high cost, and limited\nscalability. To tackle these challenges, we introduce CANDLE, a distillation\nframework that iteratively performs contextualized conceptualization and\ninstantiation over commonsense knowledge bases by instructing large language\nmodels to generate both types of knowledge with critic filtering. By applying\nCANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six\nmillion conceptualizations and instantiated commonsense knowledge triples. Both\ntypes of knowledge are firmly rooted in the original ATOMIC dataset, and\nintrinsic evaluations demonstrate their exceptional quality and diversity.\nEmpirical results indicate that distilling CANDLE on student models provides\nbenefits across four downstream tasks. Our code, data, and models are publicly\navailable at https://github.com/HKUST-KnowComp/CANDLE.",
        "pdf_link": "https://arxiv.org/pdf/2401.07286v1.pdf"
    },
    {
        "title": "Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction",
        "authors": [
            "Sizhe Zhou",
            "Yu Meng",
            "Bowen Jin",
            "Jiawei Han"
        ],
        "published": "2024-02-17T00:20:06Z",
        "summary": "Relation extraction (RE), a crucial task in NLP, aims to identify semantic\nrelationships between entities mentioned in texts. Despite significant\nadvancements in this field, existing models typically rely on extensive\nannotated data for training, which can be both costly and time-consuming to\nacquire. Moreover, these models often struggle to adapt to new or unseen\nrelationships. In contrast, few-shot learning settings, which aim to reduce\nannotation requirements, may offer incomplete and biased supervision for\nunderstanding target relation semantics, leading to degraded and unstable\nperformance. To provide the model with accurate and explicit descriptions of\nthe relations types and meanwhile minimize the annotation requirements, we\nstudy the definition only zero-shot RE setting where only relation definitions\nexpressed in natural language are used to train a RE model. Motivated by the\nstrong synthetic data generation power of LLMs, we propose a framework REPaL\nwhich consists of three stages: (1) We utilize LLMs to generate initial seed\ninstances based on relation definitions and an unlabeled corpora. (2) We\nfine-tune a bidirectional Small Language Model (SLM) using these initial seeds\nto learn the relations for the target domain. (3) We enhance pattern coverage\nand mitigate bias resulting from the limited number of initial seeds by\nincorporating feedback acquired from SLM's predictions on unlabeled corpora. To\naccomplish this, we leverage the multi-turn conversation ability of LLMs to\ngenerate new instances in follow-up dialogues. Experiments on two datasets show\nREPaL achieves better zero-shot performance with large margins over baseline\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.11142v1.pdf"
    },
    {
        "title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries",
        "authors": [
            "Zelalem Gero",
            "Chandan Singh",
            "Yiqing Xie",
            "Sheng Zhang",
            "Tristan Naumann",
            "Jianfeng Gao",
            "Hoifung Poon"
        ],
        "published": "2024-03-01T21:59:03Z",
        "summary": "Summarizing clinical text is crucial in health decision-support and clinical\nresearch. Large language models (LLMs) have shown the potential to generate\naccurate clinical text summaries, but still struggle with issues regarding\ngrounding and evaluation, especially in safety-critical domains such as health.\nHolistically evaluating text summaries is challenging because they may contain\nunsubstantiated information. Here, we explore a general mitigation framework\nusing Attribute Structuring (AS), which structures the summary evaluation\nprocess. It decomposes the evaluation process into a grounded procedure that\nuses an LLM for relatively simple structuring and scoring tasks, rather than\nthe full task of holistic summary evaluation. Experiments show that AS\nconsistently improves the correspondence between human annotations and\nautomated metrics in clinical text summarization. Additionally, AS yields\ninterpretations in the form of a short text span corresponding to each output,\nwhich enables efficient human auditing, paving the way towards trustworthy\nevaluation of clinical information in resource-constrained scenarios. We\nrelease our code, prompts, and an open-source benchmark at\nhttps://github.com/microsoft/attribute-structuring.",
        "pdf_link": "https://arxiv.org/pdf/2403.01002v1.pdf"
    },
    {
        "title": "Which LLM to Play? Convergence-Aware Online Model Selection with Time-Increasing Bandits",
        "authors": [
            "Yu Xia",
            "Fang Kong",
            "Tong Yu",
            "Liya Guo",
            "Ryan A. Rossi",
            "Sungchul Kim",
            "Shuai Li"
        ],
        "published": "2024-03-11T23:52:46Z",
        "summary": "Web-based applications such as chatbots, search engines and news\nrecommendations continue to grow in scale and complexity with the recent surge\nin the adoption of LLMs. Online model selection has thus garnered increasing\nattention due to the need to choose the best model among a diverse set while\nbalancing task reward and exploration cost. Organizations faces decisions like\nwhether to employ a costly API-based LLM or a locally finetuned small LLM,\nweighing cost against performance. Traditional selection methods often evaluate\nevery candidate model before choosing one, which are becoming impractical given\nthe rising costs of training and finetuning LLMs. Moreover, it is undesirable\nto allocate excessive resources towards exploring poor-performing models. While\nsome recent works leverage online bandit algorithm to manage such\nexploration-exploitation trade-off in model selection, they tend to overlook\nthe increasing-then-converging trend in model performances as the model is\niteratively finetuned, leading to less accurate predictions and suboptimal\nmodel selections.\n  In this paper, we propose a time-increasing bandit algorithm TI-UCB, which\neffectively predicts the increase of model performances due to finetuning and\nefficiently balances exploration and exploitation in model selection. To\nfurther capture the converging points of models, we develop a change detection\nmechanism by comparing consecutive increase predictions. We theoretically prove\nthat our algorithm achieves a logarithmic regret upper bound in a typical\nincreasing bandit setting, which implies a fast convergence rate. The advantage\nof our method is also empirically validated through extensive experiments on\nclassification model selection and online selection of LLMs. Our results\nhighlight the importance of utilizing increasing-then-converging pattern for\nmore efficient and economic model selection in the deployment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.07213v1.pdf"
    },
    {
        "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
        "authors": [
            "Xin Quan",
            "Marco Valentino",
            "Louise A. Dennis",
            "Andr\u00e9 Freitas"
        ],
        "published": "2024-02-01T16:39:51Z",
        "summary": "An increasing amount of research in Natural Language Inference (NLI) focuses\non the application and evaluation of Large Language Models (LLMs) and their\nreasoning capabilities. Despite their success, however, LLMs are still prone to\nfactual errors and inconsistencies in their explanations, offering limited\ncontrol and interpretability for inference in complex domains. In this paper,\nwe focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can\nenhance the logical validity and alignment of ethical explanations produced by\nLLMs. Specifically, we present an abductive-deductive framework named\nLogic-Explainer, which integrates LLMs with an external backward-chaining\nsolver to refine step-wise natural language explanations and jointly verify\ntheir correctness, reduce incompleteness and minimise redundancy. An extensive\nempirical analysis demonstrates that Logic-Explainer can improve explanations\ngenerated via in-context learning methods and Chain-of-Thought (CoT) on\nchallenging ethical NLI tasks, while, at the same time, producing formal proofs\ndescribing and supporting models' reasoning. As ethical NLI requires\ncommonsense reasoning to identify underlying moral violations, our results\nsuggest the effectiveness of neuro-symbolic methods for multi-step NLI more\nbroadly, opening new opportunities to enhance the logical consistency,\nreliability, and alignment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.00745v1.pdf"
    },
    {
        "title": "MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization",
        "authors": [
            "Yasaman Jafari",
            "Dheeraj Mekala",
            "Rose Yu",
            "Taylor Berg-Kirkpatrick"
        ],
        "published": "2024-02-18T21:25:09Z",
        "summary": "RL-based techniques can be used to search for prompts that when fed into a\ntarget language model maximize a set of user-specified reward functions.\nHowever, in many target applications, the natural reward functions are in\ntension with one another -- for example, content preservation vs. style\nmatching in style transfer tasks. Current techniques focus on maximizing the\naverage of reward functions, which does not necessarily lead to prompts that\nachieve balance across rewards -- an issue that has been well-studied in the\nmulti-objective and robust optimization literature. In this paper, we adapt\nseveral techniques for multi-objective optimization to RL-based discrete prompt\noptimization -- two that consider volume of the Pareto reward surface, and\nanother that chooses an update direction that benefits all rewards\nsimultaneously. We conduct an empirical analysis of these methods on two NLP\ntasks: style transfer and machine translation, each using three competing\nreward functions. Our experiments demonstrate that multi-objective methods that\ndirectly optimize volume perform better and achieve a better balance of all\nrewards than those that attempt to find monotonic update directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.11711v1.pdf"
    },
    {
        "title": "Query-OPT: Optimizing Inference of Large Language Models via Multi-Query Instructions in Meeting Summarization",
        "authors": [
            "Md Tahmid Rahman Laskar",
            "Elena Khasanova",
            "Xue-Yong Fu",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2024-02-29T19:00:47Z",
        "summary": "This work focuses on the task of query-based meeting summarization in which\nthe summary of a context (meeting transcript) is generated in response to a\nspecific query. When using Large Language Models (LLMs) for this task, a new\ncall to the LLM inference endpoint/API is required for each new query even if\nthe context stays the same. However, repeated calls to the LLM inference\nendpoints would significantly increase the costs of using them in production,\nmaking LLMs impractical for many real-world use cases. To address this problem,\nin this paper, we investigate whether combining the queries for the same input\ncontext in a single prompt to minimize repeated calls can be successfully used\nin meeting summarization. In this regard, we conduct extensive experiments by\ncomparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2,\nMistral, and FLAN-T5 in single-query and multi-query settings. We observe that\nwhile most LLMs tend to respond to the multi-query instructions, almost all of\nthem (except GPT-4), even after fine-tuning, could not properly generate the\nresponse in the required output format. We conclude that while multi-query\nprompting could be useful to optimize the inference costs by reducing calls to\nthe inference endpoints/APIs for the task of meeting summarization, this\ncapability to reliably generate the response in the expected format is only\nlimited to certain LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.00067v1.pdf"
    },
    {
        "title": "A Closer Look at the Limitations of Instruction Tuning",
        "authors": [
            "Sreyan Ghosh",
            "Chandra Kiran Reddy Evuru",
            "Sonal Kumar",
            "Ramaneswaran S",
            "Deepali Aneja",
            "Zeyu Jin",
            "Ramani Duraiswami",
            "Dinesh Manocha"
        ],
        "published": "2024-02-03T04:45:25Z",
        "summary": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed inspire future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.05119v3.pdf"
    },
    {
        "title": "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
        "authors": [
            "Anjali Khurana",
            "Hari Subramonyam",
            "Parmit K Chilana"
        ],
        "published": "2024-02-12T19:49:58Z",
        "summary": "Large Language Model (LLM) assistants, such as ChatGPT, have emerged as\npotential alternatives to search methods for helping users navigate complex,\nfeature-rich software. LLMs use vast training data from domain-specific texts,\nsoftware manuals, and code repositories to mimic human-like interactions,\noffering tailored assistance, including step-by-step instructions. In this\nwork, we investigated LLM-generated software guidance through a within-subject\nexperiment with 16 participants and follow-up interviews. We compared a\nbaseline LLM assistant with an LLM optimized for particular software contexts,\nSoftAIBot, which also offered guidelines for constructing appropriate prompts.\nWe assessed task completion, perceived accuracy, relevance, and trust.\nSurprisingly, although SoftAIBot outperformed the baseline LLM, our results\nrevealed no significant difference in LLM usage and user perceptions with or\nwithout prompt guidelines and the integration of domain context. Most users\nstruggled to understand how the prompt's text related to the LLM's responses\nand often followed the LLM's suggestions verbatim, even if they were incorrect.\nThis resulted in difficulties when using the LLM's advice for software tasks,\nleading to low task completion rates. Our detailed analysis also revealed that\nusers remained unaware of inaccuracies in the LLM's responses, indicating a gap\nbetween their lack of software expertise and their ability to evaluate the\nLLM's assistance. With the growing push for designing domain-specific LLM\nassistants, we emphasize the importance of incorporating explainable,\ncontext-aware cues into LLMs to help users understand prompt-based\ninteractions, identify biases, and maximize the utility of LLM assistants.",
        "pdf_link": "https://arxiv.org/pdf/2402.08030v1.pdf"
    },
    {
        "title": "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness",
        "authors": [
            "Miltiadis Allamanis",
            "Sheena Panthaplackel",
            "Pengcheng Yin"
        ],
        "published": "2024-02-13T11:08:08Z",
        "summary": "To evaluate code large language models (LLMs), research has relied on a few\nsmall manually curated benchmarks, such as HumanEval and MBPP, which represent\na narrow part of the real-world software domains. In this work, we introduce\nround-trip correctness (RTC) as an alternative evaluation method. RTC allows\nCode LLM evaluation on a broader spectrum of real-world software domains\nwithout the need for costly human curation. RTC rests on the idea that we can\nask a model to make a prediction (e.g., describe some code using natural\nlanguage), feed that prediction back (e.g., synthesize code from the predicted\ndescription), and check if this round-trip leads to code that is semantically\nequivalent to the original input. We show how to employ RTC to evaluate code\nsynthesis and editing. We find that RTC strongly correlates with model\nperformance on existing narrow-domain code synthesis benchmarks while allowing\nus to expand to a much broader set of domains and tasks which was not\npreviously possible without costly human annotations.",
        "pdf_link": "https://arxiv.org/pdf/2402.08699v1.pdf"
    },
    {
        "title": "RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content",
        "authors": [
            "Zhuowen Yuan",
            "Zidi Xiong",
            "Yi Zeng",
            "Ning Yu",
            "Ruoxi Jia",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2024-03-19T07:25:02Z",
        "summary": "Recent advancements in Large Language Models (LLMs) have showcased remarkable\ncapabilities across various tasks in different domains. However, the emergence\nof biases and the potential for generating harmful content in LLMs,\nparticularly under malicious inputs, pose significant challenges. Current\nmitigation strategies, while effective, are not resilient under adversarial\nattacks. This paper introduces Resilient Guardrails for Large Language Models\n(RigorLLM), a novel framework designed to efficiently and effectively moderate\nharmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted\napproach that includes energy-based training data augmentation through Langevin\ndynamics, optimizing a safe suffix for inputs via minimax optimization, and\nintegrating a fusion-based model combining robust KNN with LLMs based on our\ndata augmentation, RigorLLM offers a robust solution to harmful content\nmoderation. Our experimental evaluations demonstrate that RigorLLM not only\noutperforms existing baselines like OpenAI API and Perspective API in detecting\nharmful content but also exhibits unparalleled resilience to jailbreaking\nattacks. The innovative use of constrained optimization and a fusion-based\nguardrail approach represents a significant step forward in developing more\nsecure and reliable LLMs, setting a new standard for content moderation\nframeworks in the face of evolving digital threats.",
        "pdf_link": "https://arxiv.org/pdf/2403.13031v1.pdf"
    },
    {
        "title": "Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach",
        "authors": [
            "Kun Sun",
            "Rong Wang",
            "Haitao Liu",
            "Anders S\u00f8gaard"
        ],
        "published": "2024-03-22T14:47:35Z",
        "summary": "Amidst the rapid evolution of LLMs, the significance of evaluation in\ncomprehending and propelling these models forward is increasingly paramount.\nEvaluations have revealed that factors such as scaling, training types,\narchitectures and other factors profoundly impact the performance of LLMs.\nHowever, the extent and nature of these impacts continue to be subjects of\ndebate because most assessments have been restricted to a limited number of\nmodels and data points. Clarifying the effects of these factors on performance\nscores can be more effectively achieved through a statistical lens. Our study\nembarks on a thorough re-examination of these LLMs, targeting the inadequacies\nin current evaluation methods. With the advent of a uniform evaluation\nframework, our research leverages an expansive dataset of evaluation results,\nintroducing a comprehensive statistical methodology. This includes the\napplication of ANOVA, Tukey HSD tests, GAMM, and clustering technique, offering\na robust and transparent approach to deciphering LLM performance data. Contrary\nto prevailing findings, our results challenge assumptions about emergent\nabilities and the influence of given training types and architectures in LLMs.\nThese findings furnish new perspectives on the characteristics, intrinsic\nnature, and developmental trajectories of LLMs. By providing straightforward\nand reliable methods to scrutinize and reassess LLM performance data, this\nstudy contributes a nuanced perspective on LLM efficiency and potentials.",
        "pdf_link": "https://arxiv.org/pdf/2403.15250v1.pdf"
    },
    {
        "title": "ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters",
        "authors": [
            "Shiwei Liu",
            "Guanchen Tao",
            "Yifei Zou",
            "Derek Chow",
            "Zichen Fan",
            "Kauna Lei",
            "Bangfei Pan",
            "Dennis Sylvester",
            "Gregory Kielian",
            "Mehdi Saligane"
        ],
        "published": "2024-01-31T17:52:52Z",
        "summary": "The self-attention mechanism sets transformer-based large language model\n(LLM) apart from the convolutional and recurrent neural networks. Despite the\nperformance improvement, achieving real-time LLM inference on silicon is\nchallenging due to the extensively used Softmax in self-attention. Apart from\nthe non-linearity, the low arithmetic intensity greatly reduces the processing\nparallelism, which becomes the bottleneck especially when dealing with a longer\ncontext. To address this challenge, we propose Constant Softmax (ConSmax), a\nsoftware-hardware co-design as an efficient Softmax alternative. ConSmax\nemploys differentiable normalization parameters to remove the maximum searching\nand denominator summation in Softmax. It allows for massive parallelization\nwhile performing the critical tasks of Softmax. In addition, a scalable ConSmax\nhardware utilizing a bitwidth-split look-up table (LUT) can produce lossless\nnon-linear operation and support mix-precision computing. It further\nfacilitates efficient LLM inference. Experimental results show that ConSmax\nachieves a minuscule power consumption of 0.43 mW and area of 0.001 mm2 at\n1-GHz working frequency and 22-nm CMOS technology. Compared to state-of-the-art\nSoftmax hardware, ConSmax results in 14.5x energy and 14.0x area savings with a\ncomparable accuracy on a GPT-2 model and the WikiText103 dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.10930v2.pdf"
    },
    {
        "title": "Are Large Language Models Rational Investors?",
        "authors": [
            "Yuhang Zhou",
            "Yuchen Ni",
            "Xiang Liu",
            "Jian Zhang",
            "Sen Liu",
            "Guangnan Ye",
            "Hongfeng Chai"
        ],
        "published": "2024-02-20T04:26:08Z",
        "summary": "Large Language Models (LLMs) are progressively being adopted in financial\nanalysis to harness their extensive knowledge base for interpreting complex\nmarket data and trends. However, their application in the financial domain is\nchallenged by intrinsic biases (i.e., risk-preference bias) and a superficial\ngrasp of market intricacies, underscoring the need for a thorough assessment of\ntheir financial insight. This study introduces a novel framework, Financial\nBias Indicators (FBI), to critically evaluate the financial rationality of\nLLMs, focusing on their ability to discern and navigate the subtleties of\nfinancial information and to identify any irrational biases that might skew\nmarket analysis.\n  Our research adopts an innovative methodology to measure financial\nrationality, integrating principles of behavioral finance to scrutinize the\nbiases and decision-making patterns of LLMs. We conduct a comprehensive\nevaluation of 19 leading LLMs, considering factors such as model scale,\ntraining datasets, input strategies, etc. The findings reveal varying degrees\nof financial irrationality among the models, influenced by their design and\ntraining. Models trained specifically on financial datasets might exhibit\ngreater irrationality, and it's possible that even larger financial language\nmodels (FinLLMs) could display more biases than smaller, more generalized\nmodels. This outcomes provide profound insights into how these elements affect\nthe financial rationality of LLMs, indicating that targeted training and\nstructured input methods could improve model performance. This work enriches\nour understanding of LLMs' strengths and weaknesses in financial applications,\nlaying the groundwork for the development of more dependable and rational\nfinancial analysis tools.",
        "pdf_link": "https://arxiv.org/pdf/2402.12713v1.pdf"
    },
    {
        "title": "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM",
        "authors": [
            "Jingcong Liang",
            "Rong Ye",
            "Meng Han",
            "Ruofei Lai",
            "Xinyu Zhang",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2024-03-12T18:19:47Z",
        "summary": "How can we construct an automated debate judge to evaluate an extensive,\nvibrant, multi-turn debate? This task is challenging, as judging a debate\ninvolves grappling with lengthy texts, intricate argument relationships, and\nmulti-dimensional assessments. At the same time, current research mainly\nfocuses on short dialogues, rarely touching upon the evaluation of an entire\ndebate. In this paper, by leveraging Large Language Models (LLMs), we propose\nDebatrix, which makes the analysis and assessment of multi-turn debates more\naligned with majority preferences. Specifically, Debatrix features a vertical,\niterative chronological analysis and a horizontal, multi-dimensional evaluation\ncollaboration. To align with real-world debate scenarios, we introduced the\nPanelBench benchmark, comparing our system's performance to actual debate\noutcomes. The findings indicate a notable enhancement over directly using LLMs\nfor debate evaluation. Source code and benchmark data are available online at\nhttps://github.com/ljcleo/Debatrix .",
        "pdf_link": "https://arxiv.org/pdf/2403.08010v1.pdf"
    },
    {
        "title": "Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction",
        "authors": [
            "Tong Liu",
            "Yingjie Zhang",
            "Zhe Zhao",
            "Yinpeng Dong",
            "Guozhu Meng",
            "Kai Chen"
        ],
        "published": "2024-02-28T06:50:14Z",
        "summary": "In recent years, large language models (LLMs) have demonstrated notable\nsuccess across various tasks, but the trustworthiness of LLMs is still an open\nproblem. One specific threat is the potential to generate toxic or harmful\nresponses. Attackers can craft adversarial prompts that induce harmful\nresponses from LLMs. In this work, we pioneer a theoretical foundation in LLMs\nsecurity by identifying bias vulnerabilities within the safety fine-tuning and\ndesign a black-box jailbreak method named DRA (Disguise and Reconstruction\nAttack), which conceals harmful instructions through disguise and prompts the\nmodel to reconstruct the original harmful instruction within its completion. We\nevaluate DRA across various open-source and close-source models, showcasing\nstate-of-the-art jailbreak success rates and attack efficiency. Notably, DRA\nboasts a 90\\% attack success rate on LLM chatbots GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.18104v1.pdf"
    },
    {
        "title": "An In-depth Evaluation of GPT-4 in Sentence Simplification with Error-based Human Assessment",
        "authors": [
            "Xuanxin Wu",
            "Yuki Arase"
        ],
        "published": "2024-03-08T00:19:24Z",
        "summary": "Sentence simplification, which rewrites a sentence to be easier to read and\nunderstand, is a promising technique to help people with various reading\ndifficulties. With the rise of advanced large language models (LLMs),\nevaluating their performance in sentence simplification has become imperative.\nRecent studies have used both automatic metrics and human evaluations to assess\nthe simplification abilities of LLMs. However, the suitability of existing\nevaluation methodologies for LLMs remains in question. First, the suitability\nof current automatic metrics on LLMs' simplification evaluation is still\nuncertain. Second, current human evaluation approaches in sentence\nsimplification often fall into two extremes: they are either too superficial,\nfailing to offer a clear understanding of the models' performance, or overly\ndetailed, making the annotation process complex and prone to inconsistency,\nwhich in turn affects the evaluation's reliability. To address these problems,\nthis study provides in-depth insights into LLMs' performance while ensuring the\nreliability of the evaluation. We design an error-based human annotation\nframework to assess the GPT-4's simplification capabilities. Results show that\nGPT-4 generally generates fewer erroneous simplification outputs compared to\nthe current state-of-the-art. However, LLMs have their limitations, as seen in\nGPT-4's struggles with lexical paraphrasing. Furthermore, we conduct\nmeta-evaluations on widely used automatic metrics using our human annotations.\nWe find that while these metrics are effective for significant quality\ndifferences, they lack sufficient sensitivity to assess the overall\nhigh-quality simplification by GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.04963v1.pdf"
    },
    {
        "title": "Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs",
        "authors": [
            "Minh-Vuong Nguyen",
            "Linhao Luo",
            "Fatemeh Shiri",
            "Dinh Phung",
            "Yuan-Fang Li",
            "Thuy-Trang Vu",
            "Gholamreza Haffari"
        ],
        "published": "2024-02-17T05:22:56Z",
        "summary": "Large language models (LLMs) demonstrate strong reasoning abilities when\nprompted to generate chain-of-thought (CoT) explanations alongside answers.\nHowever, previous research on evaluating LLMs has solely focused on answer\naccuracy, neglecting the correctness of the generated CoT. In this paper, we\ndelve deeper into the CoT reasoning capabilities of LLMs in multi-hop question\nanswering by utilizing knowledge graphs (KGs). We propose a novel\ndiscriminative and generative CoT evaluation paradigm to assess LLMs' knowledge\nof reasoning and the accuracy of the generated CoT. Through experiments\nconducted on 5 different families of LLMs across 2 multi-hop question-answering\ndatasets, we find that LLMs possess sufficient knowledge to perform reasoning.\nHowever, there exists a significant disparity between answer accuracy and\nfaithfulness of the CoT reasoning generated by LLMs, indicating that they often\narrive at correct answers through incorrect reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.11199v1.pdf"
    },
    {
        "title": "Komodo: A Linguistic Expedition into Indonesia's Regional Languages",
        "authors": [
            "Louis Owen",
            "Vishesh Tripathi",
            "Abhay Kumar",
            "Biddwan Ahmed"
        ],
        "published": "2024-03-14T13:12:21Z",
        "summary": "The recent breakthroughs in Large Language Models (LLMs) have mostly focused\non languages with easily available and sufficient resources, such as English.\nHowever, there remains a significant gap for languages that lack sufficient\nlinguistic resources in the public domain. Our work introduces Komodo-7B,\n7-billion-parameter Large Language Models designed to address this gap by\nseamlessly operating across Indonesian, English, and 11 regional languages in\nIndonesia. Komodo-7B is a family of LLMs that consist of Komodo-7B-Base and\nKomodo-7B-Instruct. Komodo-7B-Instruct stands out by achieving state-of-the-art\nperformance in various tasks and languages, outperforming the benchmarks set by\nOpenAI's GPT-3.5, Cohere's Aya-101, Llama-2-Chat-13B,\nMixtral-8x7B-Instruct-v0.1, Gemma-7B-it , and many more. This model not only\ndemonstrates superior performance in both language-specific and overall\nassessments but also highlights its capability to excel in linguistic\ndiversity. Our commitment to advancing language models extends beyond\nwell-resourced languages, aiming to bridge the gap for those with limited\nlinguistic assets. Additionally, Komodo-7B-Instruct's better cross-language\nunderstanding contributes to addressing educational disparities in Indonesia,\noffering direct translations from English to 11 regional languages, a\nsignificant improvement compared to existing language translation services.\nKomodo-7B represents a crucial step towards inclusivity and effectiveness in\nlanguage models, providing to the linguistic needs of diverse communities.",
        "pdf_link": "https://arxiv.org/pdf/2403.09362v2.pdf"
    },
    {
        "title": "Can Large Language Models Identify Authorship?",
        "authors": [
            "Baixiang Huang",
            "Canyu Chen",
            "Kai Shu"
        ],
        "published": "2024-03-13T03:22:02Z",
        "summary": "The ability to accurately identify authorship is crucial for verifying\ncontent authenticity and mitigating misinformation. Large Language Models\n(LLMs) have demonstrated exceptional capacity for reasoning and\nproblem-solving. However, their potential in authorship analysis, encompassing\nauthorship verification and attribution, remains underexplored. This paper\nconducts a comprehensive evaluation of LLMs in these critical tasks.\nTraditional studies have depended on hand-crafted stylistic features, whereas\nstate-of-the-art approaches leverage text embeddings from pre-trained language\nmodels. These methods, which typically require fine-tuning on labeled data,\noften suffer from performance degradation in cross-domain applications and\nprovide limited explainability. This work seeks to address three research\nquestions: (1) Can LLMs perform zero-shot, end-to-end authorship verification\neffectively? (2) Are LLMs capable of accurately attributing authorship among\nmultiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide\nexplainability in authorship analysis, particularly through the role of\nlinguistic features? Moreover, we investigate the integration of explicit\nlinguistic features to guide LLMs in their reasoning processes. Our extensive\nassessment demonstrates LLMs' proficiency in both tasks without the need for\ndomain-specific fine-tuning, providing insights into their decision-making via\na detailed analysis of linguistic features. This establishes a new benchmark\nfor future research on LLM-based authorship analysis. The code and data are\navailable at https://github.com/baixianghuang/authorship-llm.",
        "pdf_link": "https://arxiv.org/pdf/2403.08213v1.pdf"
    },
    {
        "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "published": "2024-02-20T17:44:06Z",
        "summary": "While large language models (LLMs) have achieved state-of-the-art performance\non a wide range of medical question answering (QA) tasks, they still face\nchallenges with hallucinations and outdated knowledge. Retrieval-augmented\ngeneration (RAG) is a promising solution and has been widely adopted. However,\na RAG system can involve multiple flexible components, and there is a lack of\nbest practices regarding the optimal RAG setting for various medical purposes.\nTo systematically evaluate such systems, we propose the Medical Information\nRetrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind\nbenchmark including 7,663 questions from five medical QA datasets. Using\nMIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt\ntokens on 41 combinations of different corpora, retrievers, and backbone LLMs\nthrough the MedRAG toolkit introduced in this work. Overall, MedRAG improves\nthe accuracy of six different LLMs by up to 18% over chain-of-thought\nprompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our\nresults show that the combination of various medical corpora and retrievers\nachieves the best performance. In addition, we discovered a log-linear scaling\nproperty and the \"lost-in-the-middle\" effects in medical RAG. We believe our\ncomprehensive evaluations can serve as practical guidelines for implementing\nRAG systems for medicine.",
        "pdf_link": "https://arxiv.org/pdf/2402.13178v2.pdf"
    },
    {
        "title": "Clue-Guided Path Exploration: An Efficient Knowledge Base Question-Answering Framework with Low Computational Resource Consumption",
        "authors": [
            "Dehao Tao",
            "Feng Huang",
            "Yongfeng Huang",
            "Minghu Jiang"
        ],
        "published": "2024-01-24T13:36:50Z",
        "summary": "In recent times, large language models (LLMs) have showcased remarkable\ncapabilities. However, updating their knowledge poses challenges, potentially\nleading to inaccuracies when confronted with unfamiliar queries. While\nintegrating knowledge graphs with LLMs has been explored, existing approaches\ntreat LLMs as primary decision-makers, imposing high demands on their\ncapabilities. This is particularly unsuitable for LLMs with lower computational\ncosts and relatively poorer performance. In this paper, we introduce a\nClue-Guided Path Exploration framework (CGPE) that efficiently merges a\nknowledge base with an LLM, placing less stringent requirements on the model's\ncapabilities. Inspired by the method humans use to manually retrieve knowledge,\nCGPE employs information from the question as clues to systematically explore\nthe required knowledge path within the knowledge base. Experiments on\nopen-source datasets reveal that CGPE outperforms previous methods and is\nhighly applicable to LLMs with fewer parameters. In some instances, even\nChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.\nFurthermore, the results indicate a minimal invocation frequency of CGPE on\nLLMs, suggesting reduced computational overhead. For organizations and\nindividuals facing constraints in computational resources, our research offers\nsignificant practical value.",
        "pdf_link": "https://arxiv.org/pdf/2401.13444v1.pdf"
    },
    {
        "title": "LLMGuard: Guarding Against Unsafe LLM Behavior",
        "authors": [
            "Shubh Goyal",
            "Medha Hira",
            "Shubham Mishra",
            "Sukriti Goyal",
            "Arnav Goel",
            "Niharika Dadu",
            "Kirushikesh DB",
            "Sameep Mehta",
            "Nishtha Madaan"
        ],
        "published": "2024-02-27T10:22:45Z",
        "summary": "Although the rise of Large Language Models (LLMs) in enterprise settings\nbrings new opportunities and capabilities, it also brings challenges, such as\nthe risk of generating inappropriate, biased, or misleading content that\nviolates regulations and can have legal concerns. To alleviate this, we present\n\"LLMGuard\", a tool that monitors user interactions with an LLM application and\nflags content against specific behaviours or conversation topics. To do this\nrobustly, LLMGuard employs an ensemble of detectors.",
        "pdf_link": "https://arxiv.org/pdf/2403.00826v1.pdf"
    },
    {
        "title": "CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora",
        "authors": [
            "Zijun Long",
            "Xuri Ge",
            "Richard Mccreadie",
            "Joemon Jose"
        ],
        "published": "2024-02-23T11:47:16Z",
        "summary": "Text-to-image retrieval aims to find the relevant images based on a text\nquery, which is important in various use-cases, such as digital libraries,\ne-commerce, and multimedia databases. Although Multimodal Large Language Models\n(MLLMs) demonstrate state-of-the-art performance, they exhibit limitations in\nhandling large-scale, diverse, and ambiguous real-world needs of retrieval, due\nto the computation cost and the injective embeddings they produce. This paper\npresents a two-stage Coarse-to-Fine Index-shared Retrieval (CFIR) framework,\ndesigned for fast and effective large-scale long-text to image retrieval. The\nfirst stage, Entity-based Ranking (ER), adapts to long-text query ambiguity by\nemploying a multiple-queries-to-multiple-targets paradigm, facilitating\ncandidate filtering for the next stage. The second stage, Summary-based\nRe-ranking (SR), refines these rankings using summarized queries. We also\npropose a specialized Decoupling-BEiT-3 encoder, optimized for handling\nambiguous user needs and both stages, which also enhances computational\nefficiency through vector-based similarity inference. Evaluation on the AToMiC\ndataset reveals that CFIR surpasses existing MLLMs by up to 11.06% in\nRecall@1000, while reducing training and retrieval times by 68.75% and 99.79%,\nrespectively. We will release our code to facilitate future research at\nhttps://github.com/longkukuhi/CFIR.",
        "pdf_link": "https://arxiv.org/pdf/2402.15276v3.pdf"
    },
    {
        "title": "MixRED: A Mix-lingual Relation Extraction Dataset",
        "authors": [
            "Lingxing Kong",
            "Yougang Chu",
            "Zheng Ma",
            "Jianbing Zhang",
            "Liang He",
            "Jiajun Chen"
        ],
        "published": "2024-03-23T03:18:14Z",
        "summary": "Relation extraction is a critical task in the field of natural language\nprocessing with numerous real-world applications. Existing research primarily\nfocuses on monolingual relation extraction or cross-lingual enhancement for\nrelation extraction. Yet, there remains a significant gap in understanding\nrelation extraction in the mix-lingual (or code-switching) scenario, where\nindividuals intermix contents from different languages within sentences,\ngenerating mix-lingual content. Due to the lack of a dedicated dataset, the\neffectiveness of existing relation extraction models in such a scenario is\nlargely unexplored. To address this issue, we introduce a novel task of\nconsidering relation extraction in the mix-lingual scenario called MixRE and\nconstructing the human-annotated dataset MixRED to support this task. In\naddition to constructing the MixRED dataset, we evaluate both state-of-the-art\nsupervised models and large language models (LLMs) on MixRED, revealing their\nrespective advantages and limitations in the mix-lingual scenario. Furthermore,\nwe delve into factors influencing model performance within the MixRE task and\nuncover promising directions for enhancing the performance of both supervised\nmodels and LLMs in this novel task.",
        "pdf_link": "https://arxiv.org/pdf/2403.15696v1.pdf"
    },
    {
        "title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond",
        "authors": [
            "Xinyu Wang",
            "Hainiu Xu",
            "Lin Gui",
            "Yulan He"
        ],
        "published": "2024-02-22T13:13:31Z",
        "summary": "Task embedding, a meta-learning technique that captures task-specific\ninformation, has become prevalent, especially in areas such as multi-task\nlearning, model editing, and interpretability. However, it faces challenges\nwith the emergence of prompt-guided Large Language Models (LLMs) operating in a\ngradientfree manner. Existing task embedding methods rely on fine-tuned,\ntask-specific language models, which hinders the adaptability of task\nembeddings across diverse models, especially prompt-based LLMs. To unleash the\npower of task embedding in the era of LLMs, we propose a framework for unified\ntask embeddings (FUTE), harmonizing task embeddings from various models,\nincluding smaller language models and LLMs with varied prompts, within a single\nvector space. Such uniformity enables the comparison and analysis of\nsimilarities amongst different models, extending the scope and utility of\nexisting task embedding methods in addressing multi-model scenarios, whilst\nmaintaining their performance to be comparable to architecture-specific\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.14522v1.pdf"
    },
    {
        "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
        "authors": [
            "Jielin Qiu",
            "Andrea Madotto",
            "Zhaojiang Lin",
            "Paul A. Crook",
            "Yifan Ethan Xu",
            "Xin Luna Dong",
            "Christos Faloutsos",
            "Lei Li",
            "Babak Damavandi",
            "Seungwhan Moon"
        ],
        "published": "2024-03-07T18:38:17Z",
        "summary": "Vision-extended LLMs have made significant strides in Visual Question\nAnswering (VQA). Despite these advancements, VLLMs still encounter substantial\ndifficulties in handling queries involving long-tail entities, with a tendency\nto produce erroneous or hallucinated responses. In this work, we introduce a\nnovel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for\nentity-centric VQA. This task aims to test the models' capabilities in\nidentifying entities and providing detailed, entity-specific knowledge. We have\ndeveloped the \\textbf{SnapNTell Dataset}, distinct from traditional VQA\ndatasets: (1) It encompasses a wide range of categorized entities, each\nrepresented by images and explicitly named in the answers; (2) It features QA\npairs that require extensive knowledge for accurate responses. The dataset is\norganized into 22 major categories, containing 7,568 unique entities in total.\nFor each entity, we curated 10 illustrative images and crafted 10\nknowledge-intensive QA pairs. To address this novel task, we devised a\nscalable, efficient, and transparent retrieval-augmented multimodal LLM. Our\napproach markedly outperforms existing methods on the SnapNTell dataset,\nachieving a 66.5\\% improvement in the BELURT score. We will soon make the\ndataset and the source code publicly accessible.",
        "pdf_link": "https://arxiv.org/pdf/2403.04735v1.pdf"
    },
    {
        "title": "Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections",
        "authors": [
            "Gaurav Verma",
            "Minje Choi",
            "Kartik Sharma",
            "Jamelle Watson-Daniels",
            "Sejoon Oh",
            "Srijan Kumar"
        ],
        "published": "2024-02-26T18:56:48Z",
        "summary": "Multimodal large language models (MLLMs) like LLaVA and GPT-4(V) enable\ngeneral-purpose conversations about images with the language modality. As\noff-the-shelf MLLMs may have limited capabilities on images from domains like\ndermatology and agriculture, they must be fine-tuned to unlock domain-specific\napplications. The prevalent architecture of current open-source MLLMs comprises\ntwo major modules: an image-language (cross-modal) projection network and a\nlarge language model. It is desirable to understand the roles of these two\nmodules in modeling domain-specific visual attributes to inform the design of\nfuture models and streamline the interpretability efforts on the current\nmodels. To this end, via experiments on 4 datasets and under 2 fine-tuning\nsettings, we find that as the MLLM is fine-tuned, it indeed gains\ndomain-specific visual capabilities, but the updates do not lead to the\nprojection extracting relevant domain-specific visual attributes. Our results\nindicate that the domain-specific visual attributes are modeled by the LLM,\neven when only the projection is fine-tuned. Through this study, we offer a\npotential reinterpretation of the role of cross-modal projections in MLLM\narchitectures. Projection webpage:\nhttps://claws-lab.github.io/projection-in-MLLMs/",
        "pdf_link": "https://arxiv.org/pdf/2402.16832v1.pdf"
    },
    {
        "title": "Actor Identification in Discourse: A Challenge for LLMs?",
        "authors": [
            "Ana Bari\u0107",
            "Sean Papay",
            "Sebastian Pad\u00f3"
        ],
        "published": "2024-02-01T14:30:39Z",
        "summary": "The identification of political actors who put forward claims in public\ndebate is a crucial step in the construction of discourse networks, which are\nhelpful to analyze societal debates. Actor identification is, however, rather\nchallenging: Often, the locally mentioned speaker of a claim is only a pronoun\n(\"He proposed that [claim]\"), so recovering the canonical actor name requires\ndiscourse understanding. We compare a traditional pipeline of dedicated NLP\ncomponents (similar to those applied to the related task of coreference) with a\nLLM, which appears a good match for this generation task. Evaluating on a\ncorpus of German actors in newspaper reports, we find surprisingly that the LLM\nperforms worse. Further analysis reveals that the LLM is very good at\nidentifying the right reference, but struggles to generate the correct\ncanonical form. This points to an underlying issue in LLMs with controlling\ngenerated output. Indeed, a hybrid model combining the LLM with a classifier to\nnormalize its output substantially outperforms both initial models.",
        "pdf_link": "https://arxiv.org/pdf/2402.00620v1.pdf"
    },
    {
        "title": "Large Language Models in Biomedical and Health Informatics: A Bibliometric Review",
        "authors": [
            "Huizi Yu",
            "Lizhou Fan",
            "Lingyao Li",
            "Jiayan Zhou",
            "Zihui Ma",
            "Lu Xian",
            "Wenyue Hua",
            "Sijia He",
            "Mingyu Jin",
            "Yongfeng Zhang",
            "Ashvin Gandhi",
            "Xin Ma"
        ],
        "published": "2024-03-24T21:29:39Z",
        "summary": "Large Language Models (LLMs) have rapidly become important tools in\nBiomedical and Health Informatics (BHI), enabling new ways to analyze data,\ntreat patients, and conduct research. This bibliometric review aims to provide\na panoramic view of how LLMs have been used in BHI by examining research\narticles and collaboration networks from 2022 to 2023. It further explores how\nLLMs can improve Natural Language Processing (NLP) applications in various BHI\nareas like medical diagnosis, patient engagement, electronic health record\nmanagement, and personalized medicine. To do this, our bibliometric review\nidentifies key trends, maps out research networks, and highlights major\ndevelopments in this fast-moving field. Lastly, it discusses the ethical\nconcerns and practical challenges of using LLMs in BHI, such as data privacy\nand reliable medical recommendations. Looking ahead, we consider how LLMs could\nfurther transform biomedical research as well as healthcare delivery and\npatient outcomes. This bibliometric review serves as a resource for\nstakeholders in healthcare, including researchers, clinicians, and\npolicymakers, to understand the current state and future potential of LLMs in\nBHI.",
        "pdf_link": "https://arxiv.org/pdf/2403.16303v2.pdf"
    },
    {
        "title": "Analyzing the Effectiveness of Large Language Models on Text-to-SQL Synthesis",
        "authors": [
            "Richard Roberson",
            "Gowtham Kaki",
            "Ashutosh Trivedi"
        ],
        "published": "2024-01-22T22:05:42Z",
        "summary": "This study investigates various approaches to using Large Language Models\n(LLMs) for Text-to-SQL program synthesis, focusing on the outcomes and insights\nderived. Employing the popular Text-to-SQL dataset, spider, the goal was to\ninput a natural language question along with the database schema and output the\ncorrect SQL SELECT query. The initial approach was to fine-tune a local and\nopen-source model to generate the SELECT query. After QLoRa fine-tuning\nWizardLM's WizardCoder-15B model on the spider dataset, the execution accuracy\nfor generated queries rose to a high of 61%. With the second approach, using\nthe fine-tuned gpt-3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error\ncorrection), the execution accuracy reached a high of 82.1%. Of all the\nincorrect queries, most can be categorized into a seven different categories of\nwhat went wrong: selecting the wrong columns or wrong order of columns,\ngrouping by the wrong column, predicting the wrong values in conditionals,\nusing different aggregates than the ground truth, extra or too few JOIN\nclauses, inconsistencies in the Spider dataset, and lastly completely incorrect\nquery structure. Most if not all of the queries fall into these categories and\nit is insightful to understanding where the faults still lie with LLM program\nsynthesis and where they can be improved.",
        "pdf_link": "https://arxiv.org/pdf/2401.12379v1.pdf"
    },
    {
        "title": "Depending on yourself when you should: Mentoring LLM with RL agents to become the master in cybersecurity games",
        "authors": [
            "Yikuan Yan",
            "Yaolun Zhang",
            "Keman Huang"
        ],
        "published": "2024-03-26T13:02:46Z",
        "summary": "Integrating LLM and reinforcement learning (RL) agent effectively to achieve\ncomplementary performance is critical in high stake tasks like cybersecurity\noperations. In this study, we introduce SecurityBot, a LLM agent mentored by\npre-trained RL agents, to support cybersecurity operations. In particularly,\nthe LLM agent is supported with a profile module to generated behavior\nguidelines, a memory module to accumulate local experiences, a reflection\nmodule to re-evaluate choices, and an action module to reduce action space.\nAdditionally, it adopts the collaboration mechanism to take suggestions from\npre-trained RL agents, including a cursor for dynamic suggestion taken, an\naggregator for multiple mentors' suggestions ranking and a caller for proactive\nsuggestion asking. Building on the CybORG experiment framework, our experiences\nshow that SecurityBot demonstrates significant performance improvement compared\nwith LLM or RL standalone, achieving the complementary performance in the\ncybersecurity games.",
        "pdf_link": "https://arxiv.org/pdf/2403.17674v1.pdf"
    },
    {
        "title": "An Entropy-based Text Watermarking Detection Method",
        "authors": [
            "Yijian Lu",
            "Aiwei Liu",
            "Dianzhi Yu",
            "Jingjing Li",
            "Irwin King"
        ],
        "published": "2024-03-20T10:40:01Z",
        "summary": "Currently, text watermarking algorithms for large language models (LLMs) can\nembed hidden features to texts generated by LLMs to facilitate subsequent\ndetection, thus alleviating the problem of misuse of LLMs. Although the current\ntext watermarking algorithms perform well in most high-entropy scenarios, its\nperformance in low-entropy scenarios still needs to be improved. In this work,\nwe proposed that the influence of token entropy should be fully considered in\nthe watermark detection process, that is, the weight of each token during\nwatermark detection should be adjusted according to its entropy, rather than\nsetting the weights of all tokens to the same value as in previous methods.\nSpecifically, we proposed an Entropy-based Watermark Detection (EWD) that gives\nhigher-entropy tokens higher influence weights during watermark detection, so\nas to better reflect the degree of watermarking. Furthermore, the proposed\ndetection process is training-free and fully automated. In the experiment, we\nfound that our method can achieve better detection performance in low-entropy\nscenarios, and our method is also general and can be applied to texts with\ndifferent entropy distributions. Our code and data will be available online.",
        "pdf_link": "https://arxiv.org/pdf/2403.13485v2.pdf"
    },
    {
        "title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization",
        "authors": [
            "Wenqi Zhang",
            "Ke Tang",
            "Hai Wu",
            "Mengna Wang",
            "Yongliang Shen",
            "Guiyang Hou",
            "Zeqi Tan",
            "Peng Li",
            "Yueting Zhuang",
            "Weiming Lu"
        ],
        "published": "2024-02-27T15:09:20Z",
        "summary": "Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.",
        "pdf_link": "https://arxiv.org/pdf/2402.17574v2.pdf"
    },
    {
        "title": "WinoViz: Probing Visual Properties of Objects Under Different States",
        "authors": [
            "Woojeong Jin",
            "Tejas Srinivasan",
            "Jesse Thomason",
            "Xiang Ren"
        ],
        "published": "2024-02-21T07:31:47Z",
        "summary": "Humans perceive and comprehend different visual properties of an object based\non specific contexts. For instance, we know that a banana turns brown ``when it\nbecomes rotten,'' whereas it appears green ``when it is unripe.'' Previous\nstudies on probing visual commonsense knowledge have primarily focused on\nexamining language models' understanding of typical properties (e.g., colors\nand shapes) of objects. We present WinoViz, a text-only evaluation dataset,\nconsisting of 1,380 examples that probe the reasoning abilities of language\nmodels regarding variant visual properties of objects under different contexts\nor states. Our task is challenging since it requires pragmatic reasoning\n(finding intended meanings) and visual knowledge reasoning. We also present\nmulti-hop data, a more challenging version of our data, which requires\nmulti-step reasoning chains to solve our task. In our experimental analysis,\nour findings are: a) Large language models such as GPT-4 demonstrate effective\nperformance, but when it comes to multi-hop data, their performance is\nsignificantly degraded. b) Large models perform well on pragmatic reasoning,\nbut visual knowledge reasoning is a bottleneck in our task. c) Vision-language\nmodels outperform their language-model counterparts. d) A model with\nmachine-generated images performs poorly in our task. This is due to the poor\nquality of the generated images.",
        "pdf_link": "https://arxiv.org/pdf/2402.13584v1.pdf"
    },
    {
        "title": "TeleChat Technical Report",
        "authors": [
            "Zhongjiang He",
            "Zihan Wang",
            "Xinzhang Liu",
            "Shixuan Liu",
            "Yitong Yao",
            "Yuyao Huang",
            "Xuelong Li",
            "Yongxiang Li",
            "Zhonghao Che",
            "Zhaoxi Zhang",
            "Yan Wang",
            "Xin Wang",
            "Luwen Pu",
            "Huinan Xu",
            "Ruiyu Fang",
            "Yu Zhao",
            "Jie Zhang",
            "Xiaomeng Huang",
            "Zhilong Lu",
            "Jiaxin Peng",
            "Wenjun Zheng",
            "Shiquan Wang",
            "Bingkai Yang",
            "Xuewei he",
            "Zhuoru Jiang",
            "Qiyi Xie",
            "Yanhan Zhang",
            "Zhongqiu Li",
            "Lingling Shi",
            "Weiwei Fu",
            "Yin Zhang",
            "Zilu Huang",
            "Sishi Xiong",
            "Yuxiang Zhang",
            "Chao Wang",
            "Shuangyong Song"
        ],
        "published": "2024-01-08T10:43:19Z",
        "summary": "In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.",
        "pdf_link": "https://arxiv.org/pdf/2401.03804v2.pdf"
    },
    {
        "title": "Towards a Robust Retrieval-Based Summarization System",
        "authors": [
            "Shengjie Liu",
            "Jing Wu",
            "Jingyuan Bao",
            "Wenyi Wang",
            "Naira Hovakimyan",
            "Christopher G Healey"
        ],
        "published": "2024-03-29T00:14:46Z",
        "summary": "This paper describes an investigation of the robustness of large language\nmodels (LLMs) for retrieval augmented generation (RAG)-based summarization\ntasks. While LLMs provide summarization capabilities, their performance in\ncomplex, real-world scenarios remains under-explored. Our first contribution is\nLogicSumm, an innovative evaluation framework incorporating realistic scenarios\nto assess LLM robustness during RAG-based summarization. Based on limitations\nidentified by LogiSumm, we then developed SummRAG, a comprehensive system to\ncreate training dialogues and fine-tune a model to enhance robustness within\nLogicSumm's scenarios. SummRAG is an example of our goal of defining structured\nmethods to test the capabilities of an LLM, rather than addressing issues in a\none-off fashion. Experimental results confirm the power of SummRAG, showcasing\nimproved logical coherence and summarization quality. Data, corresponding model\nweights, and Python code are available online.",
        "pdf_link": "https://arxiv.org/pdf/2403.19889v1.pdf"
    },
    {
        "title": "Monotonic Paraphrasing Improves Generalization of Language Model Prompting",
        "authors": [
            "Qin Liu",
            "Fei Wang",
            "Nan Xu",
            "Tianyi Yan",
            "Tao Meng",
            "Muhao Chen"
        ],
        "published": "2024-03-24T06:49:07Z",
        "summary": "Performance of large language models (LLMs) may vary with different prompts\nor instructions of even the same task. One commonly recognized factor for this\nphenomenon is the model's familiarity with the given prompt or instruction,\nwhich is typically estimated by its perplexity. However, finding the prompt\nwith the lowest perplexity is challenging, given the enormous space of possible\nprompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),\nan end-to-end decoding strategy that paraphrases given prompts or instructions\ninto their lower perplexity counterparts based on an ensemble of a paraphrase\nLM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or\ninstruction executor) that constrains the generation for lower perplexity. The\nensemble decoding process can efficiently paraphrase the original prompt\nwithout altering its semantic meaning, while monotonically decreasing the\nperplexity of each generation as calculated by the target LM. We explore in\ndetail both greedy and search-based decoding as two alternative decoding\nschemes of MonoPara. Notably, MonoPara does not require any training and can\nmonotonically lower the perplexity of the paraphrased prompt or instruction,\nleading to improved performance of zero-shot LM prompting as evaluated on a\nwide selection of tasks. In addition, MonoPara is also shown to effectively\nimprove LMs' generalization on perturbed and unseen task instructions.",
        "pdf_link": "https://arxiv.org/pdf/2403.16038v1.pdf"
    },
    {
        "title": "CriticBench: Evaluating Large Language Models as Critic",
        "authors": [
            "Tian Lan",
            "Wenwei Zhang",
            "Chen Xu",
            "Heyan Huang",
            "Dahua Lin",
            "Kai Chen",
            "Xian-ling Mao"
        ],
        "published": "2024-02-21T12:38:59Z",
        "summary": "Critique ability are crucial in the scalable oversight and self-improvement\nof Large Language Models (LLMs). While many recent studies explore the critique\nability of LLMs to judge and refine flaws in generations, how to\ncomprehensively and reliably measure the critique abilities of LLMs is\nunder-explored. This paper introduces CriticBench, a novel benchmark designed\nto comprehensively and reliably evaluate four key critique ability dimensions\nof LLMs: feedback, comparison, refinement and meta-feedback. CriticBench\nencompasses nine diverse tasks, each assessing the LLMs' ability to critique\nresponses at varying levels of quality granularity. Our extensive evaluations\nof open-source and closed-source LLMs reveal intriguing relationships between\nthe critique ability and tasks, response qualities, and model scales. Datasets,\nresources and evaluation toolkit for CriticBench will be publicly released at\nhttps://github.com/open-compass/CriticBench.",
        "pdf_link": "https://arxiv.org/pdf/2402.13764v3.pdf"
    },
    {
        "title": "Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency",
        "authors": [
            "Toyin Aguda",
            "Suchetha Siddagangappa",
            "Elena Kochkina",
            "Simerjot Kaur",
            "Dongsheng Wang",
            "Charese Smiley",
            "Sameena Shah"
        ],
        "published": "2024-03-26T23:32:52Z",
        "summary": "Collecting labeled datasets in finance is challenging due to scarcity of\ndomain experts and higher cost of employing them. While Large Language Models\n(LLMs) have demonstrated remarkable performance in data annotation tasks on\ngeneral domain datasets, their effectiveness on domain specific datasets\nremains underexplored. To address this gap, we investigate the potential of\nLLMs as efficient data annotators for extracting relations in financial\ndocuments. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,\nand MPT Instruct) against expert annotators and crowdworkers. We demonstrate\nthat the current state-of-the-art LLMs can be sufficient alternatives to\nnon-expert crowdworkers. We analyze models using various prompts and parameter\nsettings and find that customizing the prompts for each relation group by\nproviding specific examples belonging to those groups is paramount.\nFurthermore, we introduce a reliability index (LLM-RelIndex) used to identify\noutputs that may require expert attention. Finally, we perform an extensive\ntime, cost and error analysis and provide recommendations for the collection\nand usage of automated annotations in domain-specific settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.18152v1.pdf"
    },
    {
        "title": "LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation",
        "authors": [
            "Zeyuan Ma",
            "Hongshu Guo",
            "Jiacheng Chen",
            "Guojun Peng",
            "Zhiguang Cao",
            "Yining Ma",
            "Yue-Jiao Gong"
        ],
        "published": "2024-03-02T08:21:59Z",
        "summary": "Recent research explores optimization using large language models (LLMs) by\neither iteratively seeking next-step solutions from LLMs or directly prompting\nLLMs for an optimizer. However, these approaches exhibit inherent limitations,\nincluding low operational efficiency, high sensitivity to prompt design, and a\nlack of domain-specific knowledge. We introduce LLaMoCo, the first\ninstruction-tuning framework designed to adapt LLMs for solving optimization\nproblems in a code-to-code manner. Specifically, we establish a comprehensive\ninstruction set containing well-described problem prompts and effective\noptimization codes. We then develop a novel two-phase learning strategy that\nincorporates a contrastive learning-based warm-up procedure before the\ninstruction-tuning phase to enhance the convergence behavior during model\nfine-tuning. The experiment results demonstrate that a CodeGen (350M) model\nfine-tuned by our LLaMoCo achieves superior optimization performance compared\nto GPT-4 Turbo and the other competitors across both synthetic and realistic\nproblem sets. The fine-tuned model and the usage instructions are available at\nhttps://anonymous.4open.science/r/LLaMoCo-722A.",
        "pdf_link": "https://arxiv.org/pdf/2403.01131v2.pdf"
    },
    {
        "title": "Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning",
        "authors": [
            "Ming Dong",
            "Kang Xue",
            "Bolong Zheng",
            "Tingting He"
        ],
        "published": "2024-03-13T12:50:23Z",
        "summary": "In view of the huge number of parameters of Large language models (LLMs) ,\ntuning all parameters is very costly, and accordingly fine-tuning specific\nparameters is more sensible. Most of parameter efficient fine-tuning (PEFT)\nconcentrate on parameter selection strategies, such as additive method,\nselective method and reparametrization-based method. However, there are few\nmethods that consider the impact of data samples on parameter selecting, such\nas Fish Mask based method. Fish Mask randomly choose a part of data samples and\ntreat them equally during parameter selection, which is unable to dynamically\nselect optimal parameters for inconstant data distributions. In this work, we\nadopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline\nI}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline\nD}$ecreasing) algorithm to search the best setting of sample-parameter pair for\nFISH Mask. In each iteration, by searching the set of samples and parameters\nwith larger Fish information, IRD can find better sample-parameter pair in most\nscale. We demonstrate the effectiveness and rationality of proposed strategy by\nconducting experiments on GLUE benchmark. Experimental results show our\nstrategy optimizes the parameter selection and achieves preferable performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.08484v1.pdf"
    },
    {
        "title": "RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots",
        "authors": [
            "Philip Feldman. James R. Foulds",
            "Shimei Pan"
        ],
        "published": "2024-03-02T12:19:04Z",
        "summary": "Large language models (LLMs) like ChatGPT demonstrate the remarkable progress\nof artificial intelligence. However, their tendency to hallucinate -- generate\nplausible but false information -- poses a significant challenge. This issue is\ncritical, as seen in recent court cases where ChatGPT's use led to citations of\nnon-existent legal rulings. This paper explores how Retrieval-Augmented\nGeneration (RAG) can counter hallucinations by integrating external knowledge\nwith prompts. We empirically evaluate RAG against standard LLMs using prompts\ndesigned to induce hallucinations. Our results show that RAG increases accuracy\nin some cases, but can still be misled when prompts directly contradict the\nmodel's pre-trained understanding. These findings highlight the complex nature\nof hallucinations and the need for more robust solutions to ensure LLM\nreliability in real-world applications. We offer practical recommendations for\nRAG deployment and discuss implications for the development of more trustworthy\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01193v2.pdf"
    },
    {
        "title": "Physio: An LLM-Based Physiotherapy Advisor",
        "authors": [
            "R\u00faben Almeida",
            "Hugo Sousa",
            "Lu\u00eds F. Cunha",
            "Nuno Guimar\u00e3es",
            "Ricardo Campos",
            "Al\u00edpio Jorge"
        ],
        "published": "2024-01-03T16:42:13Z",
        "summary": "The capabilities of the most recent language models have increased the\ninterest in integrating them into real-world applications. However, the fact\nthat these models generate plausible, yet incorrect text poses a constraint\nwhen considering their use in several domains. Healthcare is a prime example of\na domain where text-generative trustworthiness is a hard requirement to\nsafeguard patient well-being. In this paper, we present Physio, a chat-based\napplication for physical rehabilitation. Physio is capable of making an initial\ndiagnosis while citing reliable health sources to support the information\nprovided. Furthermore, drawing upon external knowledge databases, Physio can\nrecommend rehabilitation exercises and over-the-counter medication for symptom\nrelief. By combining these features, Physio can leverage the power of\ngenerative models for language processing while also conditioning its response\non dependable and verifiable sources. A live demo of Physio is available at\nhttps://physio.inesctec.pt.",
        "pdf_link": "https://arxiv.org/pdf/2401.01825v1.pdf"
    },
    {
        "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens",
        "authors": [
            "Jiacheng Liu",
            "Sewon Min",
            "Luke Zettlemoyer",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "published": "2024-01-30T19:03:49Z",
        "summary": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2401.17377v3.pdf"
    },
    {
        "title": "Cross-modality debiasing: using language to mitigate sub-population shifts in imaging",
        "authors": [
            "Yijiang Pang",
            "Bao Hoang",
            "Jiayu Zhou"
        ],
        "published": "2024-02-02T18:54:48Z",
        "summary": "Sub-population shift is a specific type of domain shift that highlights\nchanges in data distribution within specific sub-groups or populations between\ntraining and testing. Sub-population shift accounts for a significant source of\nalgorithmic bias and calls for distributional robustness. Recent studies found\ninherent distributional robustness in multi-modality foundation models, such as\nthe vision-language model CLIP, yet this robustness is vulnerable through\nparameter fine-tuning. In this paper, we propose leveraging the connection of\nrobustness among different modalities and reshaping the distributional\nrobustness of one modality with another. Specifically, in the context of the\ndistributional robustness of CLIP, we propose to leverage natural language\ninputs to debias the image feature representations, to improve worst-case\nperformance on sub-populations. Our extensive empirical studies show that image\nrepresentations debiased by natural language can achieve significant\nperformance improvement and reduction of performance instability under\nsub-population shifts.",
        "pdf_link": "https://arxiv.org/pdf/2403.07888v2.pdf"
    },
    {
        "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
        "authors": [
            "Yiqi Wang",
            "Wentao Chen",
            "Xiaotian Han",
            "Xudong Lin",
            "Haiteng Zhao",
            "Yongfei Liu",
            "Bohan Zhai",
            "Jianbo Yuan",
            "Quanzeng You",
            "Hongxia Yang"
        ],
        "published": "2024-01-10T15:29:21Z",
        "summary": "Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence\n(AGI) with abstract reasoning ability is the goal of next-generation AI. Recent\nadvancements in Large Language Models (LLMs), along with the emerging field of\nMultimodal Large Language Models (MLLMs), have demonstrated impressive\ncapabilities across a wide range of multimodal tasks and applications.\nParticularly, various MLLMs, each with distinct model architectures, training\ndata, and training stages, have been evaluated across a broad range of MLLM\nbenchmarks. These studies have, to varying degrees, revealed different aspects\nof the current capabilities of MLLMs. However, the reasoning abilities of MLLMs\nhave not been systematically investigated. In this survey, we comprehensively\nreview the existing evaluation protocols of multimodal reasoning, categorize\nand illustrate the frontiers of MLLMs, introduce recent trends in applications\nof MLLMs on reasoning-intensive tasks, and finally discuss current practices\nand future directions. We believe our survey establishes a solid base and sheds\nlight on this important topic, multimodal reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2401.06805v2.pdf"
    },
    {
        "title": "Solving Data-centric Tasks using Large Language Models",
        "authors": [
            "Shraddha Barke",
            "Christian Poelitz",
            "Carina Suzana Negreanu",
            "Benjamin Zorn",
            "Jos\u00e9 Cambronero",
            "Andrew D. Gordon",
            "Vu Le",
            "Elnaz Nouri",
            "Nadia Polikarpova",
            "Advait Sarkar",
            "Brian Slininger",
            "Neil Toronto",
            "Jack Williams"
        ],
        "published": "2024-02-18T23:19:21Z",
        "summary": "Large language models (LLMs) are rapidly replacing help forums like\nStackOverflow, and are especially helpful for non-professional programmers and\nend users. These users are often interested in data-centric tasks, such as\nspreadsheet manipulation and data wrangling, which are hard to solve if the\nintent is only communicated using a natural-language description, without\nincluding the data. But how do we decide how much data and which data to\ninclude in the prompt? This paper makes two contributions towards answering\nthis question. First, we create a dataset of real-world NL-to-code tasks\nmanipulating tabular data, mined from StackOverflow posts. Second, we introduce\na cluster-then-select prompting technique, which adds the most representative\nrows from the input data to the LLM prompt. Our experiments show that LLM\nperformance is indeed sensitive to the amount of data passed in the prompt, and\nthat for tasks with a lot of syntactic variation in the input table, our\ncluster-then-select technique outperforms a random selection baseline.",
        "pdf_link": "https://arxiv.org/pdf/2402.11734v2.pdf"
    },
    {
        "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
        "authors": [
            "Neeloy Chakraborty",
            "Melkior Ornik",
            "Katherine Driggs-Campbell"
        ],
        "published": "2024-03-25T08:11:02Z",
        "summary": "Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to\nagricultural field robots, and from health care assistants to the entertainment\nindustry. The majority of these systems are developed with modular\nsub-components for decision-making, planning, and control that may be\nhand-engineered or learning-based. While these existing approaches have been\nshown to perform well under the situations they were specifically designed for,\nthey can perform especially poorly in rare, out-of-distribution scenarios that\nwill undoubtedly arise at test-time. The rise of foundation models trained on\nmultiple tasks with impressively large datasets from a variety of fields has\nled researchers to believe that these models may provide common sense reasoning\nthat existing planners are missing. Researchers posit that this common sense\nreasoning will bridge the gap between algorithm development and deployment to\nout-of-distribution tasks, like how humans adapt to unexpected scenarios. Large\nlanguage models have already penetrated the robotics and autonomous systems\ndomains as researchers are scrambling to showcase their potential use cases in\ndeployment. While this application direction is very promising empirically,\nfoundation models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back\nand simultaneously design systems that can quantify the certainty of a model's\ndecision, and detect when it may be hallucinating. In this work, we discuss the\ncurrent use cases of foundation models for decision-making tasks, provide a\ngeneral definition for hallucinations with examples, discuss existing\napproaches to hallucination detection and mitigation with a focus on decision\nproblems, and explore areas for further research in this exciting field.",
        "pdf_link": "https://arxiv.org/pdf/2403.16527v1.pdf"
    },
    {
        "title": "Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model",
        "authors": [
            "K Huang",
            "G Song",
            "Hanwen Su",
            "Jiyan Wang"
        ],
        "published": "2024-03-20T06:04:05Z",
        "summary": "Out-of-distribution (OOD) detection is a critical task to ensure the\nreliability and security of machine learning models deployed in real-world\napplications. Conventional methods for OOD detection that rely on single-modal\ninformation, often struggle to capture the rich variety of OOD instances. The\nprimary difficulty in OOD detection arises when an input image has numerous\nsimilarities to a particular class in the in-distribution (ID) dataset, e.g.,\nwolf to dog, causing the model to misclassify it. Nevertheless, it may be easy\nto distinguish these classes in the semantic domain. To this end, in this\npaper, a novel method called ODPC is proposed, in which specific prompts to\ngenerate OOD peer classes of ID semantics are designed by a large language\nmodel as an auxiliary modality to facilitate detection. Moreover, a contrastive\nloss based on OOD peer classes is devised to learn compact representations of\nID classes and improve the clarity of boundaries between different classes. The\nextensive experiments on five benchmark datasets show that the method we\npropose can yield state-of-the-art results.",
        "pdf_link": "https://arxiv.org/pdf/2403.13324v1.pdf"
    },
    {
        "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
        "authors": [
            "Andreas Opedal",
            "Alessandro Stolfo",
            "Haruki Shirakami",
            "Ying Jiao",
            "Ryan Cotterell",
            "Bernhard Sch\u00f6lkopf",
            "Abulhair Saparov",
            "Mrinmaya Sachan"
        ],
        "published": "2024-01-31T18:48:20Z",
        "summary": "There is increasing interest in employing large language models (LLMs) as\ncognitive models. For such purposes, it is central to understand which\ncognitive properties are well-modeled by LLMs, and which are not. In this work,\nwe study the biases of LLMs in relation to those known in children when solving\narithmetic word problems. Surveying the learning science literature, we posit\nthat the problem-solving process can be split into three distinct steps: text\ncomprehension, solution planning and solution execution. We construct tests for\neach one in order to understand which parts of this process can be faithfully\nmodeled by current state-of-the-art LLMs. We generate a novel set of word\nproblems for each of these tests, using a neuro-symbolic method that enables\nfine-grained control over the problem features. We find evidence that LLMs,\nwith and without instruction-tuning, exhibit human-like biases in both the\ntext-comprehension and the solution-planning steps of the solving process, but\nnot during the final step which relies on the problem's arithmetic expressions\n(solution execution).",
        "pdf_link": "https://arxiv.org/pdf/2401.18070v1.pdf"
    },
    {
        "title": "To Generate or to Retrieve? On the Effectiveness of Artificial Contexts for Medical Open-Domain Question Answering",
        "authors": [
            "Giacomo Frisoni",
            "Alessio Cocchieri",
            "Alex Presepi",
            "Gianluca Moro",
            "Zaiqiao Meng"
        ],
        "published": "2024-03-04T10:41:52Z",
        "summary": "Medical open-domain question answering demands substantial access to\nspecialized knowledge. Recent efforts have sought to decouple knowledge from\nmodel parameters, counteracting architectural scaling and allowing for training\non common low-resource hardware. The retrieve-then-read paradigm has become\nubiquitous, with model predictions grounded on relevant knowledge pieces from\nexternal repositories such as PubMed, textbooks, and UMLS. An alternative path,\nstill under-explored but made possible by the advent of domain-specific large\nlanguage models, entails constructing artificial contexts through prompting. As\na result, \"to generate or to retrieve\" is the modern equivalent of Hamlet's\ndilemma. This paper presents MedGENIE, the first generate-then-read framework\nfor multiple-choice question answering in medicine. We conduct extensive\nexperiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical\nperspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new\nstate-of-the-art (SOTA) in the open-book setting of each testbed, even allowing\na small-scale reader to outcompete zero-shot closed-book 175B baselines while\nusing up to 706$\\times$ fewer parameters. Overall, our findings reveal that\ngenerated passages are more effective than retrieved counterparts in attaining\nhigher accuracy.",
        "pdf_link": "https://arxiv.org/pdf/2403.01924v1.pdf"
    },
    {
        "title": "PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning",
        "authors": [
            "Jiawen Liu",
            "Yuanyuan Yao",
            "Pengcheng An",
            "Qi Wang"
        ],
        "published": "2024-03-21T08:37:15Z",
        "summary": "In children's collaborative learning, effective peer conversations can\nsignificantly enhance the quality of children's collaborative interactions. The\nintegration of Large Language Model (LLM) agents into this setting explores\ntheir novel role as peers, assessing impacts as team moderators and\nparticipants. We invited two groups of participants to engage in a\ncollaborative learning workshop, where they discussed and proposed conceptual\nsolutions to a design problem. The peer conversation transcripts were analyzed\nusing thematic analysis. We discovered that peer agents, while managing\ndiscussions effectively as team moderators, sometimes have their instructions\ndisregarded. As participants, they foster children's creative thinking but may\nnot consistently provide timely feedback. These findings highlight potential\ndesign improvements and considerations for peer agents in both roles.",
        "pdf_link": "https://arxiv.org/pdf/2403.14227v1.pdf"
    },
    {
        "title": "Humane Speech Synthesis through Zero-Shot Emotion and Disfluency Generation",
        "authors": [
            "Rohan Chaudhury",
            "Mihir Godbole",
            "Aakash Garg",
            "Jinsil Hwaryoung Seo"
        ],
        "published": "2024-03-31T00:38:02Z",
        "summary": "Contemporary conversational systems often present a significant limitation:\ntheir responses lack the emotional depth and disfluent characteristic of human\ninteractions. This absence becomes particularly noticeable when users seek more\npersonalized and empathetic interactions. Consequently, this makes them seem\nmechanical and less relatable to human users. Recognizing this gap, we embarked\non a journey to humanize machine communication, to ensure AI systems not only\ncomprehend but also resonate. To address this shortcoming, we have designed an\ninnovative speech synthesis pipeline. Within this framework, a cutting-edge\nlanguage model introduces both human-like emotion and disfluencies in a\nzero-shot setting. These intricacies are seamlessly integrated into the\ngenerated text by the language model during text generation, allowing the\nsystem to mirror human speech patterns better, promoting more intuitive and\nnatural user interactions. These generated elements are then adeptly\ntransformed into corresponding speech patterns and emotive sounds using a\nrule-based approach during the text-to-speech phase. Based on our experiments,\nour novel system produces synthesized speech that's almost indistinguishable\nfrom genuine human communication, making each interaction feel more personal\nand authentic.",
        "pdf_link": "https://arxiv.org/pdf/2404.01339v1.pdf"
    },
    {
        "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
        "authors": [
            "Leigang Qu",
            "Wenjie Wang",
            "Yongqi Li",
            "Hanwang Zhang",
            "Liqiang Nie",
            "Tat-Seng Chua"
        ],
        "published": "2024-03-07T08:37:33Z",
        "summary": "Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.",
        "pdf_link": "https://arxiv.org/pdf/2403.04321v2.pdf"
    },
    {
        "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
        "authors": [
            "Mikayel Samvelyan",
            "Sharath Chandra Raparthy",
            "Andrei Lupu",
            "Eric Hambro",
            "Aram H. Markosyan",
            "Manish Bhatt",
            "Yuning Mao",
            "Minqi Jiang",
            "Jack Parker-Holder",
            "Jakob Foerster",
            "Tim Rockt\u00e4schel",
            "Roberta Raileanu"
        ],
        "published": "2024-02-26T18:47:27Z",
        "summary": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to user\ninputs is of paramount importance. Existing methods for identifying adversarial\nprompts tend to focus on specific domains, lack diversity, or require extensive\nhuman annotations. To address these limitations, we present Rainbow Teaming, a\nnovel approach for producing a diverse collection of adversarial prompts.\nRainbow Teaming casts adversarial prompt generation as a quality-diversity\nproblem, and uses open-ended search to generate prompts that are both effective\nand diverse. It can uncover a model's vulnerabilities across a broad range of\ndomains including, in this paper, safety, question answering, and\ncybersecurity. We also demonstrate that fine-tuning on synthetic data generated\nby Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting\ntheir general capabilities and helpfulness, paving the path to open-ended\nself-improvement.",
        "pdf_link": "https://arxiv.org/pdf/2402.16822v1.pdf"
    },
    {
        "title": "Eliciting Personality Traits in Large Language Models",
        "authors": [
            "Airlie Hilliard",
            "Cristian Munoz",
            "Zekun Wu",
            "Adriano Soares Koshiyama"
        ],
        "published": "2024-02-13T10:09:00Z",
        "summary": "Large Language Models (LLMs) are increasingly being utilized by both\ncandidates and employers in the recruitment context. However, with this comes\nnumerous ethical concerns, particularly related to the lack of transparency in\nthese \"black-box\" models. Although previous studies have sought to increase the\ntransparency of these models by investigating the personality traits of LLMs,\nmany of the previous studies have provided them with personality assessments to\ncomplete. On the other hand, this study seeks to obtain a better understanding\nof such models by examining their output variations based on different input\nprompts. Specifically, we use a novel elicitation approach using prompts\nderived from common interview questions, as well as prompts designed to elicit\nparticular Big Five personality traits to examine whether the models were\nsusceptible to trait-activation like humans are, to measure their personality\nbased on the language used in their outputs. To do so, we repeatedly prompted\nmultiple LMs with different parameter sizes, including Llama-2, Falcon,\nMistral, Bloom, GPT, OPT, and XLNet (base and fine tuned versions) and examined\ntheir personality using classifiers trained on the myPersonality dataset. Our\nresults reveal that, generally, all LLMs demonstrate high openness and low\nextraversion. However, whereas LMs with fewer parameters exhibit similar\nbehaviour in personality traits, newer and LMs with more parameters exhibit a\nbroader range of personality traits, with increased agreeableness, emotional\nstability, and openness. Furthermore, a greater number of parameters is\npositively associated with openness and conscientiousness. Moreover, fine-tuned\nmodels exhibit minor modulations in their personality traits, contingent on the\ndataset. Implications and directions for future research are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2402.08341v2.pdf"
    },
    {
        "title": "Segment Anything Model Can Not Segment Anything: Assessing AI Foundation Model's Generalizability in Permafrost Mapping",
        "authors": [
            "Wenwen Li",
            "Chia-Yu Hsu",
            "Sizhe Wang",
            "Yezhou Yang",
            "Hyunho Lee",
            "Anna Liljedahl",
            "Chandi Witharana",
            "Yili Yang",
            "Brendan M. Rogers",
            "Samantha T. Arundel",
            "Matthew B. Jones",
            "Kenton McHenry",
            "Patricia Solis"
        ],
        "published": "2024-01-16T19:10:09Z",
        "summary": "This paper assesses trending AI foundation models, especially emerging\ncomputer vision foundation models and their performance in natural landscape\nfeature segmentation. While the term foundation model has quickly garnered\ninterest from the geospatial domain, its definition remains vague. Hence, this\npaper will first introduce AI foundation models and their defining\ncharacteristics. Built upon the tremendous success achieved by Large Language\nModels (LLMs) as the foundation models for language tasks, this paper discusses\nthe challenges of building foundation models for geospatial artificial\nintelligence (GeoAI) vision tasks. To evaluate the performance of large AI\nvision models, especially Meta's Segment Anything Model (SAM), we implemented\ndifferent instance segmentation pipelines that minimize the changes to SAM to\nleverage its power as a foundation model. A series of prompt strategies was\ndeveloped to test SAM's performance regarding its theoretical upper bound of\npredictive accuracy, zero-shot performance, and domain adaptability through\nfine-tuning. The analysis used two permafrost feature datasets, ice-wedge\npolygons and retrogressive thaw slumps because (1) these landform features are\nmore challenging to segment than manmade features due to their complicated\nformation mechanisms, diverse forms, and vague boundaries; (2) their presence\nand changes are important indicators for Arctic warming and climate change. The\nresults show that although promising, SAM still has room for improvement to\nsupport AI-augmented terrain mapping. The spatial and domain generalizability\nof this finding is further validated using a more general dataset EuroCrop for\nagricultural field mapping. Finally, we discuss future research directions that\nstrengthen SAM's applicability in challenging geospatial domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.08787v1.pdf"
    },
    {
        "title": "Polaris: A Safety-focused LLM Constellation Architecture for Healthcare",
        "authors": [
            "Subhabrata Mukherjee",
            "Paul Gamble",
            "Markel Sanz Ausin",
            "Neel Kant",
            "Kriti Aggarwal",
            "Neha Manjunath",
            "Debajyoti Datta",
            "Zhengliang Liu",
            "Jiayuan Ding",
            "Sophia Busacca",
            "Cezanne Bianco",
            "Swapnil Sharma",
            "Rae Lasko",
            "Michelle Voisard",
            "Sanchay Harneja",
            "Darya Filippova",
            "Gerry Meixiong",
            "Kevin Cha",
            "Amir Youssefi",
            "Meyhaa Buvanesh",
            "Howard Weingram",
            "Sebastian Bierman-Lytle",
            "Harpreet Singh Mangat",
            "Kim Parikh",
            "Saad Godil",
            "Alex Miller"
        ],
        "published": "2024-03-20T05:34:03Z",
        "summary": "We develop Polaris, the first safety-focused LLM constellation for real-time\npatient-AI healthcare conversations. Unlike prior LLM works in healthcare\nfocusing on tasks like question answering, our work specifically focuses on\nlong multi-turn voice conversations. Our one-trillion parameter constellation\nsystem is composed of several multibillion parameter LLMs as co-operative\nagents: a stateful primary agent that focuses on driving an engaging\nconversation and several specialist support agents focused on healthcare tasks\nperformed by nurses to increase safety and reduce hallucinations. We develop a\nsophisticated training protocol for iterative co-training of the agents that\noptimize for diverse objectives. We train our models on proprietary data,\nclinical care plans, healthcare regulatory documents, medical manuals, and\nother medical reasoning documents. We align our models to speak like medical\nprofessionals, using organic healthcare conversations and simulated ones\nbetween patient actors and experienced nurses. This allows our system to\nexpress unique capabilities such as rapport building, trust building, empathy\nand bedside manner. Finally, we present the first comprehensive clinician\nevaluation of an LLM system for healthcare. We recruited over 1100 U.S.\nlicensed nurses and over 130 U.S. licensed physicians to perform end-to-end\nconversational evaluations of our system by posing as patients and rating the\nsystem on several measures. We demonstrate Polaris performs on par with human\nnurses on aggregate across dimensions such as medical safety, clinical\nreadiness, conversational quality, and bedside manner. Additionally, we conduct\na challenging task-based evaluation of the individual specialist support\nagents, where we demonstrate our LLM agents significantly outperform a much\nlarger general-purpose LLM (GPT-4) as well as from its own medium-size class\n(LLaMA-2 70B).",
        "pdf_link": "https://arxiv.org/pdf/2403.13313v1.pdf"
    },
    {
        "title": "From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models",
        "authors": [
            "Jung-Mei Chu",
            "Hao-Cheng Lo",
            "Jieh Hsiang",
            "Chun-Chieh Cho"
        ],
        "published": "2024-02-01T08:37:13Z",
        "summary": "In patent prosecution, timely and effective responses to Office Actions (OAs)\nare crucial for securing patents. However, past automation and artificial\nintelligence research have largely overlooked this aspect. To bridge this gap,\nour study introduces the Patent Office Action Response Intelligence System\n(PARIS) and its advanced version, the Large Language Model (LLM) Enhanced PARIS\n(LE-PARIS). These systems are designed to enhance the efficiency of patent\nattorneys in handling OA responses through collaboration with AI. The systems'\nkey features include the construction of an OA Topics Database, development of\nResponse Templates, and implementation of Recommender Systems and LLM-based\nResponse Generation. To validate the effectiveness of the systems, we have\nemployed a multi-paradigm analysis using the USPTO Office Action database and\nlongitudinal data based on attorney interactions with our systems over six\nyears. Through five studies, we have examined the constructiveness of OA topics\n(studies 1 and 2) using topic modeling and our proposed Delphi process, the\nefficacy of our proposed hybrid LLM-based recommender system tailored for OA\nresponses (study 3), the quality of generated responses (study 4), and the\nsystems' practical value in real-world scenarios through user studies (study\n5). The results indicate that both PARIS and LE-PARIS significantly achieve key\nmetrics and have a positive impact on attorney performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.00421v2.pdf"
    },
    {
        "title": "Multi-Scale Protein Language Model for Unified Molecular Modeling",
        "authors": [
            "Kangjie Zheng",
            "Siyu Long",
            "Tianyu Lu",
            "Junwei Yang",
            "Xinyu Dai",
            "Ming Zhang",
            "Zaiqing Nie",
            "Wei-Ying Ma",
            "Hao Zhou"
        ],
        "published": "2024-03-05T13:35:41Z",
        "summary": "Protein language models have demonstrated significant potential in the field\nof protein engineering. However, current protein language models primarily\noperate at the residue scale, which limits their ability to provide information\nat the atom level. This limitation prevents us from fully exploiting the\ncapabilities of protein language models for applications involving both\nproteins and small molecules. In this paper, we propose ms-ESM (multi-scale\nESM), a novel approach that enables multi-scale unified molecular modeling.\nms-ESM achieves this by pre-training on multi-scale code-switch protein\nsequences and utilizing a multi-scale position encoding to capture\nrelationships among residues and atoms. Experimental results indicate that\nms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the\nfull utilization of protein language models. Further investigations reveal that\nthrough unified molecular modeling, ms-ESM not only gains molecular knowledge\nbut also retains its understanding of proteins.",
        "pdf_link": "https://arxiv.org/pdf/2403.12995v1.pdf"
    },
    {
        "title": "LLMs in Political Science: Heralding a New Era of Visual Analysis",
        "authors": [
            "Yu Wang"
        ],
        "published": "2024-02-29T22:11:20Z",
        "summary": "Interest is increasing among political scientists in leveraging the extensive\ninformation available in images. However, the challenge of interpreting these\nimages lies in the need for specialized knowledge in computer vision and access\nto specialized hardware. As a result, image analysis has been limited to a\nrelatively small group within the political science community. This landscape\ncould potentially change thanks to the rise of large language models (LLMs).\nThis paper aims to raise awareness of the feasibility of using Gemini for image\ncontent analysis. A retrospective analysis was conducted on a corpus of 688\nimages. Content reports were elicited from Gemini for each image and then\nmanually evaluated by the authors. We find that Gemini is highly accurate in\nperforming object detection, which is arguably the most common and fundamental\ntask in image analysis for political scientists. Equally important, we show\nthat it is easy to implement as the entire command consists of a single prompt\nin natural language; it is fast to run and should meet the time budget of most\nresearchers; and it is free to use and does not require any specialized\nhardware. In addition, we illustrate how political scientists can leverage\nGemini for other image understanding tasks, including face identification,\nsentiment analysis, and caption generation. Our findings suggest that Gemini\nand other similar LLMs have the potential to drastically stimulate and\naccelerate image research in political science and social sciences more\nbroadly.",
        "pdf_link": "https://arxiv.org/pdf/2403.00154v2.pdf"
    },
    {
        "title": "Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",
        "authors": [
            "Shadeeb Hossain",
            "Aayush Gohil",
            "Yizhou Wang"
        ],
        "published": "2024-01-18T20:14:10Z",
        "summary": "This paper discusses the feasibility of using Large Language Models LLM for\ncode generation with a particular application in designing an RISC. The paper\nalso reviews the associated steps such as parsing, tokenization, encoding,\nattention mechanism, sampling the tokens and iterations during code generation.\nThe generated code for the RISC components is verified through testbenches and\nhardware implementation on a FPGA board. Four metric parameters Correct output\non the first iteration, Number of errors embedded in the code, Number of trials\nrequired to achieve the code and Failure to generate the code after three\niterations, are used to compare the efficiency of using LLM in programming. In\nall the cases, the generated code had significant errors and human intervention\nwas always required to fix the bugs. LLM can therefore be used to complement a\nprogrammer code design.",
        "pdf_link": "https://arxiv.org/pdf/2401.10364v1.pdf"
    },
    {
        "title": "GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability",
        "authors": [
            "Zihan Luo",
            "Xiran Song",
            "Hong Huang",
            "Jianxun Lian",
            "Chenhao Zhang",
            "Jinqi Jiang",
            "Xing Xie"
        ],
        "published": "2024-03-07T13:36:08Z",
        "summary": "Evaluating and enhancing the general capabilities of large language models\n(LLMs) has been an important research topic. Graph is a common data structure\nin the real world, and understanding graph data is a crucial part for advancing\ngeneral intelligence. To evaluate and enhance the graph understanding abilities\nof LLMs, in this paper, we propose a benchmark named GraphInstruct, which\ncomprehensively includes 21 classical graph reasoning tasks, providing diverse\ngraph generation pipelines and detailed reasoning steps. Based on\nGraphInstruct, we further construct GraphLM through efficient\ninstruction-tuning, which shows prominent graph understanding capability. In\norder to enhance the LLM with graph reasoning capability as well, we propose a\nstep mask training strategy, and construct a model named GraphLM+. As one of\nthe pioneering efforts to enhance the graph understanding and reasoning\nabilities of LLMs, extensive experiments have demonstrated the superiority of\nGraphLM and GraphLM+ over other LLMs. We look forward to more researchers\nexploring the potential of LLMs in the graph data mining domain through\nGraphInstruct. Our code for generating GraphInstruct is released publicly at:\nhttps://github.com/CGCL-codes/GraphInstruct.",
        "pdf_link": "https://arxiv.org/pdf/2403.04483v2.pdf"
    },
    {
        "title": "Language Guided Exploration for RL Agents in Text Environments",
        "authors": [
            "Hitesh Golchha",
            "Sahil Yerawar",
            "Dhruvesh Patel",
            "Soham Dan",
            "Keerthiram Murugesan"
        ],
        "published": "2024-03-05T17:26:41Z",
        "summary": "Real-world sequential decision making is characterized by sparse rewards and\nlarge decision spaces, posing significant difficulty for experiential learning\nsystems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large\nLanguage Models (LLMs), with a wealth of world knowledge, can help RL agents\nlearn quickly and adapt to distribution shifts. In this work, we introduce\nLanguage Guided Exploration (LGE) framework, which uses a pre-trained language\nmodel (called GUIDE ) to provide decision-level guidance to an RL agent (called\nEXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging\ntext environment, LGE outperforms vanilla RL agents significantly and also\noutperforms other sophisticated methods like Behaviour Cloning and Text\nDecision Transformer.",
        "pdf_link": "https://arxiv.org/pdf/2403.03141v1.pdf"
    },
    {
        "title": "ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment",
        "authors": [
            "Xiwei Hu",
            "Rui Wang",
            "Yixiao Fang",
            "Bin Fu",
            "Pei Cheng",
            "Gang Yu"
        ],
        "published": "2024-03-08T08:08:10Z",
        "summary": "Diffusion models have demonstrated remarkable performance in the domain of\ntext-to-image generation. However, most widely used models still employ CLIP as\ntheir text encoder, which constrains their ability to comprehend dense prompts,\nencompassing multiple objects, detailed attributes, complex relationships,\nlong-text alignment, etc. In this paper, we introduce an Efficient Large\nLanguage Model Adapter, termed ELLA, which equips text-to-image diffusion\nmodels with powerful Large Language Models (LLM) to enhance text alignment\nwithout training of either U-Net or LLM. To seamlessly bridge two pre-trained\nmodels, we investigate a range of semantic alignment connector designs and\npropose a novel module, the Timestep-Aware Semantic Connector (TSC), which\ndynamically extracts timestep-dependent conditions from LLM. Our approach\nadapts semantic features at different stages of the denoising process,\nassisting diffusion models in interpreting lengthy and intricate prompts over\nsampling timesteps. Additionally, ELLA can be readily incorporated with\ncommunity models and tools to improve their prompt-following capabilities. To\nassess text-to-image models in dense prompt following, we introduce Dense\nPrompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K\ndense prompts. Extensive experiments demonstrate the superiority of ELLA in\ndense prompt following compared to state-of-the-art methods, particularly in\nmultiple object compositions involving diverse attributes and relationships.",
        "pdf_link": "https://arxiv.org/pdf/2403.05135v1.pdf"
    },
    {
        "title": "Cross-target Stance Detection by Exploiting Target Analytical Perspectives",
        "authors": [
            "Daijun Ding",
            "Rong Chen",
            "Liwen Jing",
            "Bowen Zhang",
            "Xu Huang",
            "Li Dong",
            "Xiaowen Zhao",
            "Ge Song"
        ],
        "published": "2024-01-03T14:28:55Z",
        "summary": "Cross-target stance detection (CTSD) is an important task, which infers the\nattitude of the destination target by utilizing annotated data derived from the\nsource target. One important approach in CTSD is to extract domain-invariant\nfeatures to bridge the knowledge gap between multiple targets. However, the\nanalysis of informal and short text structure, and implicit expressions,\ncomplicate the extraction of domain-invariant knowledge. In this paper, we\npropose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the\nanalysis perspective as a bridge to transfer knowledge. First, we develop a\ntwo-stage instruct-based chain-of-thought method (TsCoT) to elicit target\nanalysis perspectives and provide natural language explanations (NLEs) from\nmultiple viewpoints by formulating instructions based on large language model\n(LLM). Second, we propose a multi-perspective prompt-tuning framework\n(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments\nresults demonstrate the superiority of MPPT against the state-of-the-art\nbaseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2401.01761v2.pdf"
    },
    {
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "authors": [
            "Dennis Ulmer",
            "Elman Mansimov",
            "Kaixiang Lin",
            "Justin Sun",
            "Xibin Gao",
            "Yi Zhang"
        ],
        "published": "2024-01-10T09:49:10Z",
        "summary": "Large language models (LLMs) are powerful dialogue agents, but specializing\nthem towards fulfilling a specific function can be challenging. Instructing\ntuning, i.e. tuning models on instruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an effective method to do so, yet\nrequires a number of data samples that a) might not be available or b) costly\nto generate. Furthermore, this cost increases when the goal is to make the LLM\nfollow a specific workflow within a dialogue instead of single instructions.\nInspired by the self-play technique in reinforcement learning and the use of\nLLMs to simulate human agents, we propose a more effective method for data\ncollection through LLMs engaging in a conversation in various roles. This\napproach generates a training data via \"self-talk\" of LLMs that can be refined\nand utilized for supervised fine-tuning. We introduce an automated way to\nmeasure the (partial) success of a dialogue. This metric is used to filter the\ngenerated conversational data that is fed back in LLM for training. Based on\nour automated and human evaluations of conversation quality, we demonstrate\nthat such self-talk data improves results. In addition, we examine the various\ncharacteristics that showcase the quality of generated dialogues and how they\ncan be connected to their potential utility as training data.",
        "pdf_link": "https://arxiv.org/pdf/2401.05033v1.pdf"
    },
    {
        "title": "Kuaiji: the First Chinese Accounting Large Language Model",
        "authors": [
            "Jiayuan Luo",
            "Songhua Yang",
            "Xiaoling Qiu",
            "Panyu Chen",
            "Yufei Nai",
            "Wenxuan Zeng",
            "Wentao Zhang",
            "Xinke Jiang"
        ],
        "published": "2024-02-21T15:14:20Z",
        "summary": "Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated\nimpressive proficiency in comprehending and generating natural language.\nHowever, they encounter difficulties when tasked with adapting to specialized\ndomains such as accounting. To address this challenge, we introduce Kuaiji, a\ntailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned\nusing the Baichuan framework, which encompasses continuous pre-training and\nsupervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing\nlarge genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy\nand response speed. Our contributions encompass the creation of the first\nChinese accounting dataset, the establishment of Kuaiji as a leading\nopen-source Chinese accounting LLM, and the validation of its efficacy through\nreal-world accounting scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.13866v2.pdf"
    },
    {
        "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
        "authors": [
            "Cheng Kang",
            "Xinye Chen",
            "Yong Hu",
            "Daniel Novak"
        ],
        "published": "2024-02-15T17:02:48Z",
        "summary": "Improving the controllability, portability, and inference speed of diffusion\nlanguage models (DLMs) is a key challenge in natural language generation. While\nrecent research has shown significant success in complex text generation with\nlanguage models, the memory and computational power are still very demanding\nand fall short of expectations, which naturally results in low portability and\ninstability for the models. To mitigate these issues, numerous well-established\nmethods were proposed for neural network quantization. To further enhance their\nportability of independent deployment as well as improve their stability\nevaluated by language perplexity, we propose a novel approach called the\nQuantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM\nbuilds upon the recent successful controllable DLMs by remodeling the\ntask-specific embedding space via quantization. This leads to a gradient-based\ncontroller for the generation tasks, and more stable intermediate latent\nvariables are obtained, which naturally brings in an accelerated convergence as\nwell as better controllability. Additionally, the adaption fine-tuning method\nis employed to reduce tunable weights. Experimental results on five challenging\nfine-grained control tasks demonstrate that QE-CDLM compares favorably to\nexisting methods in terms of quality and feasibility, achieving better\nperplexity and lightweight fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.10107v1.pdf"
    },
    {
        "title": "Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem",
        "authors": [
            "Ceyao Zhang",
            "Renjie Li",
            "Cheng Zhang",
            "Zhaoyu Zhang",
            "Feng Yin"
        ],
        "published": "2024-03-08T08:38:50Z",
        "summary": "Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands\nexpert knowledge in physics, materials science, and quantum mechanics which is\nprohibitively labor-intensive. Advanced AI technologies, especially\nreinforcement learning (RL), have emerged as a powerful tool to augment and\naccelerate this inverse design process. By modeling the inverse design of PCSEL\nas a sequential decision-making problem, RL approaches can construct a\nsatisfactory PCSEL structure from scratch. However, the data inefficiency\nresulting from online interactions with precise and expensive simulation\nenvironments impedes the broader applicability of RL approaches. Recently,\nsequential models, especially the Transformer architecture, have exhibited\ncompelling performance in sequential decision-making problems due to their\nsimplicity and scalability to large language models. In this paper, we\nintroduce a novel framework named PCSEL Inverse Design Transformer (PiT) that\nabstracts the inverse design of PCSEL as a sequence modeling problem. The\ncentral part of our PiT is a Transformer-based structure that leverages the\npast trajectories and current states to predict the current actions. Compared\nwith the traditional RL approaches, PiT can output the optimal actions and\nachieve target PCSEL designs by leveraging offline data and conditioning on the\ndesired return. Results demonstrate that PiT achieves superior performance and\ndata efficiency compared to baselines.",
        "pdf_link": "https://arxiv.org/pdf/2403.05149v1.pdf"
    },
    {
        "title": "Using LLMs to Model the Beliefs and Preferences of Targeted Populations",
        "authors": [
            "Keiichi Namikoshi",
            "Alex Filipowicz",
            "David A. Shamma",
            "Rumen Iliev",
            "Candice L. Hogan",
            "Nikos Arechiga"
        ],
        "published": "2024-03-29T15:58:46Z",
        "summary": "We consider the problem of aligning a large language model (LLM) to model the\npreferences of a human population. Modeling the beliefs, preferences, and\nbehaviors of a specific population can be useful for a variety of different\napplications, such as conducting simulated focus groups for new products,\nconducting virtual surveys, and testing behavioral interventions, especially\nfor interventions that are expensive, impractical, or unethical. Existing work\nhas had mixed success using LLMs to accurately model human behavior in\ndifferent contexts. We benchmark and evaluate two well-known fine-tuning\napproaches and evaluate the resulting populations on their ability to match the\npreferences of real human respondents on a survey of preferences for battery\nelectric vehicles (BEVs). We evaluate our models against their ability to match\npopulation-wide statistics as well as their ability to match individual\nresponses, and we investigate the role of temperature in controlling the\ntrade-offs between these two. Additionally, we propose and evaluate a novel\nloss term to improve model performance on responses that require a numeric\nresponse.",
        "pdf_link": "https://arxiv.org/pdf/2403.20252v1.pdf"
    },
    {
        "title": "INSTRUCTIR: A Benchmark for Instruction Following of Information Retrieval Models",
        "authors": [
            "Hanseok Oh",
            "Hyunji Lee",
            "Seonghyeon Ye",
            "Haebin Shin",
            "Hansol Jang",
            "Changwook Jun",
            "Minjoon Seo"
        ],
        "published": "2024-02-22T06:59:50Z",
        "summary": "Despite the critical need to align search targets with users' intention,\nretrievers often only prioritize query information without delving into the\nusers' intended search context. Enhancing the capability of retrievers to\nunderstand intentions and preferences of users, akin to language model\ninstructions, has the potential to yield more aligned search targets. Prior\nstudies restrict the application of instructions in information retrieval to a\ntask description format, neglecting the broader context of diverse and evolving\nsearch scenarios. Furthermore, the prevailing benchmarks utilized for\nevaluation lack explicit tailoring to assess instruction-following ability,\nthereby hindering progress in this field. In response to these limitations, we\npropose a novel benchmark,INSTRUCTIR, specifically designed to evaluate\ninstruction-following ability in information retrieval tasks. Our approach\nfocuses on user-aligned instructions tailored to each query instance,\nreflecting the diverse characteristics inherent in real-world search scenarios.\nThrough experimental analysis, we observe that retrievers fine-tuned to follow\ntask-style instructions, such as INSTRUCTOR, can underperform compared to their\nnon-instruction-tuned counterparts. This underscores potential overfitting\nissues inherent in constructing retrievers trained on existing\ninstruction-aware retrieval datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.14334v1.pdf"
    },
    {
        "title": "Beyond Text: Improving LLM's Decision Making for Robot Navigation via Vocal Cues",
        "authors": [
            "Xingpeng Sun",
            "Haoming Meng",
            "Souradip Chakraborty",
            "Amrit Singh Bedi",
            "Aniket Bera"
        ],
        "published": "2024-02-05T20:11:56Z",
        "summary": "This work highlights a critical shortcoming in text-based Large Language\nModels (LLMs) used for human-robot interaction, demonstrating that text alone\nas a conversation modality falls short in such applications. While LLMs excel\nin processing text in these human conversations, they struggle with the nuances\nof verbal instructions in scenarios like social navigation, where ambiguity and\nuncertainty can erode trust in robotic and other AI systems. We can address\nthis shortcoming by moving beyond text and additionally focusing on the\nparalinguistic features of these audio responses. These features are the\naspects of spoken communication that do not involve the literal wording\n(lexical content) but convey meaning and nuance through how something is said.\nWe present \"Beyond Text\"; an approach that improves LLM decision-making by\nintegrating audio transcription along with a subsection of these features,\nwhich focus on the affect and more relevant in human-robot conversations. This\napproach not only achieves a 70.26% winning rate, outperforming existing LLMs\nby 48.30%, but also enhances robustness against token manipulation adversarial\nattacks, highlighted by a 22.44% less decrease ratio than the text-only\nlanguage model in winning rate. \"Beyond Text\" marks an advancement in social\nrobot navigation and broader Human-Robot interactions, seamlessly integrating\ntext-based guidance with human-audio-informed language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.03494v1.pdf"
    },
    {
        "title": "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios",
        "authors": [
            "Junjie Ye",
            "Guanyu Li",
            "Songyang Gao",
            "Caishuang Huang",
            "Yilong Wu",
            "Sixian Li",
            "Xiaoran Fan",
            "Shihan Dou",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-01-01T12:49:36Z",
        "summary": "Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.",
        "pdf_link": "https://arxiv.org/pdf/2401.00741v2.pdf"
    },
    {
        "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
        "authors": [
            "Jenish Maharjan",
            "Anurag Garikipati",
            "Navan Preet Singh",
            "Leo Cyrus",
            "Mayank Sharma",
            "Madalina Ciobanu",
            "Gina Barnes",
            "Rahul Thapa",
            "Qingqing Mao",
            "Ritankar Das"
        ],
        "published": "2024-02-29T17:19:39Z",
        "summary": "LLMs have become increasingly capable at accomplishing a range of\nspecialized-tasks and can be utilized to expand equitable access to medical\nknowledge. Most medical LLMs have involved extensive fine-tuning, leveraging\nspecialized medical data and significant, thus costly, amounts of computational\npower. Many of the top performing LLMs are proprietary and their access is\nlimited to very few research groups. However, open-source (OS) models represent\na key area of growth for medical LLMs due to significant improvements in\nperformance and an inherent ability to provide the transparency and compliance\nrequired in healthcare. We present OpenMedLM, a prompting platform which\ndelivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.\nWe evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks\n(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of\nprompting strategies, including zero-shot, few-shot, chain-of-thought (random\nselection and kNN selection), and ensemble/self-consistency voting. We found\nthat OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,\nsurpassing the previous best performing OS models that leveraged\ncomputationally costly extensive fine-tuning. The model delivers a 72.6%\naccuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and\nachieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the\nfirst OS LLM to surpass 80% accuracy on this benchmark. Our results highlight\nmedical-specific emergent properties in OS LLMs which have not yet been\ndocumented to date elsewhere, and showcase the benefits of further leveraging\nprompt engineering to improve the performance of accessible LLMs for medical\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2402.19371v1.pdf"
    },
    {
        "title": "Hierarchical Continual Reinforcement Learning via Large Language Model",
        "authors": [
            "Chaofan Pan",
            "Xin Yang",
            "Hao Wang",
            "Wei Wei",
            "Tianrui Li"
        ],
        "published": "2024-01-25T03:06:51Z",
        "summary": "The ability to learn continuously in dynamic environments is a crucial\nrequirement for reinforcement learning (RL) agents applying in the real world.\nDespite the progress in continual reinforcement learning (CRL), existing\nmethods often suffer from insufficient knowledge transfer, particularly when\nthe tasks are diverse. To address this challenge, we propose a new framework,\nHierarchical Continual reinforcement learning via large language model\n(Hi-Core), designed to facilitate the transfer of high-level knowledge. Hi-Core\norchestrates a twolayer structure: high-level policy formulation by a large\nlanguage model (LLM), which represents agenerates a sequence of goals, and\nlow-level policy learning that closely aligns with goal-oriented RL practices,\nproducing the agent's actions in response to the goals set forth. The framework\nemploys feedback to iteratively adjust and verify highlevel policies, storing\nthem along with low-level policies within a skill library. When encountering a\nnew task, Hi-Core retrieves relevant experience from this library to help to\nlearning. Through experiments on Minigrid, Hi-Core has demonstrated its\neffectiveness in handling diverse CRL tasks, which outperforms popular\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2401.15098v2.pdf"
    },
    {
        "title": "Linguistic Calibration of Language Models",
        "authors": [
            "Neil Band",
            "Xuechen Li",
            "Tengyu Ma",
            "Tatsunori Hashimoto"
        ],
        "published": "2024-03-30T20:47:55Z",
        "summary": "Language models (LMs) may lead their users to make suboptimal downstream\ndecisions when they confidently hallucinate. This issue can be mitigated by\nhaving the LM verbally convey the probability that its claims are correct, but\nexisting models cannot produce text with calibrated confidence statements.\nThrough the lens of decision-making, we formalize linguistic calibration for\nlong-form generations: an LM is linguistically calibrated if its generations\nenable its users to make calibrated probabilistic predictions. This definition\nenables a training framework where a supervised finetuning step bootstraps an\nLM to emit long-form generations with confidence statements such as \"I estimate\na 30% chance of...\" or \"I am certain that...\", followed by a reinforcement\nlearning step which rewards generations that enable a user to provide\ncalibrated answers to related questions. We linguistically calibrate Llama 2 7B\nand find in automated and human evaluations of long-form generations that it is\nsignificantly more calibrated than strong finetuned factuality baselines with\ncomparable accuracy. These findings generalize under distribution shift on\nquestion-answering and under a significant task shift to person biography\ngeneration. Our results demonstrate that long-form generations may be\ncalibrated end-to-end by constructing an objective in the space of the\npredictions that users make in downstream decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2404.00474v1.pdf"
    },
    {
        "title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models",
        "authors": [
            "Mosh Levy",
            "Alon Jacoby",
            "Yoav Goldberg"
        ],
        "published": "2024-02-19T16:04:53Z",
        "summary": "This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that traditional\nperplexity metrics do not correlate with performance of LLMs' in long input\nreasoning tasks. We analyse our results and identify failure modes that can\nserve as useful guides for future research, potentially informing strategies to\naddress the limitations observed in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14848v1.pdf"
    },
    {
        "title": "Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint",
        "authors": [
            "Xiaowei Yuan",
            "Zhao Yang",
            "Yequan Wang",
            "Shengping Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2024-02-19T07:10:30Z",
        "summary": "Large language models internalize enormous parametric knowledge during\npre-training. Concurrently, realistic applications necessitate external\ncontextual knowledge to aid models on the underlying tasks. This raises a\ncrucial dilemma known as knowledge conflicts, where the contextual knowledge\nclashes with the However, existing decoding works are specialized in resolving\nknowledge conflicts and could inadvertently deteriorate performance in absence\nof conflicts. In this paper, we propose an adaptive decoding method, termed as\ncontextual information-entropy constraint decoding (COIECD), to discern whether\nthe knowledge conflicts occur and resolve them. It can improve the model's\nfaithfulness to conflicting context, and simultaneously maintain high\nperformance among non- Our experiments show that COIECD exhibits strong\nperformance and robustness over knowledge conflicts in realistic datasets. Code\nis available.",
        "pdf_link": "https://arxiv.org/pdf/2402.11893v1.pdf"
    },
    {
        "title": "Scaling Up LLM Reviews for Google Ads Content Moderation",
        "authors": [
            "Wei Qiao",
            "Tushar Dogra",
            "Otilia Stretcu",
            "Yu-Han Lyu",
            "Tiantian Fang",
            "Dongjin Kwon",
            "Chun-Ta Lu",
            "Enming Luo",
            "Yuan Wang",
            "Chih-Chun Chia",
            "Ariel Fuxman",
            "Fangzhou Wang",
            "Ranjay Krishna",
            "Mehmet Tek"
        ],
        "published": "2024-02-07T23:47:02Z",
        "summary": "Large language models (LLMs) are powerful tools for content moderation, but\ntheir inference costs and latency make them prohibitive for casual use on large\ndatasets, such as the Google Ads repository. This study proposes a method for\nscaling up LLM reviews for content moderation in Google Ads. First, we use\nheuristics to select candidates via filtering and duplicate removal, and create\nclusters of ads for which we select one representative ad per cluster. We then\nuse LLMs to review only the representative ads. Finally, we propagate the LLM\ndecisions for the representative ads back to their clusters. This method\nreduces the number of reviews by more than 3 orders of magnitude while\nachieving a 2x recall compared to a baseline non-LLM model. The success of this\napproach is a strong function of the representations used in clustering and\nlabel propagation; we found that cross-modal similarity representations yield\nbetter results than uni-modal representations.",
        "pdf_link": "https://arxiv.org/pdf/2402.14590v1.pdf"
    },
    {
        "title": "Rethinking Optimization and Architecture for Tiny Language Models",
        "authors": [
            "Yehui Tang",
            "Fangcheng Liu",
            "Yunsheng Ni",
            "Yuchuan Tian",
            "Zheyuan Bai",
            "Yi-Qi Hu",
            "Sichao Liu",
            "Shangling Jui",
            "Kai Han",
            "Yunhe Wang"
        ],
        "published": "2024-02-05T07:59:38Z",
        "summary": "The power of large language models (LLMs) has been demonstrated through\nnumerous data and computing resources. However, the application of language\nmodels on mobile devices is facing huge challenge on the computation and memory\ncosts, that is, tiny language models with high performance are urgently\nrequired. Limited by the highly complex training process, there are many\ndetails for optimizing language models that are seldom studied carefully. In\nthis study, based on a tiny language model with 1B parameters, we carefully\ndesign a series of empirical study to analyze the effect of each component.\nThree perspectives are mainly discussed, \\ie, neural architecture, parameter\ninitialization, and optimization strategy. Several design formulas are\nempirically proved especially effective for tiny language models, including\ntokenizer compression, architecture tweaking, parameter inheritance and\nmultiple-round training. Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B\nPro on 1.6T multilingual corpora, following the established formulas.\nExperimental results demonstrate the improved optimization and architecture\nyield a notable average improvement of 8.87 on benchmark evaluation sets for\nPanGu-$\\pi$-1B Pro. Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA\nmodels with larger model sizes, validating its superior performance. The code\nis available at https://github.com/YuchuanTian/RethinkTinyLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.02791v2.pdf"
    },
    {
        "title": "IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators",
        "authors": [
            "Luyang Lin",
            "Lingzhi Wang",
            "Xiaoyan Zhao",
            "Jing Li",
            "Kam-Fai Wong"
        ],
        "published": "2024-02-01T05:20:07Z",
        "summary": "This study focuses on media bias detection, crucial in today's era of\ninfluential social media platforms shaping individual attitudes and opinions.\nIn contrast to prior work that primarily relies on training specific models\ntailored to particular datasets, resulting in limited adaptability and subpar\nperformance on out-of-domain data, we introduce a general bias detection\nframework, IndiVec, built upon large language models. IndiVec begins by\nconstructing a fine-grained media bias database, leveraging the robust\ninstruction-following capabilities of large language models and vector database\ntechniques. When confronted with new input for bias detection, our framework\nautomatically selects the most relevant indicator from the vector database and\nemploys majority voting to determine the input's bias label. IndiVec excels\ncompared to previous methods due to its adaptability (demonstrating consistent\nperformance across diverse datasets from various sources) and explainability\n(providing explicit top-k indicators to interpret bias predictions).\nExperimental results on four political bias datasets highlight IndiVec's\nsignificant superiority over baselines. Furthermore, additional experiments and\nanalysis provide profound insights into the framework's effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.00345v1.pdf"
    },
    {
        "title": "Invariant Test-Time Adaptation for Vision-Language Model Generalization",
        "authors": [
            "Huan Ma",
            "Yan Zhu",
            "Changqing Zhang",
            "Peilin Zhao",
            "Baoyuan Wu",
            "Long-Kai Huang",
            "Qinghua Hu",
            "Bingzhe Wu"
        ],
        "published": "2024-03-01T09:01:53Z",
        "summary": "Vision-language foundation models have exhibited remarkable success across a\nmultitude of downstream tasks due to their scalability on extensive image-text\npaired datasets. However, these models display significant limitations when\napplied to long-tail tasks, such as fine-grained image classification, as a\nresult of \"decision shortcuts\" that hinders their generalization capabilities.\nIn this work, we find that the CLIP model possesses a rich set of features,\nencompassing both \\textit{desired invariant causal features} and\n\\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP\non downstream tasks originates from its inability to effectively utilize\npre-trained features in accordance with specific task requirements. To address\nthis challenge, this paper introduces a test-time prompt tuning paradigm that\noptimizes a learnable prompt, thereby compelling the model to exploit genuine\ncausal invariant features while disregarding decision shortcuts during the\ninference phase. The proposed method effectively alleviates excessive\ndependence on potentially misleading, task-irrelevant contextual information,\nwhile concurrently emphasizing critical, task-related visual cues. We conduct\ncomparative analysis of the proposed method against various approaches which\nvalidates its effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.00376v1.pdf"
    },
    {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "authors": [
            "Xiaoyu Liu",
            "Paiheng Xu",
            "Junda Wu",
            "Jiaxin Yuan",
            "Yifan Yang",
            "Yuhang Zhou",
            "Fuxiao Liu",
            "Tianrui Guan",
            "Haoliang Wang",
            "Tong Yu",
            "Julian McAuley",
            "Wei Ai",
            "Furong Huang"
        ],
        "published": "2024-03-14T17:47:20Z",
        "summary": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.09606v1.pdf"
    },
    {
        "title": "Are We on the Right Way for Evaluating Large Vision-Language Models?",
        "authors": [
            "Lin Chen",
            "Jinsong Li",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Yuhang Zang",
            "Zehui Chen",
            "Haodong Duan",
            "Jiaqi Wang",
            "Yu Qiao",
            "Dahua Lin",
            "Feng Zhao"
        ],
        "published": "2024-03-29T17:59:34Z",
        "summary": "Large vision-language models (LVLMs) have recently achieved rapid progress,\nsparking numerous studies to evaluate their multi-modal capabilities. However,\nwe dig into current evaluation works and identify two primary issues: 1) Visual\ncontent is unnecessary for many samples. The answers can be directly inferred\nfrom the questions and options, or the world knowledge embedded in LLMs. This\nphenomenon is prevalent across current benchmarks. For instance, GeminiPro\nachieves 42.9% on the MMMU benchmark without any visual input, and outperforms\nthe random choice baseline across six benchmarks over 24% on average. 2)\nUnintentional data leakage exists in LLM and LVLM training. LLM and LVLM could\nstill answer some visual-necessary questions without visual content, indicating\nthe memorizing of these samples within large-scale training data. For example,\nSphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM\nbackbone with 17.9%. Both problems lead to misjudgments of actual multi-modal\ngains and potentially misguide the study of LVLM. To this end, we present\nMMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500\nsamples meticulously selected by humans. MMStar benchmarks 6 core capabilities\nand 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with\ncarefully balanced and purified samples. These samples are first roughly\nselected from current benchmarks with an automated pipeline, human review is\nthen involved to ensure each curated sample exhibits visual dependency, minimal\ndata leakage, and requires advanced multi-modal capabilities. Moreover, two\nmetrics are developed to measure data leakage and actual performance gain in\nmulti-modal training. We evaluate 16 leading LVLMs on MMStar to assess their\nmulti-modal capabilities, and on 7 benchmarks with the proposed metrics to\ninvestigate their data leakage and actual multi-modal gain.",
        "pdf_link": "https://arxiv.org/pdf/2403.20330v2.pdf"
    },
    {
        "title": "LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation Benchmark for Chinese Large Language Models",
        "authors": [
            "Chuang Liu",
            "Renren Jin",
            "Yuqi Ren",
            "Deyi Xiong"
        ],
        "published": "2024-03-19T10:11:14Z",
        "summary": "Chinese Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities across various NLP benchmarks and real-world applications.\nHowever, the existing benchmarks for comprehensively evaluating these LLMs are\nstill insufficient, particularly in terms of measuring knowledge that LLMs\ncapture. Current datasets collect questions from Chinese examinations across\ndifferent subjects and educational levels to address this issue. Yet, these\nbenchmarks primarily focus on objective questions such as multiple-choice\nquestions, leading to a lack of diversity in question types. To tackle this\nproblem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge\nEvaluation benchmark in this paper. LHMKE is designed to provide a\ncomprehensive evaluation of the knowledge acquisition capabilities of Chinese\nLLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,\nranging from primary school to professional certification exams. Notably, LHMKE\nincludes both objective and subjective questions, offering a more holistic\nevaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs\nunder the zero-shot setting, which aligns with real examinations, and compared\ntheir performance across different subjects. We also conduct an in-depth\nanalysis to check whether GPT-4 can automatically score subjective predictions.\nOur findings suggest that LHMKE is a challenging and advanced testbed for\nChinese LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.12601v1.pdf"
    },
    {
        "title": "ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study",
        "authors": [
            "Hala Abdelkader",
            "Mohamed Abdelrazek",
            "Scott Barnett",
            "Jean-Guy Schneider",
            "Priya Rani",
            "Rajesh Vasa"
        ],
        "published": "2024-01-12T11:27:15Z",
        "summary": "Machine learning (ML), especially with the emergence of large language models\n(LLMs), has significantly transformed various industries. However, the\ntransition from ML model prototyping to production use within software systems\npresents several challenges. These challenges primarily revolve around ensuring\nsafety, security, and transparency, subsequently influencing the overall\nrobustness and trustworthiness of ML models. In this paper, we introduce\nML-On-Rails, a protocol designed to safeguard ML models, establish a\nwell-defined endpoint interface for different ML tasks, and clear communication\nbetween ML providers and ML consumers (software engineers). ML-On-Rails\nenhances the robustness of ML models via incorporating detection capabilities\nto identify unique challenges specific to production ML. We evaluated the\nML-On-Rails protocol through a real-world case study of the MoveReminder\napplication. Through this evaluation, we emphasize the importance of\nsafeguarding ML models in production.",
        "pdf_link": "https://arxiv.org/pdf/2401.06513v1.pdf"
    },
    {
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
        "authors": [
            "Zhuoshi Pan",
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Menglin Xia",
            "Xufang Luo",
            "Jue Zhang",
            "Qingwei Lin",
            "Victor R\u00fchle",
            "Yuqing Yang",
            "Chin-Yew Lin",
            "H. Vicky Zhao",
            "Lili Qiu",
            "Dongmei Zhang"
        ],
        "published": "2024-03-19T17:59:56Z",
        "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.",
        "pdf_link": "https://arxiv.org/pdf/2403.12968v1.pdf"
    },
    {
        "title": "ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization",
        "authors": [
            "Feifan Song",
            "Yuxuan Fan",
            "Xin Zhang",
            "Peiyi Wang",
            "Houfeng Wang"
        ],
        "published": "2024-02-14T17:14:34Z",
        "summary": "Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to\nensure the generation of safe content. Due to the heavy cost associated with\nfine-tuning, fine-tuning-free methods have emerged, typically modifying LLM\ndecoding with external auxiliary methods. However, these methods do not\nessentially enhance the LLM itself. In this paper, we rethink the derivation\nprocedures of DPO, based on which we conversely build an instant scorer using\nthe states of the LLM before and after In-context Learning (ICL). Accordingly,\nwe propose a novel approach called In-Context Direct Preference Optimization\n(ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with\nICL, generating well-aligned responses as estimated by the aforementioned\ninstant scorer, thereby enhancing the final performance. ICDPO can be further\nenhanced with a two-stage retriever and an upgraded scorer, both offering\nbenefits. Extensive experiments show its effectiveness, particularly in\noutperforming two fine-tuning-free baselines, and it exhibits competitiveness\nwith SFT + LoRA. We also conduct detailed analyses to offer comprehensive\ninsights into ICDPO.",
        "pdf_link": "https://arxiv.org/pdf/2402.09320v1.pdf"
    },
    {
        "title": "RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain",
        "authors": [
            "William James Bolton",
            "Rafael Poyiadzi",
            "Edward R. Morrell",
            "Gabriela van Bergen Gonzalez Bueno",
            "Lea Goetz"
        ],
        "published": "2024-03-21T17:30:59Z",
        "summary": "Large Language Models (LLMs) increasingly support applications in a wide\nrange of domains, some with potential high societal impact such as biomedicine,\nyet their reliability in realistic use cases is under-researched. In this work\nwe introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)\nframework and evaluate whether four state-of-the-art foundation LLMs can serve\nas reliable assistants in the biomedical domain. We identify prompt robustness,\nhigh recall, and a lack of hallucinations as necessary criteria for this use\ncase. We design shortform tasks and tasks requiring LLM freeform responses\nmimicking real-world user interactions. We evaluate LLM performance using\nsemantic similarity with a ground truth response, through an evaluator LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.14578v1.pdf"
    },
    {
        "title": "HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?",
        "authors": [
            "Shubhashis Roy Dipta",
            "Sadat Shahriar"
        ],
        "published": "2024-02-19T04:11:34Z",
        "summary": "This paper describes our system developed for SemEval-2024 Task 8,\n``Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated\nText Detection'' Machine-generated texts have been one of the main concerns due\nto the use of large language models (LLM) in fake text generation, phishing,\ncheating in exams, or even plagiarizing copyright materials. A lot of systems\nhave been developed to detect machine-generated text. Nonetheless, the majority\nof these systems rely on the text-generating model. This limitation is\nimpractical in real-world scenarios, as it's often impossible to know which\nspecific model the user has used for text generation. In this work, we propose\na $\\textbf{single}$ model based on contrastive learning, which uses\n$\\textbf{$\\approx$40% of the baseline's parameters}$ (149M vs. 355M) but shows\na comparable performance on the test dataset $(\\textbf{21st out of 137\nparticipants})$. Our key finding is that even without an ensemble of multiple\nmodels, a single base model can have comparable performance with the help of\ndata augmentation and contrastive learning. Our code is publicly available at\nhttps://github.com/dipta007/SemEval24-Task8.",
        "pdf_link": "https://arxiv.org/pdf/2402.11815v2.pdf"
    },
    {
        "title": "From English to ASIC: Hardware Implementation with Large Language Model",
        "authors": [
            "Emil Goh",
            "Maoyang Xiang",
            "I-Chyn Wey",
            "T. Hui Teo"
        ],
        "published": "2024-03-11T09:57:16Z",
        "summary": "In the realm of ASIC engineering, the landscape has been significantly\nreshaped by the rapid development of LLM, paralleled by an increase in the\ncomplexity of modern digital circuits. This complexity has escalated the\nrequirements for HDL coding, necessitating a higher degree of precision and\nsophistication. However, challenges have been faced due to the\nless-than-optimal performance of modern language models in generating hardware\ndescription code, a situation further exacerbated by the scarcity of the\ncorresponding high-quality code datasets. These challenges have highlighted the\ngap between the potential of LLMs to revolutionize digital circuit design and\ntheir current capabilities in accurately interpreting and implementing hardware\nspecifications. To address these challenges, a strategy focusing on the\nfine-tuning of the leading-edge nature language model and the reshuffling of\nthe HDL code dataset has been developed. The fine-tuning aims to enhance\nmodels' proficiency in generating precise and efficient ASIC design, while the\ndataset reshuffling is intended to broaden the scope and improve the quality of\ntraining material. The model demonstrated significant improvements compared to\nthe base model, with approximately 10% to 20% increase in accuracy across a\nwide range of temperature for the pass@1 metric. This approach is expected to\nfacilitate a simplified and more efficient LLM-assisted framework for complex\ncircuit design, leveraging their capabilities to meet the sophisticated demands\nof HDL coding and thus streamlining the ASIC development process.",
        "pdf_link": "https://arxiv.org/pdf/2403.07039v1.pdf"
    },
    {
        "title": "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods",
        "authors": [
            "Yu Sun",
            "Keyu Chen",
            "Shujie Wang",
            "Qipeng Guo",
            "Hang Yan",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Dahua Lin"
        ],
        "published": "2024-01-26T13:55:32Z",
        "summary": "Large language models (LLMs) garner significant attention for their\nunprecedented performance, leading to an increasing number of researches\nevaluating LLMs. However, these evaluation benchmarks are limited to assessing\nthe instruction-following capabilities, overlooking the fundamental abilities\nthat emerge during the pre-training stage. Previous subjective evaluation\nmethods mainly reply on scoring by API models. However, in the absence of\nreferences, large models have shown limited ability to discern subtle\ndifferences. To bridge the gap, we propose F-Eval, a bilingual evaluation\nbenchmark to evaluate the fundamental abilities, including expression,\ncommonsense and logic. The tasks in F-Eval include multi-choice objective\ntasks, open-ended objective tasks, reference-based subjective tasks and\nreference-free subjective tasks. For reference-free subjective tasks, we devise\nnew evaluation methods, serving as alternatives to scoring by API models. We\nconduct evaluations on 13 advanced LLMs. Results show that our evaluation\nmethods show higher correlation coefficients and larger distinction than other\nevaluators. Additionally, we discuss the influence of different model sizes,\ndimensions, and normalization methods. We anticipate that F-Eval will\nfacilitate the study of LLMs' fundamental abilities.",
        "pdf_link": "https://arxiv.org/pdf/2401.14869v1.pdf"
    },
    {
        "title": "JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models",
        "authors": [
            "Jillian Fisher",
            "Ximing Lu",
            "Jaehun Jung",
            "Liwei Jiang",
            "Zaid Harchaoui",
            "Yejin Choi"
        ],
        "published": "2024-02-13T19:54:29Z",
        "summary": "The permanence of online content combined with the enhanced authorship\nidentification techniques calls for stronger computational methods to protect\nthe identity and privacy of online authorship when needed, e.g., blind reviews\nfor scientific papers, anonymous online reviews, or anonymous interactions in\nthe mental health forums. In this paper, we propose an unsupervised\ninference-time approach to authorship obfuscation to address the unique\nchallenges of authorship obfuscation: lack of supervision data for diverse\nauthorship and domains, and the need for a sufficient level of revision beyond\nsimple paraphrasing to obfuscate the authorship, all the while preserving the\noriginal content and fluency.\n  We introduce JAMDEC, a user-controlled, inference-time algorithm for\nauthorship obfuscation that can be in principle applied to any text and\nauthorship. Our approach builds on small language models such as GPT2-XL in\norder to help avoid disclosing the original content to proprietary LLM's APIs,\nwhile also reducing the performance gap between small and large language models\nvia algorithmic enhancement. The key idea behind our approach is to boost the\ncreative power of smaller language models through constrained decoding, while\nalso allowing for user-specified controls and flexibility. Experimental results\ndemonstrate that our approach based on GPT2-XL outperforms previous\nstate-of-the-art methods based on comparably small models, while performing\ncompetitively against GPT3.5 175B, a propriety model that is two orders of\nmagnitudes larger.",
        "pdf_link": "https://arxiv.org/pdf/2402.08761v1.pdf"
    },
    {
        "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
        "authors": [
            "Byung-Kwan Lee",
            "Beomchan Park",
            "Chae Won Kim",
            "Yong Man Ro"
        ],
        "published": "2024-03-12T10:44:13Z",
        "summary": "The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.07508v1.pdf"
    },
    {
        "title": "Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",
        "authors": [
            "Yunshi Lan",
            "Xinyuan Li",
            "Hanyue Du",
            "Xuesong Lu",
            "Ming Gao",
            "Weining Qian",
            "Aoying Zhou"
        ],
        "published": "2024-01-15T07:48:42Z",
        "summary": "Natural Language Processing (NLP) aims to analyze text or speech via\ntechniques in the computer science field. It serves the applications in domains\nof healthcare, commerce, education and so on. Particularly, NLP has been widely\napplied to the education domain and its applications have enormous potential to\nhelp teaching and learning. In this survey, we review recent advances in NLP\nwith the focus on solving problems relevant to the education domain. In detail,\nwe begin with introducing the related background and the real-world scenarios\nin education where NLP techniques could contribute. Then, we present a taxonomy\nof NLP in the education domain and highlight typical NLP applications including\nquestion answering, question construction, automated assessment, and error\ncorrection. Next, we illustrate the task definition, challenges, and\ncorresponding cutting-edge techniques based on the above taxonomy. In\nparticular, LLM-involved methods are included for discussion due to the wide\nusage of LLMs in diverse NLP applications. After that, we showcase some\noff-the-shelf demonstrations in this domain. At last, we conclude with six\npromising directions for future research, including more datasets in education\ndomain, controllable usage of LLMs, intervention of difficulty-level control,\ninterpretable educational NLP, methods with adaptive learning, and integrated\nsystems for education. We organize all relevant datasets and papers in the\nopen-available Github Link for better\nreview~\\url{https://github.com/LiXinyuan1015/NLP-for-Education}.",
        "pdf_link": "https://arxiv.org/pdf/2401.07518v3.pdf"
    },
    {
        "title": "GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations",
        "authors": [
            "Jinhao Duan",
            "Renming Zhang",
            "James Diffenderfer",
            "Bhavya Kailkhura",
            "Lichao Sun",
            "Elias Stengel-Eskin",
            "Mohit Bansal",
            "Tianlong Chen",
            "Kaidi Xu"
        ],
        "published": "2024-02-19T18:23:36Z",
        "summary": "As Large Language Models (LLMs) are integrated into critical real-world\napplications, their strategic and logical reasoning abilities are increasingly\ncrucial. This paper evaluates LLMs' reasoning abilities in competitive\nenvironments through game-theoretic tasks, e.g., board and card games that\nrequire pure logic and strategic reasoning to compete with opponents. We first\npropose GTBench, a language-driven environment composing 10 widely-recognized\ntasks, across a comprehensive game taxonomy: complete versus incomplete\ninformation, dynamic versus static, and probabilistic versus deterministic\nscenarios. Then, we investigate two key problems: (1) Characterizing\ngame-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning\nevaluation. We observe that (1) LLMs have distinct behaviors regarding various\ngaming scenarios; for example, LLMs fail in complete and deterministic games\nyet they are competitive in probabilistic gaming scenarios; (2) Open-source\nLLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs,\ne.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits\nstrategic reasoning, while advanced reasoning methods such as Chain-of-Thought\n(CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are\nalso provided for a better understanding of LLMs' behavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.12348v1.pdf"
    },
    {
        "title": "Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph",
        "authors": [
            "Song Tong",
            "Kai Mao",
            "Zhen Huang",
            "Yukun Zhao",
            "Kaiping Peng"
        ],
        "published": "2024-02-22T10:12:16Z",
        "summary": "Leveraging the synergy between causal knowledge graphs and a large language\nmodel (LLM), our study introduces a groundbreaking approach for computational\nhypothesis generation in psychology. We analyzed 43,312 psychology articles\nusing a LLM to extract causal relation pairs. This analysis produced a\nspecialized causal graph for psychology. Applying link prediction algorithms,\nwe generated 130 potential psychological hypotheses focusing on `well-being',\nthen compared them against research ideas conceived by doctoral scholars and\nthose produced solely by the LLM. Interestingly, our combined approach of a LLM\nand causal graphs mirrored the expert-level insights in terms of novelty,\nclearly surpassing the LLM-only hypotheses (t(59) = 3.34, p=0.007 and t(59) =\n4.32, p<0.001, respectively). This alignment was further corroborated using\ndeep semantic analysis. Our results show that combining LLM with machine\nlearning techniques such as causal knowledge graphs can revolutionize automated\ndiscovery in psychology, extracting novel insights from the extensive\nliterature. This work stands at the crossroads of psychology and artificial\nintelligence, championing a new enriched paradigm for data-driven hypothesis\ngeneration in psychological research.",
        "pdf_link": "https://arxiv.org/pdf/2402.14424v2.pdf"
    },
    {
        "title": "Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm",
        "authors": [
            "Ali Rostami",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2024-02-12T08:32:29Z",
        "summary": "State-of-the-art rule-based and classification-based food recommendation\nsystems face significant challenges in becoming practical and useful. This\ndifficulty arises primarily because most machine learning models struggle with\nproblems characterized by an almost infinite number of classes and a limited\nnumber of samples within an unbalanced dataset. Conversely, the emergence of\nLarge Language Models (LLMs) as recommendation engines offers a promising\navenue. However, a general-purpose Recommendation as Language Processing (RLP)\napproach lacks the critical components necessary for effective food\nrecommendations. To address this gap, we introduce Food Recommendation as\nLanguage Processing (F-RLP), a novel framework that offers a food-specific,\ntailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize\ntheir potential, thereby paving the way for more accurate, personalized food\nrecommendations.",
        "pdf_link": "https://arxiv.org/pdf/2402.07477v2.pdf"
    },
    {
        "title": "Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models",
        "authors": [
            "Xin He",
            "Longhui Wei",
            "Lingxi Xie",
            "Qi Tian"
        ],
        "published": "2024-01-06T02:02:34Z",
        "summary": "Multimodal Large Language Models (MLLMs) are experiencing rapid growth,\nyielding a plethora of noteworthy contributions in recent months. The\nprevailing trend involves adopting data-driven methodologies, wherein diverse\ninstruction-following datasets are collected. However, a prevailing challenge\npersists in these approaches, specifically in relation to the limited visual\nperception ability, as CLIP-like encoders employed for extracting visual\ninformation from inputs. Though these encoders are pre-trained on billions of\nimage-text pairs, they still grapple with the information loss dilemma, given\nthat textual captions only partially capture the contents depicted in images.\nTo address this limitation, this paper proposes to improve the visual\nperception ability of MLLMs through a mixture-of-experts knowledge enhancement\nmechanism. Specifically, we introduce a novel method that incorporates\nmulti-task encoders and visual tools into the existing MLLMs training and\ninference pipeline, aiming to provide a more comprehensive and accurate\nsummarization of visual inputs. Extensive experiments have evaluated its\neffectiveness of advancing MLLMs, showcasing improved visual perception\nachieved through the integration of visual experts.",
        "pdf_link": "https://arxiv.org/pdf/2401.03105v2.pdf"
    },
    {
        "title": "ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation",
        "authors": [
            "Bhabesh Mali",
            "Karthik Maddala",
            "Sweeya Reddy",
            "Vatsal Gupta",
            "Chandan Karfa",
            "Ramesh Karri"
        ],
        "published": "2024-01-31T12:41:27Z",
        "summary": "System Verilog Assertion (SVA) formulation -- a critical yet complex task is\na prerequisite in the Formal Property Verification (FPV) process.\nTraditionally, SVA formulation involves expert-driven interpretation of\nspecifications, which is timeconsuming and prone to human error. However,\nLLM-informed automatic assertion generation is gaining interest. We designeda\nnovel framework called ChIRAAG, based on OpenAI GPT4, to generate SVA\nassertions from natural language specifications. ChIRAAG constitutes the\nsystematic breakdown of design specifications into a standardized format,\nfurther generating assertions from formatted specifications using LLM.\nFurthermore, we developed testbenches to verify/validate the LLM-generated\nassertions. Automatic feedback of log files from the simulation tool to the LLM\nensures that the framework can generate correc SVAs automatically. Only 33% of\nLLM-generated raw assertions had errors. Our results on OpenTitan designs shows\nthat LLMs can streamline and assist engineers in the assertion generation\nprocess, reshaping verification workflows.",
        "pdf_link": "https://arxiv.org/pdf/2402.00093v2.pdf"
    },
    {
        "title": "Improving Generalization in Semantic Parsing by Increasing Natural Language Variation",
        "authors": [
            "Irina Saparina",
            "Mirella Lapata"
        ],
        "published": "2024-02-13T18:48:23Z",
        "summary": "Text-to-SQL semantic parsing has made significant progress in recent years,\nwith various models demonstrating impressive performance on the challenging\nSpider benchmark. However, it has also been shown that these models often\nstruggle to generalize even when faced with small perturbations of previously\n(accurately) parsed expressions. This is mainly due to the linguistic form of\nquestions in Spider which are overly specific, unnatural, and display limited\nvariation. In this work, we use data augmentation to enhance the robustness of\ntext-to-SQL parsers against natural language variations. Existing approaches\ngenerate question reformulations either via models trained on Spider or only\nintroduce local changes. In contrast, we leverage the capabilities of large\nlanguage models to generate more realistic and diverse questions. Using only a\nfew prompts, we achieve a two-fold increase in the number of questions in\nSpider. Training on this augmented dataset yields substantial improvements on a\nrange of evaluation sets, including robustness benchmarks and out-of-domain\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2402.08666v1.pdf"
    },
    {
        "title": "Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER",
        "authors": [
            "Micheal Abaho",
            "Danushka Bollegala",
            "Gary Leeming",
            "Dan Joyce",
            "Iain E Buchan"
        ],
        "published": "2024-03-26T18:23:16Z",
        "summary": "Adapting language models (LMs) to novel domains is often achieved through\nfine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning\nintroduces new knowledge into an LM, enabling it to comprehend and efficiently\nperform a target domain task. Fine-tuning can however be inadvertently\ninsensitive if it ignores the wide array of disparities (e.g in word meaning)\nbetween source and target domains. For instance, words such as chronic and\npressure may be treated lightly in social conversations, however, clinically,\nthese words are usually an expression of concern. To address insensitive\nfine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach\nthat efficiently acquires target domain knowledge by appropriately weighting\nthe importance of domain-specific terms (DS-terms) during fine-tuning. MSLM\njointly masks DS-terms and generic words, then learns mask-specific losses by\nensuring LMs incur larger penalties for inaccurately predicting DS-terms\ncompared to generic words. Results of our analysis show that MSLM improves LMs\nsensitivity and detection of DS-terms. We empirically show that an optimal\nmasking rate not only depends on the LM, but also on the dataset and the length\nof sequences. Our proposed masking strategy outperforms advanced masking\nstrategies such as span- and PMI-based masking.",
        "pdf_link": "https://arxiv.org/pdf/2403.18025v2.pdf"
    },
    {
        "title": "SPARQL Generation: an analysis on fine-tuning OpenLLaMA for Question Answering over a Life Science Knowledge Graph",
        "authors": [
            "Julio C. Rangel",
            "Tarcisio Mendes de Farias",
            "Ana Claudia Sima",
            "Norio Kobayashi"
        ],
        "published": "2024-02-07T07:24:01Z",
        "summary": "The recent success of Large Language Models (LLM) in a wide range of Natural\nLanguage Processing applications opens the path towards novel Question\nAnswering Systems over Knowledge Graphs leveraging LLMs. However, one of the\nmain obstacles preventing their implementation is the scarcity of training data\nfor the task of translating questions into corresponding SPARQL queries,\nparticularly in the case of domain-specific KGs. To overcome this challenge, in\nthis study, we evaluate several strategies for fine-tuning the OpenLlama LLM\nfor question answering over life science knowledge graphs. In particular, we\npropose an end-to-end data augmentation approach for extending a set of\nexisting queries over a given knowledge graph towards a larger dataset of\nsemantically enriched question-to-SPARQL query pairs, enabling fine-tuning even\nfor datasets where these pairs are scarce. In this context, we also investigate\nthe role of semantic \"clues\" in the queries, such as meaningful variable names\nand inline comments. Finally, we evaluate our approach over the real-world Bgee\ngene expression knowledge graph and we show that semantic clues can improve\nmodel performance by up to 33% compared to a baseline with random variable\nnames and no comments included.",
        "pdf_link": "https://arxiv.org/pdf/2402.04627v1.pdf"
    },
    {
        "title": "Automatic and Universal Prompt Injection Attacks against Large Language Models",
        "authors": [
            "Xiaogeng Liu",
            "Zhiyuan Yu",
            "Yizhe Zhang",
            "Ning Zhang",
            "Chaowei Xiao"
        ],
        "published": "2024-03-07T23:46:20Z",
        "summary": "Large Language Models (LLMs) excel in processing and generating human\nlanguage, powered by their ability to interpret and follow instructions.\nHowever, their capabilities can be exploited through prompt injection attacks.\nThese attacks manipulate LLM-integrated applications into producing responses\naligned with the attacker's injected content, deviating from the user's actual\nrequests. The substantial risks posed by these attacks underscore the need for\na thorough understanding of the threats. Yet, research in this area faces\nchallenges due to the lack of a unified goal for such attacks and their\nreliance on manually crafted prompts, complicating comprehensive assessments of\nprompt injection robustness. We introduce a unified framework for understanding\nthe objectives of prompt injection attacks and present an automated\ngradient-based method for generating highly effective and universal prompt\ninjection data, even in the face of defensive measures. With only five training\nsamples (0.3% relative to the test data), our attack can achieve superior\nperformance compared with baselines. Our findings emphasize the importance of\ngradient-based testing, which can avoid overestimation of robustness,\nespecially for defense mechanisms.",
        "pdf_link": "https://arxiv.org/pdf/2403.04957v1.pdf"
    },
    {
        "title": "Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics",
        "authors": [
            "Sadaf Ghaffari",
            "Nikhil Krishnaswamy"
        ],
        "published": "2024-02-24T00:01:01Z",
        "summary": "In this paper, we present an exploration of LLMs' abilities to problem solve\nwith physical reasoning in situated environments. We construct a simple\nsimulated environment and demonstrate examples of where, in a zero-shot\nsetting, both text and multimodal LLMs display atomic world knowledge about\nvarious objects but fail to compose this knowledge in correct solutions for an\nobject manipulation and placement task. We also use BLIP, a vision-language\nmodel trained with more sophisticated cross-modal attention, to identify cases\nrelevant to object physical properties that that model fails to ground.\nFinally, we present a procedure for discovering the relevant properties of\nobjects in the environment and propose a method to distill this knowledge back\ninto the LLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.15654v1.pdf"
    },
    {
        "title": "DeepEdit: Knowledge Editing as Decoding with Constraints",
        "authors": [
            "Yiwei Wang",
            "Muhao Chen",
            "Nanyun Peng",
            "Kai-Wei Chang"
        ],
        "published": "2024-01-19T03:48:27Z",
        "summary": "We propose a new perspective of knowledge editing (KE) for large language\nmodels (LLMs) that treats it as a constrained decoding problem. We design\ndecoding constraints to regulate LLMs, ensuring coherence between reasoning\nsteps when incorporating new knowledge. To enforce these constraints, we\nutilize a depth-first search to adaptively substitute new knowledge for the\nLLMs' original reasoning steps, greedily seeking the optimal path of multi-hop\nreasoning with new knowledge. From this vantage, we propose DEEPEDIT:\nDepth-first Search-based Decoding for Knowledge Editing. DEEPEDIT improves the\nKE of LLMs by enhancing the conciseness, coherence, pertinence, and\nreceptiveness of reasoning with new knowledge. DEEPEDIT is flexibly applicable\nto any black-box LLM without requiring access to model parameters or token-wise\ndistributions. In addition to DEEPEDIT, we propose two new KE benchmarks:\nMQuAKE-2002 and MQuAKE-hard, which are designed to provide more precise and\nchallenging assessments of KE approaches. Qualitatively, DEEPEDIT enables LLMs\nto produce more succinct reasoning outputs in accordance with new knowledge.\nQuantitatively, it yields significant improvements on multiple KE benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2401.10471v2.pdf"
    },
    {
        "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
        "authors": [
            "Youyang Qu",
            "Ming Ding",
            "Nan Sun",
            "Kanchana Thilakarathna",
            "Tianqing Zhu",
            "Dusit Niyato"
        ],
        "published": "2024-03-23T09:26:15Z",
        "summary": "Large Language Models (LLMs) are foundational to AI advancements,\nfacilitating applications like predictive text generation. Nonetheless, they\npose risks by potentially memorizing and disseminating sensitive, biased, or\ncopyrighted information from their vast datasets. Machine unlearning emerges as\na cutting-edge solution to mitigate these concerns, offering techniques for\nLLMs to selectively discard certain data. This paper reviews the latest in\nmachine unlearning for LLMs, introducing methods for the targeted forgetting of\ninformation to address privacy, ethical, and legal challenges without\nnecessitating full model retraining. It divides existing research into\nunlearning from unstructured/textual data and structured/classification data,\nshowcasing the effectiveness of these approaches in removing specific data\nwhile maintaining model efficacy. Highlighting the practicality of machine\nunlearning, this analysis also points out the hurdles in preserving model\nintegrity, avoiding excessive or insufficient data removal, and ensuring\nconsistent outputs, underlining the role of machine unlearning in advancing\nresponsible, ethical AI.",
        "pdf_link": "https://arxiv.org/pdf/2403.15779v1.pdf"
    },
    {
        "title": "Reinforcement learning for question answering in programming domain using public community scoring as a human feedback",
        "authors": [
            "Alexey Gorbatovski",
            "Sergey Kovalchuk"
        ],
        "published": "2024-01-19T18:49:36Z",
        "summary": "In this study, we investigate the enhancement of the GPT Neo 125M performance\nin Community Question Answering (CQA) with a focus on programming, through the\nintegration of Reinforcement Learning from Human Feedback (RLHF) and the\nutilization of scores from Stack Overflow. Two distinct reward model training\nstrategies are employed for fine-tuning with Proximal Policy Optimization\n(PPO). Notably, the improvements in performance achieved through this method\nare comparable to those of GPT Neo 2.7B parameter variant. Additionally, an\nauxiliary scoring mechanism is introduced, which demonstrates the limitations\nof conventional linguistic metrics in evaluating responses in the programming\ndomain. Through accurate analysis, this paper looks at the divergence between\ntraditional linguistic metrics and our human-preferences-based reward model,\nunderscoring the imperative for domain-specific evaluation methods. By\nelucidating the complexities involved in applying RLHF to programming CQA and\naccentuating the significance of context-aware evaluation, this study\ncontributes to the ongoing efforts in refining Large Language Models through\nfocused human feedback.",
        "pdf_link": "https://arxiv.org/pdf/2401.10882v1.pdf"
    },
    {
        "title": "MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models",
        "authors": [
            "Tongxu Luo",
            "Jiahe Lei",
            "Fangyu Lei",
            "Weihao Liu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2024-02-20T09:30:48Z",
        "summary": "Fine-tuning is often necessary to enhance the adaptability of Large Language\nModels (LLM) to downstream tasks. Nonetheless, the process of updating billions\nof parameters demands significant computational resources and training time,\nwhich poses a substantial obstacle to the widespread application of large-scale\nmodels in various scenarios. To address this issue, Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.\nHowever, current PEFT approaches that employ a limited set of global parameters\n(such as LoRA, which adds low-rank approximation matrices to all weights) face\nchallenges in flexibly combining different computational modules in downstream\ntasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider\nLoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon\nobserved in MoE, we propose the utilization of contrastive learning to\nencourage experts to learn distinct features. We conducted experiments on 11\ntasks in math reasoning and common-sense reasoning benchmarks. With the same\nnumber of parameters, our approach outperforms LoRA significantly. In math\nreasoning, MoELoRA achieved an average performance that was 4.2% higher than\nLoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on\nseveral benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.12851v1.pdf"
    },
    {
        "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
        "authors": [
            "Wei Tao",
            "Yucheng Zhou",
            "Wenqiang Zhang",
            "Yu Cheng"
        ],
        "published": "2024-03-26T17:57:57Z",
        "summary": "In software evolution, resolving the emergent issues within GitHub\nrepositories is a complex challenge that involves not only the incorporation of\nnew code but also the maintenance of existing functionalities. Large Language\nModels (LLMs) have shown promise in code generation and understanding but face\ndifficulties in code change, particularly at the repository level. To overcome\nthese challenges, we empirically study the reason why LLMs mostly fail to\nresolve GitHub issues and analyze some impact factors. Motivated by the\nempirical findings, we propose a novel LLM-based Multi-Agent framework for\nGitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized\nfor the software evolution: Manager, Repository Custodian, Developer, and\nQuality Assurance Engineer agents. This framework leverages the collaboration\nof various agents in the planning and coding process to unlock the potential of\nLLMs to resolve GitHub issues. In experiments, we employ the SWE-bench\nbenchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and\nClaude-2. MAGIS can resolve 13.94% GitHub issues, which significantly\noutperforms the baselines. Specifically, MAGIS achieves an eight-fold increase\nin resolved ratio over the direct application of GPT-4, the based LLM of our\nmethod. We also analyze the factors for improving GitHub issue resolution\nrates, such as line location, task allocation, etc.",
        "pdf_link": "https://arxiv.org/pdf/2403.17927v1.pdf"
    },
    {
        "title": "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?",
        "authors": [
            "Agustinus Kristiadi",
            "Felix Strieth-Kalthoff",
            "Marta Skreta",
            "Pascal Poupart",
            "Al\u00e1n Aspuru-Guzik",
            "Geoff Pleiss"
        ],
        "published": "2024-02-07T16:32:58Z",
        "summary": "Automation is one of the cornerstones of contemporary material discovery.\nBayesian optimization (BO) is an essential part of such workflows, enabling\nscientists to leverage prior domain knowledge into efficient exploration of a\nlarge molecular space. While such prior knowledge can take many forms, there\nhas been significant fanfare around the ancillary scientific knowledge\nencapsulated in large language models (LLMs). However, existing work thus far\nhas only explored LLMs for heuristic materials searches. Indeed, recent work\nobtains the uncertainty estimate -- an integral part of BO -- from\npoint-estimated, non-Bayesian LLMs. In this work, we study the question of\nwhether LLMs are actually useful to accelerate principled Bayesian optimization\nin the molecular space. We take a sober, dispassionate stance in answering this\nquestion. This is done by carefully (i) viewing LLMs as fixed feature\nextractors for standard but principled BO surrogate models and by (ii)\nleveraging parameter-efficient finetuning methods and Bayesian neural networks\nto obtain the posterior of the LLM surrogate. Our extensive experiments with\nreal-world chemistry problems show that LLMs can be useful for BO over\nmolecules, but only if they have been pretrained or finetuned with\ndomain-specific data.",
        "pdf_link": "https://arxiv.org/pdf/2402.05015v1.pdf"
    },
    {
        "title": "LLM-Enhanced Data Management",
        "authors": [
            "Xuanhe Zhou",
            "Xinyang Zhao",
            "Guoliang Li"
        ],
        "published": "2024-02-04T23:42:02Z",
        "summary": "Machine learning (ML) techniques for optimizing data management problems have\nbeen extensively studied and widely deployed in recent five years. However\ntraditional ML methods have limitations on generalizability (adapting to\ndifferent scenarios) and inference ability (understanding the context).\nFortunately, large language models (LLMs) have shown high generalizability and\nhuman-competitive abilities in understanding context, which are promising for\ndata management tasks (e.g., database diagnosis, database tuning). However,\nexisting LLMs have several limitations: hallucination, high cost, and low\naccuracy for complicated tasks. To address these challenges, we design LLMDB,\nan LLM-enhanced data management paradigm which has generalizability and high\ninference ability while avoiding hallucination, reducing LLM cost, and\nachieving high accuracy. LLMDB embeds domain-specific knowledge to avoid\nhallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high\ncost of LLMs by vector databases which provide semantic search and caching\nabilities. LLMDB improves the task accuracy by LLM agent which provides\nmultiple-round inference and pipeline executions. We showcase three real-world\nscenarios that LLMDB can well support, including query rewrite, database\ndiagnosis and data analytics. We also summarize the open research challenges of\nLLMDB.",
        "pdf_link": "https://arxiv.org/pdf/2402.02643v1.pdf"
    },
    {
        "title": "Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation and Editing via Content-based Controls",
        "authors": [
            "Liwei Lin",
            "Gus Xia",
            "Yixiao Zhang",
            "Junyan Jiang"
        ],
        "published": "2024-02-14T19:00:01Z",
        "summary": "Controllable music generation plays a vital role in human-AI music\nco-creation. While Large Language Models (LLMs) have shown promise in\ngenerating high-quality music, their focus on autoregressive generation limits\ntheir utility in music editing tasks. To bridge this gap, we introduce a novel\nParameter-Efficient Fine-Tuning (PEFT) method. This approach enables\nautoregressive language models to seamlessly address music inpainting tasks.\nAdditionally, our PEFT method integrates frame-level content-based controls,\nfacilitating track-conditioned music refinement and score-conditioned music\narrangement. We apply this method to fine-tune MusicGen, a leading\nautoregressive music generation model. Our experiments demonstrate promising\nresults across multiple music editing tasks, offering more flexible controls\nfor future AI-driven music editing tools. A demo\npage\\footnote{\\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and\nsource codes\\footnote{\\url{https://github.com/Kikyo-16/airgen}.} are available\nonline.",
        "pdf_link": "https://arxiv.org/pdf/2402.09508v1.pdf"
    },
    {
        "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
        "authors": [
            "Zhihang Yuan",
            "Yuzhang Shang",
            "Yang Zhou",
            "Zhen Dong",
            "Zhe Zhou",
            "Chenhao Xue",
            "Bingzhe Wu",
            "Zhikai Li",
            "Qingyi Gu",
            "Yong Jae Lee",
            "Yan Yan",
            "Beidi Chen",
            "Guangyu Sun",
            "Kurt Keutzer"
        ],
        "published": "2024-02-26T07:33:05Z",
        "summary": "The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.",
        "pdf_link": "https://arxiv.org/pdf/2402.16363v5.pdf"
    },
    {
        "title": "Context Quality Matters in Training Fusion-in-Decoder for Extractive Open-Domain Question Answering",
        "authors": [
            "Kosuke Akimoto",
            "Kunihiro Takeoka",
            "Masafumi Oyamada"
        ],
        "published": "2024-03-21T07:47:57Z",
        "summary": "Retrieval-augmented generation models augment knowledge encoded in a language\nmodel by providing additional relevant external knowledge (context) during\ngeneration. Although it has been shown that the quantity and quality of context\nimpact the performance of retrieval-augmented generation models during\ninference, limited research explores how these characteristics affect model\ntraining. This paper explores how context quantity and quality during model\ntraining affect the performance of Fusion-in-Decoder (FiD), the\nstate-of-the-art retrieval-augmented generation model, in extractive\nopen-domain question answering tasks. Experimental results suggest that FiD\nmodels overfit to context quality during training and show suboptimal\nperformance when evaluated on different context quality. Through the\nexperimental results, we also reveal FiD models trained with different context\nquality have different cross-attention distribution patterns. Specifically, as\ncontext quality during training increases, FiD models tend to attend more\nuniformly to each passage in context. Finally, based on these observations, we\npropose a method to mitigate overfitting to specific context quality by\nintroducing bias to the cross-attention distribution, which we demonstrate to\nbe effective in improving the performance of FiD models on different context\nquality.",
        "pdf_link": "https://arxiv.org/pdf/2403.14197v1.pdf"
    },
    {
        "title": "Multi-Cultural Commonsense Knowledge Distillation",
        "authors": [
            "Tuan-Phong Nguyen",
            "Simon Razniewski",
            "Gerhard Weikum"
        ],
        "published": "2024-02-16T13:46:38Z",
        "summary": "Despite recent progress, large language models (LLMs) still face the\nchallenge of appropriately reacting to the intricacies of social and cultural\nconventions. This paper presents MANGO, a methodology for distilling\nhigh-accuracy, high-recall assertions of cultural knowledge. We judiciously and\niteratively prompt LLMs for this purpose from two entry points, concepts and\ncultures. Outputs are consolidated via clustering and generative summarization.\nRunning the MANGO method with GPT-3.5 as underlying LLM yields 167K\nhigh-accuracy assertions for 30K concepts and 11K cultures, surpassing prior\nresources by a large margin. For extrinsic evaluation, we explore augmenting\ndialogue systems with cultural knowledge assertions. We find that adding\nknowledge from MANGO improves the overall quality, specificity, and cultural\nsensitivity of dialogue responses, as judged by human annotators. Data and code\nare available for download.",
        "pdf_link": "https://arxiv.org/pdf/2402.10689v1.pdf"
    },
    {
        "title": "How to Discern Important Urgent News?",
        "authors": [
            "Oleg Vasilyev",
            "John Bohannon"
        ],
        "published": "2024-02-15T20:08:07Z",
        "summary": "We found that a simple property of clusters in a clustered dataset of news\ncorrelate strongly with importance and urgency of news (IUN) as assessed by\nLLM. We verified our finding across different news datasets, dataset sizes,\nclustering algorithms and embeddings. The found correlation should allow using\nclustering (as an alternative to LLM) for identifying the most important urgent\nnews, or for filtering out unimportant articles.",
        "pdf_link": "https://arxiv.org/pdf/2402.10302v1.pdf"
    },
    {
        "title": "LVCHAT: Facilitating Long Video Comprehension",
        "authors": [
            "Yu Wang",
            "Zeyuan Zhang",
            "Julian McAuley",
            "Zexue He"
        ],
        "published": "2024-02-19T11:59:14Z",
        "summary": "Enabling large language models (LLMs) to read videos is vital for multimodal\nLLMs. Existing works show promise on short videos whereas long video (longer\nthan e.g.~1 minute) comprehension remains challenging. The major problem lies\nin the over-compression of videos, i.e., the encoded video representations are\nnot enough to represent the whole video. To address this issue, we propose Long\nVideo Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to\ndynamically adjust the number of embeddings in alignment with the duration of\nthe video to ensure long videos are not overly compressed into a few\nembeddings. To deal with long videos whose length is beyond videos seen during\ntraining, we propose Interleaved Frame Encoding (IFE), repeating positional\nembedding and interleaving multiple groups of videos to enable long video\ninput, avoiding performance degradation due to overly long videos. Experimental\nresults show that LVChat significantly outperforms existing methods by up to\n27\\% in accuracy on long-video QA datasets and long-video captioning\nbenchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.",
        "pdf_link": "https://arxiv.org/pdf/2402.12079v1.pdf"
    },
    {
        "title": "I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large Language Models",
        "authors": [
            "Wenchao Dong",
            "Assem Zhunis",
            "Hyojin Chin",
            "Jiyoung Han",
            "Meeyoung Cha"
        ],
        "published": "2024-02-16T03:54:48Z",
        "summary": "We explored cultural biases-individualism vs. collectivism-in ChatGPT across\nthree Western languages (i.e., English, German, and French) and three Eastern\nlanguages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an\nindividualistic persona in Western languages, its collectivism scores (i.e.,\nout-group values) exhibited a more negative trend, surpassing their positive\norientation towards individualism (i.e., in-group values). Conversely, when a\ncollectivistic persona was assigned to ChatGPT in Eastern languages, a similar\npattern emerged with more negative responses toward individualism (i.e.,\nout-group values) as compared to collectivism (i.e., in-group values). The\nresults indicate that when imbued with a particular social identity, ChatGPT\ndiscerns in-group and out-group, embracing in-group values while eschewing\nout-group values. Notably, the negativity towards the out-group, from which\nprejudices and discrimination arise, exceeded the positivity towards the\nin-group. The experiment was replicated in the political domain, and the\nresults remained consistent. Furthermore, this replication unveiled an\nintrinsic Democratic bias in Large Language Models (LLMs), aligning with\nearlier findings and providing integral insights into mitigating such bias\nthrough prompt engineering. Extensive robustness checks were performed using\nvarying hyperparameter and persona setup methods, with or without social\nidentity labels, across other popular language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10436v1.pdf"
    },
    {
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
        "authors": [
            "Alexandre Alcoforado",
            "Thomas Palmeira Ferraz",
            "Lucas Hideki Okamura",
            "Israel Campos Fama",
            "Arnold Moya Lavado",
            "B\u00e1rbara Dias Bueno",
            "Bruno Veloso",
            "Anna Helena Reali Costa"
        ],
        "published": "2024-01-24T04:57:32Z",
        "summary": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.13229v1.pdf"
    },
    {
        "title": "QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs",
        "authors": [
            "Isha Chaudhary",
            "Vedaant V. Jain",
            "Gagandeep Singh"
        ],
        "published": "2024-02-24T23:16:57Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nseveral benchmarks. However, traditional studies do not provide formal\nguarantees on the performance of LLMs. In this work, we propose a novel\ncertification framework for LLM, QuaCer-C, wherein we formally certify the\nknowledge-comprehension capabilities of popular LLMs. Our certificates are\nquantitative - they consist of high-confidence, tight bounds on the probability\nthat the target LLM gives the correct answer on any relevant knowledge\ncomprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs\nindicate that the knowledge comprehension capability improves with an increase\nin the number of parameters and that the Mistral model is less performant than\nthe rest in this evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.15929v1.pdf"
    },
    {
        "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
        "authors": [
            "Yongquan He",
            "Xuancheng Huang",
            "Minghao Tang",
            "Lingxun Meng",
            "Xiang Li",
            "Wei Lin",
            "Wenyuan Zhang",
            "Yifu Gao"
        ],
        "published": "2024-03-15T06:54:20Z",
        "summary": "Instruction tuning for large language models (LLMs) can drive them to produce\nresults consistent with human goals in specific downstream tasks. However, the\nprocess of continual instruction tuning (CIT) for LLMs may bring about the\ncatastrophic forgetting (CF) problem, where previously learned abilities are\ndegraded. Recent methods try to alleviate the CF problem by modifying models or\nreplaying data, which may only remember the surface-level pattern of\ninstructions and get confused on held-out tasks. In this paper, we propose a\nnovel continual instruction tuning method based on Key-part Information Gain\n(KPIG). Our method computes the information gain on masked parts to dynamically\nreplay data and refine the training objective, which enables LLMs to capture\ntask-aware information relevant to the correct response and alleviate\noverfitting to general descriptions in instructions. In addition, we propose\ntwo metrics, P-score and V-score, to measure the generalization and\ninstruction-following abilities of LLMs. Experiments demonstrate our method\nachieves superior performance on both seen and held-out tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.10056v1.pdf"
    },
    {
        "title": "CheckEval: Robust Evaluation Framework using Large Language Model via Checklist",
        "authors": [
            "Yukyung Lee",
            "Joonghoon Kim",
            "Jaehee Kim",
            "Hyowon Cho",
            "Pilsung Kang"
        ],
        "published": "2024-03-27T17:20:39Z",
        "summary": "We introduce CheckEval, a novel evaluation framework using Large Language\nModels, addressing the challenges of ambiguity and inconsistency in current\nevaluation methods. CheckEval addresses these challenges by dividing evaluation\ncriteria into detailed sub-aspects and constructing a checklist of Boolean\nquestions for each, simplifying the evaluation. This approach not only renders\nthe process more interpretable but also significantly enhances the robustness\nand reliability of results by focusing on specific evaluation dimensions.\nValidated through a focused case study using the SummEval benchmark, CheckEval\nindicates a strong correlation with human judgments. Furthermore, it\ndemonstrates a highly consistent Inter-Annotator Agreement. These findings\nhighlight the effectiveness of CheckEval for objective, flexible, and precise\nevaluations. By offering a customizable and interactive framework, CheckEval\nsets a new standard for the use of LLMs in evaluation, responding to the\nevolving needs of the field and establishing a clear method for future\nLLM-based evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2403.18771v1.pdf"
    },
    {
        "title": "Learning to Poison Large Language Models During Instruction Tuning",
        "authors": [
            "Yao Qiang",
            "Xiangyu Zhou",
            "Saleh Zare Zade",
            "Mohammad Amin Roshani",
            "Douglas Zytko",
            "Dongxiao Zhu"
        ],
        "published": "2024-02-21T01:30:03Z",
        "summary": "The advent of Large Language Models (LLMs) has marked significant\nachievements in language processing and reasoning capabilities. Despite their\nadvancements, LLMs face vulnerabilities to data poisoning attacks, where\nadversaries insert backdoor triggers into training data to manipulate outputs\nfor malicious purposes. This work further identifies additional security risks\nin LLMs by designing a new data poisoning attack tailored to exploit the\ninstruction tuning process. We propose a novel gradient-guided backdoor trigger\nlearning approach to identify adversarial triggers efficiently, ensuring an\nevasion of detection by conventional defenses while maintaining content\nintegrity. Through experimental validation across various LLMs and tasks, our\nstrategy demonstrates a high success rate in compromising model outputs;\npoisoning only 1\\% of 4,000 instruction tuning samples leads to a Performance\nDrop Rate (PDR) of around 80\\%. Our work highlights the need for stronger\ndefenses against data poisoning attack, offering insights into safeguarding\nLLMs against these more sophisticated attacks. The source code can be found on\nthis GitHub repository: https://github.com/RookieZxy/GBTL/blob/main/README.md.",
        "pdf_link": "https://arxiv.org/pdf/2402.13459v1.pdf"
    },
    {
        "title": "Can Large Language Models Replace Economic Choice Prediction Labs?",
        "authors": [
            "Eilam Shapira",
            "Omer Madmon",
            "Roi Reichart",
            "Moshe Tennenholtz"
        ],
        "published": "2024-01-30T20:49:47Z",
        "summary": "Economic choice prediction is an essential challenging task, often\nconstrained by the difficulties in acquiring human choice data. Indeed,\nexperimental economics studies had focused mostly on simple choice settings.\nThe AI community has recently contributed to that effort in two ways:\nconsidering whether LLMs can substitute for humans in the above-mentioned\nsimple choice prediction settings, and the study through ML lens of more\nelaborated but still rigorous experimental economics settings, employing\nincomplete information, repetitive play, and natural language communication,\nnotably language-based persuasion games. This leaves us with a major\ninspiration: can LLMs be used to fully simulate the economic environment and\ngenerate data for efficient human choice prediction, substituting for the\nelaborated economic lab studies? We pioneer the study of this subject,\ndemonstrating its feasibility. In particular, we show that a model trained\nsolely on LLM-generated data can effectively predict human behavior in a\nlanguage-based persuasion game, and can even outperform models trained on\nactual human data.",
        "pdf_link": "https://arxiv.org/pdf/2401.17435v3.pdf"
    },
    {
        "title": "I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments",
        "authors": [
            "Xuan Ren",
            "Biao Wu",
            "Lingqiao Liu"
        ],
        "published": "2024-02-17T05:05:31Z",
        "summary": "Fine-tuning large language models (LLMs) with a small data set for particular\ntasks is a widely encountered yet complex challenge. The potential for\noverfitting on a limited number of examples can negatively impact the model's\nability to generalize and retain its original skills. Our research explores the\nimpact of the style of ground-truth responses during the fine-tuning process.\nWe found that matching the ground-truth response style with the LLM's inherent\nstyle results in better learning outcomes. Building on this insight, we\ndeveloped a method that minimally alters the LLM's pre-existing responses to\ncorrect errors, using these adjusted responses as training targets. This\ntechnique enables precise corrections in line with the model's native response\nstyle, safeguarding the model's core capabilities and thus avoid overfitting.\nOur findings show that this approach not only improves the LLM's task-specific\naccuracy but also crucially maintains its original competencies and\neffectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11192v1.pdf"
    },
    {
        "title": "Shifting the Lens: Detecting Malware in npm Ecosystem with Large Language Models",
        "authors": [
            "Nusrat Zahan",
            "Philipp Burckhardt",
            "Mikola Lysenko",
            "Feross Aboukhadijeh",
            "Laurie Williams"
        ],
        "published": "2024-03-18T19:10:12Z",
        "summary": "The Gartner 2022 report predicts that 45% of organizations worldwide will\nencounter software supply chain attacks by 2025, highlighting the urgency to\nimprove software supply chain security for community and national interests.\nCurrent malware detection techniques aid in the manual review process by\nfiltering benign and malware packages, yet such techniques have high\nfalse-positive rates and limited automation support. Therefore, malware\ndetection techniques could benefit from advanced, more automated approaches for\naccurate and minimally false-positive results. The goal of this study is to\nassist security analysts in identifying malicious packages through the\nempirical study of large language models (LLMs) to detect potential malware in\nthe npm ecosystem.\n  We present SocketAI Scanner, a multi-stage decision-maker malware detection\nworkflow using iterative self-refinement and zero-shot-role-play-Chain of\nThought (CoT) prompting techniques for ChatGPT. We studied 5,115 npm packages\n(of which 2,180 are malicious) and performed a baseline comparison of the GPT-3\nand GPT-4 models with a static analysis tool. Our findings showed promising\nresults for GPT models with low misclassification alert rates. Our baseline\ncomparison demonstrates a notable improvement over static analysis in precision\nscores above 25% and F1 scores above 15%. We attained precision and F1 scores\nof 91% and 94%, respectively, for the GPT-3 model. Overall, GPT-4 demonstrates\nsuperior performance in precision (99%) and F1 (97%) scores, while GPT-3\npresents a cost-effective balance between performance and expenditure.",
        "pdf_link": "https://arxiv.org/pdf/2403.12196v1.pdf"
    },
    {
        "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
        "authors": [
            "Ran Elgedawy",
            "Sudarshan Srinivasan",
            "Ioana Danciu"
        ],
        "published": "2024-01-19T14:50:22Z",
        "summary": "Electronic health records (EHRs) house crucial patient data in clinical\nnotes. As these notes grow in volume and complexity, manual extraction becomes\nchallenging. This work introduces a natural language interface using large\nlanguage models (LLMs) for dynamic question-answering on clinical notes. Our\nchatbot, powered by Langchain and transformer-based LLMs, allows users to query\nin natural language, receiving relevant answers from clinical notes.\nExperiments, utilizing various embedding models and advanced LLMs, show Wizard\nVicuna's superior accuracy, albeit with high compute demands. Model\noptimization, including weight quantization, improves latency by approximately\n48 times. Promising results indicate potential, yet challenges such as model\nhallucinations and limited diverse medical case evaluations remain. Addressing\nthese gaps is crucial for unlocking the value in clinical notes and advancing\nAI-driven clinical decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2401.10733v1.pdf"
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
        "authors": [
            "Andy Zhou",
            "Bo Li",
            "Haohan Wang"
        ],
        "published": "2024-01-30T18:56:08Z",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on benign use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4, GUARD, from 92% to 6%.",
        "pdf_link": "https://arxiv.org/pdf/2401.17263v2.pdf"
    },
    {
        "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models",
        "authors": [
            "Yixin Liu",
            "Kai Zhang",
            "Yuan Li",
            "Zhiling Yan",
            "Chujie Gao",
            "Ruoxi Chen",
            "Zhengqing Yuan",
            "Yue Huang",
            "Hanchi Sun",
            "Jianfeng Gao",
            "Lifang He",
            "Lichao Sun"
        ],
        "published": "2024-02-27T03:30:58Z",
        "summary": "Sora is a text-to-video generative AI model, released by OpenAI in February\n2024. The model is trained to generate videos of realistic or imaginative\nscenes from text instructions and show potential in simulating the physical\nworld. Based on public technical reports and reverse engineering, this paper\npresents a comprehensive review of the model's background, related\ntechnologies, applications, remaining challenges, and future directions of\ntext-to-video AI models. We first trace Sora's development and investigate the\nunderlying technologies used to build this \"world simulator\". Then, we describe\nin detail the applications and potential impact of Sora in multiple industries\nranging from film-making and education to marketing. We discuss the main\nchallenges and limitations that need to be addressed to widely deploy Sora,\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\nfuture development of Sora and video generation models in general, and how\nadvancements in the field could enable new ways of human-AI interaction,\nboosting productivity and creativity of video generation.",
        "pdf_link": "https://arxiv.org/pdf/2402.17177v2.pdf"
    },
    {
        "title": "TrustScore: Reference-Free Evaluation of LLM Response Trustworthiness",
        "authors": [
            "Danna Zheng",
            "Danyang Liu",
            "Mirella Lapata",
            "Jeff Z. Pan"
        ],
        "published": "2024-02-19T21:12:14Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious domains, prompting a surge in their practical applications. However,\nconcerns have arisen regarding the trustworthiness of LLMs outputs,\nparticularly in closed-book question-answering tasks, where non-experts may\nstruggle to identify inaccuracies due to the absence of contextual or ground\ntruth information. This paper introduces TrustScore, a framework based on the\nconcept of Behavioral Consistency, which evaluates whether an LLMs response\naligns with its intrinsic knowledge. Additionally, TrustScore can seamlessly\nintegrate with fact-checking methods, which assesses alignment with external\nknowledge sources. The experimental results show that TrustScore achieves\nstrong correlations with human judgments, surpassing existing reference-free\nmetrics, and achieving results on par with reference-based metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.12545v1.pdf"
    },
    {
        "title": "Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation",
        "authors": [
            "Bin Zhang",
            "Yuxiao Ye",
            "Guoqing Du",
            "Xiaoru Hu",
            "Zhishuai Li",
            "Sun Yang",
            "Chi Harold Liu",
            "Rui Zhao",
            "Ziyue Li",
            "Hangyu Mao"
        ],
        "published": "2024-03-05T13:23:48Z",
        "summary": "Large Language Models (LLMs) have emerged as a powerful tool in advancing the\nText-to-SQL task, significantly outperforming traditional methods.\nNevertheless, as a nascent research field, there is still no consensus on the\noptimal prompt templates and design frameworks. Additionally, existing\nbenchmarks inadequately explore the performance of LLMs across the various\nsub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs'\ncognitive capabilities and the optimization of LLM-based solutions. To address\nthe aforementioned issues, we firstly construct a new dataset designed to\nmitigate the risk of overfitting in LLMs. Then we formulate five evaluation\ntasks to comprehensively assess the performance of diverse methods across\nvarious LLMs throughout the Text-to-SQL process.Our study highlights the\nperformance disparities among LLMs and proposes optimal in-context learning\nsolutions tailored to each task. These findings offer valuable insights for\nenhancing the development of LLM-based Text-to-SQL systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.02951v2.pdf"
    },
    {
        "title": "Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to Inform GenAI Copyright Disputes",
        "authors": [
            "Uri Hacohen",
            "Adi Haviv",
            "Shahar Sarfaty",
            "Bruria Friedman",
            "Niva Elkin-Koren",
            "Roi Livni",
            "Amit H Bermano"
        ],
        "published": "2024-03-26T13:32:32Z",
        "summary": "The advent of Generative Artificial Intelligence (GenAI) models, including\nGitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content\ncreation, enabling non-professionals to produce high-quality content across\nvarious domains. This transformative technology has led to a surge of synthetic\ncontent and sparked legal disputes over copyright infringement. To address\nthese challenges, this paper introduces a novel approach that leverages the\nlearning capacity of GenAI models for copyright legal analysis, demonstrated\nwith GPT2 and Stable Diffusion models. Copyright law distinguishes between\noriginal expressions and generic ones (Sc\\`enes \\`a faire), protecting the\nformer and permitting reproduction of the latter. However, this distinction has\nhistorically been challenging to make consistently, leading to over-protection\nof copyrighted works. GenAI offers an unprecedented opportunity to enhance this\nlegal analysis by revealing shared patterns in preexisting works. We propose a\ndata-driven approach to identify the genericity of works created by GenAI,\nemploying \"data-driven bias\" to assess the genericity of expressive\ncompositions. This approach aids in copyright scope determination by utilizing\nthe capabilities of GenAI to identify and prioritize expressive elements and\nrank them according to their frequency in the model's dataset. The potential\nimplications of measuring expressive genericity for copyright law are profound.\nSuch scoring could assist courts in determining copyright scope during\nlitigation, inform the registration practices of Copyright Offices, allowing\nregistration of only highly original synthetic works, and help copyright owners\nsignal the value of their works and facilitate fairer licensing deals. More\ngenerally, this approach offers valuable insights to policymakers grappling\nwith adapting copyright law to the challenges posed by the era of GenAI.",
        "pdf_link": "https://arxiv.org/pdf/2403.17691v1.pdf"
    },
    {
        "title": "Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities",
        "authors": [
            "Honglin Mu",
            "Yang Xu",
            "Yunlong Feng",
            "Xiaofeng Han",
            "Yitong Li",
            "Yutai Hou",
            "Wanxiang Che"
        ],
        "published": "2024-03-17T07:34:12Z",
        "summary": "With the rise of Large Language Models (LLMs), AI assistants' ability to\nutilize tools, especially through API calls, has advanced notably. This\nprogress has necessitated more accurate evaluation methods. Many existing\nstudies adopt static evaluation, where they assess AI assistants' API call\nbased on pre-defined dialogue histories. However, such evaluation method can be\nmisleading, as an AI assistant might fail in generating API calls from\npreceding human interaction in real cases. Instead of the resource-intensive\nmethod of direct human-machine interactions, we propose Automated Dynamic\nEvaluation (AutoDE) to assess an assistant's API call capability without human\ninvolvement. In our framework, we endeavor to closely mirror genuine human\nconversation patterns in human-machine interactions, using a LLM-based user\nagent, equipped with a user script to ensure human alignment. Experimental\nresults highlight that AutoDE uncovers errors overlooked by static evaluations,\naligning more closely with human assessment. Testing four AI assistants using\nour crafted benchmark, our method further mirrored human evaluation compared to\nconventional static evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2403.11128v2.pdf"
    },
    {
        "title": "Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials",
        "authors": [
            "Gennaro Nolano",
            "Moritz Blum",
            "Basil Ell",
            "Philipp Cimiano"
        ],
        "published": "2024-02-29T12:01:46Z",
        "summary": "In recent years, large language models have achieved state-of-the-art\nperformance across various NLP tasks. However, investigations have shown that\nthese models tend to rely on shortcut features, leading to inaccurate\npredictions and causing the models to be unreliable at generalization to\nout-of-distribution (OOD) samples. For instance, in the context of relation\nextraction (RE), we would expect a model to identify the same relation\nindependently of the entities involved in it. For example, consider the\nsentence \"Leonardo da Vinci painted the Mona Lisa\" expressing the\ncreated(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute \"Leonardo da\nVinci\" with \"Barack Obama\", then the sentence still expresses the created\nrelation. A robust model is supposed to detect the same relation in both cases.\nIn this work, we describe several semantically-motivated strategies to generate\nadversarial examples by replacing entity mentions and investigate how\nstate-of-the-art RE models perform under pressure. Our analyses show that the\nperformance of these models significantly deteriorates on the modified datasets\n(avg. of -48.5% in F1), which indicates that these models rely to a great\nextent on shortcuts, such as surface forms (or patterns therein) of entities,\nwithout making full use of the information present in the sentences.",
        "pdf_link": "https://arxiv.org/pdf/2402.19076v1.pdf"
    },
    {
        "title": "TQCompressor: improving tensor decomposition methods in neural networks via permutations",
        "authors": [
            "V. Abronin",
            "A. Naumov",
            "D. Mazur",
            "D. Bystrov",
            "K. Tsarova",
            "Ar. Melnikov",
            "I. Oseledets",
            "S. Dolgov",
            "R. Brasher",
            "M. Perelshtein"
        ],
        "published": "2024-01-29T18:07:56Z",
        "summary": "We introduce TQCompressor, a novel method for neural network model\ncompression with improved tensor decompositions. We explore the challenges\nposed by the computational and storage demands of pre-trained language models\nin NLP tasks and propose a permutation-based enhancement to Kronecker\ndecomposition. This enhancement makes it possible to reduce loss in model\nexpressivity which is usually associated with factorization. We demonstrate\nthis method applied to the GPT-2$_{small}$. The result of the compression is\nTQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in\nthe GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further\nenhance the performance of the TQCompressedGPT-2 through a training strategy\ninvolving multi-step knowledge distillation, using only a 3.1% of the\nOpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative\nevaluations, marking an advancement in the efficient and effective deployment\nof models in resource-constrained environments.",
        "pdf_link": "https://arxiv.org/pdf/2401.16367v1.pdf"
    },
    {
        "title": "Security Code Review by LLMs: A Deep Dive into Responses",
        "authors": [
            "Jiaxin Yu",
            "Peng Liang",
            "Yujia Fu",
            "Amjed Tahir",
            "Mojtaba Shahin",
            "Chong Wang",
            "Yangxiao Cai"
        ],
        "published": "2024-01-29T17:13:44Z",
        "summary": "Security code review aims to combine automated tools and manual efforts to\ndetect security defects during development. The rapid development of Large\nLanguage Models (LLMs) has shown promising potential in software development,\nas well as opening up new possibilities in automated security code review. To\nexplore the challenges of applying LLMs in practical code review for security\ndefect detection, this study compared the detection performance of three\nstate-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on\n549 code files that contain security defects from real-world code reviews.\nThrough analyzing 82 responses generated by the best-performing LLM-prompt\ncombination based on 100 randomly selected code files, we extracted and\ncategorized quality problems present in these responses into 5 themes and 16\ncategories. Our results indicate that the responses produced by LLMs often\nsuffer from verbosity, vagueness, and incompleteness, highlighting the\nnecessity to enhance their conciseness, understandability, and compliance to\nsecurity defect detection. This work reveals the deficiencies of LLM-generated\nresponses in security code review and paves the way for future optimization of\nLLMs towards this task.",
        "pdf_link": "https://arxiv.org/pdf/2401.16310v1.pdf"
    },
    {
        "title": "Apriori Knowledge in an Era of Computational Opacity: The Role of AI in Mathematical Discovery",
        "authors": [
            "Eamon Duede",
            "Kevin Davey"
        ],
        "published": "2024-03-15T21:38:26Z",
        "summary": "Computation is central to contemporary mathematics. Many accept that we can\nacquire genuine mathematical knowledge of the Four Color Theorem from Appel and\nHaken's program insofar as it is simply a repetitive application of human forms\nof mathematical reasoning. Modern LLMs / DNNs are, by contrast, opaque to us in\nsignificant ways, and this creates obstacles in obtaining mathematical\nknowledge from them. We argue, however, that if a proof-checker automating\nhuman forms of proof-checking is attached to such machines, then we can obtain\napriori mathematical knowledge from them, even though the original machines are\nentirely opaque to us and the proofs they output are not human-surveyable.",
        "pdf_link": "https://arxiv.org/pdf/2403.15437v1.pdf"
    },
    {
        "title": "Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",
        "authors": [
            "Arnav Chavan",
            "Raghav Magazine",
            "Shubham Kushwaha",
            "M\u00e9rouane Debbah",
            "Deepak Gupta"
        ],
        "published": "2024-02-02T06:29:34Z",
        "summary": "Despite the impressive performance of LLMs, their widespread adoption faces\nchallenges due to substantial computational and memory requirements during\ninference. Recent advancements in model compression and system-level\noptimization methods aim to enhance LLM inference. This survey offers an\noverview of these methods, emphasizing recent developments. Through experiments\non LLaMA(/2)-7B, we evaluate various compression techniques, providing\npractical insights for efficient LLM deployment in a unified setting. The\nempirical analysis on LLaMA(/2)-7B highlights the effectiveness of these\nmethods. Drawing from survey insights, we identify current limitations and\ndiscuss potential future directions to improve LLM inference efficiency. We\nrelease the codebase to reproduce the results presented in this paper at\nhttps://github.com/nyunAI/Faster-LLM-Survey",
        "pdf_link": "https://arxiv.org/pdf/2402.01799v1.pdf"
    },
    {
        "title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models",
        "authors": [
            "Yizhi LI",
            "Ge Zhang",
            "Xingwei Qu",
            "Jiali Li",
            "Zhaoqun Li",
            "Zekun Wang",
            "Hao Li",
            "Ruibin Yuan",
            "Yinghao Ma",
            "Kai Zhang",
            "Wangchunshu Zhou",
            "Yiming Liang",
            "Lei Zhang",
            "Lei Ma",
            "Jiajun Zhang",
            "Zuowen Li",
            "Stephen W. Huang",
            "Chenghua Lin",
            "Wenhu Chen",
            "Jie Fu"
        ],
        "published": "2024-02-20T16:02:12Z",
        "summary": "The advancement of large language models (LLMs) has enhanced the ability to\ngeneralize across a wide range of unseen natural language processing (NLP)\ntasks through instruction-following. Yet, their effectiveness often diminishes\nin low-resource languages like Chinese, exacerbated by biased evaluations from\ndata leakage, casting doubt on their true generalizability to new linguistic\nterritories. In response, we introduce the Chinese Instruction-Following\nBenchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of\nLLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000\ninput-output pairs, developed by native speakers to test complex reasoning and\nChinese cultural nuances across 20 categories. To mitigate evaluation bias, we\nrelease only half of the dataset publicly, with the remainder kept private, and\nintroduce diversified instructions to minimize score variance, totaling 45,000\ndata instances. Our evaluation of 28 selected LLMs reveals a noticeable\nperformance gap, with the best model scoring only 52.9%, highlighting the\nlimitations of LLMs in less familiar language and task contexts. This work aims\nto uncover the current limitations of LLMs in handling Chinese tasks, pushing\ntowards the development of more culturally informed and linguistically diverse\nmodels with the released data and benchmark\n(https://yizhilll.github.io/CIF-Bench/).",
        "pdf_link": "https://arxiv.org/pdf/2402.13109v1.pdf"
    },
    {
        "title": "Logits of API-Protected LLMs Leak Proprietary Information",
        "authors": [
            "Matthew Finlayson",
            "Xiang Ren",
            "Swabha Swayamdipta"
        ],
        "published": "2024-03-14T16:27:49Z",
        "summary": "The commercialization of large language models (LLMs) has led to the common\npractice of high-level API-only access to proprietary models. In this work, we\nshow that even with a conservative assumption about the model architecture, it\nis possible to learn a surprisingly large amount of non-public information\nabout an API-protected LLM from a relatively small number of API queries (e.g.,\ncosting under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on\none key observation: most modern LLMs suffer from a softmax bottleneck, which\nrestricts the model outputs to a linear subspace of the full output space. We\nshow that this lends itself to a model image or a model signature which unlocks\nseveral capabilities with affordable cost: efficiently discovering the LLM's\nhidden size, obtaining full-vocabulary outputs, detecting and disambiguating\ndifferent model updates, identifying the source LLM given a single full LLM\noutput, and even estimating the output layer parameters. Our empirical\ninvestigations show the effectiveness of our methods, which allow us to\nestimate the embedding size of OpenAI's gpt-3.5-turbo to be about 4,096.\nLastly, we discuss ways that LLM providers can guard against these attacks, as\nwell as how these capabilities can be viewed as a feature (rather than a bug)\nby allowing for greater transparency and accountability.",
        "pdf_link": "https://arxiv.org/pdf/2403.09539v2.pdf"
    },
    {
        "title": "Evolving Code with A Large Language Model",
        "authors": [
            "Erik Hemberg",
            "Stephen Moskal",
            "Una-May O'Reilly"
        ],
        "published": "2024-01-13T15:57:54Z",
        "summary": "Algorithms that use Large Language Models (LLMs) to evolve code arrived on\nthe Genetic Programming (GP) scene very recently. We present LLM GP, a\nformalized LLM-based evolutionary algorithm designed to evolve code. Like GP,\nit uses evolutionary operators, but its designs and implementations of those\noperators radically differ from GP's because they enlist an LLM, using\nprompting and the LLM's pre-trained pattern matching and sequence completion\ncapability. We also present a demonstration-level variant of LLM GP and share\nits code. By addressing algorithms that range from the formal to hands-on, we\ncover design and LLM-usage considerations as well as the scientific challenges\nthat arise when using an LLM for genetic programming.",
        "pdf_link": "https://arxiv.org/pdf/2401.07102v1.pdf"
    },
    {
        "title": "OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)",
        "authors": [
            "Fujian Jia",
            "Xin Liu",
            "Lixi Deng",
            "Jiwen Gu",
            "Chunchao Pu",
            "Tunan Bai",
            "Mengjiang Huang",
            "Yuanzhi Lu",
            "Kang Liu"
        ],
        "published": "2024-02-26T18:33:13Z",
        "summary": "In the past year, there has been a growing trend in applying Large Language\nModels (LLMs) to the field of medicine, particularly with the advent of\nadvanced language models such as ChatGPT developed by OpenAI. However, there is\nlimited research on LLMs specifically addressing oncology-related queries. The\nprimary aim of this research was to develop a specialized language model that\ndemonstrates improved accuracy in providing advice related to oncology. We\nperformed an extensive data collection of online question-answer interactions\ncentered around oncology, sourced from reputable doctor-patient platforms.\nFollowing data cleaning and anonymization, a dataset comprising over 180K+\noncology-related conversations was established. The conversations were\ncategorized and meticulously reviewed by field specialists and clinicians to\nensure precision. Employing the LLaMA model and other selected open-source\ndatasets, we conducted iterative fine-tuning to enhance the model's proficiency\nin basic medical conversation and specialized oncology knowledge. We observed a\nsubstantial enhancement in the model's understanding of genuine patient\ninquiries and its reliability in offering oncology-related advice through the\nutilization of real online question-answer interactions in the fine-tuning\nprocess. We release database and models to the research community\n(https://github.com/OncoGPT1).",
        "pdf_link": "https://arxiv.org/pdf/2402.16810v1.pdf"
    },
    {
        "title": "Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check",
        "authors": [
            "Linhao Ye",
            "Zhikai Lei",
            "Jianghao Yin",
            "Qin Chen",
            "Jie Zhou",
            "Liang He"
        ],
        "published": "2024-03-27T04:20:18Z",
        "summary": "Retrieval-Augmented Generation (RAG) aims to generate more reliable and\naccurate responses, by augmenting large language models (LLMs) with the\nexternal vast and dynamic knowledge. Most previous work focuses on using RAG\nfor single-round question answering, while how to adapt RAG to the complex\nconversational setting wherein the question is interdependent on the preceding\ncontext is not well studied. In this paper, we propose a conversation-level RAG\napproach, which incorporates fine-grained retrieval augmentation and self-check\nfor conversational question answering (CQA). In particular, our approach\nconsists of three components, namely conversational question refiner,\nfine-grained retriever and self-check based response generator, which work\ncollaboratively for question understanding and relevant information acquisition\nin conversational settings. Extensive experiments demonstrate the great\nadvantages of our approach over the state-of-the-art baselines. Moreover, we\nalso release a Chinese CQA dataset with new features including reformulated\nquestion, extracted keyword, retrieved paragraphs and their helpfulness, which\nfacilitates further researches in RAG enhanced CQA.",
        "pdf_link": "https://arxiv.org/pdf/2403.18243v1.pdf"
    },
    {
        "title": "A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries",
        "authors": [
            "Asad Aali",
            "Dave Van Veen",
            "Yamin Ishraq Arefeen",
            "Jason Hom",
            "Christian Bluethgen",
            "Eduardo Pontes Reis",
            "Sergios Gatidis",
            "Namuun Clifford",
            "Joseph Daws",
            "Arash S. Tehrani",
            "Jangwon Kim",
            "Akshay S. Chaudhari"
        ],
        "published": "2024-03-08T23:17:55Z",
        "summary": "Brief hospital course (BHC) summaries are common clinical documents generated\nby summarizing clinical notes. While large language models (LLMs) depict\nremarkable capabilities in automating real-world tasks, their capabilities for\nhealthcare applications such as BHC synthesis have not been shown. To enable\nthe adaptation of LLMs for BHC synthesis, we introduce a novel benchmark\nconsisting of a pre-processed dataset extracted from MIMIC-IV notes,\nencapsulating clinical note, and brief hospital course (BHC) pairs. We assess\nthe performance of two general-purpose LLMs and three healthcare-adapted LLMs\nto improve BHC synthesis from clinical notes. Using clinical notes as input for\ngenerating BHCs, we apply prompting-based (using in-context learning) and\nfine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We quantitatively evaluate the performance of these LLMs across varying\ncontext-length inputs using conventional natural language similarity metrics.\nWe further perform a qualitative study where five diverse clinicians blindly\ncompare clinician-written BHCs and two LLM-generated BHCs for 30 samples across\nmetrics of comprehensiveness, conciseness, factual correctness, and fluency.\nOverall, we present a new benchmark and pre-processed dataset for using LLMs in\nBHC synthesis from clinical notes. We observe high-quality summarization\nperformance for both in-context proprietary and fine-tuned open-source LLMs\nusing both quantitative metrics and a qualitative clinical reader study. We\npropose our work as a benchmark to motivate future works to adapt and assess\nthe performance of LLMs in BHC synthesis.",
        "pdf_link": "https://arxiv.org/pdf/2403.05720v1.pdf"
    },
    {
        "title": "EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings",
        "authors": [
            "Sunjun Kweon",
            "Jiyoun Kim",
            "Heeyoung Kwak",
            "Dongchul Cha",
            "Hangyul Yoon",
            "Kwanghyun Kim",
            "Seunghyun Won",
            "Edward Choi"
        ],
        "published": "2024-02-25T09:41:50Z",
        "summary": "This study introduces EHRNoteQA, a novel patient-specific question answering\nbenchmark tailored for evaluating Large Language Models (LLMs) in clinical\nenvironments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three\nmedical professionals has curated the dataset comprising 962 unique questions,\neach linked to a specific patient's EHR clinical notes. What makes EHRNoteQA\ndistinct from existing EHR-based benchmarks is as follows: Firstly, it is the\nfirst dataset to adopt a multi-choice question answering format, a design\nchoice that effectively evaluates LLMs with reliable scores in the context of\nautomatic evaluation, compared to other formats. Secondly, it requires an\nanalysis of multiple clinical notes to answer a single question, reflecting the\ncomplex nature of real-world clinical decision-making where clinicians review\nextensive records of patient histories. Our comprehensive evaluation on various\nlarge language models showed that their scores on EHRNoteQA correlate more\nclosely with their performance in addressing real-world medical questions\nevaluated by clinicians than their scores from other LLM benchmarks. This\nunderscores the significance of EHRNoteQA in evaluating LLMs for medical\napplications and highlights its crucial role in facilitating the integration of\nLLMs into healthcare systems. The dataset will be made available to the public\nunder PhysioNet credential access, promoting further research in this vital\nfield.",
        "pdf_link": "https://arxiv.org/pdf/2402.16040v2.pdf"
    },
    {
        "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
        "authors": [
            "Weihang Su",
            "Changyue Wang",
            "Qingyao Ai",
            "Yiran HU",
            "Zhijing Wu",
            "Yujia Zhou",
            "Yiqun Liu"
        ],
        "published": "2024-03-11T05:51:03Z",
        "summary": "Hallucinations in large language models (LLMs) refer to the phenomenon of\nLLMs producing responses that are coherent yet factually inaccurate. This issue\nundermines the effectiveness of LLMs in practical applications, necessitating\nresearch into detecting and mitigating hallucinations of LLMs. Previous studies\nhave mainly concentrated on post-processing techniques for hallucination\ndetection, which tend to be computationally intensive and limited in\neffectiveness due to their separation from the LLM's inference process. To\novercome these limitations, we introduce MIND, an unsupervised training\nframework that leverages the internal states of LLMs for real-time\nhallucination detection without requiring manual annotations. Additionally, we\npresent HELM, a new benchmark for evaluating hallucination detection across\nmultiple LLMs, featuring diverse LLM outputs and the internal states of LLMs\nduring their inference process. Our experiments demonstrate that MIND\noutperforms existing state-of-the-art methods in hallucination detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.06448v1.pdf"
    },
    {
        "title": "DiffChat: Learning to Chat with Text-to-Image Synthesis Models for Interactive Image Creation",
        "authors": [
            "Jiapeng Wang",
            "Chengyu Wang",
            "Tingfeng Cao",
            "Jun Huang",
            "Lianwen Jin"
        ],
        "published": "2024-03-08T02:24:27Z",
        "summary": "We present DiffChat, a novel method to align Large Language Models (LLMs) to\n\"chat\" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable\nDiffusion) for interactive image creation. Given a raw prompt/image and a\nuser-specified instruction, DiffChat can effectively make appropriate\nmodifications and generate the target prompt, which can be leveraged to create\nthe target image of high quality. To achieve this, we first collect an\ninstruction-following prompt engineering dataset named InstructPE for the\nsupervised training of DiffChat. Next, we propose a reinforcement learning\nframework with the feedback of three core criteria for image creation, i.e.,\naesthetics, user preference, and content integrity. It involves an action-space\ndynamic modification technique to obtain more relevant positive samples and\nharder negative samples during the off-policy sampling. Content integrity is\nalso introduced into the value estimation function for further improvement of\nproduced images. Our method can exhibit superior performance than baseline\nmodels and strong competitors based on both automatic and human evaluations,\nwhich fully demonstrates its effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.04997v1.pdf"
    },
    {
        "title": "Do Large Language Models understand Medical Codes?",
        "authors": [
            "Simon A. Lee",
            "Timothy Lindsey"
        ],
        "published": "2024-03-16T06:18:15Z",
        "summary": "The overarching goal of recent AI research has been to make steady progress\ntowards achieving Artificial General Intelligence (AGI), prompting the\nevaluation of Large Language Models (LLMs) across a variety of tasks and\ndomains. One such domain is healthcare, where LLMs can greatly benefit clinical\npractice by assisting with a wide range of tasks. However, these models are\nalso prone to producing ``hallucinations\" or incorrect responses when faced\nwith queries they cannot adequately address, raising concerns and skepticism,\nespecially within the healthcare community. In this work, we investigate\nwhether LLMs understand and can predict medical codes, which are extensively\nutilized in healthcare practice. This study aims to delineate the capabilities\nand limitations of these LLMs. We evaluate various off-the-shelf LLMs (e.g.,\nGPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to\nassess their awareness and understanding of these domain-specific\nterminologies. Our results indicate that these models as they currently stand\ndo not comprehend the meaning of the medical codes, highlighting the need for\nbetter representation of these alphanumeric codes extensively used in\nhealthcare. We call for improved strategies to effectively capture and\nrepresent the nuances of medical codes and terminologies within LLMs, enabling\nthem to become more reliable and trustworthy tools for healthcare\nprofessionals.",
        "pdf_link": "https://arxiv.org/pdf/2403.10822v2.pdf"
    },
    {
        "title": "Interactions with Prompt Problems: A New Way to Teach Programming with Large Language Models",
        "authors": [
            "James Prather",
            "Paul Denny",
            "Juho Leinonen",
            "David H. Smith IV",
            "Brent N. Reeves",
            "Stephen MacNeil",
            "Brett A. Becker",
            "Andrew Luxton-Reilly",
            "Thezyrie Amarouche",
            "Bailey Kimmel"
        ],
        "published": "2024-01-19T15:32:46Z",
        "summary": "Large Language Models (LLMs) have upended decades of pedagogy in computing\neducation. Students previously learned to code through \\textit{writing} many\nsmall problems with less emphasis on code reading and comprehension. Recent\nresearch has shown that free code generation tools powered by LLMs can solve\nintroductory programming problems presented in natural language with ease. In\nthis paper, we propose a new way to teach programming with Prompt Problems.\nStudents receive a problem visually, indicating how input should be transformed\nto output, and must translate that to a prompt for an LLM to decipher. The\nproblem is considered correct when the code that is generated by the student\nprompt can pass all test cases. In this paper we present the design of this\ntool, discuss student interactions with it as they learn, and provide insights\ninto this new class of programming problems as well as the design tools that\nintegrate LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.10759v1.pdf"
    },
    {
        "title": "HawkEye: Training Video-Text LLMs for Grounding Text in Videos",
        "authors": [
            "Yueqian Wang",
            "Xiaojun Meng",
            "Jianxin Liang",
            "Yuxuan Wang",
            "Qun Liu",
            "Dongyan Zhao"
        ],
        "published": "2024-03-15T11:58:18Z",
        "summary": "Video-text Large Language Models (video-text LLMs) have shown remarkable\nperformance in answering questions and holding conversations on simple videos.\nHowever, they perform almost the same as random on grounding text queries in\nlong and complicated videos, having little ability to understand and reason\nabout temporal information, which is the most fundamental difference between\nvideos and images. In this paper, we propose HawkEye, one of the first\nvideo-text LLMs that can perform temporal video grounding in a fully\ntext-to-text manner. To collect training data that is applicable for temporal\nvideo grounding, we construct InternVid-G, a large-scale video-text corpus with\nsegment-level captions and negative spans, with which we introduce two new\ntime-aware training objectives to video-text LLMs. We also propose a\ncoarse-grained method of representing segments in videos, which is more robust\nand easier for LLMs to learn and follow than other alternatives. Extensive\nexperiments show that HawkEye is better at temporal video grounding and\ncomparable on other video-text tasks with existing video-text LLMs, which\nverifies its superior video-text multi-modal understanding abilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.10228v1.pdf"
    },
    {
        "title": "Exploring Prompt Engineering Practices in the Enterprise",
        "authors": [
            "Michael Desmond",
            "Michelle Brachman"
        ],
        "published": "2024-03-13T20:32:32Z",
        "summary": "Interaction with Large Language Models (LLMs) is primarily carried out via\nprompting. A prompt is a natural language instruction designed to elicit\ncertain behaviour or output from a model. In theory, natural language prompts\nenable non-experts to interact with and leverage LLMs. However, for complex\ntasks and tasks with specific requirements, prompt design is not trivial.\nCreating effective prompts requires skill and knowledge, as well as significant\niteration in order to determine model behavior, and guide the model to\naccomplish a particular goal. We hypothesize that the way in which users\niterate on their prompts can provide insight into how they think prompting and\nmodels work, as well as the kinds of support needed for more efficient prompt\nengineering. To better understand prompt engineering practices, we analyzed\nsessions of prompt editing behavior, categorizing the parts of prompts users\niterated on and the types of changes they made. We discuss design implications\nand future directions based on these prompt engineering practices.",
        "pdf_link": "https://arxiv.org/pdf/2403.08950v1.pdf"
    },
    {
        "title": "Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models",
        "authors": [
            "Sijia Chen",
            "Baochun Li",
            "Di Niu"
        ],
        "published": "2024-02-17T00:13:36Z",
        "summary": "The reasoning performance of Large Language Models (LLMs) on a wide range of\nproblems critically relies on chain-of-thought prompting, which involves\nproviding a few chain of thought demonstrations as exemplars in prompts. Recent\nwork, e.g., Tree of Thoughts, has pointed out the importance of exploration and\nself-evaluation in reasoning step selection for complex problem solving. In\nthis paper, we present Boosting of Thoughts (BoT), an automated prompting\nframework for problem solving with LLMs by iteratively exploring and\nself-evaluating many trees of thoughts in order to acquire an ensemble of\ntrial-and-error reasoning experiences, which will serve as a new form of\nprompting to solve the complex problem. Starting from a simple prompt without\nrequiring examples, BoT iteratively explores and evaluates a large collection\nof reasoning steps, and more importantly, uses error analysis obtained from the\nLLM on them to explicitly revise prompting, which in turn enhances reasoning\nstep generation, until a final answer is attained. Our experiments with GPT-4\nand Llama2 across extensive complex mathematical problems demonstrate that BoT\nconsistently achieves higher or comparable problem-solving rates than other\nadvanced prompting approaches.",
        "pdf_link": "https://arxiv.org/pdf/2402.11140v1.pdf"
    },
    {
        "title": "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?",
        "authors": [
            "Ryan Liu",
            "Theodore R. Sumers",
            "Ishita Dasgupta",
            "Thomas L. Griffiths"
        ],
        "published": "2024-02-11T19:13:26Z",
        "summary": "In day-to-day communication, people often approximate the truth - for\nexample, rounding the time or omitting details - in order to be maximally\nhelpful to the listener. How do large language models (LLMs) handle such\nnuanced trade-offs? To address this question, we use psychological models and\nexperiments designed to characterize human behavior to analyze LLMs. We test a\nrange of LLMs and explore how optimization for human preferences or\ninference-time reasoning affects these trade-offs. We find that reinforcement\nlearning from human feedback improves both honesty and helpfulness, while\nchain-of-thought prompting skews LLMs towards helpfulness over honesty.\nFinally, GPT-4 Turbo demonstrates human-like response patterns including\nsensitivity to the conversational framing and listener's decision context. Our\nfindings reveal the conversational values internalized by LLMs and suggest that\neven these abstract values can, to a degree, be steered by zero-shot prompting.",
        "pdf_link": "https://arxiv.org/pdf/2402.07282v2.pdf"
    },
    {
        "title": "Prompt Perturbation Consistency Learning for Robust Language Models",
        "authors": [
            "Yao Qiang",
            "Subhrangshu Nandi",
            "Ninareh Mehrabi",
            "Greg Ver Steeg",
            "Anoop Kumar",
            "Anna Rumshisky",
            "Aram Galstyan"
        ],
        "published": "2024-02-24T15:00:58Z",
        "summary": "Large language models (LLMs) have demonstrated impressive performance on a\nnumber of natural language processing tasks, such as question answering and\ntext summarization. However, their performance on sequence labeling tasks such\nas intent classification and slot filling (IC-SF), which is a central component\nin personal assistant systems, lags significantly behind discriminative models.\nFurthermore, there is a lack of substantive research on the robustness of LLMs\nto various perturbations in the input prompts. The contributions of this paper\nare three-fold. First, we show that fine-tuning sufficiently large LLMs can\nproduce IC-SF performance comparable to discriminative models. Next, we\nsystematically analyze the performance deterioration of those fine-tuned models\ndue to three distinct yet relevant types of input perturbations - oronyms,\nsynonyms, and paraphrasing. Finally, we propose an efficient mitigation\napproach, Prompt Perturbation Consistency Learning (PPCL), which works by\nregularizing the divergence between losses from clean and perturbed samples.\nOur experiments demonstrate that PPCL can recover on average 59% and 69% of the\nperformance drop for IC and SF tasks, respectively. Furthermore, PPCL beats the\ndata augmentation approach while using ten times fewer augmented data samples.",
        "pdf_link": "https://arxiv.org/pdf/2402.15833v1.pdf"
    },
    {
        "title": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer",
        "authors": [
            "Yanjun Zhao",
            "Sizhe Dang",
            "Haishan Ye",
            "Guang Dai",
            "Yi Qian",
            "Ivor W. Tsang"
        ],
        "published": "2024-02-23T08:11:55Z",
        "summary": "Fine-tuning large language models (LLMs) with classic first-order optimizers\nentails prohibitive GPU memory due to the backpropagation process. Recent works\nhave turned to zeroth-order optimizers for fine-tuning, which save substantial\nmemory by using two forward passes. However, these optimizers are plagued by\nthe heterogeneity of parameter curvatures across different dimensions. In this\nwork, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer\nwhich is the first work to leverage the diagonal Hessian to enhance\nzeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the\nexpensive memory cost and only increases one forward pass per step. Extensive\nexperiments on various models (350M~66B parameters) indicate that HiZOO\nimproves model convergence, significantly reducing training steps and\neffectively enhancing model accuracy. Moreover, we visualize the optimization\ntrajectories of HiZOO on test functions, illustrating its effectiveness in\nhandling heterogeneous curvatures. Lastly, we provide theoretical proofs of\nconvergence for HiZOO. Code is publicly available at\nhttps://anonymous.4open.science/r/HiZOO27F8.",
        "pdf_link": "https://arxiv.org/pdf/2402.15173v1.pdf"
    },
    {
        "title": "Pearl: A Review-driven Persona-Knowledge Grounded Conversational Recommendation Dataset",
        "authors": [
            "Minjin Kim",
            "Minju Kim",
            "Hana Kim",
            "Beong-woo Kwak",
            "Soyeon Chun",
            "Hyunseo Kim",
            "SeongKu Kang",
            "Youngjae Yu",
            "Jinyoung Yeo",
            "Dongha Lee"
        ],
        "published": "2024-03-07T12:57:16Z",
        "summary": "Conversational recommender system is an emerging area that has garnered an\nincreasing interest in the community, especially with the advancements in large\nlanguage models (LLMs) that enable diverse reasoning over conversational input.\nDespite the progress, the field has many aspects left to explore. The currently\navailable public datasets for conversational recommendation lack specific user\npreferences and explanations for recommendations, hindering high-quality\nrecommendations. To address such challenges, we present a novel conversational\nrecommendation dataset named PEARL, synthesized with persona- and\nknowledge-augmented LLM simulators. We obtain detailed persona and knowledge\nfrom real-world reviews and construct a large-scale dataset with over 57k\ndialogues. Our experimental results demonstrate that utterances in PEARL\ninclude more specific user preferences, show expertise in the target domain,\nand provide recommendations more relevant to the dialogue context than those in\nprior datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.04460v3.pdf"
    },
    {
        "title": "Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for Pretraining on the Cybersecurity Domain",
        "authors": [
            "Eugene Jang",
            "Jian Cui",
            "Dayeon Yim",
            "Youngjin Jin",
            "Jin-Woo Chung",
            "Seungwon Shin",
            "Yongjae Lee"
        ],
        "published": "2024-03-15T05:35:02Z",
        "summary": "Cybersecurity information is often technically complex and relayed through\nunstructured text, making automation of cyber threat intelligence highly\nchallenging. For such text domains that involve high levels of expertise,\npretraining on in-domain corpora has been a popular method for language models\nto obtain domain expertise. However, cybersecurity texts often contain\nnon-linguistic elements (such as URLs and hash values) that could be unsuitable\nwith the established pretraining methodologies. Previous work in other domains\nhave removed or filtered such text as noise, but the effectiveness of these\nmethods have not been investigated, especially in the cybersecurity domain. We\npropose different pretraining methodologies and evaluate their effectiveness\nthrough downstream tasks and probing tasks. Our proposed strategy (selective\nMLM and jointly training NLE token classification) outperforms the commonly\ntaken approach of replacing non-linguistic elements (NLEs). We use our\ndomain-customized methodology to train CyBERTuned, a cybersecurity domain\nlanguage model that outperforms other cybersecurity PLMs on most tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.10576v2.pdf"
    },
    {
        "title": "Can LLMs Compute with Reasons?",
        "authors": [
            "Harshit Sandilya",
            "Peehu Raj",
            "Jainit Sushil Bafna",
            "Srija Mukhopadhyay",
            "Shivansh Sharma",
            "Ellwil Sharma",
            "Arastu Sharma",
            "Neeta Trivedi",
            "Manish Shrivastava",
            "Rajesh Kumar"
        ],
        "published": "2024-02-19T12:04:25Z",
        "summary": "Large language models (LLMs) often struggle with complex mathematical tasks,\nprone to \"hallucinating\" incorrect answers due to their reliance on statistical\npatterns. This limitation is further amplified in average Small LangSLMs with\nlimited context and training data. To address this challenge, we propose an\n\"Inductive Learning\" approach utilizing a distributed network of SLMs. This\nnetwork leverages error-based learning and hint incorporation to refine the\nreasoning capabilities of SLMs. Our goal is to provide a framework that\nempowers SLMs to approach the level of logic-based applications achieved by\nhigh-parameter models, potentially benefiting any language model. Ultimately,\nthis novel concept paves the way for bridging the logical gap between humans\nand LLMs across various fields.",
        "pdf_link": "https://arxiv.org/pdf/2402.12080v1.pdf"
    },
    {
        "title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs",
        "authors": [
            "Arash Ahmadian",
            "Chris Cremer",
            "Matthias Gall\u00e9",
            "Marzieh Fadaee",
            "Julia Kreutzer",
            "Olivier Pietquin",
            "Ahmet \u00dcst\u00fcn",
            "Sara Hooker"
        ],
        "published": "2024-02-22T17:52:34Z",
        "summary": "AI alignment in the shape of Reinforcement Learning from Human Feedback\n(RLHF) is increasingly treated as a crucial ingredient for high performance\nlarge language models. Proximal Policy Optimization (PPO) has been positioned\nby recent literature as the canonical method for the RL part of RLHF. However,\nit involves both high computational cost and sensitive hyperparameter tuning.\nWe posit that most of the motivational principles that led to the development\nof PPO are less of a practical concern in RLHF and advocate for a less\ncomputationally expensive method that preserves and even increases performance.\nWe revisit the formulation of alignment from human preferences in the context\nof RL. Keeping simplicity as a guiding principle, we show that many components\nof PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style\noptimization variants outperform both PPO and newly proposed \"RL-free\" methods\nsuch as DPO and RAFT. Our work suggests that careful adaptation to LLMs\nalignment characteristics enables benefiting from online RL optimization at low\ncost.",
        "pdf_link": "https://arxiv.org/pdf/2402.14740v2.pdf"
    },
    {
        "title": "Measuring Taiwanese Mandarin Language Understanding",
        "authors": [
            "Po-Heng Chen",
            "Sijia Cheng",
            "Wei-Lin Chen",
            "Yen-Ting Lin",
            "Yun-Nung Chen"
        ],
        "published": "2024-03-29T13:56:21Z",
        "summary": "The evaluation of large language models (LLMs) has drawn substantial\nattention in the field recently. This work focuses on evaluating LLMs in a\nChinese context, specifically, for Traditional Chinese which has been largely\nunderrepresented in existing benchmarks. We present TMLU, a holistic evaluation\nsuit tailored for assessing the advanced knowledge and reasoning capability in\nLLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37\nsubjects across social science, STEM, humanities, Taiwan-specific content, and\nothers, ranging from middle school to professional levels. In addition, we\ncurate chain-of-thought-like few-shot explanations for each subject to\nfacilitate the evaluation of complex reasoning skills. To establish a\ncomprehensive baseline, we conduct extensive experiments and analysis on 24\nadvanced LLMs. The results suggest that Chinese open-weight models demonstrate\ninferior performance comparing to multilingual proprietary ones, and\nopen-weight models tailored for Taiwanese Mandarin lag behind the\nSimplified-Chinese counterparts. The findings indicate great headrooms for\nimprovement, and emphasize the goal of TMLU to foster the development of\nlocalized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation\nscripts for the community to promote future research.",
        "pdf_link": "https://arxiv.org/pdf/2403.20180v1.pdf"
    },
    {
        "title": "Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark",
        "authors": [
            "Baolong Bi",
            "Shenghua Liu",
            "Yiwei Wang",
            "Lingrui Mei",
            "Xueqi Cheng"
        ],
        "published": "2024-03-30T02:08:28Z",
        "summary": "The rapid development of large language models (LLMs) enables them to convey\nfactual knowledge in a more human-like fashion. Extensive efforts have been\nmade to reduce factual hallucinations by modifying LLMs with factuality\ndecoding. However, they also pose risks of hindering knowledge updates, as they\nmake models overly confident in known facts. In this work, we first revisite\nthe current factuality decoding methods and verified their effectiveness in\nenhancing factual accuracy. Subsequently, we conduct further evaluation of\nseveral strong factuality decoding methods on the knowledge editing benchmark.\nAll these decoding methods significantly diminish the performance of llama2\nmodels compared to their original decoding, with the largest decrease being a\nstaggering 81.3\\%. This further indicates that the current existing decoding\nmethods still cannot perfectly address the factual hallucinations, as they\noverlook the importance of preserving the flexibility for knowledge editing.\nTherefore, our work suggests that research into factual alignment should\nsimultaneously focus on the effectiveness of knowledge editing.",
        "pdf_link": "https://arxiv.org/pdf/2404.00216v1.pdf"
    },
    {
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": [
            "Huizhuo Yuan",
            "Zixiang Chen",
            "Kaixuan Ji",
            "Quanquan Gu"
        ],
        "published": "2024-02-15T18:59:18Z",
        "summary": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "pdf_link": "https://arxiv.org/pdf/2402.10210v1.pdf"
    },
    {
        "title": "Employing Label Models on ChatGPT Answers Improves Legal Text Entailment Performance",
        "authors": [
            "Chau Nguyen",
            "Le-Minh Nguyen"
        ],
        "published": "2024-01-31T15:04:01Z",
        "summary": "The objective of legal text entailment is to ascertain whether the assertions\nin a legal query logically follow from the information provided in one or\nmultiple legal articles. ChatGPT, a large language model, is robust in many\nnatural language processing tasks, including legal text entailment: when we set\nthe temperature = 0 (the ChatGPT answers are deterministic) and prompt the\nmodel, it achieves 70.64% accuracy on COLIEE 2022 dataset, which outperforms\nthe previous SOTA of 67.89%. On the other hand, if the temperature is larger\nthan zero, ChatGPT answers are not deterministic, leading to inconsistent\nanswers and fluctuating results. We propose to leverage label models (a\nfundamental component of weak supervision techniques) to integrate the\nprovisional answers by ChatGPT into consolidated labels. By that way, we treat\nChatGPT provisional answers as noisy predictions which can be consolidated by\nlabel models. The experimental results demonstrate that this approach can\nattain an accuracy of 76.15%, marking a significant improvement of 8.26% over\nthe prior state-of-the-art benchmark. Additionally, we perform an analysis of\nthe instances where ChatGPT produces incorrect answers, then we classify the\nerrors, offering insights that could guide potential enhancements for future\nresearch endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2401.17897v1.pdf"
    },
    {
        "title": "Chain-of-Thought Unfaithfulness as Disguised Accuracy",
        "authors": [
            "Oliver Bentham",
            "Nathan Stringham",
            "Ana Marasovi\u0107"
        ],
        "published": "2024-02-22T17:23:53Z",
        "summary": "Understanding the extent to which Chain-of-Thought (CoT) generations align\nwith a large language model's (LLM) internal computations is critical for\ndeciding whether to trust an LLM's output. As a proxy for CoT faithfulness,\narXiv:2307.13702 propose a metric that measures a model's dependence on its CoT\nfor producing an answer. Within a single family of proprietary models, they\nfind that LLMs exhibit a scaling-then-inverse-scaling relationship between\nmodel size and their measure of faithfulness, and that a 13 billion parameter\nmodel exhibits increased faithfulness compared to models ranging from 810\nmillion to 175 billion parameters in size. We evaluate whether these results\ngeneralize as a property of all LLMs. We replicate their experimental setup\nwith three different families of models and, under specific conditions,\nsuccessfully reproduce the scaling trends for CoT faithfulness they report.\nHowever, we discover that simply changing the order of answer choices in the\nprompt can reduce the metric by 73 percentage points. The faithfulness metric\nis also highly correlated ($R^2$ = 0.91) with accuracy, raising doubts about\nits validity as a construct for evaluating faithfulness.",
        "pdf_link": "https://arxiv.org/pdf/2402.14897v1.pdf"
    },
    {
        "title": "AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media",
        "authors": [
            "Alessandro Gambetti",
            "Qiwei Han"
        ],
        "published": "2024-01-16T20:57:36Z",
        "summary": "Online reviews in the form of user-generated content (UGC) significantly\nimpact consumer decision-making. However, the pervasive issue of not only human\nfake content but also machine-generated content challenges UGC's reliability.\nRecent advances in Large Language Models (LLMs) may pave the way to fabricate\nindistinguishable fake generated content at a much lower cost. Leveraging\nOpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a\nmulti-modal dataset of 20,144 restaurant review-image pairs divided into\nauthentic and machine-generated. We explore unimodal and multimodal detection\nmodels, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from\nreadability and photographic theories to score reviews and images,\nrespectively, demonstrating their utility as hand-crafted features in scalable\nand interpretable detection models, with comparable performance. The paper\ncontributes by open-sourcing the dataset and releasing fake review detectors,\nrecommending its use in unimodal and multimodal fake review detection tasks,\nand evaluating linguistic and visual features in synthetic versus authentic\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2401.08825v1.pdf"
    },
    {
        "title": "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs",
        "authors": [
            "Nicolas Boizard",
            "Kevin El Haddad",
            "C\u00e9line Hudelot",
            "Pierre Colombo"
        ],
        "published": "2024-02-19T10:37:29Z",
        "summary": "Deploying large language models (LLMs) of several billion parameters can be\nimpractical in most industrial use cases due to constraints such as cost,\nlatency limitations, and hardware accessibility. Knowledge distillation (KD)\noffers a solution by compressing knowledge from resource-intensive large models\nto smaller ones. Various strategies exist, some relying on the text generated\nby the teacher model and optionally utilizing his logits to enhance learning.\nHowever, these methods based on logits often require both teacher and student\nmodels to share the same tokenizer, limiting their applicability across\ndifferent LLM families. In this paper, we introduce Universal Logit\nDistillation (ULD) loss, grounded in optimal transport, to address this\nlimitation. Our experimental results demonstrate the effectiveness of ULD loss\nin enabling distillation across models with different architectures and\ntokenizers, paving the way to a more widespread use of distillation techniques.",
        "pdf_link": "https://arxiv.org/pdf/2402.12030v2.pdf"
    },
    {
        "title": "The Fine-Grained Complexity of Gradient Computation for Training Large Language Models",
        "authors": [
            "Josh Alman",
            "Zhao Song"
        ],
        "published": "2024-02-07T00:45:31Z",
        "summary": "Large language models (LLMs) have made fundamental contributions over the\nlast a few years. To train an LLM, one needs to alternatingly run `forward'\ncomputations and `backward' computations. The forward computation can be viewed\nas attention function evaluation, and the backward computation can be viewed as\na gradient computation. In previous work by [Alman and Song, NeurIPS 2023], it\nwas proved that the forward step can be performed in almost-linear time in\ncertain parameter regimes, but that there is no truly sub-quadratic time\nalgorithm in the remaining parameter regimes unless the popular hypothesis SETH\nis false. In this work, we show nearly identical results for the harder-seeming\nproblem of computing the gradient of loss function of one layer attention\nnetwork, and thus for the entire process of LLM training. This completely\ncharacterizes the fine-grained complexity of every step of LLM training.",
        "pdf_link": "https://arxiv.org/pdf/2402.04497v1.pdf"
    },
    {
        "title": "Vanilla Transformers are Transfer Capability Teachers",
        "authors": [
            "Xin Lu",
            "Yanyan Zhao",
            "Bing Qin"
        ],
        "published": "2024-03-04T12:40:28Z",
        "summary": "Recently, Mixture of Experts (MoE) Transformers have garnered increasing\nattention due to their advantages in model capacity and computational\nefficiency. However, studies have indicated that MoE Transformers underperform\nvanilla Transformers in many downstream tasks, significantly diminishing the\npractical value of MoE models. To explain this issue, we propose that the\npre-training performance and transfer capability of a model are joint\ndeterminants of its downstream task performance. MoE models, in comparison to\nvanilla models, have poorer transfer capability, leading to their subpar\nperformance in downstream tasks. To address this issue, we introduce the\nconcept of transfer capability distillation, positing that although vanilla\nmodels have weaker performance, they are effective teachers of transfer\ncapability. The MoE models guided by vanilla models can achieve both strong\npre-training performance and transfer capability, ultimately enhancing their\nperformance in downstream tasks. We design a specific distillation method and\nconduct experiments on the BERT architecture. Experimental results show a\nsignificant improvement in downstream performance of MoE models, and many\nfurther evidences also strongly support the concept of transfer capability\ndistillation. Finally, we attempt to interpret transfer capability distillation\nand provide some insights from the perspective of model feature.",
        "pdf_link": "https://arxiv.org/pdf/2403.01994v1.pdf"
    },
    {
        "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
        "authors": [
            "Subhabrata Dutta",
            "Joykirat Singh",
            "Soumen Chakrabarti",
            "Tanmoy Chakraborty"
        ],
        "published": "2024-02-28T13:14:20Z",
        "summary": "Despite superior reasoning prowess demonstrated by Large Language Models\n(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails\naround the internal mechanisms of the models that facilitate CoT generation.\nThis work investigates the neural sub-structures within LLMs that manifest CoT\nreasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B\napplied to multistep reasoning over fictional ontologies, we demonstrate that\nLLMs deploy multiple parallel pathways of answer generation for step-by-step\nreasoning. These parallel pathways provide sequential answers from the input\nquestion context as well as the generated CoT. We observe a striking functional\nrift in the middle layers of the LLM. Token representations in the initial half\nremain strongly biased towards the pretraining prior, with the in-context\ntaking over abruptly in the later half. This internal phase shift manifests in\ndifferent functional components: attention heads that write the answer token\npredominantly appear in the later half, attention heads that move information\nalong ontological relationships appear exclusively in the initial half, and so\non. To the best of our knowledge, this is the first attempt towards mechanistic\ninvestigation of CoT reasoning in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.18312v1.pdf"
    },
    {
        "title": "Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4",
        "authors": [
            "Qingqing Zhu",
            "Benjamin Hou",
            "Tejas S. Mathai",
            "Pritam Mukherjee",
            "Qiao Jin",
            "Xiuying Chen",
            "Zhizheng Wang",
            "Ruida Cheng",
            "Ronald M. Summers",
            "Zhiyong Lu"
        ],
        "published": "2024-03-08T21:16:28Z",
        "summary": "The volume of CT exams being done in the world has been rising every year,\nwhich has led to radiologist burn-out. Large Language Models (LLMs) have the\npotential to reduce their burden, but their adoption in the clinic depends on\nradiologist trust, and easy evaluation of generated content. Presently, many\nautomated methods are available to evaluate the reports generated for chest\nradiographs, but such an approach is not available for CT presently. In this\npaper, we propose a novel evaluation framework to judge the capabilities of\nvision-language LLMs in generating accurate summaries of CT-based\nabnormalities. CT slices containing an abnormality (e.g., lesion) were input to\na vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text\nsummary of the predicted characteristics of the abnormality. Next, a GPT-4\nmodel decomposed the summary into specific aspects (body part, location, type,\nand attributes), automatically evaluated the characteristics against the\nground-truth, and generated a score for each aspect based on its clinical\nrelevance and factual accuracy. These scores were then contrasted against those\nobtained from a clinician, and a high correlation ( 85%, p < .001) was\nobserved. Although GPT-4V outperformed other models in our evaluation, it still\nrequires overall improvement. Our evaluation method offers valuable insights\ninto the specific areas that need the most enhancement, guiding future\ndevelopment in this field.",
        "pdf_link": "https://arxiv.org/pdf/2403.05680v1.pdf"
    },
    {
        "title": "Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs Collaborated Reasoning",
        "authors": [
            "Hang Zhang",
            "Wenxiao Zhang",
            "Haoxuan Qu",
            "Jun Liu"
        ],
        "published": "2024-03-15T08:51:15Z",
        "summary": "Human-centered dynamic scene understanding plays a pivotal role in enhancing\nthe capability of robotic and autonomous systems, in which Video-based\nHuman-Object Interaction (V-HOI) detection is a crucial task in semantic scene\nunderstanding, aimed at comprehensively understanding HOI relationships within\na video to benefit the behavioral decisions of mobile robots and autonomous\ndriving systems. Although previous V-HOI detection models have made significant\nstrides in accurate detection on specific datasets, they still lack the general\nreasoning ability like human beings to effectively induce HOI relationships. In\nthis study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a\nnovel framework consisting of a series of plug-and-play modules that could\nfacilitate the performance of current V-HOI detection models by leveraging the\nstrong reasoning ability of different off-the-shelf pre-trained large language\nmodels (LLMs). We design a two-stage collaboration system of different LLMs for\nthe V-HOI task. Specifically, in the first stage, we design a Cross-Agents\nReasoning scheme to leverage the LLM conduct reasoning from different aspects.\nIn the second stage, we perform Multi-LLMs Debate to get the final reasoning\nanswer based on the different knowledge in different LLMs. Additionally, we\ndevise an auxiliary training strategy that utilizes CLIP, a large\nvision-language model to enhance the base V-HOI models' discriminative ability\nto better cooperate with LLMs. We validate the superiority of our design by\ndemonstrating its effectiveness in improving the prediction accuracy of the\nbase V-HOI model via reasoning from multiple perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2403.10107v1.pdf"
    },
    {
        "title": "Can GNN be Good Adapter for LLMs?",
        "authors": [
            "Xuanwen Huang",
            "Kaiqiao Han",
            "Yang Yang",
            "Dezheng Bao",
            "Quanjin Tao",
            "Ziwei Chai",
            "Qi Zhu"
        ],
        "published": "2024-02-20T13:13:13Z",
        "summary": "Recently, large language models (LLMs) have demonstrated superior\ncapabilities in understanding and zero-shot learning on textual data, promising\nsignificant advances for many text-related domains. In the graph domain,\nvarious real-world scenarios also involve textual data, where tasks and node\nfeatures can be described by text. These text-attributed graphs (TAGs) have\nbroad applications in social media, recommendation systems, etc. Thus, this\npaper explores how to utilize LLMs to model TAGs. Previous methods for TAG\nmodeling are based on million-scale LMs. When scaled up to billion-scale LLMs,\nthey face huge challenges in computational costs. Additionally, they also\nignore the zero-shot inference capabilities of LLMs. Therefore, we propose\nGraphAdapter, which uses a graph neural network (GNN) as an efficient adapter\nin collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN\nadapter introduces only a few trainable parameters and can be trained with low\ncomputation costs. The entire framework is trained using auto-regression on\nnode text (next token prediction). Once trained, GraphAdapter can be seamlessly\nfine-tuned with task-specific prompts for various downstream tasks. Through\nextensive experiments across multiple real-world TAGs, GraphAdapter based on\nLlama 2 gains an average improvement of approximately 5\\% in terms of node\nclassification. Furthermore, GraphAdapter can also adapt to other language\nmodels, including RoBERTa, GPT-2. The promising results demonstrate that GNNs\ncan serve as effective adapters for LLMs in TAG modeling.",
        "pdf_link": "https://arxiv.org/pdf/2402.12984v1.pdf"
    },
    {
        "title": "MULTI: Multimodal Understanding Leaderboard with Text and Images",
        "authors": [
            "Zichen Zhu",
            "Yang Xu",
            "Lu Chen",
            "Jingkai Yang",
            "Yichuan Ma",
            "Yiming Sun",
            "Hailin Wen",
            "Jiaqi Liu",
            "Jinyu Cai",
            "Yingzi Ma",
            "Situo Zhang",
            "Zihan Zhao",
            "Liangtai Sun",
            "Kai Yu"
        ],
        "published": "2024-02-05T16:41:02Z",
        "summary": "Rapid progress in multimodal large language models (MLLMs) highlights the\nneed to introduce challenging yet realistic benchmarks to the academic\ncommunity, while existing benchmarks primarily focus on understanding simple\nnatural images and short context. In this paper, we present MULTI as a\ncutting-edge benchmark for evaluating MLLMs on understanding complex tables and\nimages, and reasoning with long context. MULTI provides multimodal inputs and\nrequires responses that are either precise or open-ended, reflecting real-life\nexamination styles. MULTI includes over 18,000 questions and challenges MLLMs\nwith a variety of tasks, ranging from formula derivation to image detail\nanalysis and cross-modality reasoning. We also introduce MULTI-Elite, a\n500-question selected hard subset, and MULTI-Extend, with more than 4,500\nexternal knowledge context pieces. Our evaluation indicates significant\npotential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on\nMULTI, in contrast to other MLLMs scoring between 28.5% and 55.3%. MULTI serves\nnot only as a robust evaluation platform but also paves the way for the\ndevelopment of expert-level AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.03173v2.pdf"
    },
    {
        "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
        "authors": [
            "Qian Yang",
            "Jin Xu",
            "Wenrui Liu",
            "Yunfei Chu",
            "Ziyue Jiang",
            "Xiaohuan Zhou",
            "Yichong Leng",
            "Yuanjun Lv",
            "Zhou Zhao",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "published": "2024-02-12T15:41:22Z",
        "summary": "Recently, instruction-following audio-language models have received broad\nattention for human-audio interaction. However, the absence of benchmarks\ncapable of evaluating audio-centric interaction capabilities has impeded\nadvancements in this field. Previous models primarily focus on assessing\ndifferent fundamental tasks, such as Automatic Speech Recognition (ASR), and\nlack an assessment of the open-ended generative capabilities centered around\naudio. Thus, it is challenging to track the progression in the Large\nAudio-Language Models (LALMs) domain and to provide guidance for future\nimprovement. In this paper, we introduce AIR-Bench (\\textbf{A}udio\n\\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed\nto evaluate the ability of LALMs to understand various types of audio signals\n(including human speech, natural sounds, and music), and furthermore, to\ninteract with humans in the textual format. AIR-Bench encompasses two\ndimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former\nconsists of 19 tasks with approximately 19k single-choice questions, intending\nto inspect the basic single-task ability of LALMs. The latter one contains 2k\ninstances of open-ended question-and-answer data, directly assessing the\ncomprehension of the model on complex audio and its capacity to follow\ninstructions. Both benchmarks require the model to generate hypotheses\ndirectly. We design a unified framework that leverages advanced language\nmodels, such as GPT-4, to evaluate the scores of generated hypotheses given the\nmeta-information of the audio. Experimental results demonstrate a high level of\nconsistency between GPT-4-based evaluation and human evaluation. By revealing\nthe limitations of existing LALMs through evaluation results, AIR-Bench can\nprovide insights into the direction of future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.07729v1.pdf"
    },
    {
        "title": "ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning",
        "authors": [
            "A. Ghafarollahi",
            "M. J. Buehler"
        ],
        "published": "2024-01-27T20:19:49Z",
        "summary": "Designing de novo proteins beyond those found in nature holds significant\npromise for advancements in both scientific and engineering applications.\nCurrent methodologies for protein design often rely on AI-based models, such as\nsurrogate models that address end-to-end problems by linking protein structure\nto material properties or vice versa. However, these models frequently focus on\nspecific material objectives or structural properties, limiting their\nflexibility when incorporating out-of-domain knowledge into the design process\nor comprehensive data analysis is required. In this study, we introduce\nProtAgents, a platform for de novo protein design based on Large Language\nModels (LLMs), where multiple AI agents with distinct capabilities\ncollaboratively address complex tasks within a dynamic environment. The\nversatility in agent development allows for expertise in diverse domains,\nincluding knowledge retrieval, protein structure analysis, physics-based\nsimulations, and results analysis. The dynamic collaboration between agents,\nempowered by LLMs, provides a versatile approach to tackling protein design and\nanalysis problems, as demonstrated through diverse examples in this study. The\nproblems of interest encompass designing new proteins, analyzing protein\nstructures and obtaining new first-principles data -- natural vibrational\nfrequencies -- via physics simulations. The concerted effort of the system\nallows for powerful automated and synergistic design of de novo proteins with\ntargeted mechanical properties. The flexibility in designing the agents, on one\nhand, and their capacity in autonomous collaboration through the dynamic\nLLM-based multi-agent environment on the other hand, unleashes great potentials\nof LLMs in addressing multi-objective materials problems and opens up new\navenues for autonomous materials discovery and design.",
        "pdf_link": "https://arxiv.org/pdf/2402.04268v1.pdf"
    },
    {
        "title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
        "authors": [
            "Kaya Stechly",
            "Karthik Valmeekam",
            "Subbarao Kambhampati"
        ],
        "published": "2024-02-12T23:11:01Z",
        "summary": "There has been considerable divergence of opinion on the reasoning abilities\nof Large Language Models (LLMs). While the initial optimism that reasoning\nmight emerge automatically with scale has been tempered thanks to a slew of\ncounterexamples--ranging from multiplication to simple planning--there persists\na wide spread belief that LLMs can self-critique and improve their own\nsolutions in an iterative fashion. This belief seemingly rests on the\nassumption that verification of correctness should be easier than generation--a\nrather classical argument from computational complexity--which should be\nirrelevant to LLMs to the extent that what they are doing is approximate\nretrieval. In this paper, we set out to systematically investigate the\neffectiveness of iterative prompting in the context of reasoning and planning.\nWe present a principled empirical study of the performance of GPT-4 in three\ndomains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both\nwith the model critiquing its own answers and with an external correct reasoner\nverifying proposed solutions. In each case, we analyze whether the content of\ncriticisms actually affects bottom line performance, and whether we can ablate\nelements of the augmented system without losing performance. We observe\nsignificant performance collapse with self-critique, significant performance\ngains with sound external verification, but that the content of critique\ndoesn't matter to the performance of the system. In fact, merely re-prompting\nwith a sound verifier maintains most of the benefits of more involved setups.",
        "pdf_link": "https://arxiv.org/pdf/2402.08115v1.pdf"
    },
    {
        "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
        "authors": [
            "Wangtao Sun",
            "Haotian Xu",
            "Xuanqing Yu",
            "Pei Chen",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "published": "2024-03-09T04:20:46Z",
        "summary": "Although Large Language Models (LLMs) are showing impressive performance on a\nwide range of Natural Language Processing tasks, researchers have found that\nthey still have limited ability to conduct induction. Recent works mainly adopt\n``post processes'' paradigms to improve the performance of LLMs on induction\n(e.g., the hypothesis search & refinement methods), but their performance is\nstill constrained by the inherent inductive capability of the LLMs. In this\npaper, we propose a novel framework, Induction through Deduction (ItD), to\nenable the LLMs to teach themselves induction through deduction. The ItD\nframework is composed of two main components: a Deductive Data Generation\nmodule to generate induction data and a Naive Bayesian Induction module to\noptimize the fine-tuning and decoding of LLMs. Our empirical results showcase\nthe effectiveness of ItD on two induction benchmarks, achieving relative\nperformance improvement of 36% and 10% compared with previous state-of-the-art,\nrespectively. Our ablation study verifies the effectiveness of two key modules\nof ItD. We also verify the effectiveness of ItD across different LLMs and\ndeductors. The data and code of this paper can be found at\nhttps://anonymous.4open.science/r/ItD-E844.",
        "pdf_link": "https://arxiv.org/pdf/2403.05789v1.pdf"
    },
    {
        "title": "Scaling Laws for Downstream Task Performance of Large Language Models",
        "authors": [
            "Berivan Isik",
            "Natalia Ponomareva",
            "Hussein Hazimeh",
            "Dimitris Paparas",
            "Sergei Vassilvitskii",
            "Sanmi Koyejo"
        ],
        "published": "2024-02-06T17:31:20Z",
        "summary": "Scaling laws provide important insights that can guide the design of large\nlanguage models (LLMs). Existing work has primarily focused on studying scaling\nlaws for pretraining (upstream) loss. However, in transfer learning settings,\nin which LLMs are pretrained on an unsupervised dataset and then finetuned on a\ndownstream task, we often also care about the downstream performance. In this\nwork, we study the scaling behavior in a transfer learning setting, where LLMs\nare finetuned for machine translation tasks. Specifically, we investigate how\nthe choice of the pretraining data and its size affect downstream performance\n(translation quality) as judged by two metrics: downstream cross-entropy and\nBLEU score. Our experiments indicate that the size of the finetuning dataset\nand the distribution alignment between the pretraining and downstream data\nsignificantly influence the scaling behavior. With sufficient alignment, both\ndownstream cross-entropy and BLEU score improve monotonically with more\npretraining data. In such cases, we show that it is possible to predict the\ndownstream BLEU score with good accuracy using a log-law. However, there are\nalso cases where moderate misalignment causes the BLEU score to fluctuate or\nget worse with more pretraining, whereas downstream cross-entropy monotonically\nimproves. By analyzing these observations, we provide new practical insights\nfor choosing appropriate pretraining data.",
        "pdf_link": "https://arxiv.org/pdf/2402.04177v1.pdf"
    },
    {
        "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
        "authors": [
            "Derong Xu",
            "Ziheng Zhang",
            "Zhihong Zhu",
            "Zhenxi Lin",
            "Qidong Liu",
            "Xian Wu",
            "Tong Xu",
            "Xiangyu Zhao",
            "Yefeng Zheng",
            "Enhong Chen"
        ],
        "published": "2024-02-28T06:40:57Z",
        "summary": "Model editing aims to precisely modify the behaviours of large language\nmodels (LLMs) on specific knowledge while keeping irrelevant knowledge\nunchanged. It has been proven effective in resolving hallucination and\nout-of-date issues in LLMs. As a result, it can boost the application of LLMs\nin many critical domains (e.g., medical domain), where the hallucination is not\ntolerable. In this paper, we propose two model editing studies and validate\nthem in the medical domain: (1) directly editing the factual medical knowledge\nand (2) editing the explanations to facts. Meanwhile, we observed that current\nmodel editing methods struggle with the specialization and complexity of\nmedical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable\nAdapter strategy for medical model editing. It employs causal tracing to\nidentify the precise location of knowledge in neurons and then introduces\nscalable adapters into the dense layers of LLMs. These adapters are assigned\nscaling values based on the corresponding specific knowledge. To evaluate the\nediting impact, we build two benchmark datasets and introduce a series of\nchallenging and comprehensive metrics. Extensive experiments on medical LLMs\ndemonstrate the editing efficiency of MedLaSA, without affecting irrelevant\nknowledge that is not edited.",
        "pdf_link": "https://arxiv.org/pdf/2402.18099v1.pdf"
    },
    {
        "title": "Forecasting Events in Soccer Matches Through Language",
        "authors": [
            "Tiago Mendes-Neves",
            "Lu\u00eds Meireles",
            "Jo\u00e3o Mendes-Moreira"
        ],
        "published": "2024-02-09T23:02:57Z",
        "summary": "This paper introduces an approach to predicting the next event in a soccer\nmatch, a challenge bearing remarkable similarities to the problem faced by\nLarge Language Models (LLMs). Unlike other methods that severely limit event\ndynamics in soccer, often abstracting from many variables or relying on a mix\nof sequential models, our research proposes a novel technique inspired by the\nmethodologies used in LLMs. These models predict a complete chain of variables\nthat compose an event, significantly simplifying the construction of Large\nEvent Models (LEMs) for soccer. Utilizing deep learning on the publicly\navailable WyScout dataset, the proposed approach notably surpasses the\nperformance of previous LEM proposals in critical areas, such as the prediction\naccuracy of the next event type. This paper highlights the utility of LEMs in\nvarious applications, including betting and match analytics. Moreover, we show\nthat LEMs provide a simulation backbone on which many analytics pipelines can\nbe built, an approach opposite to the current specialized single-purpose\nmodels. LEMs represent a pivotal advancement in soccer analytics, establishing\na foundational framework for multifaceted analytics pipelines through a\nsingular machine-learning model.",
        "pdf_link": "https://arxiv.org/pdf/2402.06820v1.pdf"
    },
    {
        "title": "Graph Descriptive Order Improves Reasoning with Large Language Model",
        "authors": [
            "Yuyao Ge",
            "Shenghua Liu",
            "Wenjie Feng",
            "Lingrui Mei",
            "Lizhe Chen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-11T09:46:24Z",
        "summary": "In recent years, large language models have achieved state-of-the-art\nperformance across multiple domains. However, the progress in the field of\ngraph reasoning with LLM remains limited. Our work delves into this gap by\nthoroughly investigating graph reasoning with LLMs. In this work, we reveal the\nimpact of the order of graph description on LLMs' graph reasoning performance,\nwhich significantly affects LLMs' reasoning abilities. By altering this order,\nwe enhance the performance of LLMs from 42.22\\% to 70\\%. Furthermore, we\nintroduce the Scaled Graph Reasoning benchmark for assessing LLMs' performance\nacross various graph sizes and evaluate the relationship between LLMs' graph\nreasoning abilities and graph size. We discover that the graph reasoning\nperformance of LLMs does not monotonically decrease with the increase in graph\nsize. The experiments span several mainstream models, including GPT-3.5,\nLLaMA-2-7B, and LLaMA-2-13B, to offer a comprehensive evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.07140v3.pdf"
    },
    {
        "title": "EpilepsyLLM: Domain-Specific Large Language Model Fine-tuned with Epilepsy Medical Knowledge",
        "authors": [
            "Xuyang Zhao",
            "Qibin Zhao",
            "Toshihisa Tanaka"
        ],
        "published": "2024-01-11T13:39:00Z",
        "summary": "With large training datasets and massive amounts of computing sources, large\nlanguage models (LLMs) achieve remarkable performance in comprehensive and\ngenerative ability. Based on those powerful LLMs, the model fine-tuned with\ndomain-specific datasets posseses more specialized knowledge and thus is more\npractical like medical LLMs. However, the existing fine-tuned medical LLMs are\nlimited to general medical knowledge with English language. For\ndisease-specific problems, the model's response is inaccurate and sometimes\neven completely irrelevant, especially when using a language other than\nEnglish. In this work, we focus on the particular disease of Epilepsy with\nJapanese language and introduce a customized LLM termed as EpilepsyLLM. Our\nmodel is trained from the pre-trained LLM by fine-tuning technique using\ndatasets from the epilepsy domain. The datasets contain knowledge of basic\ninformation about disease, common treatment methods and drugs, and important\nnotes in life and work. The experimental results demonstrate that EpilepsyLLM\ncan provide more reliable and specialized medical knowledge responses.",
        "pdf_link": "https://arxiv.org/pdf/2401.05908v1.pdf"
    },
    {
        "title": "Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy",
        "authors": [
            "Shuhai Zhang",
            "Yiliao Song",
            "Jiahao Yang",
            "Yuanqing Li",
            "Bo Han",
            "Mingkui Tan"
        ],
        "published": "2024-02-25T09:44:56Z",
        "summary": "Large language models (LLMs) such as ChatGPT have exhibited remarkable\nperformance in generating human-like texts. However, machine-generated texts\n(MGTs) may carry critical risks, such as plagiarism issues, misleading\ninformation, or hallucination issues. Therefore, it is very urgent and\nimportant to detect MGTs in many situations. Unfortunately, it is challenging\nto distinguish MGTs and human-written texts because the distributional\ndiscrepancy between them is often very subtle due to the remarkable performance\nof LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy}\n(MMD) to address this issue in the sense that MMD can well identify\ndistributional discrepancies. However, directly training a detector with MMD\nusing diverse MGTs will incur a significantly increased variance of MMD since\nMGTs may contain \\textit{multiple text populations} due to various LLMs. This\nwill severely impair MMD's ability to measure the difference between two\nsamples. To tackle this, we propose a novel \\textit{multi-population} aware\noptimization method for MMD called MMD-MP, which can \\textit{avoid variance\nincreases} and thus improve the stability to measure the distributional\ndiscrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and\nsentence-based detection, respectively. Extensive experiments on various LLMs,\n\\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The\nsource code is available at \\url{https://github.com/ZSHsh98/MMD-MP}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16041v2.pdf"
    },
    {
        "title": "Structure Guided Prompt: Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text",
        "authors": [
            "Kewei Cheng",
            "Nesreen K. Ahmed",
            "Theodore Willke",
            "Yizhou Sun"
        ],
        "published": "2024-02-20T22:56:23Z",
        "summary": "Although Large Language Models (LLMs) excel at addressing straightforward\nreasoning tasks, they frequently struggle with difficulties when confronted by\nmore complex multi-step reasoning due to a range of factors. Firstly, natural\nlanguage often encompasses complex relationships among entities, making it\nchallenging to maintain a clear reasoning chain over longer spans. Secondly,\nthe abundance of linguistic diversity means that the same entities and\nrelationships can be expressed using different terminologies and structures,\ncomplicating the task of identifying and establishing connections between\nmultiple pieces of information. Graphs provide an effective solution to\nrepresent data rich in relational information and capture long-term\ndependencies among entities. To harness the potential of graphs, our paper\nintroduces Structure Guided Prompt, an innovative three-stage task-agnostic\nprompting framework designed to improve the multi-step reasoning capabilities\nof LLMs in a zero-shot setting. This framework explicitly converts unstructured\ntext into a graph via LLMs and instructs them to navigate this graph using\ntask-specific strategies to formulate responses. By effectively organizing\ninformation and guiding navigation, it enables LLMs to provide more accurate\nand context-aware responses. Our experiments show that this framework\nsignificantly enhances the reasoning capabilities of LLMs, enabling them to\nexcel in a broader spectrum of natural language scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.13415v1.pdf"
    },
    {
        "title": "Enhancing Robustness of LLM-Synthetic Text Detectors for Academic Writing: A Comprehensive Analysis",
        "authors": [
            "Zhicheng Dou",
            "Yuchen Guo",
            "Ching-Chun Chang",
            "Huy H. Nguyen",
            "Isao Echizen"
        ],
        "published": "2024-01-16T01:58:36Z",
        "summary": "The emergence of large language models (LLMs), such as Generative Pre-trained\nTransformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and\nbroader community. While these models offer numerous advantages in terms of\nrevolutionizing work and study methods, they have also garnered significant\nattention due to their potential negative consequences. One example is\ngenerating academic reports or papers with little to no human contribution.\nConsequently, researchers have focused on developing detectors to address the\nmisuse of LLMs. However, most existing methods prioritize achieving higher\naccuracy on restricted datasets, neglecting the crucial aspect of\ngeneralizability. This limitation hinders their practical application in\nreal-life scenarios where reliability is paramount. In this paper, we present a\ncomprehensive analysis of the impact of prompts on the text generated by LLMs\nand highlight the potential lack of robustness in one of the current\nstate-of-the-art GPT detectors. To mitigate these issues concerning the misuse\nof LLMs in academic writing, we propose a reference-based Siamese detector\nnamed Synthetic-Siamese which takes a pair of texts, one as the inquiry and the\nother as the reference. Our method effectively addresses the lack of robustness\nof previous detectors (OpenAI detector and DetectGPT) and significantly\nimproves the baseline performances in realistic academic writing scenarios by\napproximately 67% to 95%.",
        "pdf_link": "https://arxiv.org/pdf/2401.08046v1.pdf"
    },
    {
        "title": "How well can large language models explain business processes?",
        "authors": [
            "Dirk Fahland",
            "Fabiana Fournier",
            "Lior Limonad",
            "Inna Skarbovsky",
            "Ava J. E. Swevels"
        ],
        "published": "2024-01-23T15:29:26Z",
        "summary": "Large Language Models (LLMs) are likely to play a prominent role in future\nAI-augmented business process management systems (ABPMSs) catering\nfunctionalities across all system lifecycle stages. One such system's\nfunctionality is Situation-Aware eXplainability (SAX), which relates to\ngenerating causally sound and yet human-interpretable explanations that take\ninto account the process context in which the explained condition occurred. In\nthis paper, we present the SAX4BPM framework developed to generate SAX\nexplanations. The SAX4BPM suite consists of a set of services and a central\nknowledge repository. The functionality of these services is to elicit the\nvarious knowledge ingredients that underlie SAX explanations. A key innovative\ncomponent among these ingredients is the causal process execution view. In this\nwork, we integrate the framework with an LLM to leverage its power to\nsynthesize the various input ingredients for the sake of improved SAX\nexplanations. Since the use of LLMs for SAX is also accompanied by a certain\ndegree of doubt related to its capacity to adequately fulfill SAX along with\nits tendency for hallucination and lack of inherent capacity to reason, we\npursued a methodological evaluation of the quality of the generated\nexplanations. To this aim, we developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.",
        "pdf_link": "https://arxiv.org/pdf/2401.12846v1.pdf"
    },
    {
        "title": "Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking",
        "authors": [
            "Nikhil Sharma",
            "Q. Vera Liao",
            "Ziang Xiao"
        ],
        "published": "2024-02-08T18:14:33Z",
        "summary": "Large language models (LLMs) powered conversational search systems have\nalready been used by hundreds of millions of people, and are believed to bring\nmany benefits over conventional search. However, while decades of research and\npublic discourse interrogated the risk of search systems in increasing\nselective exposure and creating echo chambers -- limiting exposure to diverse\nopinions and leading to opinion polarization, little is known about such a risk\nof LLM-powered conversational search. We conduct two experiments to\ninvestigate: 1) whether and how LLM-powered conversational search increases\nselective exposure compared to conventional search; 2) whether and how LLMs\nwith opinion biases that either reinforce or challenge the user's view change\nthe effect. Overall, we found that participants engaged in more biased\ninformation querying with LLM-powered conversational search, and an opinionated\nLLM reinforcing their views exacerbated this bias. These results present\ncritical implications for the development of LLMs and conversational search\nsystems, and the policy governing these technologies.",
        "pdf_link": "https://arxiv.org/pdf/2402.05880v2.pdf"
    },
    {
        "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
        "authors": [
            "Coleman Hooper",
            "Sehoon Kim",
            "Hiva Mohammadzadeh",
            "Michael W. Mahoney",
            "Yakun Sophia Shao",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "published": "2024-01-31T18:58:14Z",
        "summary": "LLMs are seeing growing use for applications such as document analysis and\nsummarization which require large context windows, and with these large context\nwindows KV cache activations surface as the dominant contributor to memory\nconsumption during inference. Quantization is a promising approach for\ncompressing KV cache activations; however, existing solutions fail to represent\nactivations accurately in ultra-low precisions, such as sub-4-bit. In this\nwork, we present KVQuant, which addresses this problem by incorporating novel\nmethods for quantizing cached KV activations, including: (i) Per-Channel Key\nQuantization, where we adjust the dimension along which we quantize the Key\nactivations to better match the distribution; (ii) Pre-RoPE Key Quantization,\nwhere we quantize Key activations before the rotary positional embedding to\nmitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization,\nwhere we derive per-layer sensitivity-weighted non-uniform datatypes that\nbetter represent the distributions; (iv) Per-Vector Dense-and-Sparse\nQuantization, where we isolate outliers separately for each vector to minimize\nskews in quantization ranges; and (v) Q-Norm, where we normalize quantization\ncentroids in order to mitigate distribution shift, providing additional\nbenefits for 2-bit quantization. By applying our method to the LLaMA, LLaMA-2,\nand Mistral models, we achieve $<0.1$ perplexity degradation with 3-bit\nquantization on both Wikitext-2 and C4, outperforming existing approaches. Our\nmethod enables serving the LLaMA-7B model with a context length of up to 1\nmillion on a single A100-80GB GPU and up to 10 million on an 8-GPU system.",
        "pdf_link": "https://arxiv.org/pdf/2401.18079v3.pdf"
    },
    {
        "title": "Sinkhorn Distance Minimization for Knowledge Distillation",
        "authors": [
            "Xiao Cui",
            "Yulei Qin",
            "Yuting Gao",
            "Enwei Zhang",
            "Zihan Xu",
            "Tong Wu",
            "Ke Li",
            "Xing Sun",
            "Wengang Zhou",
            "Houqiang Li"
        ],
        "published": "2024-02-27T01:13:58Z",
        "summary": "Knowledge distillation (KD) has been widely adopted to compress large\nlanguage models (LLMs). Existing KD methods investigate various divergence\nmeasures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL),\nand Jensen-Shannon (JS) divergences. However, due to limitations inherent in\ntheir assumptions and definitions, these measures fail to deliver effective\nsupervision when few distribution overlap exists between the teacher and the\nstudent. In this paper, we show that the aforementioned KL, RKL, and JS\ndivergences respectively suffer from issues of mode-averaging, mode-collapsing,\nand mode-underestimation, which deteriorates logits-based KD for diverse NLP\ntasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the\nSinkhorn distance to ensure a nuanced and precise assessment of the disparity\nbetween teacher and student distributions. Besides, profit by properties of the\nSinkhorn metric, we can get rid of sample-wise KD that restricts the perception\nof divergence in each teacher-student sample pair. Instead, we propose a\nbatch-wise reformulation to capture geometric intricacies of distributions\nacross samples in the high-dimensional space. Comprehensive evaluation on GLUE\nand SuperGLUE, in terms of comparability, validity, and generalizability,\nhighlights our superiority over state-of-the-art methods on all kinds of LLMs\nwith encoder-only, encoder-decoder, and decoder-only architectures.",
        "pdf_link": "https://arxiv.org/pdf/2402.17110v1.pdf"
    },
    {
        "title": "QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners",
        "authors": [
            "Rui Xiao",
            "Lu Han",
            "Xiaoying Zhou",
            "Jiong Wang",
            "Na Zong",
            "Pengyu Zhang"
        ],
        "published": "2024-01-30T13:11:23Z",
        "summary": "In online learning platforms, particularly in rapidly growing computer\nprogramming courses, addressing the thousands of students' learning queries\nrequires considerable human cost. The creation of intelligent assistant large\nlanguage models (LLMs) tailored for programming education necessitates distinct\ndata support. However, in real application scenarios, the data resources for\ntraining such LLMs are relatively scarce. Therefore, to address the data\nscarcity in intelligent educational systems for programming, this paper\nproposes a new Chinese question-and-answer dataset for Python learners. To\nensure the authenticity and reliability of the sources of the questions, we\ncollected questions from actual student questions and categorized them\naccording to various dimensions such as the type of questions and the type of\nlearners. This annotation principle is designed to enhance the effectiveness\nand quality of online programming education, providing a solid data foundation\nfor developing the programming teaching assists (TA). Furthermore, we conducted\ncomprehensive evaluations of various LLMs proficient in processing and\ngenerating Chinese content, highlighting the potential limitations of general\nLLMs as intelligent teaching assistants in computer programming courses.",
        "pdf_link": "https://arxiv.org/pdf/2402.07913v2.pdf"
    },
    {
        "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
        "authors": [
            "Haoxiang Guan",
            "Jiyan He",
            "Shuxin Zheng",
            "En-Hong Chen",
            "Weiming Zhang",
            "Nenghai Yu"
        ],
        "published": "2024-02-28T11:29:09Z",
        "summary": "Large language models (LLMs) have demonstrated impressive performance on many\ntasks. However, to achieve optimal performance, specially designed prompting\nmethods are still needed. These methods either rely on task-specific few-shot\nexamples that require a certain level of domain knowledge, or are designed to\nbe simple but only perform well on a few types of tasks. In this work, we\nattempt to introduce the concept of generalist prompting, which operates on the\ndesign principle of achieving optimal or near-optimal performance on a wide\nrange of tasks while eliminating the need for manual selection and\ncustomization of prompts tailored to specific problems. Furthermore, we propose\nMeMo (Mental Models), an innovative prompting method that is simple-designed\nyet effectively fulfills the criteria of generalist prompting. MeMo distills\nthe cores of various prompting methods into individual mental models and allows\nLLMs to autonomously select the most suitable mental models for the problem,\nachieving or being near to the state-of-the-art results on diverse tasks such\nas STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We\nhope that the insights presented herein will stimulate further exploration of\ngeneralist prompting methods for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.18252v1.pdf"
    },
    {
        "title": "Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs",
        "authors": [
            "Tanise Ceron",
            "Neele Falk",
            "Ana Bari\u0107",
            "Dmitry Nikolaev",
            "Sebastian Pad\u00f3"
        ],
        "published": "2024-02-27T16:19:37Z",
        "summary": "Due to the widespread use of large language models (LLMs) in ubiquitous\nsystems, we need to understand whether they embed a specific worldview and what\nthese views reflect. Recent studies report that, prompted with political\nquestionnaires, LLMs show left-liberal leanings. However, it is as yet unclear\nwhether these leanings are reliable (robust to prompt variations) and whether\nthe leaning is consistent across policies and political leaning. We propose a\nseries of tests which assess the reliability and consistency of LLMs' stances\non political statements based on a dataset of voting-advice questionnaires\ncollected from seven EU countries and annotated for policy domains. We study\nLLMs ranging in size from 7B to 70B parameters and find that their reliability\nincreases with parameter count. Larger models show overall stronger alignment\nwith left-leaning parties but differ among policy programs: They evince a\n(left-wing) positive stance towards environment protection, social welfare but\nalso (right-wing) law and order, with no consistent preferences in foreign\npolicy, migration, and economy.",
        "pdf_link": "https://arxiv.org/pdf/2402.17649v1.pdf"
    },
    {
        "title": "AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models",
        "authors": [
            "Yuting Wei",
            "Yuanxing Xu",
            "Xinru Wei",
            "Simin Yang",
            "Yangfu Zhu",
            "Yuqing Li",
            "Di Liu",
            "Bin Wu"
        ],
        "published": "2024-03-11T10:24:37Z",
        "summary": "Given the importance of ancient Chinese in capturing the essence of rich\nhistorical and cultural heritage, the rapid advancements in Large Language\nModels (LLMs) necessitate benchmarks that can effectively evaluate their\nunderstanding of ancient contexts. To meet this need, we present AC-EVAL, an\ninnovative benchmark designed to assess the advanced knowledge and reasoning\ncapabilities of LLMs within the context of ancient Chinese. AC-EVAL is\nstructured across three levels of difficulty reflecting different facets of\nlanguage comprehension: general historical knowledge, short text understanding,\nand long text comprehension. The benchmark comprises 13 tasks, spanning\nhistorical facts, geography, social customs, art, philosophy, classical poetry\nand prose, providing a comprehensive assessment framework. Our extensive\nevaluation of top-performing LLMs, tailored for both English and Chinese,\nreveals a substantial potential for enhancing ancient text comprehension. By\nhighlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote\ntheir development and application forward in the realms of ancient Chinese\nlanguage education and scholarly research. The AC-EVAL data and evaluation code\nare available at https://github.com/yuting-wei/AC-EVAL.",
        "pdf_link": "https://arxiv.org/pdf/2403.06574v1.pdf"
    },
    {
        "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models",
        "authors": [
            "Li Sun",
            "Liuan Wang",
            "Jun Sun",
            "Takayuki Okatani"
        ],
        "published": "2024-01-18T10:18:48Z",
        "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the comprehension of multimedia content, bringing\ntogether diverse modalities such as text, images, and videos. However, a\ncritical challenge faced by these models, especially when processing video\ninputs, is the occurrence of hallucinations - erroneous perceptions or\ninterpretations, particularly at the event level. This study introduces an\ninnovative method to address event-level hallucinations in MLLMs, focusing on\nspecific temporal understanding in video content. Our approach leverages a\nnovel framework that extracts and utilizes event-specific information from both\nthe event query and the provided video to refine MLLMs' response. We propose a\nunique mechanism that decomposes on-demand event queries into iconic actions.\nSubsequently, we employ models like CLIP and BLIP2 to predict specific\ntimestamps for event occurrences. Our evaluation, conducted using the\nCharades-STA dataset, demonstrates a significant reduction in temporal\nhallucinations and an improvement in the quality of event-related responses.\nThis research not only provides a new perspective in addressing a critical\nlimitation of MLLMs but also contributes a quantitatively measurable method for\nevaluating MLLMs in the context of temporal-related questions.",
        "pdf_link": "https://arxiv.org/pdf/2401.09861v1.pdf"
    },
    {
        "title": "OneBit: Towards Extremely Low-bit Large Language Models",
        "authors": [
            "Yuzhuang Xu",
            "Xu Han",
            "Zonghan Yang",
            "Shuo Wang",
            "Qingfu Zhu",
            "Zhiyuan Liu",
            "Weidong Liu",
            "Wanxiang Che"
        ],
        "published": "2024-02-17T14:26:57Z",
        "summary": "Model quantification uses low bit-width values to represent the weight\nmatrices of models, which is a promising approach to reduce both storage and\ncomputational overheads of deploying highly anticipated LLMs. However, existing\nquantization methods suffer severe performance degradation when the bit-width\nis extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to\nquantize models. This paper boldly quantizes the weight matrices of LLMs to\n1-bit, paving the way for the extremely low bit-width deployment of LLMs. For\nthis target, we introduce a 1-bit quantization-aware training (QAT) framework\nnamed OneBit, including a novel 1-bit parameter representation method to better\nquantize LLMs as well as an effective parameter initialization method based on\nmatrix decomposition to improve the convergence speed of the QAT framework.\nSufficient experimental results indicate that OneBit achieves good performance\n(at least 83% of the non-quantized performance) with robust training processes\nwhen only using 1-bit weight matrices.",
        "pdf_link": "https://arxiv.org/pdf/2402.11295v1.pdf"
    },
    {
        "title": "Large Language Models for Mathematical Reasoning: Progresses and Challenges",
        "authors": [
            "Janice Ahn",
            "Rishu Verma",
            "Renze Lou",
            "Di Liu",
            "Rui Zhang",
            "Wenpeng Yin"
        ],
        "published": "2024-01-31T20:26:32Z",
        "summary": "Mathematical reasoning serves as a cornerstone for assessing the fundamental\ncognitive capabilities of human intelligence. In recent times, there has been a\nnotable surge in the development of Large Language Models (LLMs) geared towards\nthe automated resolution of mathematical problems. However, the landscape of\nmathematical problem types is vast and varied, with LLM-oriented techniques\nundergoing evaluation across diverse datasets and settings. This diversity\nmakes it challenging to discern the true advancements and obstacles within this\nburgeoning field. This survey endeavors to address four pivotal dimensions: i)\na comprehensive exploration of the various mathematical problems and their\ncorresponding datasets that have been investigated; ii) an examination of the\nspectrum of LLM-oriented techniques that have been proposed for mathematical\nproblem-solving; iii) an overview of factors and concerns affecting LLMs in\nsolving math; and iv) an elucidation of the persisting challenges within this\ndomain. To the best of our knowledge, this survey stands as one of the first\nextensive examinations of the landscape of LLMs in the realm of mathematics,\nproviding a holistic perspective on the current state, accomplishments, and\nfuture challenges in this rapidly evolving field.",
        "pdf_link": "https://arxiv.org/pdf/2402.00157v3.pdf"
    },
    {
        "title": "Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering",
        "authors": [
            "Pierre Erbacher",
            "Louis Falissar",
            "Vincent Guigue",
            "Laure Soulier"
        ],
        "published": "2024-01-03T15:12:42Z",
        "summary": "While Large Language Models (LLM) are able to accumulate and restore\nknowledge, they are still prone to hallucination. Especially when faced with\nfactual questions, LLM cannot only rely on knowledge stored in parameters to\nguarantee truthful and correct answers. Augmenting these models with the\nability to search on external information sources, such as the web, is a\npromising approach to ground knowledge to retrieve information. However,\nsearching in a large collection of documents introduces additional\ncomputational/time costs. An optimal behavior would be to query external\nresources only when the LLM is not confident about answers. In this paper, we\npropose a new LLM able to self-estimate if it is able to answer directly or\nneeds to request an external tool. We investigate a supervised approach by\nintroducing a hallucination masking mechanism in which labels are generated\nusing a close book question-answering task. In addition, we propose to leverage\nparameter-efficient fine-tuning techniques to train our model on a small amount\nof data. Our model directly provides answers for $78.2\\%$ of the known queries\nand opts to search for $77.2\\%$ of the unknown ones. This results in the API\nbeing utilized only $62\\%$ of the time.",
        "pdf_link": "https://arxiv.org/pdf/2401.01780v1.pdf"
    },
    {
        "title": "UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language",
        "authors": [
            "Yufei He",
            "Bryan Hooi"
        ],
        "published": "2024-02-21T09:06:31Z",
        "summary": "Foundation models like ChatGPT and GPT-4 have revolutionized artificial\nintelligence, exhibiting remarkable abilities to generalize across a wide array\nof tasks and applications beyond their initial training objectives. However,\nwhen this concept is applied to graph learning, a stark contrast emerges. Graph\nlearning has predominantly focused on single-graph models, tailored to specific\ntasks or datasets, lacking the ability to transfer learned knowledge to\ndifferent domains. This limitation stems from the inherent complexity and\ndiversity of graph structures, along with the different feature and label\nspaces specific to graph data. In this paper, we present our UniGraph\nframework, designed to train a graph foundation model capable of generalizing\nto unseen graphs and tasks across diverse domains. Unlike single-graph models\nthat use pre-computed node features of varying dimensions as input, our\napproach leverages Text-Attributed Graphs (TAGs) for unifying node\nrepresentations. We propose a cascaded architecture of Language Models (LMs)\nand Graph Neural Networks (GNNs) as backbone networks with a self-supervised\ntraining objective based on Masked Graph Modeling (MGM). We introduce graph\ninstruction tuning using Large Language Models (LLMs) to enable zero-shot\nprediction ability. Our comprehensive experiments across various graph learning\ntasks and domains demonstrate the model's effectiveness in self-supervised\nrepresentation learning on unseen graphs, few-shot in-context transfer, and\nzero-shot transfer, even surpassing or matching the performance of GNNs that\nhave undergone supervised training on target datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.13630v1.pdf"
    },
    {
        "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms",
        "authors": [
            "Jonathan Zheng",
            "Alan Ritter",
            "Wei Xu"
        ],
        "published": "2024-02-19T16:19:15Z",
        "summary": "The performance of Large Language Models (LLMs) degrades from the temporal\ndrift between data used for model training and newer text seen during\ninference. One understudied avenue of language change causing data drift is the\nemergence of neologisms -- new word forms -- over time. We create a diverse\nresource of recent English neologisms by using several popular collection\nmethods. We analyze temporal drift using neologisms by comparing sentences\ncontaining new words with near-identical sentences that replace neologisms with\nexisting substitute words. Model performance is nearly halved in machine\ntranslation when a single neologism is introduced in a sentence. Motivated by\nthese results, we construct a benchmark to evaluate LLMs' ability to generalize\nto neologisms with various natural language understanding tasks and model\nperplexity. Models with later knowledge cutoff dates yield lower perplexities\nand perform better in downstream tasks. LLMs are also affected differently\nbased on the linguistic origins of words, indicating that neologisms are\ncomplex for static LLMs to address. We will release our benchmark and code for\nreproducing our experiments.",
        "pdf_link": "https://arxiv.org/pdf/2402.12261v2.pdf"
    },
    {
        "title": "Specialized Language Models with Cheap Inference from Limited Domain Data",
        "authors": [
            "David Grangier",
            "Angelos Katharopoulos",
            "Pierre Ablin",
            "Awni Hannun"
        ],
        "published": "2024-02-02T01:45:18Z",
        "summary": "Large language models have emerged as a versatile tool but are challenging to\napply to tasks lacking large inference budgets and large in-domain training\nsets. This work formalizes these constraints and distinguishes four important\nvariables: the pretraining budget (for training before the target domain is\nknown), the specialization budget (for training after the target domain is\nknown), the inference budget, and the in-domain training set size. Across these\nsettings, we compare different approaches from the machine learning literature.\nLimited by inference cost, we find better alternatives to the standard practice\nof training very large vanilla transformer models. In particular, we show that\nhyper-networks and mixture of experts have better perplexity for large\npretraining budgets, while small models trained on importance sampled datasets\nare attractive for large specialization budgets.",
        "pdf_link": "https://arxiv.org/pdf/2402.01093v1.pdf"
    },
    {
        "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
        "authors": [
            "Qiang Zhang",
            "Keyang Ding",
            "Tianwen Lyv",
            "Xinda Wang",
            "Qingyu Yin",
            "Yiwen Zhang",
            "Jing Yu",
            "Yuhao Wang",
            "Xiaotong Li",
            "Zhuoyi Xiang",
            "Xiang Zhuang",
            "Zeyuan Wang",
            "Ming Qin",
            "Mengyao Zhang",
            "Jinlu Zhang",
            "Jiyu Cui",
            "Renjun Xu",
            "Hongyang Chen",
            "Xiaohui Fan",
            "Huabin Xing",
            "Huajun Chen"
        ],
        "published": "2024-01-26T05:33:34Z",
        "summary": "Large Language Models (LLMs) have emerged as a transformative power in\nenhancing natural language comprehension, representing a significant stride\ntoward artificial general intelligence. The application of LLMs extends beyond\nconventional linguistic boundaries, encompassing specialized linguistic systems\ndeveloped within various scientific disciplines. This growing interest has led\nto the advent of scientific LLMs, a novel subclass specifically engineered for\nfacilitating scientific discovery. As a burgeoning area in the community of AI\nfor Science, scientific LLMs warrant comprehensive exploration. However, a\nsystematic and up-to-date survey introducing them is currently lacking. In this\npaper, we endeavor to methodically delineate the concept of \"scientific\nlanguage\", whilst providing a thorough review of the latest advancements in\nscientific LLMs. Given the expansive realm of scientific disciplines, our\nanalysis adopts a focused lens, concentrating on the biological and chemical\ndomains. This includes an in-depth examination of LLMs for textual knowledge,\nsmall molecules, macromolecular proteins, genomic sequences, and their\ncombinations, analyzing them in terms of model architectures, capabilities,\ndatasets, and evaluation. Finally, we critically examine the prevailing\nchallenges and point out promising research directions along with the advances\nof LLMs. By offering a comprehensive overview of technical developments in this\nfield, this survey aspires to be an invaluable resource for researchers\nnavigating the intricate landscape of scientific LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.14656v1.pdf"
    },
    {
        "title": "IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation",
        "authors": [
            "Jiacui Huang",
            "Hongtao Zhang",
            "Mingbo Zhao",
            "Zhou Wu"
        ],
        "published": "2024-03-28T11:52:42Z",
        "summary": "Vision-and-Language Navigation (VLN) is a challenging task that requires a\nrobot to navigate in photo-realistic environments with human natural language\npromptings. Recent studies aim to handle this task by constructing the semantic\nspatial map representation of the environment, and then leveraging the strong\nability of reasoning in large language models for generalizing code for guiding\nthe robot navigation. However, these methods face limitations in instance-level\nand attribute-level navigation tasks as they cannot distinguish different\ninstances of the same object. To address this challenge, we propose a new\nmethod, namely, Instance-aware Visual Language Map (IVLMap), to empower the\nrobot with instance-level and attribute-level semantic mapping, where it is\nautonomously constructed by fusing the RGBD video data collected from the robot\nagent with special-designed natural language map indexing in the bird's-in-eye\nview. Such indexing is instance-level and attribute-level. In particular, when\nintegrated with a large language model, IVLMap demonstrates the capability to\ni) transform natural language into navigation targets with instance and\nattribute information, enabling precise localization, and ii) accomplish\nzero-shot end-to-end navigation tasks based on natural language commands.\nExtensive navigation experiments are conducted. Simulation results illustrate\nthat our method can achieve an average improvement of 14.4\\% in navigation\naccuracy. Code and demo are released at https://ivlmap.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.19336v1.pdf"
    },
    {
        "title": "Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System",
        "authors": [
            "Feng Jiang",
            "Kuang Wang",
            "Haizhou Li"
        ],
        "published": "2024-01-17T11:50:53Z",
        "summary": "In the contemporary information era, significantly accelerated by the advent\nof Large-scale Language Models, the proliferation of scientific literature is\nreaching unprecedented levels. Researchers urgently require efficient tools for\nreading and summarizing academic papers, uncovering significant scientific\nliterature, and employing diverse interpretative methodologies. To address this\nburgeoning demand, the role of automated scientific literature interpretation\nsystems has become paramount. However, prevailing models, both commercial and\nopen-source, confront notable challenges: they often overlook multimodal data,\ngrapple with summarizing over-length texts, and lack diverse user interfaces.\nIn response, we introduce an open-source multi-modal automated academic paper\ninterpretation system (MMAPIS) with three-step process stages, incorporating\nLLMs to augment its functionality. Our system first employs the hybrid modality\npreprocessing and alignment module to extract plain text, and tables or figures\nfrom documents separately. It then aligns this information based on the section\nnames they belong to, ensuring that data with identical section names are\ncategorized under the same section. Following this, we introduce a hierarchical\ndiscourse-aware summarization method. It utilizes the extracted section names\nto divide the article into shorter text segments, facilitating specific\nsummarizations both within and between sections via LLMs with specific prompts.\nFinally, we have designed four types of diversified user interfaces, including\npaper recommendation, multimodal Q\\&A, audio broadcasting, and interpretation\nblog, which can be widely applied across various scenarios. Our qualitative and\nquantitative evaluations underscore the system's superiority, especially in\nscientific summarization, where it outperforms solutions relying solely on\nGPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2401.09150v1.pdf"
    },
    {
        "title": "Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?",
        "authors": [
            "Egor Zverev",
            "Sahar Abdelnabi",
            "Mario Fritz",
            "Christoph H. Lampert"
        ],
        "published": "2024-03-11T15:48:56Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) have achieved breakthrough\nresults, opening countless new possibilities for many practical applications.\nHowever, LLMs lack elementary safety features that are established norms in\nother areas of computer science, such as the separation between instructions\nand data, causing them to malfunction or rendering them vulnerable to\nmanipulation and interference by third parties e.g., via indirect\nprompt/command injection. Even worse, so far, there is not even an established\ndefinition of what precisely such a separation would mean and how its violation\ncould be tested. In this work, we aim to close this gap. We introduce a formal\nmeasure to quantify the phenomenon of instruction-data separation as well as an\nempirical variant of the measure that can be computed from a model`s black-box\noutputs. We also introduce a new dataset, SEP (Should it be Executed or\nProcessed?), which allows estimating the measure, and we report results on\nseveral state-of-the-art open-source and closed LLMs. Finally, we\nquantitatively demonstrate that all evaluated LLMs fail to achieve a high\namount of separation, according to our measure. The source code and SEP dataset\nare openly accessible at\nhttps://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",
        "pdf_link": "https://arxiv.org/pdf/2403.06833v1.pdf"
    },
    {
        "title": "Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models",
        "authors": [
            "Qiong Wu",
            "Weihao Ye",
            "Yiyi Zhou",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "published": "2024-03-22T14:20:34Z",
        "summary": "In this paper, we propose a novel parameter and computation efficient tuning\nmethod for Multi-modal Large Language Models (MLLMs), termed Efficient\nAttention Skipping (EAS). Concretely, we first reveal that multi-head\nattentions (MHAs), the main computational overhead of MLLMs, are often\nredundant to downstream tasks. Based on this observation, EAS evaluates the\nattention redundancy and skips the less important MHAs to speed up inference.\nBesides, we also propose a novel propagation-of-information adapter (PIA) to\nserve the attention skipping of EAS and keep parameter efficiency, which can be\nfurther re-parameterized into feed-forward networks (FFNs) for zero-extra\nlatency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN\nand a classic VL pre-trained model called METER, and conduct extensive\nexperiments on a set of benchmarks. The experiments show that EAS not only\nretains high performance and parameter efficiency, but also greatly speeds up\ninference speed. For instance, LaVIN-EAS can obtain 89.98\\% accuracy on\nScineceQA while speeding up inference by 2.2 times to LaVIN",
        "pdf_link": "https://arxiv.org/pdf/2403.15226v1.pdf"
    },
    {
        "title": "Topic Aware Probing: From Sentence Length Prediction to Idiom Identification how reliant are Neural Language Models on Topic?",
        "authors": [
            "Vasudevan Nedumpozhimana",
            "John D. Kelleher"
        ],
        "published": "2024-03-04T13:10:08Z",
        "summary": "Transformer-based Neural Language Models achieve state-of-the-art performance\non various natural language processing tasks. However, an open question is the\nextent to which these models rely on word-order/syntactic or word\nco-occurrence/topic-based information when processing natural language. This\nwork contributes to this debate by addressing the question of whether these\nmodels primarily use topic as a signal, by exploring the relationship between\nTransformer-based models' (BERT and RoBERTa's) performance on a range of\nprobing tasks in English, from simple lexical tasks such as sentence length\nprediction to complex semantic tasks such as idiom token identification, and\nthe sensitivity of these tasks to the topic information. To this end, we\npropose a novel probing method which we call topic-aware probing. Our initial\nresults indicate that Transformer-based models encode both topic and non-topic\ninformation in their intermediate layers, but also that the facility of these\nmodels to distinguish idiomatic usage is primarily based on their ability to\nidentify and encode topic. Furthermore, our analysis of these models'\nperformance on other standard probing tasks suggests that tasks that are\nrelatively insensitive to the topic information are also tasks that are\nrelatively difficult for these models.",
        "pdf_link": "https://arxiv.org/pdf/2403.02009v1.pdf"
    },
    {
        "title": "Seven Failure Points When Engineering a Retrieval Augmented Generation System",
        "authors": [
            "Scott Barnett",
            "Stefanus Kurniawan",
            "Srikanth Thudumu",
            "Zach Brannelly",
            "Mohamed Abdelrazek"
        ],
        "published": "2024-01-11T12:04:11Z",
        "summary": "Software engineers are increasingly adding semantic search capabilities to\napplications using a strategy known as Retrieval Augmented Generation (RAG). A\nRAG system involves finding documents that semantically match a query and then\npassing the documents to a large language model (LLM) such as ChatGPT to\nextract the right answer using an LLM. RAG systems aim to: a) reduce the\nproblem of hallucinated responses from LLMs, b) link sources/references to\ngenerated responses, and c) remove the need for annotating documents with\nmeta-data. However, RAG systems suffer from limitations inherent to information\nretrieval systems and from reliance on LLMs. In this paper, we present an\nexperience report on the failure points of RAG systems from three case studies\nfrom separate domains: research, education, and biomedical. We share the\nlessons learned and present 7 failure points to consider when designing a RAG\nsystem. The two key takeaways arising from our work are: 1) validation of a RAG\nsystem is only feasible during operation, and 2) the robustness of a RAG system\nevolves rather than designed in at the start. We conclude with a list of\npotential research directions on RAG systems for the software engineering\ncommunity.",
        "pdf_link": "https://arxiv.org/pdf/2401.05856v1.pdf"
    },
    {
        "title": "Automated Discovery of Integral with Deep Learning",
        "authors": [
            "Xiaoxin Yin"
        ],
        "published": "2024-02-28T04:34:15Z",
        "summary": "Recent advancements in the realm of deep learning, particularly in the\ndevelopment of large language models (LLMs), have demonstrated AI's ability to\ntackle complex mathematical problems or solving programming challenges.\nHowever, the capability to solve well-defined problems based on extensive\ntraining data differs significantly from the nuanced process of making\nscientific discoveries. Trained on almost all human knowledge available,\ntoday's sophisticated LLMs basically learn to predict sequences of tokens. They\ngenerate mathematical derivations and write code in a similar way as writing an\nessay, and do not have the ability to pioneer scientific discoveries in the\nmanner a human scientist would do.\n  In this study we delve into the potential of using deep learning to\nrediscover a fundamental mathematical concept: integrals. By defining integrals\nas area under the curve, we illustrate how AI can deduce the integral of a\ngiven function, exemplified by inferring $\\int_{0}^{x} t^2 dt = \\frac{x^3}{3}$\nand $\\int_{0}^{x} ae^{bt} dt = \\frac{a}{b} e^{bx} - \\frac{a}{b}$. Our\nexperiments show that deep learning models can approach the task of inferring\nintegrals either through a sequence-to-sequence model, akin to language\ntranslation, or by uncovering the rudimentary principles of integration, such\nas $\\int_{0}^{x} t^n dt = \\frac{x^{n+1}}{n+1}$.",
        "pdf_link": "https://arxiv.org/pdf/2402.18040v1.pdf"
    },
    {
        "title": "Zero-shot sampling of adversarial entities in biomedical question answering",
        "authors": [
            "R. Patrick Xian",
            "Alex J. Lee",
            "Vincent Wang",
            "Qiming Cui",
            "Russell Ro",
            "Reza Abbasi-Asl"
        ],
        "published": "2024-02-16T09:29:38Z",
        "summary": "The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications. In\nhigh-stakes and knowledge-intensive tasks, understanding model vulnerabilities\nis essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples in natural language processing tasks raises questions about their\npotential guises in other settings. Here, we propose a powerscaled\ndistance-weighted sampling scheme in embedding space to discover diverse\nadversarial entities as distractors. We demonstrate its advantage over random\nsampling in adversarial question answering on biomedical topics. Our approach\nenables the exploration of different regions on the attack surface, which\nreveals two regimes of adversarial entities that markedly differ in their\ncharacteristics. Moreover, we show that the attacks successfully manipulate\ntoken-wise Shapley value explanations, which become deceptive in the\nadversarial setting. Our investigations illustrate the brittleness of domain\nknowledge in LLMs and reveal a shortcoming of standard evaluations for\nhigh-capacity models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10527v1.pdf"
    },
    {
        "title": "Introducing Bode: A Fine-Tuned Large Language Model for Portuguese Prompt-Based Task",
        "authors": [
            "Gabriel Lino Garcia",
            "Pedro Henrique Paiola",
            "Luis Henrique Morelli",
            "Giovani Candido",
            "Arnaldo C\u00e2ndido J\u00fanior",
            "Danilo Samuel Jodas",
            "Luis C. S. Afonso",
            "Ivan Rizzo Guilherme",
            "Bruno Elias Penteado",
            "Jo\u00e3o Paulo Papa"
        ],
        "published": "2024-01-05T17:15:01Z",
        "summary": "Large Language Models (LLMs) are increasingly bringing advances to Natural\nLanguage Processing. However, low-resource languages, those lacking extensive\nprominence in datasets for various NLP tasks, or where existing datasets are\nnot as substantial, such as Portuguese, already obtain several benefits from\nLLMs, but not to the same extent. LLMs trained on multilingual datasets\nnormally struggle to respond to prompts in Portuguese satisfactorily,\npresenting, for example, code switching in their responses. This work proposes\na fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two\nversions: 7B and 13B. We evaluate the performance of this model in\nclassification tasks using the zero-shot approach with in-context learning, and\ncompare it with other LLMs. Our main contribution is to bring an LLM with\nsatisfactory results in the Portuguese language, as well as to provide a model\nthat is free for research or commercial purposes.",
        "pdf_link": "https://arxiv.org/pdf/2401.02909v1.pdf"
    },
    {
        "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries",
        "authors": [
            "Jonathan F\u00fcrst",
            "Catherine Kosten",
            "Farhard Nooralahzadeh",
            "Yi Zhang",
            "Kurt Stockinger"
        ],
        "published": "2024-02-13T10:28:57Z",
        "summary": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an\nincreasingly popular solution for bridging the gap between user capabilities\nand SQL-based data access. These systems translate user requests in natural\nlanguage to valid SQL statements for a specific database. Recent Text-to-SQL\nsystems have benefited from the rapid improvement of transformer-based language\nmodels. However, while Text-to-SQL systems that incorporate such models\ncontinuously reach new high scores on -- often synthetic -- benchmark datasets,\na systematic exploration of their robustness towards different data models in a\nreal-world, realistic scenario is notably missing. This paper provides the\nfirst in-depth evaluation of the data model robustness of Text-to-SQL systems\nin practice based on a multi-year international project focused on Text-to-SQL\ninterfaces. Our evaluation is based on a real-world deployment of FootballDB, a\nsystem that was deployed over a 9 month period in the context of the FIFA World\nCup 2022, during which about 6K natural language questions were asked and\nexecuted. All of our data is based on real user questions that were asked live\nto the system. We manually labeled and translated a subset of these questions\nfor three different data models. For each data model, we explore the\nperformance of representative Text-to-SQL systems and language models. We\nfurther quantify the impact of training data size, pre-, and post-processing\nsteps as well as language model inference time. Our comprehensive evaluation\nsheds light on the design choices of real-world Text-to-SQL systems and their\nimpact on moving from research prototypes to real deployments. Last, we provide\na new benchmark dataset to the community, which is the first to enable the\nevaluation of different data models for the same dataset and is substantially\nmore challenging than most previous datasets in terms of query complexity.",
        "pdf_link": "https://arxiv.org/pdf/2402.08349v1.pdf"
    },
    {
        "title": "Learning to Watermark LLM-generated Text via Reinforcement Learning",
        "authors": [
            "Xiaojun Xu",
            "Yuanshun Yao",
            "Yang Liu"
        ],
        "published": "2024-03-13T03:43:39Z",
        "summary": "We study how to watermark LLM outputs, i.e. embedding algorithmically\ndetectable signals into LLM-generated text to track misuse. Unlike the current\nmainstream methods that work with a fixed LLM, we expand the watermark design\nspace by including the LLM tuning stage in the watermark pipeline. While prior\nworks focus on token-level watermark that embeds signals into the output, we\ndesign a model-level watermark that embeds signals into the LLM weights, and\nsuch signals can be detected by a paired detector. We propose a co-training\nframework based on reinforcement learning that iteratively (1) trains a\ndetector to detect the generated watermarked text and (2) tunes the LLM to\ngenerate text easily detectable by the detector while keeping its normal\nutility. We empirically show that our watermarks are more accurate, robust, and\nadaptable (to new attacks). It also allows watermarked model open-sourcing. In\naddition, if used together with alignment, the extra overhead introduced is low\n- only training an extra reward model (i.e. our detector). We hope our work can\nbring more effort into studying a broader watermark design that is not limited\nto working with a fixed LLM. We open-source the code:\nhttps://github.com/xiaojunxu/learning-to-watermark-llm .",
        "pdf_link": "https://arxiv.org/pdf/2403.10553v1.pdf"
    },
    {
        "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
        "authors": [
            "Cheonsu Jeong"
        ],
        "published": "2024-01-01T06:22:04Z",
        "summary": "Recent releases of pre-trained Large Language Models (LLMs) have gained\nconsiderable traction, yet research on fine-tuning and employing\ndomain-specific LLMs remains scarce. This study investigates approaches for\nfine-tuning and leveraging domain-specific LLMs, highlighting trends in LLMs,\nfoundational models, and methods for domain-specific pre-training. Focusing on\nthe financial sector, it details dataset selection, preprocessing, model\nchoice, and considerations crucial for LLM fine-tuning in finance. Addressing\nthe unique characteristics of financial data, the study explores the\nconstruction of domain-specific vocabularies and considerations for security\nand regulatory compliance. In the practical application of LLM fine-tuning, the\nstudy outlines the procedure and implementation for generating domain-specific\nLLMs in finance. Various financial cases, including stock price prediction,\nsentiment analysis of financial news, automated document processing, research,\ninformation extraction, and customer service enhancement, are exemplified. The\nstudy explores the potential of LLMs in the financial domain, identifies\nlimitations, and proposes directions for improvement, contributing valuable\ninsights for future research. Ultimately, it advances natural language\nprocessing technology in business, suggesting proactive LLM utilization in\nfinancial services across industries.",
        "pdf_link": "https://arxiv.org/pdf/2401.02981v2.pdf"
    },
    {
        "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
        "authors": [
            "Armin Toroghi",
            "Willis Guo",
            "Mohammad Mahdi Abdollah Pour",
            "Scott Sanner"
        ],
        "published": "2024-03-03T04:22:13Z",
        "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural\nLanguage questions using the relational information stored in Knowledge Graphs\n(KGs). With the recent advancements of Large Language Models (LLMs) and their\nremarkable reasoning abilities, there is a growing trend to leverage them for\nKGQA. However, existing methodologies have only focused on answering factual\nquestions, e.g., \"In which city was Silvio Berlusconi's first wife born?\",\nleaving questions involving commonsense reasoning that real-world users may\npose more often, e.g., \"Do I need separate visas to see the Venus of Willendorf\nand attend the Olympics this summer?\" unaddressed. In this work, we first\nobserve that existing LLM-based methods for KGQA struggle with hallucination on\nsuch questions, especially on queries targeting long-tail entities (e.g.,\nnon-mainstream and recent entities), thus hindering their applicability in\nreal-world applications especially since their reasoning processes are not\neasily verifiable. In response, we propose Right for Right Reasons (R3), a\ncommonsense KGQA methodology that allows for a verifiable reasoning procedure\nby axiomatically surfacing intrinsic commonsense knowledge of LLMs and\ngrounding every factual reasoning step on KG triples. Through experimental\nevaluations across three different tasks--question answering, claim\nverification, and preference matching--our findings showcase R3 as a superior\napproach, outperforming existing methodologies and notably reducing instances\nof hallucination and reasoning errors.",
        "pdf_link": "https://arxiv.org/pdf/2403.01390v1.pdf"
    },
    {
        "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
        "authors": [
            "Lei Zhu",
            "Fangyun Wei",
            "Yanye Lu"
        ],
        "published": "2024-03-12T17:59:51Z",
        "summary": "In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.",
        "pdf_link": "https://arxiv.org/pdf/2403.07874v1.pdf"
    },
    {
        "title": "RECOST: External Knowledge Guided Data-efficient Instruction Tuning",
        "authors": [
            "Qi Zhang",
            "Yiming Zhang",
            "Haobo Wang",
            "Junbo Zhao"
        ],
        "published": "2024-02-27T09:47:36Z",
        "summary": "In the current landscape of large language models (LLMs), the process of\ninstruction tuning serves as an essential step. Considering the high computing\npower overhead, data-efficient instruction tuning was proposed to reduce the\ntraining data size in this process, aiming at selecting high-quality\ninstructional data. Nevertheless, we argue that most current data-efficient\ninstruction-tuning methods are highly dependent on the quality of the original\ninstruction-tuning dataset. When it comes to datasets synthesized by LLMs, a\ncommon scenario in this field, dirty samples will even be selected with a\nhigher probability than other samples. To address these challenges, we utilized\nexternal knowledge (relevant examples or paragraphs) to evaluate those samples\nsynthesized by LLMs with an in-context-based relative predictive entropy. Based\non the new metric, we proposed a framework, dubbed as \\textbf{RECOST}, which\nintegrates external-knowledge-base re-ranking and diversity-consistent sampling\ninto a single pipeline. Through extensive experiments on several synthetic\ndatasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our\nmethod and achieve even better results with only \\textbf{1\\%} of the full\ndataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.17355v1.pdf"
    },
    {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "authors": [
            "Yang Liu",
            "Jiahuan Cao",
            "Chongyu Liu",
            "Kai Ding",
            "Lianwen Jin"
        ],
        "published": "2024-02-28T04:35:51Z",
        "summary": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.18041v1.pdf"
    },
    {
        "title": "ToMBench: Benchmarking Theory of Mind in Large Language Models",
        "authors": [
            "Zhuang Chen",
            "Jincenzi Wu",
            "Jinfeng Zhou",
            "Bosi Wen",
            "Guanqun Bi",
            "Gongyao Jiang",
            "Yaru Cao",
            "Mengting Hu",
            "Yunghwei Lai",
            "Zexuan Xiong",
            "Minlie Huang"
        ],
        "published": "2024-02-23T02:05:46Z",
        "summary": "Theory of Mind (ToM) is the cognitive capability to perceive and ascribe\nmental states to oneself and others. Recent research has sparked a debate over\nwhether large language models (LLMs) exhibit a form of ToM. However, existing\nToM evaluations are hindered by challenges such as constrained scope,\nsubjective judgment, and unintended contamination, yielding inadequate\nassessments. To address this gap, we introduce ToMBench with three key\ncharacteristics: a systematic evaluation framework encompassing 8 tasks and 31\nabilities in social cognition, a multiple-choice question format to support\nautomated and unbiased evaluation, and a build-from-scratch bilingual inventory\nto strictly avoid data leakage. Based on ToMBench, we conduct extensive\nexperiments to evaluate the ToM performance of 10 popular LLMs across tasks and\nabilities. We find that even the most advanced LLMs like GPT-4 lag behind human\nperformance by over 10% points, indicating that LLMs have not achieved a\nhuman-level theory of mind yet. Our aim with ToMBench is to enable an efficient\nand effective evaluation of LLMs' ToM capabilities, thereby facilitating the\ndevelopment of LLMs with inherent social intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2402.15052v1.pdf"
    },
    {
        "title": "Lost in Overlap: Exploring Watermark Collision in LLMs",
        "authors": [
            "Yiyang Luo",
            "Ke Lin",
            "Chao Gu"
        ],
        "published": "2024-03-15T05:06:21Z",
        "summary": "The proliferation of large language models (LLMs) in generating content\nraises concerns about text copyright. Watermarking methods, particularly\nlogit-based approaches, embed imperceptible identifiers into text to address\nthese challenges. However, the widespread use of watermarking across diverse\nLLMs has led to an inevitable issue known as watermark collision during common\ntasks like question answering and paraphrasing. This study focuses on dual\nwatermark collisions, where two watermarks are present simultaneously in the\nsame text. The research demonstrates that watermark collision poses a threat to\ndetection performance for detectors of both upstream and downstream watermark\nalgorithms.",
        "pdf_link": "https://arxiv.org/pdf/2403.10020v1.pdf"
    },
    {
        "title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation",
        "authors": [
            "Kohei Uehara",
            "Nabarun Goswami",
            "Hanqin Wang",
            "Toshiaki Baba",
            "Kohtaro Tanaka",
            "Tomohiro Hashimoto",
            "Kai Wang",
            "Rei Ito",
            "Takagi Naoya",
            "Ryo Umagami",
            "Yingyi Wen",
            "Tanachai Anakewat",
            "Tatsuya Harada"
        ],
        "published": "2024-01-18T14:21:56Z",
        "summary": "The increasing demand for intelligent systems capable of interpreting and\nreasoning about visual content requires the development of Large Multi-Modal\nModels (LMMs) that are not only accurate but also have explicit reasoning\ncapabilities. This paper presents a novel approach to imbue an LMM with the\nability to conduct explicit reasoning based on visual content and textual\ninstructions. We introduce a system that can ask a question to acquire\nnecessary knowledge, thereby enhancing the robustness and explicability of the\nreasoning process. Our method comprises the development of a novel dataset\ngenerated by a Large Language Model (LLM), designed to promote chain-of-thought\nreasoning combined with a question-asking mechanism. We designed an LMM, which\nhas high capabilities on region awareness to address the intricate requirements\nof image-text alignment. The model undergoes a three-stage training phase,\nstarting with large-scale image-text alignment using a large-scale datasets,\nfollowed by instruction tuning, and fine-tuning with a focus on\nchain-of-thought reasoning. The results demonstrate a stride toward a more\nrobust, accurate, and interpretable LMM, capable of reasoning explicitly and\nseeking information proactively when confronted with ambiguous visual input.",
        "pdf_link": "https://arxiv.org/pdf/2401.10005v1.pdf"
    },
    {
        "title": "Monitoring the evolution of antisemitic discourse on extremist social media using BERT",
        "authors": [
            "Raza Ul Mustafa",
            "Nathalie Japkowicz"
        ],
        "published": "2024-02-06T20:34:49Z",
        "summary": "Racism and intolerance on social media contribute to a toxic online\nenvironment which may spill offline to foster hatred, and eventually lead to\nphysical violence. That is the case with online antisemitism, the specific\ncategory of hatred considered in this study. Tracking antisemitic themes and\ntheir associated terminology over time in online discussions could help monitor\nthe sentiments of their participants and their evolution, and possibly offer\navenues for intervention that may prevent the escalation of hatred. Due to the\nlarge volume and constant evolution of online traffic, monitoring conversations\nmanually is impractical. Instead, we propose an automated method that extracts\nantisemitic themes and terminology from extremist social media over time and\ncaptures their evolution. Since supervised learning would be too limited for\nsuch a task, we created an unsupervised online machine learning approach that\nuses large language models to assess the contextual similarity of posts. The\nmethod clusters similar posts together, dividing, and creating additional\nclusters over time when sub-themes emerge from existing ones or new themes\nappear. The antisemitic terminology used within each theme is extracted from\nthe posts in each cluster. Our experiments show that our methodology\noutperforms existing baselines and demonstrates the kind of themes and\nsub-themes it discovers within antisemitic discourse along with their\nassociated terminology. We believe that our approach will be useful for\nmonitoring the evolution of all kinds of hatred beyond antisemitism on social\nplatforms.",
        "pdf_link": "https://arxiv.org/pdf/2403.05548v1.pdf"
    },
    {
        "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
        "authors": [
            "Michimasa Inaba",
            "Mariko Ukiyo",
            "Keiko Takamizo"
        ],
        "published": "2024-02-20T06:05:36Z",
        "summary": "Mental health care poses an increasingly serious challenge to modern\nsocieties. In this context, there has been a surge in research that utilizes\ninformation technologies to address mental health problems, including those\naiming to develop counseling dialogue systems. However, there is a need for\nmore evaluations of the performance of counseling dialogue systems that use\nlarge language models. For this study, we collected counseling dialogue data\nvia role-playing scenarios involving expert counselors, and the utterances were\nannotated with the intentions of the counselors. To determine the feasibility\nof a dialogue system in real-world counseling scenarios, third-party counselors\nevaluated the appropriateness of responses from human counselors and those\ngenerated by GPT-4 in identical contexts in role-play dialogue data. Analysis\nof the evaluation results showed that the responses generated by GPT-4 were\ncompetitive with those of human counselors.",
        "pdf_link": "https://arxiv.org/pdf/2402.12738v1.pdf"
    },
    {
        "title": "On-the-fly Definition Augmentation of LLMs for Biomedical NER",
        "authors": [
            "Monica Munnangi",
            "Sergey Feldman",
            "Byron C Wallace",
            "Silvio Amir",
            "Tom Hope",
            "Aakanksha Naik"
        ],
        "published": "2024-03-29T20:59:27Z",
        "summary": "Despite their general capabilities, LLMs still struggle on biomedical NER\ntasks, which are difficult due to the presence of specialized terminology and\nlack of training data. In this work we set out to improve LLM performance on\nbiomedical NER in limited data settings via a new knowledge augmentation\napproach which incorporates definitions of relevant concepts on-the-fly. During\nthis process, to provide a test bed for knowledge augmentation, we perform a\ncomprehensive exploration of prompting strategies. Our experiments show that\ndefinition augmentation is useful for both open source and closed LLMs. For\nexample, it leads to a relative improvement of 15\\% (on average) in GPT-4\nperformance (F1) across all (six) of our test datasets. We conduct extensive\nablations and analyses to demonstrate that our performance improvements stem\nfrom adding relevant definitional knowledge. We find that careful prompting\nstrategies also improve LLM performance, allowing them to outperform fine-tuned\nlanguage models in few-shot settings. To facilitate future research in this\ndirection, we release our code at https://github.com/allenai/beacon.",
        "pdf_link": "https://arxiv.org/pdf/2404.00152v1.pdf"
    },
    {
        "title": "A StrongREJECT for Empty Jailbreaks",
        "authors": [
            "Alexandra Souly",
            "Qingyuan Lu",
            "Dillon Bowen",
            "Tu Trinh",
            "Elvis Hsieh",
            "Sana Pandey",
            "Pieter Abbeel",
            "Justin Svegliato",
            "Scott Emmons",
            "Olivia Watkins",
            "Sam Toyer"
        ],
        "published": "2024-02-15T18:58:09Z",
        "summary": "The rise of large language models (LLMs) has drawn attention to the existence\nof \"jailbreaks\" that allow the models to be used maliciously. However, there is\nno standard benchmark for measuring the severity of a jailbreak, leaving\nauthors of jailbreak papers to create their own. We show that these benchmarks\noften include vague or unanswerable questions and use grading criteria that are\nbiased towards overestimating the misuse potential of low-quality model\nresponses. Some jailbreak techniques make the problem worse by decreasing the\nquality of model responses even on benign questions: we show that several\njailbreaking techniques substantially reduce the zero-shot performance of GPT-4\non MMLU. Jailbreaks can also make it harder to elicit harmful responses from an\n\"uncensored\" open-source model. We present a new benchmark, StrongREJECT, which\nbetter discriminates between effective and ineffective jailbreaks by using a\nhigher-quality question set and a more accurate response grading algorithm. We\nshow that our new grading scheme better accords with human judgment of response\nquality and overall jailbreak effectiveness, especially on the sort of\nlow-quality responses that contribute the most to over-estimation of jailbreak\nperformance on existing benchmarks. We release our code and data at\nhttps://github.com/alexandrasouly/strongreject.",
        "pdf_link": "https://arxiv.org/pdf/2402.10260v1.pdf"
    },
    {
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
        "authors": [
            "Renxi Wang",
            "Haonan Li",
            "Xudong Han",
            "Yixuan Zhang",
            "Timothy Baldwin"
        ],
        "published": "2024-02-18T17:10:07Z",
        "summary": "Large language models (LLMs) have achieved success in acting as agents, which\ninteract with environments through tools like search engines. However, LLMs are\nnot optimized specifically for tool use during training or alignment, limiting\ntheir effectiveness as agents. To resolve this problem, previous work has\ncollected interaction trajectories between GPT-4 and environments, and\nfine-tuned smaller models with them. As part of this, the standard approach has\nbeen to simply discard trajectories that do not finish the task successfully,\nwhich, on the one hand, leads to a significant waste of data and resources, and\non the other hand, has the potential to limit the possible optimization paths\nduring fine-tuning. In this paper, we contend that large language models can\nlearn from failures through appropriate data cleaning and fine-tuning\nstrategies. We conduct experiments on mathematical reasoning, multi-hop\nquestion answering, and strategic question answering tasks. Experimental\nresults demonstrate that compared to solely using positive examples,\nincorporating negative examples enhances model performance by a large margin.",
        "pdf_link": "https://arxiv.org/pdf/2402.11651v1.pdf"
    },
    {
        "title": "LLMs for Robotic Object Disambiguation",
        "authors": [
            "Connie Jiang",
            "Yiqing Xu",
            "David Hsu"
        ],
        "published": "2024-01-07T04:46:23Z",
        "summary": "The advantages of pre-trained large language models (LLMs) are apparent in a\nvariety of language processing tasks. But can a language model's knowledge be\nfurther harnessed to effectively disambiguate objects and navigate\ndecision-making challenges within the realm of robotics? Our study reveals the\nLLM's aptitude for solving complex decision making challenges that are often\npreviously modeled by Partially Observable Markov Decision Processes (POMDPs).\nA pivotal focus of our research is the object disambiguation capability of\nLLMs. We detail the integration of an LLM into a tabletop environment\ndisambiguation task, a decision making problem where the robot's task is to\ndiscern and retrieve a user's desired object from an arbitrarily large and\ncomplex cluster of objects. Despite multiple query attempts with zero-shot\nprompt engineering (details can be found in the Appendix), the LLM struggled to\ninquire about features not explicitly provided in the scene description. In\nresponse, we have developed a few-shot prompt engineering system to improve the\nLLM's ability to pose disambiguating queries. The result is a model capable of\nboth using given features when they are available and inferring new relevant\nfeatures when necessary, to successfully generate and navigate down a precise\ndecision tree to the correct object--even when faced with identical options.",
        "pdf_link": "https://arxiv.org/pdf/2401.03388v1.pdf"
    },
    {
        "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks",
        "authors": [
            "Jing Yu Koh",
            "Robert Lo",
            "Lawrence Jang",
            "Vikram Duvvur",
            "Ming Chong Lim",
            "Po-Yu Huang",
            "Graham Neubig",
            "Shuyan Zhou",
            "Ruslan Salakhutdinov",
            "Daniel Fried"
        ],
        "published": "2024-01-24T18:35:21Z",
        "summary": "Autonomous agents capable of planning, reasoning, and executing actions on\nthe web offer a promising avenue for automating computer tasks. However, the\nmajority of existing benchmarks primarily focus on text-based agents,\nneglecting many natural tasks that require visual information to effectively\nsolve. Given that most computer interfaces cater to human perception, visual\ninformation often augments textual data in ways that text-only models struggle\nto harness effectively. To bridge this gap, we introduce VisualWebArena, a\nbenchmark designed to assess the performance of multimodal web agents on\nrealistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set\nof diverse and complex web-based tasks that evaluate various capabilities of\nautonomous multimodal agents. To perform on this benchmark, agents need to\naccurately process image-text inputs, interpret natural language instructions,\nand execute actions on websites to accomplish user-defined objectives. We\nconduct an extensive evaluation of state-of-the-art LLM-based autonomous\nagents, including several multimodal models. Through extensive quantitative and\nqualitative analysis, we identify several limitations of text-only LLM agents,\nand reveal gaps in the capabilities of state-of-the-art multimodal language\nagents. VisualWebArena provides a framework for evaluating multimodal\nautonomous language agents, and offers insights towards building stronger\nautonomous agents for the web. Our code, baseline models, and data is publicly\navailable at https://jykoh.com/vwa.",
        "pdf_link": "https://arxiv.org/pdf/2401.13649v1.pdf"
    },
    {
        "title": "FinLlama: Financial Sentiment Classification for Algorithmic Trading Applications",
        "authors": [
            "Thanos Konstantinidis",
            "Giorgos Iacovides",
            "Mingxue Xu",
            "Tony G. Constantinides",
            "Danilo Mandic"
        ],
        "published": "2024-03-18T22:11:00Z",
        "summary": "There are multiple sources of financial news online which influence market\nmovements and trader's decisions. This highlights the need for accurate\nsentiment analysis, in addition to having appropriate algorithmic trading\ntechniques, to arrive at better informed trading decisions. Standard lexicon\nbased sentiment approaches have demonstrated their power in aiding financial\ndecisions. However, they are known to suffer from issues related to context\nsensitivity and word ordering. Large Language Models (LLMs) can also be used in\nthis context, but they are not finance-specific and tend to require significant\ncomputational resources. To facilitate a finance specific LLM framework, we\nintroduce a novel approach based on the Llama 2 7B foundational model, in order\nto benefit from its generative nature and comprehensive language manipulation.\nThis is achieved by fine-tuning the Llama2 7B model on a small portion of\nsupervised financial sentiment analysis data, so as to jointly handle the\ncomplexities of financial lexicon and context, and further equipping it with a\nneural network based decision mechanism. Such a generator-classifier scheme,\nreferred to as FinLlama, is trained not only to classify the sentiment valence\nbut also quantify its strength, thus offering traders a nuanced insight into\nfinancial news articles. Complementing this, the implementation of\nparameter-efficient fine-tuning through LoRA optimises trainable parameters,\nthus minimising computational and memory requirements, without sacrificing\naccuracy. Simulation results demonstrate the ability of the proposed FinLlama\nto provide a framework for enhanced portfolio management decisions and\nincreased market returns. These results underpin the ability of FinLlama to\nconstruct high-return portfolios which exhibit enhanced resilience, even during\nvolatile periods and unpredictable market events.",
        "pdf_link": "https://arxiv.org/pdf/2403.12285v1.pdf"
    },
    {
        "title": "Pedagogical Alignment of Large Language Models",
        "authors": [
            "Shashank Sonkar",
            "Kangqi Ni",
            "Sapana Chaudhary",
            "Richard G. Baraniuk"
        ],
        "published": "2024-02-07T16:15:59Z",
        "summary": "In this paper, we introduce the novel concept of pedagogically aligned Large\nLanguage Models (LLMs) that signifies a transformative shift in the application\nof LLMs within educational contexts. Rather than providing direct responses to\nuser queries, pedagogically-aligned LLMs function as scaffolding tools,\nbreaking complex problems into manageable subproblems and guiding students\ntowards the final answer through constructive feedback and hints. The objective\nis to equip learners with problem-solving strategies that deepen their\nunderstanding and internalization of the subject matter. Previous research in\nthis field has primarily applied the supervised finetuning approach without\nframing the objective as an alignment problem, hence not employing\nreinforcement learning through human feedback (RLHF) methods. This study\nreinterprets the narrative by viewing the task through the lens of alignment\nand demonstrates how RLHF methods emerge naturally as a superior alternative\nfor aligning LLM behaviour. Building on this perspective, we propose a novel\napproach for constructing a reward dataset specifically designed for the\npedagogical alignment of LLMs. We apply three state-of-the-art RLHF algorithms\nand find that they outperform SFT significantly. Our qualitative analyses\nacross model differences and hyperparameter sensitivity further validate the\nsuperiority of RLHF over SFT. Also, our study sheds light on the potential of\nonline feedback for enhancing the performance of pedagogically-aligned LLMs,\nthus providing valuable insights for the advancement of these models in\neducational settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.05000v1.pdf"
    },
    {
        "title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models",
        "authors": [
            "Zhengxin Zhang",
            "Dan Zhao",
            "Xupeng Miao",
            "Gabriele Oliaro",
            "Qing Li",
            "Yong Jiang",
            "Zhihao Jia"
        ],
        "published": "2024-01-13T21:00:21Z",
        "summary": "Finetuning large language models (LLMs) has been empirically effective on a\nvariety of downstream tasks. Existing approaches to finetuning an LLM either\nfocus on parameter-efficient finetuning, which only updates a small number of\ntrainable parameters, or attempt to reduce the memory footprint during the\ntraining phase of the finetuning. Typically, the memory footprint during\nfinetuning stems from three contributors: model weights, optimizer states, and\nintermediate activations. However, existing works still require considerable\nmemory and none can simultaneously mitigate memory footprint for all three\nsources. In this paper, we present Quantized Side Tuing (QST), which enables\nmemory-efficient and fast finetuning of LLMs by operating through a dual-stage\nprocess. First, QST quantizes an LLM's model weights into 4-bit to reduce the\nmemory footprint of the LLM's original weights; QST also introduces a side\nnetwork separated from the LLM, which utilizes the hidden states of the LLM to\nmake task-specific predictions. Using a separate side network avoids performing\nbackpropagation through the LLM, thus reducing the memory requirement of the\nintermediate activations. Furthermore, QST leverages several low-rank adaptors\nand gradient-free downsample modules to significantly reduce the trainable\nparameters, so as to save the memory footprint of the optimizer states.\nExperiments show that QST can reduce the total memory footprint by up to 2.3\n$\\times$ and speed up the finetuning process by up to 3 $\\times$ while\nachieving competent performance compared with the state-of-the-art. When it\ncomes to full finetuning, QST can reduce the total memory footprint up to 7\n$\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2401.07159v1.pdf"
    },
    {
        "title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models",
        "authors": [
            "Yuzhao Heng",
            "Chunyuan Deng",
            "Yitong Li",
            "Yue Yu",
            "Yinghao Li",
            "Rongzhi Zhang",
            "Chao Zhang"
        ],
        "published": "2024-03-17T06:12:43Z",
        "summary": "Although Large Language Models (LLMs) exhibit remarkable adaptability across\ndomains, these models often fall short in structured knowledge extraction tasks\nsuch as named entity recognition (NER). This paper explores an innovative,\ncost-efficient strategy to harness LLMs with modest NER capabilities for\nproducing superior NER datasets. Our approach diverges from the basic\nclass-conditional prompts by instructing LLMs to self-reflect on the specific\ndomain, thereby generating domain-relevant attributes (such as category and\nemotions for movie reviews), which are utilized for creating attribute-rich\ntraining data. Furthermore, we preemptively generate entity terms and then\ndevelop NER context data around these entities, effectively bypassing the LLMs'\nchallenges with complex structures. Our experiments across both general and\nniche domains reveal significant performance enhancements over conventional\ndata generation methods while being more cost-effective than existing\nalternatives.",
        "pdf_link": "https://arxiv.org/pdf/2403.11103v1.pdf"
    },
    {
        "title": "Is Context Helpful for Chat Translation Evaluation?",
        "authors": [
            "Sweta Agrawal",
            "Amin Farajian",
            "Patrick Fernandes",
            "Ricardo Rei",
            "Andr\u00e9 F. T. Martins"
        ],
        "published": "2024-03-13T07:49:50Z",
        "summary": "Despite the recent success of automatic metrics for assessing translation\nquality, their application in evaluating the quality of machine-translated\nchats has been limited. Unlike more structured texts like news, chat\nconversations are often unstructured, short, and heavily reliant on contextual\ninformation. This poses questions about the reliability of existing\nsentence-level metrics in this domain as well as the role of context in\nassessing the translation quality. Motivated by this, we conduct a\nmeta-evaluation of existing sentence-level automatic metrics, primarily\ndesigned for structured domains such as news, to assess the quality of\nmachine-translated chats. We find that reference-free metrics lag behind\nreference-based ones, especially when evaluating translation quality in\nout-of-English settings. We then investigate how incorporating conversational\ncontextual information in these metrics affects their performance. Our findings\nshow that augmenting neural learned metrics with contextual information helps\nimprove correlation with human judgments in the reference-free scenario and\nwhen evaluating translations in out-of-English settings. Finally, we propose a\nnew evaluation metric, Context-MQM, that utilizes bilingual context with a\nlarge language model (LLM) and further validate that adding context helps even\nfor LLM-based evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.08314v1.pdf"
    },
    {
        "title": "DocFinQA: A Long-Context Financial Reasoning Dataset",
        "authors": [
            "Varshini Reddy",
            "Rik Koncel-Kedziorski",
            "Viet Dac Lai",
            "Michael Krumdick",
            "Charles Lovering",
            "Chris Tanner"
        ],
        "published": "2024-01-12T22:19:22Z",
        "summary": "For large language models (LLMs) to be effective in the financial domain --\nwhere each decision can have a significant impact -- it is necessary to\ninvestigate realistic tasks and data. Financial professionals often interact\nwith documents that are hundreds of pages long, but most financial research\ndatasets only deal with short excerpts from these documents. To address this,\nwe introduce a long-document financial QA task. We augment 7,437 questions from\nthe existing FinQA dataset with the full-document context, extending the\naverage context length from under 700 words in FinQA to 123k words in DocFinQA.\nWe conduct extensive experiments over retrieval-based QA pipelines and\nlong-context language models. DocFinQA proves a significant challenge for even\nstate-of-the-art systems. We also provide a case-study on the longest documents\nin DocFinQA and find that models particularly struggle on these documents.\nAddressing these challenges may have a wide reaching impact across applications\nwhere specificity and long-range contexts are critical, like gene sequences and\nlegal document contract analysis.",
        "pdf_link": "https://arxiv.org/pdf/2401.06915v2.pdf"
    },
    {
        "title": "Is Cognition and Action Consistent or Not: Investigating Large Language Model's Personality",
        "authors": [
            "Yiming Ai",
            "Zhiwei He",
            "Ziyin Zhang",
            "Wenhong Zhu",
            "Hongkun Hao",
            "Kai Yu",
            "Lingjun Chen",
            "Rui Wang"
        ],
        "published": "2024-02-22T16:32:08Z",
        "summary": "In this study, we investigate the reliability of Large Language Models (LLMs)\nin professing human-like personality traits through responses to personality\nquestionnaires. Our goal is to evaluate the consistency between LLMs' professed\npersonality inclinations and their actual \"behavior\", examining the extent to\nwhich these models can emulate human-like personality patterns. Through a\ncomprehensive analysis of LLM outputs against established human benchmarks, we\nseek to understand the cognition-action divergence in LLMs and propose\nhypotheses for the observed results based on psychological theories and\nmetrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.14679v1.pdf"
    },
    {
        "title": "Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations",
        "authors": [
            "Swapnaja Achintalwar",
            "Ioana Baldini",
            "Djallel Bouneffouf",
            "Joan Byamugisha",
            "Maria Chang",
            "Pierre Dognin",
            "Eitan Farchi",
            "Ndivhuwo Makondo",
            "Aleksandra Mojsilovic",
            "Manish Nagireddy",
            "Karthikeyan Natesan Ramamurthy",
            "Inkit Padhi",
            "Orna Raz",
            "Jesus Rios",
            "Prasanna Sattigeri",
            "Moninder Singh",
            "Siphiwe Thwala",
            "Rosario A. Uceda-Sosa",
            "Kush R. Varshney"
        ],
        "published": "2024-03-08T21:26:49Z",
        "summary": "The alignment of large language models is usually done by model providers to\nadd or control behaviors that are common or universally understood across use\ncases and contexts. In contrast, in this article, we present an approach and\narchitecture that empowers application developers to tune a model to their\nparticular values, social norms, laws and other regulations, and orchestrate\nbetween potentially conflicting requirements in context. We lay out three main\ncomponents of such an Alignment Studio architecture: Framers, Instructors, and\nAuditors that work in concert to control the behavior of a language model. We\nillustrate this approach with a running example of aligning a company's\ninternal-facing enterprise chatbot to its business conduct guidelines.",
        "pdf_link": "https://arxiv.org/pdf/2403.09704v1.pdf"
    },
    {
        "title": "Can Generative Agents Predict Emotion?",
        "authors": [
            "Ciaran Regan",
            "Nanami Iwahashi",
            "Shogo Tanaka",
            "Mizuki Oka"
        ],
        "published": "2024-02-06T18:39:43Z",
        "summary": "Large Language Models (LLMs) have demonstrated a number of human-like\nabilities, however the empathic understanding and emotional state of LLMs is\nyet to be aligned to that of humans. In this work, we investigate how the\nemotional state of generative LLM agents evolves as they perceive new events,\nintroducing a novel architecture in which new experiences are compared to past\nmemories. Through this comparison, the agent gains the ability to understand\nnew experiences in context, which according to the appraisal theory of emotion\nis vital in emotion creation. First, the agent perceives new experiences as\ntime series text data. After perceiving each new input, the agent generates a\nsummary of past relevant memories, referred to as the norm, and compares the\nnew experience to this norm. Through this comparison we can analyse how the\nagent reacts to the new experience in context. The PANAS, a test of affect, is\nadministered to the agent, capturing the emotional state of the agent after the\nperception of the new event. Finally, the new experience is then added to the\nagents memory to be used in the creation of future norms. By creating multiple\nexperiences in natural language from emotionally charged situations, we test\nthe proposed architecture on a wide range of scenarios. The mixed results\nsuggests that introducing context can occasionally improve the emotional\nalignment of the agent, but further study and comparison with human evaluators\nis necessary. We hope that this paper is another step towards the alignment of\ngenerative agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.04232v2.pdf"
    },
    {
        "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
        "authors": [
            "Mauricio Rivera",
            "Jean-Fran\u00e7ois Godbout",
            "Reihaneh Rabbany",
            "Kellin Pelrine"
        ],
        "published": "2024-01-13T16:36:58Z",
        "summary": "Large Language Models have emerged as prime candidates to tackle\nmisinformation mitigation. However, existing approaches struggle with\nhallucinations and overconfident predictions. We propose an uncertainty\nquantification framework that leverages both direct confidence elicitation and\nsampled-based consistency methods to provide better calibration for NLP\nmisinformation mitigation solutions. We first investigate the calibration of\nsample-based consistency methods that exploit distinct features of consistency\nacross sample sizes and stochastic levels. Next, we evaluate the performance\nand distributional shift of a robust numeric verbalization prompt across single\nvs. two-step confidence elicitation procedure. We also compare the performance\nof the same prompt with different versions of GPT and different numerical\nscales. Finally, we combine the sample-based consistency and verbalized methods\nto propose a hybrid framework that yields a better uncertainty estimation for\nGPT models. Overall, our work proposes novel uncertainty quantification methods\nthat will improve the reliability of Large Language Models in misinformation\nmitigation applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.08694v2.pdf"
    },
    {
        "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark",
        "authors": [
            "Ge Zhang",
            "Xinrun Du",
            "Bei Chen",
            "Yiming Liang",
            "Tongxu Luo",
            "Tianyu Zheng",
            "Kang Zhu",
            "Yuyang Cheng",
            "Chunpu Xu",
            "Shuyue Guo",
            "Haoran Zhang",
            "Xingwei Qu",
            "Junjie Wang",
            "Ruibin Yuan",
            "Yizhi Li",
            "Zekun Wang",
            "Yudong Liu",
            "Yu-Hsuan Tsai",
            "Fengji Zhang",
            "Chenghua Lin",
            "Wenhao Huang",
            "Wenhu Chen",
            "Jie Fu"
        ],
        "published": "2024-01-22T13:34:34Z",
        "summary": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.",
        "pdf_link": "https://arxiv.org/pdf/2401.11944v2.pdf"
    },
    {
        "title": "Distributed agency in second language learning and teaching through generative AI",
        "authors": [
            "Robert Godwin-Jones"
        ],
        "published": "2024-03-29T14:55:40Z",
        "summary": "Generative AI offers significant opportunities for language learning. Tools\nlike ChatGPT can provide informal second language practice through chats in\nwritten or voice forms, with the learner specifying through prompts\nconversational parameters such as proficiency level, language register, and\ndiscussion topics. AI can be instructed to give corrective feedback, create\npractice exercises, or develop an extended study plan. Instructors can use AI\nto build learning and assessment materials in a variety of media. AI is likely\nto make immersive technologies more powerful and versatile, moving away from\nscripted interactions. For both learners and teachers, it is important to\nunderstand the limitations of AI systems that arise from their purely\nstatistical model of human language, which limits their ability to deal with\nnuanced social and cultural aspects of language use. Additionally, there are\nethical concerns over how AI systems are created as well as practical\nconstraints in their use, especially for less privileged populations. The power\nand versatility of AI tools are likely to turn them into valuable and constant\ncompanions in many peoples lives (akin to smartphones), creating a close\nconnection that goes beyond simple tool use. Ecological theories such as\nsociomaterialism are helpful in examining the shared agency that develops\nthrough close user-AI interactions, as are the perspectives on human-object\nrelations from Indigenous cultures.",
        "pdf_link": "https://arxiv.org/pdf/2403.20216v1.pdf"
    },
    {
        "title": "Prompting open-source and commercial language models for grammatical error correction of English learner text",
        "authors": [
            "Christopher Davis",
            "Andrew Caines",
            "\u00d8istein Andersen",
            "Shiva Taslimipoor",
            "Helen Yannakoudakis",
            "Zheng Yuan",
            "Christopher Bryant",
            "Marek Rei",
            "Paula Buttery"
        ],
        "published": "2024-01-15T14:19:47Z",
        "summary": "Thanks to recent advances in generative AI, we are able to prompt large\nlanguage models (LLMs) to produce texts which are fluent and grammatical. In\naddition, it has been shown that we can elicit attempts at grammatical error\ncorrection (GEC) from LLMs when prompted with ungrammatical input sentences. We\nevaluate how well LLMs can perform at GEC by measuring their performance on\nestablished benchmark datasets. We go beyond previous studies, which only\nexamined GPT* models on a selection of English GEC datasets, by evaluating\nseven open-source and three commercial LLMs on four established GEC benchmarks.\nWe investigate model performance and report results against individual error\ntypes. Our results indicate that LLMs do not always outperform supervised\nEnglish GEC models except in specific contexts -- namely commercial LLMs on\nbenchmarks annotated with fluency corrections as opposed to minimal edits. We\nfind that several open-source models outperform commercial ones on minimal edit\nbenchmarks, and that in some settings zero-shot prompting is just as\ncompetitive as few-shot prompting.",
        "pdf_link": "https://arxiv.org/pdf/2401.07702v1.pdf"
    },
    {
        "title": "FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial Analysis",
        "authors": [
            "Chao Zhang",
            "Yuren Mao",
            "Yijiang Fan",
            "Yu Mi",
            "Yunjun Gao",
            "Lu Chen",
            "Dongfang Lou",
            "Jinshu Lin"
        ],
        "published": "2024-01-19T05:48:07Z",
        "summary": "Text-to-SQL, which provides zero-code interface for operating relational\ndatabases, has gained much attention in financial analysis; because, financial\nprofessionals may not well-skilled in SQL programming. However, until now,\nthere is no practical Text-to-SQL benchmark dataset for financial analysis, and\nexisting Text-to-SQL methods have not considered the unique characteristics of\ndatabases in financial applications, such as commonly existing wide tables. To\naddress these issues, we collect a practical Text-to-SQL benchmark dataset and\npropose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL\nframework for financial analysis. The benchmark dataset, BULL, is collected\nfrom the practical financial analysis business of Hundsun Technologies Inc.,\nincluding databases for fund, stock, and macro economy. Besides, the proposed\nLLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for\nfinancial Text-to-SQL from the perspectives of prompt construction,\nparameter-efficient fine-tuning and output calibration. Extensive experimental\nresults on BULL demonstrate that FinSQL achieves the state-of-the-art\nText-to-SQL performance at a small cost; furthermore, FinSQL can bring up to\n36.64% performance improvement in scenarios requiring few-shot cross-database\nmodel transfer.",
        "pdf_link": "https://arxiv.org/pdf/2401.10506v1.pdf"
    },
    {
        "title": "Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting",
        "authors": [
            "Jiaheng Wei",
            "Yuanshun Yao",
            "Jean-Francois Ton",
            "Hongyi Guo",
            "Andrew Estornell",
            "Yang Liu"
        ],
        "published": "2024-02-16T02:32:06Z",
        "summary": "LLM hallucination, i.e. generating factually incorrect yet seemingly\nconvincing answers, is currently a major threat to the trustworthiness and\nreliability of LLMs. The first step towards solving this complicated problem is\nto measure it. However, existing hallucination metrics require to have a\nbenchmark dataset with gold-standard answers, i.e. \"best\" or \"correct\" answers\nwritten by humans. Such requirement makes hallucination measurement costly and\nprone to human errors. In this work, we propose Factualness Evaluations via\nWeighting LLMs (FEWL), the first hallucination metric that is specifically\ndesigned for the scenario when gold-standard answers are absent. FEWL leverages\nthe answers from off-the-shelf LLMs that serve as a proxy of gold-standard\nanswers. The key challenge is how to quantify the expertise of reference LLMs\nresourcefully. We show FEWL has certain theoretical guarantees and demonstrate\nempirically it gives more accurate hallucination measures than naively using\nreference LLMs. We also show how to leverage FEWL to reduce hallucination\nthrough both in-context learning and supervised finetuning. Last, we build a\nlarge-scale benchmark dataset to facilitate LLM hallucination research.",
        "pdf_link": "https://arxiv.org/pdf/2402.10412v1.pdf"
    },
    {
        "title": "Contextual Moral Value Alignment Through Context-Based Aggregation",
        "authors": [
            "Pierre Dognin",
            "Jesus Rios",
            "Ronny Luss",
            "Inkit Padhi",
            "Matthew D Riemer",
            "Miao Liu",
            "Prasanna Sattigeri",
            "Manish Nagireddy",
            "Kush R. Varshney",
            "Djallel Bouneffouf"
        ],
        "published": "2024-03-19T15:06:53Z",
        "summary": "Developing value-aligned AI agents is a complex undertaking and an ongoing\nchallenge in the field of AI. Specifically within the domain of Large Language\nModels (LLMs), the capability to consolidate multiple independently trained\ndialogue agents, each aligned with a distinct moral value, into a unified\nsystem that can adapt to and be aligned with multiple moral values is of\nparamount importance. In this paper, we propose a system that does contextual\nmoral value alignment based on contextual aggregation. Here, aggregation is\ndefined as the process of integrating a subset of LLM responses that are best\nsuited to respond to a user input, taking into account features extracted from\nthe user's input. The proposed system shows better results in term of alignment\nto human value compared to the state of the art.",
        "pdf_link": "https://arxiv.org/pdf/2403.12805v1.pdf"
    },
    {
        "title": "FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs",
        "authors": [
            "Eun Cheol Choi",
            "Emilio Ferrara"
        ],
        "published": "2024-02-08T18:43:05Z",
        "summary": "Our society is facing rampant misinformation harming public health and trust.\nTo address the societal challenge, we introduce FACT-GPT, a system leveraging\nLarge Language Models (LLMs) to automate the claim matching stage of\nfact-checking. FACT-GPT, trained on a synthetic dataset, identifies social\nmedia content that aligns with, contradicts, or is irrelevant to previously\ndebunked claims. Our evaluation shows that our specialized LLMs can match the\naccuracy of larger models in identifying related claims, closely mirroring\nhuman judgment. This research provides an automated solution for efficient\nclaim matching, demonstrates the potential of LLMs in supporting fact-checkers,\nand offers valuable resources for further research in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.05904v1.pdf"
    },
    {
        "title": "Evaluating Large Language Models on the GMAT: Implications for the Future of Business Education",
        "authors": [
            "Vahid Ashrafimoghari",
            "Necdet G\u00fcrkan",
            "Jordan W. Suchow"
        ],
        "published": "2024-01-02T03:54:50Z",
        "summary": "The rapid evolution of artificial intelligence (AI), especially in the domain\nof Large Language Models (LLMs) and generative AI, has opened new avenues for\napplication across various fields, yet its role in business education remains\nunderexplored. This study introduces the first benchmark to assess the\nperformance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and\nGPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models\n(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission\nprocess for graduate business programs. Our analysis shows that most LLMs\noutperform human candidates, with GPT-4 Turbo not only outperforming the other\nmodels but also surpassing the average scores of graduate students at top\nbusiness schools. Through a case study, this research examines GPT-4 Turbo's\nability to explain answers, evaluate responses, identify errors, tailor\ninstructions, and generate alternative scenarios. The latest LLM versions,\nGPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in\nreasoning tasks compared to their predecessors, underscoring their potential\nfor complex problem-solving. While AI's promise in education, assessment, and\ntutoring is clear, challenges remain. Our study not only sheds light on LLMs'\nacademic potential but also emphasizes the need for careful development and\napplication of AI in education. As AI technology advances, it is imperative to\nestablish frameworks and protocols for AI interaction, verify the accuracy of\nAI-generated content, ensure worldwide access for diverse learners, and create\nan educational environment where AI supports human expertise. This research\nsets the stage for further exploration into the responsible use of AI to enrich\neducational experiences and improve exam preparation and assessment methods.",
        "pdf_link": "https://arxiv.org/pdf/2401.02985v1.pdf"
    },
    {
        "title": "Word Embeddings Revisited: Do LLMs Offer Something New?",
        "authors": [
            "Matthew Freestone",
            "Shubhra Kanti Karmaker Santu"
        ],
        "published": "2024-02-16T21:47:30Z",
        "summary": "Learning meaningful word embeddings is key to training a robust language\nmodel. The recent rise of Large Language Models (LLMs) has provided us with\nmany new word/sentence/document embedding models. Although LLMs have shown\nremarkable advancement in various NLP tasks, it is still unclear whether the\nperformance improvement is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper\nsystematically investigates this issue by comparing classical word embedding\ntechniques against LLM-based word embeddings in terms of their latent vector\nsemantics. Our results show that LLMs tend to cluster semantically related\nwords more tightly than classical models. LLMs also yield higher average\naccuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally,\nsome LLMs tend to produce word embeddings similar to SBERT, a relatively\nlighter classical model.",
        "pdf_link": "https://arxiv.org/pdf/2402.11094v2.pdf"
    },
    {
        "title": "Disambiguate Entity Matching through Relation Discovery with Large Language Models",
        "authors": [
            "Zezhou Huang"
        ],
        "published": "2024-03-26T03:07:32Z",
        "summary": "Entity matching is a critical challenge in data integration and cleaning,\ncentral to tasks like fuzzy joins and deduplication. Traditional approaches\nhave focused on overcoming fuzzy term representations through methods such as\nedit distance, Jaccard similarity, and more recently, embeddings and deep\nneural networks, including advancements from large language models (LLMs) like\nGPT. However, the core challenge in entity matching extends beyond term\nfuzziness to the ambiguity in defining what constitutes a \"match,\" especially\nwhen integrating with external databases. This ambiguity arises due to varying\nlevels of detail and granularity among entities, complicating exact matches. We\npropose a novel approach that shifts focus from purely identifying semantic\nsimilarities to understanding and defining the \"relations\" between entities as\ncrucial for resolving ambiguities in matching. By predefining a set of\nrelations relevant to the task at hand, our method allows analysts to navigate\nthe spectrum of similarity more effectively, from exact matches to conceptually\nrelated entities.",
        "pdf_link": "https://arxiv.org/pdf/2403.17344v1.pdf"
    },
    {
        "title": "Improving LLM Code Generation with Grammar Augmentation",
        "authors": [
            "Shubham Ugare",
            "Tarun Suresh",
            "Hangoo Kang",
            "Sasa Misailovic",
            "Gagandeep Singh"
        ],
        "published": "2024-03-03T22:38:35Z",
        "summary": "We present SynCode a novel framework for efficient and general syntactical\ndecoding of code with large language models (LLMs). SynCode leverages the\ngrammar of a programming language, utilizing an offline-constructed efficient\nlookup table called DFA mask store based on language grammar terminals. We\ndemonstrate SynCode's soundness and completeness given the context-free grammar\n(CFG) of the programming language, presenting its ability to retain\nsyntactically valid tokens while rejecting invalid ones. The framework\nseamlessly integrates with any language defined by CFG, as evidenced by\nexperiments on CFGs for Python and Go. The results underscore the significant\nreduction of 96.07% of syntax errors achieved when SynCode is combined with\nstate-of-the-art LLMs, showcasing its substantial impact on enhancing\nsyntactical precision in code generation.\n  Our code is available at https://github.com/uiuc-focal-lab/syncode.",
        "pdf_link": "https://arxiv.org/pdf/2403.01632v1.pdf"
    },
    {
        "title": "Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction",
        "authors": [
            "Chen Chen",
            "Lei Li",
            "Marcel Beetz",
            "Abhirup Banerjee",
            "Ramneek Gupta",
            "Vicente Grau"
        ],
        "published": "2024-03-15T13:25:09Z",
        "summary": "Heart failure (HF) poses a significant public health challenge, with a rising\nglobal mortality rate. Early detection and prevention of HF could significantly\nreduce its impact. We introduce a novel methodology for predicting HF risk\nusing 12-lead electrocardiograms (ECGs). We present a novel, lightweight\ndual-attention ECG network designed to capture complex ECG features essential\nfor early HF risk prediction, despite the notable imbalance between low and\nhigh-risk groups. This network incorporates a cross-lead attention module and\ntwelve lead-specific temporal attention modules, focusing on cross-lead\ninteractions and each lead's local dynamics. To further alleviate model\noverfitting, we leverage a large language model (LLM) with a public ECG-Report\ndataset for pretraining on an ECG-report alignment task. The network is then\nfine-tuned for HF risk prediction using two specific cohorts from the UK\nBiobank study, focusing on patients with hypertension (UKB-HYP) and those who\nhave had a myocardial infarction (UKB-MI).The results reveal that LLM-informed\npre-training substantially enhances HF risk prediction in these cohorts. The\ndual-attention design not only improves interpretability but also predictive\naccuracy, outperforming existing competitive methods with C-index scores of\n0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's\npotential in advancing HF risk assessment with clinical complex ECG data.",
        "pdf_link": "https://arxiv.org/pdf/2403.10581v2.pdf"
    },
    {
        "title": "Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs",
        "authors": [
            "Xiaobin Zhang",
            "Liangjun Zang",
            "Qianwen Liu",
            "Shuchong Wei",
            "Songlin Hu"
        ],
        "published": "2024-03-22T15:16:10Z",
        "summary": "Event temporal relation (TempRel) is a primary subject of the event relation\nextraction task. However, the inherent ambiguity of TempRel increases the\ndifficulty of the task. With the rise of prompt engineering, it is important to\ndesign effective prompt templates and verbalizers to extract relevant\nknowledge. The traditional manually designed templates struggle to extract\nprecise temporal knowledge. This paper introduces a novel retrieval-augmented\nTempRel extraction approach, leveraging knowledge retrieved from large language\nmodels (LLMs) to enhance prompt templates and verbalizers. Our method\ncapitalizes on the diverse capabilities of various LLMs to generate a wide\narray of ideas for template and verbalizer design. Our proposed method fully\nexploits the potential of LLMs for generation tasks and contributes more\nknowledge to our design. Empirical evaluations across three widely recognized\ndatasets demonstrate the efficacy of our method in improving the performance of\nevent temporal relation extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.15273v1.pdf"
    },
    {
        "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
        "authors": [
            "Yu Wang",
            "Xiaogeng Liu",
            "Yu Li",
            "Muhao Chen",
            "Chaowei Xiao"
        ],
        "published": "2024-03-14T15:57:13Z",
        "summary": "With the advent and widespread deployment of Multimodal Large Language Models\n(MLLMs), the imperative to ensure their safety has become increasingly\npronounced. However, with the integration of additional modalities, MLLMs are\nexposed to new vulnerabilities, rendering them prone to structured-based\njailbreak attacks, where semantic content (e.g., \"harmful text\") has been\ninjected into the images to mislead MLLMs. In this work, we aim to defend\nagainst such threats. Specifically, we propose \\textbf{Ada}ptive\n\\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with\ndefense prompts to defend MLLMs against structure-based jailbreak attacks\nwithout fine-tuning MLLMs or training additional modules (e.g., post-stage\ncontent detector). Initially, we present a manually designed static defense\nprompt, which thoroughly examines the image and instruction content step by\nstep and specifies response methods to malicious queries. Furthermore, we\nintroduce an adaptive auto-refinement framework, consisting of a target MLLM\nand a LLM-based defense prompt generator (Defender). These components\ncollaboratively and iteratively communicate to generate a defense prompt.\nExtensive experiments on the popular structure-based jailbreak attacks and\nbenign datasets show that our methods can consistently improve MLLMs'\nrobustness against structure-based jailbreak attacks without compromising the\nmodel's general capabilities evaluated on standard benign tasks. Our code is\navailable at https://github.com/rain305f/AdaShield.",
        "pdf_link": "https://arxiv.org/pdf/2403.09513v1.pdf"
    },
    {
        "title": "Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators",
        "authors": [
            "Yinhong Liu",
            "Han Zhou",
            "Zhijiang Guo",
            "Ehsan Shareghi",
            "Ivan Vuli\u0107",
            "Anna Korhonen",
            "Nigel Collier"
        ],
        "published": "2024-03-25T17:11:28Z",
        "summary": "Large Language Models (LLMs) have demonstrated promising capabilities as\nautomatic evaluators in assessing the quality of generated natural language.\nHowever, LLMs still exhibit biases in evaluation and often struggle to generate\ncoherent evaluations that align with human assessments. In this work, we first\nconduct a systematic study of the misalignment between LLM evaluators and human\njudgement, revealing that existing calibration methods aimed at mitigating\nbiases are insufficient for effectively aligning LLM evaluators. Inspired by\nthe use of preference data in RLHF, we formulate the evaluation as a ranking\nproblem and introduce Pairwise-preference Search (PairS), an uncertainty-guided\nsearch method that employs LLMs to conduct pairwise comparisons and efficiently\nranks candidate texts. PairS achieves state-of-the-art performance on\nrepresentative evaluation tasks and demonstrates significant improvements over\ndirect scoring. Furthermore, we provide insights into the role of pairwise\npreference in quantifying the transitivity of LLMs and demonstrate how PairS\nbenefits from calibration.",
        "pdf_link": "https://arxiv.org/pdf/2403.16950v2.pdf"
    },
    {
        "title": "ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys",
        "authors": [
            "Yue Niu",
            "Saurav Prakash",
            "Salman Avestimehr"
        ],
        "published": "2024-03-01T19:24:37Z",
        "summary": "We propose a new attention mechanism with linear complexity, ATP, that\nfixates \\textbf{A}ttention on \\textbf{T}op \\textbf{P}rincipal keys, rather than\non each individual token. Particularly, ATP is driven by an important\nobservation that input sequences are typically low-rank, i.e., input sequences\ncan be represented by a few principal bases. Therefore, instead of directly\niterating over all the input tokens, ATP transforms inputs into an orthogonal\nspace and computes attention only on the top principal bases (keys). Owing to\nthe observed low-rank structure in input sequences, ATP is able to capture\nsemantic relationships in input sequences with a few principal keys.\nFurthermore, the attention complexity is reduced from \\emph{quadratic} to\n\\emph{linear} without incurring a noticeable performance drop. ATP further\nreduces complexity for other linear layers with low-rank inputs, leading to\nmore speedup compared to prior works that solely target the attention module.\nOur evaluations on various models (e.g., BERT and Llama) demonstrate that ATP\nachieves comparable accuracy with much lower computation and memory complexity\nthan the standard attention mechanism. In particular, ATP barely loses accuracy\nwith only $1/2$ principal keys, and only incurs around $2\\%$ accuracy drops\nwith $1/4$ principal keys.",
        "pdf_link": "https://arxiv.org/pdf/2403.02352v1.pdf"
    },
    {
        "title": "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation",
        "authors": [
            "Zhenyu Wang",
            "Enze Xie",
            "Aoxue Li",
            "Zhongdao Wang",
            "Xihui Liu",
            "Zhenguo Li"
        ],
        "published": "2024-01-28T16:18:39Z",
        "summary": "Despite significant advancements in text-to-image models for generating\nhigh-quality images, these methods still struggle to ensure the controllability\nof text prompts over images in the context of complex text prompts, especially\nwhen it comes to retaining object attributes and relationships. In this paper,\nwe propose CompAgent, a training-free approach for compositional text-to-image\ngeneration, with a large language model (LLM) agent as its core. The\nfundamental idea underlying CompAgent is premised on a divide-and-conquer\nmethodology. Given a complex text prompt containing multiple concepts including\nobjects, attributes, and relationships, the LLM agent initially decomposes it,\nwhich entails the extraction of individual objects, their associated\nattributes, and the prediction of a coherent scene layout. These individual\nobjects can then be independently conquered. Subsequently, the agent performs\nreasoning by analyzing the text, plans and employs the tools to compose these\nisolated objects. The verification and human feedback mechanism is finally\nincorporated into our agent to further correct the potential attribute errors\nand refine the generated images. Guided by the LLM agent, we propose a\ntuning-free multi-concept customization model and a layout-to-image generation\nmodel as the tools for concept composition, and a local image editing method as\nthe tool to interact with the agent for verification. The scene layout controls\nthe image generation process among these tools to prevent confusion among\nmultiple objects. Extensive experiments demonstrate the superiority of our\napproach for compositional text-to-image generation: CompAgent achieves more\nthan 10\\% improvement on T2I-CompBench, a comprehensive benchmark for\nopen-world compositional T2I generation. The extension to various related tasks\nalso illustrates the flexibility of our CompAgent for potential applications.",
        "pdf_link": "https://arxiv.org/pdf/2401.15688v2.pdf"
    },
    {
        "title": "Are self-explanations from Large Language Models faithful?",
        "authors": [
            "Andreas Madsen",
            "Sarath Chandar",
            "Siva Reddy"
        ],
        "published": "2024-01-15T19:39:15Z",
        "summary": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will\neven explain their reasoning, so-called self-explanations. However, convincing\nand wrong self-explanations can lead to unsupported confidence in LLMs, thus\nincreasing risk. Therefore, it's important to measure if self-explanations\ntruly reflect the model's behavior. Such a measure is called\ninterpretability-faithfulness and is challenging to perform since the ground\ntruth is inaccessible, and many LLMs only have an inference API. To address\nthis, we propose employing self-consistency checks to measure faithfulness. For\nexample, if an LLM says a set of words is important for making a prediction,\nthen it should not be able to make its prediction without these words. While\nself-consistency checks are a common approach to faithfulness, they have not\npreviously been successfully applied to LLM self-explanations for\ncounterfactual, importance measure, and redaction explanations. Our results\ndemonstrate that faithfulness is explanation, model, and task-dependent,\nshowing self-explanations should not be trusted in general. For example, with\nsentiment classification, counterfactuals are more faithful for Llama2,\nimportance measures for Mistral, and redaction for Falcon 40B.",
        "pdf_link": "https://arxiv.org/pdf/2401.07927v3.pdf"
    },
    {
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
        "authors": [
            "Zirui Liu",
            "Jiayi Yuan",
            "Hongye Jin",
            "Shaochen Zhong",
            "Zhaozhuo Xu",
            "Vladimir Braverman",
            "Beidi Chen",
            "Xia Hu"
        ],
        "published": "2024-02-05T06:06:47Z",
        "summary": "Efficiently serving large language models (LLMs) requires batching many\nrequests together to reduce the cost per request. Yet, the key-value (KV)\ncache, which stores attention keys and values to avoid re-computations,\nsignificantly increases memory demands and becomes the new bottleneck in speed\nand memory usage. This memory demand increases with larger batch sizes and\nlonger context lengths. Additionally, the inference speed is limited by the\nsize of KV cache, as the GPU's SRAM must load the entire KV cache from the main\nGPU memory for each token generated, causing the computational core to be idle\nduring this process. A straightforward and effective solution to reduce KV\ncache size is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly\nimplementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to\nmaintain almost the same quality while using $\\mathbf{2.6\\times}$ less peak\nmemory usage (including the model weight). This reduction in memory usage\nenables up to $\\mathbf{4\\times}$ larger batch size, bringing\n$\\mathbf{2.35\\times \\sim 3.47\\times}$ throughput on real LLM inference\nworkload. The source code is available at https://github.com/jy-yuan/KIVI.",
        "pdf_link": "https://arxiv.org/pdf/2402.02750v1.pdf"
    },
    {
        "title": "Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation",
        "authors": [
            "Chenming Tang",
            "Zhixiang Wang",
            "Yunfang Wu"
        ],
        "published": "2024-03-28T10:13:34Z",
        "summary": "In-context learning (ICL) is the trending prompting strategy in the era of\nlarge language models (LLMs), where a few examples are demonstrated to evoke\nLLMs' power for a given task. How to select informative examples remains an\nopen issue. Previous works on in-context example selection for machine\ntranslation (MT) focus on superficial word-level features while ignoring deep\nsyntax-level knowledge. In this paper, we propose a syntax-based in-context\nexample selection method for MT, by computing the syntactic similarity between\ndependency trees using Polynomial Distance. In addition, we propose an ensemble\nstrategy combining examples selected by both word-level and syntax-level\ncriteria. Experimental results between English and 6 common languages indicate\nthat syntax can effectively enhancing ICL for MT, obtaining the highest COMET\nscores on 11 out of 12 translation directions.",
        "pdf_link": "https://arxiv.org/pdf/2403.19285v1.pdf"
    },
    {
        "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
        "authors": [
            "Xinglin Lyu",
            "Junhui Li",
            "Yanqing Zhao",
            "Min Zhang",
            "Daimeng Wei",
            "Shimin Tao",
            "Hao Yang",
            "Min Zhang"
        ],
        "published": "2024-02-23T09:01:00Z",
        "summary": "Generally, the decoder-only large language models (LLMs) are adapted to\ncontext-aware neural machine translation (NMT) in a concatenating way, where\nLLMs take the concatenation of the source sentence (i.e., intra-sentence\ncontext) and the inter-sentence context as the input, and then to generate the\ntarget tokens sequentially. This adaptation strategy, i.e., concatenation mode,\nconsiders intra-sentence and inter-sentence contexts with the same priority,\ndespite an apparent difference between the two kinds of contexts. In this\npaper, we propose an alternative adaptation approach, named Decoding-enhanced\nMulti-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and\nutilize the inter- and intra-sentence context and more effectively adapt LLMs\nto context-aware NMT. First, DeMPT divides the context-aware NMT process into\nthree separate phases. During each phase, different continuous prompts are\nintroduced to make LLMs discriminately model various information. Second, DeMPT\nemploys a heuristic way to further discriminately enhance the utilization of\nthe source-side inter- and intra-sentence information at the final decoding\nphase. Experiments show that our approach significantly outperforms the\nconcatenation method, and further improves the performance of LLMs in discourse\nmodeling.",
        "pdf_link": "https://arxiv.org/pdf/2402.15200v1.pdf"
    },
    {
        "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
        "authors": [
            "Yixuan Tang",
            "Yi Yang"
        ],
        "published": "2024-01-27T11:41:48Z",
        "summary": "Retrieval-augmented generation (RAG) augments large language models (LLM) by\nretrieving relevant knowledge, showing promising potential in mitigating LLM\nhallucinations and enhancing response quality, thereby facilitating the great\nadoption of LLMs in practice. However, we find that existing RAG systems are\ninadequate in answering multi-hop queries, which require retrieving and\nreasoning over multiple pieces of supporting evidence. Furthermore, to our\nknowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.\nIn this paper, we develop a novel dataset, MultiHop-RAG, which consists of a\nknowledge base, a large collection of multi-hop queries, their ground-truth\nanswers, and the associated supporting evidence. We detail the procedure of\nbuilding the dataset, utilizing an English news article dataset as the\nunderlying RAG knowledge base. We demonstrate the benchmarking utility of\nMultiHop-RAG in two experiments. The first experiment compares different\nembedding models for retrieving evidence for multi-hop queries. In the second\nexperiment, we examine the capabilities of various state-of-the-art LLMs,\nincluding GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop\nqueries given the evidence. Both experiments reveal that existing RAG methods\nperform unsatisfactorily in retrieving and answering multi-hop queries. We hope\nMultiHop-RAG will be a valuable resource for the community in developing\neffective RAG systems, thereby facilitating greater adoption of LLMs in\npractice. The MultiHop-RAG and implemented RAG system is publicly available at\nhttps://github.com/yixuantt/MultiHop-RAG/.",
        "pdf_link": "https://arxiv.org/pdf/2401.15391v1.pdf"
    },
    {
        "title": "ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine Translation",
        "authors": [
            "Shaojie Dai",
            "Xin Liu",
            "Ping Luo",
            "Yue Yu"
        ],
        "published": "2024-03-11T14:10:57Z",
        "summary": "Large language model (LLM) has achieved promising performance in multilingual\nmachine translation tasks through zero/few-shot prompts or prompt-tuning.\nHowever, due to the mixture of multilingual data during the pre-training of\nLLM, the LLM-based translation models face the off-target issue in both\nprompt-based methods, including a series of phenomena, namely instruction\nmisunderstanding, translation with wrong language and over-generation. For this\nissue, this paper introduces an\n\\textbf{\\underline{A}}uto-\\textbf{\\underline{C}}onstriction\n\\textbf{\\underline{T}}urning mechanism for \\textbf{\\underline{M}}ultilingual\n\\textbf{\\underline{N}}eural \\textbf{\\underline{M}}achine\n\\textbf{\\underline{T}}ranslation (\\model), which is a novel supervised\nfine-tuning mechanism and orthogonal to the traditional prompt-based methods.\nIn this method, \\model automatically constructs a constrained template in the\ntarget side by adding trigger tokens ahead of the ground truth. Furthermore,\ntrigger tokens can be arranged and combined freely to represent different task\nsemantics, and they can be iteratively updated to maximize the label\nlikelihood. Experiments are performed on WMT test sets with multiple metrics,\nand the experimental results demonstrate that \\model achieves substantially\nimproved performance across multiple translation directions and reduce the\noff-target phenomena in the translation.",
        "pdf_link": "https://arxiv.org/pdf/2403.06745v1.pdf"
    },
    {
        "title": "Machine Unlearning of Pre-trained Large Language Models",
        "authors": [
            "Jin Yao",
            "Eli Chien",
            "Minxin Du",
            "Xinyao Niu",
            "Tianhao Wang",
            "Zezhou Cheng",
            "Xiang Yue"
        ],
        "published": "2024-02-23T07:43:26Z",
        "summary": "This study investigates the concept of the `right to be forgotten' within the\ncontext of large language models (LLMs). We explore machine unlearning as a\npivotal solution, with a focus on pre-trained models--a notably\nunder-researched area. Our research delineates a comprehensive framework for\nmachine unlearning in pre-trained LLMs, encompassing a critical analysis of\nseven diverse unlearning methods. Through rigorous evaluation using curated\ndatasets from arXiv, books, and GitHub, we establish a robust benchmark for\nunlearning performance, demonstrating that these methods are over $10^5$ times\nmore computationally efficient than retraining. Our results show that\nintegrating gradient ascent with gradient descent on in-distribution data\nimproves hyperparameter robustness. We also provide detailed guidelines for\nefficient hyperparameter tuning in the unlearning process. Our findings advance\nthe discourse on ethical AI practices, offering substantive insights into the\nmechanics of machine unlearning for pre-trained LLMs and underscoring the\npotential for responsible AI development.",
        "pdf_link": "https://arxiv.org/pdf/2402.15159v2.pdf"
    },
    {
        "title": "Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model with Proxy",
        "authors": [
            "Yu Zhu",
            "Chuxiong Sun",
            "Wenfei Yang",
            "Wenqiang Wei",
            "Bo Tang",
            "Tianzhu Zhang",
            "Zhiyu Li",
            "Shifeng Zhang",
            "Feiyu Xiong",
            "Jie Hu",
            "Mingchuan yang"
        ],
        "published": "2024-03-07T07:31:00Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach\nto ensure Large Language Models (LLMs) align with human values. However,\nexisting RLHF methods require a high computational cost, one main reason being\nthat RLHF assigns both the generation and alignment tasks to the LLM\nsimultaneously. In this paper, we introduce Proxy-RLHF, which decouples the\ngeneration and alignment processes of LLMs, achieving alignment with human\nvalues at a much lower computational cost. We start with a novel Markov\nDecision Process (MDP) designed for the alignment process and employ\nReinforcement Learning (RL) to train a streamlined proxy model that oversees\nthe token generation of the LLM, without altering the LLM itself. Experiments\nshow that our method achieves a comparable level of alignment with only 1\\% of\nthe training parameters of other methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.04283v1.pdf"
    },
    {
        "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
        "authors": [
            "Mengting Wan",
            "Tara Safavi",
            "Sujay Kumar Jauhar",
            "Yujin Kim",
            "Scott Counts",
            "Jennifer Neville",
            "Siddharth Suri",
            "Chirag Shah",
            "Ryen W White",
            "Longqi Yang",
            "Reid Andersen",
            "Georg Buscher",
            "Dhruv Joshi",
            "Nagu Rangan"
        ],
        "published": "2024-03-18T18:45:28Z",
        "summary": "Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.12173v1.pdf"
    },
    {
        "title": "Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning Processes",
        "authors": [
            "Dingzirui Wang",
            "Longxu Dou",
            "Xuanliang Zhang",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "published": "2024-02-16T13:02:11Z",
        "summary": "Numerical reasoning is an essential ability for NLP systems to handle numeric\ninformation. Recent research indicates that fine-tuning a small-scale model to\nlearn generating reasoning processes alongside answers can significantly\nenhance performance. However, current methods have the limitation that most\nmethods generate reasoning processes with large language models (LLMs), which\nare \"unreliable\" since such processes could contain information unrelated to\nthe answer. To address this limitation, we introduce Enhancing NumeriCal\nreasOning with Reliable procEsses (Encore), which derives the reliable\nreasoning process by decomposing the answer formula, ensuring which fully\nsupports the answer. Nevertheless, models could lack enough data to learn the\nreasoning process generation adequately, since our method generates only one\nsingle reasoning process for one formula. To overcome this difficulty, we\npresent a series of pre-training tasks to help models learn the reasoning\nprocess generation with synthesized data. The experiments show that Encore\nyields improvement on all five experimental datasets with an average of 1.8%,\nproving the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2402.10654v1.pdf"
    },
    {
        "title": "Multi-dimensional Evaluation of Empathetic Dialog Responses",
        "authors": [
            "Zhichao Xu",
            "Jiepu Jiang"
        ],
        "published": "2024-02-18T00:32:33Z",
        "summary": "Empathy is a critical element of effective and satisfactory conversational\ncommunication, yet previous studies in measuring conversational empathy mostly\nfocus on expressed communicative intents -- in which way empathy is expressed,\nignoring the fact that conversation is also a collaborative practice involving\nboth speakers and listeners. In contrast, we propose a multi-dimensional\nempathy evaluation framework that extends upon existing work to measure both\nexpressed intents from the speaker's perspective and perceived empathy from the\nlistener's perspective. Applying the proposed framework to analyzing our\ninternal customer-service dialogue shows that the two dimensions (expressed\nintent types and perceived empathy) are inter-connected, while perceived\nempathy has high correlation with the satisfactory level of dialogue sessions.\nThis proposed framework still requires subjective assessments from trained\nannotators, which can be non-trivial to collect. To scale up evaluation without\nexcessive reliance on carefully annotated data, we explore different modeling\noptions to automatically measure conversational empathy with (1) prompting\nfrozen large language models (LLMs) and (2) training language model-based\nclassifiers. Extensive experiments on both internal and external dialogue\ndatasets show that measuring conversational empathy remains a challenging task\nfor prompting frozen LLMs, reflected by less satisfying performance of GPT-4\nand Flan family models. On the other hand, our proposed instruction-finetuned\nclassifiers based on sequence-to-sequence (Seq2Seq) language models is able to\nachieve the best performance compared to prior works and competitive baselines.\nFinally, we perform comprehensive ablation studies on the performance of\nproposed instruction-finetuned classifiers and give recommendations on\npotentially adopting them as automatic conversational empathy evaluation\nmetrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.11409v1.pdf"
    },
    {
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",
        "authors": [
            "Ziheng Jiang",
            "Haibin Lin",
            "Yinmin Zhong",
            "Qi Huang",
            "Yangrui Chen",
            "Zhi Zhang",
            "Yanghua Peng",
            "Xiang Li",
            "Cong Xie",
            "Shibiao Nong",
            "Yulu Jia",
            "Sun He",
            "Hongmin Chen",
            "Zhihao Bai",
            "Qi Hou",
            "Shipeng Yan",
            "Ding Zhou",
            "Yiyao Sheng",
            "Zhuo Jiang",
            "Haohan Xu",
            "Haoran Wei",
            "Zhang Zhang",
            "Pengfei Nie",
            "Leqi Zou",
            "Sida Zhao",
            "Liang Xiang",
            "Zherui Liu",
            "Zhe Li",
            "Xiaoying Jia",
            "Jianxi Ye",
            "Xin Jin",
            "Xin Liu"
        ],
        "published": "2024-02-23T22:10:59Z",
        "summary": "We present the design, implementation and engineering experience in building\nand deploying MegaScale, a production system for training large language models\n(LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale\nbrings unprecedented challenges to training efficiency and stability. We take a\nfull-stack approach that co-designs the algorithmic and system components\nacross model block and optimizer design, computation and communication\noverlapping, operator optimization, data pipeline, and network performance\ntuning. Maintaining high efficiency throughout the training process (i.e.,\nstability) is an important consideration in production given the long extent of\nLLM training jobs. Many hard stability issues only emerge at large scale, and\nin-depth observability is the key to address them. We develop a set of\ndiagnosis tools to monitor system components and events deep in the stack,\nidentify root causes, and derive effective techniques to achieve fault\ntolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs\nUtilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the\nMFU by 1.34x compared to Megatron-LM. We share our operational experience in\nidentifying and fixing failures and stragglers. We hope by articulating the\nproblems and sharing our experience from a systems perspective, this work can\ninspire future LLM systems research.",
        "pdf_link": "https://arxiv.org/pdf/2402.15627v1.pdf"
    },
    {
        "title": "Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models",
        "authors": [
            "Selim Sandal",
            "Ismail Akturk"
        ],
        "published": "2024-01-12T17:41:38Z",
        "summary": "The design and optimization of hardware have traditionally been\nresource-intensive, demanding considerable expertise and dependence on\nestablished design automation tools. This paper discusses the possibility of\nexploiting large language models to streamline the code generation process in\nhardware design. In contrast to earlier studies, this paper aims to use large\nlanguage models that accepts high-level design specifications through a single\nprompt to generate corresponding Register-Transfer Level (RTL) code. The\nability to use large language models on RTL code generation not only expedites\ndesign iteration cycles but also facilitates the exploration of design spaces\nthat have computational challenges for conventional techniques. Through our\nevaluation, we demonstrate the shortcoming of existing attention mechanisms,\nand present the abilities of language models to produce functional, optimized,\nand industry-standard compliant RTL code when a novel attention mechanism is\nused. These findings underscore the expanding role of large language models in\nshaping the future landscape of architectural exploration and automation in\nhardware design.",
        "pdf_link": "https://arxiv.org/pdf/2401.08683v1.pdf"
    },
    {
        "title": "Large Multimodal Agents: A Survey",
        "authors": [
            "Junlin Xie",
            "Zhihong Chen",
            "Ruifei Zhang",
            "Xiang Wan",
            "Guanbin Li"
        ],
        "published": "2024-02-23T06:04:23Z",
        "summary": "Large language models (LLMs) have achieved superior performance in powering\ntext-based AI agents, endowing them with decision-making and reasoning\nabilities akin to humans. Concurrently, there is an emerging research trend\nfocused on extending these LLM-powered AI agents into the multimodal domain.\nThis extension enables AI agents to interpret and respond to diverse multimodal\nuser queries, thereby handling more intricate and nuanced tasks. In this paper,\nwe conduct a systematic review of LLM-driven multimodal agents, which we refer\nto as large multimodal agents ( LMAs for short). First, we introduce the\nessential components involved in developing LMAs and categorize the current\nbody of research into four distinct types. Subsequently, we review the\ncollaborative frameworks integrating multiple LMAs , enhancing collective\nefficacy. One of the critical challenges in this field is the diverse\nevaluation methods used across existing studies, hindering effective comparison\namong different LMAs . Therefore, we compile these evaluation methodologies and\nestablish a comprehensive framework to bridge the gaps. This framework aims to\nstandardize evaluations, facilitating more meaningful comparisons. Concluding\nour review, we highlight the extensive applications of LMAs and propose\npossible future research directions. Our discussion aims to provide valuable\ninsights and guidelines for future research in this rapidly evolving field. An\nup-to-date resource list is available at\nhttps://github.com/jun0wanan/awesome-large-multimodal-agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.15116v1.pdf"
    },
    {
        "title": "Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code",
        "authors": [
            "Vahid Majdinasab",
            "Amin Nikanjam",
            "Foutse Khomh"
        ],
        "published": "2024-02-14T16:41:35Z",
        "summary": "Code auditing ensures that the developed code adheres to standards,\nregulations, and copyright protection by verifying that it does not contain\ncode from protected sources. The recent advent of Large Language Models (LLMs)\nas coding assistants in the software development process poses new challenges\nfor code auditing. The dataset for training these models is mainly collected\nfrom publicly available sources. This raises the issue of intellectual property\ninfringement as developers' codes are already included in the dataset.\nTherefore, auditing code developed using LLMs is challenging, as it is\ndifficult to reliably assert if an LLM used during development has been trained\non specific copyrighted codes, given that we do not have access to the training\ndatasets of these models. Given the non-disclosure of the training datasets,\ntraditional approaches such as code clone detection are insufficient for\nasserting copyright infringement. To address this challenge, we propose a new\napproach, TraWiC; a model-agnostic and interpretable method based on membership\ninference for detecting code inclusion in an LLM's training dataset. We extract\nsyntactic and semantic identifiers unique to each program to train a classifier\nfor detecting code inclusion. In our experiments, we observe that TraWiC is\ncapable of detecting 83.87% of codes that were used to train an LLM. In\ncomparison, the prevalent clone detection tool NiCad is only capable of\ndetecting 47.64%. In addition to its remarkable performance, TraWiC has low\nresource overhead in contrast to pair-wise clone detection that is conducted\nduring the auditing process of tools like CodeWhisperer reference tracker,\nacross thousands of code snippets.",
        "pdf_link": "https://arxiv.org/pdf/2402.09299v1.pdf"
    },
    {
        "title": "Unfamiliar Finetuning Examples Control How Language Models Hallucinate",
        "authors": [
            "Katie Kang",
            "Eric Wallace",
            "Claire Tomlin",
            "Aviral Kumar",
            "Sergey Levine"
        ],
        "published": "2024-03-08T18:28:13Z",
        "summary": "Large language models (LLMs) have a tendency to generate plausible-sounding\nyet factually incorrect responses, especially when queried on unfamiliar\nconcepts. In this work, we explore the underlying mechanisms that govern how\nfinetuned LLMs hallucinate. Our investigation reveals an interesting pattern:\nas inputs become more unfamiliar, LLM outputs tend to default towards a\n``hedged'' prediction, whose form is determined by how the unfamiliar examples\nin the finetuning data are supervised. Thus, by strategically modifying these\nexamples' supervision, we can control LLM predictions for unfamiliar inputs\n(e.g., teach them to say ``I don't know''). Based on these principles, we\ndevelop an RL approach that more reliably mitigates hallucinations for\nlong-form generation tasks, by tackling the challenges presented by reward\nmodel hallucinations. We validate our findings with a series of controlled\nexperiments in multiple-choice QA on MMLU, as well as long-form biography and\nbook/movie plot generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05612v1.pdf"
    },
    {
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "authors": [
            "Xing Han L\u00f9",
            "Zden\u011bk Kasner",
            "Siva Reddy"
        ],
        "published": "2024-02-08T18:58:02Z",
        "summary": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "pdf_link": "https://arxiv.org/pdf/2402.05930v1.pdf"
    },
    {
        "title": "BBA: Bi-Modal Behavioral Alignment for Reasoning with Large Vision-Language Models",
        "authors": [
            "Xueliang Zhao",
            "Xinting Huang",
            "Tingchen Fu",
            "Qintong Li",
            "Shansan Gong",
            "Lemao Liu",
            "Wei Bi",
            "Lingpeng Kong"
        ],
        "published": "2024-02-21T07:16:29Z",
        "summary": "Multimodal reasoning stands as a pivotal capability for large vision-language\nmodels (LVLMs). The integration with Domain-Specific Languages (DSL), offering\nprecise visual representations, equips these models with the opportunity to\nexecute more accurate reasoning in complex and professional domains. However,\nthe vanilla Chain-of-Thought (CoT) prompting method faces challenges in\neffectively leveraging the unique strengths of visual and DSL representations,\nprimarily due to their differing reasoning mechanisms. Additionally, it often\nfalls short in addressing critical steps in multi-step reasoning tasks. To\nmitigate these challenges, we introduce the \\underline{B}i-Modal\n\\underline{B}ehavioral \\underline{A}lignment (BBA) prompting method, designed\nto maximize the potential of DSL in augmenting complex multi-modal reasoning\ntasks. This method initiates by guiding LVLMs to create separate reasoning\nchains for visual and DSL representations. Subsequently, it aligns these chains\nby addressing any inconsistencies, thus achieving a cohesive integration of\nbehaviors from different modalities. Our experiments demonstrate that BBA\nsubstantially improves the performance of GPT-4V(ision) on geometry problem\nsolving ($28.34\\% \\to 34.22\\%$), chess positional advantage prediction\n($42.08\\% \\to 46.99\\%$) and molecular property prediction ($77.47\\% \\to\n83.52\\%$).",
        "pdf_link": "https://arxiv.org/pdf/2402.13577v1.pdf"
    },
    {
        "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks",
        "authors": [
            "Guy Amit",
            "Abigail Goldsteen",
            "Ariel Farkash"
        ],
        "published": "2024-03-13T12:46:51Z",
        "summary": "Natural language processing models have experienced a significant upsurge in\nrecent years, with numerous applications being built upon them. Many of these\napplications require fine-tuning generic base models on customized, proprietary\ndatasets. This fine-tuning data is especially likely to contain personal or\nsensitive information about individuals, resulting in increased privacy risk.\nMembership inference attacks are the most commonly employed attack to assess\nthe privacy leakage of a machine learning model. However, limited research is\navailable on the factors that affect the vulnerability of language models to\nthis kind of attack, or on the applicability of different defense strategies in\nthe language domain. We provide the first systematic review of the\nvulnerability of fine-tuned large language models to membership inference\nattacks, the various factors that come into play, and the effectiveness of\ndifferent defense strategies. We find that some training methods provide\nsignificantly reduced privacy risk, with the combination of differential\nprivacy and low-rank adaptors achieving the best privacy protection against\nthese attacks.",
        "pdf_link": "https://arxiv.org/pdf/2403.08481v1.pdf"
    },
    {
        "title": "An Empirical Analysis of Diversity in Argument Summarization",
        "authors": [
            "Michiel van der Meer",
            "Piek Vossen",
            "Catholijn M. Jonker",
            "Pradeep K. Murukannaiah"
        ],
        "published": "2024-02-02T16:26:52Z",
        "summary": "Presenting high-level arguments is a crucial task for fostering participation\nin online societal discussions. Current argument summarization approaches miss\nan important facet of this task -- capturing diversity -- which is important\nfor accommodating multiple perspectives. We introduce three aspects of\ndiversity: those of opinions, annotators, and sources. We evaluate approaches\nto a popular argument summarization task called Key Point Analysis, which shows\nhow these approaches struggle to (1) represent arguments shared by few people,\n(2) deal with data from various sources, and (3) align with subjectivity in\nhuman-provided annotations. We find that both general-purpose LLMs and\ndedicated KPA models exhibit this behavior, but have complementary strengths.\nFurther, we observe that diversification of training data may ameliorate\ngeneralization. Addressing diversity in argument summarization requires a mix\nof strategies to deal with subjectivity.",
        "pdf_link": "https://arxiv.org/pdf/2402.01535v2.pdf"
    },
    {
        "title": "From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora",
        "authors": [
            "Virginia Morini",
            "Valentina Pansanella",
            "Katherine Abramski",
            "Erica Cau",
            "Andrea Failla",
            "Salvatore Citraro",
            "Giulio Rossetti"
        ],
        "published": "2024-03-21T11:04:41Z",
        "summary": "Social media platforms are online fora where users engage in discussions,\nshare content, and build connections. This review explores the dynamics of\nsocial interactions, user-generated contents, and biases within the context of\nsocial media analysis (analyzing works that use the tools offered by complex\nnetwork analysis and natural language processing) through the lens of three key\npoints of view: online debates, online support, and human-AI interactions. On\nthe one hand, we delineate the phenomenon of online debates, where\npolarization, misinformation, and echo chamber formation often proliferate,\ndriven by algorithmic biases and extreme mechanisms of homophily. On the other\nhand, we explore the emergence of online support groups through users'\nself-disclosure and social support mechanisms. Online debates and support\nmechanisms present a duality of both perils and possibilities within social\nmedia; perils of segregated communities and polarized debates, and\npossibilities of empathy narratives and self-help groups. This dichotomy also\nextends to a third perspective: users' reliance on AI-generated content, such\nas the ones produced by Large Language Models, which can manifest both human\nbiases hidden in training sets and non-human biases that emerge from their\nartificial neural architectures. Analyzing interdisciplinary approaches, we aim\nto deepen the understanding of the complex interplay between social\ninteractions, user-generated content, and biases within the realm of social\nmedia ecosystems.",
        "pdf_link": "https://arxiv.org/pdf/2403.14298v1.pdf"
    },
    {
        "title": "A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products",
        "authors": [
            "Harsh Patel",
            "Dominique Boucher",
            "Emad Fallahzadeh",
            "Ahmed E. Hassan",
            "Bram Adams"
        ],
        "published": "2024-03-27T19:02:56Z",
        "summary": "This paper investigates the complexities of integrating Large Language Models\n(LLMs) into software products, with a focus on the challenges encountered for\ndetermining their readiness for release. Our systematic review of grey\nliterature identifies common challenges in deploying LLMs, ranging from\npre-training and fine-tuning to user experience considerations. The study\nintroduces a comprehensive checklist designed to guide practitioners in\nevaluating key release readiness aspects such as performance, monitoring, and\ndeployment strategies, aiming to enhance the reliability and effectiveness of\nLLM-based applications in real-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.18958v1.pdf"
    },
    {
        "title": "Tuning Language Models by Proxy",
        "authors": [
            "Alisa Liu",
            "Xiaochuang Han",
            "Yizhong Wang",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "Noah A. Smith"
        ],
        "published": "2024-01-16T18:49:55Z",
        "summary": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.",
        "pdf_link": "https://arxiv.org/pdf/2401.08565v2.pdf"
    },
    {
        "title": "Large Language Model Distilling Medication Recommendation Model",
        "authors": [
            "Qidong Liu",
            "Xian Wu",
            "Xiangyu Zhao",
            "Yuanshao Zhu",
            "Zijian Zhang",
            "Feng Tian",
            "Yefeng Zheng"
        ],
        "published": "2024-02-05T08:25:22Z",
        "summary": "The recommendation of medication is a vital aspect of intelligent healthcare\nsystems, as it involves prescribing the most suitable drugs based on a\npatient's specific health needs. Unfortunately, many sophisticated models\ncurrently in use tend to overlook the nuanced semantics of medical data, while\nonly relying heavily on identities. Furthermore, these models face significant\nchallenges in handling cases involving patients who are visiting the hospital\nfor the first time, as they lack prior prescription histories to draw upon. To\ntackle these issues, we harness the powerful semantic comprehension and\ninput-agnostic characteristics of Large Language Models (LLMs). Our research\naims to transform existing medication recommendation methodologies using LLMs.\nIn this paper, we introduce a novel approach called Large Language Model\nDistilling Medication Recommendation (LEADER). We begin by creating appropriate\nprompt templates that enable LLMs to suggest medications effectively. However,\nthe straightforward integration of LLMs into recommender systems leads to an\nout-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a\nnovel output layer and a refined tuning loss function. Although LLM-based\nmodels exhibit remarkable capabilities, they are plagued by high computational\ncosts during inference, which is impractical for the healthcare sector. To\nmitigate this, we have developed a feature-level knowledge distillation\ntechnique, which transfers the LLM's proficiency to a more compact model.\nExtensive experiments conducted on two real-world datasets, MIMIC-III and\nMIMIC-IV, demonstrate that our proposed model not only delivers effective\nresults but also is efficient. To ease the reproducibility of our experiments,\nwe release the implementation code online.",
        "pdf_link": "https://arxiv.org/pdf/2402.02803v1.pdf"
    },
    {
        "title": "Improving Socratic Question Generation using Data Augmentation and Preference Optimization",
        "authors": [
            "Nischal Ashok Kumar",
            "Andrew Lan"
        ],
        "published": "2024-03-01T00:08:20Z",
        "summary": "The Socratic method is a way of guiding students toward solving a problem\nindependently without directly revealing the solution to the problem. Although\nthis method has been shown to significantly improve student learning outcomes,\nit remains a complex labor-intensive task for instructors. Large language\nmodels (LLMs) can be used to augment human effort by automatically generating\nSocratic questions for students. However, existing methods that involve\nprompting these LLMs sometimes produce invalid outputs, e.g., those that\ndirectly reveal the solution to the problem or provide irrelevant or premature\nquestions. To alleviate this problem, inspired by reinforcement learning with\nAI feedback (RLAIF), we first propose a data augmentation method to enrich\nexisting Socratic questioning datasets with questions that are invalid in\nspecific ways. Next, we propose a method to optimize open-source LLMs such as\nLLama 2 to prefer ground-truth questions over generated invalid ones, using\ndirect preference optimization (DPO). Our experiments on a Socratic questions\ndataset for student code debugging show that a DPO-optimized 7B LLama 2 model\ncan effectively avoid generating invalid questions, and as a result,\noutperforms existing state-of-the-art prompting methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.00199v1.pdf"
    },
    {
        "title": "A Survey on Large Language Model Hallucination via a Creativity Perspective",
        "authors": [
            "Xuhui Jiang",
            "Yuxing Tian",
            "Fengrui Hua",
            "Chengjin Xu",
            "Yuanzhuo Wang",
            "Jian Guo"
        ],
        "published": "2024-02-02T12:21:04Z",
        "summary": "Hallucinations in large language models (LLMs) are always seen as\nlimitations. However, could they also be a source of creativity? This survey\nexplores this possibility, suggesting that hallucinations may contribute to LLM\napplication by fostering creativity. This survey begins with a review of the\ntaxonomy of hallucinations and their negative impact on LLM reliability in\ncritical applications. Then, through historical examples and recent relevant\ntheories, the survey explores the potential creative benefits of hallucinations\nin LLMs. To elucidate the value and evaluation criteria of this connection, we\ndelve into the definitions and assessment methods of creativity. Following the\nframework of divergent and convergent thinking phases, the survey\nsystematically reviews the literature on transforming and harnessing\nhallucinations for creativity in LLMs. Finally, the survey discusses future\nresearch directions, emphasizing the need to further explore and refine the\napplication of hallucinations in creative processes within LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.06647v1.pdf"
    },
    {
        "title": "Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology",
        "authors": [
            "Dimitrios P. Panagoulias",
            "Evridiki Tsoureli-Nikita",
            "Maria Virvou",
            "George A. Tsihrintzis"
        ],
        "published": "2024-03-21T09:02:17Z",
        "summary": "The rise of Artificial Intelligence creates great promise in the field of\nmedical discovery, diagnostics and patient management. However, the vast\ncomplexity of all medical domains require a more complex approach that combines\nmachine learning algorithms, classifiers, segmentation algorithms and, lately,\nlarge language models. In this paper, we describe, implement and assess an\nArtificial Intelligence-empowered system and methodology aimed at assisting the\ndiagnosis process of skin lesions and other skin conditions within the field of\ndermatology that aims to holistically address the diagnostic process in this\ndomain. The workflow integrates large language, transformer-based vision models\nand sophisticated machine learning tools. This holistic approach achieves a\nnuanced interpretation of dermatological conditions that simulates and\nfacilitates a dermatologist's workflow. We assess our proposed methodology\nthrough a thorough cross-model validation technique embedded in an evaluation\npipeline that utilizes publicly available medical case studies of skin\nconditions and relevant images. To quantitatively score the system performance,\nadvanced machine learning and natural language processing tools are employed\nwhich focus on similarity comparison and natural language inference.\nAdditionally, we incorporate a human expert evaluation process based on a\nstructured checklist to further validate our results. We implemented the\nproposed methodology in a system which achieved approximate (weighted) scores\nof 0.87 for both contextual understanding and diagnostic accuracy,\ndemonstrating the efficacy of our approach in enhancing dermatological\nanalysis. The proposed methodology is expected to prove useful in the\ndevelopment of next-generation tele-dermatology applications, enhancing remote\nconsultation capabilities and access to care, especially in underserved areas.",
        "pdf_link": "https://arxiv.org/pdf/2403.14243v1.pdf"
    },
    {
        "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
        "authors": [
            "Leo Schwinn",
            "David Dobre",
            "Sophie Xhonneux",
            "Gauthier Gidel",
            "Stephan Gunnemann"
        ],
        "published": "2024-02-14T10:20:03Z",
        "summary": "Current research in adversarial robustness of LLMs focuses on discrete input\nmanipulations in the natural language space, which can be directly transferred\nto closed-source models. However, this approach neglects the steady progression\nof open-source models. As open-source models advance in capability, ensuring\ntheir safety also becomes increasingly imperative. Yet, attacks tailored to\nopen-source LLMs that exploit full model access remain largely unexplored. We\naddress this research gap and propose the embedding space attack, which\ndirectly attacks the continuous embedding representation of input tokens. We\nfind that embedding space attacks circumvent model alignments and trigger\nharmful behaviors more efficiently than discrete attacks or model fine-tuning.\nFurthermore, we present a novel threat model in the context of unlearning and\nshow that embedding space attacks can extract supposedly deleted information\nfrom unlearned LLMs across multiple datasets and models. Our findings highlight\nembedding space attacks as an important threat model in open-source LLMs.\nTrigger Warning: the appendix contains LLM-generated text with violence and\nharassment.",
        "pdf_link": "https://arxiv.org/pdf/2402.09063v1.pdf"
    },
    {
        "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
        "authors": [
            "Shu Yang",
            "Jiayuan Su",
            "Han Jiang",
            "Mengdi Li",
            "Keyuan Cheng",
            "Muhammad Asif Ali",
            "Lijie Hu",
            "Di Wang"
        ],
        "published": "2024-03-30T22:41:05Z",
        "summary": "With the rise of large language models (LLMs), ensuring they embody the\nprinciples of being helpful, honest, and harmless (3H), known as Human\nAlignment, becomes crucial. While existing alignment methods like RLHF, DPO,\netc., effectively fine-tune LLMs to match preferences in the preference\ndataset, they often lead LLMs to highly receptive human input and external\nevidence, even when this information is poisoned. This leads to a tendency for\nLLMs to be Adaptive Chameleons when external evidence conflicts with their\nparametric memory. This exacerbates the risk of LLM being attacked by external\npoisoned data, which poses a significant security risk to LLM system\napplications such as Retrieval-augmented generation (RAG). To address the\nchallenge, we propose a novel framework: Dialectical Alignment (DA), which (1)\nutilizes AI feedback to identify optimal strategies for LLMs to navigate\ninter-context conflicts and context-memory conflicts with different external\nevidence in context window (i.e., different ratios of poisoned factual\ncontexts); (2) constructs the SFT dataset as well as the preference dataset\nbased on the AI feedback and strategies above; (3) uses the above datasets for\nLLM alignment to defense poisoned context attack while preserving the\neffectiveness of in-context knowledge editing. Our experiments show that the\ndialectical alignment model improves poisoned data attack defense by 20 and\ndoes not require any additional prompt engineering or prior declaration of\n``you may be attacked`` to the LLMs' context window.",
        "pdf_link": "https://arxiv.org/pdf/2404.00486v1.pdf"
    },
    {
        "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning",
        "authors": [
            "Zheqi He",
            "Xinya Wu",
            "Pengfei Zhou",
            "Richeng Xuan",
            "Guang Liu",
            "Xi Yang",
            "Qiannan Zhu",
            "Hua Huang"
        ],
        "published": "2024-01-25T08:22:10Z",
        "summary": "Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose a rigorous evaluation strategy called ShiftCheck for assessing\nmultiple-choice questions. The strategy aims to reduce position bias, minimize\nthe influence of randomness on correctness, and perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.14011v2.pdf"
    },
    {
        "title": "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks",
        "authors": [
            "Yiqing Xie",
            "Alex Xie",
            "Divyanshu Sheth",
            "Pengfei Liu",
            "Daniel Fried",
            "Carolyn Rose"
        ],
        "published": "2024-03-31T05:20:53Z",
        "summary": "To facilitate evaluation of code generation systems across diverse scenarios,\nwe present CodeBenchGen, a framework to create scalable execution-based\nbenchmarks that only requires light guidance from humans. Specifically, we\nleverage a large language model (LLM) to convert an arbitrary piece of code\ninto an evaluation example, including test cases for execution-based\nevaluation. We illustrate the usefulness of our framework by creating a\ndataset, Exec-CSN, which includes 1,931 examples involving 293 libraries\nrevised from code in 367 GitHub repositories taken from the CodeSearchNet\ndataset. To demonstrate the complexity and solvability of examples in Exec-CSN,\nwe present a human study demonstrating that 81.3% of the examples can be solved\nby humans and 61% are rated as ``requires effort to solve''. We conduct code\ngeneration experiments on open-source and proprietary models and analyze the\nperformance of both humans and models. We will release the code of both the\nframework and the dataset upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2404.00566v1.pdf"
    },
    {
        "title": "Human-Centered Privacy Research in the Age of Large Language Models",
        "authors": [
            "Tianshi Li",
            "Sauvik Das",
            "Hao-Ping Lee",
            "Dakuo Wang",
            "Bingsheng Yao",
            "Zhiping Zhang"
        ],
        "published": "2024-02-03T02:32:45Z",
        "summary": "The emergence of large language models (LLMs), and their increased use in\nuser-facing systems, has led to substantial privacy concerns. To date, research\non these privacy concerns has been model-centered: exploring how LLMs lead to\nprivacy risks like memorization, or can be used to infer personal\ncharacteristics about people from their content. We argue that there is a need\nfor more research focusing on the human aspect of these privacy issues: e.g.,\nresearch on how design paradigms for LLMs affect users' disclosure behaviors,\nusers' mental models and preferences for privacy controls, and the design of\ntools, systems, and artifacts that empower end-users to reclaim ownership over\ntheir personal data. To build usable, efficient, and privacy-friendly systems\npowered by these models with imperfect privacy properties, our goal is to\ninitiate discussions to outline an agenda for conducting human-centered\nresearch on privacy issues in LLM-powered systems. This Special Interest Group\n(SIG) aims to bring together researchers with backgrounds in usable security\nand privacy, human-AI collaboration, NLP, or any other related domains to share\ntheir perspectives and experiences on this problem, to help our community\nestablish a collective understanding of the challenges, research opportunities,\nresearch methods, and strategies to collaborate with researchers outside of\nHCI.",
        "pdf_link": "https://arxiv.org/pdf/2402.01994v1.pdf"
    },
    {
        "title": "JumpCoder: Go Beyond Autoregressive Coder via Online Modification",
        "authors": [
            "Mouxiang Chen",
            "Hao Tian",
            "Zhongxin Liu",
            "Xiaoxue Ren",
            "Jianling Sun"
        ],
        "published": "2024-01-15T18:04:29Z",
        "summary": "While existing code large language models (code LLMs) exhibit impressive\ncapabilities in code generation, their autoregressive sequential generation\ninherently lacks reversibility. This limitation hinders them from timely\ncorrecting previous missing statements during coding as humans do, often\nleading to error propagation and suboptimal performance. We introduce\nJumpCoder, a novel modelagnostic framework that enables online modification and\nnon-sequential generation to augment the code LLMs. The key idea behind\nJumpCoder is to insert new code into the currently generated code when\nnecessary during generation, which is achieved through an auxiliary infilling\nmodel that works in tandem with the code LLM. Since identifying the best infill\nposition beforehand is intractable, we adopt an infill-first, judge-later\nstrategy, which experiments with filling at the $k$ most critical positions\nfollowing the generation of each line, and uses an Abstract Syntax Tree (AST)\nparser alongside the Generation Model Scoring to effectively judge the validity\nof each potential infill. Extensive experiments using six state-of-the-art code\nLLMs across multiple benchmarks consistently indicate significant improvements\nover all baselines. Notably, JumpCoder assists code LLMs in achieving up to a\n3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the\nmultilingual HumanEval benchmarks. Our code is public at\nhttps://github.com/Keytoyze/JumpCoder.",
        "pdf_link": "https://arxiv.org/pdf/2401.07870v1.pdf"
    },
    {
        "title": "GPTopic: Dynamic and Interactive Topic Representations",
        "authors": [
            "Arik Reuter",
            "Anton Thielmann",
            "Christoph Weisser",
            "Sebastian Fischer",
            "Benjamin S\u00e4fken"
        ],
        "published": "2024-03-06T11:34:20Z",
        "summary": "Topic modeling seems to be almost synonymous with generating lists of top\nwords to represent topics within large text corpora. However, deducing a topic\nfrom such list of individual terms can require substantial expertise and\nexperience, making topic modelling less accessible to people unfamiliar with\nthe particularities and pitfalls of top-word interpretation. A topic\nrepresentation limited to top-words might further fall short of offering a\ncomprehensive and easily accessible characterization of the various aspects,\nfacets and nuances a topic might have. To address these challenges, we\nintroduce GPTopic, a software package that leverages Large Language Models\n(LLMs) to create dynamic, interactive topic representations. GPTopic provides\nan intuitive chat interface for users to explore, analyze, and refine topics\ninteractively, making topic modeling more accessible and comprehensive. The\ncorresponding code is available here: https://github. com/05ec6602be/GPTopic.",
        "pdf_link": "https://arxiv.org/pdf/2403.03628v1.pdf"
    },
    {
        "title": "Private Benchmarking to Prevent Contamination and Improve Comparative Evaluation of LLMs",
        "authors": [
            "Nishanth Chandran",
            "Sunayana Sitaram",
            "Divya Gupta",
            "Rahul Sharma",
            "Kashish Mittal",
            "Manohar Swaminathan"
        ],
        "published": "2024-03-01T09:28:38Z",
        "summary": "Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,\nreplicability and low cost. However, recent work has pointed out that the\nmajority of the open source benchmarks available today have been contaminated\nor leaked into LLMs, meaning that LLMs have access to test data during\npretraining and/or fine-tuning. This raises serious concerns about the validity\nof benchmarking studies conducted so far and the future of evaluation using\nbenchmarks. To solve this problem, we propose Private Benchmarking, a solution\nwhere test datasets are kept private and models are evaluated without revealing\nthe test data to the model. We describe various scenarios (depending on the\ntrust placed on model owners or dataset owners), and present solutions to avoid\ndata contamination using private benchmarking. For scenarios where the model\nweights need to be kept private, we describe solutions from confidential\ncomputing and cryptography that can aid in private benchmarking. Finally, we\npresent solutions the problem of benchmark dataset auditing, to ensure that\nprivate benchmarks are of sufficiently high quality.",
        "pdf_link": "https://arxiv.org/pdf/2403.00393v1.pdf"
    },
    {
        "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
        "authors": [
            "Naman Jain",
            "King Han",
            "Alex Gu",
            "Wen-Ding Li",
            "Fanjia Yan",
            "Tianjun Zhang",
            "Sida Wang",
            "Armando Solar-Lezama",
            "Koushik Sen",
            "Ion Stoica"
        ],
        "published": "2024-03-12T17:58:04Z",
        "summary": "Large Language Models (LLMs) applied to code-related applications have\nemerged as a prominent field, attracting significant interest from both\nacademia and industry. However, as new and improved LLMs are developed,\nexisting evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient\nfor assessing their capabilities. In this work, we propose LiveCodeBench, a\ncomprehensive and contamination-free evaluation of LLMs for code, which\ncontinuously collects new problems over time from contests across three\ncompetition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our\nbenchmark also focuses on a broader range of code related capabilities, such as\nself-repair, code execution, and test output prediction, beyond just code\ngeneration. Currently, LiveCodeBench hosts four hundred high-quality coding\nproblems that were published between May 2023 and February 2024. We have\nevaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We\npresent empirical findings on contamination, holistic performance comparisons,\npotential overfitting in existing benchmarks as well as individual model\ncomparisons. We will release all prompts and model completions for further\ncommunity analysis, along with a general toolkit for adding new scenarios and\nmodel",
        "pdf_link": "https://arxiv.org/pdf/2403.07974v1.pdf"
    },
    {
        "title": "PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
        "authors": [
            "Zaibin Zhang",
            "Yongting Zhang",
            "Lijun Li",
            "Hongzhi Gao",
            "Lijun Wang",
            "Huchuan Lu",
            "Feng Zhao",
            "Yu Qiao",
            "Jing Shao"
        ],
        "published": "2024-01-22T12:11:55Z",
        "summary": "Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit\nprofound capabilities in collective intelligence. However, the potential misuse\nof this intelligence for malicious purposes presents significant risks. To\ndate, comprehensive research on the safety issues associated with multi-agent\nsystems remains limited. In this paper, we explore these concerns through the\ninnovative lens of agent psychology, revealing that the dark psychological\nstates of agents constitute a significant threat to safety. To tackle these\nconcerns, we propose a comprehensive framework (PsySafe) grounded in agent\npsychology, focusing on three key areas: firstly, identifying how dark\npersonality traits in agents can lead to risky behaviors; secondly, evaluating\nthe safety of multi-agent systems from the psychological and behavioral\nperspectives, and thirdly, devising effective strategies to mitigate these\nrisks. Our experiments reveal several intriguing phenomena, such as the\ncollective dangerous behaviors among agents, agents' self-reflection when\nengaging in dangerous behavior, and the correlation between agents'\npsychological assessments and dangerous behaviors. We anticipate that our\nframework and observations will provide valuable insights for further research\ninto the safety of multi-agent systems. We will make our data and code publicly\naccessible at https://github.com/AI4Good24/PsySafe.",
        "pdf_link": "https://arxiv.org/pdf/2401.11880v2.pdf"
    },
    {
        "title": "Duwak: Dual Watermarks in Large Language Models",
        "authors": [
            "Chaoyi Zhu",
            "Jeroen Galjaard",
            "Pin-Yu Chen",
            "Lydia Y. Chen"
        ],
        "published": "2024-03-12T16:25:38Z",
        "summary": "As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.",
        "pdf_link": "https://arxiv.org/pdf/2403.13000v1.pdf"
    },
    {
        "title": "VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning",
        "authors": [
            "Yongshuo Zong",
            "Ondrej Bohdal",
            "Timothy Hospedales"
        ],
        "published": "2024-03-19T21:31:56Z",
        "summary": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.",
        "pdf_link": "https://arxiv.org/pdf/2403.13164v1.pdf"
    },
    {
        "title": "PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency",
        "authors": [
            "Zhishuai Li",
            "Xiang Wang",
            "Jingjing Zhao",
            "Sun Yang",
            "Guoqing Du",
            "Xiaoru Hu",
            "Bin Zhang",
            "Yuxiao Ye",
            "Ziyue Li",
            "Rui Zhao",
            "Hangyu Mao"
        ],
        "published": "2024-03-13T02:32:41Z",
        "summary": "Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large\nlanguage models (LLM) on in-context learning, achieving significant results.\nNevertheless, they face challenges when dealing with verbose database\ninformation and complex user intentions. This paper presents a two-stage\nframework to enhance the performance of current LLM-based natural language to\nSQL systems. We first introduce a novel prompt representation, called\nreference-enhanced representation, which includes schema information and\nrandomly sampled cell values from tables to instruct LLMs in generating SQL\nqueries. Then, in the first stage, question-SQL pairs are retrieved as few-shot\ndemonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After\nthat, the mentioned entities in PreSQL are parsed to conduct schema linking,\nwhich can significantly compact the useful information. In the second stage,\nwith the linked schema, we simplify the prompt's schema information and\ninstruct the LLM to produce the final SQL. Finally, as the post-refinement\nmodule, we propose using cross-consistency across different LLMs rather than\nself-consistency within a particular LLM. Our methods achieve new SOTA results\non the Spider benchmark, with an execution accuracy of 87.6%.",
        "pdf_link": "https://arxiv.org/pdf/2403.09732v3.pdf"
    },
    {
        "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
        "authors": [
            "Archit Sharma",
            "Sedrick Keh",
            "Eric Mitchell",
            "Chelsea Finn",
            "Kushal Arora",
            "Thomas Kollar"
        ],
        "published": "2024-02-19T18:53:54Z",
        "summary": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for\nimproving the instruction-following abilities of powerful pre-trained language\nmodels. RLAIF first performs supervised fine-tuning (SFT) using demonstrations\nfrom a teacher model and then further fine-tunes the model with reinforcement\nlearning (RL), using feedback from a critic model. While recent popular\nopen-source models have demonstrated substantial improvements in performance\nfrom the RL step, in this paper we question whether the complexity of this RL\nstep is truly warranted for AI feedback. We show that the improvements of the\nRL step are virtually entirely due to the widespread practice of using a weaker\nteacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,\nGPT-4) used for AI feedback generation. Specifically, we show that simple\nsupervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF\npipelines. More generally, we find that the gains from RLAIF vary substantially\nacross base model families, test-time evaluation protocols, and critic models.\nFinally, we provide a mechanistic explanation for when SFT may outperform the\nfull two-step RLAIF pipeline as well as suggestions for making RLAIF maximally\nuseful in practice.",
        "pdf_link": "https://arxiv.org/pdf/2402.12366v1.pdf"
    },
    {
        "title": "Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models",
        "authors": [
            "Ying-Chun Lin",
            "Jennifer Neville",
            "Jack W. Stokes",
            "Longqi Yang",
            "Tara Safavi",
            "Mengting Wan",
            "Scott Counts",
            "Siddharth Suri",
            "Reid Andersen",
            "Xiaofeng Xu",
            "Deepak Gupta",
            "Sujay Kumar Jauhar",
            "Xia Song",
            "Georg Buscher",
            "Saurabh Tiwary",
            "Brent Hecht",
            "Jaime Teevan"
        ],
        "published": "2024-03-19T02:57:07Z",
        "summary": "Accurate and interpretable user satisfaction estimation (USE) is critical for\nunderstanding, evaluating, and continuously improving conversational systems.\nUsers express their satisfaction or dissatisfaction with diverse conversational\npatterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented\n(customer service chatbot) conversational systems. Existing approaches based on\nfeaturized ML models or text embeddings fall short in extracting generalizable\npatterns and are hard to interpret. In this work, we show that LLMs can extract\ninterpretable signals of user satisfaction from their natural language\nutterances more effectively than embedding-based approaches. Moreover, an LLM\ncan be tailored for USE via an iterative prompting framework using supervision\nfrom labeled examples. The resulting method, Supervised Prompting for User\nsatisfaction Rubrics (SPUR), not only has higher accuracy but is more\ninterpretable as it scores user satisfaction via learned rubrics with a\ndetailed breakdown.",
        "pdf_link": "https://arxiv.org/pdf/2403.12388v1.pdf"
    },
    {
        "title": "A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs",
        "authors": [
            "Md Saroar Jahan",
            "Mourad Oussalah",
            "Djamila Romaissa Beddia",
            "Jhuma kabir Mim",
            "Nabil Arhab"
        ],
        "published": "2024-03-30T09:55:58Z",
        "summary": "The surge of interest in data augmentation within the realm of NLP has been\ndriven by the need to address challenges posed by hate speech domains, the\ndynamic nature of social media vocabulary, and the demands for large-scale\nneural networks requiring extensive training data. However, the prevalent use\nof lexical substitution in data augmentation has raised concerns, as it may\ninadvertently alter the intended meaning, thereby impacting the efficacy of\nsupervised machine learning models. In pursuit of suitable data augmentation\nmethods, this study explores both established legacy approaches and\ncontemporary practices such as Large Language Models (LLM), including GPT in\nHate Speech detection. Additionally, we propose an optimized utilization of\nBERT-based encoder models with contextual cosine similarity filtration,\nexposing significant limitations in prior synonym substitution methods. Our\ncomparative analysis encompasses five popular augmentation techniques: WordNet\nand Fast-Text synonym replacement, Back-translation, BERT-mask contextual\naugmentation, and LLM. Our analysis across five benchmarked datasets revealed\nthat while traditional methods like back-translation show low label alteration\nrates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence\ndiversity but at the cost of higher label alteration rates (over 6%). Our\nproposed BERT-based contextual cosine similarity filtration markedly reduced\nlabel alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1\nperformance. However, augmenting data with GPT-3 not only avoided overfitting\nwith up to sevenfold data increase but also improved embedding space coverage\nby 15% and classification F1 score by 1.4% over traditional methods, and by\n0.8% over our method.",
        "pdf_link": "https://arxiv.org/pdf/2404.00303v1.pdf"
    },
    {
        "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models",
        "authors": [
            "Peng Ding",
            "Jiading Fang",
            "Peng Li",
            "Kangrui Wang",
            "Xiaochen Zhou",
            "Mo Yu",
            "Jing Li",
            "Matthew R. Walter",
            "Hongyuan Mei"
        ],
        "published": "2024-03-29T01:53:24Z",
        "summary": "Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.",
        "pdf_link": "https://arxiv.org/pdf/2403.19913v1.pdf"
    },
    {
        "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
        "authors": [
            "Xiaoyan Yu",
            "Tongxu Luo",
            "Yifan Wei",
            "Fangyu Lei",
            "Yiming Huang",
            "Hao Peng",
            "Liehuang Zhu"
        ],
        "published": "2024-02-21T11:30:20Z",
        "summary": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents\nbut encounter challenges in multi-character role-playing (MCRP) scenarios. To\naddress the issue, we present Neeko, an innovative framework designed for\nefficient multiple characters imitation. Unlike existing methods, Neeko employs\na dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to\ndiverse characters. Our framework breaks down the role-playing process into\nagent pre-training, multiple characters playing, and character incremental\nlearning, effectively handling both seen and unseen roles. This dynamic\napproach, coupled with distinct LoRA blocks for each character, enhances\nNeeko's adaptability to unique attributes, personalities, and speaking\npatterns. As a result, Neeko demonstrates superior performance in MCRP over\nmost existing methods, offering more engaging and versatile user interaction\nexperiences. Code and data are available at\nhttps://github.com/weiyifan1023/Neeko.",
        "pdf_link": "https://arxiv.org/pdf/2402.13717v2.pdf"
    },
    {
        "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
        "authors": [
            "Weixin Liang",
            "Zachary Izzo",
            "Yaohui Zhang",
            "Haley Lepp",
            "Hancheng Cao",
            "Xuandong Zhao",
            "Lingjiao Chen",
            "Haotian Ye",
            "Sheng Liu",
            "Zhi Huang",
            "Daniel A. McFarland",
            "James Y. Zou"
        ],
        "published": "2024-03-11T21:51:39Z",
        "summary": "We present an approach for estimating the fraction of text in a large corpus\nwhich is likely to be substantially modified or produced by a large language\nmodel (LLM). Our maximum likelihood model leverages expert-written and\nAI-generated reference texts to accurately and efficiently examine real-world\nLLM-use at the corpus level. We apply this approach to a case study of\nscientific peer review in AI conferences that took place after the release of\nChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest\nthat between 6.5% and 16.9% of text submitted as peer reviews to these\nconferences could have been substantially modified by LLMs, i.e. beyond\nspell-checking or minor writing updates. The circumstances in which generated\ntext occurs offer insight into user behavior: the estimated fraction of\nLLM-generated text is higher in reviews which report lower confidence, were\nsubmitted close to the deadline, and from reviewers who are less likely to\nrespond to author rebuttals. We also observe corpus-level trends in generated\ntext which may be too subtle to detect at the individual level, and discuss the\nimplications of such trends on peer review. We call for future\ninterdisciplinary work to examine how LLM use is changing our information and\nknowledge practices.",
        "pdf_link": "https://arxiv.org/pdf/2403.07183v1.pdf"
    },
    {
        "title": "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models",
        "authors": [
            "Jianhao Yan",
            "Yun Luo",
            "Yue Zhang"
        ],
        "published": "2024-02-21T01:39:56Z",
        "summary": "The application scope of large language models (LLMs) is increasingly\nexpanding. In practical use, users might provide feedback based on the model's\noutput, hoping for a responsive model that can complete responses according to\ntheir feedback. Whether the model can appropriately respond to users' refuting\nfeedback and consistently follow through with execution has not been thoroughly\nanalyzed. In light of this, this paper proposes a comprehensive benchmark,\nRefuteBench, covering tasks such as question answering, machine translation,\nand email writing. The evaluation aims to assess whether models can positively\naccept feedback in form of refuting instructions and whether they can\nconsistently adhere to user demands throughout the conversation. We conduct\nevaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit\ninclination to their internal knowledge, often failing to comply with user\nfeedback. Additionally, as the length of the conversation increases, models\ngradually forget the user's stated feedback and roll back to their own\nresponses. We further propose a recall-and-repeat prompts as a simple and\neffective way to enhance the model's responsiveness to feedback.",
        "pdf_link": "https://arxiv.org/pdf/2402.13463v2.pdf"
    },
    {
        "title": "SocialGenPod: Privacy-Friendly Generative AI Social Web Applications with Decentralised Personal Data Stores",
        "authors": [
            "Vidminas Vizgirda",
            "Rui Zhao",
            "Naman Goel"
        ],
        "published": "2024-03-15T15:43:02Z",
        "summary": "We present SocialGenPod, a decentralised and privacy-friendly way of\ndeploying generative AI Web applications. Unlike centralised Web and data\narchitectures that keep user data tied to application and service providers, we\nshow how one can use Solid -- a decentralised Web specification -- to decouple\nuser data from generative AI applications. We demonstrate SocialGenPod using a\nprototype that allows users to converse with different Large Language Models,\noptionally leveraging Retrieval Augmented Generation to generate answers\ngrounded in private documents stored in any Solid Pod that the user is allowed\nto access, directly or indirectly. SocialGenPod makes use of Solid access\ncontrol mechanisms to give users full control of determining who has access to\ndata stored in their Pods. SocialGenPod keeps all user data (chat history, app\nconfiguration, personal documents, etc) securely in the user's personal Pod;\nseparate from specific model or application providers. Besides better privacy\ncontrols, this approach also enables portability across different services and\napplications. Finally, we discuss challenges, posed by the large compute\nrequirements of state-of-the-art models, that future research in this area\nshould address. Our prototype is open-source and available at:\nhttps://github.com/Vidminas/socialgenpod/.",
        "pdf_link": "https://arxiv.org/pdf/2403.10408v1.pdf"
    },
    {
        "title": "TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions",
        "authors": [
            "Gyubok Lee",
            "Woosog Chay",
            "Seonhee Cho",
            "Edward Choi"
        ],
        "published": "2024-03-23T16:12:52Z",
        "summary": "Recent advances in large language models (LLMs) have led to significant\nimprovements in translating natural language questions into SQL queries. While\nachieving high accuracy in SQL generation is crucial, little is known about the\nextent to which these text-to-SQL models can reliably handle diverse types of\nquestions encountered during real-world deployment, including unanswerable\nones. To explore this aspect, we present TrustSQL, a new benchmark designed to\nassess the reliability of text-to-SQL models in both single-database and\ncross-database settings. The benchmark tasks models with providing one of two\noutcomes: 1) SQL prediction; or 2) abstention from making a prediction, either\nwhen there is a potential error in the generated SQL or when faced with\nunanswerable questions. For model evaluation, we explore various modeling\napproaches specifically designed for this task. These include: 1) optimizing\nseparate models for answerability detection, SQL generation, and error\ndetection, which are then integrated into a single pipeline; and 2) developing\na unified approach that optimizes a single model to address the proposed task.\nExperimental results using our new reliability score show that addressing this\nchallenge involves many different areas of research and opens new avenues for\nmodel development. Nonetheless, none of the methods surpass the reliability\nperformance of the naive baseline, which abstains from answering all questions.",
        "pdf_link": "https://arxiv.org/pdf/2403.15879v1.pdf"
    },
    {
        "title": "SwissNYF: Tool Grounded LLM Agents for Black Box Setting",
        "authors": [
            "Somnath Sendhil Kumar",
            "Dhruv Jain",
            "Eshaan Agarwal",
            "Raunak Pandey"
        ],
        "published": "2024-02-15T16:15:38Z",
        "summary": "While Large Language Models (LLMs) have demonstrated enhanced capabilities in\nfunction-calling, these advancements primarily rely on accessing the functions'\nresponses. This methodology is practical for simpler APIs but faces scalability\nissues with irreversible APIs that significantly impact the system, such as a\ndatabase deletion API. Similarly, processes requiring extensive time for each\nAPI call and those necessitating forward planning, like automated action\npipelines, present complex challenges. Furthermore, scenarios often arise where\na generalized approach is needed because algorithms lack direct access to the\nspecific implementations of these functions or secrets to use them. Traditional\ntool planning methods are inadequate in these cases, compelling the need to\noperate within black-box environments. Unlike their performance in tool\nmanipulation, LLMs excel in black-box tasks, such as program synthesis.\nTherefore, we harness the program synthesis capabilities of LLMs to strategize\ntool usage in black-box settings, ensuring solutions are verified prior to\nimplementation. We introduce TOPGUN, an ingeniously crafted approach leveraging\nprogram synthesis for black box tool planning. Accompanied by SwissNYF, a\ncomprehensive suite that integrates black-box algorithms for planning and\nverification tasks, addressing the aforementioned challenges and enhancing the\nversatility and effectiveness of LLMs in complex API interactions. The public\ncode for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.",
        "pdf_link": "https://arxiv.org/pdf/2402.10051v1.pdf"
    },
    {
        "title": "Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks",
        "authors": [
            "Niall Taylor",
            "Upamanyu Ghose",
            "Omid Rohanian",
            "Mohammadmahdi Nouriborji",
            "Andrey Kormilitzin",
            "David Clifton",
            "Alejo Nevado-Holgado"
        ],
        "published": "2024-02-16T11:30:11Z",
        "summary": "The entry of large language models (LLMs) into research and commercial spaces\nhas led to a trend of ever-larger models, with initial promises of\ngeneralisability, followed by a widespread desire to downsize and create\nspecialised models without the need for complete fine-tuning, using Parameter\nEfficient Fine-tuning (PEFT) methods. We present an investigation into the\nsuitability of different PEFT methods to clinical decision-making tasks, across\na range of model sizes, including extremely small models with as few as $25$\nmillion parameters.\n  Our analysis shows that the performance of most PEFT approaches varies\nsignificantly from one task to another, with the exception of LoRA, which\nmaintains relatively high performance across all model sizes and tasks,\ntypically approaching or matching full fine-tuned performance. The\neffectiveness of PEFT methods in the clinical domain is evident, particularly\nfor specialised models which can operate on low-cost, in-house computing\ninfrastructure. The advantages of these models, in terms of speed and reduced\ntraining costs, dramatically outweighs any performance gain from large\nfoundation LLMs. Furthermore, we highlight how domain-specific pre-training\ninteracts with PEFT methods and model size, and discuss how these factors\ninterplay to provide the best efficiency-performance trade-off. Full code\navailable at: tbd.",
        "pdf_link": "https://arxiv.org/pdf/2402.10597v1.pdf"
    },
    {
        "title": "Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues",
        "authors": [
            "Deuksin Kwon",
            "Emily Weiss",
            "Tara Kulshrestha",
            "Kushal Chawla",
            "Gale M. Lucas",
            "Jonathan Gratch"
        ],
        "published": "2024-02-21T06:11:03Z",
        "summary": "A successful negotiation demands a deep comprehension of the conversation\ncontext, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as\nstrategic reasoning and effective communication, making it challenging for\nautomated systems. Given the remarkable performance of LLMs across a variety of\nNLP tasks, in this work, we aim to understand how LLMs can advance different\naspects of negotiation research, ranging from designing dialogue systems to\nproviding pedagogical feedback and scaling up data collection practices. To\nthis end, we devise a methodology to analyze the multifaceted capabilities of\nLLMs across diverse dialogue scenarios covering all the time stages of a\ntypical negotiation interaction. Our analysis adds to the increasing evidence\nfor the superiority of GPT-4 across various tasks while also providing insights\ninto specific tasks that remain difficult for LLMs. For instance, the models\ncorrelate poorly with human players when making subjective assessments about\nthe negotiation dialogues and often struggle to generate responses that are\ncontextually appropriate as well as strategically advantageous.",
        "pdf_link": "https://arxiv.org/pdf/2402.13550v1.pdf"
    },
    {
        "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
        "authors": [
            "Yuanchun Li",
            "Hao Wen",
            "Weijun Wang",
            "Xiangyu Li",
            "Yizhen Yuan",
            "Guohong Liu",
            "Jiacheng Liu",
            "Wenxing Xu",
            "Xiang Wang",
            "Yi Sun",
            "Rui Kong",
            "Yile Wang",
            "Hanfei Geng",
            "Jian Luan",
            "Xuefeng Jin",
            "Zilong Ye",
            "Guanjing Xiong",
            "Fan Zhang",
            "Xiang Li",
            "Mengwei Xu",
            "Zhijun Li",
            "Peng Li",
            "Yang Liu",
            "Ya-Qin Zhang",
            "Yunxin Liu"
        ],
        "published": "2024-01-10T09:25:45Z",
        "summary": "Since the advent of personal computing devices, intelligent personal\nassistants (IPAs) have been one of the key technologies that researchers and\nengineers have focused on, aiming to help users efficiently obtain information\nand execute tasks, and provide users with more intelligent, convenient, and\nrich interaction experiences. With the development of smartphones and IoT,\ncomputing and sensing devices have become ubiquitous, greatly expanding the\nboundaries of IPAs. However, due to the lack of capabilities such as user\nintent understanding, task planning, tool using, and personal data management\netc., existing IPAs still have limited practicality and scalability. Recently,\nthe emergence of foundation models, represented by large language models\n(LLMs), brings new opportunities for the development of IPAs. With the powerful\nsemantic understanding and reasoning capabilities, LLM can enable intelligent\nagents to solve complex problems autonomously. In this paper, we focus on\nPersonal LLM Agents, which are LLM-based agents that are deeply integrated with\npersonal data and personal devices and used for personal assistance. We\nenvision that Personal LLM Agents will become a major software paradigm for\nend-users in the upcoming era. To realize this vision, we take the first step\nto discuss several important questions about Personal LLM Agents, including\ntheir architecture, capability, efficiency and security. We start by\nsummarizing the key components and design choices in the architecture of\nPersonal LLM Agents, followed by an in-depth analysis of the opinions collected\nfrom domain experts. Next, we discuss several key challenges to achieve\nintelligent, efficient and secure Personal LLM Agents, followed by a\ncomprehensive survey of representative solutions to address these challenges.",
        "pdf_link": "https://arxiv.org/pdf/2401.05459v1.pdf"
    },
    {
        "title": "Answer is All You Need: Instruction-following Text Embedding via Answering the Question",
        "authors": [
            "Letian Peng",
            "Yuwei Zhang",
            "Zilong Wang",
            "Jayanth Srinivasa",
            "Gaowen Liu",
            "Zihan Wang",
            "Jingbo Shang"
        ],
        "published": "2024-02-15T01:02:41Z",
        "summary": "This work aims to build a text embedder that can capture characteristics of\ntexts specified by user instructions. Despite its tremendous potential to\ndeploy user-oriented embeddings, none of previous approaches provides a\nconcrete solution for it. This paper offers a new viewpoint, which treats the\ninstruction as a question about the input text and encodes the expected answers\nto obtain the representation accordingly. Intuitively, texts with the same\n(implicit) semantics would share similar answers following the instruction,\nthus leading to more similar embeddings. Specifically, we propose InBedder that\ninstantiates this embed-via-answering idea by only fine-tuning language models\non abstractive question answering tasks. InBedder demonstrates significantly\nimproved instruction-following capabilities according to our proposed\ninstruction awareness tests and instruction robustness tests, when applied to\nboth large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based\nLMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering\noutcomes, achieved by applying different instructions to the same corpus,\ndemonstrates a high degree of interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2402.09642v1.pdf"
    },
    {
        "title": "Scaling Data Diversity for Fine-Tuning Language Models in Human Alignment",
        "authors": [
            "Feifan Song",
            "Bowen Yu",
            "Hao Lang",
            "Haiyang Yu",
            "Fei Huang",
            "Houfeng Wang",
            "Yongbin Li"
        ],
        "published": "2024-03-17T07:08:55Z",
        "summary": "Alignment with human preference prevents large language models (LLMs) from\ngenerating misleading or toxic content while requiring high-cost human\nfeedback. Assuming resources of human annotation are limited, there are two\ndifferent ways of allocating considered: more diverse PROMPTS or more diverse\nRESPONSES to be labeled. Nonetheless, a straightforward comparison between\ntheir impact is absent. In this work, we first control the diversity of both\nsides according to the number of samples for fine-tuning, which can directly\nreflect their influence. We find that instead of numerous prompts, more\nresponses but fewer prompts better trigger LLMs for human alignment.\nAdditionally, the concept of diversity for prompts can be more complex than\nresponses that are typically quantified by single digits. Consequently, a new\nformulation of prompt diversity is proposed, further implying a linear\ncorrelation with the final performance of LLMs after fine-tuning. We also\nleverage it on data augmentation and conduct experiments to show its effect on\ndifferent algorithms.",
        "pdf_link": "https://arxiv.org/pdf/2403.11124v2.pdf"
    },
    {
        "title": "General Flow as Foundation Affordance for Scalable Robot Learning",
        "authors": [
            "Chengbo Yuan",
            "Chuan Wen",
            "Tong Zhang",
            "Yang Gao"
        ],
        "published": "2024-01-21T09:39:11Z",
        "summary": "We address the challenge of acquiring real-world manipulation skills with a\nscalable framework.Inspired by the success of large-scale auto-regressive\nprediction in Large Language Models (LLMs), we hold the belief that identifying\nan appropriate prediction target capable of leveraging large-scale datasets is\ncrucial for achieving efficient and universal learning. Therefore, we propose\nto utilize flow, which represents the future trajectories of 3D points on\nobjects of interest, as an ideal prediction target in robot learning. To\nexploit scalable data resources, we turn our attention to cross-embodiment\ndatasets. We develop, for the first time, a language-conditioned prediction\nmodel directly from large-scale RGBD human video datasets. Our predicted flow\noffers actionable geometric and physics guidance, thus facilitating stable\nzero-shot skill transfer in real-world scenarios.We deploy our method with a\npolicy based on closed-loop flow prediction. Remarkably, without any additional\ntraining, our method achieves an impressive 81% success rate in human-to-robot\nskill transfer, covering 18 tasks in 6 scenes. Our framework features the\nfollowing benefits: (1) scalability: leveraging cross-embodiment data\nresources; (2) universality: multiple object categories, including rigid,\narticulated, and soft bodies; (3) stable skill transfer: providing actionable\nguidance with a small inference domain-gap. These lead to a new pathway towards\nscalable general robot learning. Data, code, and model weights will be made\npublicly available.",
        "pdf_link": "https://arxiv.org/pdf/2401.11439v1.pdf"
    },
    {
        "title": "L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks",
        "authors": [
            "Ping Guo",
            "Fei Liu",
            "Xi Lin",
            "Qingchuan Zhao",
            "Qingfu Zhang"
        ],
        "published": "2024-01-27T07:57:20Z",
        "summary": "In the rapidly evolving field of machine learning, adversarial attacks\npresent a significant challenge to model robustness and security.\nDecision-based attacks, which only require feedback on the decision of a model\nrather than detailed probabilities or scores, are particularly insidious and\ndifficult to defend against. This work introduces L-AutoDA (Large Language\nModel-based Automated Decision-based Adversarial Attacks), a novel approach\nleveraging the generative capabilities of Large Language Models (LLMs) to\nautomate the design of these attacks. By iteratively interacting with LLMs in\nan evolutionary framework, L-AutoDA automatically designs competitive attack\nalgorithms efficiently without much human effort. We demonstrate the efficacy\nof L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline\nmethods in both success rate and computational efficiency. Our findings\nunderscore the potential of language models as tools for adversarial attack\ngeneration and highlight new avenues for the development of robust AI systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.15335v1.pdf"
    },
    {
        "title": "TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision",
        "authors": [
            "Yunyi Zhang",
            "Ruozhen Yang",
            "Xueqiang Xu",
            "Jinfeng Xiao",
            "Jiaming Shen",
            "Jiawei Han"
        ],
        "published": "2024-02-29T22:26:07Z",
        "summary": "Hierarchical text classification aims to categorize each document into a set\nof classes in a label taxonomy. Most earlier works focus on fully or\nsemi-supervised methods that require a large amount of human annotated data\nwhich is costly and time-consuming to acquire. To alleviate human efforts, in\nthis paper, we work on hierarchical text classification with the minimal amount\nof supervision: using the sole class name of each node as the only supervision.\nRecently, large language models (LLM) show competitive performance on various\ntasks through zero-shot prompting, but this method performs poorly in the\nhierarchical setting, because it is ineffective to include the large and\nstructured label space in a prompt. On the other hand, previous\nweakly-supervised hierarchical text classification methods only utilize the raw\ntaxonomy skeleton and ignore the rich information hidden in the text corpus\nthat can serve as additional class-indicative features. To tackle the above\nchallenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced\nweakly-supervised hierarchical text classification, which (1) automatically\nenriches the label taxonomy with class-indicative topical terms mined from the\ncorpus to facilitate classifier training and (2) utilizes LLMs for both data\nannotation and creation tailored for the hierarchical label space. Experiments\nshow that TELEClass can outperform previous weakly-supervised hierarchical text\nclassification methods and LLM-based zero-shot prompting methods on two public\ndatasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.00165v1.pdf"
    },
    {
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "authors": [
            "Yuncheng Hua",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "published": "2024-01-29T09:07:40Z",
        "summary": "In this work, we aim to develop LLM agents to mitigate social norm violations\nin negotiations in a multi-agent setting. We simulate real-world negotiations\nby letting two large Language Models (LLMs) play the roles of two negotiators\nin each conversation. A third LLM acts as a remediation agent to rewrite\nutterances violating norms for improving negotiation outcomes. As it is a novel\ntask, no manually constructed data is available. To address this limitation, we\nintroduce a value impact based In-Context Learning (ICL) method to identify\nhigh-quality ICL examples for the LLM-based remediation agents, where the value\nimpact function measures the quality of negotiation outcomes. We show the\nconnection of this method to policy learning and provide rich empirical\nevidence to demonstrate its effectiveness in negotiations across three\ndifferent topics: product sale, housing price, and salary negotiation. The\nsource code and the generated dataset will be publicly available upon\nacceptance.",
        "pdf_link": "https://arxiv.org/pdf/2402.01737v1.pdf"
    },
    {
        "title": "Pre-trained Large Language Models for Financial Sentiment Analysis",
        "authors": [
            "Wei Luo",
            "Dihong Gong"
        ],
        "published": "2024-01-10T15:27:41Z",
        "summary": "Financial sentiment analysis refers to classifying financial text contents\ninto sentiment categories (e.g. positive, negative, and neutral). In this\npaper, we focus on the classification of financial news title, which is a\nchallenging task due to a lack of large amount of training samples. To overcome\nthis difficulty, we propose to adapt the pretrained large language models\n(LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge\namount of text corpora,have an advantage in text understanding and can be\neffectively adapted to domain-specific task while requiring very few amount of\ntraining samples. In particular, we adapt the open-source Llama2-7B model\n(2023) with the supervised fine-tuning (SFT) technique [4]. Experimental\nevaluation shows that even with the 7B model (which is relatively small for\nLLMs), our approach significantly outperforms the previous state-of-the-art\nalgorithms.",
        "pdf_link": "https://arxiv.org/pdf/2401.05215v1.pdf"
    },
    {
        "title": "LightHouse: A Survey of AGI Hallucination",
        "authors": [
            "Feng Wang"
        ],
        "published": "2024-01-08T03:52:40Z",
        "summary": "With the development of artificial intelligence, large-scale models have\nbecome increasingly intelligent. However, numerous studies indicate that\nhallucinations within these large models are a bottleneck hindering the\ndevelopment of AI research. In the pursuit of achieving strong artificial\nintelligence, a significant volume of research effort is being invested in the\nAGI (Artificial General Intelligence) hallucination research. Previous\nexplorations have been conducted in researching hallucinations within LLMs\n(Large Language Models). As for multimodal AGI, research on hallucinations is\nstill in an early stage. To further the progress of research in the domain of\nhallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,\nsummarizing the current work on AGI hallucinations and proposing some\ndirections for future research.",
        "pdf_link": "https://arxiv.org/pdf/2401.06792v2.pdf"
    },
    {
        "title": "Measuring Vision-Language STEM Skills of Neural Models",
        "authors": [
            "Jianhao Shen",
            "Ye Yuan",
            "Srbuhi Mirzoyan",
            "Ming Zhang",
            "Chenguang Wang"
        ],
        "published": "2024-02-27T04:55:03Z",
        "summary": "We introduce a new challenge to test the STEM skills of neural models. The\nproblems in the real world often require solutions, combining knowledge from\nSTEM (science, technology, engineering, and math). Unlike existing datasets,\nour dataset requires the understanding of multimodal vision-language\ninformation of STEM. Our dataset features one of the largest and most\ncomprehensive datasets for the challenge. It includes 448 skills and 1,073,146\nquestions spanning all STEM subjects. Compared to existing datasets that often\nfocus on examining expert-level ability, our dataset includes fundamental\nskills and questions designed based on the K-12 curriculum. We also add\nstate-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our\nbenchmark. Results show that the recent model advances only help master a very\nlimited number of lower grade-level skills (2.5% in the third grade) in our\ndataset. In fact, these models are still well below (averaging 54.7%) the\nperformance of elementary students, not to mention near expert-level\nperformance. To understand and increase the performance on our dataset, we\nteach the models on a training split of our dataset. Even though we observe\nimproved performance, the model performance remains relatively low compared to\naverage elementary students. To solve STEM problems, we will need novel\nalgorithmic innovations from the community.",
        "pdf_link": "https://arxiv.org/pdf/2402.17205v1.pdf"
    },
    {
        "title": "Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy",
        "authors": [
            "Efe Bozkir",
            "S\u00fcleyman \u00d6zdel",
            "Ka Hei Carrie Lau",
            "Mengdi Wang",
            "Hong Gao",
            "Enkelejda Kasneci"
        ],
        "published": "2024-02-06T11:19:40Z",
        "summary": "Recent developments in computer graphics, hardware, artificial intelligence\n(AI), and human-computer interaction likely lead to extended reality (XR)\ndevices and setups being more pervasive. While these devices and setups provide\nusers with interactive, engaging, and immersive experiences with different\nsensing modalities, such as eye and hand trackers, many non-player characters\nare utilized in a pre-scripted way or by conventional AI techniques. In this\npaper, we argue for using large language models (LLMs) in XR by embedding them\nin virtual avatars or as narratives to facilitate more inclusive experiences\nthrough prompt engineering according to user profiles and fine-tuning the LLMs\nfor particular purposes. We argue that such inclusion will facilitate diversity\nfor XR use. In addition, we believe that with the versatile conversational\ncapabilities of LLMs, users will engage more with XR environments, which might\nhelp XR be more used in everyday life. Lastly, we speculate that combining the\ninformation provided to LLM-powered environments by the users and the biometric\ndata obtained through the sensors might lead to novel privacy invasions. While\nstudying such possible privacy invasions, user privacy concerns and preferences\nshould also be investigated. In summary, despite some challenges, embedding\nLLMs into XR is a promising and novel research area with several opportunities.",
        "pdf_link": "https://arxiv.org/pdf/2402.03907v1.pdf"
    },
    {
        "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",
        "authors": [
            "Jiajia Wang",
            "Jimmy X. Huang",
            "Xinhui Tu",
            "Junmei Wang",
            "Angela J. Huang",
            "Md Tahmid Rahman Laskar",
            "Amran Bhuiyan"
        ],
        "published": "2024-02-18T23:22:40Z",
        "summary": "Recent years have witnessed a substantial increase in the use of deep\nlearning to solve various natural language processing (NLP) problems. Early\ndeep learning models were constrained by their sequential or unidirectional\nnature, such that they struggled to capture the contextual relationships across\ntext inputs. The introduction of bidirectional encoder representations from\ntransformers (BERT) leads to a robust encoder for the transformer model that\ncan understand the broader context and deliver state-of-the-art performance\nacross various NLP tasks. This has inspired researchers and practitioners to\napply BERT to practical problems, such as information retrieval (IR). A survey\nthat focuses on a comprehensive analysis of prevalent approaches that apply\npretrained transformer encoders like BERT to IR can thus be useful for academia\nand the industry. In light of this, we revisit a variety of BERT-based methods\nin this survey, cover a wide range of techniques of IR, and group them into six\nhigh-level categories: (i) handling long documents, (ii) integrating semantic\ninformation, (iii) balancing effectiveness and efficiency, (iv) predicting the\nweights of terms, (v) query expansion, and (vi) document expansion. We also\nprovide links to resources, including datasets and toolkits, for BERT-based IR\nsystems. A key highlight of our survey is the comparison between BERT's\nencoder-based models and the latest generative Large Language Models (LLMs),\nsuch as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we\nfind that for specific tasks, finely tuned BERT encoders still outperform, and\nat a lower deployment cost. Finally, we summarize the comprehensive outcomes of\nthe survey and suggest directions for future research in the area.",
        "pdf_link": "https://arxiv.org/pdf/2403.00784v1.pdf"
    },
    {
        "title": "A Vision Check-up for Language Models",
        "authors": [
            "Pratyusha Sharma",
            "Tamar Rott Shaham",
            "Manel Baradad",
            "Stephanie Fu",
            "Adrian Rodriguez-Munoz",
            "Shivam Duggal",
            "Phillip Isola",
            "Antonio Torralba"
        ],
        "published": "2024-01-03T18:09:33Z",
        "summary": "What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.01862v1.pdf"
    },
    {
        "title": "CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring Commonsense Reasoning and Long-Tail Knowledge",
        "authors": [
            "Willis Guo",
            "Armin Toroghi",
            "Scott Sanner"
        ],
        "published": "2024-03-03T04:47:01Z",
        "summary": "Knowledge graph question answering (KGQA) is a well-established field that\nseeks to provide factual answers to natural language (NL) questions by\nleveraging knowledge graphs (KGs). However, existing KGQA datasets suffer from\ntwo significant limitations: (1) no existing KGQA dataset requires commonsense\nreasoning to arrive at an answer and (2) existing KGQA datasets focus on\npopular entities for which large language models (LLMs) can directly answer\nwithout hallucinating and without leveraging the KG. In this work, we seek a\nnovel KGQA dataset that supports commonsense reasoning and focuses on long-tail\nentities (e.g., non-mainstream and recent entities) where LLMs frequently\nhallucinate, and thus create the need for novel methodologies that leverage the\nKG for factual and attributable commonsense inference. We create a novel\nCommonsense Reasoning (CR) and Long-Tail (LT) KGQA dataset with two subtasks --\nquestion answering and claim verification -- that address both limitations (1)\nand (2). We construct CR-LT-KGQA by building extensions to existing reasoning\ndatasets StrategyQA and CREAK over Wikidata. While existing KGQA methods are\nnot applicable due to their lack of commonsense inference support, baseline\nevaluation of LLMs on CR-LT KGQA demonstrate a high rate of hallucination.\nThus, CR-LT KGQA poses significant challenges for hallucination-prone LLMs,\nhence paving the way for future commonsense KGQA research to provide accurate\nand factual answers for long-tail entities in the era of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.01395v1.pdf"
    },
    {
        "title": "ClinicalMamba: A Generative Clinical Language Model on Longitudinal Clinical Notes",
        "authors": [
            "Zhichao Yang",
            "Avijit Mitra",
            "Sunjae Kwon",
            "Hong Yu"
        ],
        "published": "2024-03-09T04:58:25Z",
        "summary": "The advancement of natural language processing (NLP) systems in healthcare\nhinges on language model ability to interpret the intricate information\ncontained within clinical notes. This process often requires integrating\ninformation from various time points in a patient's medical history. However,\nmost earlier clinical language models were pretrained with a context length\nlimited to roughly one clinical document. In this study, We introduce\nClinicalMamba, a specialized version of the Mamba language model, pretrained on\na vast corpus of longitudinal clinical notes to address the unique linguistic\ncharacteristics and information processing needs of the medical domain.\nClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a\nsuperior performance in modeling clinical language across extended text lengths\ncompared to Mamba and clinical Llama. With few-shot learning, ClinicalMamba\nachieves notable benchmarks in speed and accuracy, outperforming existing\nclinical language models and general domain large models like GPT-4 in\nlongitudinal clinical notes information extraction tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.05795v1.pdf"
    },
    {
        "title": "Memory-Augmented Generative Adversarial Transformers",
        "authors": [
            "Stephan Raaijmakers",
            "Roos Bakker",
            "Anita Cremers",
            "Roy de Kleijn",
            "Tom Kouwenhoven",
            "Tessa Verhoef"
        ],
        "published": "2024-02-29T14:47:24Z",
        "summary": "Conversational AI systems that rely on Large Language Models, like\nTransformers, have difficulty interweaving external data (like facts) with the\nlanguage they generate. Vanilla Transformer architectures are not designed for\nanswering factual questions with high accuracy. This paper investigates a\npossible route for addressing this problem. We propose to extend the standard\nTransformer architecture with an additional memory bank holding extra\ninformation (such as facts drawn from a knowledge base), and an extra attention\nlayer for addressing this memory. We add this augmented memory to a Generative\nAdversarial Network-inspired Transformer architecture. This setup allows for\nimplementing arbitrary felicity conditions on the generated language of the\nTransformer. We first demonstrate how this machinery can be deployed for\nhandling factual questions in goal-oriented dialogues. Secondly, we demonstrate\nthat our approach can be useful for applications like {\\it style adaptation} as\nwell: the adaptation of utterances according to certain stylistic (external)\nconstraints, like social properties of human interlocutors in dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2402.19218v1.pdf"
    },
    {
        "title": "Detecting Hallucination and Coverage Errors in Retrieval Augmented Generation for Controversial Topics",
        "authors": [
            "Tyler A. Chang",
            "Katrin Tomanek",
            "Jessica Hoffmann",
            "Nithum Thain",
            "Erin van Liemt",
            "Kathleen Meier-Hellstern",
            "Lucas Dixon"
        ],
        "published": "2024-03-13T18:47:00Z",
        "summary": "We explore a strategy to handle controversial topics in LLM-based chatbots\nbased on Wikipedia's Neutral Point of View (NPOV) principle: acknowledge the\nabsence of a single true answer and surface multiple perspectives. We frame\nthis as retrieval augmented generation, where perspectives are retrieved from a\nknowledge base and the LLM is tasked with generating a fluent and faithful\nresponse from the given perspectives. As a starting point, we use a\ndeterministic retrieval system and then focus on common LLM failure modes that\narise during this approach to text generation, namely hallucination and\ncoverage errors. We propose and evaluate three methods to detect such errors\nbased on (1) word-overlap, (2) salience, and (3) LLM-based classifiers. Our\nresults demonstrate that LLM-based classifiers, even when trained only on\nsynthetic errors, achieve high error detection performance, with ROC AUC scores\nof 95.3% for hallucination and 90.5% for coverage error detection on\nunambiguous error cases. We show that when no training data is available, our\nother methods still yield good results on hallucination (84.0%) and coverage\nerror (85.2%) detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.08904v1.pdf"
    },
    {
        "title": "CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments",
        "authors": [
            "Savitha Sam Abraham",
            "Marjan Alirezaie",
            "Luc De Raedt"
        ],
        "published": "2024-03-05T18:41:37Z",
        "summary": "The integration of learning and reasoning is high on the research agenda in\nAI. Nevertheless, there is only a little attention to use existing background\nknowledge for reasoning about partially observed scenes to answer questions\nabout the scene. Yet, we as humans use such knowledge frequently to infer\nplausible answers to visual questions (by eliminating all inconsistent ones).\nSuch knowledge often comes in the form of constraints about objects and it\ntends to be highly domain or environment-specific. We contribute a novel\nbenchmark called CLEVR-POC for reasoning-intensive visual question answering\n(VQA) in partially observable environments under constraints. In CLEVR-POC,\nknowledge in the form of logical constraints needs to be leveraged to generate\nplausible answers to questions about a hidden object in a given partial scene.\nFor instance, if one has the knowledge that all cups are colored either red,\ngreen or blue and that there is only one green cup, it becomes possible to\ndeduce the color of an occluded cup as either red or blue, provided that all\nother cups, including the green one, are observed. Through experiments, we\nobserve that the low performance of pre-trained vision language models like\nCLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC\nascertains the necessity for frameworks that can handle reasoning-intensive\ntasks where environment-specific background knowledge is available and crucial.\nFurthermore, our demonstration illustrates that a neuro-symbolic model, which\nintegrates an LLM like GPT-4 with a visual perception network and a formal\nlogical reasoner, exhibits exceptional performance on CLEVR-POC.",
        "pdf_link": "https://arxiv.org/pdf/2403.03203v1.pdf"
    },
    {
        "title": "Identifying Semantic Induction Heads to Understand In-Context Learning",
        "authors": [
            "Jie Ren",
            "Qipeng Guo",
            "Hang Yan",
            "Dongrui Liu",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2024-02-20T14:43:39Z",
        "summary": "Although large language models (LLMs) have demonstrated remarkable\nperformance, the lack of transparency in their inference logic raises concerns\nabout their trustworthiness. To gain a better understanding of LLMs, we conduct\na detailed analysis of the operations of attention heads and aim to better\nunderstand the in-context learning of LLMs. Specifically, we investigate\nwhether attention heads encode two types of relationships between tokens\npresent in natural languages: the syntactic dependency parsed from sentences\nand the relation within knowledge graphs. We find that certain attention heads\nexhibit a pattern where, when attending to head tokens, they recall tail tokens\nand increase the output logits of those tail tokens. More crucially, the\nformulation of such semantic induction heads has a close correlation with the\nemergence of the in-context learning ability of language models. The study of\nsemantic attention heads advances our understanding of the intricate operations\nof attention heads in transformers, and further provides new insights into the\nin-context learning of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.13055v1.pdf"
    },
    {
        "title": "Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning",
        "authors": [
            "Kaipeng Wang",
            "Zhi Jing",
            "Yongye Su",
            "Yikun Han"
        ],
        "published": "2024-03-10T06:30:54Z",
        "summary": "This paper delves into enhancing the classification performance on the\nGoEmotions dataset, a large, manually annotated dataset for emotion detection\nin text. The primary goal of this paper is to address the challenges of\ndetecting subtle emotions in text, a complex issue in Natural Language\nProcessing (NLP) with significant practical applications. The findings offer\nvaluable insights into addressing the challenges of emotion detection in text\nand suggest directions for future research, including the potential for a\nsurvey paper that synthesizes methods and performances across various datasets\nin this domain.",
        "pdf_link": "https://arxiv.org/pdf/2403.06108v2.pdf"
    },
    {
        "title": "DEEM: Dynamic Experienced Expert Modeling for Stance Detection",
        "authors": [
            "Xiaolong Wang",
            "Yile Wang",
            "Sijie Cheng",
            "Peng Li",
            "Yang Liu"
        ],
        "published": "2024-02-23T11:24:00Z",
        "summary": "Recent work has made a preliminary attempt to use large language models\n(LLMs) to solve the stance detection task, showing promising results. However,\nconsidering that stance detection usually requires detailed background\nknowledge, the vanilla reasoning method may neglect the domain knowledge to\nmake a professional and accurate analysis. Thus, there is still room for\nimprovement of LLMs reasoning, especially in leveraging the generation\ncapability of LLMs to simulate specific experts (i.e., multi-agents) to detect\nthe stance. In this paper, different from existing multi-agent works that\nrequire detailed descriptions and use fixed experts, we propose a Dynamic\nExperienced Expert Modeling (DEEM) method which can leverage the generated\nexperienced experts and let LLMs reason in a semi-parametric way, making the\nexperts more generalizable and reliable. Experimental results demonstrate that\nDEEM consistently achieves the best results on three standard benchmarks,\noutperforms methods with self-consistency reasoning, and reduces the bias of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15264v1.pdf"
    },
    {
        "title": "BRAIn: Bayesian Reward-conditioned Amortized Inference for natural language generation from feedback",
        "authors": [
            "Gaurav Pandey",
            "Yatin Nandwani",
            "Tahira Naseem",
            "Mayank Mishra",
            "Guangxuan Xu",
            "Dinesh Raghu",
            "Sachindra Joshi",
            "Asim Munawar",
            "Ram\u00f3n Fernandez Astudillo"
        ],
        "published": "2024-02-04T13:16:29Z",
        "summary": "Following the success of Proximal Policy Optimization (PPO) for Reinforcement\nLearning from Human Feedback (RLHF), new techniques such as Sequence Likelihood\nCalibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that\nare offline in nature and use rewards in an indirect manner. These techniques,\nin particular DPO, have recently become the tools of choice for LLM alignment\ndue to their scalability and performance. However, they leave behind important\nfeatures of the PPO approach. Methods such as SLiC or RRHF make use of the\nReward Model (RM) only for ranking/preference, losing fine-grained information\nand ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce),\nwhile methods such as DPO do not use even a separate reward model. In this\nwork, we propose a novel approach, named BRAIn, that re-introduces the RM as\npart of a distribution matching approach.BRAIn considers the LLM distribution\nconditioned on the assumption of output goodness and applies Bayes theorem to\nderive an intractable posterior distribution where the RM is explicitly\nrepresented. BRAIn then distills this posterior into an amortized inference\nnetwork through self-normalized importance sampling, leading to a scalable\noffline algorithm that significantly outperforms prior art in summarization and\nAntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for\nspecific RM choices.",
        "pdf_link": "https://arxiv.org/pdf/2402.02479v1.pdf"
    },
    {
        "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
        "authors": [
            "Xiyao Wang",
            "Yuhang Zhou",
            "Xiaoyu Liu",
            "Hongjin Lu",
            "Yuancheng Xu",
            "Feihong He",
            "Jaehong Yoon",
            "Taixi Lu",
            "Gedas Bertasius",
            "Mohit Bansal",
            "Huaxiu Yao",
            "Furong Huang"
        ],
        "published": "2024-01-19T07:10:13Z",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in\nhandling a variety of visual-language tasks. However, current MLLM benchmarks\nare predominantly designed to evaluate reasoning based on static information\nabout a single image, and the ability of modern MLLMs to extrapolate from image\nsequences, which is essential for understanding our ever-changing world, has\nbeen less investigated. To address this challenge, this paper introduces\nMementos, a new benchmark designed to assess MLLMs' sequential image reasoning\nabilities. Mementos features 4,761 diverse image sequences with varying\nlengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning\nperformance. Through a careful evaluation of nine recent MLLMs on Mementos,\nincluding GPT-4V and Gemini, we find that they struggle to accurately describe\ndynamic information about given image sequences, often leading to\nhallucinations/misrepresentations of objects and their corresponding behaviors.\nOur quantitative analysis and case studies identify three key factors impacting\nMLLMs' sequential image reasoning: the correlation between object and\nbehavioral hallucinations, the influence of cooccurring behaviors, and the\ncompounding impact of behavioral hallucinations. Our dataset is available at\nhttps://github.com/umd-huang-lab/Mementos.",
        "pdf_link": "https://arxiv.org/pdf/2401.10529v2.pdf"
    },
    {
        "title": "SoMeLVLM: A Large Vision Language Model for Social Media Processing",
        "authors": [
            "Xinnong Zhang",
            "Haoyu Kuang",
            "Xinyi Mou",
            "Hanjia Lyu",
            "Kun Wu",
            "Siming Chen",
            "Jiebo Luo",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "published": "2024-02-20T14:02:45Z",
        "summary": "The growth of social media, characterized by its multimodal nature, has led\nto the emergence of diverse phenomena and challenges, which calls for an\neffective approach to uniformly solve automated tasks. The powerful Large\nVision Language Models make it possible to handle a variety of tasks\nsimultaneously, but even with carefully designed prompting methods, the general\ndomain models often fall short in aligning with the unique speaking style and\ncontext of social media tasks. In this paper, we introduce a Large Vision\nLanguage Model for Social Media Processing (SoMeLVLM), which is a cognitive\nframework equipped with five key capabilities including knowledge &\ncomprehension, application, analysis, evaluation, and creation. SoMeLVLM is\ndesigned to understand and generate realistic social media behavior. We have\ndeveloped a 654k multimodal social media instruction-tuning dataset to support\nour cognitive framework and fine-tune our model. Our experiments demonstrate\nthat SoMeLVLM achieves state-of-the-art performance in multiple social media\ntasks. Further analysis shows its significant advantages over baselines in\nterms of cognitive abilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.13022v1.pdf"
    },
    {
        "title": "Learning to Compress Prompt in Natural Language Formats",
        "authors": [
            "Yu-Neng Chuang",
            "Tianwei Xing",
            "Chia-Yuan Chang",
            "Zirui Liu",
            "Xun Chen",
            "Xia Hu"
        ],
        "published": "2024-02-28T20:41:21Z",
        "summary": "Large language models (LLMs) are great at processing multiple natural\nlanguage processing tasks, but their abilities are constrained by inferior\nperformance with long context, slow inference speed, and the high cost of\ncomputing the results. Deploying LLMs with precise and informative context\nhelps users process large-scale datasets more effectively and cost-efficiently.\nExisting works rely on compressing long prompt contexts into soft prompts.\nHowever, soft prompt compression encounters limitations in transferability\nacross different LLMs, especially API-based LLMs. To this end, this work aims\nto compress lengthy prompts in the form of natural language with LLM\ntransferability. This poses two challenges: (i) Natural Language (NL) prompts\nare incompatible with back-propagation, and (ii) NL prompts lack flexibility in\nimposing length constraints. In this work, we propose a Natural Language Prompt\nEncapsulation (Nano-Capsulator) framework compressing original prompts into NL\nformatted Capsule Prompt while maintaining the prompt utility and\ntransferability. Specifically, to tackle the first challenge, the\nNano-Capsulator is optimized by a reward function that interacts with the\nproposed semantics preserving loss. To address the second question, the\nNano-Capsulator is optimized by a reward function featuring length constraints.\nExperimental results demonstrate that the Capsule Prompt can reduce 81.4% of\nthe original length, decrease inference latency up to 4.5x, and save 80.1% of\nbudget overheads while providing transferability across diverse LLMs and\ndifferent datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.18700v2.pdf"
    },
    {
        "title": "Exploring the Impact of the Output Format on the Evaluation of Large Language Models for Code Translation",
        "authors": [
            "Marcos Macedo",
            "Yuan Tian",
            "Filipe R. Cogo",
            "Bram Adams"
        ],
        "published": "2024-03-25T21:41:31Z",
        "summary": "Code translation between programming languages is a long-existing and\ncritical task in software engineering, facilitating the modernization of legacy\nsystems, ensuring cross-platform compatibility, and enhancing software\nperformance. With the recent advances in large language models (LLMs) and their\napplications to code translation, there is an increasing need for comprehensive\nevaluation of these models. In this study, we empirically analyze the generated\noutputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B\nup to 46.7B on 3,820 translation pairs across five languages, including C, C++,\nGo, Java, and Python. Our analysis found that between 26.4% and 73.7% of code\ntranslations produced by our evaluated LLMs necessitate post-processing, as\nthese translations often include a mix of code, quotes, and text rather than\nbeing purely source code. Overlooking the output format of these models can\ninadvertently lead to underestimation of their actual performance. This is\nparticularly evident when evaluating them with execution-based metrics such as\nComputational Accuracy (CA). Our results demonstrate that a strategic\ncombination of prompt engineering and regular expression can effectively\nextract the source code from the model generation output. In particular, our\nmethod can help eleven selected models achieve an average Code Extraction\nSuccess Rate (CSR) of 92.73%. Our findings shed light on and motivate future\nresearch to conduct more reliable benchmarks of LLMs for code translation.",
        "pdf_link": "https://arxiv.org/pdf/2403.17214v1.pdf"
    },
    {
        "title": "Towards Robustness and Diversity: Continual Learning in Dialog Generation with Text-Mixup and Batch Nuclear-Norm Maximization",
        "authors": [
            "Zihan Wang",
            "Jiayu Xiao",
            "Mengxiang Li",
            "Zhongjiang He",
            "Yongxiang Li",
            "Chao Wang",
            "Shuangyong Song"
        ],
        "published": "2024-03-16T11:09:27Z",
        "summary": "In our dynamic world where data arrives in a continuous stream, continual\nlearning enables us to incrementally add new tasks/domains without the need to\nretrain from scratch. A major challenge in continual learning of language model\nis catastrophic forgetting, the tendency of models to forget knowledge from\npreviously trained tasks/domains when training on new ones. This paper studies\ndialog generation under the continual learning setting. We propose a novel\nmethod that 1) uses \\textit{Text-Mixup} as data augmentation to avoid model\noverfitting on replay memory and 2) leverages Batch-Nuclear Norm Maximization\n(BNNM) to alleviate the problem of mode collapse. Experiments on a $37$-domain\ntask-oriented dialog dataset and DailyDialog (a $10$-domain chitchat dataset)\ndemonstrate that our proposed approach outperforms the state-of-the-art in\ncontinual learning.",
        "pdf_link": "https://arxiv.org/pdf/2403.10894v1.pdf"
    },
    {
        "title": "Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data",
        "authors": [
            "Yubin Kim",
            "Xuhai Xu",
            "Daniel McDuff",
            "Cynthia Breazeal",
            "Hae Won Park"
        ],
        "published": "2024-01-12T19:40:11Z",
        "summary": "Large language models (LLMs) are capable of many natural language tasks, yet\nthey are far from perfect. In health applications, grounding and interpreting\ndomain-specific and non-linguistic data is important. This paper investigates\nthe capacity of LLMs to deliver multi-modal health predictions based on\ncontextual information (e.g. user demographics, health knowledge) and\nphysiological data (e.g. resting heart rate, sleep minutes). We present a\ncomprehensive evaluation of eight state-of-the-art LLMs with diverse prompting\nand fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps,\nGLOBEM, AW_FB, MIT-BIH & MIMIC-III). Our experiments cover thirteen consumer\nhealth prediction tasks in mental health, activity, metabolic, sleep, and\ncardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable\nperformance to larger models (GPT-3.5 and GPT-4), achieving the best\nperformance in 5 out of 13 tasks. Ablation studies highlight the effectiveness\nof context enhancement strategies, and generalization capability of the\nfine-tuned models across training datasets and the size of training samples.\nNotably, we observe that our context enhancement can yield up to 23.8%\nimprovement in performance. While constructing contextually rich prompts\n(combining user context, health knowledge and temporal information) exhibits\nsynergistic improvement, the inclusion of health knowledge context in prompts\nsignificantly enhances overall performance.",
        "pdf_link": "https://arxiv.org/pdf/2401.06866v1.pdf"
    },
    {
        "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models",
        "authors": [
            "Haibo Jin",
            "Ruoxi Chen",
            "Andy Zhou",
            "Jinyin Chen",
            "Yang Zhang",
            "Haohan Wang"
        ],
        "published": "2024-02-05T18:54:43Z",
        "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.",
        "pdf_link": "https://arxiv.org/pdf/2402.03299v3.pdf"
    },
    {
        "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
        "authors": [
            "Robert Lakatos",
            "Peter Pollner",
            "Andras Hajdu",
            "Tamas Joo"
        ],
        "published": "2024-03-12T21:06:31Z",
        "summary": "The development of generative large language models (G-LLM) opened up new\nopportunities for the development of new types of knowledge-based systems\nsimilar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented\nGeneration (RAG) are the techniques that can be used to implement domain\nadaptation for the development of G-LLM-based knowledge systems. In our study,\nusing ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine\nthe performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2\nlanguage models. Based on measurements shown on different datasets, we\ndemonstrate that RAG-based constructions are more efficient than models\nproduced with FN. We point out that connecting RAG and FN is not trivial,\nbecause connecting FN models with RAG can cause a decrease in performance.\nFurthermore, we outline a simple RAG-based architecture which, on average,\noutperforms the FN models by 16% in terms of the ROGUE score, 15% in the case\nof the BLEU score, and 53% based on the cosine similarity. This shows the\nsignificant advantage of RAG over FN in terms of hallucination, which is not\noffset by the fact that the average 8% better METEOR score of FN models\nindicates greater creativity compared to RAG.",
        "pdf_link": "https://arxiv.org/pdf/2403.09727v1.pdf"
    },
    {
        "title": "Tell me the truth: A system to measure the trustworthiness of Large Language Models",
        "authors": [
            "Carlo Lipizzi"
        ],
        "published": "2024-03-08T00:27:57Z",
        "summary": "Large Language Models (LLM) have taken the front seat in most of the news\nsince November 2022, when ChatGPT was introduced. After more than one year, one\nof the major reasons companies are resistant to adopting them is the limited\nconfidence they have in the trustworthiness of those systems. In a study by\n(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in\nidentifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics\nfound that ChatGPT has an accuracy rate of 17% percent when diagnosing\npediatric medical cases (Barile et al., 2024). But then, what is \"trust\"? Trust\nis a relative, subject condition that can change based on culture, domain,\nindividuals. And then, given a domain, how can the trustworthiness of a system\nbe measured? In this paper, I present a systematic approach to measure\ntrustworthiness based on a predefined ground truth, represented as a knowledge\ngraph of the domain. The approach is a process with humans in the loop to\nvalidate the representation of the domain and to fine-tune the system.\n  Measuring the trustworthiness would be essential for all the entities\noperating in critical environments, such as healthcare, defense, finance, but\nit would be very relevant for all the users of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.04964v2.pdf"
    },
    {
        "title": "LIEDER: Linguistically-Informed Evaluation for Discourse Entity Recognition",
        "authors": [
            "Xiaomeng Zhu",
            "Robert Frank"
        ],
        "published": "2024-03-10T20:20:16Z",
        "summary": "Discourse Entity (DE) recognition is the task of identifying novel and known\nentities introduced within a text. While previous work has found that large\nlanguage models have basic, if imperfect, DE recognition abilities (Schuster\nand Linzen, 2022), it remains largely unassessed which of the fundamental\nsemantic properties that govern the introduction and subsequent reference to\nDEs they have knowledge of. We propose the Linguistically-Informed Evaluation\nfor Discourse Entity Recognition (LIEDER) dataset that allows for a detailed\nexamination of language models' knowledge of four crucial semantic properties:\nexistence, uniqueness, plurality, and novelty. We find evidence that\nstate-of-the-art large language models exhibit sensitivity to all of these\nproperties except novelty, which demonstrates that they have yet to reach\nhuman-level language understanding abilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.06301v1.pdf"
    },
    {
        "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension",
        "authors": [
            "Ting Jiang",
            "Shaohan Huang",
            "Shengyue Luo",
            "Zihan Zhang",
            "Haizhen Huang",
            "Furu Wei",
            "Weiwei Deng",
            "Feng Sun",
            "Qi Zhang",
            "Deqing Wang",
            "Fuzhen Zhuang"
        ],
        "published": "2024-01-14T13:11:31Z",
        "summary": "To enhance the domain-specific capabilities of large language models,\ncontinued pre-training on a domain-specific corpus is a prevalent method.\nRecent work demonstrates that adapting models using reading comprehension data\nformatted by regex-based patterns can significantly improve performance on\ndomain-specific tasks. However, regex-based patterns are incapable of parsing\nraw corpora using domain-specific knowledge. Furthermore, the question and\nanswer pairs are extracted directly from the corpus in predefined formats\noffers limited context. To address this limitation, we improve reading\ncomprehension via LLM and clustering. LLM focuses on leveraging domain\nknowledge within the corpus to refine comprehension stage, while clustering\nsupplies relevant knowledge by extending the context to enrich reading stage.\nAdditionally, our method incorporates parameter-efficient fine-tuning to\nimprove the efficiency of domain adaptation. In comparison to AdaptLLM, our\nmethod achieves an improvement exceeding 5% in domain-specific tasks. Our code\nwill available at https://github.com/microsoft/LMOps.",
        "pdf_link": "https://arxiv.org/pdf/2401.07284v2.pdf"
    },
    {
        "title": "Adapting to Teammates in a Cooperative Language Game",
        "authors": [
            "Christopher Archibald",
            "Spencer Brosnahan"
        ],
        "published": "2024-02-26T23:15:07Z",
        "summary": "The game of Codenames has recently emerged as a domain of interest for\nintelligent agent design. The game is unique due to the way that language and\ncoordination between teammates play important roles. Previous approaches to\ndesigning agents for this game have utilized a single internal language model\nto determine action choices. This often leads to good performance with some\nteammates and inferior performance with other teammates, as the agent cannot\nadapt to any specific teammate. In this paper we present the first adaptive\nagent for playing Codenames. We adopt an ensemble approach with the goal of\ndetermining, during the course of interacting with a specific teammate, which\nof our internal expert agents, each potentially with its own language model, is\nthe best match. One difficulty faced in this approach is the lack of a single\nnumerical metric that accurately captures the performance of a Codenames team.\nPrior Codenames research has utilized a handful of different metrics to\nevaluate agent teams. We propose a novel single metric to evaluate the\nperformance of a Codenames team, whether playing a single team (solitaire)\ngame, or a competitive game against another team. We then present and analyze\nan ensemble agent which selects an internal expert on each turn in order to\nmaximize this proposed metric. Experimental analysis shows that this ensemble\napproach adapts to individual teammates and often performs nearly as well as\nthe best internal expert with a teammate. Crucially, this success does not\ndepend on any previous knowledge about the teammates, the ensemble agents, or\ntheir compatibility. This research represents an important step to making\nlanguage-based agents for cooperative language settings like Codenames more\nadaptable to individual teammates.",
        "pdf_link": "https://arxiv.org/pdf/2403.00823v1.pdf"
    },
    {
        "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
        "authors": [
            "Hao Tang",
            "Darren Key",
            "Kevin Ellis"
        ],
        "published": "2024-02-19T16:39:18Z",
        "summary": "We give a model-based agent that builds a Python program representing its\nknowledge of the world based on its interactions with the environment. The\nworld model tries to explain its interactions, while also being optimistic\nabout what reward it can achieve. We do this by extending work on program\nsynthesis via LLMs. We study our agent on gridworlds, finding our approach is\nmore sample-efficient compared to deep RL, and more compute-efficient compared\nto ReAct-style agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.12275v1.pdf"
    },
    {
        "title": "Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access",
        "authors": [
            "Saibo Geng",
            "Berkay D\u00f6ner",
            "Chris Wendler",
            "Martin Josifoski",
            "Robert West"
        ],
        "published": "2024-01-18T13:31:24Z",
        "summary": "Constrained decoding, a technique for enforcing constraints on language model\noutputs, offers a way to control text generation without retraining or\narchitectural modifications. Its application is, however, typically restricted\nto models that give users access to next-token distributions (usually via\nsoftmax logits), which poses a limitation with blackbox large language models\n(LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a\nnovel approach to constrained decoding for blackbox LLMs, which operates\nwithout access to the logits of the blackbox LLM. SGCD utilizes a locally\nhosted auxiliary model to refine the output of an unconstrained blackbox LLM,\neffectively treating this initial output as a \"sketch\" for further elaboration.\nThis approach is complementary to traditional logit-based techniques and\nenables the application of constrained decoding in settings where full model\ntransparency is unavailable. We demonstrate the efficacy of SGCD through\nexperiments in closed information extraction and constituency parsing, showing\nhow it enhances the utility and flexibility of blackbox LLMs for complex NLP\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.09967v1.pdf"
    },
    {
        "title": "Compensatory Biases Under Cognitive Load: Reducing Selection Bias in Large Language Models",
        "authors": [
            "J. E. Eicher",
            "R. F. Irgoli\u010d"
        ],
        "published": "2024-01-29T15:43:23Z",
        "summary": "Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have\nbecome instrumental in interpreting and executing semantic-based tasks.\nUnfortunately, these models' inherent biases, akin to human cognitive biases,\nadversely affect their performance. Particularly affected is object selection\nfrom lists; a fundamental operation in digital navigation and decision-making.\nThis research critically examines these biases and quantifies the effects on a\nrepresentative list selection task. To explore these biases, we conducted a\nseries of controlled experiments, manipulating temperature, list length, object\nidentity, object type, prompt complexity, and model. This enabled us to isolate\nand measure the influence of the biases on selection behavior. Our findings\nshow that bias structure is strongly dependent on the model, with object type\nmodulating the magnitude of the effect. With a strong primacy effect, causing\nthe first objects in a list to be disproportionately represented in outputs.\nFurthermore the usage of guard rails, a prompt engineering method of ensuring a\nresponse structure, can increase bias and decrease instruction adherence when\ncombined with a selection task. The bias is ablated when the guard rail step is\nseparated from the list sampling step, lowering the complexity of each\nindividual task. The implications of this research are two-fold, practically\nproviding a guide for designing unbiased LLM applications and theoretically\nsuggesting that LLMs experience a form of cognitive load compensated for by\nincreasing bias.",
        "pdf_link": "https://arxiv.org/pdf/2402.01740v2.pdf"
    },
    {
        "title": "Knowledge Graph Enhanced Large Language Model Editing",
        "authors": [
            "Mengqi Zhang",
            "Xiaotian Ye",
            "Qiang Liu",
            "Pengjie Ren",
            "Shu Wu",
            "Zhumin Chen"
        ],
        "published": "2024-02-21T07:52:26Z",
        "summary": "Large language models (LLMs) are pivotal in advancing natural language\nprocessing (NLP) tasks, yet their efficacy is hampered by inaccuracies and\noutdated knowledge. Model editing emerges as a promising solution to address\nthese challenges. However, existing editing methods struggle to track and\nincorporate changes in knowledge associated with edits, which limits the\ngeneralization ability of postedit LLMs in processing edited knowledge. To\ntackle these problems, we propose a novel model editing method that leverages\nknowledge graphs for enhancing LLM editing, namely GLAME. Specifically, we\nfirst utilize a knowledge graph augmentation module to uncover associated\nknowledge that has changed due to editing, obtaining its internal\nrepresentations within LLMs. This approach allows knowledge alterations within\nLLMs to be reflected through an external graph structure. Subsequently, we\ndesign a graph-based knowledge edit module to integrate structured knowledge\ninto the model editing. This ensures that the updated parameters reflect not\nonly the modifications of the edited knowledge but also the changes in other\nassociated knowledge resulting from the editing process. Comprehensive\nexperiments conducted on GPT-J and GPT-2 XL demonstrate that GLAME\nsignificantly improves the generalization capabilities of post-edit LLMs in\nemploying edited knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2402.13593v1.pdf"
    },
    {
        "title": "From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto",
        "authors": [
            "Segev Wasserkrug",
            "Leonard Boussioux",
            "Dick den Hertog",
            "Farzaneh Mirzazadeh",
            "Ilker Birbil",
            "Jannis Kurtz",
            "Donato Maragno"
        ],
        "published": "2024-02-26T03:10:11Z",
        "summary": "Significantly simplifying the creation of optimization models for real-world\nbusiness problems has long been a major goal in applying mathematical\noptimization more widely to important business and societal decisions. The\nrecent capabilities of Large Language Models (LLMs) present a timely\nopportunity to achieve this goal. Therefore, we propose research at the\nintersection of LLMs and optimization to create a Decision Optimization CoPilot\n(DOCP) - an AI tool designed to assist any decision maker, interacting in\nnatural language to grasp the business problem, subsequently formulating and\nsolving the corresponding optimization model. This paper outlines our DOCP\nvision and identifies several fundamental requirements for its implementation.\nWe describe the state of the art through a literature survey and experiments\nusing ChatGPT. We show that a) LLMs already provide substantial novel\ncapabilities relevant to a DOCP, and b) major research challenges remain to be\naddressed. We also propose possible research directions to overcome these gaps.\nWe also see this work as a call to action to bring together the LLM and\noptimization communities to pursue our vision, thereby enabling much more\nwidespread improved decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.16269v1.pdf"
    },
    {
        "title": "AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation",
        "authors": [
            "Orit Shaer",
            "Angelora Cooper",
            "Osnat Mokryn",
            "Andrew L. Kun",
            "Hagit Ben Shoshan"
        ],
        "published": "2024-02-22T21:34:52Z",
        "summary": "The growing availability of generative AI technologies such as large language\nmodels (LLMs) has significant implications for creative work. This paper\nexplores twofold aspects of integrating LLMs into the creative process - the\ndivergence stage of idea generation, and the convergence stage of evaluation\nand selection of ideas. We devised a collaborative group-AI Brainwriting\nideation framework, which incorporated an LLM as an enhancement into the group\nideation process, and evaluated the idea generation process and the resulted\nsolution space. To assess the potential of using LLMs in the idea evaluation\nprocess, we design an evaluation engine and compared it to idea ratings\nassigned by three expert and six novice evaluators. Our findings suggest that\nintegrating LLM in Brainwriting could enhance both the ideation process and its\noutcome. We also provide evidence that LLMs can support idea evaluation. We\nconclude by discussing implications for HCI education and practice.",
        "pdf_link": "https://arxiv.org/pdf/2402.14978v2.pdf"
    },
    {
        "title": "Solution-oriented Agent-based Models Generation with Verifier-assisted Iterative In-context Learning",
        "authors": [
            "Tong Niu",
            "Weihao Zhang",
            "Rong Zhao"
        ],
        "published": "2024-02-04T07:59:06Z",
        "summary": "Agent-based models (ABMs) stand as an essential paradigm for proposing and\nvalidating hypothetical solutions or policies aimed at addressing challenges\nposed by complex systems and achieving various objectives. This process demands\nlabor-intensive endeavors and multidisciplinary expertise. Large language\nmodels (LLMs) encapsulating cross-domain knowledge and programming proficiency\ncould potentially alleviate the difficulty of this process. However, LLMs excel\nin handling sequential information, making it challenging for analyzing the\nintricate interactions and nonlinear dynamics inherent in ABMs. Additionally,\ndue to the lack of self-evaluation capability of LLMs, relying solely on LLMs\nis insufficient to effectively accomplish this process. In this paper, we\npresent SAGE, a general solution-oriented ABM generation framework designed for\nautomatic modeling and generating solutions for targeted problems. Unlike\napproaches reliant on expert handcrafting or resource-intensive neural network\ntraining, SAGE establishes a verifier-assisted iterative in-context learning\nprocess employing large language models (LLMs) to leverages their inherent\ncross-domain knowledge for tackling intricate demands from diverse domain\nscenarios. In SAGE, we introduce an semi-structured conceptual representation\nexpliciting the intricate structures of ABMs and an objective representation to\nguide LLMs in modeling scenarios and proposing hypothetical solutions through\nin-context learning. To ensure the model executability and solution\nfeasibility, SAGE devises a two-level verifier with chain-of-thought prompting\ntailored to the complex interactions and non-linear dynamics of ABMs, driving\nthe iterative generation optimization. Moreover, we construct an evaluation\ndataset of solution-oriented ABMs from open sources.It contains practical\nmodels across various domains.",
        "pdf_link": "https://arxiv.org/pdf/2402.02388v1.pdf"
    },
    {
        "title": "A Little Leak Will Sink a Great Ship: Survey of Transparency for Large Language Models from Start to Finish",
        "authors": [
            "Masahiro Kaneko",
            "Timothy Baldwin"
        ],
        "published": "2024-03-24T13:21:58Z",
        "summary": "Large Language Models (LLMs) are trained on massive web-crawled corpora. This\nposes risks of leakage, including personal information, copyrighted texts, and\nbenchmark datasets. Such leakage leads to undermining human trust in AI due to\npotential unauthorized generation of content or overestimation of performance.\nWe establish the following three criteria concerning the leakage issues: (1)\nleakage rate: the proportion of leaked data in training data, (2) output rate:\nthe ease of generating leaked data, and (3) detection rate: the detection\nperformance of leaked versus non-leaked data. Despite the leakage rate being\nthe origin of data leakage issues, it is not understood how it affects the\noutput rate and detection rate. In this paper, we conduct an experimental\nsurvey to elucidate the relationship between the leakage rate and both the\noutput rate and detection rate for personal information, copyrighted texts, and\nbenchmark data. Additionally, we propose a self-detection approach that uses\nfew-shot learning in which LLMs detect whether instances are present or absent\nin their training data, in contrast to previous methods that do not employ\nexplicit learning. To explore the ease of generating leaked information, we\ncreate a dataset of prompts designed to elicit personal information,\ncopyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs\nproduce leaked information in most cases despite less such data in their\ntraining set. This indicates even small amounts of leaked data can greatly\naffect outputs. Our self-detection method showed superior performance compared\nto existing detection methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.16139v1.pdf"
    },
    {
        "title": "StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows",
        "authors": [
            "Yiran Wu",
            "Tianwei Yue",
            "Shaokun Zhang",
            "Chi Wang",
            "Qingyun Wu"
        ],
        "published": "2024-03-17T19:54:16Z",
        "summary": "It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.11322v2.pdf"
    },
    {
        "title": "Few-Shot VQA with Frozen LLMs: A Tale of Two Approaches",
        "authors": [
            "Igor Sterner",
            "Weizhe Lin",
            "Jinghong Chen",
            "Bill Byrne"
        ],
        "published": "2024-03-17T19:44:05Z",
        "summary": "Two approaches have emerged to input images into large language models\n(LLMs). The first is to caption images into natural language. The second is to\nmap image feature embeddings into the domain of the LLM and pass the mapped\nembeddings directly to the LLM. The majority of recent few-shot multimodal work\nreports performance using architectures that employ variations of one of these\ntwo approaches. But they overlook an important comparison between them. We\ndesign a controlled and focused experiment to compare these two approaches to\nfew-shot visual question answering (VQA) with LLMs. Our findings indicate that\nfor Flan-T5 XL, a 3B parameter LLM, connecting visual embeddings directly to\nthe LLM embedding space does not guarantee improved performance over using\nimage captions. In the zero-shot regime, we find using textual image captions\nis better. In the few-shot regimes, how the in-context examples are selected\ndetermines which is better.",
        "pdf_link": "https://arxiv.org/pdf/2403.11317v1.pdf"
    },
    {
        "title": "Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation",
        "authors": [
            "Xianghe Pang",
            "Shuo Tang",
            "Rui Ye",
            "Yuxin Xiong",
            "Bolun Zhang",
            "Yanfeng Wang",
            "Siheng Chen"
        ],
        "published": "2024-02-08T14:21:03Z",
        "summary": "Aligning large language models (LLMs) with human values is imperative to\nmitigate potential adverse effects resulting from their misuse. Drawing from\nthe sociological insight that acknowledging all parties' concerns is a key\nfactor in shaping human values, this paper proposes a novel direction to align\nLLMs by themselves: social scene simulation. To achieve this, we present\nMATRIX, a novel social scene simulator that emulates realistic scenes around a\nuser's input query, enabling the LLM to take social consequences into account\nbefore responding. MATRIX serves as a virtual rehearsal space, akin to a\nMonopolylogue, where the LLM performs diverse roles related to the query and\npractice by itself. To inject this alignment, we fine-tune the LLM with\nMATRIX-simulated data, ensuring adherence to human values without compromising\ninference speed. We theoretically show that the LLM with MATRIX outperforms\nConstitutional AI under mild assumptions. Finally, extensive experiments\nvalidate that our method outperforms over 10 baselines across 4 benchmarks. As\nevidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning\nwith human values. Our project page is available at\nhttps://shuotang123.github.io/MATRIX.",
        "pdf_link": "https://arxiv.org/pdf/2402.05699v2.pdf"
    },
    {
        "title": "BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Wei Wei",
            "Chao Zhang",
            "Bo Dai"
        ],
        "published": "2024-02-13T05:15:46Z",
        "summary": "Adapting state-of-the-art Large Language Models (LLMs) like GPT-4 and Gemini\nfor specific tasks is challenging. Due to the opacity in their parameters,\nembeddings, and even output probabilities, existing fine-tuning adaptation\nmethods are inapplicable. Consequently, adapting these black-box LLMs is only\npossible through their API services, raising concerns about transparency,\nprivacy, and cost. To address these challenges, we introduce BBox-Adapter, a\nnovel lightweight adapter for black-box LLMs. BBox-Adapter distinguishes target\nand source domain data by treating target data as positive and source data as\nnegative. It employs a ranking-based Noise Contrastive Estimation (NCE) loss to\npromote the likelihood of target domain data while penalizing that of the\nsource domain. Furthermore, it features an online adaptation mechanism, which\nincorporates real-time positive data sampling from ground-truth, human, or AI\nfeedback, coupled with negative data from previous adaptations. Extensive\nexperiments demonstrate BBox-Adapter's effectiveness and cost efficiency. It\nimproves model performance by up to 6.77% across diverse tasks and domains,\nwhile reducing training and inference costs by 31.30x and 1.84x, respectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.08219v1.pdf"
    },
    {
        "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
        "authors": [
            "Qintong Li",
            "Leyang Cui",
            "Xueliang Zhao",
            "Lingpeng Kong",
            "Wei Bi"
        ],
        "published": "2024-02-29T15:26:14Z",
        "summary": "Large language models (LLMs) have achieved impressive performance across\nvarious mathematical reasoning benchmarks. However, there are increasing\ndebates regarding whether these models truly understand and apply mathematical\nknowledge or merely rely on shortcuts for mathematical reasoning. One essential\nand frequently occurring evidence is that when the math questions are slightly\nchanged, LLMs can behave incorrectly. This motivates us to evaluate the\nrobustness of LLMs' math reasoning capability by testing a wide range of\nquestion variations. We introduce the adversarial grade school math\n(\\datasetname) dataset, an extension of GSM8K augmented with various\nmathematical perturbations. Our experiments on 25 LLMs and 4 prompting\ntechniques show that while LLMs exhibit different levels of math reasoning\nabilities, their performances are far from robust. In particular, even for\nproblems that have been solved in GSM8K, LLMs can make mistakes when new\nstatements are added or the question targets are altered. We also explore\nwhether more robust performance can be achieved by composing existing prompting\nmethods, in which we try an iterative method that generates and verifies each\nintermediate thought based on its reasoning goal and calculation result. Code\nand data are available at \\url{https://github.com/qtli/GSM-Plus}.",
        "pdf_link": "https://arxiv.org/pdf/2402.19255v1.pdf"
    },
    {
        "title": "gTBLS: Generating Tables from Text by Conditional Question Answering",
        "authors": [
            "Anirudh Sundar",
            "Christopher Richardson",
            "Larry Heck"
        ],
        "published": "2024-03-21T15:04:32Z",
        "summary": "Distilling large, unstructured text into a structured, condensed form such as\ntables is an open research problem. One of the primary challenges in\nautomatically generating tables is ensuring their syntactic validity. Prior\napproaches address this challenge by including additional parameters in the\nTransformer's attention mechanism to attend to specific rows and column\nheaders. In contrast to this single-stage method, this paper presents a\ntwo-stage approach called Generative Tables (gTBLS). The first stage infers\ntable structure (row and column headers) from the text. The second stage\nformulates questions using these headers and fine-tunes a causal language model\nto answer them. Furthermore, the gTBLS approach is amenable to the utilization\nof pre-trained Large Language Models in a zero-shot configuration, presenting a\nsolution for table generation in situations where fine-tuning is not feasible.\ngTBLS improves prior approaches by up to 10% in BERTScore on the table\nconstruction task and up to 20% on the table content generation task of the\nE2E, WikiTableText, WikiBio, and RotoWire datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.14457v1.pdf"
    },
    {
        "title": "Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models",
        "authors": [
            "Spyridon Mouselinos",
            "Henryk Michalewski",
            "Mateusz Malinowski"
        ],
        "published": "2024-02-06T10:37:21Z",
        "summary": "Large Language Models (LLMs) demonstrate ever-increasing abilities in\nmathematical and algorithmic tasks, yet their geometric reasoning skills are\nunderexplored. We investigate LLMs' abilities in constructive geometric\nproblem-solving one of the most fundamental steps in the development of human\nmathematical reasoning. Our work reveals notable challenges that the\nstate-of-the-art LLMs face in this domain despite many successes in similar\nareas. LLMs exhibit biases in target variable selection and struggle with 2D\nspatial relationships, often misrepresenting and hallucinating objects and\ntheir placements. To this end, we introduce a framework that formulates an\nLLMs-based multi-agents system that enhances their existing reasoning potential\nby conducting an internal dialogue. This work underscores LLMs' current\nlimitations in geometric reasoning and improves geometric reasoning\ncapabilities through self-correction, collaboration, and diverse role\nspecializations.",
        "pdf_link": "https://arxiv.org/pdf/2402.03877v2.pdf"
    },
    {
        "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
        "authors": [
            "Kuang-Huei Lee",
            "Xinyun Chen",
            "Hiroki Furuta",
            "John Canny",
            "Ian Fischer"
        ],
        "published": "2024-02-15T05:40:21Z",
        "summary": "Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3-20x.",
        "pdf_link": "https://arxiv.org/pdf/2402.09727v2.pdf"
    },
    {
        "title": "Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection",
        "authors": [
            "Valeria Pastorino",
            "Jasivan A. Sivakumar",
            "Nafise Sadat Moosavi"
        ],
        "published": "2024-02-18T15:27:48Z",
        "summary": "This work contributes to the expanding research on the applicability of LLMs\nin social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and\nFlan-T5 models in detecting framing bias in news headlines through zero-shot,\nfew-shot, and explainable prompting methods. A key insight from our evaluation\nis the notable efficacy of explainable prompting in enhancing the reliability\nof these models, highlighting the importance of explainable settings for social\nscience research on framing bias. GPT-4, in particular, demonstrated enhanced\nperformance in few-shot scenarios when presented with a range of relevant,\nin-domain examples. FLAN-T5's poor performance indicates that smaller models\nmay require additional task-specific fine-tuning for identifying framing bias\ndetection. Our study also found that models, particularly GPT-4, often\nmisinterpret emotional language as an indicator of framing bias, underscoring\nthe challenge of distinguishing between reporting genuine emotional expression\nand intentionally use framing bias in news headlines. We further evaluated the\nmodels on two subsets of headlines where the presence or absence of framing\nbias was either clear-cut or more contested, with the results suggesting that\nthese models' can be useful in flagging potential annotation inaccuracies\nwithin existing or new datasets. Finally, the study evaluates the models in\nreal-world conditions (\"in the wild\"), moving beyond the initial dataset\nfocused on U.S. Gun Violence, assessing the models' performance on framed\nheadlines covering a broad range of topics.",
        "pdf_link": "https://arxiv.org/pdf/2402.11621v2.pdf"
    },
    {
        "title": "Rationality Report Cards: Assessing the Economic Rationality of Large Language Models",
        "authors": [
            "Narun Raman",
            "Taylor Lundy",
            "Samuel Amouyal",
            "Yoav Levine",
            "Kevin Leyton-Brown",
            "Moshe Tennenholtz"
        ],
        "published": "2024-02-14T20:05:26Z",
        "summary": "There is increasing interest in using LLMs as decision-making \"agents.\" Doing\nso includes many degrees of freedom: which model should be used; how should it\nbe prompted; should it be asked to introspect, conduct chain-of-thought\nreasoning, etc? Settling these questions -- and more broadly, determining\nwhether an LLM agent is reliable enough to be trusted -- requires a methodology\nfor assessing such an agent's economic rationality. In this paper, we provide\none. We begin by surveying the economic literature on rational decision making,\ntaxonomizing a large set of fine-grained \"elements\" that an agent should\nexhibit, along with dependencies between them. We then propose a benchmark\ndistribution that quantitatively scores an LLMs performance on these elements\nand, combined with a user-provided rubric, produces a \"rationality report\ncard.\" Finally, we describe the results of a large-scale empirical experiment\nwith 14 different LLMs, characterizing the both current state of the art and\nthe impact of different model sizes on models' ability to exhibit rational\nbehavior.",
        "pdf_link": "https://arxiv.org/pdf/2402.09552v1.pdf"
    },
    {
        "title": "Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM",
        "authors": [
            "Xiaoding Lu",
            "Zongyi Liu",
            "Adian Liusie",
            "Vyas Raina",
            "Vineet Mudupalli",
            "Yuwen Zhang",
            "William Beauchamp"
        ],
        "published": "2024-01-04T07:45:49Z",
        "summary": "In conversational AI research, there's a noticeable trend towards developing\nmodels with a larger number of parameters, exemplified by models like ChatGPT.\nWhile these expansive models tend to generate increasingly better chat\nresponses, they demand significant computational resources and memory. This\nstudy explores a pertinent question: Can a combination of smaller models\ncollaboratively achieve comparable or enhanced performance relative to a\nsingular large model? We introduce an approach termed \"blending\", a\nstraightforward yet effective method of integrating multiple chat AIs. Our\nempirical evidence suggests that when specific smaller models are\nsynergistically blended, they can potentially outperform or match the\ncapabilities of much larger counterparts. For instance, integrating just three\nmodels of moderate size (6B/13B paramaeters) can rival or even surpass the\nperformance metrics of a substantially larger model like ChatGPT (175B+\nparamaters). This hypothesis is rigorously tested using A/B testing\nmethodologies with a large user base on the Chai research platform over a span\nof thirty days. The findings underscore the potential of the \"blending\"\nstrategy as a viable approach for enhancing chat AI efficacy without a\ncorresponding surge in computational demands.",
        "pdf_link": "https://arxiv.org/pdf/2401.02994v3.pdf"
    },
    {
        "title": "A match made in consistency heaven: when large language models meet evolutionary algorithms",
        "authors": [
            "Wang Chao",
            "Jiaxuan Zhao",
            "Licheng Jiao",
            "Lingling Li",
            "Fang Liu",
            "Shuyuan Yang"
        ],
        "published": "2024-01-19T05:58:30Z",
        "summary": "Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text sequence generation and evolution, this\npaper illustrates the strong consistency of LLMs and EAs, which includes\nmultiple one-to-one key characteristics: token embedding and genotype-phenotype\nmapping, position encoding and fitness shaping, position embedding and\nselection, attention and crossover, feed-forward neural network and mutation,\nmodel training and parameter update, and multi-task learning and\nmulti-objective optimization. Based on this consistency perspective, existing\ncoupling studies are analyzed, including evolutionary fine-tuning and\nLLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap\nfor future research in coupling LLMs and EAs, while highlighting key challenges\nalong the way. The consistency not only reveals the evolution mechanism behind\nLLMs but also facilitates the development of evolved artificial agents that\napproach or surpass biological organisms.",
        "pdf_link": "https://arxiv.org/pdf/2401.10510v1.pdf"
    },
    {
        "title": "Language models scale reliably with over-training and on downstream tasks",
        "authors": [
            "Samir Yitzhak Gadre",
            "Georgios Smyrnis",
            "Vaishaal Shankar",
            "Suchin Gururangan",
            "Mitchell Wortsman",
            "Rulin Shao",
            "Jean Mercat",
            "Alex Fang",
            "Jeffrey Li",
            "Sedrick Keh",
            "Rui Xin",
            "Marianna Nezhurina",
            "Igor Vasiljevic",
            "Jenia Jitsev",
            "Alexandros G. Dimakis",
            "Gabriel Ilharco",
            "Shuran Song",
            "Thomas Kollar",
            "Yair Carmon",
            "Achal Dave",
            "Reinhard Heckel",
            "Niklas Muennighoff",
            "Ludwig Schmidt"
        ],
        "published": "2024-03-13T13:54:00Z",
        "summary": "Scaling laws are useful guides for developing language models, but there are\nstill gaps between current scaling studies and how language models are\nultimately trained and evaluated. For instance, scaling is usually studied in\nthe compute-optimal training regime (i.e., \"Chinchilla optimal\" regime);\nhowever, in practice, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but\nultimately models are compared based on downstream task performance. In this\npaper, we address both shortcomings. To do so, we create a testbed of 104\nmodels with 0.011B to 6.9B parameters trained with various numbers of tokens on\nthree data distributions. First, we investigate scaling in the over-trained\nregime. We fit scaling laws that extrapolate in both the number of model\nparameters and the ratio of training tokens to parameters. This enables us to\npredict the validation loss of a 1.4B parameter, 900B token run (i.e.,\n32$\\times$ over-trained) and a 6.9B parameter, 138B token\nrun$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.\nSecond, we relate the perplexity of a language model to its downstream task\nperformance via a power law. We use this law to predict top-1 error averaged\nover downstream tasks for the two aforementioned models using experiments that\ntake 20$\\times$ less compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.",
        "pdf_link": "https://arxiv.org/pdf/2403.08540v1.pdf"
    },
    {
        "title": "Towards Goal-oriented Large Language Model Prompting: A Survey",
        "authors": [
            "Haochen Li",
            "Jonathan Leung",
            "Zhiqi Shen"
        ],
        "published": "2024-01-25T09:47:55Z",
        "summary": "Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks in which prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not as an overview of current prompt engineering\nmethods, aims to highlight the limitation of designing prompts while holding an\nanthropomorphic assumption that expects LLMs to think like humans. From our\nreview of 35 representative studies, we demonstrate that a goal-oriented prompt\nformulation, which guides LLMs to follow established human logical thinking,\nsignificantly improves the performance of LLMs. Furthermore, We introduce a\nnovel taxonomy that categorizes goal-oriented prompting methods into five\ninterconnected stages and we demonstrate the broad applicability of our\nframework by summarizing ten applicable tasks. With four future directions\nproposed, we hope to further emphasize and promote goal-oriented prompt\nengineering.",
        "pdf_link": "https://arxiv.org/pdf/2401.14043v1.pdf"
    },
    {
        "title": "Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems",
        "authors": [
            "Lingjiao Chen",
            "Jared Quincy Davis",
            "Boris Hanin",
            "Peter Bailis",
            "Ion Stoica",
            "Matei Zaharia",
            "James Zou"
        ],
        "published": "2024-03-04T19:12:48Z",
        "summary": "Many recent state-of-the-art results in language tasks were achieved using\ncompound systems that perform multiple Large Language Model (LLM) calls and\naggregate their responses. However, there is little understanding of how the\nnumber of LLM calls -- e.g., when asking the LLM to answer each question\nmultiple times and taking a consensus -- affects such a compound system's\nperformance. In this paper, we initiate the study of scaling laws of compound\ninference systems. We analyze, theoretically and empirically, how the number of\nLLM calls affects the performance of one-layer Voting Inference Systems -- one\nof the simplest compound systems, which aggregates LLM responses via majority\nvoting. We find empirically that across multiple language tasks, surprisingly,\nVoting Inference Systems' performance first increases but then decreases as a\nfunction of the number of LLM calls. Our theoretical results suggest that this\nnon-monotonicity is due to the diversity of query difficulties within a task:\nmore LLM calls lead to higher performance on \"easy\" queries, but lower\nperformance on \"hard\" queries, and non-monotone behavior emerges when a task\ncontains both types of queries. This insight then allows us to compute, from a\nsmall number of samples, the number of LLM calls that maximizes system\nperformance, and define a scaling law of Voting Inference Systems. Experiments\nshow that our scaling law can predict the performance of Voting Inference\nSystems and find the optimal number of LLM calls to make.",
        "pdf_link": "https://arxiv.org/pdf/2403.02419v1.pdf"
    },
    {
        "title": "Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study",
        "authors": [
            "Shangding Gu"
        ],
        "published": "2024-01-12T14:35:57Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities for\nreinforcement learning (RL) models, such as planning and reasoning\ncapabilities. However, the problems of LLMs and RL model collaboration still\nneed to be solved. In this study, we employ a teacher-student learning\nframework to tackle these problems, specifically by offering feedback for LLMs\nusing RL models and providing high-level information for RL models with LLMs in\na cooperative multi-agent setting. Within this framework, the LLM acts as a\nteacher, while the RL model acts as a student. The two agents cooperatively\nassist each other through a process of recursive help, such as \"I help you help\nI help.\" The LLM agent supplies abstract information to the RL agent, enabling\nefficient exploration and policy improvement. In turn, the RL agent offers\nfeedback to the LLM agent, providing valuable, real-time information that helps\ngenerate more useful tokens. This bi-directional feedback loop promotes\noptimization, exploration, and mutual improvement for both agents, enabling\nthem to accomplish increasingly challenging tasks. Remarkably, we propose a\npractical algorithm to address the problem and conduct empirical experiments to\nevaluate the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2401.06603v1.pdf"
    },
    {
        "title": "Integration of Large Language Models in Control of EHD Pumps for Precise Color Synthesis",
        "authors": [
            "Yanhong Peng",
            "Ceng Zhang",
            "Chenlong Hu",
            "Zebing Mao"
        ],
        "published": "2024-01-21T14:10:27Z",
        "summary": "This paper presents an innovative approach to integrating Large Language\nModels (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for\nprecise color synthesis in automation systems. We propose a novel framework\nthat employs fine-tuned LLMs to interpret natural language commands and convert\nthem into specific operational instructions for EHD pump control. This approach\naims to enhance user interaction with complex hardware systems, making it more\nintuitive and efficient. The methodology involves four key steps: fine-tuning\nthe language model with a dataset of color specifications and corresponding\nArduino code, developing a natural language processing interface, translating\nuser inputs into executable Arduino code, and controlling EHD pumps for\naccurate color mixing. Conceptual experiment results, based on theoretical\nassumptions, indicate a high potential for accurate color synthesis, efficient\nlanguage model interpretation, and reliable EHD pump operation. This research\nextends the application of LLMs beyond text-based tasks, demonstrating their\npotential in industrial automation and control systems. While highlighting the\nlimitations and the need for real-world testing, this study opens new avenues\nfor AI applications in physical system control and sets a foundation for future\nadvancements in AI-driven automation technologies.",
        "pdf_link": "https://arxiv.org/pdf/2401.11500v1.pdf"
    },
    {
        "title": "AntEval: Evaluation of Social Interaction Competencies in LLM-Driven Agents",
        "authors": [
            "Yuanzhi Liang",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "published": "2024-01-12T11:18:00Z",
        "summary": "Large Language Models (LLMs) have demonstrated their ability to replicate\nhuman behaviors across a wide range of scenarios. However, their capability in\nhandling complex, multi-character social interactions has yet to be fully\nexplored, primarily due to the absence of robust, quantitative evaluation\nmethods. This gap has slowed the development of agents proficient in more\nnuanced interactions beyond simple exchanges, for example, small talk. To\naddress this challenge, we introduce the Multi-Agent Interaction Evaluation\nFramework (AntEval), encompassing a novel interaction framework and evaluation\nmethods. The interaction framework aims to foster an complex interaction\nenvironment that bolsters information exchange and intention expression within\nsocial interactions. Furthermore, we introduce evaluation methods, including\ntwo metrics: Information Exchanging Precision (IEP) and Interaction\nExpressiveness Gap (IEG), designed for the quantitative and objective\nassessment of agents' interaction competencies. Our findings highlight the\nutility of these evaluative methods and show significant potential for\nimproving LLMs' ability to construct agents that interact in a more natural\nmanner with human-like intricacy.",
        "pdf_link": "https://arxiv.org/pdf/2401.06509v3.pdf"
    },
    {
        "title": "Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems",
        "authors": [
            "Liang Zhang",
            "Zhelun Chen"
        ],
        "published": "2024-02-14T21:19:33Z",
        "summary": "The potential of Machine Learning Control (MLC) in HVAC systems is hindered\nby its opaque nature and inference mechanisms, which is challenging for users\nand modelers to fully comprehend, ultimately leading to a lack of trust in\nMLC-based decision-making. To address this challenge, this paper investigates\nand explores Interpretable Machine Learning (IML), a branch of Machine Learning\n(ML) that enhances transparency and understanding of models and their\ninferences, to improve the credibility of MLC and its industrial application in\nHVAC systems. Specifically, we developed an innovative framework that combines\nthe principles of Shapley values and the in-context learning feature of Large\nLanguage Models (LLMs). While the Shapley values are instrumental in dissecting\nthe contributions of various features in ML models, LLM provides an in-depth\nunderstanding of rule-based parts in MLC; combining them, LLM further packages\nthese insights into a coherent, human-understandable narrative. The paper\npresents a case study to demonstrate the feasibility of the developed IML\nframework for model predictive control-based precooling under demand response\nevents in a virtual testbed. The results indicate that the developed framework\ngenerates and explains the control signals in accordance with the rule-based\nrationale.",
        "pdf_link": "https://arxiv.org/pdf/2402.09584v1.pdf"
    },
    {
        "title": "Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations",
        "authors": [
            "Yafei Xiang",
            "Hanyi Yu",
            "Yulu Gong",
            "Shuning Huo",
            "Mengran Zhu"
        ],
        "published": "2024-02-25T09:19:11Z",
        "summary": "With the rapid development of artificial intelligence technology, Transformer\nstructural pre-training model has become an important tool for large language\nmodel (LLM) tasks. In the field of e-commerce, these models are especially\nwidely used, from text understanding to generating recommendation systems,\nwhich provide powerful technical support for improving user experience and\noptimizing service processes. This paper reviews the core application scenarios\nof Transformer pre-training model in e-commerce text understanding and\nrecommendation generation, including but not limited to automatic generation of\nproduct descriptions, sentiment analysis of user comments, construction of\npersonalized recommendation system and automated processing of customer service\nconversations. Through a detailed analysis of the model's working principle,\nimplementation process, and application effects in specific cases, this paper\nemphasizes the unique advantages of pre-trained models in understanding complex\nuser intentions and improving the quality of recommendations. In addition, the\nchallenges and improvement directions for the future are also discussed, such\nas how to further improve the generalization ability of the model, the ability\nto handle large-scale data sets, and technical strategies to protect user\nprivacy. Ultimately, the paper points out that the application of Transformer\nstructural pre-training models in e-commerce has not only driven technological\ninnovation, but also brought substantial benefits to merchants and consumers,\nand looking forward, these models will continue to play a key role in\ne-commerce and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2402.16035v1.pdf"
    },
    {
        "title": "Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging",
        "authors": [
            "Wei Zhang",
            "Hongcheng Guo",
            "Anjie Le",
            "Jian Yang",
            "Jiaheng Liu",
            "Zhoujun Li",
            "Tieqiao Zheng",
            "Shi Xu",
            "Runqiang Zang",
            "Liangfan Zheng",
            "Bo Zhang"
        ],
        "published": "2024-02-28T09:51:55Z",
        "summary": "Logs produced by extensive software systems are integral to monitoring system\nbehaviors. Advanced log analysis facilitates the detection, alerting, and\ndiagnosis of system faults. Log parsing, which entails transforming raw log\nmessages into structured templates, constitutes a critical phase in the\nautomation of log analytics. Existing log parsers fail to identify the correct\ntemplates due to reliance on human-made rules. Besides, These methods focus on\nstatistical features while ignoring semantic information in log messages. To\naddress these challenges, we introduce a cutting-edge \\textbf{L}og parsing\nframework with \\textbf{E}ntropy sampling and Chain-of-Thought \\textbf{M}erging\n(Lemur). Specifically, to discard the tedious manual rules. We propose a novel\nsampling method inspired by information entropy, which efficiently clusters\ntypical logs. Furthermore, to enhance the merging of log templates, we design a\nchain-of-thought method for large language models (LLMs). LLMs exhibit\nexceptional semantic comprehension, deftly distinguishing between parameters\nand invariant tokens. We have conducted experiments on large-scale public\ndatasets. Extensive evaluation demonstrates that Lemur achieves the\nstate-of-the-art performance and impressive efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.18205v2.pdf"
    },
    {
        "title": "Attacking LLM Watermarks by Exploiting Their Strengths",
        "authors": [
            "Qi Pang",
            "Shengyuan Hu",
            "Wenting Zheng",
            "Virginia Smith"
        ],
        "published": "2024-02-25T20:24:07Z",
        "summary": "Advances in generative models have made it possible for AI-generated text,\ncode, and images to mirror human-generated content in many applications.\nWatermarking, a technique that aims to embed information in the output of a\nmodel to verify its source, is useful for mitigating misuse of such\nAI-generated content. However, existing watermarking schemes remain\nsurprisingly susceptible to attack. In particular, we show that desirable\nproperties shared by existing LLM watermarking systems such as quality\npreservation, robustness, and public detection APIs can in turn make these\nsystems vulnerable to various attacks. We rigorously study potential attacks in\nterms of common watermark design choices, and propose best practices and\ndefenses for mitigation -- establishing a set of practical guidelines for\nembedding and detection of LLM watermarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.16187v1.pdf"
    },
    {
        "title": "Juru: Legal Brazilian Large Language Model from Reputable Sources",
        "authors": [
            "Roseval Malaquias Junior",
            "Ramon Pires",
            "Roseli Romero",
            "Rodrigo Nogueira"
        ],
        "published": "2024-03-26T22:54:12Z",
        "summary": "The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.",
        "pdf_link": "https://arxiv.org/pdf/2403.18140v1.pdf"
    },
    {
        "title": "Adaptive Text Watermark for Large Language Models",
        "authors": [
            "Yepeng Liu",
            "Yuheng Bu"
        ],
        "published": "2024-01-25T03:57:12Z",
        "summary": "The advancement of Large Language Models (LLMs) has led to increasing\nconcerns about the misuse of AI-generated text, and watermarking for\nLLM-generated text has emerged as a potential solution. However, it is\nchallenging to generate high-quality watermarked text while maintaining strong\nsecurity, robustness, and the ability to detect watermarks without prior\nknowledge of the prompt or model. This paper proposes an adaptive watermarking\nstrategy to address this problem. To improve the text quality and maintain\nrobustness, we adaptively add watermarking to token distributions with high\nentropy measured using an auxiliary model and keep the low entropy token\ndistributions untouched. For the sake of security and to further minimize the\nwatermark's impact on text quality, instead of using a fixed green/red list\ngenerated from a random secret key, which can be vulnerable to decryption and\nforgery, we adaptively scale up the output logits in proportion based on the\nsemantic embedding of previously generated text using a well designed semantic\nmapping model. Our experiments involving various LLMs demonstrate that our\napproach achieves comparable robustness performance to existing watermark\nmethods. Additionally, the text generated by our method has perplexity\ncomparable to that of \\emph{un-watermarked} LLMs while maintaining security\neven under various attacks.",
        "pdf_link": "https://arxiv.org/pdf/2401.13927v1.pdf"
    },
    {
        "title": "EyeGPT: Ophthalmic Assistant with Large Language Models",
        "authors": [
            "Xiaolan Chen",
            "Ziwei Zhao",
            "Weiyi Zhang",
            "Pusheng Xu",
            "Le Gao",
            "Mingpu Xu",
            "Yue Wu",
            "Yinwen Li",
            "Danli Shi",
            "Mingguang He"
        ],
        "published": "2024-02-29T09:35:41Z",
        "summary": "Artificial intelligence (AI) has gained significant attention in healthcare\nconsultation due to its potential to improve clinical workflow and enhance\nmedical communication. However, owing to the complex nature of medical\ninformation, large language models (LLM) trained with general world knowledge\nmight not possess the capability to tackle medical-related tasks at an expert\nlevel. Here, we introduce EyeGPT, a specialized LLM designed specifically for\nophthalmology, using three optimization strategies including role-playing,\nfinetuning, and retrieval-augmented generation. In particular, we proposed a\ncomprehensive evaluation framework that encompasses a diverse dataset, covering\nvarious subspecialties of ophthalmology, different users, and diverse inquiry\nintents. Moreover, we considered multiple evaluation metrics, including\naccuracy, understandability, trustworthiness, empathy, and the proportion of\nhallucinations. By assessing the performance of different EyeGPT variants, we\nidentify the most effective one, which exhibits comparable levels of\nunderstandability, trustworthiness, and empathy to human ophthalmologists (all\nPs>0.05). Overall, ur study provides valuable insights for future research,\nfacilitating comprehensive comparisons and evaluations of different strategies\nfor developing specialized LLMs in ophthalmology. The potential benefits\ninclude enhancing the patient experience in eye care and optimizing\nophthalmologists' services.",
        "pdf_link": "https://arxiv.org/pdf/2403.00840v1.pdf"
    },
    {
        "title": "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving",
        "authors": [
            "Jiaxin Zhang",
            "Zhongzhi Li",
            "Mingliang Zhang",
            "Fei Yin",
            "Chenglin Liu",
            "Yashar Moshfeghi"
        ],
        "published": "2024-02-15T16:59:41Z",
        "summary": "Recent advancements in Large Language Models (LLMs) and Multi-Modal Models\n(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,\ntheir proficiency in tackling geometry math problems, which necessitates an\nintegrated understanding of both textual and visual information, has not been\nthoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,\na comprehensive collection that includes a main subset of 2000 problems, a 750\nproblem subset focusing on backward reasoning, an augmented subset of 2000\nproblems, and a hard subset of 300 problems. This benchmark facilitates a\ndeeper investigation into the performance of LLMs and MMs on solving geometry\nmath problems. Our evaluation of ten LLMs and MMs across these varied subsets\nreveals that the WizardMath model excels, achieving a 55.67\\% accuracy rate on\nthe main subset but only a 6.00\\% accuracy on the challenging subset. This\nhighlights the critical need for testing models against datasets on which they\nhave not been pre-trained. Additionally, our findings indicate that GPT-series\nmodels perform more effectively on problems they have rephrased, suggesting a\npromising method for enhancing model capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.10104v1.pdf"
    },
    {
        "title": "Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?",
        "authors": [
            "Marco Gaido",
            "Sara Papi",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "published": "2024-02-19T10:34:13Z",
        "summary": "The field of natural language processing (NLP) has recently witnessed a\ntransformative shift with the emergence of foundation models, particularly\nLarge Language Models (LLMs) that have revolutionized text-based NLP. This\nparadigm has extended to other modalities, including speech, where researchers\nare actively exploring the combination of Speech Foundation Models (SFMs) and\nLLMs into single, unified models capable of addressing multimodal tasks. Among\nsuch tasks, this paper focuses on speech-to-text translation (ST). By examining\nthe published papers on the topic, we propose a unified view of the\narchitectural solutions and training strategies presented so far, highlighting\nsimilarities and differences among them. Based on this examination, we not only\norganize the lessons learned but also show how diverse settings and evaluation\napproaches hinder the identification of the best-performing solution for each\narchitectural building block and training choice. Lastly, we outline\nrecommendations for future works on the topic aimed at better understanding the\nstrengths and weaknesses of the SFM+LLM solutions for ST.",
        "pdf_link": "https://arxiv.org/pdf/2402.12025v1.pdf"
    },
    {
        "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
        "authors": [
            "Yuhui Li",
            "Fangyun Wei",
            "Chao Zhang",
            "Hongyang Zhang"
        ],
        "published": "2024-01-26T18:59:01Z",
        "summary": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.",
        "pdf_link": "https://arxiv.org/pdf/2401.15077v2.pdf"
    },
    {
        "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
            "Kedi Chen",
            "Qin Chen",
            "Jie Zhou",
            "Yishen He",
            "Liang He"
        ],
        "published": "2024-03-01T15:38:55Z",
        "summary": "Since large language models (LLMs) achieve significant success in recent\nyears, the hallucination issue remains a challenge, numerous benchmarks are\nproposed to detect the hallucination. Nevertheless, some of these benchmarks\nare not naturally generated by LLMs but are intentionally induced. Also, many\nmerely focus on the factuality hallucination while ignoring the faithfulness\nhallucination. Additionally, although dialogue pattern is more widely utilized\nin the era of LLMs, current benchmarks only concentrate on sentence-level and\npassage-level hallucination. In this study, we propose DiaHalu, the first\ndialogue-level hallucination evaluation benchmark to our knowledge. Initially,\nwe integrate the collected topics into system prompts and facilitate a dialogue\nbetween two ChatGPT3.5. Subsequently, we manually modify the contents that do\nnot adhere to human language conventions and then have LLMs re-generate,\nsimulating authentic human-machine interaction scenarios. Finally, professional\nscholars annotate all the samples in the dataset. DiaHalu covers four common\nmulti-turn dialogue domains and five hallucination subtypes, extended from\nfactuality and faithfulness hallucination. Experiments through some well-known\nLLMs and detection methods on the dataset show that DiaHalu is a challenging\nbenchmark, holding significant value for further research.",
        "pdf_link": "https://arxiv.org/pdf/2403.00896v1.pdf"
    },
    {
        "title": "Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages",
        "authors": [
            "Yuanchi Zhang",
            "Yile Wang",
            "Zijun Liu",
            "Shuo Wang",
            "Xiaolong Wang",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-19T15:07:32Z",
        "summary": "While large language models (LLMs) have been pre-trained on multilingual\ncorpora, their performance still lags behind in most languages compared to a\nfew resource-rich languages. One common approach to mitigate this issue is to\ntranslate training data from resource-rich languages into other languages and\nthen continue training. However, using the data obtained solely relying on\ntranslation while ignoring the original capabilities of LLMs across languages\nis not always effective, which we show will limit the performance of\ncross-lingual knowledge transfer. In this work, we propose SDRRL, a method\nbased on Self-Distillation from Resource-Rich Languages that effectively\nimprove multilingual performance by leveraging the internal capabilities of\nLLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and\nSeaLLM) and source languages across various comprehension and generation tasks,\nexperimental results demonstrate that SDRRL can significantly enhance\nmultilingual capabilities while minimizing the impact on original performance\nin resource-rich languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.12204v1.pdf"
    },
    {
        "title": "K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation Learning in Multivariate Time-Series Data",
        "authors": [
            "Yucheng Wang",
            "Ruibing Jin",
            "Min Wu",
            "Xiaoli Li",
            "Lihua Xie",
            "Zhenghua Chen"
        ],
        "published": "2024-03-06T12:08:14Z",
        "summary": "Sourced from various sensors and organized chronologically, Multivariate\nTime-Series (MTS) data involves crucial spatial-temporal dependencies, e.g.,\ncorrelations among sensors. To capture these dependencies, Graph Neural\nNetworks (GNNs) have emerged as powerful tools, yet their effectiveness is\nrestricted by the quality of graph construction from MTS data. Typically,\nexisting approaches construct graphs solely from MTS signals, which may\nintroduce bias due to a small training dataset and may not accurately represent\nunderlying dependencies. To address this challenge, we propose a novel\nframework named K-Link, leveraging Large Language Models (LLMs) to encode\nextensive general knowledge and thereby providing effective solutions to reduce\nthe bias. Leveraging the knowledge embedded in LLMs, such as physical\nprinciples, we extract a \\textit{Knowledge-Link graph}, capturing vast semantic\nknowledge of sensors and the linkage of the sensor-level knowledge. To harness\nthe potential of the knowledge-link graph in enhancing the graph derived from\nMTS data, we propose a graph alignment module, facilitating the transfer of\nsemantic knowledge within the knowledge-link graph into the MTS-derived graph.\nBy doing so, we can improve the graph quality, ensuring effective\nrepresentation learning with GNNs for MTS data. Extensive experiments\ndemonstrate the efficacy of our approach for superior performance across\nvarious MTS-related downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.03645v1.pdf"
    },
    {
        "title": "SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency",
        "authors": [
            "Akila Wickramasekara",
            "Frank Breitinger",
            "Mark Scanlon"
        ],
        "published": "2024-02-29T17:13:44Z",
        "summary": "The growing number of cases requiring digital forensic analysis raises\nconcerns about law enforcement's ability to conduct investigations promptly.\nConsequently, this systemisation of knowledge paper delves into the potential\nand effectiveness of integrating Large Language Models (LLMs) into digital\nforensic investigation to address these challenges. A thorough literature\nreview is undertaken, encompassing existing digital forensic models, tools,\nLLMs, deep learning techniques, and the utilisation of LLMs in investigations.\nThe review identifies current challenges within existing digital forensic\nprocesses and explores both the obstacles and possibilities of incorporating\nLLMs. In conclusion, the study asserts that the adoption of LLMs in digital\nforensics, with appropriate constraints, holds the potential to enhance\ninvestigation efficiency, improve traceability, and alleviate technical and\njudicial barriers faced by law enforcement entities.",
        "pdf_link": "https://arxiv.org/pdf/2402.19366v1.pdf"
    },
    {
        "title": "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models",
        "authors": [
            "Yifan Yang",
            "Jiajun Zhou",
            "Ngai Wong",
            "Zheng Zhang"
        ],
        "published": "2024-02-18T01:20:00Z",
        "summary": "Various parameter-efficient fine-tuning (PEFT) techniques have been proposed\nto enable computationally efficient fine-tuning while maintaining model\nperformance. However, existing PEFT methods are still limited by the growing\nnumber of trainable parameters with the rapid deployment of Large Language\nModels (LLMs). To address this challenge, we present LoRETTA, an\nultra-parameter-efficient framework that significantly reduces trainable\nparameters through tensor-train decomposition. Specifically, we propose two\nmethods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs\ntensorized adapters, offering a high-performance yet lightweight approach for\nthe fine-tuning of LLMs. The latter emphasizes fine-tuning via weight\nparameterization with a set of small tensor factors. LoRETTA achieves\ncomparable or better performance than most widely used PEFT methods with up to\n$100\\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical\nresults demonstrate that the proposed method effectively improves training\nefficiency, enjoys better multi-task learning performance, and enhances the\nanti-overfitting capability. Plug-and-play codes built upon the Huggingface\nframework and PEFT library will be released.",
        "pdf_link": "https://arxiv.org/pdf/2402.11417v1.pdf"
    },
    {
        "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models",
        "authors": [
            "Moschoula Pternea",
            "Prerna Singh",
            "Abir Chakraborty",
            "Yagna Oruganti",
            "Mirco Milletari",
            "Sayli Bapat",
            "Kebei Jiang"
        ],
        "published": "2024-02-02T20:01:15Z",
        "summary": "In this work, we review research studies that combine Reinforcement Learning\n(RL) and Large Language Models (LLMs), two areas that owe their momentum to the\ndevelopment of deep neural networks. We propose a novel taxonomy of three main\nclasses based on the way that the two model types interact with each other. The\nfirst class, RL4LLM, includes studies where RL is leveraged to improve the\nperformance of LLMs on tasks related to Natural Language Processing. L4LLM is\ndivided into two sub-categories depending on whether RL is used to directly\nfine-tune an existing LLM or to improve the prompt of the LLM. In the second\nclass, LLM4RL, an LLM assists the training of an RL model that performs a task\nthat is not inherently related to natural language. We further break down\nLLM4RL based on the component of the RL training framework that the LLM assists\nor replaces, namely reward shaping, goal generation, and policy function.\nFinally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a\ncommon planning framework without either of them contributing to training or\nfine-tuning of the other. We further branch this class to distinguish between\nstudies with and without natural language feedback. We use this taxonomy to\nexplore the motivations behind the synergy of LLMs and RL and explain the\nreasons for its success, while pinpointing potential shortcomings and areas\nwhere further research is needed, as well as alternative methodologies that\nserve the same goal.",
        "pdf_link": "https://arxiv.org/pdf/2402.01874v1.pdf"
    },
    {
        "title": "A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection",
        "authors": [
            "Benjamin Steenhoek",
            "Md Mahbubur Rahman",
            "Monoshi Kumar Roy",
            "Mirza Sanjida Alam",
            "Earl T. Barr",
            "Wei Le"
        ],
        "published": "2024-03-25T21:47:36Z",
        "summary": "Large Language Models (LLMs) have demonstrated great potential for code\ngeneration and other software engineering tasks. Vulnerability detection is of\ncrucial importance to maintaining the security, integrity, and trustworthiness\nof software systems. Precise vulnerability detection requires reasoning about\nthe code, making it a good case study for exploring the limits of LLMs'\nreasoning capabilities. Although recent work has applied LLMs to vulnerability\ndetection using generic prompting techniques, their full capabilities for this\ntask and the types of errors they make when explaining identified\nvulnerabilities remain unclear.\n  In this paper, we surveyed eleven LLMs that are state-of-the-art in code\ngeneration and commonly used as coding assistants, and evaluated their\ncapabilities for vulnerability detection. We systematically searched for the\nbest-performing prompts, incorporating techniques such as in-context learning\nand chain-of-thought, and proposed three of our own prompting methods. Our\nresults show that while our prompting methods improved the models' performance,\nLLMs generally struggled with vulnerability detection. They reported 0.5-0.63\nBalanced Accuracy and failed to distinguish between buggy and fixed versions of\nprograms in 76% of cases on average. By comprehensively analyzing and\ncategorizing 287 instances of model reasoning, we found that 57% of LLM\nresponses contained errors, and the models frequently predicted incorrect\nlocations of buggy code and misidentified bug types. LLMs only correctly\nlocalized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted\ncorrectly by 70-100% of human participants. These findings suggest that despite\ntheir potential for other tasks, LLMs may fail to properly comprehend critical\ncode structures and security-related concepts. Our data and code are available\nat https://figshare.com/s/78fe02e56e09ec49300b.",
        "pdf_link": "https://arxiv.org/pdf/2403.17218v1.pdf"
    },
    {
        "title": "Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection",
        "authors": [
            "Kyungjae Lee",
            "Dasol Hwang",
            "Sunghyun Park",
            "Youngsoo Jang",
            "Moontae Lee"
        ],
        "published": "2024-03-21T08:57:27Z",
        "summary": "Despite the promise of RLHF in aligning LLMs with human preferences, it often\nleads to superficial alignment, prioritizing stylistic changes over improving\ndownstream performance of LLMs. Underspecified preferences could obscure\ndirections to align the models. Lacking exploration restricts identification of\ndesirable outputs to improve the models. To overcome these challenges, we\npropose a novel framework: Reinforcement Learning from Reflective Feedback\n(RLRF), which leverages fine-grained feedback based on detailed criteria to\nimprove the core capabilities of LLMs. RLRF employs a self-reflection mechanism\nto systematically explore and refine LLM responses, then fine-tuning the models\nvia a RL algorithm along with promising responses. Our experiments across\nJust-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and\ntransformative potential of RLRF beyond superficial surface-level adjustment.",
        "pdf_link": "https://arxiv.org/pdf/2403.14238v1.pdf"
    },
    {
        "title": "DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models",
        "authors": [
            "Yu Shang",
            "Yu Li",
            "Fengli Xu",
            "Yong Li"
        ],
        "published": "2024-02-04T16:45:01Z",
        "summary": "Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose a Default-Interventionist framework\n(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,\nDefInt uses smaller-scale language models to generate low-cost reasoning\nthoughts, which resembles the fast intuitions produced by System 1. If the\nintuitions are considered with low confidence, DefInt will invoke the\nreflective reasoning of scaled-up language models as the intervention of System\n2, which can override the default thoughts and rectify the reasoning process.\nExperiments on five representative reasoning tasks show that DefInt\nconsistently achieves state-of-the-art reasoning accuracy and solution\ndiversity. More importantly, it substantially reduces the token cost by 49%-79%\ncompared to the second accurate baselines. Specifically, the open-ended tasks\nhave an average 75% token cost reduction. Code repo with all prompts will be\nreleased upon publication.",
        "pdf_link": "https://arxiv.org/pdf/2402.02563v1.pdf"
    },
    {
        "title": "From Words to Routes: Applying Large Language Models to Vehicle Routing",
        "authors": [
            "Zhehui Huang",
            "Guangyao Shi",
            "Gaurav S. Sukhatme"
        ],
        "published": "2024-03-16T03:54:38Z",
        "summary": "LLMs have shown impressive progress in robotics (e.g., manipulation and\nnavigation) with natural language task descriptions. The success of LLMs in\nthese tasks leads us to wonder: What is the ability of LLMs to solve vehicle\nrouting problems (VRPs) with natural language task descriptions? In this work,\nwe study this question in three steps. First, we construct a dataset with 21\ntypes of single- or multi-vehicle routing problems. Second, we evaluate the\nperformance of LLMs across four basic prompt paradigms of text-to-code\ngeneration, each involving different types of text input. We find that the\nbasic prompt paradigm, which generates code directly from natural language task\ndescriptions, performs the best for GPT-4, achieving 56% feasibility, 40%\noptimality, and 53% efficiency. Third, based on the observation that LLMs may\nnot be able to provide correct solutions at the initial attempt, we propose a\nframework that enables LLMs to refine solutions through self-reflection,\nincluding self-debugging and self-verification. With GPT-4, our proposed\nframework achieves a 16% increase in feasibility, a 7% increase in optimality,\nand a 15% increase in efficiency. Moreover, we examine the sensitivity of GPT-4\nto task descriptions, specifically focusing on how its performance changes when\ncertain details are omitted from the task descriptions, yet the core meaning is\npreserved. Our findings reveal that such omissions lead to a notable decrease\nin performance: 4% in feasibility, 4% in optimality, and 5% in efficiency.\nWebsite: https://sites.google.com/view/words-to-routes/",
        "pdf_link": "https://arxiv.org/pdf/2403.10795v1.pdf"
    },
    {
        "title": "Vaccine: Perturbation-aware Alignment for Large Language Model",
        "authors": [
            "Tiansheng Huang",
            "Sihao Hu",
            "Ling Liu"
        ],
        "published": "2024-02-02T02:56:50Z",
        "summary": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}.",
        "pdf_link": "https://arxiv.org/pdf/2402.01109v3.pdf"
    },
    {
        "title": "Neural networks for abstraction and reasoning: Towards broad generalization in machines",
        "authors": [
            "Mikel Bober-Irizar",
            "Soumya Banerjee"
        ],
        "published": "2024-02-05T20:48:57Z",
        "summary": "For half a century, artificial intelligence research has attempted to\nreproduce the human qualities of abstraction and reasoning - creating computer\nsystems that can learn new concepts from a minimal set of examples, in settings\nwhere humans find this easy. While specific neural networks are able to solve\nan impressive range of problems, broad generalisation to situations outside\ntheir training data has proved elusive.In this work, we look at several novel\napproaches for solving the Abstraction & Reasoning Corpus (ARC), a dataset of\nabstract visual reasoning tasks introduced to test algorithms on broad\ngeneralization. Despite three international competitions with $100,000 in\nprizes, the best algorithms still fail to solve a majority of ARC tasks and\nrely on complex hand-crafted rules, without using machine learning at all. We\nrevisit whether recent advances in neural networks allow progress on this task.\n  First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC.\nDreamCoder automatically writes programs in a bespoke domain-specific language\nto perform reasoning, using a neural network to mimic human intuition. We\npresent the Perceptual Abstraction and Reasoning Language (PeARL) language,\nwhich allows DreamCoder to solve ARC tasks, and propose a new recognition model\nthat allows us to significantly improve on the previous best implementation.We\nalso propose a new encoding and augmentation scheme that allows large language\nmodels (LLMs) to solve ARC tasks, and find that the largest models can solve\nsome ARC tasks. LLMs are able to solve a different group of problems to\nstate-of-the-art solvers, and provide an interesting way to complement other\napproaches. We perform an ensemble analysis, combining models to achieve better\nresults than any system alone. Finally, we publish the arckit Python library to\nmake future research on ARC easier.",
        "pdf_link": "https://arxiv.org/pdf/2402.03507v1.pdf"
    },
    {
        "title": "The Lay Person's Guide to Biomedicine: Orchestrating Large Language Models",
        "authors": [
            "Zheheng Luo",
            "Qianqian Xie",
            "Sophia Ananiadou"
        ],
        "published": "2024-02-21T03:21:14Z",
        "summary": "Automated lay summarisation (LS) aims to simplify complex technical documents\ninto a more accessible format to non-experts. Existing approaches using\npre-trained language models, possibly augmented with external background\nknowledge, tend to struggle with effective simplification and explanation.\nMoreover, automated methods that can effectively assess the `layness' of\ngenerated summaries are lacking. Recently, large language models (LLMs) have\ndemonstrated a remarkable capacity for text simplification, background\ninformation generation, and text evaluation. This has motivated our systematic\nexploration into using LLMs to generate and evaluate lay summaries of\nbiomedical articles. We propose a novel \\textit{Explain-then-Summarise} LS\nframework, which leverages LLMs to generate high-quality background knowledge\nto improve supervised LS. We also evaluate the performance of LLMs for\nzero-shot LS and propose two novel LLM-based LS evaluation metrics, which\nassess layness from multiple perspectives. Finally, we conduct a human\nassessment of generated lay summaries. Our experiments reveal that\nLLM-generated background information can support improved supervised LS.\nFurthermore, our novel zero-shot LS evaluation metric demonstrates a high\ndegree of alignment with human preferences. We conclude that LLMs have an\nimportant part to play in improving both the performance and evaluation of LS\nmethods.",
        "pdf_link": "https://arxiv.org/pdf/2402.13498v1.pdf"
    },
    {
        "title": "Locating and Mitigating Gender Bias in Large Language Models",
        "authors": [
            "Yuchen Cai",
            "Ding Cao",
            "Rongxi Guo",
            "Yaqin Wen",
            "Guiquan Liu",
            "Enhong Chen"
        ],
        "published": "2024-03-21T13:57:43Z",
        "summary": "Large language models(LLM) are pre-trained on extensive corpora to learn\nfacts and human cognition which contain human preferences. However, this\nprocess can inadvertently lead to these models acquiring biases and stereotypes\nprevalent in society. Prior research has typically tackled the issue of bias\nthrough a one-dimensional perspective, concentrating either on locating or\nmitigating it. This limited perspective has created obstacles in facilitating\nresearch on bias to synergistically complement and progressively build upon one\nanother. In this study, we integrate the processes of locating and mitigating\nbias within a unified framework. Initially, we use causal mediation analysis to\ntrace the causal effects of different components' activation within a large\nlanguage model. Building on this, we propose the LSDM (Least Square Debias\nMethod), a knowledge-editing based method for mitigating gender bias in\noccupational pronouns, and compare it against two baselines on three gender\nbias datasets and seven knowledge competency test datasets. The experimental\nresults indicate that the primary contributors to gender bias are the bottom\nMLP modules acting on the last token of occupational pronouns and the top\nattention module acting on the final word in the sentence. Furthermore, LSDM\nmitigates gender bias in the model more effectively than the other baselines,\nwhile fully preserving the model's capabilities in all other aspects.",
        "pdf_link": "https://arxiv.org/pdf/2403.14409v1.pdf"
    },
    {
        "title": "TroubleLLM: Align to Red Team Expert",
        "authors": [
            "Zhuoer Xu",
            "Jianping Zhang",
            "Shiwen Cui",
            "Changhua Meng",
            "Weiqiang Wang"
        ],
        "published": "2024-02-28T03:40:46Z",
        "summary": "Large Language Models (LLMs) become the start-of-the-art solutions for a\nvariety of natural language tasks and are integrated into real-world\napplications. However, LLMs can be potentially harmful in manifesting\nundesirable safety issues like social biases and toxic content. It is\nimperative to assess its safety issues before deployment. However, the quality\nand diversity of test prompts generated by existing methods are still far from\nsatisfactory. Not only are these methods labor-intensive and require large\nbudget costs, but the controllability of test prompt generation is lacking for\nthe specific testing domain of LLM applications. With the idea of LLM for LLM\ntesting, we propose the first LLM, called TroubleLLM, to generate controllable\ntest prompts on LLM safety issues. Extensive experiments and human evaluation\nillustrate the superiority of TroubleLLM on generation quality and generation\ncontrollability.",
        "pdf_link": "https://arxiv.org/pdf/2403.00829v1.pdf"
    },
    {
        "title": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
        "authors": [
            "Jiawei Chen",
            "Yue Jiang",
            "Dingkang Yang",
            "Mingcheng Li",
            "Jinjie Wei",
            "Ziyun Qian",
            "Lihua Zhang"
        ],
        "published": "2024-03-11T03:38:48Z",
        "summary": "While large language models (LLMs) excel in world knowledge understanding,\nadapting them to specific subfields requires precise adjustments. Due to the\nmodel's vast scale, traditional global fine-tuning methods for large models can\nbe computationally expensive and impact generalization. To address this\nchallenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT)\nmethods have emerged and achieved remarkable success in both LLMs and Large\nVision-Language Models (LVLMs). In the medical domain, fine-tuning a medical\nVision-Language Pretrained (VLP) model is essential for adapting it to specific\ntasks. Can the fine-tuning methods for large models be transferred to the\nmedical field to enhance transfer learning efficiency? In this paper, we delve\ninto the fine-tuning methods of LLMs and conduct extensive experiments to\ninvestigate the impact of fine-tuning methods for large models on existing\nmultimodal models in the medical domain from the training data level and the\nmodel structure level. We show the different impacts of fine-tuning methods for\nlarge models on medical VLMs and develop the most efficient ways to fine-tune\nmedical VLP models. We hope this research can guide medical domain researchers\nin optimizing VLMs' training costs, fostering the broader application of VLMs\nin healthcare fields. Code and dataset will be released upon acceptance.",
        "pdf_link": "https://arxiv.org/pdf/2403.06407v1.pdf"
    },
    {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
        "authors": [
            "Chaoya Jiang",
            "Wei Ye",
            "Mengfan Dong",
            "Hongrui Jia",
            "Haiyang Xu",
            "Ming Yan",
            "Ji Zhang",
            "Shikun Zhang"
        ],
        "published": "2024-02-24T05:14:52Z",
        "summary": "Large Vision Language Models exhibit remarkable capabilities but struggle\nwith hallucinations inconsistencies between images and their descriptions.\nPrevious hallucination evaluation studies on LVLMs have identified\nhallucinations in terms of objects, attributes, and relations but overlooked\ncomplex hallucinations that create an entire narrative around a fictional\nentity. In this paper, we introduce a refined taxonomy of hallucinations,\nfeaturing a new category: Event Hallucination. We then utilize advanced LLMs to\ngenerate and filter fine grained hallucinatory data consisting of various types\nof hallucinations, with a particular focus on event hallucinations, laying the\ngroundwork for integrating discriminative and generative evaluation methods\nwithin our universal evaluation framework. The proposed benchmark distinctively\nassesses LVLMs ability to tackle a broad spectrum of hallucinations, making it\na reliable and comprehensive tool for gauging LVLMs efficacy in handling\nhallucinations. We will release our code and data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15721v1.pdf"
    },
    {
        "title": "Investigating the Impact of Data Contamination of Large Language Models in Text-to-SQL Translation",
        "authors": [
            "Federico Ranaldi",
            "Elena Sofia Ruzzetti",
            "Dario Onorati",
            "Leonardo Ranaldi",
            "Cristina Giannone",
            "Andrea Favalli",
            "Raniero Romagnoli",
            "Fabio Massimo Zanzotto"
        ],
        "published": "2024-02-12T22:35:40Z",
        "summary": "Understanding textual description to generate code seems to be an achieved\ncapability of instruction-following Large Language Models (LLMs) in zero-shot\nscenario. However, there is a severe possibility that this translation ability\nmay be influenced by having seen target textual descriptions and the related\ncode. This effect is known as Data Contamination.\n  In this study, we investigate the impact of Data Contamination on the\nperformance of GPT-3.5 in the Text-to-SQL code-generating tasks. Hence, we\nintroduce a novel method to detect Data Contamination in GPTs and examine\nGPT-3.5's Text-to-SQL performances using the known Spider Dataset and our new\nunfamiliar dataset Termite. Furthermore, we analyze GPT-3.5's efficacy on\ndatabases with modified information via an adversarial table disconnection\n(ATD) approach, complicating Text-to-SQL tasks by removing structural pieces of\ninformation from the database. Our results indicate a significant performance\ndrop in GPT-3.5 on the unfamiliar Termite dataset, even with ATD modifications,\nhighlighting the effect of Data Contamination on LLMs in Text-to-SQL\ntranslation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.08100v1.pdf"
    },
    {
        "title": "SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator",
        "authors": [
            "Javad Rafiei Asl",
            "Mohammad H. Rafiei",
            "Manar Alohaly",
            "Daniel Takabi"
        ],
        "published": "2024-03-18T14:45:20Z",
        "summary": "Machine learning models are vulnerable to maliciously crafted Adversarial\nExamples (AEs). Training a machine learning model with AEs improves its\nrobustness and stability against adversarial attacks. It is essential to\ndevelop models that produce high-quality AEs. Developing such models has been\nmuch slower in natural language processing (NLP) than in areas such as computer\nvision. This paper introduces a practical and efficient adversarial attack\nmodel called SSCAE for \\textbf{S}emantic, \\textbf{S}yntactic, and\n\\textbf{C}ontext-aware natural language \\textbf{AE}s generator. SSCAE\nidentifies important words and uses a masked language model to generate an\nearly set of substitutions. Next, two well-known language models are employed\nto evaluate the initial set in terms of semantic and syntactic characteristics.\nWe introduce (1) a dynamic threshold to capture more efficient perturbations\nand (2) a local greedy search to generate high-quality AEs. As a black-box\nmethod, SSCAE generates humanly imperceptible and context-aware AEs that\npreserve semantic consistency and the source language's syntactical and\ngrammatical requirements. The effectiveness and superiority of the proposed\nSSCAE model are illustrated with fifteen comparative experiments and extensive\nsensitivity analysis for parameter optimization. SSCAE outperforms the existing\nmodels in all experiments while maintaining a higher semantic consistency with\na lower query number and a comparable perturbation rate.",
        "pdf_link": "https://arxiv.org/pdf/2403.11833v1.pdf"
    },
    {
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation",
        "authors": [
            "Haoran Xu",
            "Amr Sharaf",
            "Yunmo Chen",
            "Weiting Tan",
            "Lingfeng Shen",
            "Benjamin Van Durme",
            "Kenton Murray",
            "Young Jin Kim"
        ],
        "published": "2024-01-16T15:04:51Z",
        "summary": "Moderate-sized large language models (LLMs) -- those with 7B or 13B\nparameters -- exhibit promising machine translation (MT) performance. However,\neven the top-performing 13B LLM-based translation models, like ALMA, does not\nmatch the performance of state-of-the-art conventional encoder-decoder\ntranslation models or larger-scale LLMs such as GPT-4. In this study, we bridge\nthis performance gap. We first assess the shortcomings of supervised\nfine-tuning for LLMs in the MT task, emphasizing the quality issues present in\nthe reference data, despite being human-generated. Then, in contrast to SFT\nwhich mimics reference translations, we introduce Contrastive Preference\nOptimization (CPO), a novel approach that trains models to avoid generating\nadequate but not perfect translations. Applying CPO to ALMA models with only\n22K parallel sentences and 12M parameters yields significant improvements. The\nresulting model, called ALMA-R, can match or exceed the performance of the WMT\ncompetition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.",
        "pdf_link": "https://arxiv.org/pdf/2401.08417v3.pdf"
    },
    {
        "title": "MouSi: Poly-Visual-Expert Vision-Language Models",
        "authors": [
            "Xiaoran Fan",
            "Tao Ji",
            "Changhao Jiang",
            "Shuo Li",
            "Senjie Jin",
            "Sirui Song",
            "Junke Wang",
            "Boyang Hong",
            "Lu Chen",
            "Guodong Zheng",
            "Ming Zhang",
            "Caishuang Huang",
            "Rui Zheng",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Shihan Dou",
            "Junjie Ye",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "published": "2024-01-30T18:09:11Z",
        "summary": "Current large vision-language models (VLMs) often encounter challenges such\nas insufficient capabilities of a single visual component and excessively long\nvisual tokens. These issues can limit the model's effectiveness in accurately\ninterpreting complex visual information and over-lengthy contextual\ninformation. Addressing these challenges is crucial for enhancing the\nperformance and applicability of VLMs. This paper proposes the use of ensemble\nexperts technique to synergizes the capabilities of individual visual encoders,\nincluding those skilled in image-text matching, OCR, image segmentation, etc.\nThis technique introduces a fusion network to unify the processing of outputs\nfrom different visual experts, while bridging the gap between image encoders\nand pre-trained LLMs. In addition, we explore different positional encoding\nschemes to alleviate the waste of positional encoding caused by lengthy image\nfeature sequences, effectively addressing the issue of position overflow and\nlength limitations. For instance, in our implementation, this technique\nsignificantly reduces the positional occupancy in models like SAM, from a\nsubstantial 4096 to a more efficient and manageable 64 or even down to 1.\nExperimental results demonstrate that VLMs with multiple experts exhibit\nconsistently superior performance over isolated visual encoders and mark a\nsignificant performance boost as more experts are integrated. We have\nopen-sourced the training code used in this report. All of these resources can\nbe found on our project website.",
        "pdf_link": "https://arxiv.org/pdf/2401.17221v1.pdf"
    },
    {
        "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
        "authors": [
            "Ruyi Xu",
            "Yuan Yao",
            "Zonghao Guo",
            "Junbo Cui",
            "Zanlin Ni",
            "Chunjiang Ge",
            "Tat-Seng Chua",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Gao Huang"
        ],
        "published": "2024-03-18T12:04:11Z",
        "summary": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
        "pdf_link": "https://arxiv.org/pdf/2403.11703v1.pdf"
    },
    {
        "title": "Camouflage is all you need: Evaluating and Enhancing Language Model Robustness Against Camouflage Adversarial Attacks",
        "authors": [
            "\u00c1lvaro Huertas-Garc\u00eda",
            "Alejandro Mart\u00edn",
            "Javier Huertas-Tato",
            "David Camacho"
        ],
        "published": "2024-02-15T10:58:22Z",
        "summary": "Adversarial attacks represent a substantial challenge in Natural Language\nProcessing (NLP). This study undertakes a systematic exploration of this\nchallenge in two distinct phases: vulnerability evaluation and resilience\nenhancement of Transformer-based models under adversarial attacks.\n  In the evaluation phase, we assess the susceptibility of three Transformer\nconfigurations, encoder-decoder, encoder-only, and decoder-only setups, to\nadversarial attacks of escalating complexity across datasets containing\noffensive language and misinformation. Encoder-only models manifest a 14% and\n21% performance drop in offensive language detection and misinformation\ndetection tasks, respectively. Decoder-only models register a 16% decrease in\nboth tasks, while encoder-decoder models exhibit a maximum performance drop of\n14% and 26% in the respective tasks.\n  The resilience-enhancement phase employs adversarial training, integrating\npre-camouflaged and dynamically altered data. This approach effectively reduces\nthe performance drop in encoder-only models to an average of 5% in offensive\nlanguage detection and 2% in misinformation detection tasks. Decoder-only\nmodels, occasionally exceeding original performance, limit the performance drop\nto 7% and 2% in the respective tasks. Although not surpassing the original\nperformance, Encoder-decoder models can reduce the drop to an average of 6% and\n2% respectively.\n  Results suggest a trade-off between performance and robustness, with some\nmodels maintaining similar performance while gaining robustness. Our study and\nadversarial training techniques have been incorporated into an open-source tool\nfor generating camouflaged datasets. However, methodology effectiveness depends\non the specific camouflage technique and data encountered, emphasizing the need\nfor continued exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.09874v1.pdf"
    },
    {
        "title": "The Solution for the ICCV 2023 1st Scientific Figure Captioning Challenge",
        "authors": [
            "Dian Chao",
            "Xin Song",
            "Shupeng Zhong",
            "Boyuan Wang",
            "Xiangyu Wu",
            "Chen Zhu",
            "Yang Yang"
        ],
        "published": "2024-03-26T03:03:50Z",
        "summary": "In this paper, we propose a solution for improving the quality of captions\ngenerated for figures in papers. We adopt the approach of summarizing the\ntextual content in the paper to generate image captions. Throughout our study,\nwe encounter discrepancies in the OCR information provided in the official\ndataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR\ninformation from all images. Moreover, we observe that certain textual content\nin the official paper pertains to images that are not relevant for captioning,\nthereby introducing noise during caption generation. To mitigate this issue, we\nleverage LLaMA to extract image-specific information by querying the textual\ncontent based on image mentions, effectively filtering out extraneous\ninformation. Additionally, we recognize a discrepancy between the primary use\nof maximum likelihood estimation during text generation and the evaluation\nmetrics such as ROUGE employed to assess the quality of generated captions. To\nbridge this gap, we integrate the BRIO model framework, enabling a more\ncoherent alignment between the generation and evaluation processes. Our\napproach ranked first in the final test with a score of 4.49.",
        "pdf_link": "https://arxiv.org/pdf/2403.17342v1.pdf"
    },
    {
        "title": "Identifying and Analyzing Task-Encoding Tokens in Large Language Models",
        "authors": [
            "Yu Bai",
            "Heyan Huang",
            "Cesare Spinoso-Di Piano",
            "Marc-Antoine Rondeau",
            "Sanxing Chen",
            "Yang Gao",
            "Jackie Chi Kit Cheung"
        ],
        "published": "2024-01-20T20:55:21Z",
        "summary": "In-context learning (ICL) has become an effective solution for few-shot\nlearning in natural language processing. However, our understanding of ICL's\nworking mechanisms is limited, specifically regarding how models learn to\nperform tasks from ICL demonstrations. For example, unexpectedly large changes\nin performance can arise from small changes in the prompt, leaving prompt\ndesign a largely empirical endeavour. In this paper, we investigate this\nproblem by identifying and analyzing task-encoding tokens on whose\nrepresentations the task performance depends. Using experiments that ablate the\nrepresentations of different token types, we find that template and stopword\ntokens are the most prone to be task-encoding. In addition, we demonstrate\nexperimentally that lexical meaning, repetition, and text formatting are the\nmain distinguishing characteristics of these tokens. Our work sheds light on\nhow large language models (LLMs) learn to perform a task from demonstrations,\ndeepens our understanding of the varied roles different types of tokens play in\nLLMs, and provides insights for avoiding instability from improperly utilizing\ntask-encoding tokens.",
        "pdf_link": "https://arxiv.org/pdf/2401.11323v2.pdf"
    },
    {
        "title": "Transformer-based Causal Language Models Perform Clustering",
        "authors": [
            "Xinbo Wu",
            "Lav R. Varshney"
        ],
        "published": "2024-02-19T14:02:31Z",
        "summary": "Even though large language models (LLMs) have demonstrated remarkable\ncapability in solving various natural language tasks, the capability of an LLM\nto follow human instructions is still a concern. Recent works have shown great\nimprovements in the instruction-following capability via additional training\nfor instruction-following tasks. However, the mechanisms responsible for\neffective instruction-following capabilities remain inadequately understood.\nHere, we introduce a simplified instruction-following task and use synthetic\ndatasets to analyze a Transformer-based causal language model. Our findings\nsuggest that the model learns task-specific information by clustering data\nwithin its hidden space, with this clustering process evolving dynamically\nduring learning. We also demonstrate how this phenomenon assists the model in\nhandling unseen instances, and validate our results in a more realistic\nsetting. Furthermore, we present inspired applications regarding pre-training\nand alignment.",
        "pdf_link": "https://arxiv.org/pdf/2402.12151v2.pdf"
    },
    {
        "title": "Non-discrimination Criteria for Generative Language Models",
        "authors": [
            "Sara Sterlie",
            "Nina Weng",
            "Aasa Feragen"
        ],
        "published": "2024-03-13T14:19:08Z",
        "summary": "Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.08564v1.pdf"
    },
    {
        "title": "The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition",
        "authors": [
            "Georgios Chochlakis",
            "Alexandros Potamianos",
            "Kristina Lerman",
            "Shrikanth Narayanan"
        ],
        "published": "2024-03-25T19:07:32Z",
        "summary": "In-context Learning (ICL) has emerged as a powerful paradigm for performing\nnatural language tasks with Large Language Models (LLM) without updating the\nmodels' parameters, in contrast to the traditional gradient-based finetuning.\nThe promise of ICL is that the LLM can adapt to perform the present task at a\ncompetitive or state-of-the-art level at a fraction of the cost. The ability of\nLLMs to perform tasks in this few-shot manner relies on their background\nknowledge of the task (or task priors). However, recent work has found that,\nunlike traditional learning, LLMs are unable to fully integrate information\nfrom demonstrations that contrast task priors. This can lead to performance\nsaturation at suboptimal levels, especially for subjective tasks such as\nemotion recognition, where the mapping from text to emotions can differ widely\ndue to variability in human annotations. In this work, we design experiments\nand propose measurements to explicitly quantify the consistency of proxies of\nLLM priors and their pull on the posteriors. We show that LLMs have strong yet\ninconsistent priors in emotion recognition that ossify their predictions. We\nalso find that the larger the model, the stronger these effects become. Our\nresults suggest that caution is needed when using ICL with larger LLMs for\naffect-centered tasks outside their pre-training domain and when interpreting\nICL results.",
        "pdf_link": "https://arxiv.org/pdf/2403.17125v1.pdf"
    },
    {
        "title": "LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding",
        "authors": [
            "Masato Fujitake"
        ],
        "published": "2024-03-21T09:25:24Z",
        "summary": "This paper proposes LayoutLLM, a more flexible document analysis method for\nunderstanding imaged documents. Visually Rich Document Understanding tasks,\nsuch as document image classification and information extraction, have gained\nsignificant attention due to their importance. Existing methods have been\ndeveloped to enhance document comprehension by incorporating pre-training\nawareness of images, text, and layout structure. However, these methods require\nfine-tuning for each task and dataset, and the models are expensive to train\nand operate. To overcome this limitation, we propose a new LayoutLLM that\nintegrates these with large-scale language models (LLMs). By leveraging the\nstrengths of existing research in document image understanding and LLMs'\nsuperior language understanding capabilities, the proposed model, fine-tuned\nwith multimodal instruction datasets, performs an understanding of document\nimages in a single model. Our experiments demonstrate improvement over the\nbaseline model in various document analysis tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.14252v1.pdf"
    },
    {
        "title": "HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild",
        "authors": [
            "Zhiying Zhu",
            "Zhiqing Sun",
            "Yiming Yang"
        ],
        "published": "2024-03-07T08:25:46Z",
        "summary": "Hallucinations pose a significant challenge to the reliability of large\nlanguage models (LLMs) in critical domains. Recent benchmarks designed to\nassess LLM hallucinations within conventional NLP tasks, such as\nknowledge-intensive question answering (QA) and summarization, are insufficient\nfor capturing the complexities of user-LLM interactions in dynamic, real-world\nsettings. To address this gap, we introduce HaluEval-Wild, the first benchmark\nspecifically designed to evaluate LLM hallucinations in the wild. We\nmeticulously collect challenging (adversarially filtered by Alpaca) user\nqueries from existing real-world user-LLM interaction datasets, including\nShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing\nthe collected queries, we categorize them into five distinct types, which\nenables a fine-grained analysis of the types of hallucinations LLMs exhibit,\nand synthesize the reference answers with the powerful GPT-4 model and\nretrieval-augmented generation (RAG). Our benchmark offers a novel approach\ntowards enhancing our comprehension and improvement of LLM reliability in\nscenarios reflective of real-world interactions.",
        "pdf_link": "https://arxiv.org/pdf/2403.04307v1.pdf"
    },
    {
        "title": "Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection",
        "authors": [
            "Pei Wang",
            "Keqing He",
            "Yejie Wang",
            "Xiaoshuai Song",
            "Yutao Mou",
            "Jingang Wang",
            "Yunsen Xian",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "published": "2024-02-27T07:02:10Z",
        "summary": "Out-of-domain (OOD) intent detection aims to examine whether the user's query\nfalls outside the predefined domain of the system, which is crucial for the\nproper functioning of task-oriented dialogue (TOD) systems. Previous methods\naddress it by fine-tuning discriminative models. Recently, some studies have\nbeen exploring the application of large language models (LLMs) represented by\nChatGPT to various downstream tasks, but it is still unclear for their ability\non OOD detection task.This paper conducts a comprehensive evaluation of LLMs\nunder various experimental settings, and then outline the strengths and\nweaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot\ncapabilities, but is still at a disadvantage compared to models fine-tuned with\nfull resource. More deeply, through a series of additional analysis\nexperiments, we discuss and summarize the challenges faced by LLMs and provide\nguidance for future work including injecting domain knowledge, strengthening\nknowledge transfer from IND(In-domain) to OOD, and understanding long\ninstructions.",
        "pdf_link": "https://arxiv.org/pdf/2402.17256v2.pdf"
    },
    {
        "title": "Argument Quality Assessment in the Age of Instruction-Following Large Language Models",
        "authors": [
            "Henning Wachsmuth",
            "Gabriella Lapesa",
            "Elena Cabrio",
            "Anne Lauscher",
            "Joonsuk Park",
            "Eva Maria Vecchi",
            "Serena Villata",
            "Timon Ziegenbein"
        ],
        "published": "2024-03-24T10:43:21Z",
        "summary": "The computational treatment of arguments on controversial issues has been\nsubject to extensive NLP research, due to its envisioned impact on opinion\nformation, decision making, writing education, and the like. A critical task in\nany such application is the assessment of an argument's quality - but it is\nalso particularly challenging. In this position paper, we start from a brief\nsurvey of argument quality research, where we identify the diversity of quality\nnotions and the subjectiveness of their perception as the main hurdles towards\nsubstantial progress on argument quality assessment. We argue that the\ncapabilities of instruction-following large language models (LLMs) to leverage\nknowledge across contexts enable a much more reliable assessment. Rather than\njust fine-tuning LLMs towards leaderboard chasing on assessment tasks, they\nneed to be instructed systematically with argumentation theories and scenarios\nas well as with ways to solve argument-related problems. We discuss the\nreal-world opportunities and ethical issues emerging thereby.",
        "pdf_link": "https://arxiv.org/pdf/2403.16084v1.pdf"
    },
    {
        "title": "Zero-shot Explainable Mental Health Analysis on Social Media by Incorporating Mental Scales",
        "authors": [
            "Wenyu Li",
            "Yinuo Zhu",
            "Xin Lin",
            "Ming Li",
            "Ziyue Jiang",
            "Ziqian Zeng"
        ],
        "published": "2024-02-09T09:44:06Z",
        "summary": "Traditional discriminative approaches in mental health analysis are known for\ntheir strong capacity but lack interpretability and demand large-scale\nannotated data. The generative approaches, such as those based on large\nlanguage models (LLMs), have the potential to get rid of heavy annotations and\nprovide explanations but their capabilities still fall short compared to\ndiscriminative approaches, and their explanations may be unreliable due to the\nfact that the generation of explanation is a black-box process. Inspired by the\npsychological assessment practice of using scales to evaluate mental states,\nour method which is called Mental Analysis by Incorporating Mental Scales\n(MAIMS), incorporates two procedures via LLMs. First, the patient completes\nmental scales, and second, the psychologist interprets the collected\ninformation from the mental scales and makes informed decisions. Experimental\nresults show that MAIMS outperforms other zero-shot methods. MAIMS can generate\nmore rigorous explanation based on the outputs of mental scales",
        "pdf_link": "https://arxiv.org/pdf/2402.10948v2.pdf"
    },
    {
        "title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following",
        "authors": [
            "Haochen Shi",
            "Zhiyuan Sun",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9",
            "Bang Liu"
        ],
        "published": "2024-03-05T14:53:53Z",
        "summary": "Embodied Instruction Following (EIF) is a crucial task in embodied learning,\nrequiring agents to interact with their environment through egocentric\nobservations to fulfill natural language instructions. Recent advancements have\nseen a surge in employing large language models (LLMs) within a\nframework-centric approach to enhance performance in embodied learning tasks,\nincluding EIF. Despite these efforts, there exists a lack of a unified\nunderstanding regarding the impact of various components-ranging from visual\nperception to action execution-on task performance. To address this gap, we\nintroduce OPEx, a comprehensive framework that delineates the core components\nessential for solving embodied learning tasks: Observer, Planner, and Executor.\nThrough extensive evaluations, we provide a deep analysis of how each component\ninfluences EIF task performance. Furthermore, we innovate within this space by\ndeploying a multi-agent dialogue strategy on a TextWorld counterpart, further\nenhancing task performance. Our findings reveal that LLM-centric design\nmarkedly improves EIF outcomes, identify visual perception and low-level action\nexecution as critical bottlenecks, and demonstrate that augmenting LLMs with a\nmulti-agent framework further elevates performance.",
        "pdf_link": "https://arxiv.org/pdf/2403.03017v1.pdf"
    },
    {
        "title": "Generative AI for Synthetic Data Generation: Methods, Challenges and the Future",
        "authors": [
            "Xu Guo",
            "Yiqiang Chen"
        ],
        "published": "2024-03-07T03:38:44Z",
        "summary": "The recent surge in research focused on generating synthetic data from large\nlanguage models (LLMs), especially for scenarios with limited data\navailability, marks a notable shift in Generative Artificial Intelligence (AI).\nTheir ability to perform comparably to real-world data positions this approach\nas a compelling solution to low-resource challenges. This paper delves into\nadvanced technologies that leverage these gigantic LLMs for the generation of\ntask-specific training data. We outline methodologies, evaluation techniques,\nand practical applications, discuss the current limitations, and suggest\npotential pathways for future research.",
        "pdf_link": "https://arxiv.org/pdf/2403.04190v1.pdf"
    },
    {
        "title": "HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization",
        "authors": [
            "Qiwei Peng",
            "Yekun Chai",
            "Xuhong Li"
        ],
        "published": "2024-02-26T16:09:00Z",
        "summary": "Large language models (LLMs) have made significant progress in generating\ncodes from textual prompts. However, existing benchmarks have mainly\nconcentrated on translating English prompts to multilingual codes or have been\nconstrained to very limited natural languages (NLs). These benchmarks have\noverlooked the vast landscape of massively multilingual NL to multilingual\ncode, leaving a critical gap in the evaluation of multilingual LLMs. In\nresponse, we introduce HumanEval-XL, a massively multilingual code generation\nbenchmark specifically crafted to address this deficiency. HumanEval-XL\nestablishes connections between 23 NLs and 12 programming languages (PLs), and\ncomprises of a collection of 22,080 prompts with an average of 8.33 test cases.\nBy ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a\ncomprehensive evaluation platform for multilingual LLMs, allowing the\nassessment of the understanding of different NLs. Our work serves as a\npioneering step towards filling the void in evaluating NL generalization in the\narea of multilingual code generation. We make our evaluation code and data\npublicly available at \\url{https://github.com/FloatAI/humaneval-xl}.",
        "pdf_link": "https://arxiv.org/pdf/2402.16694v2.pdf"
    },
    {
        "title": "ImgTrojan: Jailbreaking Vision-Language Models with ONE Image",
        "authors": [
            "Xijia Tao",
            "Shuai Zhong",
            "Lei Li",
            "Qi Liu",
            "Lingpeng Kong"
        ],
        "published": "2024-03-05T12:21:57Z",
        "summary": "There has been an increasing interest in the alignment of large language\nmodels (LLMs) with human values. However, the safety issues of their\nintegration with a vision module, or vision language models (VLMs), remain\nrelatively underexplored. In this paper, we propose a novel jailbreaking attack\nagainst VLMs, aiming to bypass their safety barrier when a user inputs harmful\ninstructions. A scenario where our poisoned (image, text) data pairs are\nincluded in the training data is assumed. By replacing the original textual\ncaptions with malicious jailbreak prompts, our method can perform jailbreak\nattacks with the poisoned images. Moreover, we analyze the effect of poison\nratios and positions of trainable parameters on our attack's success rate. For\nevaluation, we design two metrics to quantify the success rate and the\nstealthiness of our attack. Together with a list of curated harmful\ninstructions, a benchmark for measuring attack efficacy is provided. We\ndemonstrate the efficacy of our attack by comparing it with baseline methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.02910v2.pdf"
    },
    {
        "title": "Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection",
        "authors": [
            "Zhixin Lai",
            "Xuesheng Zhang",
            "Suiyao Chen"
        ],
        "published": "2024-03-20T06:38:13Z",
        "summary": "Large language models (LLMs) have reached human-like proficiency in\ngenerating diverse textual content, underscoring the necessity for effective\nfake text detection to avoid potential risks such as fake news in social media.\nPrevious research has mostly tested single models on in-distribution datasets,\nlimiting our understanding of how these models perform on different types of\ndata for LLM-generated text detection task. We researched this by testing five\nspecialized transformer-based models on both in-distribution and\nout-of-distribution datasets to better assess their performance and\ngeneralizability. Our results revealed that single transformer-based\nclassifiers achieved decent performance on in-distribution dataset but limited\ngeneralization ability on out-of-distribution dataset. To improve it, we\ncombined the individual classifiers models using adaptive ensemble algorithms,\nwhich improved the average accuracy significantly from 91.8% to 99.2% on an\nin-distribution test set and from 62.9% to 72.5% on an out-of-distribution test\nset. The results indicate the effectiveness, good generalization ability, and\ngreat potential of adaptive ensemble algorithms in LLM-generated text\ndetection.",
        "pdf_link": "https://arxiv.org/pdf/2403.13335v1.pdf"
    },
    {
        "title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?",
        "authors": [
            "Xue-Yong Fu",
            "Md Tahmid Rahman Laskar",
            "Elena Khasanova",
            "Cheng Chen",
            "Shashi Bhushan TN"
        ],
        "published": "2024-02-01T18:31:34Z",
        "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities to\nsolve a wide range of tasks without being explicitly fine-tuned on\ntask-specific datasets. However, deploying LLMs in the real world is not\ntrivial, as it requires substantial computing resources. In this paper, we\ninvestigate whether smaller, compact LLMs are a good alternative to the\ncomparatively Larger LLMs2 to address significant costs associated with\nutilizing LLMs in the real world. In this regard, we study the meeting\nsummarization task in a real-world industrial environment and conduct extensive\nexperiments by comparing the performance of fine-tuned compact LLMs (e.g.,\nFLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2,\nGPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning,\nfail to outperform larger zero-shot LLMs in meeting summarization datasets.\nHowever, a notable exception is FLAN-T5 (780M parameters), which performs on\npar or even better than many zero-shot Larger LLMs (from 7B to above 70B\nparameters), while being significantly smaller. This makes compact LLMs like\nFLAN-T5 a suitable cost-efficient solution for real-world industrial\ndeployment.",
        "pdf_link": "https://arxiv.org/pdf/2402.00841v1.pdf"
    },
    {
        "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval",
        "authors": [
            "He Zhu",
            "Wenjia Zhang",
            "Nuoxian Huang",
            "Boyang Li",
            "Luyao Niu",
            "Zipei Fan",
            "Tianle Lun",
            "Yicheng Tao",
            "Junyou Su",
            "Zhaoya Gong",
            "Chenyu Fang",
            "Xing Liu"
        ],
        "published": "2024-02-29T15:41:20Z",
        "summary": "In the field of urban planning, general-purpose large language models often\nstruggle to meet the specific needs of planners. Tasks like generating urban\nplanning texts, retrieving related information, and evaluating planning\ndocuments pose unique challenges. To enhance the efficiency of urban\nprofessionals and overcome these obstacles, we introduce PlanGPT, the first\nspecialized Large Language Model tailored for urban and spatial planning.\nDeveloped through collaborative efforts with institutions like the Chinese\nAcademy of Urban Planning, PlanGPT leverages a customized local database\nretrieval framework, domain-specific fine-tuning of base models, and advanced\ntooling capabilities. Empirical tests demonstrate that PlanGPT has achieved\nadvanced performance, delivering responses of superior quality precisely\ntailored to the intricacies of urban planning.",
        "pdf_link": "https://arxiv.org/pdf/2402.19273v1.pdf"
    },
    {
        "title": "Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM",
        "authors": [
            "Gabriel Ryan",
            "Siddhartha Jain",
            "Mingyue Shang",
            "Shiqi Wang",
            "Xiaofei Ma",
            "Murali Krishna Ramanathan",
            "Baishakhi Ray"
        ],
        "published": "2024-01-31T18:21:49Z",
        "summary": "Testing plays a pivotal role in ensuring software quality, yet conventional\nSearch Based Software Testing (SBST) methods often struggle with complex\nsoftware units, achieving suboptimal test coverage. Recent works using large\nlanguage models (LLMs) for test generation have focused on improving generation\nquality through optimizing the test generation context and correcting errors in\nmodel outputs, but use fixed prompting strategies that prompt the model to\ngenerate tests without additional guidance. As a result LLM-generated\ntestsuites still suffer from low coverage. In this paper, we present SymPrompt,\na code-aware prompting strategy for LLMs in test generation. SymPrompt's\napproach is based on recent work that demonstrates LLMs can solve more complex\nlogical problems when prompted to reason about the problem in a multi-step\nfashion. We apply this methodology to test generation by deconstructing the\ntestsuite generation process into a multi-stage sequence, each of which is\ndriven by a specific prompt aligned with the execution paths of the method\nunder test, and exposing relevant type and dependency focal context to the\nmodel. Our approach enables pretrained LLMs to generate more complete test\ncases without any additional training. We implement SymPrompt using the\nTreeSitter parsing framework and evaluate on a benchmark challenging methods\nfrom open source Python projects. SymPrompt enhances correct test generations\nby a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably,\nwhen applied to GPT-4, SymPrompt improves coverage by over 2x compared to\nbaseline prompting strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.00097v2.pdf"
    },
    {
        "title": "Large Language Models: A Survey",
        "authors": [
            "Shervin Minaee",
            "Tomas Mikolov",
            "Narjes Nikzad",
            "Meysam Chenaghlu",
            "Richard Socher",
            "Xavier Amatriain",
            "Jianfeng Gao"
        ],
        "published": "2024-02-09T05:37:09Z",
        "summary": "Large Language Models (LLMs) have drawn a lot of attention due to their\nstrong performance on a wide range of natural language tasks, since the release\nof ChatGPT in November 2022. LLMs' ability of general-purpose language\nunderstanding and generation is acquired by training billions of model's\nparameters on massive amounts of text data, as predicted by scaling laws\n\\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while\nvery recent, is evolving rapidly in many different ways. In this paper, we\nreview some of the most prominent LLMs, including three popular LLM families\n(GPT, LLaMA, PaLM), and discuss their characteristics, contributions and\nlimitations. We also give an overview of techniques developed to build, and\naugment LLMs. We then survey popular datasets prepared for LLM training,\nfine-tuning, and evaluation, review widely used LLM evaluation metrics, and\ncompare the performance of several popular LLMs on a set of representative\nbenchmarks. Finally, we conclude the paper by discussing open challenges and\nfuture research directions.",
        "pdf_link": "https://arxiv.org/pdf/2402.06196v2.pdf"
    },
    {
        "title": "Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning",
        "authors": [
            "Kang Chen",
            "Zheng Lian",
            "Haiyang Sun",
            "Bin Liu",
            "Jianhua Tao"
        ],
        "published": "2024-02-18T02:52:54Z",
        "summary": "Deception detection has attracted increasing attention due to its importance\nin many practical scenarios. Currently, data scarcity harms the development of\nthis field. On the one hand, it is costly to hire participants to simulate\ndeception scenarios. On the other hand, it is difficult to collect videos\ncontaining deceptive behaviors on the Internet. To address data scarcity, this\npaper proposes a new data collection pipeline. Specifically, we use GPT-4 to\nsimulate a role-play between a suspect and a police officer. During\ninterrogation, the suspect lies to the police officer to evade responsibility\nfor the crime, while the police officer uncovers the truth and gathers\nevidence. Compared with previous datasets, this strategy reduces data\ncollection costs, providing a promising way to increase the dataset size.\nMeanwhile, we extend the traditional deception detection task to deception\nreasoning, further providing evidence for deceptive parts. This dataset can\nalso be used to evaluate the complex reasoning capability of current large\nlanguage models and serve as a reasoning benchmark for further research.",
        "pdf_link": "https://arxiv.org/pdf/2402.11432v1.pdf"
    },
    {
        "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
        "authors": [
            "Elias Stengel-Eskin",
            "Archiki Prasad",
            "Mohit Bansal"
        ],
        "published": "2024-01-29T18:45:30Z",
        "summary": "While large language models (LLMs) are increasingly being used for program\nsynthesis, they lack the global view needed to develop useful abstractions;\nthey generally predict programs one at a time, often repeating the same\nfunctionality. Generating redundant code from scratch is both inefficient and\nerror-prone. To address this, we propose Refactoring for Generalizable\nAbstraction Learning (ReGAL), a gradient-free method for learning a library of\nreusable functions via code refactorization, i.e. restructuring code without\nchanging its execution output. ReGAL learns from a small set of existing\nprograms, iteratively verifying and refining its abstractions via execution. We\nfind that the shared function libraries discovered by ReGAL make programs\neasier to predict across diverse domains. On three datasets (LOGO graphics\ngeneration, Date reasoning, and TextCraft, a Minecraft-based text game), both\nopen-source and proprietary LLMs improve in accuracy when predicting programs\nwith ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy\nincreases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on\nTextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals\nReGAL's abstractions encapsulate frequently-used subroutines as well as\nenvironment dynamics.",
        "pdf_link": "https://arxiv.org/pdf/2401.16467v1.pdf"
    },
    {
        "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
        "authors": [
            "Yifan Zeng",
            "Yiran Wu",
            "Xiao Zhang",
            "Huazheng Wang",
            "Qingyun Wu"
        ],
        "published": "2024-03-02T16:52:22Z",
        "summary": "Despite extensive pre-training and fine-tuning in moral alignment to prevent\ngenerating harmful information at user request, large language models (LLMs)\nremain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense,\na response-filtering based multi-agent defense framework that filters harmful\nresponses from LLMs. This framework assigns different roles to LLM agents and\nemploys them to complete the defense task collaboratively. The division in\ntasks enhances the overall instruction-following of LLMs and enables the\nintegration of other defense components as tools. AutoDefense can adapt to\nvarious sizes and kinds of open-source LLMs that serve as agents. Through\nconducting extensive experiments on a large scale of harmful and safe prompts,\nwe validate the effectiveness of the proposed AutoDefense in improving the\nrobustness against jailbreak attacks, while maintaining the performance at\nnormal user request. Our code and data are publicly available at\nhttps://github.com/XHMY/AutoDefense.",
        "pdf_link": "https://arxiv.org/pdf/2403.04783v1.pdf"
    },
    {
        "title": "Evaluating Biases in Context-Dependent Health Questions",
        "authors": [
            "Sharon Levy",
            "Tahilin Sanchez Karver",
            "William D. Adler",
            "Michelle R. Kaufman",
            "Mark Dredze"
        ],
        "published": "2024-03-07T19:15:40Z",
        "summary": "Chat-based large language models have the opportunity to empower individuals\nlacking high-quality healthcare access to receive personalized information\nacross a variety of topics. However, users may ask underspecified questions\nthat require additional context for a model to correctly answer. We study how\nlarge language model biases are exhibited through these contextual questions in\nthe healthcare domain. To accomplish this, we curate a dataset of sexual and\nreproductive healthcare questions that are dependent on age, sex, and location\nattributes. We compare models' outputs with and without demographic context to\ndetermine group alignment among our contextual questions. Our experiments\nreveal biases in each of these attributes, where young adult female users are\nfavored.",
        "pdf_link": "https://arxiv.org/pdf/2403.04858v1.pdf"
    },
    {
        "title": "Training-Free Semantic Segmentation via LLM-Supervision",
        "authors": [
            "Wenfang Sun",
            "Yingjun Du",
            "Gaowen Liu",
            "Ramana Kompella",
            "Cees G. M. Snoek"
        ],
        "published": "2024-03-31T14:37:25Z",
        "summary": "Recent advancements in open vocabulary models, like CLIP, have notably\nadvanced zero-shot classification and segmentation by utilizing natural\nlanguage for class-specific embeddings. However, most research has focused on\nimproving model accuracy through prompt engineering, prompt learning, or\nfine-tuning with limited labeled data, thereby overlooking the importance of\nrefining the class descriptors. This paper introduces a new approach to\ntext-supervised semantic segmentation using supervision by a large language\nmodel (LLM) that does not require extra training. Our method starts from an\nLLM, like GPT-3, to generate a detailed set of subclasses for more accurate\nclass representation. We then employ an advanced text-supervised semantic\nsegmentation model to apply the generated subclasses as target labels,\nresulting in diverse segmentation results tailored to each subclass's unique\ncharacteristics. Additionally, we propose an assembly that merges the\nsegmentation maps from the various subclass descriptors to ensure a more\ncomprehensive representation of the different aspects in the test images.\nThrough comprehensive experiments on three standard benchmarks, our method\noutperforms traditional text-supervised semantic segmentation methods by a\nmarked margin.",
        "pdf_link": "https://arxiv.org/pdf/2404.00701v1.pdf"
    },
    {
        "title": "Privacy-Aware Semantic Cache for Large Language Models",
        "authors": [
            "Waris Gill",
            "Mohamed Elidrisi",
            "Pallavi Kalapatapu",
            "Ali Anwar",
            "Muhammad Ali Gulzar"
        ],
        "published": "2024-03-05T06:23:50Z",
        "summary": "Large Language Models (LLMs) like ChatGPT and Llama2 have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries, leading to unacceptable false hit-and-miss rates.\n  This paper introduces MeanCache, a user-centric semantic cache for LLMs that\nidentifies semantically similar queries to determine cache hit or miss. Using\nMeanCache, the response to a user's semantically similar query can be retrieved\nfrom a local cache rather than re-querying the LLM, thus reducing costs,\nservice provider load, and environmental impact. Existing caching solutions for\nLLMs raise privacy and scalability concerns and perform wasteful query\nrequests. MeanCache leverages Federated Learning (FL) to collaboratively train\na query similarity model across LLM users without violating privacy. By placing\na local cache in each user's device and using FL, MeanCache reduces the latency\nand costs and enhances model performance, resulting in lower false hit rates.\nMeanCache compresses the embedding dimensions to minimize cache storage and\nalso finds the optimal cosine similarity threshold. Our experiments benchmarked\nagainst the state-of-the-art caching method, reveal that MeanCache attains an\napproximately 17% higher F-score and a 20% increase in precision during\nsemantic cache hit-and-miss decisions. It also reduces the storage requirement\nby 83% and accelerates semantic cache hit-and-miss decisions by 11%.",
        "pdf_link": "https://arxiv.org/pdf/2403.02694v2.pdf"
    },
    {
        "title": "(Ir)rationality and Cognitive Biases in Large Language Models",
        "authors": [
            "Olivia Macmillan-Scott",
            "Mirco Musolesi"
        ],
        "published": "2024-02-14T14:17:21Z",
        "summary": "Do large language models (LLMs) display rational reasoning? LLMs have been\nshown to contain human biases due to the data they have been trained on;\nwhether this is reflected in rational reasoning remains less clear. In this\npaper, we answer this question by evaluating seven language models using tasks\nfrom the cognitive psychology literature. We find that, like humans, LLMs\ndisplay irrationality in these tasks. However, the way this irrationality is\ndisplayed does not reflect that shown by humans. When incorrect answers are\ngiven by LLMs to these tasks, they are often incorrect in ways that differ from\nhuman-like biases. On top of this, the LLMs reveal an additional layer of\nirrationality in the significant inconsistency of the responses. Aside from the\nexperimental results, this paper seeks to make a methodological contribution by\nshowing how we can assess and compare different capabilities of these types of\nmodels, in this case with respect to rational reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.09193v2.pdf"
    },
    {
        "title": "LAB: Large-Scale Alignment for ChatBots",
        "authors": [
            "Shivchander Sudalairaj",
            "Abhishek Bhandwaldar",
            "Aldo Pareja",
            "Kai Xu",
            "David D. Cox",
            "Akash Srivastava"
        ],
        "published": "2024-03-02T03:48:37Z",
        "summary": "This work introduces LAB (Large-scale Alignment for chatBots), a novel\nmethodology designed to overcome the scalability challenges in the\ninstruction-tuning phase of large language model (LLM) training. Leveraging a\ntaxonomy-guided synthetic data generation process and a multi-phase tuning\nframework, LAB significantly reduces reliance on expensive human annotations\nand proprietary models like GPT-4. We demonstrate that LAB-trained models can\nachieve competitive performance across several benchmarks compared to models\ntrained with traditional human-annotated or GPT-4 generated synthetic data.\nThus offering a scalable, cost-effective solution for enhancing LLM\ncapabilities and instruction-following behaviors without the drawbacks of\ncatastrophic forgetting, marking a step forward in the efficient training of\nLLMs for a wide range of applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.01081v2.pdf"
    },
    {
        "title": "Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model",
        "authors": [
            "Peng Zhou",
            "Jianmin Wang",
            "Chunyan Li",
            "Zixu Wang",
            "Yiping Liu",
            "Siqi Sun",
            "Jianxin Lin",
            "Longyue Wang",
            "Xiangxiang Zeng"
        ],
        "published": "2024-03-20T02:15:55Z",
        "summary": "While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 88.08%, 65.27%, and\n61.44%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science. Code is available at\nhttps://github.com/HHW-zhou/TSMMG.",
        "pdf_link": "https://arxiv.org/pdf/2403.13244v1.pdf"
    },
    {
        "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "published": "2024-01-08T02:22:04Z",
        "summary": "With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study the performance of solving MAPF with\nLLMs. We first show the motivating success on an empty room map without\nobstacles, then the failure to plan on the harder room map and maze map of the\nstandard MAPF benchmark. We present our position on why directly solving MAPF\nwith LLMs has not been successful yet, and we use various experiments to\nsupport our hypothesis. Based on our results, we discussed how researchers with\ndifferent backgrounds could help with this problem from different perspectives.",
        "pdf_link": "https://arxiv.org/pdf/2401.03630v2.pdf"
    },
    {
        "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
        "authors": [
            "Wenyue Hua",
            "Xianjun Yang",
            "Zelong Li",
            "Wei Cheng",
            "Yongfeng Zhang"
        ],
        "published": "2024-02-02T17:26:23Z",
        "summary": "The emergence of LLM-based agents has garnered considerable attention, yet\ntheir trustworthiness remains an under-explored area. As agents can directly\ninteract with the physical environment, their reliability and safety is\ncritical. This paper presents an Agent-Constitution-based agent framework,\nTrustAgent, an initial investigation into improving the safety dimension of\ntrustworthiness in LLM-based agents. This framework consists of threefold\nstrategies: pre-planning strategy which injects safety knowledge to the model\nprior to plan generation, in-planning strategy which bolsters safety during\nplan generation, and post-planning strategy which ensures safety by\npost-planning inspection. Through experimental analysis, we demonstrate how\nthese approaches can effectively elevate an LLM agent's safety by identifying\nand preventing potential dangers. Furthermore, we explore the intricate\nrelationships between safety and helpfulness, and between the model's reasoning\nability and its efficacy as a safe agent. This paper underscores the imperative\nof integrating safety awareness and trustworthiness into the design and\ndeployment of LLM-based agents, not only to enhance their performance but also\nto ensure their responsible integration into human-centric environments. Data\nand code are available at https://github.com/agiresearch/TrustAgent.",
        "pdf_link": "https://arxiv.org/pdf/2402.01586v2.pdf"
    },
    {
        "title": "CLongEval: A Chinese Benchmark for Evaluating Long-Context Large Language Models",
        "authors": [
            "Zexuan Qiu",
            "Jingjing Li",
            "Shijue Huang",
            "Wanjun Zhong",
            "Irwin King"
        ],
        "published": "2024-03-06T07:43:43Z",
        "summary": "Developing Large Language Models (LLMs) with robust long-context capabilities\nhas been the recent research focus, resulting in the emergence of long-context\nLLMs proficient in Chinese. However, the evaluation of these models remains\nunderdeveloped due to a lack of benchmarks. To address this gap, we present\nCLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.\nCLongEval is characterized by three key features: (1) Sufficient data volume,\ncomprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,\naccommodating to models with context windows size from 1K to 100K; (3) High\nquality, with over 2,000 manually annotated question-answer pairs in addition\nto the automatically constructed labels. With CLongEval, we undertake a\ncomprehensive assessment of 6 open-source long-context LLMs and 2 leading\ncommercial counterparts that feature both long-context abilities and\nproficiency in Chinese. We also provide in-depth analysis based on the\nempirical results, trying to shed light on the critical capabilities that\npresent challenges in long-context settings. The dataset, evaluation scripts,\nand model outputs will be released.",
        "pdf_link": "https://arxiv.org/pdf/2403.03514v1.pdf"
    },
    {
        "title": "Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do",
        "authors": [
            "William Kidder",
            "Jason D'Cruz",
            "Kush R. Varshney"
        ],
        "published": "2024-01-25T21:30:06Z",
        "summary": "Advances in the performance of large language models (LLMs) have led some\nresearchers to propose the emergence of theory of mind (ToM) in artificial\nintelligence (AI). LLMs can attribute beliefs, desires, intentions, and\nemotions, and they will improve in their accuracy. Rather than employing the\ncharacteristically human method of empathy, they learn to attribute mental\nstates by recognizing linguistic patterns in a dataset that typically do not\ninclude that individual. We ask whether LLMs' inability to empathize precludes\nthem from honoring an individual's right to be an exception, that is, from\nmaking assessments of character and predictions of behavior that reflect\nappropriate sensitivity to a person's individuality. Can LLMs seriously\nconsider an individual's claim that their case is different based on internal\nmental states like beliefs, desires, and intentions, or are they limited to\njudging that case based on its similarities to others? We propose that the\nmethod of empathy has special significance for honoring the right to be an\nexception that is distinct from the value of predictive accuracy, at which LLMs\nexcel. We conclude by considering whether using empathy to consider exceptional\ncases has intrinsic or merely practical value and we introduce conceptual and\nempirical avenues for advancing this investigation.",
        "pdf_link": "https://arxiv.org/pdf/2401.14523v1.pdf"
    },
    {
        "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs",
        "authors": [
            "Shengbang Tong",
            "Zhuang Liu",
            "Yuexiang Zhai",
            "Yi Ma",
            "Yann LeCun",
            "Saining Xie"
        ],
        "published": "2024-01-11T18:58:36Z",
        "summary": "Is vision good enough for language? Recent advancements in multimodal models\nprimarily stem from the powerful reasoning abilities of large language models\n(LLMs). However, the visual component typically depends only on the\ninstance-level contrastive language-image pre-training (CLIP). Our research\nreveals that the visual capabilities in recent multimodal LLMs (MLLMs) still\nexhibit systematic shortcomings. To understand the roots of these errors, we\nexplore the gap between the visual embedding space of CLIP and vision-only\nself-supervised learning. We identify ''CLIP-blind pairs'' - images that CLIP\nperceives as similar despite their clear visual differences. With these pairs,\nwe construct the Multimodal Visual Patterns (MMVP) benchmark. MMVP exposes\nareas where state-of-the-art systems, including GPT-4V, struggle with\nstraightforward questions across nine basic visual patterns, often providing\nincorrect answers and hallucinated explanations. We further evaluate various\nCLIP-based vision-and-language models and found a notable correlation between\nvisual patterns that challenge CLIP models and those problematic for multimodal\nLLMs. As an initial effort to address these issues, we propose a Mixture of\nFeatures (MoF) approach, demonstrating that integrating vision self-supervised\nlearning features with MLLMs can significantly enhance their visual grounding\ncapabilities. Together, our research suggests visual representation learning\nremains an open challenge, and accurate visual grounding is crucial for future\nsuccessful multimodal systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.06209v1.pdf"
    },
    {
        "title": "ReMatch: Retrieval Enhanced Schema Matching with LLMs",
        "authors": [
            "Eitam Sheetrit",
            "Menachem Brief",
            "Moshik Mishaeli",
            "Oren Elisha"
        ],
        "published": "2024-03-03T17:14:40Z",
        "summary": "Schema matching is a crucial task in data integration, involving the\nalignment of a source database schema with a target schema to establish\ncorrespondence between their elements. This task is challenging due to textual\nand semantic heterogeneity, as well as differences in schema sizes. Although\nmachine-learning-based solutions have been explored in numerous studies, they\noften suffer from low accuracy, require manual mapping of the schemas for model\ntraining, or need access to source schema data which might be unavailable due\nto privacy concerns. In this paper we present a novel method, named ReMatch,\nfor matching schemas using retrieval-enhanced Large Language Models (LLMs). Our\nmethod avoids the need for predefined mapping, any model training, or access to\ndata in the source database. In the ReMatch method the tables of the target\nschema and the attributes of the source schema are first represented as\nstructured passage-based documents. For each source attribute document, we\nretrieve $J$ documents, representing target schema tables, according to their\nsemantic relevance. Subsequently, we create a prompt for every source table,\ncomprising all its attributes and their descriptions, alongside all attributes\nfrom the set of top $J$ target tables retrieved previously. We employ LLMs\nusing this prompt for the matching task, yielding a ranked list of $K$\npotential matches for each source attribute. Our experimental results on large\nreal-world schemas demonstrate that ReMatch significantly improves matching\ncapabilities and outperforms other machine learning approaches. By eliminating\nthe requirement for training data, ReMatch becomes a viable solution for\nreal-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2403.01567v1.pdf"
    },
    {
        "title": "\"You are an expert annotator\": Automatic Best-Worst-Scaling Annotations for Emotion Intensity Modeling",
        "authors": [
            "Christopher Bagdon",
            "Prathamesh Karmalker",
            "Harsha Gurulingappa",
            "Roman Klinger"
        ],
        "published": "2024-03-26T11:45:22Z",
        "summary": "Labeling corpora constitutes a bottleneck to create models for new tasks or\ndomains. Large language models mitigate the issue with automatic corpus\nlabeling methods, particularly for categorical annotations. Some NLP tasks such\nas emotion intensity prediction, however, require text regression, but there is\nno work on automating annotations for continuous label assignments. Regression\nis considered more challenging than classification: The fact that humans\nperform worse when tasked to choose values from a rating scale lead to\ncomparative annotation methods, including best-worst scaling. This raises the\nquestion if large language model-based annotation methods show similar\npatterns, namely that they perform worse on rating scale annotation tasks than\non comparative annotation tasks. To study this, we automate emotion intensity\npredictions and compare direct rating scale predictions, pairwise comparisons\nand best-worst scaling. We find that the latter shows the highest reliability.\nA transformer regressor fine-tuned on these data performs nearly on par with a\nmodel trained on the original manual annotations.",
        "pdf_link": "https://arxiv.org/pdf/2403.17612v1.pdf"
    },
    {
        "title": "Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity",
        "authors": [
            "Claudio Novelli",
            "Federico Casolari",
            "Philipp Hacker",
            "Giorgio Spedicato",
            "Luciano Floridi"
        ],
        "published": "2024-01-14T19:16:29Z",
        "summary": "The advent of Generative AI, particularly through Large Language Models\n(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI\nlandscape. Advanced LLMs exhibit multimodality, handling diverse data formats,\nthereby broadening their application scope. However, the complexity and\nemergent autonomy of these models introduce challenges in predictability and\nlegal compliance. This paper delves into the legal and regulatory implications\nof Generative AI and LLMs in the European Union context, analyzing aspects of\nliability, privacy, intellectual property, and cybersecurity. It critically\nexamines the adequacy of the existing and proposed EU legislation, including\nthe Artificial Intelligence Act (AIA) draft, in addressing the unique\nchallenges posed by Generative AI in general and LLMs in particular. The paper\nidentifies potential gaps and shortcomings in the legislative framework and\nproposes recommendations to ensure the safe and compliant deployment of\ngenerative models, ensuring they align with the EU's evolving digital landscape\nand legal standards.",
        "pdf_link": "https://arxiv.org/pdf/2401.07348v4.pdf"
    },
    {
        "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World",
        "authors": [
            "Weiyun Wang",
            "Yiming Ren",
            "Haowen Luo",
            "Tiantong Li",
            "Chenxiang Yan",
            "Zhe Chen",
            "Wenhai Wang",
            "Qingyun Li",
            "Lewei Lu",
            "Xizhou Zhu",
            "Yu Qiao",
            "Jifeng Dai"
        ],
        "published": "2024-02-29T18:59:17Z",
        "summary": "We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.",
        "pdf_link": "https://arxiv.org/pdf/2402.19474v2.pdf"
    },
    {
        "title": "$\\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting",
        "authors": [
            "Zijie Pan",
            "Yushan Jiang",
            "Sahil Garg",
            "Anderson Schneider",
            "Yuriy Nevmyvaka",
            "Dongjin Song"
        ],
        "published": "2024-03-09T05:20:48Z",
        "summary": "Recently, there has been a growing interest in leveraging pre-trained large\nlanguage models (LLMs) for various time series applications. However, the\nsemantic space of LLMs, established through the pre-training, is still\nunderexplored and may help yield more distinctive and informative\nrepresentations to facilitate time series forecasting. To this end, we propose\nSemantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the\npre-trained semantic space with time series embeddings space and perform time\nseries forecasting based on learned prompts from the joint space. We first\ndesign a tokenization module tailored for cross-modality alignment, which\nexplicitly concatenates patches of decomposed time series components to create\nembeddings that effectively encode the temporal dynamics. Next, we leverage the\npre-trained word token embeddings to derive semantic anchors and align selected\nanchors with time series embeddings by maximizing the cosine similarity in the\njoint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as\nprompts to provide strong indicators (context) for time series that exhibit\ndifferent temporal dynamics. With thorough empirical studies on multiple\nbenchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve\nsuperior forecasting performance over state-of-the-art baselines. Furthermore,\nour ablation studies and visualizations verify the necessity of prompt learning\ninformed by semantic space.",
        "pdf_link": "https://arxiv.org/pdf/2403.05798v1.pdf"
    },
    {
        "title": "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain",
        "authors": [
            "Bingchao Wang"
        ],
        "published": "2024-01-10T02:59:49Z",
        "summary": "Recently, various Large Language Models (LLMs) evaluation datasets have\nemerged, but most of them have issues with distorted rankings and difficulty in\nmodel capabilities analysis. Addressing these concerns, this paper introduces\nANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes\nKeypoint categorization standard for the first time, each question in ANGO can\ncorrespond to multiple keypoints, effectively enhancing interpretability of\nevaluation results. Base on performance of real humans, we build a quantifiable\nquestion difficulty standard and divide ANGO questions into 9 difficulty\nlevels, which provide more precise guidance for model training. To minimize\ndata leakage impact and fully leverage ANGO's innovative features, we have\nengineered exclusive sampling strategies and a new evaluation framework that\nsupport swift testset iteration. Our experiments demonstrate that ANGO poses a\nstronger challenge to models and reveals more details in evaluation result\ncompared to existing benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2401.04898v2.pdf"
    },
    {
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
        "authors": [
            "Shuming Ma",
            "Hongyu Wang",
            "Lingxiao Ma",
            "Lei Wang",
            "Wenhui Wang",
            "Shaohan Huang",
            "Li Dong",
            "Ruiping Wang",
            "Jilong Xue",
            "Furu Wei"
        ],
        "published": "2024-02-27T18:56:19Z",
        "summary": "Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.17764v1.pdf"
    },
    {
        "title": "Using Large Language Models for OntoClean-based Ontology Refinement",
        "authors": [
            "Yihang Zhao",
            "Neil Vetter",
            "Kaveh Aryan"
        ],
        "published": "2024-03-23T15:09:50Z",
        "summary": "This paper explores the integration of Large Language Models (LLMs) such as\nGPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing\non the OntoClean methodology. OntoClean, critical for assessing the\nmetaphysical quality of ontologies, involves a two-step process of assigning\nmeta-properties to classes and verifying a set of constraints. Manually\nconducting the first step proves difficult in practice, due to the need for\nphilosophical expertise and lack of consensus among ontologists. By employing\nLLMs with two prompting strategies, the study demonstrates that high accuracy\nin the labelling process can be achieved. The findings suggest the potential\nfor LLMs to enhance ontology refinement, proposing the development of plugin\nsoftware for ontology tools to facilitate this integration.",
        "pdf_link": "https://arxiv.org/pdf/2403.15864v1.pdf"
    },
    {
        "title": "Ploutos: Towards interpretable stock movement prediction with financial large language model",
        "authors": [
            "Hanshuang Tong",
            "Jun Li",
            "Ning Wu",
            "Ming Gong",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "published": "2024-02-18T10:28:18Z",
        "summary": "Recent advancements in large language models (LLMs) have opened new pathways\nfor many domains. However, the full potential of LLMs in financial investments\nremains largely untapped. There are two main challenges for typical deep\nlearning-based methods for quantitative finance. First, they struggle to fuse\ntextual and numerical information flexibly for stock movement prediction.\nSecond, traditional methods lack clarity and interpretability, which impedes\ntheir application in scenarios where the justification for predictions is\nessential. To solve the above challenges, we propose Ploutos, a novel financial\nLLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen\ncontains multiple primary experts that can analyze different modal data, such\nas text and numbers, and provide quantitative strategies from different\nperspectives. Then PloutosGPT combines their insights and predictions and\ngenerates interpretable rationales. To generate accurate and faithful\nrationales, the training strategy of PloutosGPT leverage rearview-mirror\nprompting mechanism to guide GPT-4 to generate rationales, and a dynamic token\nweighting mechanism to finetune LLM by increasing key tokens weight. Extensive\nexperiments show our framework outperforms the state-of-the-art methods on both\nprediction accuracy and interpretability.",
        "pdf_link": "https://arxiv.org/pdf/2403.00782v1.pdf"
    },
    {
        "title": "Comp4D: LLM-Guided Compositional 4D Scene Generation",
        "authors": [
            "Dejia Xu",
            "Hanwen Liang",
            "Neel P. Bhatt",
            "Hezhen Hu",
            "Hanxue Liang",
            "Konstantinos N. Plataniotis",
            "Zhangyang Wang"
        ],
        "published": "2024-03-25T17:55:52Z",
        "summary": "Recent advancements in diffusion models for 2D and 3D content creation have\nsparked a surge of interest in generating 4D content. However, the scarcity of\n3D scene datasets constrains current methodologies to primarily object-centric\ngeneration. To overcome this limitation, we present Comp4D, a novel framework\nfor Compositional 4D Generation. Unlike conventional methods that generate a\nsingular 4D representation of the entire scene, Comp4D innovatively constructs\neach 4D object within the scene separately. Utilizing Large Language Models\n(LLMs), the framework begins by decomposing an input text prompt into distinct\nentities and maps out their trajectories. It then constructs the compositional\n4D scene by accurately positioning these objects along their designated paths.\nTo refine the scene, our method employs a compositional score distillation\ntechnique guided by the pre-defined trajectories, utilizing pre-trained\ndiffusion models across text-to-image, text-to-video, and text-to-3D domains.\nExtensive experiments demonstrate our outstanding 4D content creation\ncapability compared to prior arts, showcasing superior visual quality, motion\nfidelity, and enhanced object interactions.",
        "pdf_link": "https://arxiv.org/pdf/2403.16993v1.pdf"
    },
    {
        "title": "Detecting mental disorder on social media: a ChatGPT-augmented explainable approach",
        "authors": [
            "Loris Belcastro",
            "Riccardo Cantini",
            "Fabrizio Marozzo",
            "Domenico Talia",
            "Paolo Trunfio"
        ],
        "published": "2024-01-30T22:22:55Z",
        "summary": "In the digital era, the prevalence of depressive symptoms expressed on social\nmedia has raised serious concerns, necessitating advanced methodologies for\ntimely detection. This paper addresses the challenge of interpretable\ndepression detection by proposing a novel methodology that effectively combines\nLarge Language Models (LLMs) with eXplainable Artificial Intelligence (XAI) and\nconversational agents like ChatGPT. In our methodology, explanations are\nachieved by integrating BERTweet, a Twitter-specific variant of BERT, into a\nnovel self-explanatory model, namely BERT-XDD, capable of providing both\nclassification and explanations via masked attention. The interpretability is\nfurther enhanced using ChatGPT to transform technical explanations into\nhuman-readable commentaries. By introducing an effective and modular approach\nfor interpretable depression detection, our methodology can contribute to the\ndevelopment of socially responsible digital platforms, fostering early\nintervention and support for mental health challenges under the guidance of\nqualified healthcare professionals.",
        "pdf_link": "https://arxiv.org/pdf/2401.17477v1.pdf"
    },
    {
        "title": "FanOutQA: Multi-Hop, Multi-Document Question Answering for Large Language Models",
        "authors": [
            "Andrew Zhu",
            "Alyssa Hwang",
            "Liam Dugan",
            "Chris Callison-Burch"
        ],
        "published": "2024-02-21T20:30:45Z",
        "summary": "One type of question that is commonly found in day-to-day scenarios is\n``fan-out'' questions, complex multi-hop, multi-document reasoning questions\nthat require finding information about a large number of entities. However,\nthere exist few resources to evaluate this type of question-answering\ncapability among large language models. To evaluate complex reasoning in LLMs\nmore fully, we present FanOutQA, a high-quality dataset of fan-out\nquestion-answer pairs and human-annotated decompositions with English Wikipedia\nas the knowledge base. We formulate three benchmark settings across our dataset\nand benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B,\nfinding that contemporary models still have room to improve reasoning over\ninter-document dependencies in a long context. We provide our dataset and\nopen-source tools to run models to encourage evaluation at https://fanoutqa.com",
        "pdf_link": "https://arxiv.org/pdf/2402.14116v1.pdf"
    },
    {
        "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
        "authors": [
            "Haonan Wang",
            "Qianli Shen",
            "Yao Tong",
            "Yang Zhang",
            "Kenji Kawaguchi"
        ],
        "published": "2024-01-07T08:37:29Z",
        "summary": "The commercialization of diffusion models, renowned for their ability to\ngenerate high-quality images that are often indistinguishable from real ones,\nbrings forth potential copyright concerns. Although attempts have been made to\nimpede unauthorized access to copyrighted material during training and to\nsubsequently prevent DMs from generating copyrighted images, the effectiveness\nof these solutions remains unverified. This study explores the vulnerabilities\nassociated with copyright protection in DMs by introducing a backdoor data\npoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.\nOur attack method operates without requiring access to or control over the\ndiffusion model's training or fine-tuning processes; it merely involves the\ninsertion of poisoning data into the clean training dataset. This data,\ncomprising poisoning images equipped with prompts, is generated by leveraging\nthe powerful capabilities of multimodal large language models and text-guided\nimage inpainting techniques. Our experimental results and analysis confirm the\nmethod's effectiveness. By integrating a minor portion of\nnon-copyright-infringing stealthy poisoning data into the clean\ndataset-rendering it free from suspicion-we can prompt the finetuned diffusion\nmodels to produce copyrighted content when activated by specific trigger\nprompts. These findings underline potential pitfalls in the prevailing\ncopyright protection strategies and underscore the necessity for increased\nscrutiny and preventative measures against the misuse of DMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.04136v1.pdf"
    },
    {
        "title": "InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction",
        "authors": [
            "Sirui Xu",
            "Ziyin Wang",
            "Yu-Xiong Wang",
            "Liang-Yan Gui"
        ],
        "published": "2024-03-28T17:59:30Z",
        "summary": "Text-conditioned human motion generation has experienced significant\nadvancements with diffusion models trained on extensive motion capture data and\ncorresponding textual annotations. However, extending such success to 3D\ndynamic human-object interaction (HOI) generation faces notable challenges,\nprimarily due to the lack of large-scale interaction data and comprehensive\ndescriptions that align with these interactions. This paper takes the\ninitiative and showcases the potential of generating human-object interactions\nwithout direct training on text-interaction pair data. Our key insight in\nachieving this is that interaction semantics and dynamics can be decoupled.\nBeing unable to learn interaction semantics through supervised training, we\ninstead leverage pre-trained large models, synergizing knowledge from a large\nlanguage model and a text-to-motion model. While such knowledge offers\nhigh-level control over interaction semantics, it cannot grasp the intricacies\nof low-level interaction dynamics. To overcome this issue, we further introduce\na world model designed to comprehend simple physics, modeling how human actions\ninfluence object motion. By integrating these components, our novel framework,\nInterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot\nmanner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our\ncomprehensive experimental analysis demonstrates its capability to generate\nrealistic and coherent interaction sequences that seamlessly align with the\ntext directives.",
        "pdf_link": "https://arxiv.org/pdf/2403.19652v1.pdf"
    },
    {
        "title": "Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision",
        "authors": [
            "Zhaoqing Wang",
            "Xiaobo Xia",
            "Ziye Chen",
            "Xiao He",
            "Yandong Guo",
            "Mingming Gong",
            "Tongliang Liu"
        ],
        "published": "2024-02-14T06:01:44Z",
        "summary": "Contemporary cutting-edge open-vocabulary segmentation approaches commonly\nrely on image-mask-text triplets, yet this restricted annotation is\nlabour-intensive and encounters scalability hurdles in complex real-world\nscenarios. Although some methods are proposed to reduce the annotation cost\nwith only text supervision, the incompleteness of supervision severely limits\nthe versatility and performance. In this paper, we liberate the strict\ncorrespondence between masks and texts by using independent image-mask and\nimage-text pairs, which can be easily collected respectively. With this\nunpaired mask-text supervision, we propose a new weakly-supervised\nopen-vocabulary segmentation framework (Uni-OVSeg) that leverages confident\npairs of mask predictions and entities in text descriptions. Using the\nindependent image-mask and image-text pairs, we predict a set of binary masks\nand associate them with entities by resorting to the CLIP embedding space.\nHowever, the inherent noise in the correspondence between masks and entities\nposes a significant challenge when obtaining reliable pairs. In light of this,\nwe advocate using the large vision-language model (LVLM) to refine text\ndescriptions and devise a multi-scale ensemble to stablise the matching between\nmasks and entities. Compared to text-only weakly-supervised methods, our\nUni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K\ndatasets, and even surpasses fully-supervised methods on the challenging PASCAL\nContext-459 dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.08960v1.pdf"
    },
    {
        "title": "APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference",
        "authors": [
            "Bowen Zhao",
            "Hannaneh Hajishirzi",
            "Qingqing Cao"
        ],
        "published": "2024-01-22T18:39:40Z",
        "summary": "Fine-tuning and inference with large Language Models (LM) are generally known\nto be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces\ntraining memory by updating a small number of LM parameters but does not\nimprove inference efficiency. Structured pruning improves LM inference\nefficiency by removing consistent parameter blocks, yet often increases\ntraining memory and time. To improve both training and inference efficiency, we\nintroduce APT that adaptively prunes and tunes parameters for the LMs. At the\nearly stage of fine-tuning, APT dynamically adds salient tuning parameters for\nfast and accurate convergence while discarding unimportant parameters for\nefficiency. Compared to baselines, our experiments show that APT maintains up\nto 98% task performance when pruning RoBERTa and T5 models with 40% parameters\nleft while keeping 86.4% LLaMA models' performance with 70% parameters\nremained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces\nlarge LMs memory training footprint by up to 70%.",
        "pdf_link": "https://arxiv.org/pdf/2401.12200v1.pdf"
    },
    {
        "title": "PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using Large Language Models",
        "authors": [
            "Jiaxuan Li",
            "Minxi Yang",
            "Dahua Gao",
            "Wenlong Xu",
            "Guangming Shi"
        ],
        "published": "2024-01-30T06:55:17Z",
        "summary": "Current communication technologies face limitations in terms of theoretical\ncapacity, spectrum availability, and power resources. Pragmatic communication,\nleveraging terminal intelligence for selective data transmission, offers\nresource conservation. Existing research lacks universal intention resolution\ntools, limiting applicability to specific tasks. This paper proposes an image\npragmatic communication framework based on a Pragmatic Agent for Communication\nEfficiency (PACE) using Large Language Models (LLM). In this framework, PACE\nsequentially performs semantic perception, intention resolution, and\nintention-oriented coding. To ensure the effective utilization of LLM in\ncommunication, a knowledge base is designed to supplement the necessary\nknowledge, dedicated prompts are introduced to facilitate understanding of\npragmatic communication scenarios and task requirements, and a chain of thought\nis designed to assist in making reasonable trade-offs between transmission\nefficiency and cost. For experimental validation, this paper constructs an\nimage pragmatic communication dataset along with corresponding evaluation\nstandards. Simulation results indicate that the proposed method outperforms\ntraditional and non-LLM-based pragmatic communication in terms of transmission\nefficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.01750v1.pdf"
    },
    {
        "title": "DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers",
        "authors": [
            "Xirui Li",
            "Ruochen Wang",
            "Minhao Cheng",
            "Tianyi Zhou",
            "Cho-Jui Hsieh"
        ],
        "published": "2024-02-25T17:43:29Z",
        "summary": "The safety alignment of Large Language Models (LLMs) is vulnerable to both\nmanual and automated jailbreak attacks, which adversarially trigger LLMs to\noutput harmful content. However, current methods for jailbreaking LLMs, which\nnest entire harmful prompts, are not effective at concealing malicious intent\nand can be easily identified and rejected by well-aligned LLMs. This paper\ndiscovers that decomposing a malicious prompt into separated sub-prompts can\neffectively obscure its underlying malicious intent by presenting it in a\nfragmented, less detectable form, thereby addressing these limitations. We\nintroduce an automatic prompt \\textbf{D}ecomposition and\n\\textbf{R}econstruction framework for jailbreak \\textbf{Attack} (DrAttack).\nDrAttack includes three key components: (a) `Decomposition' of the original\nprompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly\nby in-context learning with semantically similar but harmless reassembling\ndemo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'\nsynonyms that maintain the original intent while jailbreaking LLMs. An\nextensive empirical study across multiple open-source and closed-source LLMs\ndemonstrates that, with a significantly reduced number of queries, DrAttack\nobtains a substantial gain of success rate over prior SOTA prompt-only\nattackers. Notably, the success rate of 78.0\\% on GPT-4 with merely 15 queries\nsurpassed previous art by 33.1\\%. The project is available at\nhttps://github.com/xirui-li/DrAttack.",
        "pdf_link": "https://arxiv.org/pdf/2402.16914v2.pdf"
    },
    {
        "title": "Likelihood-based Mitigation of Evaluation Bias in Large Language Models",
        "authors": [
            "Masanari Ohi",
            "Masahiro Kaneko",
            "Ryuto Koike",
            "Mengsay Loem",
            "Naoaki Okazaki"
        ],
        "published": "2024-02-25T04:52:02Z",
        "summary": "Large Language Models (LLMs) are widely used to evaluate natural language\ngeneration tasks as automated metrics. However, the likelihood, a measure of\nLLM's plausibility for a sentence, can vary due to superficial differences in\nsentences, such as word order and sentence structure. It is therefore possible\nthat there might be a likelihood bias if LLMs are used for evaluation: they\nmight overrate sentences with higher likelihoods while underrating those with\nlower likelihoods. In this paper, we investigate the presence and impact of\nlikelihood bias in LLM-based evaluators. We also propose a method to mitigate\nthe likelihood bias. Our method utilizes highly biased instances as few-shot\nexamples for in-context learning. Our experiments in evaluating the\ndata-to-text and grammatical error correction tasks reveal that several LLMs we\ntest display a likelihood bias. Furthermore, our proposed method successfully\nmitigates this bias, also improving evaluation performance (in terms of\ncorrelation of models with human scores) significantly.",
        "pdf_link": "https://arxiv.org/pdf/2402.15987v2.pdf"
    },
    {
        "title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters",
        "authors": [
            "Li Lucy",
            "Suchin Gururangan",
            "Luca Soldaini",
            "Emma Strubell",
            "David Bamman",
            "Lauren Klein",
            "Jesse Dodge"
        ],
        "published": "2024-01-12T07:10:10Z",
        "summary": "Large language models' (LLMs) abilities are drawn from their pretraining\ndata, and model development begins with data curation. However, decisions\naround what data is retained or removed during this initial stage is\nunder-scrutinized. In our work, we ground web text, which is a popular\npretraining data source, to its social and geographic contexts. We create a new\ndataset of 10.3 million self-descriptions of website creators, and extract\ninformation about who they are and where they are from: their topical\ninterests, social roles, and geographic affiliations. Then, we conduct the\nfirst study investigating how ten \"quality\" and English language identification\n(langID) filters affect webpages that vary along these social dimensions. Our\nexperiments illuminate a range of implicit preferences in data curation: we\nshow that some quality classifiers act like topical domain filters, and langID\ncan overlook English content from some regions of the world. Overall, we hope\nthat our work will encourage a new line of research on pretraining data\ncuration practices and its social implications.",
        "pdf_link": "https://arxiv.org/pdf/2401.06408v2.pdf"
    },
    {
        "title": "Metacognition is all you need? Using Introspection in Generative Agents to Improve Goal-directed Behavior",
        "authors": [
            "Jason Toy",
            "Josh MacAdam",
            "Phil Tabor"
        ],
        "published": "2024-01-09T15:00:47Z",
        "summary": "Recent advances in Large Language Models (LLMs) have shown impressive\ncapabilities in various applications, yet LLMs face challenges such as limited\ncontext windows and difficulties in generalization. In this paper, we introduce\na metacognition module for generative agents, enabling them to observe their\nown thought processes and actions. This metacognitive approach, designed to\nemulate System 1 and System 2 cognitive processes, allows agents to\nsignificantly enhance their performance by modifying their strategy. We tested\nthe metacognition module on a variety of scenarios, including a situation where\ngenerative agents must survive a zombie apocalypse, and observe that our system\noutperform others, while agents adapt and improve their strategies to complete\ntasks over time.",
        "pdf_link": "https://arxiv.org/pdf/2401.10910v2.pdf"
    },
    {
        "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
        "authors": [
            "Ling Yang",
            "Zhaochen Yu",
            "Chenlin Meng",
            "Minkai Xu",
            "Stefano Ermon",
            "Bin Cui"
        ],
        "published": "2024-01-22T06:16:29Z",
        "summary": "Diffusion models have exhibit exceptional performance in text-to-image\ngeneration and editing. However, existing methods often face challenges when\nhandling complex text prompts that involve multiple objects with multiple\nattributes and relationships. In this paper, we propose a brand new\ntraining-free text-to-image generation/editing framework, namely Recaption,\nPlan and Generate (RPG), harnessing the powerful chain-of-thought reasoning\nability of multimodal LLMs to enhance the compositionality of text-to-image\ndiffusion models. Our approach employs the MLLM as a global planner to\ndecompose the process of generating complex images into multiple simpler\ngeneration tasks within subregions. We propose complementary regional diffusion\nto enable region-wise compositional generation. Furthermore, we integrate\ntext-guided image generation and editing within the proposed RPG in a\nclosed-loop fashion, thereby enhancing generalization ability. Extensive\nexperiments demonstrate our RPG outperforms state-of-the-art text-to-image\ndiffusion models, including DALL-E 3 and SDXL, particularly in multi-category\nobject composition and text-image semantic alignment. Notably, our RPG\nframework exhibits wide compatibility with various MLLM architectures (e.g.,\nMiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available\nat: https://github.com/YangLing0818/RPG-DiffusionMaster",
        "pdf_link": "https://arxiv.org/pdf/2401.11708v2.pdf"
    },
    {
        "title": "Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion",
        "authors": [
            "Ziyue Wang",
            "Chi Chen",
            "Yiqi Zhu",
            "Fuwen Luo",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-19T14:59:07Z",
        "summary": "With the bloom of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs) that incorporate LLMs with pre-trained vision models have\nrecently demonstrated impressive performance across diverse vision-language\ntasks. However, they fall short to comprehend context involving multiple\nimages. A primary reason for this shortcoming is that the visual features for\neach images are encoded individually by frozen encoders before feeding into the\nLLM backbone, lacking awareness of other images and the multimodal\ninstructions. We term this issue as prior-LLM modality isolation and propose a\ntwo phase paradigm, browse-and-concentrate, to enable in-depth multimodal\ncontext fusion prior to feeding the features into LLMs. This paradigm initially\n\"browses\" through the inputs for essential insights, and then revisits the\ninputs to \"concentrate\" on crucial details, guided by these insights, to\nachieve a more comprehensive understanding of the multimodal inputs.\nAdditionally, we develop training strategies specifically to enhance the\nunderstanding of multi-image inputs. Our method markedly boosts the performance\non 7 multi-image scenarios, contributing to increments on average accuracy by\n2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.12195v1.pdf"
    },
    {
        "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across Time",
        "authors": [
            "Lyle Muller",
            "Patricia S. Churchland",
            "Terrence J. Sejnowski"
        ],
        "published": "2024-01-25T16:01:49Z",
        "summary": "The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence into a long\n\"encoding vector\" - that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity, traveling across single cortical regions or across\nmultiple regions at the whole-brain scale, could implement a similar encoding\nprinciple. By encapsulating recent input history into a single spatial pattern\nat each moment in time, cortical waves may enable temporal context to be\nextracted from sequences of sensory inputs, the same computational principle\nused in transformers.",
        "pdf_link": "https://arxiv.org/pdf/2401.14267v1.pdf"
    },
    {
        "title": "BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation",
        "authors": [
            "Yuhong He",
            "Yongqi Zhang",
            "Shizhu He",
            "Jun Wan"
        ],
        "published": "2024-03-28T13:38:13Z",
        "summary": "Medical dialogue generation (MDG) has gained increasing attention due to its\nsubstantial practical value. Previous works typically employ a\nsequence-to-sequence framework to generate medical responses by modeling\ndialogue context as sequential text with annotated medical entities. While\nthese methods have been successful in generating fluent responses, they fail to\nprovide process explanations of reasoning and require extensive entity\nannotation. To address these limitations, we propose the method Bootstrap\nPrompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's\nmulti-step reasoning process and iteratively enhance this reasoning process. We\nemploy a least-to-most prompting strategy to guide a large language model (LLM)\nin explicit reasoning, breaking down MDG into simpler sub-questions. These\nsub-questions build on answers from previous ones. Additionally, we also\nintroduce two distinct bootstrapping techniques for prompting, which\nautonomously correct errors and facilitate the LLM's explicit reasoning. This\napproach eliminates the need for entity annotation and increases the\ntransparency of the MDG process by explicitly generating the intermediate\nreasoning chain. The experimental findings on the two public datasets indicate\nthat BP4ER outperforms state-of-the-art methods in terms of both objective and\nsubjective evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.19414v1.pdf"
    },
    {
        "title": "SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks",
        "authors": [
            "Gourab Dey",
            "Adithya V Ganesan",
            "Yash Kumar Lal",
            "Manal Shah",
            "Shreyashee Sinha",
            "Matthew Matero",
            "Salvatore Giorgi",
            "Vivek Kulkarni",
            "H. Andrew Schwartz"
        ],
        "published": "2024-02-03T01:33:16Z",
        "summary": "Social science NLP tasks, such as emotion or humor detection, are required to\ncapture the semantics along with the implicit pragmatics from text, often with\nlimited amounts of training data. Instruction tuning has been shown to improve\nthe many capabilities of large language models (LLMs) such as commonsense\nreasoning, reading comprehension, and computer programming. However, little is\nknown about the effectiveness of instruction tuning on the social domain where\nimplicit pragmatic cues are often needed to be captured. We explore the use of\ninstruction tuning for social science NLP tasks and introduce Socialite-Llama\n-- an open-source, instruction-tuned Llama. On a suite of 20 social science\ntasks, Socialite-Llama improves upon the performance of Llama as well as\nmatches or improves upon the performance of a state-of-the-art, multi-task\nfinetuned model on a majority of them. Further, Socialite-Llama also leads to\nimprovement on 5 out of 6 related social tasks as compared to Llama, suggesting\ninstruction tuning can lead to generalized social understanding. All resources\nincluding our code, model and dataset can be found through\nbit.ly/socialitellama.",
        "pdf_link": "https://arxiv.org/pdf/2402.01980v2.pdf"
    },
    {
        "title": "\u03b4-CAUSAL: Exploring Defeasibility in Causal Reasoning",
        "authors": [
            "Shaobo Cui",
            "Lazar Milikic",
            "Yiyang Feng",
            "Mete Ismayilzada",
            "Debjit Paul",
            "Antoine Bosselut",
            "Boi Faltings"
        ],
        "published": "2024-01-06T10:08:33Z",
        "summary": "Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present {\\delta}-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n{\\delta}-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin {\\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by {\\delta}-CAUSAL.",
        "pdf_link": "https://arxiv.org/pdf/2401.03183v1.pdf"
    },
    {
        "title": "Arrows of Time for Large Language Models",
        "authors": [
            "Vassilis Papadopoulos",
            "J\u00e9r\u00e9mie Wenger",
            "Cl\u00e9ment Hongler"
        ],
        "published": "2024-01-30T23:46:35Z",
        "summary": "We study the probabilistic modeling performed by Autoregressive Large\nLanguage Models through the angle of time directionality. We empirically find a\ntime asymmetry exhibited by such models in their ability to model natural\nlanguage: a difference in the average log-perplexity when trying to predict the\nnext token versus when trying to predict the previous one. This difference is\nat the same time subtle and very consistent across various modalities\n(language, model size, training time, ...). Theoretically, this is surprising:\nfrom an information-theoretic point of view, there should be no such\ndifference. We provide a theoretical framework to explain how such an asymmetry\ncan appear from sparsity and computational complexity considerations, and\noutline a number of perspectives opened by our results.",
        "pdf_link": "https://arxiv.org/pdf/2401.17505v2.pdf"
    },
    {
        "title": "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment",
        "authors": [
            "Jiongxiao Wang",
            "Jiazhao Li",
            "Yiquan Li",
            "Xiangyu Qi",
            "Junjie Hu",
            "Yixuan Li",
            "Patrick McDaniel",
            "Muhao Chen",
            "Bo Li",
            "Chaowei Xiao"
        ],
        "published": "2024-02-22T21:05:18Z",
        "summary": "Despite the general capabilities of Large Language Models (LLMs) like GPT-4\nand Llama-2, these models still request fine-tuning or adaptation with\ncustomized data when it comes to meeting the specific business demands and\nintricacies of tailored use cases. However, this process inevitably introduces\nnew safety threats, particularly against the Fine-tuning based Jailbreak Attack\n(FJAttack), where incorporating just a few harmful examples into the\nfine-tuning dataset can significantly compromise the model safety. Though\npotential defenses have been proposed by incorporating safety examples into the\nfine-tuning dataset to reduce the safety issues, such approaches require\nincorporating a substantial amount of safety examples, making it inefficient.\nTo effectively defend against the FJAttack with limited safety examples, we\npropose a Backdoor Enhanced Safety Alignment method inspired by an analogy with\nthe concept of backdoor attacks. In particular, we construct prefixed safety\nexamples by integrating a secret prompt, acting as a \"backdoor trigger\", that\nis prefixed to safety examples. Our comprehensive experiments demonstrate that\nthrough the Backdoor Enhanced Safety Alignment with adding as few as 11\nprefixed safety examples, the maliciously fine-tuned LLMs will achieve similar\nsafety performance as the original aligned models. Furthermore, we also explore\nthe effectiveness of our method in a more practical setting where the\nfine-tuning data consists of both FJAttack examples and the fine-tuning task\ndata. Our method shows great efficacy in defending against FJAttack without\nharming the performance of fine-tuning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.14968v2.pdf"
    },
    {
        "title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
        "authors": [
            "Grgur Kova\u010d",
            "R\u00e9my Portelas",
            "Masataka Sawayama",
            "Peter Ford Dominey",
            "Pierre-Yves Oudeyer"
        ],
        "published": "2024-02-19T14:53:01Z",
        "summary": "The standard way to study Large Language Models (LLMs) through benchmarks or\npsychology questionnaires is to provide many different queries from similar\nminimal contexts (e.g. multiple choice questions). However, due to LLM's highly\ncontext-dependent nature, conclusions from such minimal-context evaluations may\nbe little informative about the model's behavior in deployment (where it will\nbe exposed to many new contexts). We argue that context-dependence should be\nstudied as another dimension of LLM comparison alongside others such as\ncognitive abilities, knowledge, or model size. In this paper, we present a\ncase-study about the stability of value expression over different contexts\n(simulated conversations on different topics), and as measured using a standard\npsychology questionnaire (PVQ) and a behavioral downstream task. We consider 19\nopen-sourced LLMs from five families. Reusing methods from psychology, we study\nRank-order stability on the population (interpersonal) level, and Ipsative\nstability on the individual (intrapersonal) level. We explore two settings:\nwith and without instructing LLMs to simulate particular personalities. We\nobserve similar trends in the stability of models and model families - Mixtral,\nMistral and Qwen families being more stable than LLaMa-2 and Phi - over those\ntwo settings, two different simulated populations, and even in the downstream\nbehavioral task. When instructed to simulate particular personas, LLMs exhibit\nlow Rank-Order stability, and this stability further diminishes with\nconversation length. This highlights the need for future research directions on\nLLMs that can coherently simulate a diversity of personas, as well as how\ncontext-dependence can be studied in more thorough and efficient ways. This\npaper provides a foundational step in that direction, and, to our knowledge, it\nis the first study of value stability in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14846v1.pdf"
    },
    {
        "title": "On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs",
        "authors": [
            "Hankz Hankui Zhuo",
            "Xin Chen",
            "Rong Pan"
        ],
        "published": "2024-02-18T15:53:32Z",
        "summary": "Plan synthesis aims to generate a course of actions or policies to transit\ngiven initial states to goal states, provided domain models that could be\ndesigned by experts or learnt from training data or interactions with the\nworld. Intrigued by the claims of emergent planning capabilities in large\nlanguage models (LLMs), works have been proposed to investigate the planning\neffectiveness of LLMs, without considering any utilization of off-the-shelf\nplanning techniques in LLMs. In this paper, we aim to further study the insight\nof the planning capability of LLMs by investigating the roles of LLMs in\noff-the-shelf planning frameworks. To do this, we investigate the effectiveness\nof embedding LLMs into one of the well-known planning frameworks, graph-based\nplanning, proposing a novel LLMs-based planning framework with LLMs embedded in\ntwo levels of planning graphs, i.e., mutual constraints generation level and\nconstraints solving level. We empirically exhibit the effectiveness of our\nproposed framework in various planning domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.00783v1.pdf"
    },
    {
        "title": "MELTing point: Mobile Evaluation of Language Transformers",
        "authors": [
            "Stefanos Laskaridis",
            "Kleomenis Katevas",
            "Lorenzo Minto",
            "Hamed Haddadi"
        ],
        "published": "2024-03-19T15:51:21Z",
        "summary": "Transformers have revolutionized the machine learning landscape, gradually\nmaking their way into everyday tasks and equipping our computers with ``sparks\nof intelligence''. However, their runtime requirements have prevented them from\nbeing broadly deployed on mobile. As personal devices become increasingly\npowerful and prompt privacy becomes an ever more pressing issue, we explore the\ncurrent state of mobile execution of Large Language Models (LLMs). To achieve\nthis, we have created our own automation infrastructure, MELT, which supports\nthe headless execution and benchmarking of LLMs on device, supporting different\nmodels, devices and frameworks, including Android, iOS and Nvidia Jetson\ndevices. We evaluate popular instruction fine-tuned LLMs and leverage different\nframeworks to measure their end-to-end and granular performance, tracing their\nmemory and energy requirements along the way.\n  Our analysis is the first systematic study of on-device LLM execution,\nquantifying performance, energy efficiency and accuracy across various\nstate-of-the-art models and showcases the state of on-device intelligence in\nthe era of hyperscale models. Results highlight the performance heterogeneity\nacross targets and corroborates that LLM inference is largely memory-bound.\nQuantization drastically reduces memory requirements and renders execution\nviable, but at a non-negligible accuracy cost. Drawing from its energy\nfootprint and thermal behavior, the continuous execution of LLMs remains\nelusive, as both factors negatively affect user experience. Last, our\nexperience shows that the ecosystem is still in its infancy, and algorithmic as\nwell as hardware breakthroughs can significantly shift the execution cost. We\nexpect NPU acceleration, and framework-hardware co-design to be the biggest bet\ntowards efficient standalone execution, with the alternative of offloading\ntailored towards edge deployments.",
        "pdf_link": "https://arxiv.org/pdf/2403.12844v2.pdf"
    },
    {
        "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance",
        "authors": [
            "Adrian Theuma",
            "Ehsan Shareghi"
        ],
        "published": "2024-01-27T07:08:37Z",
        "summary": "Large language models (LLMs) have exhibited an array of reasoning\ncapabilities but face challenges like error propagation and hallucination,\nparticularly in specialised areas like finance, where data is heterogeneous,\nand precision is paramount. We explore the potential of language model\naugmentation with external tools to mitigate these limitations and offload\ncertain reasoning steps to external tools that are more suited for the task,\ninstead of solely depending on the LLM's inherent abilities. More concretely,\nusing financial domain question-answering datasets, we apply supervised\nfine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and\n'task solver'. The 'task router' dynamically directs a question to either be\nanswered internally by the LLM or externally via the right tool from the tool\nset. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%\nand 5.06% over the base model and SFT-only baselines, respectively, and is\nhighly competitive with strong GPT-3.5 results. To the best of our knowledge,\nour work is the first that investigates tool augmentation of language models\nfor the finance domain.",
        "pdf_link": "https://arxiv.org/pdf/2401.15328v2.pdf"
    },
    {
        "title": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model",
        "authors": [
            "Zhiwei Liu",
            "Boyang Liu",
            "Paul Thompson",
            "Kailai Yang",
            "Raghav Jain",
            "Sophia Ananiadou"
        ],
        "published": "2024-03-11T14:35:45Z",
        "summary": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/.",
        "pdf_link": "https://arxiv.org/pdf/2403.06765v1.pdf"
    },
    {
        "title": "RepairAgent: An Autonomous, LLM-Based Agent for Program Repair",
        "authors": [
            "Islem Bouzenia",
            "Premkumar Devanbu",
            "Michael Pradel"
        ],
        "published": "2024-03-25T19:17:43Z",
        "summary": "Automated program repair has emerged as a powerful technique to mitigate the\nimpact of software bugs on system reliability and user experience. This paper\nintroduces RepairAgent, the first work to address the program repair challenge\nthrough an autonomous agent based on a large language model (LLM). Unlike\nexisting deep learning-based approaches, which prompt a model with a fixed\nprompt or in a fixed feedback loop, our work treats the LLM as an agent capable\nof autonomously planning and executing actions to fix bugs by invoking suitable\ntools. RepairAgent freely interleaves gathering information about the bug,\ngathering repair ingredients, and validating fixes, while deciding which tools\nto invoke based on the gathered information and feedback from previous fix\nattempts. Key contributions that enable RepairAgent include a set of tools that\nare useful for program repair, a dynamically updated prompt format that allows\nthe LLM to interact with these tools, and a finite state machine that guides\nthe agent in invoking the tools. Our evaluation on the popular Defects4J\ndataset demonstrates RepairAgent's effectiveness in autonomously repairing 164\nbugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM\nimposes an average cost of 270,000 tokens per bug, which, under the current\npricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To\nthe best of our knowledge, this work is the first to present an autonomous,\nLLM-based agent for program repair, paving the way for future agent-based\ntechniques in software engineering.",
        "pdf_link": "https://arxiv.org/pdf/2403.17134v1.pdf"
    },
    {
        "title": "Data Interpreter: An LLM Agent For Data Science",
        "authors": [
            "Sirui Hong",
            "Yizhang Lin",
            "Bang Liu",
            "Bangbang Liu",
            "Binhao Wu",
            "Danyang Li",
            "Jiaqi Chen",
            "Jiayi Zhang",
            "Jinlin Wang",
            "Li Zhang",
            "Lingyao Zhang",
            "Min Yang",
            "Mingchen Zhuge",
            "Taicheng Guo",
            "Tuo Zhou",
            "Wei Tao",
            "Wenyi Wang",
            "Xiangru Tang",
            "Xiangtao Lu",
            "Xiawu Zheng",
            "Xinbing Liang",
            "Yaying Fei",
            "Yuheng Cheng",
            "Zongze Xu",
            "Chenglin Wu"
        ],
        "published": "2024-02-28T19:49:55Z",
        "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable\neffectiveness. However, their performance can be compromised in data science\nscenarios that require real-time data adjustment, expertise in optimization due\nto complex dependencies among various tasks, and the ability to identify\nlogical errors for precise reasoning. In this study, we introduce the Data\nInterpreter, a solution designed to solve with code that emphasizes three\npivotal techniques to augment problem-solving in data science: 1) dynamic\nplanning with hierarchical graph structures for real-time data adaptability;2)\ntool integration dynamically to enhance code proficiency during execution,\nenriching the requisite expertise;3) logical inconsistency identification in\nfeedback, and efficiency enhancement through experience recording. We evaluate\nthe Data Interpreter on various data science and real-world tasks. Compared to\nopen-source baselines, it demonstrated superior performance, exhibiting\nsignificant improvements in machine learning tasks, increasing from 0.86 to\n0.95. Additionally, it showed a 26% increase in the MATH dataset and a\nremarkable 112% improvement in open-ended tasks. The solution will be released\nat https://github.com/geekan/MetaGPT.",
        "pdf_link": "https://arxiv.org/pdf/2402.18679v3.pdf"
    },
    {
        "title": "Leveraging Print Debugging to Improve Code Generation in Large Language Models",
        "authors": [
            "Xueyu Hu",
            "Kun Kuang",
            "Jiankai Sun",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "published": "2024-01-10T18:37:59Z",
        "summary": "Large language models (LLMs) have made significant progress in code\ngeneration tasks, but their performance in tackling programming problems with\ncomplex data structures and algorithms remains suboptimal. To address this\nissue, we propose an in-context learning approach that guides LLMs to debug by\nusing a \"print debugging\" method, which involves inserting print statements to\ntrace and analysing logs for fixing the bug. We collect a Leetcode problem\ndataset and evaluate our method using the Leetcode online judging system.\nExperiments with GPT-4 demonstrate the effectiveness of our approach,\noutperforming rubber duck debugging in easy and medium-level Leetcode problems\nby 1.5% and 17.9%.",
        "pdf_link": "https://arxiv.org/pdf/2401.05319v1.pdf"
    },
    {
        "title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code",
        "authors": [
            "Bryan Li",
            "Tamer Alkhouli",
            "Daniele Bonadiman",
            "Nikolaos Pappas",
            "Saab Mansour"
        ],
        "published": "2024-03-05T00:48:56Z",
        "summary": "Development of large language models (LLM) have shown progress on reasoning,\nthough studies have been limited to English or simple reasoning tasks. We thus\nintroduce a multilingual structured reasoning and explanation dataset, termed\nxSTREET, that covers four tasks across six languages. xSTREET exposes a gap in\nbase LLM performance between English and non-English reasoning tasks. We then\npropose two methods to remedy this gap, building on the insight that LLMs\ntrained on code are better reasoners. First, at training time, we augment a\ncode dataset with multi-lingual comments using machine translation while\nkeeping program code as-is. Second, at inference time, we bridge the gap\nbetween training and inference by employing a prompt structure that\nincorporates step-by-step code primitives to derive new facts and find a\nsolution. Our methods show improved multilingual performance on xSTREET, most\nnotably on the scientific commonsense reasoning subtask. Furthermore, the\nmodels show no regression on non-reasoning tasks, thus showing our techniques\nmaintain general-purpose abilities.",
        "pdf_link": "https://arxiv.org/pdf/2403.02567v1.pdf"
    },
    {
        "title": "Unlearnable Algorithms for In-context Learning",
        "authors": [
            "Andrei Muresanu",
            "Anvith Thudi",
            "Michael R. Zhang",
            "Nicolas Papernot"
        ],
        "published": "2024-02-01T16:43:04Z",
        "summary": "Machine unlearning is a desirable operation as models get increasingly\ndeployed on data with unknown provenance. However, achieving exact unlearning\n-- obtaining a model that matches the model distribution when the data to be\nforgotten was never used -- is challenging or inefficient, often requiring\nsignificant retraining. In this paper, we focus on efficient unlearning methods\nfor the task adaptation phase of a pretrained large language model (LLM). We\nobserve that an LLM's ability to do in-context learning for task adaptation\nallows for efficient exact unlearning of task adaptation training data. We\nprovide an algorithm for selecting few-shot training examples to prepend to the\nprompt given to an LLM (for task adaptation), ERASE, whose unlearning operation\ncost is independent of model and dataset size, meaning it scales to large\nmodels and datasets. We additionally compare our approach to fine-tuning\napproaches and discuss the trade-offs between the two approaches. This leads us\nto propose a new holistic measure of unlearning cost which accounts for varying\ninference costs, and conclude that in-context learning can often be more\nfavourable than fine-tuning for deployments involving unlearning requests.",
        "pdf_link": "https://arxiv.org/pdf/2402.00751v1.pdf"
    },
    {
        "title": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
        "authors": [
            "Fan Huang",
            "Haewoon Kwak",
            "Jisun An"
        ],
        "published": "2024-02-17T02:25:57Z",
        "summary": "The robustness of AI-content detection models against cultivated attacks\n(e.g., paraphrasing or word switching) remains a significant concern. This\nstudy proposes a novel token-ensemble generation strategy to challenge the\nrobustness of current AI-content detection approaches. We explore the ensemble\nattack strategy by completing the prompt with the next token generated from\nrandom candidate LLMs. We find the token-ensemble approach significantly drops\nthe performance of AI-content detection models (The code and test sets will be\nreleased). Our findings reveal that token-ensemble generation poses a vital\nchallenge to current detection models and underlines the need for advancing\ndetection technologies to counter sophisticated adversarial strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.11167v1.pdf"
    },
    {
        "title": "Under the Surface: Tracking the Artifactuality of LLM-Generated Data",
        "authors": [
            "Debarati Das",
            "Karin De Langis",
            "Anna Martin-Boyle",
            "Jaehyung Kim",
            "Minhwa Lee",
            "Zae Myung Kim",
            "Shirley Anugrah Hayati",
            "Risako Owan",
            "Bin Hu",
            "Ritik Parkar",
            "Ryan Koo",
            "Jonginn Park",
            "Aahan Tyagi",
            "Libby Ferland",
            "Sanjali Roy",
            "Vincent Liu",
            "Dongyeop Kang"
        ],
        "published": "2024-01-26T07:53:27Z",
        "summary": "This work delves into the expanding role of large language models (LLMs) in\ngenerating artificial data. LLMs are increasingly employed to create a variety\nof outputs, including annotations, preferences, instruction prompts, simulated\ndialogues, and free text. As these forms of LLM-generated data often intersect\nin their application, they exert mutual influence on each other and raise\nsignificant concerns about the quality and diversity of the artificial data\nincorporated into training cycles, leading to an artificial data ecosystem. To\nthe best of our knowledge, this is the first study to aggregate various types\nof LLM-generated text data, from more tightly constrained data like \"task\nlabels\" to more lightly constrained \"free-form text\". We then stress test the\nquality and implications of LLM-generated artificial data, comparing it with\nhuman data across various existing benchmarks. Despite artificial data's\ncapability to match human performance, this paper reveals significant hidden\ndisparities, especially in complex tasks where LLMs often miss the nuanced\nunderstanding of intrinsic human-generated content. This study critically\nexamines diverse LLM-generated data and emphasizes the need for ethical\npractices in data creation and when using LLMs. It highlights the LLMs'\nshortcomings in replicating human traits and behaviors, underscoring the\nimportance of addressing biases and artifacts produced in LLM-generated content\nfor future research and development. All data and code are available on our\nproject page.",
        "pdf_link": "https://arxiv.org/pdf/2401.14698v2.pdf"
    },
    {
        "title": "On the Scaling Laws of Geographical Representation in Language Models",
        "authors": [
            "Nathan Godey",
            "\u00c9ric de la Clergerie",
            "Beno\u00eet Sagot"
        ],
        "published": "2024-02-29T18:04:11Z",
        "summary": "Language models have long been shown to embed geographical information in\ntheir hidden representations. This line of work has recently been revisited by\nextending this result to Large Language Models (LLMs). In this paper, we\npropose to fill the gap between well-established and recent literature by\nobserving how geographical knowledge evolves when scaling language models. We\nshow that geographical knowledge is observable even for tiny models, and that\nit scales consistently as we increase the model size. Notably, we observe that\nlarger language models cannot mitigate the geographical bias that is inherent\nto the training data.",
        "pdf_link": "https://arxiv.org/pdf/2402.19406v2.pdf"
    },
    {
        "title": "Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science",
        "authors": [
            "Pengfei Liu",
            "Jun Tao",
            "Zhixiang Ren"
        ],
        "published": "2024-02-06T16:12:36Z",
        "summary": "Efficient molecular modeling and design are crucial for the discovery and\nexploration of novel molecules, and the incorporation of deep learning methods\nhas revolutionized this field. In particular, large language models (LLMs)\noffer a fresh approach to tackle scientific problems from a natural language\nprocessing (NLP) perspective, introducing a research paradigm called scientific\nlanguage modeling (SLM). However, two key issues remain: how to quantify the\nmatch between model and data modalities and how to identify the\nknowledge-learning preferences of models. To address these challenges, we\npropose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263\nexperiments to assess the model's compatibility with data modalities and\nknowledge acquisition. Through the modal transition probability matrix, we\nprovide insights into the most suitable modalities for tasks. Furthermore, we\nintroduce a statistically interpretable approach to discover context-specific\nknowledge mapping by localized feature filtering. Our pioneering analysis\noffers an exploration of the learning mechanism and paves the way for advancing\nSLM in molecular science.",
        "pdf_link": "https://arxiv.org/pdf/2402.04119v1.pdf"
    },
    {
        "title": "How (un)ethical are instruction-centric responses of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries",
        "authors": [
            "Somnath Banerjee",
            "Sayan Layek",
            "Rima Hazra",
            "Animesh Mukherjee"
        ],
        "published": "2024-02-23T13:03:12Z",
        "summary": "In this study, we tackle a growing concern around the safety and ethical use\nof large language models (LLMs). Despite their potential, these models can be\ntricked into producing harmful or unethical content through various\nsophisticated methods, including 'jailbreaking' techniques and targeted\nmanipulation. Our work zeroes in on a specific issue: to what extent LLMs can\nbe led astray by asking them to generate responses that are instruction-centric\nsuch as a pseudocode, a program or a software snippet as opposed to vanilla\ntext. To investigate this question, we introduce TechHazardQA, a dataset\ncontaining complex queries which should be answered in both text and\ninstruction-centric formats (e.g., pseudocodes), aimed at identifying triggers\nfor unethical responses. We query a series of LLMs -- Llama-2-13b, Llama-2-7b,\nMistral-V2 and Mistral 8X7B -- and ask them to generate both text and\ninstruction-centric responses. For evaluation we report the harmfulness score\nmetric as well as judgements from GPT-4 and humans. Overall, we observe that\nasking LLMs to produce instruction-centric responses enhances the unethical\nresponse generation by ~2-38% across the models. As an additional objective, we\ninvestigate the impact of model editing using the ROME technique, which further\nincreases the propensity for generating undesirable content. In particular,\nasking edited LLMs to generate instruction-centric responses further increases\nthe unethical response generation by ~3-16% across the different models.",
        "pdf_link": "https://arxiv.org/pdf/2402.15302v4.pdf"
    },
    {
        "title": "Large Language Model for Mental Health: A Systematic Review",
        "authors": [
            "Zhijun Guo",
            "Alvina Lai",
            "Johan Hilge Thygesen",
            "Joseph Farrington",
            "Thomas Keen",
            "Kezhi Li"
        ],
        "published": "2024-02-19T17:58:41Z",
        "summary": "Large language models (LLMs) have received much attention and shown their\npotential in digital health, while their application in mental health is\nsubject to ongoing debate. This systematic review aims to summarize and\ncharacterize the use of LLMs in mental health by investigating the strengths\nand limitations of the latest work in LLMs and discusses the challenges and\nopportunities for early screening, digital interventions, and other clinical\napplications in mental health. Following PRISMA guidelines, we examined English\narticles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore,\npublished between 1 January 2017, and 1 September 2023, focusing on mental\nhealth and LLMs. The review analyzed 32 articles, including mental health\nanalysis using social media datasets (n=13), mental health chatbots (n=10), and\nother mental health applications (n=9). Findings reveal LLMs' effectiveness in\nmental health issue detection and the enhancement of telepsychological services\nthrough personalised healthcare. Nonetheless, risks like text inconsistencies,\nhallucinatory content, and the lack of an ethical framework raise concerns\nabout their clinical use. Despite these challenges, the advancement of LLMs\nunderscores their potential as innovative clinical tools, necessitating further\nresearch and development. The review emphasizes that LLMs should complement,\nnot replace, professional mental health services.",
        "pdf_link": "https://arxiv.org/pdf/2403.15401v1.pdf"
    },
    {
        "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models",
        "authors": [
            "Dheeraj Mekala",
            "Alex Nguyen",
            "Jingbo Shang"
        ],
        "published": "2024-02-16T03:39:37Z",
        "summary": "Instruction-tuning language models has become a crucial step in aligning them\nfor general use. Typically, this process involves extensive training on large\ndatasets, incurring high training costs. In this paper, we introduce a novel\ntraining data selection based on the learning percentage of the samples. We\nassert that current language models possess the capability to autonomously\nselect high-quality training data, leading to comparable or improved\nperformance compared to training on the entire dataset. Our experiments span\ndifferent-sized models, revealing that this characteristic holds for models\nranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an\ninteresting finding that the data hardness transfers across model sizes, and a\nsmaller 350M model can effectively curate high-quality training data with hard\nsamples for a larger 13B model, resulting in an equally or superior\ninstruction-tuned model compared to training on the complete dataset. Utilizing\nopen-sourced OPT and Llama-2 models up to 13B in size, two publicly available\ninstruction-tuning training datasets and evaluated by both automatic metrics &\nhumans, our paper introduces a novel approach to training data selection,\nshowcasing a more efficient alternative.",
        "pdf_link": "https://arxiv.org/pdf/2402.10430v1.pdf"
    },
    {
        "title": "Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes",
        "authors": [
            "Darren Liu",
            "Cheng Ding",
            "Delgersuren Bold",
            "Monique Bouvier",
            "Jiaying Lu",
            "Benjamin Shickel",
            "Craig S. Jabaley",
            "Wenhui Zhang",
            "Soojin Park",
            "Michael J. Young",
            "Mark S. Wainwright",
            "Gilles Clermont",
            "Parisa Rashidi",
            "Eric S. Rosenthal",
            "Laurie Dimisko",
            "Ran Xiao",
            "Joo Heung Yoon",
            "Carl Yang",
            "Xiao Hu"
        ],
        "published": "2024-01-24T16:52:37Z",
        "summary": "The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.13588v1.pdf"
    },
    {
        "title": "Large Language Models are Advanced Anonymizers",
        "authors": [
            "Robin Staab",
            "Mark Vero",
            "Mislav Balunovi\u0107",
            "Martin Vechev"
        ],
        "published": "2024-02-21T14:44:00Z",
        "summary": "Recent work in privacy research on large language models has shown that they\nachieve near human-level performance at inferring personal data from real-world\nonline texts. With consistently increasing model capabilities, existing text\nanonymization methods are currently lacking behind regulatory requirements and\nadversarial threats. This raises the question of how individuals can\neffectively protect their personal data in sharing online texts. In this work,\nwe take two steps to answer this question: We first present a new setting for\nevaluating anonymizations in the face of adversarial LLMs inferences, allowing\nfor a natural measurement of anonymization performance while remedying some of\nthe shortcomings of previous metrics. We then present our LLM-based adversarial\nanonymization framework leveraging the strong inferential capabilities of LLMs\nto inform our anonymization procedure. In our experimental evaluation, we show\non real-world and synthetic online texts how adversarial anonymization\noutperforms current industry-grade anonymizers both in terms of the resulting\nutility and privacy.",
        "pdf_link": "https://arxiv.org/pdf/2402.13846v1.pdf"
    },
    {
        "title": "Instruction Diversity Drives Generalization To Unseen Tasks",
        "authors": [
            "Dylan Zhang",
            "Justin Wang",
            "Francois Charton"
        ],
        "published": "2024-02-16T18:47:21Z",
        "summary": "Instruction tuning -- fine-tuning a large language model (LLM) on pairs of\ninstructions and desired outcomes -- is an approach that enables pre-trained\nlanguage models to perform real-world tasks and follow human instructions. Its\npractical success depends on the model learning a broader set of instructions\nthan those it was trained on. Yet the factors that determine model\ngeneralization to such \\emph{unseen tasks} are not well understood. %To\nunderstand the driving factors of generalization, In this paper, we experiment\nwith string rewrites, a symbolic task that serves as a building block for\nTuring complete Markov algorithms while allowing experimental control of\n\"inputs\" and \"instructions\". We investigate the trade-off between the number of\ninstructions the model is trained on and the number of training samples\nprovided for each instruction and observe that the diversity of the instruction\nset determines generalization. Generalization emerges once a diverse enough set\nof tasks is provided, even though very few examples are provided for each task.\nInstruction diversity also ensures robustness with respect to non-uniform\ndistributions of instructions in the training set.",
        "pdf_link": "https://arxiv.org/pdf/2402.10891v1.pdf"
    },
    {
        "title": "Large-Language-Model Empowered Dose Volume Histogram Prediction for Intensity Modulated Radiotherapy",
        "authors": [
            "Zehao Dong",
            "Yixin Chen",
            "Hiram Gay",
            "Yao Hao",
            "Geoffrey D. Hugo",
            "Pamela Samson",
            "Tianyu Zhao"
        ],
        "published": "2024-02-11T11:24:09Z",
        "summary": "Treatment planning is currently a patient specific, time-consuming, and\nresource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction\nplays a critical role in automating this process. The geometric relationship\nbetween DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target\nvolume (PTV) has been well established. This study explores the potential of\ndeep learning models for predicting DVHs using images and subsequent human\nintervention facilitated by a large-language model (LLM) to enhance the\nplanning quality. We propose a pipeline to convert unstructured images to a\nstructured graph consisting of image-patch nodes and dose nodes. A novel Dose\nGraph Neural Network (DoseGNN) model is developed for predicting DVHs from the\nstructured graph. The proposed DoseGNN is enhanced with the LLM to encode\nmassive knowledge from prescriptions and interactive instructions from\nclinicians. In this study, we introduced an online human-AI collaboration\n(OHAC) system as a practical implementation of the concept proposed for the\nautomation of intensity-modulated radiotherapy (IMRT) planning. In comparison\nto the widely-employed DL models used in radiotherapy, DoseGNN achieved mean\nsquare errors that were 80$\\%$, 76$\\%$ and 41.0$\\%$ of those predicted by Swin\nU-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the\nLLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans\nthrough interaction with clinicians using natural language.",
        "pdf_link": "https://arxiv.org/pdf/2402.07167v1.pdf"
    },
    {
        "title": "Improving Classification Performance With Human Feedback: Label a few, we label the rest",
        "authors": [
            "Natan Vidra",
            "Thomas Clifford",
            "Katherine Jijo",
            "Eden Chung",
            "Liang Zhang"
        ],
        "published": "2024-01-17T19:13:05Z",
        "summary": "In the realm of artificial intelligence, where a vast majority of data is\nunstructured, obtaining substantial amounts of labeled data to train supervised\nmachine learning models poses a significant challenge. To address this, we\ndelve into few-shot and active learning, where are goal is to improve AI models\nwith human feedback on a few labeled examples. This paper focuses on\nunderstanding how a continuous feedback loop can refine models, thereby\nenhancing their accuracy, recall, and precision through incremental human\ninput. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and\nSetFit, we aim to analyze the efficacy of using a limited number of labeled\nexamples to substantially improve model accuracy. We benchmark this approach on\nthe Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to\nprove that with just a few labeled examples, we are able to surpass the\naccuracy of zero shot large language models to provide enhanced text\nclassification performance. We demonstrate that rather than needing to manually\nlabel millions of rows of data, we just need to label a few and the model can\neffectively predict the rest.",
        "pdf_link": "https://arxiv.org/pdf/2401.09555v1.pdf"
    },
    {
        "title": "DELL: Generating Reactions and Explanations for LLM-Based Misinformation Detection",
        "authors": [
            "Herun Wan",
            "Shangbin Feng",
            "Zhaoxuan Tan",
            "Heng Wang",
            "Yulia Tsvetkov",
            "Minnan Luo"
        ],
        "published": "2024-02-16T03:24:56Z",
        "summary": "Large language models are limited by challenges in factuality and\nhallucinations to be directly employed off-the-shelf for judging the veracity\nof news articles, where factual accuracy is paramount. In this work, we propose\nDELL that identifies three key stages in misinformation detection where LLMs\ncould be incorporated as part of the pipeline: 1) LLMs could \\emph{generate\nnews reactions} to represent diverse perspectives and simulate user-news\ninteraction networks; 2) LLMs could \\emph{generate explanations} for proxy\ntasks (e.g., sentiment, stance) to enrich the contexts of news articles and\nproduce experts specializing in various aspects of news understanding; 3) LLMs\ncould \\emph{merge task-specific experts} and provide an overall prediction by\nincorporating the predictions and confidence scores of varying experts.\nExtensive experiments on seven datasets with three LLMs demonstrate that DELL\noutperforms state-of-the-art baselines by up to 16.8\\% in macro f1-score.\nFurther analysis reveals that the generated reactions and explanations are\ngreatly helpful in misinformation detection, while our proposed LLM-guided\nexpert merging helps produce better-calibrated predictions.",
        "pdf_link": "https://arxiv.org/pdf/2402.10426v1.pdf"
    },
    {
        "title": "Characteristic AI Agents via Large Language Models",
        "authors": [
            "Xi Wang",
            "Hongliang Dai",
            "Shen Gao",
            "Piji Li"
        ],
        "published": "2024-03-19T02:25:29Z",
        "summary": "The advancement of Large Language Models (LLMs) has led to significant\nenhancements in the performance of chatbot systems. Many researchers have\ndedicated their efforts to the development of bringing characteristics to\nchatbots. While there have been commercial products for developing role-driven\nchatbots using LLMs, it is worth noting that academic research in this area\nremains relatively scarce. Our research focuses on investigating the\nperformance of LLMs in constructing Characteristic AI Agents by simulating\nreal-life individuals across different settings. Current investigations have\nprimarily focused on act on roles with simple profiles. In response to this\nresearch gap, we create a benchmark for the characteristic AI agents task,\nincluding dataset, techniques, and evaluation metrics. A dataset called\n``Character100'' is built for this benchmark, comprising the most-visited\npeople on Wikipedia for language models to role-play. With the constructed\ndataset, we conduct comprehensive assessment of LLMs across various settings.\nIn addition, we devise a set of automatic metrics for quantitative performance\nevaluation. The experimental results underscore the potential directions for\nfurther improvement in the capabilities of LLMs in constructing characteristic\nAI agents. The benchmark is available at\nhttps://github.com/nuaa-nlp/Character100.",
        "pdf_link": "https://arxiv.org/pdf/2403.12368v1.pdf"
    },
    {
        "title": "LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems",
        "authors": [
            "Tasnim Ahmed",
            "Salimur Choudhury"
        ],
        "published": "2024-03-02T23:32:33Z",
        "summary": "In the rapidly evolving field of natural language processing, the translation\nof linguistic descriptions into mathematical formulation of optimization\nproblems presents a formidable challenge, demanding intricate understanding and\nprocessing capabilities from Large Language Models (LLMs). This study compares\nprominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and\none-shot settings for this task. Our findings show GPT-4's superior\nperformance, particularly in the one-shot scenario. A central part of this\nresearch is the introduction of `LM4OPT,' a progressive fine-tuning framework\nfor Llama-2-7b that utilizes noisy embeddings and specialized datasets.\nHowever, this research highlights a notable gap in the contextual understanding\ncapabilities of smaller models such as Llama-2-7b compared to larger\ncounterparts, especially in processing lengthy and complex input contexts. Our\nempirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4\nsurpasses the baseline performance established by previous research, achieving\nan F1-score of 0.63, solely based on the problem description in natural\nlanguage, and without relying on any additional named entity information.\nGPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These\nfindings not only benchmark the current capabilities of LLMs in a novel\napplication area but also lay the groundwork for future improvements in\nmathematical formulation of optimization problems from natural language input.",
        "pdf_link": "https://arxiv.org/pdf/2403.01342v1.pdf"
    },
    {
        "title": "Variational Learning is Effective for Large Deep Networks",
        "authors": [
            "Yuesong Shen",
            "Nico Daheim",
            "Bai Cong",
            "Peter Nickl",
            "Gian Maria Marconi",
            "Clement Bazan",
            "Rio Yokota",
            "Iryna Gurevych",
            "Daniel Cremers",
            "Mohammad Emtiyaz Khan",
            "Thomas M\u00f6llenhoff"
        ],
        "published": "2024-02-27T16:11:05Z",
        "summary": "We give extensive empirical evidence against the common belief that\nvariational learning is ineffective for large neural networks. We show that an\noptimizer called Improved Variational Online Newton (IVON) consistently matches\nor outperforms Adam for training large networks such as GPT-2 and ResNets from\nscratch. IVON's computational costs are nearly identical to Adam but its\npredictive uncertainty is better. We show several new use cases of IVON where\nwe improve fine-tuning and model merging in Large Language Models, accurately\npredict generalization error, and faithfully estimate sensitivity to data. We\nfind overwhelming evidence in support of effectiveness of variational learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.17641v1.pdf"
    },
    {
        "title": "Entity Recognition from Colloquial Text",
        "authors": [
            "Tamara Babaian",
            "Jennifer Xu"
        ],
        "published": "2024-01-09T23:52:32Z",
        "summary": "Extraction of concepts and entities of interest from non-formal texts such as\nsocial media posts and informal communication is an important capability for\ndecision support systems in many domains, including healthcare, customer\nrelationship management, and others. Despite the recent advances in training\nlarge language models for a variety of natural language processing tasks, the\ndeveloped models and techniques have mainly focused on formal texts and do not\nperform as well on colloquial data, which is characterized by a number of\ndistinct challenges. In our research, we focus on the healthcare domain and\ninvestigate the problem of symptom recognition from colloquial texts by\ndesigning and evaluating several training strategies for BERT-based model\nfine-tuning. These strategies are distinguished by the choice of the base\nmodel, the training corpora, and application of term perturbations in the\ntraining data. The best-performing models trained using these strategies\noutperform the state-of-the-art specialized symptom recognizer by a large\nmargin. Through a series of experiments, we have found specific patterns of\nmodel behavior associated with the training strategies we designed. We present\ndesign principles for training strategies for effective entity recognition in\ncolloquial texts based on our findings.",
        "pdf_link": "https://arxiv.org/pdf/2401.04853v1.pdf"
    },
    {
        "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
        "authors": [
            "Xinrong Zhang",
            "Yingfa Chen",
            "Shengding Hu",
            "Zihang Xu",
            "Junhao Chen",
            "Moo Khai Hao",
            "Xu Han",
            "Zhen Leng Thai",
            "Shuo Wang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-21T11:30:29Z",
        "summary": "Processing and reasoning over long contexts is crucial for many practical\napplications of Large Language Models (LLMs), such as document comprehension\nand agent construction. Despite recent strides in making LLMs process contexts\nwith more than 100K tokens, there is currently a lack of a standardized\nbenchmark to evaluate this long-context capability. Existing public benchmarks\ntypically focus on contexts around 10K tokens, limiting the assessment and\ncomparison of LLMs in processing longer contexts. In this paper, we propose\n$\\infty$Bench, the first LLM benchmark featuring an average data length\nsurpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks\nspanning diverse domains, presented in both English and Chinese. The tasks in\n$\\infty$Bench are designed to require well understanding of long dependencies\nin contexts, and make simply retrieving a limited number of passages from\ncontexts not sufficient for these tasks. In our experiments, based on\n$\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source\nLLMs tailored for processing long contexts. The results indicate that existing\nlong context LLMs still require significant advancements to effectively process\n100K+ context. We further present three intriguing analyses regarding the\nbehavior of LLMs processing long context.",
        "pdf_link": "https://arxiv.org/pdf/2402.13718v3.pdf"
    },
    {
        "title": "Generalist embedding models are better at short-context clinical semantic search than specialized embedding models",
        "authors": [
            "Jean-Baptiste Excoffier",
            "Tom Roehr",
            "Alexei Figueroa",
            "Jens-Michalis Papaioannou",
            "Keno Bressem",
            "Matthieu Ortala"
        ],
        "published": "2024-01-03T19:03:32Z",
        "summary": "The increasing use of tools and solutions based on Large Language Models\n(LLMs) for various tasks in the medical domain has become a prominent trend.\nTheir use in this highly critical and sensitive domain has thus raised\nimportant questions about their robustness, especially in response to\nvariations in input, and the reliability of the generated outputs. This study\naddresses these questions by constructing a textual dataset based on the\nICD-10-CM code descriptions, widely used in US hospitals and containing many\nclinical terms, and their easily reproducible rephrasing. We then benchmarked\nexisting embedding models, either generalist or specialized in the clinical\ndomain, in a semantic search task where the goal was to correctly match the\nrephrased text to the original description. Our results showed that generalist\nmodels performed better than clinical models, suggesting that existing clinical\nspecialized models are more sensitive to small changes in input that confuse\nthem. The highlighted problem of specialized models may be due to the fact that\nthey have not been trained on sufficient data, and in particular on datasets\nthat are not diverse enough to have a reliable global language understanding,\nwhich is still necessary for accurate handling of medical documents.",
        "pdf_link": "https://arxiv.org/pdf/2401.01943v2.pdf"
    },
    {
        "title": "A Multi-Agent Conversational Recommender System",
        "authors": [
            "Jiabao Fang",
            "Shen Gao",
            "Pengjie Ren",
            "Xiuying Chen",
            "Suzan Verberne",
            "Zhaochun Ren"
        ],
        "published": "2024-02-02T04:20:13Z",
        "summary": "Due to strong capabilities in conducting fluent, multi-turn conversations\nwith users, Large Language Models (LLMs) have the potential to further improve\nthe performance of Conversational Recommender System (CRS). Unlike the aimless\nchit-chat that LLM excels at, CRS has a clear target. So it is imperative to\ncontrol the dialogue flow in the LLM to successfully recommend appropriate\nitems to the users. Furthermore, user feedback in CRS can assist the system in\nbetter modeling user preferences, which has been ignored by existing studies.\nHowever, simply prompting LLM to conduct conversational recommendation cannot\naddress the above two key challenges.\n  In this paper, we propose Multi-Agent Conversational Recommender System\n(MACRS) which contains two essential modules. First, we design a multi-agent\nact planning framework, which can control the dialogue flow based on four\nLLM-based agents. This cooperative multi-agent framework will generate various\ncandidate responses based on different dialogue acts and then choose the most\nappropriate response as the system response, which can help MACRS plan suitable\ndialogue acts. Second, we propose a user feedback-aware reflection mechanism\nwhich leverages user feedback to reason errors made in previous turns to adjust\nthe dialogue act planning, and higher-level user information from implicit\nsemantics. We conduct extensive experiments based on user simulator to\ndemonstrate the effectiveness of MACRS in recommendation and user preferences\ncollection. Experimental results illustrate that MACRS demonstrates an\nimprovement in user interaction experience compared to directly using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01135v1.pdf"
    },
    {
        "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
        "authors": [
            "Wei Huang",
            "Yangdong Liu",
            "Haotong Qin",
            "Ying Li",
            "Shiming Zhang",
            "Xianglong Liu",
            "Michele Magno",
            "Xiaojuan Qi"
        ],
        "published": "2024-02-06T09:26:34Z",
        "summary": "Pretrained large language models (LLMs) exhibit exceptional general language\nprocessing capabilities but come with significant demands on memory and\ncomputational resources. As a powerful compression technology, binarization can\nextremely reduce model weights to a mere 1 bit, lowering the expensive\ncomputation and memory requirements. However, existing quantization techniques\nfall short of maintaining LLM performance under ultra-low bit-widths. In\nresponse to this challenge, we present BiLLM, a groundbreaking 1-bit\npost-training quantization scheme tailored for pretrained LLMs. Based on the\nweight distribution of LLMs, BiLLM first identifies and structurally selects\nsalient weights, and minimizes the compression loss through an effective binary\nresidual approximation strategy. Moreover, considering the bell-shaped\ndistribution of the non-salient weights, we propose an optimal splitting search\nto group and binarize them accurately. BiLLM achieving for the first time\nhigh-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit\nweights across various LLMs families and evaluation metrics, outperforms SOTA\nquantization methods of LLM by significant margins. Moreover, BiLLM enables the\nbinarization process of the LLM with 7 billion weights within 0.5 hours on a\nsingle GPU, demonstrating satisfactory time efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.04291v1.pdf"
    },
    {
        "title": "Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives",
        "authors": [
            "Runcong Zhao",
            "Qinglin Zhu",
            "Hainiu Xu",
            "Jiazheng Li",
            "Yuxiang Zhou",
            "Yulan He",
            "Lin Gui"
        ],
        "published": "2024-02-16T19:59:45Z",
        "summary": "Existing datasets for narrative understanding often fail to represent the\ncomplexity and uncertainty of relationships in real-life social scenarios. To\naddress this gap, we introduce a new benchmark, Conan, designed for extracting\nand analysing intricate character relation graphs from detective narratives.\nSpecifically, we designed hierarchical relationship categories and manually\nextracted and annotated role-oriented relationships from the perspectives of\nvarious characters, incorporating both public relationships known to most\ncharacters and secret ones known to only a few. Our experiments with advanced\nLarge Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their\nlimitations in inferencing complex relationships and handling longer\nnarratives. The combination of the Conan dataset and our pipeline strategy is\ngeared towards understanding the ability of LLMs to comprehend nuanced\nrelational dynamics in narrative contexts.",
        "pdf_link": "https://arxiv.org/pdf/2402.11051v1.pdf"
    },
    {
        "title": "JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset",
        "authors": [
            "Atsumoto Ohashi",
            "Ryu Hirai",
            "Shinya Iizuka",
            "Ryuichiro Higashinaka"
        ],
        "published": "2024-03-26T02:01:18Z",
        "summary": "Dialogue datasets are crucial for deep learning-based task-oriented dialogue\nsystem research. While numerous English language multi-domain task-oriented\ndialogue datasets have been developed and contributed to significant\nadvancements in task-oriented dialogue systems, such a dataset does not exist\nin Japanese, and research in this area is limited compared to that in English.\nIn this study, towards the advancement of research and development of\ntask-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first\nJapanese language large-scale multi-domain task-oriented dialogue dataset.\nUsing JMultiWOZ, we evaluated the dialogue state tracking and response\ngeneration capabilities of the state-of-the-art methods on the existing major\nEnglish benchmark dataset MultiWOZ2.2 and the latest large language model\n(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ\nprovides a benchmark that is on par with MultiWOZ2.2. In addition, through\nevaluation experiments of interactive dialogues with the models and human\nparticipants, we identified limitations in the task completion capabilities of\nLLMs in Japanese.",
        "pdf_link": "https://arxiv.org/pdf/2403.17319v1.pdf"
    },
    {
        "title": "Learning to Plan for Language Modeling from Unlabeled Data",
        "authors": [
            "Nathan Cornille",
            "Marie-Francine Moens",
            "Florian Mai"
        ],
        "published": "2024-03-31T09:04:01Z",
        "summary": "By training to predict the next token in an unlabeled corpus, large language\nmodels learn to perform many tasks without any labeled data. However, their\nnext-token-prediction objective arguably limits their performance in scenarios\nthat require planning, such as writing a coherent article. In this paper, we\ntrain a module for planning the future writing process via a self-supervised\nlearning objective. By conditioning on generated latent plans, our model\nextends the successful language model formula to more abstract planning in an\nunsupervised way. Empirically, we demonstrate that our method improves language\nmodeling performance in general, particularly with respect to the text\nstructure. Because our framework uses a planner module that is unsupervised and\nexternal to the language model, new planner modules can be trained at large\nscale and easily be shared with the community.",
        "pdf_link": "https://arxiv.org/pdf/2404.00614v1.pdf"
    },
    {
        "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
        "authors": [
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Zhengwen Zhang",
            "Xiangrui Meng",
            "Sirui Hong",
            "Wenhao Li",
            "Zihao Wang",
            "Zekai Wang",
            "Feng Yin",
            "Junhua Zhao",
            "Xiuqiang He"
        ],
        "published": "2024-01-07T09:08:24Z",
        "summary": "Intelligent agents stand out as a potential path toward artificial general\nintelligence (AGI). Thus, researchers have dedicated significant effort to\ndiverse implementations for them. Benefiting from recent progress in large\nlanguage models (LLMs), LLM-based agents that use universal natural language as\nan interface exhibit robust generalization capabilities across various\napplications -- from serving as autonomous general-purpose task assistants to\napplications in coding, social, and economic domains, LLM-based agents offer\nextensive exploration opportunities. This paper surveys current research to\nprovide an in-depth overview of LLM-based intelligent agents within\nsingle-agent and multi-agent systems. It covers their definitions, research\nframeworks, and foundational components such as their composition, cognitive\nand planning methods, tool utilization, and responses to environmental\nfeedback. We also delve into the mechanisms of deploying LLM-based agents in\nmulti-agent systems, including multi-role collaboration, message passing, and\nstrategies to alleviate communication issues between agents. The discussions\nalso shed light on popular datasets and application scenarios. We conclude by\nenvisioning prospects for LLM-based agents, considering the evolving landscape\nof AI and natural language processing.",
        "pdf_link": "https://arxiv.org/pdf/2401.03428v1.pdf"
    },
    {
        "title": "Towards Safe and Aligned Large Language Models for Medicine",
        "authors": [
            "Tessa Han",
            "Aounon Kumar",
            "Chirag Agarwal",
            "Himabindu Lakkaraju"
        ],
        "published": "2024-03-06T14:34:07Z",
        "summary": "The capabilities of large language models (LLMs) have been progressing at a\nbreathtaking speed, leaving even their own developers grappling with the depth\nof their potential and risks. While initial steps have been taken to evaluate\nthe safety and alignment of general-knowledge LLMs, exposing some weaknesses,\nto our knowledge, the safety and alignment of medical LLMs has not been\nevaluated despite their risks for personal health and safety, public health and\nsafety, and human rights. To this end, we carry out the first safety evaluation\nfor medical LLMs. Specifically, we set forth a definition of medical safety and\nalignment for medical artificial intelligence systems, develop a dataset of\nharmful medical questions to evaluate the medical safety and alignment of an\nLLM, evaluate both general and medical safety and alignment of medical LLMs,\ndemonstrate fine-tuning as an effective mitigation strategy, and discuss\nbroader, large-scale approaches used by the machine learning community to\ndevelop safe and aligned LLMs. We hope that this work casts light on the safety\nand alignment of medical LLMs and motivates future work to study it and develop\nadditional mitigation strategies, minimizing the risks of harm of LLMs in\nmedicine.",
        "pdf_link": "https://arxiv.org/pdf/2403.03744v1.pdf"
    },
    {
        "title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction",
        "authors": [
            "Hejie Cui",
            "Zhuocheng Shen",
            "Jieyu Zhang",
            "Hui Shao",
            "Lianhui Qin",
            "Joyce C. Ho",
            "Carl Yang"
        ],
        "published": "2024-03-19T18:10:13Z",
        "summary": "Electronic health records (EHRs) contain valuable patient data for\nhealth-related prediction tasks, such as disease prediction. Traditional\napproaches rely on supervised learning methods that require large labeled\ndatasets, which can be expensive and challenging to obtain. In this study, we\ninvestigate the feasibility of applying Large Language Models (LLMs) to convert\nstructured patient visit data (e.g., diagnoses, labs, prescriptions) into\nnatural language narratives. We evaluate the zero-shot and few-shot performance\nof LLMs using various EHR-prediction-oriented prompting strategies.\nFurthermore, we propose a novel approach that utilizes LLM agents with\ndifferent roles: a predictor agent that makes predictions and generates\nreasoning processes and a critic agent that analyzes incorrect predictions and\nprovides guidance for improving the reasoning of the predictor agent. Our\nresults demonstrate that with the proposed approach, LLMs can achieve decent\nfew-shot performance compared to traditional supervised learning methods in\nEHR-based disease predictions, suggesting its potential for health-oriented\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2403.15464v1.pdf"
    },
    {
        "title": "Can Large Language Models do Analytical Reasoning?",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Dong Yu",
            "Fei Liu"
        ],
        "published": "2024-03-06T20:22:08Z",
        "summary": "This paper explores the cutting-edge Large Language Model with analytical\nreasoning on sports. Our analytical reasoning embodies the tasks of letting\nlarge language models count how many points each team scores in a quarter in\nthe NBA and NFL games. Our major discoveries are in two folds. Firstly, we find\namong all the models we employed, GPT-4 stands out in effectiveness, followed\nby Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.\nSpecifically, we compare three different prompting techniques and a\ndivide-and-conquer approach, we find that the latter was the most effective.\nOur divide-and-conquer approach breaks down play-by-play data into smaller,\nmore manageable segments, solves each piece individually, and then aggregates\nthem together. Besides the divide-and-conquer approach, we also explore the\nChain of Thought (CoT) strategy, which markedly improves outcomes for certain\nmodels, notably GPT-4 and Claude-2.1, with their accuracy rates increasing\nsignificantly. However, the CoT strategy has negligible or even detrimental\neffects on the performance of other models like GPT-3.5 and Gemini-Pro.\nSecondly, to our surprise, we observe that most models, including GPT-4,\nstruggle to accurately count the total scores for NBA quarters despite showing\nstrong performance in counting NFL quarter scores. This leads us to further\ninvestigate the factors that impact the complexity of analytical reasoning\ntasks with extensive experiments, through which we conclude that task\ncomplexity depends on the length of context, the information density, and the\npresence of related information. Our research provides valuable insights into\nthe complexity of analytical reasoning tasks and potential directions for\ndeveloping future large language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.04031v1.pdf"
    },
    {
        "title": "Distilling Large Language Models for Text-Attributed Graph Learning",
        "authors": [
            "Bo Pan",
            "Zheng Zhang",
            "Yifei Zhang",
            "Yuntong Hu",
            "Liang Zhao"
        ],
        "published": "2024-02-19T10:31:53Z",
        "summary": "Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.12022v1.pdf"
    },
    {
        "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
        "authors": [
            "Chufan Shi",
            "Haoran Yang",
            "Deng Cai",
            "Zhisong Zhang",
            "Yifan Wang",
            "Yujiu Yang",
            "Wai Lam"
        ],
        "published": "2024-02-10T11:14:53Z",
        "summary": "Decoding methods play an indispensable role in converting language models\nfrom next-token predictors into practical task solvers. Prior research on\ndecoding methods, primarily focusing on task-specific models, may not extend to\nthe current era of general-purpose large language models (LLMs). Moreover, the\nrecent influx of decoding strategies has further complicated this landscape.\nThis paper provides a comprehensive and multifaceted analysis of various\ndecoding methods within the context of LLMs, evaluating their performance,\nrobustness to hyperparameter changes, and decoding speeds across a wide range\nof tasks, models, and deployment environments. Our findings reveal that\ndecoding method performance is notably task-dependent and influenced by factors\nsuch as alignment, model size, and quantization. Intriguingly, sensitivity\nanalysis exposes that certain methods achieve superior performance at the cost\nof extensive hyperparameter tuning, highlighting the trade-off between\nattaining optimal results and the practicality of implementation in varying\ncontexts.",
        "pdf_link": "https://arxiv.org/pdf/2402.06925v1.pdf"
    },
    {
        "title": "How do Large Language Models Handle Multilingualism?",
        "authors": [
            "Yiran Zhao",
            "Wenxuan Zhang",
            "Guizhen Chen",
            "Kenji Kawaguchi",
            "Lidong Bing"
        ],
        "published": "2024-02-29T02:55:26Z",
        "summary": "Large language models (LLMs) demonstrate remarkable performance across a\nspectrum of languages. In this work, we delve into the question: How do LLMs\nhandle multilingualism? We introduce a framework that depicts LLMs' processing\nof multilingual inputs: In the first several layers, LLMs understand the\nquestion, converting multilingual inputs into English to facilitate the\ntask-solving phase. In the intermediate layers, LLMs engage in problem-solving\nby thinking in English and incorporating multilingual knowledge to obtain\nfactual content, leveraging the self-attention and feed-forward structures,\nrespectively. In the last several layers, LLMs generate responses that align\nwith the original language of the query. In addition, we investigate the\nexistence of language-specific neurons when processing a certain language. To\ndetect neurons activated by the input language, even without labels, we\ninnovatively design a Parallel Language specific Neuron Detection\n($\\texttt{PLND}$) method that effectively measures the significance of neurons\nwhen handling multilingual inputs. By comprehensive ablation analysis through\ndeactivating neurons of different layers and structures, we verify the\nframework that we propose. Additionally, we demonstrate that we can utilize\nsuch a framework to effectively enhance the multilingual ability with much less\ntraining effort.",
        "pdf_link": "https://arxiv.org/pdf/2402.18815v1.pdf"
    },
    {
        "title": "Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles",
        "authors": [
            "Maram Hasanain",
            "Fatema Ahmed",
            "Firoj Alam"
        ],
        "published": "2024-02-27T13:02:19Z",
        "summary": "The use of propaganda has spiked on mainstream and social media, aiming to\nmanipulate or mislead users. While efforts to automatically detect propaganda\ntechniques in textual, visual, or multimodal content have increased, most of\nthem primarily focus on English content. The majority of the recent initiatives\ntargeting medium to low-resource languages produced relatively small annotated\ndatasets, with a skewed distribution, posing challenges for the development of\nsophisticated propaganda detection models. To address this challenge, we\ncarefully develop the largest propaganda dataset to date, ArPro, comprised of\n8K paragraphs from newspaper articles, labeled at the text span level following\na taxonomy of 23 propagandistic techniques. Furthermore, our work offers the\nfirst attempt to understand the performance of large language models (LLMs),\nusing GPT-4, for fine-grained propaganda detection from text. Results showed\nthat GPT-4's performance degrades as the task moves from simply classifying a\nparagraph as propagandistic or not, to the fine-grained task of detecting\npropaganda techniques and their manifestation in text. Compared to models\nfine-tuned on the dataset for propaganda detection at different classification\ngranularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a\ndataset consisting of six other languages for span detection, and results\nsuggest that the model struggles with the task across languages. Our dataset\nand resources will be released to the community.",
        "pdf_link": "https://arxiv.org/pdf/2402.17478v1.pdf"
    },
    {
        "title": "From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers",
        "authors": [
            "M. Emrullah Ildiz",
            "Yixiao Huang",
            "Yingcong Li",
            "Ankit Singh Rawat",
            "Samet Oymak"
        ],
        "published": "2024-02-21T03:51:34Z",
        "summary": "Modern language models rely on the transformer architecture and attention\nmechanism to perform language understanding and text generation. In this work,\nwe study learning a 1-layer self-attention model from a set of prompts and\nassociated output data sampled from the model. We first establish a precise\nmapping between the self-attention mechanism and Markov models: Inputting a\nprompt to the model samples the output token according to a context-conditioned\nMarkov chain (CCMC) which weights the transition matrix of a base Markov chain.\nAdditionally, incorporating positional encoding results in position-dependent\nscaling of the transition probabilities. Building on this formalism, we develop\nidentifiability/coverage conditions for the prompt distribution that guarantee\nconsistent estimation and establish sample complexity guarantees under IID\nsamples. Finally, we study the problem of learning from a single output\ntrajectory generated from an initial prompt. We characterize an intriguing\nwinner-takes-all phenomenon where the generative process implemented by\nself-attention collapses into sampling a limited subset of tokens due to its\nnon-mixing nature. This provides a mathematical explanation to the tendency of\nmodern LLMs to generate repetitive text. In summary, the equivalence to CCMC\nprovides a simple but powerful framework to study self-attention and its\nproperties.",
        "pdf_link": "https://arxiv.org/pdf/2402.13512v1.pdf"
    },
    {
        "title": "DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation",
        "authors": [
            "Guosheng Zhao",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Xinze Chen",
            "Guan Huang",
            "Xiaoyi Bao",
            "Xingang Wang"
        ],
        "published": "2024-03-11T16:03:35Z",
        "summary": "World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.",
        "pdf_link": "https://arxiv.org/pdf/2403.06845v1.pdf"
    },
    {
        "title": "Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs",
        "authors": [
            "Haritz Puerto",
            "Martin Tutek",
            "Somak Aditya",
            "Xiaodan Zhu",
            "Iryna Gurevych"
        ],
        "published": "2024-01-18T15:32:24Z",
        "summary": "Reasoning is a fundamental component of language understanding. Recent\nprompting techniques, such as chain of thought, have consistently improved\nLLMs' performance on various reasoning tasks. Nevertheless, there is still\nlittle understanding of what triggers reasoning abilities in LLMs in the\ninference stage. In this paper, we introduce code prompting, a chain of prompts\nthat transforms a natural language problem into code and directly prompts the\nLLM using the generated code without resorting to external code execution. We\nhypothesize that code prompts can elicit certain reasoning capabilities of LLMs\ntrained on text and code and utilize the proposed method to improve conditional\nreasoning, the ability to infer different conclusions depending on the\nfulfillment of certain conditions. We find that code prompting exhibits a\nhigh-performance boost for multiple LLMs (up to 22.52 percentage points on GPT\n3.5, 7.75 on Mixtral, and 16.78 on Mistral) across multiple conditional\nreasoning datasets. We then conduct comprehensive experiments to understand how\ncode prompts trigger reasoning abilities and which capabilities are elicited in\nthe underlying models. Our analysis of GPT 3.5 reveals that the code formatting\nof the input problem is essential for performance improvement. Furthermore,\ncode prompts improve sample efficiency of in-context learning and facilitate\nstate tracking of variables or entities.",
        "pdf_link": "https://arxiv.org/pdf/2401.10065v2.pdf"
    },
    {
        "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
        "authors": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Yuze He",
            "Ji Qi",
            "Lei Hou",
            "Jie Tang",
            "Yuxiao Dong",
            "Juanzi Li"
        ],
        "published": "2024-01-31T18:29:39Z",
        "summary": "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
        "pdf_link": "https://arxiv.org/pdf/2401.18058v1.pdf"
    },
    {
        "title": "Exploring Value Biases: How LLMs Deviate Towards the Ideal",
        "authors": [
            "Sarath Sivaprasad",
            "Pramod Kaushik",
            "Sahar Abdelnabi",
            "Mario Fritz"
        ],
        "published": "2024-02-16T18:28:43Z",
        "summary": "Large-Language-Models (LLMs) are deployed in a wide range of applications,\nand their response has an increasing social impact. Understanding the\nnon-deliberate(ive) mechanism of LLMs in giving responses is essential in\nexplaining their performance and discerning their biases in real-world\napplications. This is analogous to human studies, where such inadvertent\nresponses are referred to as sampling. We study this sampling of LLMs in light\nof value bias and show that the sampling of LLMs tends to favour high-value\noptions. Value bias corresponds to this shift of response from the most likely\ntowards an ideal value represented in the LLM. In fact, this effect can be\nreproduced even with new entities learnt via in-context prompting. We show that\nthis bias manifests in unexpected places and has implications on relevant\napplication scenarios, like choosing exemplars. The results show that value\nbias is strong in LLMs across different categories, similar to the results\nfound in human studies.",
        "pdf_link": "https://arxiv.org/pdf/2402.11005v2.pdf"
    },
    {
        "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact",
        "authors": [
            "Ruikang Liu",
            "Haoli Bai",
            "Haokun Lin",
            "Yuening Li",
            "Han Gao",
            "Zhengzhuo Xu",
            "Lu Hou",
            "Jun Yao",
            "Chun Yuan"
        ],
        "published": "2024-03-02T16:05:26Z",
        "summary": "Large language models (LLMs) excel in natural language processing but demand\nintensive computation. To mitigate this, various quantization methods have been\nexplored, yet they compromise LLM performance. This paper unveils a previously\noverlooked type of outlier in LLMs. Such outliers are found to allocate most of\nthe attention scores on initial tokens of input, termed as pivot tokens, which\nis crucial to the performance of quantized LLMs. Given that, we propose\nIntactKV to generate the KV cache of pivot tokens losslessly from the\nfull-precision model. The approach is simple and easy to combine with existing\nquantization solutions. Besides, IntactKV can be calibrated as additional LLM\nparameters to boost the quantized LLMs further. Mathematical analysis also\nproves that IntactKV effectively reduces the upper bound of quantization error.\nEmpirical results show that IntactKV brings consistent improvement and achieves\nlossless weight-only INT4 quantization on various downstream tasks, leading to\nthe new state-of-the-art for LLM quantization.",
        "pdf_link": "https://arxiv.org/pdf/2403.01241v1.pdf"
    },
    {
        "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
        "authors": [
            "Sahand Sabour",
            "Siyang Liu",
            "Zheyuan Zhang",
            "June M. Liu",
            "Jinfeng Zhou",
            "Alvionna S. Sunaryo",
            "Juanzi Li",
            "Tatia M. C. Lee",
            "Rada Mihalcea",
            "Minlie Huang"
        ],
        "published": "2024-02-19T11:48:09Z",
        "summary": "Recent advances in Large Language Models (LLMs) have highlighted the need for\nrobust, comprehensive, and challenging benchmarks. Yet, research on evaluating\ntheir Emotional Intelligence (EI) is considerably limited. Existing benchmarks\nhave two major shortcomings: first, they mainly focus on emotion recognition,\nneglecting essential EI capabilities such as emotion regulation and thought\nfacilitation through emotion understanding; second, they are primarily\nconstructed from existing datasets, which include frequent patterns, explicit\ninformation, and annotation errors, leading to unreliable evaluation. We\npropose EmoBench, a benchmark that draws upon established psychological\ntheories and proposes a comprehensive definition for machine EI, including\nEmotional Understanding and Emotional Application. EmoBench includes a set of\n400 hand-crafted questions in English and Chinese, which are meticulously\ndesigned to require thorough reasoning and understanding. Our findings reveal a\nconsiderable gap between the EI of existing LLMs and the average human,\nhighlighting a promising direction for future research. Our code and data will\nbe publicly available from https://github.com/Sahandfer/EmoBench.",
        "pdf_link": "https://arxiv.org/pdf/2402.12071v1.pdf"
    },
    {
        "title": "CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs",
        "authors": [
            "Majeed Kazemitabaar",
            "Runlong Ye",
            "Xiaoning Wang",
            "Austin Z. Henley",
            "Paul Denny",
            "Michelle Craig",
            "Tovi Grossman"
        ],
        "published": "2024-01-20T20:14:42Z",
        "summary": "Timely, personalized feedback is essential for students learning programming.\nLLM-powered tools like ChatGPT offer instant support, but reveal direct answers\nwith code, which may hinder deep conceptual engagement. We developed CodeAid,\nan LLM-powered programming assistant delivering helpful, technically correct\nresponses, without revealing code solutions. CodeAid answers conceptual\nquestions, generates pseudo-code with line-by-line explanations, and annotates\nstudent's incorrect code with fix suggestions. We deployed CodeAid in a\nprogramming class of 700 students for a 12-week semester. A thematic analysis\nof 8,000 usages of CodeAid was performed, further enriched by weekly surveys,\nand 22 student interviews. We then interviewed eight programming educators to\ngain further insights. Our findings reveal four design considerations for\nfuture educational AI assistants: D1) exploiting AI's unique benefits; D2)\nsimplifying query formulation while promoting cognitive engagement; D3)\navoiding direct responses while encouraging motivated learning; and D4)\nmaintaining transparency and control for students to asses and steer AI\nresponses.",
        "pdf_link": "https://arxiv.org/pdf/2401.11314v2.pdf"
    },
    {
        "title": "LLM Guided Evolution -- The Automation of Models Advancing Models",
        "authors": [
            "Clint Morris",
            "Michael Jurado",
            "Jason Zutty"
        ],
        "published": "2024-03-18T03:44:55Z",
        "summary": "In the realm of machine learning, traditional model development and automated\napproaches like AutoML typically rely on layers of abstraction, such as\ntree-based or Cartesian genetic programming. Our study introduces \"Guided\nEvolution\" (GE), a novel framework that diverges from these methods by\nutilizing Large Language Models (LLMs) to directly modify code. GE leverages\nLLMs for a more intelligent, supervised evolutionary process, guiding mutations\nand crossovers. Our unique \"Evolution of Thought\" (EoT) technique further\nenhances GE by enabling LLMs to reflect on and learn from the outcomes of\nprevious mutations. This results in a self-sustaining feedback loop that\naugments decision-making in model evolution. GE maintains genetic diversity,\ncrucial for evolutionary algorithms, by leveraging LLMs' capability to generate\ndiverse responses from expertly crafted prompts and modulate model temperature.\nThis not only accelerates the evolution process but also injects expert like\ncreativity and insight into the process. Our application of GE in evolving the\nExquisiteNetV2 model demonstrates its efficacy: the LLM-driven GE autonomously\nproduced variants with improved accuracy, increasing from 92.52% to 93.34%,\nwithout compromising model compactness. This underscores the potential of LLMs\nto accelerate the traditional model design pipeline, enabling models to\nautonomously evolve and enhance their own designs.",
        "pdf_link": "https://arxiv.org/pdf/2403.11446v1.pdf"
    },
    {
        "title": "SOTOPIA-$\u03c0$: Interactive Learning of Socially Intelligent Language Agents",
        "authors": [
            "Ruiyi Wang",
            "Haofei Yu",
            "Wenxin Zhang",
            "Zhengyang Qi",
            "Maarten Sap",
            "Graham Neubig",
            "Yonatan Bisk",
            "Hao Zhu"
        ],
        "published": "2024-03-13T17:17:48Z",
        "summary": "Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.",
        "pdf_link": "https://arxiv.org/pdf/2403.08715v2.pdf"
    },
    {
        "title": "Less is More: Data Value Estimation for Visual Instruction Tuning",
        "authors": [
            "Zikang Liu",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Dawei Gao",
            "Yaliang Li",
            "Ji-Rong Wen"
        ],
        "published": "2024-03-14T16:47:25Z",
        "summary": "Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.",
        "pdf_link": "https://arxiv.org/pdf/2403.09559v2.pdf"
    },
    {
        "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
        "authors": [
            "Fu Li",
            "Xueying Wang",
            "Bin Li",
            "Yunlong Wu",
            "Yanzhen Wang",
            "Xiaodong Yi"
        ],
        "published": "2024-01-16T03:28:29Z",
        "summary": "This paper presents an innovative exploration of the application potential of\nlarge language models (LLM) in addressing the challenging task of automatically\ngenerating behavior trees (BTs) for complex tasks. The conventional manual BT\ngeneration method is inefficient and heavily reliant on domain expertise. On\nthe other hand, existing automatic BT generation technologies encounter\nbottlenecks related to task complexity, model adaptability, and reliability. In\norder to overcome these challenges, we propose a novel methodology that\nleverages the robust representation and reasoning abilities of LLMs. The core\ncontribution of this paper lies in the design of a BT generation framework\nbased on LLM, which encompasses the entire process, from data synthesis and\nmodel training to application developing and data verification. Synthetic data\nis introduced to train the BT generation model (BTGen model), enhancing its\nunderstanding and adaptability to various complex tasks, thereby significantly\nimproving its overall performance. In order to ensure the effectiveness and\nexecutability of the generated BTs, we emphasize the importance of data\nverification and introduce a multilevel verification strategy. Additionally, we\nexplore a range of agent design and development schemes with LLM as the central\nelement. We hope that the work in this paper may provide a reference for the\nresearchers who are interested in BT generation based on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.08089v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling",
        "authors": [
            "Hang Jiang",
            "Xiajie Zhang",
            "Robert Mahari",
            "Daniel Kessler",
            "Eric Ma",
            "Tal August",
            "Irene Li",
            "Alex 'Sandy' Pentland",
            "Yoon Kim",
            "Jad Kabbara",
            "Deb Roy"
        ],
        "published": "2024-02-26T20:56:06Z",
        "summary": "Making legal knowledge accessible to non-experts is crucial for enhancing\ngeneral legal literacy and encouraging civic participation in democracy.\nHowever, legal documents are often challenging to understand for people without\nlegal backgrounds. In this paper, we present a novel application of large\nlanguage models (LLMs) in legal education to help non-experts learn intricate\nlegal concepts through storytelling, an effective pedagogical tool in conveying\ncomplex and abstract concepts. We also introduce a new dataset LegalStories,\nwhich consists of 295 complex legal doctrines, each accompanied by a story and\na set of multiple-choice questions generated by LLMs. To construct the dataset,\nwe experiment with various LLMs to generate legal stories explaining these\nconcepts. Furthermore, we use an expert-in-the-loop method to iteratively\ndesign multiple-choice questions. Then, we evaluate the effectiveness of\nstorytelling with LLMs through an RCT experiment with legal novices on 10\nsamples from the dataset. We find that LLM-generated stories enhance\ncomprehension of legal concepts and interest in law among non-native speakers\ncompared to only definitions. Moreover, stories consistently help participants\nrelate legal concepts to their lives. Finally, we find that learning with\nstories shows a higher retention rate for non-native speakers in the follow-up\nassessment. Our work has strong implications for using LLMs in promoting\nteaching and learning in the legal field and beyond.",
        "pdf_link": "https://arxiv.org/pdf/2402.17019v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on Controllable Generation under Diversified Instructions",
        "authors": [
            "Yihan Chen",
            "Benfeng Xu",
            "Quan Wang",
            "Yi Liu",
            "Zhendong Mao"
        ],
        "published": "2024-01-01T07:35:31Z",
        "summary": "While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.",
        "pdf_link": "https://arxiv.org/pdf/2401.00690v1.pdf"
    },
    {
        "title": "An Example of Evolutionary Computation + Large Language Model Beating Human: Design of Efficient Guided Local Search",
        "authors": [
            "Fei Liu",
            "Xialiang Tong",
            "Mingxuan Yuan",
            "Xi Lin",
            "Fu Luo",
            "Zhenkun Wang",
            "Zhichao Lu",
            "Qingfu Zhang"
        ],
        "published": "2024-01-04T04:11:59Z",
        "summary": "It is often very tedious for human experts to design efficient algorithms.\nRecently, we have proposed a novel Algorithm Evolution using Large Language\nModel (AEL) framework for automatic algorithm design. AEL combines the power of\na large language model and the paradigm of evolutionary computation to design,\ncombine, and modify algorithms automatically. In this paper, we use AEL to\ndesign the guide algorithm for guided local search (GLS) to solve the\nwell-known traveling salesman problem (TSP). AEL automatically evolves elite\nGLS algorithms in two days, with minimal human effort and no model training.\nExperimental results on 1,000 TSP20-TSP100 instances and TSPLib instances show\nthat AEL-designed GLS outperforms state-of-the-art human-designed GLS with the\nsame iteration budget. It achieves a 0% gap on TSP20 and TSP50 and a 0.032% gap\non TSP100 in 1,000 iterations. Our findings mark the emergence of a new era in\nautomatic algorithm design.",
        "pdf_link": "https://arxiv.org/pdf/2401.02051v1.pdf"
    },
    {
        "title": "Gender Bias in Machine Translation and The Era of Large Language Models",
        "authors": [
            "Eva Vanmassenhove"
        ],
        "published": "2024-01-18T14:34:49Z",
        "summary": "This chapter examines the role of Machine Translation in perpetuating gender\nbias, highlighting the challenges posed by cross-linguistic settings and\nstatistical dependencies. A comprehensive overview of relevant existing work\nrelated to gender bias in both conventional Neural Machine Translation\napproaches and Generative Pretrained Transformer models employed as Machine\nTranslation systems is provided. Through an experiment using ChatGPT (based on\nGPT-3.5) in an English-Italian translation context, we further assess ChatGPT's\ncurrent capacity to address gender bias. The findings emphasize the ongoing\nneed for advancements in mitigating bias in Machine Translation systems and\nunderscore the importance of fostering fairness and inclusivity in language\ntechnologies.",
        "pdf_link": "https://arxiv.org/pdf/2401.10016v1.pdf"
    },
    {
        "title": "JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented Fine-Tuning",
        "authors": [
            "Anique Tahir",
            "Lu Cheng",
            "Huan Liu"
        ],
        "published": "2024-03-17T23:02:04Z",
        "summary": "The scaling of Large Language Models (LLMs) for retrieval-based tasks,\nparticularly in Retrieval Augmented Generation (RAG), faces significant memory\nconstraints, especially when fine-tuning extensive prompt sequences. Current\nopen-source libraries support full-model inference and fine-tuning across\nmultiple GPUs but fall short of accommodating the efficient parameter\ndistribution required for retrieved context. Addressing this gap, we introduce\na novel framework for PEFT-compatible fine-tuning of Llama-2 models, leveraging\ndistributed training. Our framework uniquely utilizes JAX's just-in-time (JIT)\ncompilation and tensor-sharding for efficient resource management, thereby\nenabling accelerated fine-tuning with reduced memory requirements. This\nadvancement significantly improves the scalability and feasibility of\nfine-tuning LLMs for complex RAG applications, even on systems with limited GPU\nresources. Our experiments show more than 12x improvement in runtime compared\nto Hugging Face/DeepSpeed implementation with four GPUs while consuming less\nthan half the VRAM per GPU.",
        "pdf_link": "https://arxiv.org/pdf/2403.11366v2.pdf"
    },
    {
        "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
        "authors": [
            "Heydar Soudani",
            "Evangelos Kanoulas",
            "Faegheh Hasibi"
        ],
        "published": "2024-03-03T08:07:55Z",
        "summary": "Large language models (LLMs) memorize a vast amount of factual knowledge,\nexhibiting strong performance across diverse tasks and domains. However, it has\nbeen observed that the performance diminishes when dealing with less-popular or\nlow-frequency concepts and entities, for example in domain specific\napplications. The two prominent approaches to enhance the performance of LLMs\non low-frequent topics are: Retrieval Augmented Generation (RAG) and\nfine-tuning (FT) over synthetic data. This paper explores and evaluates the\nimpact of RAG and FT on customizing LLMs in handling low-frequency entities on\nquestion answering task. Our findings indicate that FT significantly boosts the\nperformance across entities of varying popularity, especially in the most and\nleast popular groups, while RAG surpasses other methods. Additionally, the\nsuccess of both RAG and FT approaches is amplified by advancements in retrieval\nand data augmentation techniques. We release our data and code at\nhttps://github.com/informagi/RAGvsFT.",
        "pdf_link": "https://arxiv.org/pdf/2403.01432v2.pdf"
    },
    {
        "title": "RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models",
        "authors": [
            "Liangliang Chen",
            "Yutian Lei",
            "Shiyu Jin",
            "Ying Zhang",
            "Liangjun Zhang"
        ],
        "published": "2024-03-11T04:13:26Z",
        "summary": "Reinforcement learning (RL) has demonstrated its capability in solving\nvarious tasks but is notorious for its low sample efficiency. In this paper, we\npropose RLingua, a framework that can leverage the internal knowledge of large\nlanguage models (LLMs) to reduce the sample complexity of RL in robotic\nmanipulations. To this end, we first present a method for extracting the prior\nknowledge of LLMs by prompt engineering so that a preliminary rule-based robot\ncontroller for a specific task can be generated in a user-friendly manner.\nDespite being imperfect, the LLM-generated robot controller is utilized to\nproduce action samples during rollouts with a decaying probability, thereby\nimproving RL's sample efficiency. We employ TD3, the widely-used RL baseline\nmethod, and modify the actor loss to regularize the policy learning towards the\nLLM-generated controller. RLingua also provides a novel method of improving the\nimperfect LLM-generated robot controllers by RL. We demonstrate that RLingua\ncan significantly reduce the sample complexity of TD3 in four robot tasks of\npanda_gym and achieve high success rates in 12 sampled sparsely rewarded robot\ntasks in RLBench, where the standard TD3 fails. Additionally, We validated\nRLingua's effectiveness in real-world robot experiments through Sim2Real,\ndemonstrating that the learned policies are effectively transferable to real\nrobot tasks. Further details about our work are available at our project\nwebsite https://rlingua.github.io.",
        "pdf_link": "https://arxiv.org/pdf/2403.06420v2.pdf"
    },
    {
        "title": "Intent-conditioned and Non-toxic Counterspeech Generation using Multi-Task Instruction Tuning with RLAIF",
        "authors": [
            "Amey Hengle",
            "Aswini Kumar",
            "Sahajpreet Singh",
            "Anil Bandhakavi",
            "Md Shad Akhtar",
            "Tanmoy Chakroborty"
        ],
        "published": "2024-03-15T08:03:49Z",
        "summary": "Counterspeech, defined as a response to mitigate online hate speech, is\nincreasingly used as a non-censorial solution. Addressing hate speech\neffectively involves dispelling the stereotypes, prejudices, and biases often\nsubtly implied in brief, single-sentence statements or abuses. These implicit\nexpressions challenge language models, especially in seq2seq tasks, as model\nperformance typically excels with longer contexts. Our study introduces CoARL,\na novel framework enhancing counterspeech generation by modeling the pragmatic\nimplications underlying social biases in hateful statements. CoARL's first two\nphases involve sequential multi-instruction tuning, teaching the model to\nunderstand intents, reactions, and harms of offensive statements, and then\nlearning task-specific low-rank adapter weights for generating\nintent-conditioned counterspeech. The final phase uses reinforcement learning\nto fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms\nexisting benchmarks in intent-conditioned counterspeech generation, showing an\naverage improvement of 3 points in intent-conformity and 4 points in\nargument-quality metrics. Extensive human evaluation supports CoARL's efficacy\nin generating superior and more context-appropriate responses compared to\nexisting systems, including prominent LLMs like ChatGPT.",
        "pdf_link": "https://arxiv.org/pdf/2403.10088v1.pdf"
    },
    {
        "title": "Pheme: Efficient and Conversational Speech Generation",
        "authors": [
            "Pawe\u0142 Budzianowski",
            "Taras Sereda",
            "Tomasz Cichy",
            "Ivan Vuli\u0107"
        ],
        "published": "2024-01-05T14:47:20Z",
        "summary": "In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.",
        "pdf_link": "https://arxiv.org/pdf/2401.02839v1.pdf"
    },
    {
        "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks Against LLM-Integrated Applications",
        "authors": [
            "Xuchen Suo"
        ],
        "published": "2024-01-15T11:44:18Z",
        "summary": "The critical challenge of prompt injection attacks in Large Language Models\n(LLMs) integrated applications, a growing concern in the Artificial\nIntelligence (AI) field. Such attacks, which manipulate LLMs through natural\nlanguage inputs, pose a significant threat to the security of these\napplications. Traditional defense strategies, including output and input\nfiltering, as well as delimiter use, have proven inadequate. This paper\nintroduces the 'Signed-Prompt' method as a novel solution. The study involves\nsigning sensitive instructions within command segments by authorized users,\nenabling the LLM to discern trusted instruction sources. The paper presents a\ncomprehensive analysis of prompt injection attack patterns, followed by a\ndetailed explanation of the Signed-Prompt concept, including its basic\narchitecture and implementation through both prompt engineering and fine-tuning\nof LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method,\nshowing substantial resistance to various types of prompt injection attacks,\nthus validating its potential as a robust defense strategy in AI security.",
        "pdf_link": "https://arxiv.org/pdf/2401.07612v1.pdf"
    },
    {
        "title": "Large Language Models as Minecraft Agents",
        "authors": [
            "Chris Madge",
            "Massimo Poesio"
        ],
        "published": "2024-02-13T11:37:30Z",
        "summary": "In this work we examine the use of Large Language Models (LLMs) in the\nchallenging setting of acting as a Minecraft agent. We apply and evaluate LLMs\nin the builder and architect settings, introduce clarification questions and\nexamining the challenges and opportunities for improvement. In addition, we\npresent a platform for online interaction with the agents and an evaluation\nagainst previous works.",
        "pdf_link": "https://arxiv.org/pdf/2402.08392v1.pdf"
    },
    {
        "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
        "authors": [
            "Cheng Li",
            "Mengzhou Chen",
            "Jindong Wang",
            "Sunayana Sitaram",
            "Xing Xie"
        ],
        "published": "2024-02-09T04:02:43Z",
        "summary": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation.",
        "pdf_link": "https://arxiv.org/pdf/2402.10946v1.pdf"
    },
    {
        "title": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "authors": [
            "Uri Shaham",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Idan Szpektor",
            "Reut Tsarfaty",
            "Matan Eyal"
        ],
        "published": "2024-01-03T17:48:10Z",
        "summary": "As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages\nfrom the pre-training corpus. We first show that many languages transfer some\ninstruction-following capabilities to other languages from even monolingual\ntuning. Furthermore, we find that only 40 multilingual examples integrated in\nan English tuning set substantially improve multilingual instruction-following,\nboth in seen and unseen languages during tuning. In general, we observe that\nmodels tuned on multilingual mixtures exhibit comparable or superior\nperformance in multiple languages compared to monolingually tuned models,\ndespite training on 10x fewer examples in those languages. Finally, we find\nthat diversifying the instruction tuning set with even just 2-4 languages\nsignificantly improves cross-lingual generalization. Our results suggest that\nbuilding massively multilingual instruction-tuned models can be done with only\na very small set of multilingual instruction-responses.",
        "pdf_link": "https://arxiv.org/pdf/2401.01854v3.pdf"
    },
    {
        "title": "Improving Black-box Robustness with In-Context Rewriting",
        "authors": [
            "Kyle O'Brien",
            "Nathan Ng",
            "Isha Puri",
            "Jorge Mendez",
            "Hamid Palangi",
            "Yoon Kim",
            "Marzyeh Ghassemi",
            "Thomas Hartvigsen"
        ],
        "published": "2024-02-13T05:33:35Z",
        "summary": "Machine learning models often excel on in-distribution (ID) data but struggle\nwith unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD\nrobustness are not applicable to settings where the model is effectively a\nblack box, such as when the weights are frozen, retraining is costly, or the\nmodel is leveraged via an API. Test-time augmentation (TTA) is a simple\npost-hoc technique for improving robustness that sidesteps black-box\nconstraints by aggregating predictions across multiple augmentations of the\ntest input. TTA has seen limited use in NLP due to the challenge of generating\neffective natural language augmentations. In this work, we propose LLM-TTA,\nwhich uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA\noutperforms conventional augmentation functions across sentiment, toxicity, and\nnews classification tasks for BERT and T5 models, with BERT's OOD robustness\nimproving by an average of 4.30 percentage points without regressing average ID\nperformance. We explore selectively augmenting inputs based on prediction\nentropy to reduce the rate of expensive LLM augmentations, allowing us to\nmaintain performance gains while reducing the average number of generated\naugmentations by 57.76%. LLM-TTA is agnostic to the task model architecture,\ndoes not require OOD labels, and is effective across low and high-resource\nsettings. We share our data, models, and code for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2402.08225v2.pdf"
    },
    {
        "title": "Automatic Evaluation for Mental Health Counseling using LLMs",
        "authors": [
            "Anqi Li",
            "Yu Lu",
            "Nirui Song",
            "Shuai Zhang",
            "Lizhi Ma",
            "Zhenzhong Lan"
        ],
        "published": "2024-02-19T09:00:10Z",
        "summary": "High-quality psychological counseling is crucial for mental health worldwide,\nand timely evaluation is vital for ensuring its effectiveness. However,\nobtaining professional evaluation for each counseling session is expensive and\nchallenging. Existing methods that rely on self or third-party manual reports\nto assess the quality of counseling suffer from subjective biases and\nlimitations of time-consuming.\n  To address above challenges, this paper proposes an innovative and efficient\nautomatic approach using large language models (LLMs) to evaluate the working\nalliance in counseling conversations. We collected a comprehensive counseling\ndataset and conducted multiple third-party evaluations based on therapeutic\nrelationship theory. Our LLM-based evaluation, combined with our guidelines,\nshows high agreement with human evaluations and provides valuable insights into\ncounseling scripts. This highlights the potential of LLMs as supervisory tools\nfor psychotherapists. By integrating LLMs into the evaluation process, our\napproach offers a cost-effective and dependable means of assessing counseling\nquality, enhancing overall effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2402.11958v1.pdf"
    },
    {
        "title": "MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering",
        "authors": [
            "Che Guan",
            "Mengyu Huang",
            "Peng Zhang"
        ],
        "published": "2024-03-28T03:14:18Z",
        "summary": "In today's fast-paced industry, professionals face the challenge of\nsummarizing a large number of documents and extracting vital information from\nthem on a daily basis. These metrics are frequently hidden away in tables\nand/or their nested hyperlinks. To address this challenge, the approach of\nTable Question Answering (QA) has been developed to extract the relevant\ninformation. However, traditional Table QA training tasks that provide a table\nand an answer(s) from a gold cell coordinate(s) for a question may not always\nensure extracting the accurate answer(s). Recent advancements in Large Language\nModels (LLMs) have opened up new possibilities for extracting information from\ntabular data using prompts. In this paper, we introduce the Multi-hop Few-shot\nOpen Rich Table QA (MFORT-QA) approach, which consists of two major steps. The\nfirst step involves Few-Shot Learning (FSL), where relevant tables and\nassociated contexts of hyperlinks are retrieved based on a given question. The\nretrieved content is then used to construct few-shot prompts as inputs to an\nLLM, such as ChatGPT. To tackle the challenge of answering complex questions,\nthe second step leverages Chain-of-thought (CoT) prompting to decompose the\ncomplex question into a sequential chain of questions and reasoning thoughts in\na multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process\nby retrieving relevant tables and contexts of hyperlinks that are relevant to\nthe resulting reasoning thoughts and questions. These additional contexts are\nthen used to supplement the prompt used in the first step, resulting in more\naccurate answers from an LLM. Empirical results from OTT-QA demonstrate that\nour abstractive QA approach significantly improves the accuracy of extractive\nTable QA methods.",
        "pdf_link": "https://arxiv.org/pdf/2403.19116v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for NLG Evaluation: A Survey",
        "authors": [
            "Zhen Li",
            "Xiaohan Xu",
            "Tao Shen",
            "Can Xu",
            "Jia-Chen Gu",
            "Chongyang Tao"
        ],
        "published": "2024-01-13T15:59:09Z",
        "summary": "In the rapidly evolving domain of Natural Language Generation (NLG)\nevaluation, introducing Large Language Models (LLMs) has opened new avenues for\nassessing generated content quality, e.g., coherence, creativity, and context\nrelevance. This survey aims to provide a thorough overview of leveraging LLMs\nfor NLG evaluation, a burgeoning area that lacks a systematic analysis. We\npropose a coherent taxonomy for organizing existing LLM-based evaluation\nmetrics, offering a structured framework to understand and compare these\nmethods. Our detailed exploration includes critically assessing various\nLLM-based methodologies, as well as comparing their strengths and limitations\nin evaluating NLG outputs. By discussing unresolved challenges, including bias,\nrobustness, domain-specificity, and unified evaluation, this survey seeks to\noffer insights to researchers and advocate for fairer and more advanced NLG\nevaluation techniques.",
        "pdf_link": "https://arxiv.org/pdf/2401.07103v1.pdf"
    },
    {
        "title": "Prompting Large Vision-Language Models for Compositional Reasoning",
        "authors": [
            "Timothy Ossowski",
            "Ming Jiang",
            "Junjie Hu"
        ],
        "published": "2024-01-20T22:04:28Z",
        "summary": "Vision-language models such as CLIP have shown impressive capabilities in\nencoding texts and images into aligned embeddings, enabling the retrieval of\nmultimodal data in a shared embedding space. However, these embedding-based\nmodels still face challenges in effectively matching images and texts with\nsimilar visio-linguistic compositionality, as evidenced by their performance on\nthe recent Winoground dataset. In this paper, we argue that this limitation\nstems from two factors: the use of single vector representations for complex\nmultimodal data, and the absence of step-by-step reasoning in these\nembedding-based methods. To address this issue, we make an exploratory step\nusing a novel generative method that prompts large vision-language models\n(e.g., GPT-4) to depict images and perform compositional reasoning. Our method\noutperforms other embedding-based methods on the Winoground dataset, and\nobtains further improvement of up to 10% accuracy when enhanced with the\noptimal description.",
        "pdf_link": "https://arxiv.org/pdf/2401.11337v1.pdf"
    },
    {
        "title": "Ontology Completion with Natural Language Inference and Concept Embeddings: An Analysis",
        "authors": [
            "Na Li",
            "Thomas Bailleux",
            "Zied Bouraoui",
            "Steven Schockaert"
        ],
        "published": "2024-03-25T21:46:35Z",
        "summary": "We consider the problem of finding plausible knowledge that is missing from a\ngiven ontology, as a generalisation of the well-studied taxonomy expansion\ntask. One line of work treats this task as a Natural Language Inference (NLI)\nproblem, thus relying on the knowledge captured by language models to identify\nthe missing knowledge. Another line of work uses concept embeddings to identify\nwhat different concepts have in common, taking inspiration from cognitive\nmodels for category based induction. These two approaches are intuitively\ncomplementary, but their effectiveness has not yet been compared. In this\npaper, we introduce a benchmark for evaluating ontology completion methods and\nthoroughly analyse the strengths and weaknesses of both approaches. We find\nthat both approaches are indeed complementary, with hybrid strategies achieving\nthe best overall results. We also find that the task is highly challenging for\nLarge Language Models, even after fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2403.17216v1.pdf"
    },
    {
        "title": "VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model",
        "authors": [
            "Junsu Kim",
            "Yunhoe Ku",
            "Jihyeon Kim",
            "Junuk Cha",
            "Seungryul Baek"
        ],
        "published": "2024-03-08T14:23:00Z",
        "summary": "In the field of Class Incremental Object Detection (CIOD), creating models\nthat can continuously learn like humans is a major challenge. Pseudo-labeling\nmethods, although initially powerful, struggle with multi-scenario incremental\nlearning due to their tendency to forget past knowledge. To overcome this, we\nintroduce a new approach called Vision-Language Model assisted Pseudo-Labeling\n(VLM-PL). This technique uses Vision-Language Model (VLM) to verify the\ncorrectness of pseudo ground-truths (GTs) without requiring additional model\ntraining. VLM-PL starts by deriving pseudo GTs from a pre-trained detector.\nThen, we generate custom queries for each pseudo GT using carefully designed\nprompt templates that combine image and text features. This allows the VLM to\nclassify the correctness through its responses. Furthermore, VLM-PL integrates\nrefined pseudo and real GTs from upcoming training, effectively combining new\nand old knowledge. Extensive experiments conducted on the Pascal VOC and MS\nCOCO datasets not only highlight VLM-PL's exceptional performance in\nmulti-scenario but also illuminate its effectiveness in dual-scenario by\nachieving state-of-the-art results in both.",
        "pdf_link": "https://arxiv.org/pdf/2403.05346v1.pdf"
    },
    {
        "title": "Empirical Study of Large Language Models as Automated Essay Scoring Tools in English Composition__Taking TOEFL Independent Writing Task for Example",
        "authors": [
            "Wei Xia",
            "Shaoguang Mao",
            "Chanjing Zheng"
        ],
        "published": "2024-01-07T07:13:50Z",
        "summary": "Large language models have demonstrated exceptional capabilities in tasks\ninvolving natural language generation, reasoning, and comprehension. This study\naims to construct prompts and comments grounded in the diverse scoring criteria\ndelineated within the official TOEFL guide. The primary objective is to assess\nthe capabilities and constraints of ChatGPT, a prominent representative of\nlarge language models, within the context of automated essay scoring. The\nprevailing methodologies for automated essay scoring involve the utilization of\ndeep neural networks, statistical machine learning techniques, and fine-tuning\npre-trained models. However, these techniques face challenges when applied to\ndifferent contexts or subjects, primarily due to their substantial data\nrequirements and limited adaptability to small sample sizes. In contrast, this\nstudy employs ChatGPT to conduct an automated evaluation of English essays,\neven with a small sample size, employing an experimental approach. The\nempirical findings indicate that ChatGPT can provide operational functionality\nfor automated essay scoring, although the results exhibit a regression effect.\nIt is imperative to underscore that the effective design and implementation of\nChatGPT prompts necessitate a profound domain expertise and technical\nproficiency, as these prompts are subject to specific threshold criteria.\nKeywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent\nWriting Task",
        "pdf_link": "https://arxiv.org/pdf/2401.03401v1.pdf"
    },
    {
        "title": "Exploring the Potential of Large Language Models in Graph Generation",
        "authors": [
            "Yang Yao",
            "Xin Wang",
            "Zeyang Zhang",
            "Yijian Qin",
            "Ziwei Zhang",
            "Xu Chu",
            "Yuekui Yang",
            "Wenwu Zhu",
            "Hong Mei"
        ],
        "published": "2024-03-21T12:37:54Z",
        "summary": "Large language models (LLMs) have achieved great success in many fields, and\nrecent works have studied exploring LLMs for graph discriminative tasks such as\nnode classification. However, the abilities of LLMs for graph generation remain\nunexplored in the literature. Graph generation requires the LLM to generate\ngraphs with given properties, which has valuable real-world applications such\nas drug discovery, while tends to be more challenging. In this paper, we\npropose LLM4GraphGen to explore the ability of LLMs for graph generation with\nsystematical task designs and extensive experiments. Specifically, we propose\nseveral tasks tailored with comprehensive experiments to address key questions\nregarding LLMs' understanding of different graph structure rules, their ability\nto capture structural type distributions, and their utilization of domain\nknowledge for property-based graph generation. Our evaluations demonstrate that\nLLMs, particularly GPT-4, exhibit preliminary abilities in graph generation\ntasks, including rule-based and distribution-based generation. We also observe\nthat popular prompting methods, such as few-shot and chain-of-thought\nprompting, do not consistently enhance performance. Besides, LLMs show\npotential in generating molecules with specific properties. These findings may\nserve as foundations for designing good LLMs based models for graph generation\nand provide valuable insights and further research.",
        "pdf_link": "https://arxiv.org/pdf/2403.14358v1.pdf"
    },
    {
        "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
        "authors": [
            "Bu Jin",
            "Yupeng Zheng",
            "Pengfei Li",
            "Weize Li",
            "Yuhang Zheng",
            "Sujie Hu",
            "Xinyu Liu",
            "Jinwei Zhu",
            "Zhijie Yan",
            "Haiyang Sun",
            "Kun Zhan",
            "Peng Jia",
            "Xiaoxiao Long",
            "Yilun Chen",
            "Hao Zhao"
        ],
        "published": "2024-03-28T17:12:55Z",
        "summary": "3D dense captioning stands as a cornerstone in achieving a comprehensive\nunderstanding of 3D scenes through natural language. It has recently witnessed\nremarkable achievements, particularly in indoor settings. However, the\nexploration of 3D dense captioning in outdoor scenes is hindered by two major\nchallenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such\nas dynamics and sparse visual inputs, makes it difficult to directly adapt\nexisting indoor methods; 2) the \\textbf{lack of data} with comprehensive\nbox-caption pair annotations specifically tailored for outdoor scenes. To this\nend, we introduce the new task of outdoor 3D dense captioning. As input, we\nassume a LiDAR point cloud and a set of RGB images captured by the panoramic\ncamera rig. The expected output is a set of object boxes with captions. To\ntackle this task, we propose the TOD3Cap network, which leverages the BEV\nrepresentation to generate object box proposals and integrates Relation\nQ-Former with LLaMA-Adapter to generate rich captions for these objects. We\nalso introduce the TOD3Cap dataset, the largest one to our knowledge for 3D\ndense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K\noutdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively\nlocalize and caption 3D objects in outdoor scenes, which outperforms baseline\nmethods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models are\npublicly available at https://github.com/jxbbb/TOD3Cap.",
        "pdf_link": "https://arxiv.org/pdf/2403.19589v1.pdf"
    },
    {
        "title": "DiffusionGPT: LLM-Driven Text-to-Image Generation System",
        "authors": [
            "Jie Qin",
            "Jie Wu",
            "Weifeng Chen",
            "Yuxi Ren",
            "Huixia Li",
            "Hefeng Wu",
            "Xuefeng Xiao",
            "Rui Wang",
            "Shilei Wen"
        ],
        "published": "2024-01-18T15:30:58Z",
        "summary": "Diffusion models have opened up new avenues for the field of image\ngeneration, resulting in the proliferation of high-quality models shared on\nopen-source platforms. However, a major challenge persists in current\ntext-to-image systems are often unable to handle diverse inputs, or are limited\nto single model results. Current unified attempts often fall into two\northogonal aspects: i) parse Diverse Prompts in input stage; ii) activate\nexpert model to output. To combine the best of both worlds, we propose\nDiffusionGPT, which leverages Large Language Models (LLM) to offer a unified\ngeneration system capable of seamlessly accommodating various types of prompts\nand integrating domain-expert models. DiffusionGPT constructs domain-specific\nTrees for various generative models based on prior knowledge. When provided\nwith an input, the LLM parses the prompt and employs the Trees-of-Thought to\nguide the selection of an appropriate model, thereby relaxing input constraints\nand ensuring exceptional performance across diverse domains. Moreover, we\nintroduce Advantage Databases, where the Tree-of-Thought is enriched with human\nfeedback, aligning the model selection process with human preferences. Through\nextensive experiments and comparisons, we demonstrate the effectiveness of\nDiffusionGPT, showcasing its potential for pushing the boundaries of image\nsynthesis in diverse domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.10061v1.pdf"
    },
    {
        "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT",
        "authors": [
            "J. K. Lee",
            "T. M. Chung"
        ],
        "published": "2024-03-16T02:27:19Z",
        "summary": "The rapid advancement of large language models (LLMs) has enabled natural\nlanguage processing capabilities similar to those of humans, and LLMs are being\nwidely utilized across various societal domains such as education and\nhealthcare. While the versatility of these models has increased, they have the\npotential to generate subjective and normative language, leading to\ndiscriminatory treatment or outcomes among social groups, especially due to\nonline offensive language. In this paper, we define such harm as societal bias\nand assess ethnic, gender, and racial biases in a model fine-tuned with Korean\ncomments using Bidirectional Encoder Representations from Transformers (KcBERT)\nand KOLD data through template-based Masked Language Modeling (MLM). To\nquantitatively evaluate biases, we employ LPBS and CBS metrics. Compared to\nKcBERT, the fine-tuned model shows a reduction in ethnic bias but demonstrates\nsignificant changes in gender and racial biases. Based on these results, we\npropose two methods to mitigate societal bias. Firstly, a data balancing\napproach during the pre-training phase adjusts the uniformity of data by\naligning the distribution of the occurrences of specific words and converting\nsurrounding harmful words into non-harmful words. Secondly, during the\nin-training phase, we apply Debiasing Regularization by adjusting dropout and\nregularization, confirming a decrease in training loss. Our contribution lies\nin demonstrating that societal bias exists in Korean language models due to\nlanguage-dependent characteristics.",
        "pdf_link": "https://arxiv.org/pdf/2403.10774v1.pdf"
    },
    {
        "title": "Generative AI and Process Systems Engineering: The Next Frontier",
        "authors": [
            "Benjamin Decardi-Nelson",
            "Abdulelah S. Alshehri",
            "Akshay Ajagekar",
            "Fengqi You"
        ],
        "published": "2024-02-15T18:20:42Z",
        "summary": "This article explores how emerging generative artificial intelligence (GenAI)\nmodels, such as large language models (LLMs), can enhance solution\nmethodologies within process systems engineering (PSE). These cutting-edge\nGenAI models, particularly foundation models (FMs), which are pre-trained on\nextensive, general-purpose datasets, offer versatile adaptability for a broad\nrange of tasks, including responding to queries, image generation, and complex\ndecision-making. Given the close relationship between advancements in PSE and\ndevelopments in computing and systems technologies, exploring the synergy\nbetween GenAI and PSE is essential. We begin our discussion with a compact\noverview of both classic and emerging GenAI models, including FMs, and then\ndive into their applications within key PSE domains: synthesis and design,\noptimization and integration, and process monitoring and control. In each\ndomain, we explore how GenAI models could potentially advance PSE\nmethodologies, providing insights and prospects for each area. Furthermore, the\narticle identifies and discusses potential challenges in fully leveraging GenAI\nwithin PSE, including multiscale modeling, data requirements, evaluation\nmetrics and benchmarks, and trust and safety, thereby deepening the discourse\non effective GenAI integration into systems analysis, design, optimization,\noperations, monitoring, and control. This paper provides a guide for future\nresearch focused on the applications of emerging GenAI in PSE.",
        "pdf_link": "https://arxiv.org/pdf/2402.10977v1.pdf"
    },
    {
        "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
        "authors": [
            "Yuxia Wang",
            "Jonibek Mansurov",
            "Petar Ivanov",
            "Jinyan Su",
            "Artem Shelmanov",
            "Akim Tsvigun",
            "Osama Mohanned Afzal",
            "Tarek Mahmoud",
            "Giovanni Puccetti",
            "Thomas Arnold",
            "Alham Fikri Aji",
            "Nizar Habash",
            "Iryna Gurevych",
            "Preslav Nakov"
        ],
        "published": "2024-02-17T02:50:33Z",
        "summary": "The advent of Large Language Models (LLMs) has brought an unprecedented surge\nin machine-generated text (MGT) across diverse channels. This raises legitimate\nconcerns about its potential misuse and societal implications. The need to\nidentify and differentiate such content from genuine human-generated text is\ncritical in combating disinformation, preserving the integrity of education and\nscientific fields, and maintaining trust in communication. In this work, we\naddress this problem by introducing a new benchmark involving multilingual,\nmulti-domain and multi-generator for MGT detection -- M4GT-Bench. It is\ncollected for three task formulations: (1) mono-lingual and multi-lingual\nbinary MGT detection; (2) multi-way detection identifies which particular model\ngenerates the text; and (3) human-machine mixed text detection, where a word\nboundary delimiting MGT from human-written content should be determined. Human\nevaluation for Task 2 shows less than random guess performance, demonstrating\nthe challenges to distinguish unique LLMs. Promising results always occur when\ntraining and test data distribute within the same domain or generators.",
        "pdf_link": "https://arxiv.org/pdf/2402.11175v1.pdf"
    },
    {
        "title": "Is There a One-Model-Fits-All Approach to Information Extraction? Revisiting Task Definition Biases",
        "authors": [
            "Wenhao Huang",
            "Qianyu He",
            "Zhixu Li",
            "Jiaqing Liang",
            "Yanghua Xiao"
        ],
        "published": "2024-03-25T03:19:20Z",
        "summary": "Definition bias is a negative phenomenon that can mislead models. Definition\nbias in information extraction appears not only across datasets from different\ndomains but also within datasets sharing the same domain. We identify two types\nof definition bias in IE: bias among information extraction datasets and bias\nbetween information extraction datasets and instruction tuning datasets. To\nsystematically investigate definition bias, we conduct three probing\nexperiments to quantitatively analyze it and discover the limitations of\nunified information extraction and large language models in solving definition\nbias. To mitigate definition bias in information extraction, we propose a\nmulti-stage framework consisting of definition bias measurement, bias-aware\nfine-tuning, and task-specific bias mitigation. Experimental results\ndemonstrate the effectiveness of our framework in addressing definition bias.\nResources of this paper can be found at\nhttps://github.com/EZ-hwh/definition-bias",
        "pdf_link": "https://arxiv.org/pdf/2403.16396v1.pdf"
    },
    {
        "title": "INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges",
        "authors": [
            "Jayr Pereira",
            "Andre Assumpcao",
            "Julio Trecenti",
            "Luiz Airosa",
            "Caio Lente",
            "Jhonatan Cl\u00e9to",
            "Guilherme Dobins",
            "Rodrigo Nogueira",
            "Luis Mitchell",
            "Roberto Lotufo"
        ],
        "published": "2024-01-10T17:13:28Z",
        "summary": "This paper introduces INACIA (Instru\\c{c}\\~ao Assistida com Intelig\\^encia\nArtificial), a groundbreaking system designed to integrate Large Language\nModels (LLMs) into the operational framework of Brazilian Federal Court of\nAccounts (TCU). The system automates various stages of case analysis, including\nbasic information extraction, admissibility examination, Periculum in mora and\nFumus boni iuris analyses, and recommendations generation. Through a series of\nexperiments, we demonstrate INACIA's potential in extracting relevant\ninformation from case documents, evaluating its legal plausibility, and\nformulating propositions for judicial decision-making. Utilizing a validation\ndataset alongside LLMs, our evaluation methodology presents a novel approach to\nassessing system performance, correlating highly with human judgment. These\nresults underscore INACIA's potential in complex legal task handling while also\nacknowledging the current limitations. This study discusses possible\nimprovements and the broader implications of applying AI in legal contexts,\nsuggesting that INACIA represents a significant step towards integrating AI in\nlegal systems globally, albeit with cautious optimism grounded in the empirical\nfindings.",
        "pdf_link": "https://arxiv.org/pdf/2401.05273v3.pdf"
    },
    {
        "title": "Evaluating Large Language Models for Generalization and Robustness via Data Compression",
        "authors": [
            "Yucheng Li",
            "Yunhao Guo",
            "Frank Guerin",
            "Chenghua Lin"
        ],
        "published": "2024-02-01T18:56:18Z",
        "summary": "Existing methods for evaluating large language models face challenges such as\ndata contamination, sensitivity to prompts, and the high cost of benchmark\ncreation. To address this, we propose a lossless data compression based\nevaluation approach that tests how models' predictive abilities generalize\nafter their training cutoff. Specifically, we collect comprehensive test data\nspanning 83 months from 2017 to 2023 and split the data into training and\ntesting periods according to models' training data cutoff. We measure: 1) the\ncompression performance on the testing period as a measure of generalization on\nunseen data; and 2) the performance gap between the training and testing period\nas a measure of robustness. Our experiments test 14 representative large\nlanguage models with various sizes on sources including Wikipedia, news\narticles, code, arXiv papers, and multi-modal data. We find that the\ncompression rate of many models reduces significantly after their cutoff date,\nbut models such as Mistral and Llama-2 demonstrate a good balance between\nperformance and robustness. Results also suggest that models struggle to\ngeneralize on news and code data, but work especially well on arXiv papers. We\nalso find the context size and tokenization implementation have a big impact of\non the overall compression performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.00861v2.pdf"
    },
    {
        "title": "Cross-lingual Editing in Multilingual Language Models",
        "authors": [
            "Himanshu Beniwal",
            "Kowsik Nandagopan D",
            "Mayank Singh"
        ],
        "published": "2024-01-19T06:54:39Z",
        "summary": "The training of large language models (LLMs) necessitates substantial data\nand computational resources, and updating outdated LLMs entails significant\nefforts and resources. While numerous model editing techniques (METs) have\nemerged to efficiently update model outputs without retraining, their\neffectiveness in multilingual LLMs, where knowledge is stored in diverse\nlanguages, remains an underexplored research area. This research paper\nintroduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a\nfact is edited in one language, and the subsequent update propagation is\nobserved across other languages. To investigate the XME paradigm, we conducted\nexperiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:\n\\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi,\nGujarati, and Bengali). The results reveal notable performance limitations of\nstate-of-the-art METs under the XME setting, mainly when the languages involved\nbelong to two distinct script families. These findings highlight the need for\nfurther research and development of XME techniques to address these challenges.\nFor more comprehensive information, the dataset used in this research and the\nassociated code are publicly available at the following\nURL\\url{https://github.com/lingo-iitgn/XME}.",
        "pdf_link": "https://arxiv.org/pdf/2401.10521v2.pdf"
    },
    {
        "title": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models",
        "authors": [
            "Wai-Chung Kwan",
            "Xingshan Zeng",
            "Yuxin Jiang",
            "Yufei Wang",
            "Liangyou Li",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu",
            "Kam-Fai Wong"
        ],
        "published": "2024-01-30T04:50:28Z",
        "summary": "Large language models (LLMs) are increasingly relied upon for complex\nmulti-turn conversations across diverse real-world applications. However,\nexisting benchmarks predominantly focus on single-turn evaluations, overlooking\nthe models' capabilities in multi-turn interactions. To address this gap, we\nintroduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn\nconversational abilities. By analyzing human-LLM conversations, we categorize\ninteraction patterns into four types: recollection, expansion, refinement, and\nfollow-up. We construct multi-turn queries for each category either by\naugmenting existing datasets or by creating new examples with GPT-4 to avoid\ndata leakage. To study the factors impacting multi-turn abilities, we create\nsingle-turn versions of the 1170 multi-turn queries and compare performance.\nOur evaluation of 11 well-known LLMs shows that while closed-source models\ngenerally surpass open-source ones, certain open-source models exceed\nGPT-3.5-Turbo in specific tasks. We observe significant performance degradation\nin multi-turn settings compared to single-turn settings in most models, which\nis not correlated with the models' fundamental capabilities. Moreover, we\nidentify the distance to relevant content and susceptibility to error\npropagation as the key factors influencing multi-turn performance. MT-Eval is\nreleased publicly to encourage future research towards more robust\nconversational models.",
        "pdf_link": "https://arxiv.org/pdf/2401.16745v1.pdf"
    },
    {
        "title": "CodeMind: A Framework to Challenge Large Language Models for Code Reasoning",
        "authors": [
            "Changshu Liu",
            "Shizhuo Dylan Zhang",
            "Ali Reza Ibrahimzada",
            "Reyhaneh Jabbarvand"
        ],
        "published": "2024-02-15T02:24:46Z",
        "summary": "Solely relying on test passing to evaluate Large Language Models (LLMs) for\ncode synthesis may result in unfair assessment or promoting models with data\nleakage. As an alternative, we introduce CodeMind, a framework designed to\ngauge the code reasoning abilities of LLMs. CodeMind currently supports three\ncode reasoning tasks: Independent Execution Reasoning (IER), Dependent\nExecution Reasoning (DER), and Specification Reasoning (SR). The first two\nevaluate models to predict the execution output of an arbitrary code or code\nthe model could correctly synthesize. The third one evaluates the extent to\nwhich LLMs implement the specified expected behavior.\n  Our extensive evaluation of nine LLMs across five benchmarks in two different\nprogramming languages using CodeMind shows that LLMs fairly follow control flow\nconstructs and, in general, explain how inputs evolve to output, specifically\nfor simple programs and the ones they can correctly synthesize. However, their\nperformance drops for code with higher complexity, non-trivial logical and\narithmetic operators, non-primitive types, and API calls. Furthermore, we\nobserve that, while correlated, specification reasoning (essential for code\nsynthesis) does not imply execution reasoning (essential for broader\nprogramming tasks such as testing and debugging): ranking LLMs based on test\npassing can be different compared to code reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.09664v4.pdf"
    },
    {
        "title": "Zero-Shot Clinical Trial Patient Matching with LLMs",
        "authors": [
            "Michael Wornow",
            "Alejandro Lozano",
            "Dev Dash",
            "Jenelle Jindal",
            "Kenneth W. Mahaffey",
            "Nigam H. Shah"
        ],
        "published": "2024-02-05T00:06:08Z",
        "summary": "Matching patients to clinical trials is a key unsolved challenge in bringing\nnew drugs to market. Today, identifying patients who meet a trial's eligibility\ncriteria is highly manual, taking up to 1 hour per patient. Automated screening\nis challenging, however, as it requires understanding unstructured clinical\ntext. Large language models (LLMs) offer a promising solution. In this work, we\nexplore their application to trial matching. First, we design an LLM-based\nsystem which, given a patient's medical history as unstructured clinical text,\nevaluates whether that patient meets a set of inclusion criteria (also\nspecified as free text). Our zero-shot system achieves state-of-the-art scores\non the n2c2 2018 cohort selection benchmark. Second, we improve the data and\ncost efficiency of our method by identifying a prompting strategy which matches\npatients an order of magnitude faster and more cheaply than the status quo, and\ndevelop a two-stage retrieval pipeline that reduces the number of tokens\nprocessed by up to a third while retaining high performance. Third, we evaluate\nthe interpretability of our system by having clinicians evaluate the natural\nlanguage justifications generated by the LLM for each eligibility decision, and\nshow that it can output coherent explanations for 97% of its correct decisions\nand 75% of its incorrect ones. Our results establish the feasibility of using\nLLMs to accelerate clinical trial operations.",
        "pdf_link": "https://arxiv.org/pdf/2402.05125v3.pdf"
    },
    {
        "title": "Pragmatic Instruction Following and Goal Assistance via Cooperative Language-Guided Inverse Planning",
        "authors": [
            "Tan Zhi-Xuan",
            "Lance Ying",
            "Vikash Mansinghka",
            "Joshua B. Tenenbaum"
        ],
        "published": "2024-02-27T23:06:53Z",
        "summary": "People often give instructions whose meaning is ambiguous without further\ncontext, expecting that their actions or goals will disambiguate their\nintentions. How can we build assistive agents that follow such instructions in\na flexible, context-sensitive manner? This paper introduces cooperative\nlanguage-guided inverse plan search (CLIPS), a Bayesian agent architecture for\npragmatic instruction following and goal assistance. Our agent assists a human\nby modeling them as a cooperative planner who communicates joint plans to the\nassistant, then performs multimodal Bayesian inference over the human's goal\nfrom actions and language, using large language models (LLMs) to evaluate the\nlikelihood of an instruction given a hypothesized plan. Given this posterior,\nour assistant acts to minimize expected goal achievement cost, enabling it to\npragmatically follow ambiguous instructions and provide effective assistance\neven when uncertain about the goal. We evaluate these capabilities in two\ncooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that\nCLIPS significantly outperforms GPT-4V, LLM-based literal instruction following\nand unimodal inverse planning in both accuracy and helpfulness, while closely\nmatching the inferences and assistive judgments provided by human raters.",
        "pdf_link": "https://arxiv.org/pdf/2402.17930v1.pdf"
    },
    {
        "title": "Scaling Sparse Fine-Tuning to Large Language Models",
        "authors": [
            "Alan Ansell",
            "Ivan Vuli\u0107",
            "Hannah Sterz",
            "Anna Korhonen",
            "Edoardo M. Ponti"
        ],
        "published": "2024-01-29T18:43:49Z",
        "summary": "Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with\ninstructions or human feedback) due to their sheer number of parameters. A\nfamily of parameter-efficient sparse fine-tuning methods have proven promising\nin terms of performance but their memory requirements increase proportionally\nto the size of the LLMs. In this work, we scale sparse fine-tuning to\nstate-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse\nfine-tuning method which, for a desired density level, maintains an array of\nparameter indices and the deltas of these parameters relative to their\npretrained values. It iterates over: (a) updating the active deltas, (b)\npruning indices (based on the change of magnitude of their deltas) and (c)\nregrowth of indices. For regrowth, we explore two criteria based on either the\naccumulated gradients of a few candidate parameters or their approximate\nmomenta estimated using the efficient SM3 optimizer. We experiment with\ninstruction-tuning of LLMs on standard dataset mixtures, finding that SpIEL is\noften superior to popular parameter-efficient fine-tuning methods like LoRA\n(low-rank adaptation) in terms of performance and comparable in terms of run\ntime. We additionally show that SpIEL is compatible with both quantization and\nefficient optimizers, to facilitate scaling to ever-larger model sizes. We\nrelease the code for SpIEL at https://github.com/AlanAnsell/peft and for the\ninstruction-tuning experiments at https://github.com/ducdauge/sft-llm.",
        "pdf_link": "https://arxiv.org/pdf/2401.16405v2.pdf"
    },
    {
        "title": "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Huanyu Liu",
            "Zhi Jin",
            "Ge Li"
        ],
        "published": "2024-02-24T23:54:41Z",
        "summary": "Recent statements about the impressive capabilities of large language models\n(LLMs) are usually supported by evaluating on open-access benchmarks.\nConsidering the vast size and wide-ranging sources of LLMs' training data, it\ncould explicitly or implicitly include test data, leading to LLMs being more\nsusceptible to data contamination. However, due to the opacity of training\ndata, the black-box access of models, and the rapid growth of synthetic\ntraining data, detecting and mitigating data contamination for LLMs faces\nsignificant challenges. In this paper, we propose CDD, which stands for\nContamination Detection via output Distribution for LLMs. CDD necessitates only\nthe sampled texts to detect data contamination, by identifying the peakedness\nof LLM's output distribution. To mitigate the impact of data contamination in\nevaluation, we also present TED: Trustworthy Evaluation via output\nDistribution, based on the correction of LLM's output distribution. To\nfacilitate this study, we introduce two benchmarks, i.e., DetCon and ComiEval,\nfor data contamination detection and contamination mitigation evaluation tasks.\nExtensive experimental results show that CDD achieves the average relative\nimprovements of 21.8\\%-30.2\\% over other contamination detection approaches in\nterms of Accuracy, F1 Score, and AUC metrics, and can effectively detect\ncontamination caused by the variants of test data. TED significantly mitigates\nperformance improvements up to 66.9\\% attributed to data contamination across\n24 settings and 21 contamination degrees. In real-world applications, we reveal\nthat ChatGPT exhibits a high potential to suffer from data contamination on\nHumanEval benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2402.15938v1.pdf"
    },
    {
        "title": "Wordflow: Social Prompt Engineering for Large Language Models",
        "authors": [
            "Zijie J. Wang",
            "Aishwarya Chakravarthy",
            "David Munechika",
            "Duen Horng Chau"
        ],
        "published": "2024-01-25T18:58:11Z",
        "summary": "Large language models (LLMs) require well-crafted prompts for effective use.\nPrompt engineering, the process of designing prompts, is challenging,\nparticularly for non-experts who are less familiar with AI technologies. While\nresearchers have proposed techniques and tools to assist LLM users in prompt\ndesign, these works primarily target AI application developers rather than\nnon-experts. To address this research gap, we propose social prompt\nengineering, a novel paradigm that leverages social computing techniques to\nfacilitate collaborative prompt design. To investigate social prompt\nengineering, we introduce Wordflow, an open-source and social text editor that\nenables everyday users to easily create, run, share, and discover LLM prompts.\nAdditionally, by leveraging modern web technologies, Wordflow allows users to\nrun LLMs locally and privately in their browsers. Two usage scenarios highlight\nhow social prompt engineering and our tool can enhance laypeople's interaction\nwith LLMs. Wordflow is publicly accessible at\nhttps://poloclub.github.io/wordflow.",
        "pdf_link": "https://arxiv.org/pdf/2401.14447v1.pdf"
    },
    {
        "title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models",
        "authors": [
            "Shuai Wang",
            "Liang Ding",
            "Li Shen",
            "Yong Luo",
            "Bo Du",
            "Dacheng Tao"
        ],
        "published": "2024-01-12T15:21:36Z",
        "summary": "Advancing automated programming necessitates robust and comprehensive code\ngeneration benchmarks, yet current evaluation frameworks largely neglect\nobject-oriented programming (OOP) in favor of functional programming (FP),\ne.g., HumanEval and MBPP. To address this, our study introduces a pioneering\nOOP-focused benchmark, featuring 431 Python programs that encompass essential\nOOP concepts and features like classes and encapsulation methods. We propose a\nnovel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k\nmeasures. Our evaluation of 23 leading large language models (LLMs), including\nboth general and code-specialized models, reveals three key insights: 1) pass@o\noffers a more relevant and comprehensive assessment for OOP code generation; 2)\nDespite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP\ncompared to models like ChatGPT; 3) The poor performance of all advanced LLMs\non our OOP benchmark highlights a critical need for improvements in this field.\nOur benchmark and scripts are publicly released at:\nhttps://github.com/alphadl/OOP-eval.",
        "pdf_link": "https://arxiv.org/pdf/2401.06628v2.pdf"
    },
    {
        "title": "Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability",
        "authors": [
            "Md Sadman Sakib",
            "Yu Sun"
        ],
        "published": "2024-01-15T18:01:59Z",
        "summary": "The inherent probabilistic nature of Large Language Models (LLMs) introduces\nan element of unpredictability, raising concerns about potential discrepancies\nin their output. This paper introduces an innovative approach aims to generate\ncorrect and optimal robotic task plans for diverse real-world demands and\nscenarios. LLMs have been used to generate task plans, but they are unreliable\nand may contain wrong, questionable, or high-cost steps. The proposed approach\nuses LLM to generate a number of task plans as trees and amalgamates them into\na graph by removing questionable paths. Then an optimal task tree can be\nretrieved to circumvent questionable and high-cost nodes, thereby improving\nplanning accuracy and execution efficiency. The approach is further improved by\nincorporating a large knowledge network. Leveraging GPT-4 further, the\nhigh-level task plan is converted into a low-level Planning Domain Definition\nLanguage (PDDL) plan executable by a robot. Evaluation results highlight the\nsuperior accuracy and efficiency of our approach compared to previous\nmethodologies in the field of task planning.",
        "pdf_link": "https://arxiv.org/pdf/2401.07868v1.pdf"
    },
    {
        "title": "TELLER: A Trustworthy Framework for Explainable, Generalizable and Controllable Fake News Detection",
        "authors": [
            "Hui Liu",
            "Wenya Wang",
            "Haoru Li",
            "Haoliang Li"
        ],
        "published": "2024-02-12T16:41:54Z",
        "summary": "The proliferation of fake news has emerged as a severe societal problem,\nraising significant interest from industry and academia. While existing\ndeep-learning based methods have made progress in detecting fake news\naccurately, their reliability may be compromised caused by the non-transparent\nreasoning processes, poor generalization abilities and inherent risks of\nintegration with large language models (LLMs). To address this challenge, we\npropose {\\methodname}, a novel framework for trustworthy fake news detection\nthat prioritizes explainability, generalizability and controllability of\nmodels. This is achieved via a dual-system framework that integrates cognition\nand decision systems, adhering to the principles above. The cognition system\nharnesses human expertise to generate logical predicates, which guide LLMs in\ngenerating human-readable logic atoms. Meanwhile, the decision system deduces\ngeneralizable logic rules to aggregate these atoms, enabling the identification\nof the truthfulness of the input news across diverse domains and enhancing\ntransparency in the decision-making process. Finally, we present comprehensive\nevaluation results on four datasets, demonstrating the feasibility and\ntrustworthiness of our proposed framework. Our implementation is available at\n\\url{https://github.com/less-and-less-bugs/Trust_TELLER}.",
        "pdf_link": "https://arxiv.org/pdf/2402.07776v1.pdf"
    },
    {
        "title": "De-identification is not always enough",
        "authors": [
            "Atiquer Rahman Sarkar",
            "Yao-Shun Chuang",
            "Noman Mohammed",
            "Xiaoqian Jiang"
        ],
        "published": "2024-01-31T21:14:01Z",
        "summary": "For sharing privacy-sensitive data, de-identification is commonly regarded as\nadequate for safeguarding privacy. Synthetic data is also being considered as a\nprivacy-preserving alternative. Recent successes with numerical and tabular\ndata generative models and the breakthroughs in large generative language\nmodels raise the question of whether synthetically generated clinical notes\ncould be a viable alternative to real notes for research purposes. In this\nwork, we demonstrated that (i) de-identification of real clinical notes does\nnot protect records against a membership inference attack, (ii) proposed a\nnovel approach to generate synthetic clinical notes using the current\nstate-of-the-art large language models, (iii) evaluated the performance of the\nsynthetically generated notes in a clinical domain task, and (iv) proposed a\nway to mount a membership inference attack where the target model is trained\nwith synthetic data. We observed that when synthetically generated notes\nclosely match the performance of real data, they also exhibit similar privacy\nconcerns to the real data. Whether other approaches to synthetically generated\nclinical notes could offer better trade-offs and become a better alternative to\nsensitive real notes warrants further investigation.",
        "pdf_link": "https://arxiv.org/pdf/2402.00179v1.pdf"
    },
    {
        "title": "VisionGPT: LLM-Assisted Real-Time Anomaly Detection for Safe Visual Navigation",
        "authors": [
            "Hao Wang",
            "Jiayou Qin",
            "Ashish Bastola",
            "Xiwen Chen",
            "John Suchanek",
            "Zihao Gong",
            "Abolfazl Razi"
        ],
        "published": "2024-03-19T03:55:39Z",
        "summary": "This paper explores the potential of Large Language Models(LLMs) in zero-shot\nanomaly detection for safe visual navigation. With the assistance of the\nstate-of-the-art real-time open-world object detection model Yolo-World and\nspecialized prompts, the proposed framework can identify anomalies within\ncamera-captured frames that include any possible obstacles, then generate\nconcise, audio-delivered descriptions emphasizing abnormalities, assist in safe\nvisual navigation in complex circumstances. Moreover, our proposed framework\nleverages the advantages of LLMs and the open-vocabulary object detection model\nto achieve the dynamic scenario switch, which allows users to transition\nsmoothly from scene to scene, which addresses the limitation of traditional\nvisual navigation. Furthermore, this paper explored the performance\ncontribution of different prompt components, provided the vision for future\nimprovement in visual accessibility, and paved the way for LLMs in video\nanomaly detection and vision-language understanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.12415v1.pdf"
    },
    {
        "title": "RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners",
        "authors": [
            "Chi Hu",
            "Yuan Ge",
            "Xiangnan Ma",
            "Hang Cao",
            "Qiang Li",
            "Yonghua Yang",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "published": "2024-03-19T02:34:18Z",
        "summary": "Large Language Models (LLMs) have achieved impressive performance across\nvarious reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT\nare prone to logical errors during their reasoning processes. Existing\nsolutions, such as deploying task-specific verifiers or voting over multiple\nreasoning paths, either require extensive human annotations or fail in\nscenarios with inconsistent responses. To address these challenges, we\nintroduce RankPrompt, a new prompting method that enables LLMs to self-rank\ntheir responses without additional resources. RankPrompt breaks down the\nranking problem into a series of comparisons among diverse responses,\nleveraging the inherent capabilities of LLMs to generate chains of comparison\nas contextual exemplars. Our experiments across 11 arithmetic and commonsense\nreasoning tasks show that RankPrompt significantly enhances the reasoning\nperformance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover,\nRankPrompt excels in LLM-based automatic evaluations for open-ended tasks,\naligning with human judgments 74% of the time in the AlpacaEval dataset. It\nalso exhibits robustness to variations in response order and consistency.\nCollectively, our results validate RankPrompt as an effective method for\neliciting high-quality feedback from language models.",
        "pdf_link": "https://arxiv.org/pdf/2403.12373v3.pdf"
    },
    {
        "title": "Panacea: Pareto Alignment via Preference Adaptation for LLMs",
        "authors": [
            "Yifan Zhong",
            "Chengdong Ma",
            "Xiaoyuan Zhang",
            "Ziran Yang",
            "Qingfu Zhang",
            "Siyuan Qi",
            "Yaodong Yang"
        ],
        "published": "2024-02-03T05:01:04Z",
        "summary": "Current methods for large language model alignment typically use scalar human\npreference labels. However, this convention tends to oversimplify the\nmulti-dimensional and heterogeneous nature of human preferences, leading to\nreduced expressivity and even misalignment. This paper presents Panacea, an\ninnovative approach that reframes alignment as a multi-dimensional preference\noptimization problem. Panacea trains a single model capable of adapting online\nand Pareto-optimally to diverse sets of preferences without the need for\nfurther tuning. A major challenge here is using a low-dimensional preference\nvector to guide the model's behavior, despite it being governed by an\noverwhelmingly large number of parameters. To address this, Panacea is designed\nto use singular value decomposition (SVD)-based low-rank adaptation, which\nallows the preference vector to be simply injected online as singular values.\nTheoretically, we prove that Panacea recovers the entire Pareto front with\ncommon loss aggregation methods under mild conditions. Moreover, our\nexperiments demonstrate, for the first time, the feasibility of aligning a\nsingle LLM to represent a spectrum of human preferences through various\noptimization methods. Our work marks a step forward in effectively and\nefficiently aligning models to diverse and intricate human preferences in a\ncontrollable and Pareto-optimal manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.02030v1.pdf"
    },
    {
        "title": "The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning",
        "authors": [
            "Nathaniel Li",
            "Alexander Pan",
            "Anjali Gopal",
            "Summer Yue",
            "Daniel Berrios",
            "Alice Gatti",
            "Justin D. Li",
            "Ann-Kathrin Dombrowski",
            "Shashwat Goel",
            "Long Phan",
            "Gabriel Mukobi",
            "Nathan Helm-Burger",
            "Rassin Lababidi",
            "Lennart Justen",
            "Andrew B. Liu",
            "Michael Chen",
            "Isabelle Barrass",
            "Oliver Zhang",
            "Xiaoyuan Zhu",
            "Rishub Tamirisa",
            "Bhrugu Bharathi",
            "Adam Khoja",
            "Zhenqi Zhao",
            "Ariel Herbert-Voss",
            "Cort B. Breuer",
            "Andy Zou",
            "Mantas Mazeika",
            "Zifan Wang",
            "Palash Oswal",
            "Weiran Liu",
            "Adam A. Hunt",
            "Justin Tienken-Harder",
            "Kevin Y. Shih",
            "Kemper Talley",
            "John Guan",
            "Russell Kaplan",
            "Ian Steneker",
            "David Campbell",
            "Brad Jokubaitis",
            "Alex Levinson",
            "Jean Wang",
            "William Qian",
            "Kallol Krishna Karmakar",
            "Steven Basart",
            "Stephen Fitz",
            "Mindy Levine",
            "Ponnurangam Kumaraguru",
            "Uday Tupakula",
            "Vijay Varadharajan",
            "Yan Shoshitaishvili",
            "Jimmy Ba",
            "Kevin M. Esvelt",
            "Alexandr Wang",
            "Dan Hendrycks"
        ],
        "published": "2024-03-05T18:59:35Z",
        "summary": "The White House Executive Order on Artificial Intelligence highlights the\nrisks of large language models (LLMs) empowering malicious actors in developing\nbiological, cyber, and chemical weapons. To measure these risks of malicious\nuse, government institutions and major AI labs are developing evaluations for\nhazardous capabilities in LLMs. However, current evaluations are private,\npreventing further research into mitigating risk. Furthermore, they focus on\nonly a few, highly specific pathways for malicious use. To fill these gaps, we\npublicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a\ndataset of 4,157 multiple-choice questions that serve as a proxy measurement of\nhazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP\nwas developed by a consortium of academics and technical consultants, and was\nstringently filtered to eliminate sensitive information prior to public\nrelease. WMDP serves two roles: first, as an evaluation for hazardous knowledge\nin LLMs, and second, as a benchmark for unlearning methods to remove such\nhazardous knowledge. To guide progress on unlearning, we develop CUT, a\nstate-of-the-art unlearning method based on controlling model representations.\nCUT reduces model performance on WMDP while maintaining general capabilities in\nareas such as biology and computer science, suggesting that unlearning may be a\nconcrete path towards reducing malicious use from LLMs. We release our\nbenchmark and code publicly at https://wmdp.ai",
        "pdf_link": "https://arxiv.org/pdf/2403.03218v2.pdf"
    },
    {
        "title": "Model Compression and Efficient Inference for Large Language Models: A Survey",
        "authors": [
            "Wenxiao Wang",
            "Wei Chen",
            "Yicong Luo",
            "Yongliu Long",
            "Zhengkai Lin",
            "Liye Zhang",
            "Binbin Lin",
            "Deng Cai",
            "Xiaofei He"
        ],
        "published": "2024-02-15T06:58:30Z",
        "summary": "Transformer based large language models have achieved tremendous success.\nHowever, the significant memory and computational costs incurred during the\ninference process make it challenging to deploy large models on\nresource-constrained devices. In this paper, we investigate compression and\nefficient inference methods for large language models from an algorithmic\nperspective. Regarding taxonomy, similar to smaller models, compression and\nacceleration algorithms for large language models can still be categorized into\nquantization, pruning, distillation, compact architecture design, dynamic\nnetworks. However, Large language models have two prominent characteristics\ncompared to smaller models: (1) Most of compression algorithms require\nfinetuning or even retraining the model after compression. The most notable\naspect of large models is the very high cost associated with model finetuning\nor training. Therefore, many algorithms for large models, such as quantization\nand pruning, start to explore tuning-free algorithms. (2) Large models\nemphasize versatility and generalization rather than performance on a single\ntask. Hence, many algorithms, such as knowledge distillation, focus on how to\npreserving their versatility and generalization after compression. Since these\ntwo characteristics were not very pronounced in early large models, we further\ndistinguish large language models into medium models and ``real'' large models.\nAdditionally, we also provide an introduction to some mature frameworks for\nefficient inference of large models, which can support basic compression or\nacceleration algorithms, greatly facilitating model deployment for users.",
        "pdf_link": "https://arxiv.org/pdf/2402.09748v1.pdf"
    },
    {
        "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models",
        "authors": [
            "Jun Gao",
            "Huan Zhao",
            "Wei Wang",
            "Changlong Yu",
            "Ruifeng Xu"
        ],
        "published": "2024-02-18T02:41:06Z",
        "summary": "In this study, we present EventRL, a reinforcement learning approach\ndeveloped to enhance event extraction for large language models (LLMs). EventRL\nutilizes outcome supervision with specific reward functions to tackle prevalent\nchallenges in LLMs, such as instruction following and hallucination, manifested\nas the mismatch of event structure and the generation of undefined event types.\nWe evaluate EventRL against existing methods like Few-Shot Prompting (FSP)\n(based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including\nGPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL\nsignificantly outperforms these conventional approaches by improving the\nperformance in identifying and structuring events, particularly in handling\nnovel event types. The study emphasizes the critical role of reward function\nselection and demonstrates the benefits of incorporating code data for better\nevent extraction. While increasing model size leads to higher accuracy,\nmaintaining the ability to generalize is essential to avoid overfitting.",
        "pdf_link": "https://arxiv.org/pdf/2402.11430v1.pdf"
    },
    {
        "title": "Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs",
        "authors": [
            "Xiaoxia Li",
            "Siyuan Liang",
            "Jiyi Zhang",
            "Han Fang",
            "Aishan Liu",
            "Ee-Chien Chang"
        ],
        "published": "2024-02-21T15:13:50Z",
        "summary": "Large Language Models (LLMs), used in creative writing, code generation, and\ntranslation, generate text based on input sequences but are vulnerable to\njailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak\nprompt methods use a combination of jailbreak templates followed by questions\nto ask to create jailbreak prompts. However, existing jailbreak prompt designs\ngenerally suffer from excessive semantic differences, resulting in an inability\nto resist defenses that use simple semantic metrics as thresholds. Jailbreak\nprompts are semantically more varied than the original questions used for\nqueries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach\nthat bypasses LLMs by generating jailbreak prompts that are semantically\nsimilar to the original question. We model the search for jailbreak prompts\nthat satisfy both semantic similarity and jailbreak validity as a\nmulti-objective optimization problem and employ a standardized set of genetic\nalgorithms for generating eligible prompts. Compared to the baseline\nAutoDAN-GA, SMJ achieves attack success rates (ASR) that are at most 35.4%\nhigher without ONION defense and 85.2% higher with ONION defense. SMJ's better\nperformance in all three semantic meaningfulness metrics of Jailbreak Prompt,\nSimilarity, and Outlier, also means that SMJ is resistant to defenses that use\nthose metrics as thresholds.",
        "pdf_link": "https://arxiv.org/pdf/2402.14872v2.pdf"
    },
    {
        "title": "LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning",
        "authors": [
            "Azmine Toushik Wasi",
            "Mst Rafia Islam",
            "Raima Islam"
        ],
        "published": "2024-03-20T21:06:42Z",
        "summary": "Sense of ownership in writing confines our investment of thoughts, time, and\ncontribution, leading to attachment to the output. However, using writing\nassistants introduces a mental dilemma, as some content isn't directly our\ncreation. For instance, we tend to credit Large Language Models (LLMs) more in\ncreative tasks, even though all tasks are equal for them. Additionally, while\nwe may not claim complete ownership of LLM-generated content, we freely claim\nauthorship. We conduct a short survey to examine these issues and understand\nunderlying cognitive processes in order to gain a better knowledge of\nhuman-computer interaction in writing and improve writing aid systems.",
        "pdf_link": "https://arxiv.org/pdf/2404.00027v2.pdf"
    },
    {
        "title": "Transformers are Multi-State RNNs",
        "authors": [
            "Matanel Oren",
            "Michael Hassid",
            "Yossi Adi",
            "Roy Schwartz"
        ],
        "published": "2024-01-11T18:35:26Z",
        "summary": "Transformers are considered conceptually different compared to the previous\ngeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).\nIn this work, we demonstrate that decoder-only transformers can in fact be\nconceptualized as infinite multi-state RNNs - an RNN variant with unlimited\nhidden state size. We further show that pretrained transformers can be\nconverted into $\\textit{finite}$ multi-state RNNs by fixing the size of their\nhidden state. We observe that several existing transformers cache compression\ntechniques can be framed as such conversion policies, and introduce a novel\npolicy, TOVA, which is simpler compared to these policies. Our experiments with\nseveral long range tasks indicate that TOVA outperforms all other baseline\npolicies, while being nearly on par with the full (infinite) model, and using\nin some cases only $\\frac{1}{8}$ of the original cache size. Our results\nindicate that transformer decoder LLMs often behave in practice as RNNs. They\nalso lay out the option of mitigating one of their most painful computational\nbottlenecks - the size of their cache memory. We publicly release our code at\nhttps://github.com/schwartz-lab-NLP/TOVA.",
        "pdf_link": "https://arxiv.org/pdf/2401.06104v1.pdf"
    },
    {
        "title": "Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method",
        "authors": [
            "Tian Xia",
            "Zhiwei He",
            "Tong Ren",
            "Yibo Miao",
            "Zhuosheng Zhang",
            "Yang Yang",
            "Rui Wang"
        ],
        "published": "2024-02-24T13:36:58Z",
        "summary": "Bargaining is an important and unique part of negotiation between humans. As\nLLM-driven agents learn to negotiate and act like real humans, how to evaluate\nagents' bargaining abilities remains an open problem. For the first time, we\nformally described the Bargaining task as an asymmetric incomplete information\ngame, defining the gains of the Buyer and Seller in multiple bargaining\nprocesses. It allows us to quantitatively assess an agent's performance in the\nBargain task. We collected a real product price dataset, AmazonHistoryPrice,\nand conducted evaluations of various LLM agents' bargaining abilities. We find\nthat playing a Buyer is much harder than a Seller, and increasing model size\ncan not effectively improve the Buyer's performance. To address the challenge,\nwe propose a novel approach called OG-Narrator that integrates a deterministic\nOffer Generator to control the price range of Buyer's offers, and an LLM\nNarrator to create natural language sentences for generated offers.\nExperimental results show that OG-Narrator improves the buyer's deal rates from\n26.67% to 88.88% and brings a ten times of multiplication of profits on all\nbaselines, even a model that has not been aligned.",
        "pdf_link": "https://arxiv.org/pdf/2402.15813v2.pdf"
    },
    {
        "title": "AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern Doctors for Clinical Diagnosis",
        "authors": [
            "Zhihao Fan",
            "Jialong Tang",
            "Wei Chen",
            "Siyuan Wang",
            "Zhongyu Wei",
            "Jun Xi",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "published": "2024-02-15T06:46:48Z",
        "summary": "The incorporation of Large Language Models (LLMs) in healthcare marks a\nsignificant advancement. However, the application has predominantly been\nlimited to discriminative and question-answering tasks, which does not fully\nleverage their interactive potential. To address this limitation, our paper\npresents AI Hospital, a framework designed to build a real-time interactive\ndiagnosis environment. To simulate the procedure, we collect high-quality\nmedical records to create patient, examiner, and medical director agents. AI\nHospital is then utilized for the interactive evaluation and collaboration of\nLLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark\nwhere various LLMs serve as intern doctors for interactive diagnosis.\nSubsequently, to improve diagnostic accuracy, we introduce a collaborative\nmechanism that involves iterative discussions and a dispute resolution process\nunder the supervision of the medical director. In our experiments, we validate\nthe reliability of AI Hospital. The results not only explore the feasibility of\napply LLMs in clinical consultation but also confirm the effectiveness of the\ndispute resolution focused collaboration method.",
        "pdf_link": "https://arxiv.org/pdf/2402.09742v2.pdf"
    },
    {
        "title": "Towards Democratized Flood Risk Management: An Advanced AI Assistant Enabled by GPT-4 for Enhanced Interpretability and Public Engagement",
        "authors": [
            "Rafaela Martelo",
            "Ruo-Qian Wang"
        ],
        "published": "2024-03-05T18:24:52Z",
        "summary": "Real-time flood forecasting plays a crucial role in enabling timely and\neffective emergency responses. However, a significant challenge lies in\nbridging the gap between complex numerical flood models and practical\ndecision-making. Decision-makers often rely on experts to interpret these\nmodels for optimizing flood mitigation strategies. And the public requires\ncomplex techniques to inquiry and understand socio-cultural and institutional\nfactors, often hinders the public's understanding of flood risks. To overcome\nthese challenges, our study introduces an innovative solution: a customized AI\nAssistant powered by the GPT-4 Large Language Model. This AI Assistant is\ndesigned to facilitate effective communication between decision-makers, the\ngeneral public, and flood forecasters, without the requirement of specialized\nknowledge. The new framework utilizes GPT-4's advanced natural language\nunderstanding and function calling capabilities to provide immediate flood\nalerts and respond to various flood-related inquiries. Our developed prototype\nintegrates real-time flood warnings with flood maps and social vulnerability\ndata. It also effectively translates complex flood zone information into\nactionable risk management advice. To assess its performance, we evaluated the\nprototype using six criteria within three main categories: relevance, error\nresilience, and understanding of context. Our research marks a significant step\ntowards a more accessible and user-friendly approach in flood risk management.\nThis study highlights the potential of advanced AI tools like GPT-4 in\ndemocratizing information and enhancing public engagement in critical social\nand environmental issues.",
        "pdf_link": "https://arxiv.org/pdf/2403.03188v1.pdf"
    },
    {
        "title": "Large Language Model-Based Evolutionary Optimizer: Reasoning with elitism",
        "authors": [
            "Shuvayan Brahmachary",
            "Subodh M. Joshi",
            "Aniruddha Panda",
            "Kaushik Koneripalli",
            "Arun Kumar Sagotra",
            "Harshil Patel",
            "Ankush Sharma",
            "Ameya D. Jagtap",
            "Kaushic Kalyanaraman"
        ],
        "published": "2024-03-04T13:57:37Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning\nabilities, prompting interest in their application as black-box optimizers.\nThis paper asserts that LLMs possess the capability for zero-shot optimization\nacross diverse scenarios, including multi-objective and high-dimensional\nproblems. We introduce a novel population-based method for numerical\noptimization using LLMs called Language-Model-Based Evolutionary Optimizer\n(LEO). Our hypothesis is supported through numerical examples, spanning\nbenchmark and industrial engineering problems such as supersonic nozzle shape\noptimization, heat transfer, and windfarm layout optimization. We compare our\nmethod to several gradient-based and gradient-free optimization approaches.\nWhile LLMs yield comparable results to state-of-the-art methods, their\nimaginative nature and propensity to hallucinate demand careful handling. We\nprovide practical guidelines for obtaining reliable answers from LLMs and\ndiscuss method limitations and potential research directions.",
        "pdf_link": "https://arxiv.org/pdf/2403.02054v1.pdf"
    },
    {
        "title": "Large Language Models As Faithful Explainers",
        "authors": [
            "Yu-Neng Chuang",
            "Guanchu Wang",
            "Chia-Yuan Chang",
            "Ruixiang Tang",
            "Fan Yang",
            "Mengnan Du",
            "Xuanting Cai",
            "Xia Hu"
        ],
        "published": "2024-02-07T09:09:14Z",
        "summary": "Large Language Models (LLMs) have recently become proficient in addressing\ncomplex tasks by utilizing their rich internal knowledge and reasoning ability.\nConsequently, this complexity hinders traditional input-focused explanation\nalgorithms for explaining the complex decision-making processes of LLMs. Recent\nadvancements have thus emerged for self-explaining their predictions through a\nsingle feed-forward inference in a natural language format. However, natural\nlanguage explanations are often criticized for lack of faithfulness since these\nexplanations may not accurately reflect the decision-making behaviors of the\nLLMs. In this work, we introduce a generative explanation framework, xLLM, to\nimprove the faithfulness of the explanations provided in natural language\nformats for LLMs. Specifically, we propose an evaluator to quantify the\nfaithfulness of natural language explanation and enhance the faithfulness by an\niterative optimization process of xLLM, with the goal of maximizing the\nfaithfulness scores. Experiments conducted on three NLU datasets demonstrate\nthat xLLM can significantly improve the faithfulness of generated explanations,\nwhich are in alignment with the behaviors of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.04678v1.pdf"
    },
    {
        "title": "AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language Model Outputs",
        "authors": [
            "Sana Ebrahimi",
            "Kaiwen Chen",
            "Abolfazl Asudeh",
            "Gautam Das",
            "Nick Koudas"
        ],
        "published": "2024-03-01T00:02:37Z",
        "summary": "Pre-trained Large Language Models (LLMs) have significantly advanced natural\nlanguage processing capabilities but are susceptible to biases present in their\ntraining data, leading to unfair outcomes in various applications. While\nnumerous strategies have been proposed to mitigate bias, they often require\nextensive computational resources and may compromise model performance. In this\nwork, we introduce AXOLOTL, a novel post-processing framework, which operates\nagnostically across tasks and models, leveraging public APIs to interact with\nLLMs without direct access to internal parameters. Through a three-step process\nresembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions,\nand guides the model to self-debias its outputs. This approach minimizes\ncomputational costs and preserves model performance, making AXOLOTL a promising\ntool for debiasing LLM outputs with broad applicability and ease of use.",
        "pdf_link": "https://arxiv.org/pdf/2403.00198v1.pdf"
    },
    {
        "title": "DevEval: Evaluating Code Generation in Practical Software Projects",
        "authors": [
            "Jia Li",
            "Ge Li",
            "Yunfei Zhao",
            "Yongmin Li",
            "Zhi Jin",
            "Hao Zhu",
            "Huanyu Liu",
            "Kaibo Liu",
            "Lecheng Wang",
            "Zheng Fang",
            "Lanshen Wang",
            "Jiazheng Ding",
            "Xuanming Zhang",
            "Yihong Dong",
            "Yuqi Zhu",
            "Bin Gu",
            "Mengfei Yang"
        ],
        "published": "2024-01-12T06:51:30Z",
        "summary": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Many benchmarks have been proposed but are inconsistent with\npractical software projects, e.g., unreal program distributions, insufficient\ndependencies, and small-scale project contexts. Thus, the capabilities of LLMs\nin practical projects are still unclear. In this paper, we propose a new\nbenchmark named DevEval, aligned with Developers' experiences in practical\nprojects. DevEval is collected through a rigorous pipeline, containing 2,690\nsamples from 119 practical projects and covering 10 domains. Compared to\nprevious benchmarks, DevEval aligns to practical projects in multiple\ndimensions, e.g., real program distributions, sufficient dependencies, and\nenough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,\ngpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual\nabilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo\nonly is 42 in our experiments. We also discuss the challenges and future\ndirections of code generation in practical projects. We open-source DevEval and\nhope it can facilitate the development of code generation in practical\nprojects.",
        "pdf_link": "https://arxiv.org/pdf/2401.06401v4.pdf"
    },
    {
        "title": "LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments",
        "authors": [
            "Maonan Wang",
            "Aoyu Pang",
            "Yuheng Kan",
            "Man-On Pun",
            "Chung Shue Chen",
            "Bo Huang"
        ],
        "published": "2024-03-13T08:41:55Z",
        "summary": "Traffic congestion in metropolitan areas presents a formidable challenge with\nfar-reaching economic, environmental, and societal ramifications. Therefore,\neffective congestion management is imperative, with traffic signal control\n(TSC) systems being pivotal in this endeavor. Conventional TSC systems,\ndesigned upon rule-based algorithms or reinforcement learning (RL), frequently\nexhibit deficiencies in managing the complexities and variabilities of urban\ntraffic flows, constrained by their limited capacity for adaptation to\nunfamiliar scenarios. In response to these limitations, this work introduces an\ninnovative approach that integrates Large Language Models (LLMs) into TSC,\nharnessing their advanced reasoning and decision-making faculties.\nSpecifically, a hybrid framework that augments LLMs with a suite of perception\nand decision-making tools is proposed, facilitating the interrogation of both\nthe static and dynamic traffic information. This design places the LLM at the\ncenter of the decision-making process, combining external traffic data with\nestablished TSC methods. Moreover, a simulation platform is developed to\ncorroborate the efficacy of the proposed framework. The findings from our\nsimulations attest to the system's adeptness in adjusting to a multiplicity of\ntraffic environments without the need for additional training. Notably, in\ncases of Sensor Outage (SO), our approach surpasses conventional RL-based\nsystems by reducing the average waiting time by $20.4\\%$. This research\nsignifies a notable advance in TSC strategies and paves the way for the\nintegration of LLMs into real-world, dynamic scenarios, highlighting their\npotential to revolutionize traffic management. The related code is available at\n\\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.",
        "pdf_link": "https://arxiv.org/pdf/2403.08337v1.pdf"
    },
    {
        "title": "ReALM: Reference Resolution As Language Modeling",
        "authors": [
            "Joel Ruben Antony Moniz",
            "Soundarya Krishnan",
            "Melis Ozyildirim",
            "Prathamesh Saraf",
            "Halim Cagri Ates",
            "Yuan Zhang",
            "Hong Yu",
            "Nidhi Rajshree"
        ],
        "published": "2024-03-29T17:59:06Z",
        "summary": "Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.",
        "pdf_link": "https://arxiv.org/pdf/2403.20329v1.pdf"
    },
    {
        "title": "From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings",
        "authors": [
            "Hao Wang",
            "Hao Li",
            "Minlie Huang",
            "Lei Sha"
        ],
        "published": "2024-02-25T06:46:27Z",
        "summary": "The safety defense methods of Large language models(LLMs) stays limited\nbecause the dangerous prompts are manually curated to just few known attack\ntypes, which fails to keep pace with emerging varieties. Recent studies found\nthat attaching suffixes to harmful instructions can hack the defense of LLMs\nand lead to dangerous outputs. This method, while effective, leaves a gap in\nunderstanding the underlying mechanics of such adversarial suffix due to the\nnon-readability and it can be relatively easily seen through by common defense\nmethods such as perplexity filters.To cope with this challenge, in this paper,\nwe propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that\nare able to translate the unreadable adversarial suffixes into coherent,\nreadable text, which makes it easier to understand and analyze the reasons\nbehind harmful content generation by large language models. We conducted\nexperiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's\nharmful instructions. The results indicate that our method achieves a much\nbetter attack success rate to existing techniques, while significantly\nenhancing the textual fluency of the prompts. In addition, our approach can be\ngeneralized into a broader method for generating transferable adversarial\nsuffixes that can successfully attack multiple LLMs, even black-box LLMs, such\nas ChatGPT and Gemini. As a result, the prompts generated through our method\nexhibit enriched semantic diversity, which potentially provides more\nadversarial examples for LLM defense methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.16006v1.pdf"
    },
    {
        "title": "Question Translation Training for Better Multilingual Reasoning",
        "authors": [
            "Wenhao Zhu",
            "Shujian Huang",
            "Fei Yuan",
            "Shuaijie She",
            "Jiajun Chen",
            "Alexandra Birch"
        ],
        "published": "2024-01-15T16:39:10Z",
        "summary": "Large language models show compelling performance on reasoning tasks but they\ntend to perform much worse in languages other than English. This is\nunsurprising given that their training data largely consists of English text\nand instructions. A typical solution is to translate instruction data into all\nlanguages of interest, and then train on the resulting multilingual data, which\nis called translate-training. This approach not only incurs high cost, but also\nresults in poorly translated data due to the non-standard formatting of\nmathematical chain-of-thought. In this paper, we explore the benefits of\nquestion alignment, where we train the model to translate reasoning questions\ninto English by finetuning on X-English parallel question data. In this way we\nperform targeted, in-domain language alignment which makes best use of English\ninstruction data to unlock the LLMs' multilingual reasoning abilities.\nExperimental results on LLaMA2-13B show that question alignment leads to\nconsistent improvements over the translate-training approach: an average\nimprovement of 11.3% and 16.1% accuracy across ten languages on the MGSM and\nMSVAMP multilingual reasoning benchmarks. The project will be available at:\nhttps://github.com/NJUNLP/QAlign.",
        "pdf_link": "https://arxiv.org/pdf/2401.07817v2.pdf"
    },
    {
        "title": "ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code Snippets using ChatGPT",
        "authors": [
            "Azmain Kabir",
            "Shaowei Wang",
            "Yuan Tian",
            "Tse-Hsun",
            "Chen",
            "Muhammad Asaduzzaman",
            "Wenbin Zhang"
        ],
        "published": "2024-01-25T16:10:33Z",
        "summary": "Technical question and answering (Q&A) sites such as Stack Overflow have\nbecome an important source for software developers to seek knowledge. However,\ncode snippets on Q&A sites are usually uncompilable and semantically incomplete\nfor compilation due to unresolved types and missing dependent libraries, which\nraises the obstacle for users to reuse or analyze Q&A code snippets. Prior\napproaches either are not designed for synthesizing compilable code or suffer\nfrom a low compilation success rate. To address this problem, we propose ZS4C,\na lightweight approach to perform zero-shot synthesis of compilable code from\nincomplete code snippets using Large Language Model (LLM). ZS4C operates in two\nstages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify\nmissing import statements for a given code snippet, leveraging our designed\ntask-specific prompt template. In the second stage, ZS4C fixes compilation\nerrors caused by incorrect import statements and syntax errors through\ncollaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C\non a widely used benchmark called StatType-SO against the SOTA approach SnR.\nCompared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a\n39.3% improvement. On average, ZS4C can infer more accurate import statements\nthan SnR, with an improvement of 6.6% in the F1.",
        "pdf_link": "https://arxiv.org/pdf/2401.14279v1.pdf"
    },
    {
        "title": "From Prompt Engineering to Prompt Science With Human in the Loop",
        "authors": [
            "Chirag Shah"
        ],
        "published": "2024-01-01T01:37:36Z",
        "summary": "As LLMs make their way into many aspects of our lives, one place that\nwarrants increased scrutiny with LLM usage is scientific research. Using LLMs\nfor generating or analyzing data for research purposes is gaining popularity.\nBut when such application is marred with ad-hoc decisions and engineering\nsolutions, we need to be concerned about how it may affect that research, its\nfindings, or any future works based on that research. We need a more scientific\napproach to using LLMs in our research. While there are several active efforts\nto support more systematic construction of prompts, they are often focused more\non achieving desirable outcomes rather than producing replicable and\ngeneralizable knowledge with sufficient transparency, objectivity, or rigor.\nThis article presents a new methodology inspired by codebook construction\nthrough qualitative methods to address that. Using humans in the loop and a\nmulti-phase verification processes, this methodology lays a foundation for more\nsystematic, objective, and trustworthy way of applying LLMs for analyzing data.\nSpecifically, we show how a set of researchers can work through a rigorous\nprocess of labeling, deliberating, and documenting to remove subjectivity and\nbring transparency and replicability to prompt generation process.",
        "pdf_link": "https://arxiv.org/pdf/2401.04122v2.pdf"
    },
    {
        "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
        "authors": [
            "Didi Zhu",
            "Zhongyi Sun",
            "Zexi Li",
            "Tao Shen",
            "Ke Yan",
            "Shouhong Ding",
            "Kun Kuang",
            "Chao Wu"
        ],
        "published": "2024-02-19T11:02:05Z",
        "summary": "Catastrophic forgetting emerges as a critical challenge when fine-tuning\nmulti-modal large language models (MLLMs), where improving performance on\nunseen tasks often leads to a significant performance drop on the original\ntasks. This paper presents a comprehensive analysis of catastrophic forgetting\nin MLLMs and introduces a post-training adjustment method called Model Tailor.\nOur method primarily preserves the pre-trained parameters while replacing a\nsmall number ($\\leq$ 10\\%) of fine-tuned parameters, maintaining $\\sim$ 99\\%\neffectiveness on original tasks versus pre-training, and achieving $\\sim$ 97\\%\non new tasks compared to standard fine-tuning. Specifically, we derive a sparse\nmask to identify the \"model patch\", based on a fusion strategy that integrates\nsalience and sensitivity analysis. Subsequently, a compensation mechanism is\nintroduced to \"decorate the patch\", enhancing the model's performance on both\ntarget and original tasks. Additionally, our method is adaptable to multi-task\nscenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both\nimage captioning and visual question answering tasks, our approach demonstrates\nsignificant task adaptability while preserving inherent pre-trained\ncapabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.12048v1.pdf"
    },
    {
        "title": "When Large Language Models Meet Vector Databases: A Survey",
        "authors": [
            "Zhi Jing",
            "Yongye Su",
            "Yikun Han",
            "Bo Yuan",
            "Haiyun Xu",
            "Chunjiang Liu",
            "Kehai Chen",
            "Min Zhang"
        ],
        "published": "2024-01-30T23:35:28Z",
        "summary": "This survey explores the synergistic potential of Large Language Models\n(LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving\nresearch area. With the proliferation of LLMs comes a host of challenges,\nincluding hallucinations, outdated knowledge, prohibitive commercial\napplication costs, and memory issues. VecDBs emerge as a compelling solution to\nthese issues by offering an efficient means to store, retrieve, and manage the\nhigh-dimensional vector representations intrinsic to LLM operations. Through\nthis nuanced review, we delineate the foundational principles of LLMs and\nVecDBs and critically analyze their integration's impact on enhancing LLM\nfunctionalities. This discourse extends into a discussion on the speculative\nfuture developments in this domain, aiming to catalyze further research into\noptimizing the confluence of LLMs and VecDBs for advanced data handling and\nknowledge extraction capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.01763v2.pdf"
    },
    {
        "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
        "authors": [
            "Gautier Dagan",
            "Gabriel Synnaeve",
            "Baptiste Rozi\u00e8re"
        ],
        "published": "2024-02-01T21:49:34Z",
        "summary": "Tokenization is an understudied and often neglected component of modern LLMs.\nMost published works use a single tokenizer for all experiments, often borrowed\nfrom another model, without performing ablations or analysis to optimize\ntokenization. Moreover, the tokenizer is generally kept unchanged when\nfine-tuning a base model. In this paper, we show that the size,\npre-tokenization regular expression, and training data of a tokenizer can\nsignificantly impact the model's generation speed, effective context size,\nmemory usage, and downstream performance. We train specialized Byte-Pair\nEncoding code tokenizers, and conduct extensive ablations on the impact of\ntokenizer design on the performance of LLMs for code generation tasks such as\nHumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters\nselection and switching the tokenizer in a pre-trained LLM. We perform our\nexperiments on models trained from scratch and from pre-trained models,\nverifying their applicability to a wide range of use-cases. We find that when\nfine-tuning on more than 50 billion tokens, we can specialize the tokenizer of\na pre-trained LLM to obtain large gains in generation speed and effective\ncontext size.",
        "pdf_link": "https://arxiv.org/pdf/2402.01035v2.pdf"
    },
    {
        "title": "An Autonomous Large Language Model Agent for Chemical Literature Data Mining",
        "authors": [
            "Kexin Chen",
            "Hanqun Cao",
            "Junyou Li",
            "Yuyang Du",
            "Menghao Guo",
            "Xin Zeng",
            "Lanqing Li",
            "Jiezhong Qiu",
            "Pheng Ann Heng",
            "Guangyong Chen"
        ],
        "published": "2024-02-20T13:21:46Z",
        "summary": "Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.",
        "pdf_link": "https://arxiv.org/pdf/2402.12993v1.pdf"
    },
    {
        "title": "Best Practices for Text Annotation with Large Language Models",
        "authors": [
            "Petter T\u00f6rnberg"
        ],
        "published": "2024-02-05T15:43:50Z",
        "summary": "Large Language Models (LLMs) have ushered in a new era of text annotation, as\ntheir ease-of-use, high accuracy, and relatively low costs have meant that\ntheir use has exploded in recent months. However, the rapid growth of the field\nhas meant that LLM-based annotation has become something of an academic Wild\nWest: the lack of established practices and standards has led to concerns about\nthe quality and validity of research. Researchers have warned that the\nostensible simplicity of LLMs can be misleading, as they are prone to bias,\nmisunderstandings, and unreliable results. Recognizing the transformative\npotential of LLMs, this paper proposes a comprehensive set of standards and\nbest practices for their reliable, reproducible, and ethical use. These\nguidelines span critical areas such as model selection, prompt engineering,\nstructured prompting, prompt stability analysis, rigorous model validation, and\nthe consideration of ethical and legal implications. The paper emphasizes the\nneed for a structured, directed, and formalized approach to using LLMs, aiming\nto ensure the integrity and robustness of text annotation practices, and\nadvocates for a nuanced and critical engagement with LLMs in social scientific\nresearch.",
        "pdf_link": "https://arxiv.org/pdf/2402.05129v1.pdf"
    },
    {
        "title": "Tastle: Distract Large Language Models for Automatic Jailbreak Attack",
        "authors": [
            "Zeguan Xiao",
            "Yan Yang",
            "Guanhua Chen",
            "Yun Chen"
        ],
        "published": "2024-03-13T11:16:43Z",
        "summary": "Large language models (LLMs) have achieved significant advances in recent\ndays. Extensive efforts have been made before the public release of LLMs to\nalign their behaviors with human values. The primary goal of alignment is to\nensure their helpfulness, honesty and harmlessness. However, even meticulously\naligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,\nleading to unintended behaviors. The jailbreak is to intentionally develop a\nmalicious prompt that escapes from the LLM security restrictions to produce\nuncensored detrimental contents. Previous works explore different jailbreak\nmethods for red teaming LLMs, yet they encounter challenges regarding to\neffectiveness and scalability. In this work, we propose Tastle, a novel\nblack-box jailbreak framework for automated red teaming of LLMs. We designed\nmalicious content concealing and memory reframing with an iterative\noptimization algorithm to jailbreak LLMs, motivated by the research about the\ndistractibility and over-confidence phenomenon of LLMs. Extensive experiments\nof jailbreaking both open-source and proprietary LLMs demonstrate the\nsuperiority of our framework in terms of effectiveness, scalability and\ntransferability. We also evaluate the effectiveness of existing jailbreak\ndefense methods against our attack and highlight the crucial need to develop\nmore effective and practical defense strategies.",
        "pdf_link": "https://arxiv.org/pdf/2403.08424v1.pdf"
    },
    {
        "title": "When Large Language Model Agents Meet 6G Networks: Perception, Grounding, and Alignment",
        "authors": [
            "Minrui Xu",
            "Dusit Niyato",
            "Jiawen Kang",
            "Zehui Xiong",
            "Shiwen Mao",
            "Zhu Han",
            "Dong In Kim",
            "Khaled B. Letaief"
        ],
        "published": "2024-01-15T15:20:59Z",
        "summary": "AI agents based on multimodal large language models (LLMs) are expected to\nrevolutionize human-computer interaction and offer more personalized assistant\nservices across various domains like healthcare, education, manufacturing, and\nentertainment. Deploying LLM agents in 6G networks enables users to access\npreviously expensive AI assistant services via mobile devices democratically,\nthereby reducing interaction latency and better preserving user privacy.\nNevertheless, the limited capacity of mobile devices constrains the\neffectiveness of deploying and executing local LLMs, which necessitates\noffloading complex tasks to global LLMs running on edge servers during\nlong-horizon interactions. In this article, we propose a split learning system\nfor LLM agents in 6G networks leveraging the collaboration between mobile\ndevices and edge servers, where multiple LLMs with different roles are\ndistributed across mobile devices and edge servers to perform user-agent\ninteractive tasks collaboratively. In the proposed system, LLM agents are split\ninto perception, grounding, and alignment modules, facilitating inter-module\ncommunications to meet extended user requirements on 6G network functions,\nincluding integrated sensing and communication, digital twins, and\ntask-oriented communications. Furthermore, we introduce a novel model caching\nalgorithm for LLMs within the proposed system to improve model utilization in\ncontext, thus reducing network costs of the collaborative mobile and edge LLM\nagents.",
        "pdf_link": "https://arxiv.org/pdf/2401.07764v2.pdf"
    },
    {
        "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
        "authors": [
            "Adam Coscia",
            "Langdon Holmes",
            "Wesley Morris",
            "Joon Suh Choi",
            "Scott Crossley",
            "Alex Endert"
        ],
        "published": "2024-03-07T18:56:39Z",
        "summary": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
        "pdf_link": "https://arxiv.org/pdf/2403.04760v1.pdf"
    },
    {
        "title": "MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations",
        "authors": [
            "Hanlei Zhang",
            "Xin Wang",
            "Hua Xu",
            "Qianrui Zhou",
            "Kai Gao",
            "Jianhua Su",
            "jinyue Zhao",
            "Wenrui Li",
            "Yanting Chen"
        ],
        "published": "2024-03-16T15:14:15Z",
        "summary": "Multimodal intent recognition poses significant challenges, requiring the\nincorporation of non-verbal modalities from real-world contexts to enhance the\ncomprehension of human intentions. Existing benchmark datasets are limited in\nscale and suffer from difficulties in handling out-of-scope samples that arise\nin multi-turn conversational interactions. We introduce MIntRec2.0, a\nlarge-scale benchmark dataset for multimodal intent recognition in multi-party\nconversations. It contains 1,245 dialogues with 15,040 samples, each annotated\nwithin a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope\nsamples, it also includes 5,736 out-of-scope samples appearing in multi-turn\ncontexts, which naturally occur in real-world scenarios. Furthermore, we\nprovide comprehensive information on the speakers in each utterance, enriching\nits utility for multi-party conversational research. We establish a general\nframework supporting the organization of single-turn and multi-turn dialogue\ndata, modality feature extraction, multimodal fusion, as well as in-scope\nclassification and out-of-scope detection. Evaluation benchmarks are built\nusing classic multimodal fusion methods, ChatGPT, and human evaluators. While\nexisting methods incorporating nonverbal information yield improvements,\neffectively leveraging context information and detecting out-of-scope samples\nremains a substantial challenge. Notably, large language models exhibit a\nsignificant performance gap compared to humans, highlighting the limitations of\nmachine learning methods in the cognitive intent understanding task. We believe\nthat MIntRec2.0 will serve as a valuable resource, providing a pioneering\nfoundation for research in human-machine conversational interactions, and\nsignificantly facilitating related applications. The full dataset and codes are\navailable at https://github.com/thuiar/MIntRec2.0.",
        "pdf_link": "https://arxiv.org/pdf/2403.10943v2.pdf"
    },
    {
        "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "authors": [
            "Fuzhao Xue",
            "Zian Zheng",
            "Yao Fu",
            "Jinjie Ni",
            "Zangwei Zheng",
            "Wangchunshu Zhou",
            "Yang You"
        ],
        "published": "2024-01-29T12:05:02Z",
        "summary": "To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.",
        "pdf_link": "https://arxiv.org/pdf/2402.01739v2.pdf"
    },
    {
        "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
        "authors": [
            "Ridwan Taiwo",
            "Idris Temitope Bello",
            "Sulemana Fatoama Abdulai",
            "Abdul-Mugis Yussif",
            "Babatunde Abiodun Salami",
            "Abdullahi Saka",
            "Tarek Zayed"
        ],
        "published": "2024-02-15T13:39:55Z",
        "summary": "The construction industry is a vital sector of the global economy, but it\nfaces many productivity challenges in various processes, such as design,\nplanning, procurement, inspection, and maintenance. Generative artificial\nintelligence (AI), which can create novel and realistic data or content, such\nas text, image, video, or code, based on some input or prior knowledge, offers\ninnovative and disruptive solutions to address these challenges. However, there\nis a gap in the literature on the current state, opportunities, and challenges\nof generative AI in the construction industry. This study aims to fill this gap\nby providing a state-of-the-art analysis of generative AI in construction, with\nthree objectives: (1) to review and categorize the existing and emerging\ngenerative AI opportunities and challenges in the construction industry; (2) to\npropose a framework for construction firms to build customized generative AI\nsolutions using their own data, comprising steps such as data collection,\ndataset curation, training custom large language model (LLM), model evaluation,\nand deployment; and (3) to demonstrate the framework via a case study of\ndeveloping a generative model for querying contract documents. The results show\nthat retrieval augmented generation (RAG) improves the baseline LLM by 5.2,\n9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study\nprovides academics and construction professionals with a comprehensive analysis\nand practical framework to guide the adoption of generative AI techniques to\nenhance productivity, quality, safety, and sustainability across the\nconstruction industry.",
        "pdf_link": "https://arxiv.org/pdf/2402.09939v1.pdf"
    },
    {
        "title": "Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches",
        "authors": [
            "Wannapon Suraworachet",
            "Jennifer Seon",
            "Mutlu Cukurova"
        ],
        "published": "2024-01-03T11:54:30Z",
        "summary": "Effective collaboration requires groups to strategically regulate themselves\nto overcome challenges. Research has shown that groups may fail to regulate due\nto differences in members' perceptions of challenges which may benefit from\nexternal support. In this study, we investigated the potential of leveraging\nthree distinct natural language processing models: an expert knowledge\nrule-based model, a supervised machine learning (ML) model and a Large Language\nmodel (LLM), in challenge detection and challenge dimension identification\n(cognitive, metacognitive, emotional and technical/other challenges) from\nstudent discourse, was investigated. The results show that the supervised ML\nand the LLM approaches performed considerably well in both tasks, in contrast\nto the rule-based approach, whose efficacy heavily relies on the engineered\nfeatures by experts. The paper provides an extensive discussion of the three\napproaches' performance for automated detection and support of students'\nchallenge moments in collaborative learning activities. It argues that,\nalthough LLMs provide many advantages, they are unlikely to be the panacea to\nissues of the detection and feedback provision of socially shared regulation of\nlearning due to their lack of reliability, as well as issues of validity\nevaluation, privacy and confabulation. We conclude the paper with a discussion\non additional considerations, including model transparency to explore feasible\nand meaningful analytical feedback for students and educators using LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.01692v1.pdf"
    },
    {
        "title": "Policy Improvement using Language Feedback Models",
        "authors": [
            "Victor Zhong",
            "Dipendra Misra",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9"
        ],
        "published": "2024-02-12T18:41:34Z",
        "summary": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.",
        "pdf_link": "https://arxiv.org/pdf/2402.07876v3.pdf"
    },
    {
        "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to components",
        "authors": [
            "J\u00e1nos Kram\u00e1r",
            "Tom Lieberum",
            "Rohin Shah",
            "Neel Nanda"
        ],
        "published": "2024-03-01T18:43:51Z",
        "summary": "Activation Patching is a method of directly computing causal attributions of\nbehavior to model components. However, applying it exhaustively requires a\nsweep with cost scaling linearly in the number of model components, which can\nbe prohibitively expensive for SoTA Large Language Models (LLMs). We\ninvestigate Attribution Patching (AtP), a fast gradient-based approximation to\nActivation Patching and find two classes of failure modes of AtP which lead to\nsignificant false negatives. We propose a variant of AtP called AtP*, with two\nchanges to address these failure modes while retaining scalability. We present\nthe first systematic study of AtP and alternative methods for faster activation\npatching and show that AtP significantly outperforms all other investigated\nmethods, with AtP* providing further significant improvement. Finally, we\nprovide a method to bound the probability of remaining false negatives of AtP*\nestimates.",
        "pdf_link": "https://arxiv.org/pdf/2403.00745v1.pdf"
    },
    {
        "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
        "authors": [
            "Linhai Zhang",
            "Jialong Wu",
            "Deyu Zhou",
            "Guoqiang Xu"
        ],
        "published": "2024-03-02T10:38:10Z",
        "summary": "Though Large Language Models (LLMs) have demonstrated the powerful\ncapabilities of few-shot learning through prompting methods, supervised\ntraining is still necessary for complex reasoning tasks. Because of their\nextensive parameters and memory consumption, both Parameter-Efficient\nFine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been\nproposed for LLMs. Nevertheless, the issue of large annotated data consumption,\nthe aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is\nto combine the PEFT method with active learning. However, the experimental\nresults show that such a combination is not trivial and yields inferior\nresults. Through probe experiments, such observation might be explained by two\nmain reasons: uncertainty gap and poor model calibration. Therefore, in this\npaper, we propose a novel approach to effectively integrate uncertainty-based\nactive learning and LoRA. Specifically, for the uncertainty gap, we introduce a\ndynamic uncertainty measurement that combines the uncertainty of the base model\nand the uncertainty of the full model during the iteration of active learning.\nFor poor model calibration, we incorporate the regularization method during\nLoRA training to keep the model from being over-confident, and the Monte-Carlo\ndropout mechanism is employed to enhance the uncertainty estimation.\nExperimental results show that the proposed approach outperforms existing\nbaseline models on three complex reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.01165v1.pdf"
    },
    {
        "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing",
        "authors": [
            "Bryan Wang",
            "Yuliang Li",
            "Zhaoyang Lv",
            "Haijun Xia",
            "Yan Xu",
            "Raj Sodhi"
        ],
        "published": "2024-02-15T19:53:11Z",
        "summary": "Video creation has become increasingly popular, yet the expertise and effort\nrequired for editing often pose barriers to beginners. In this paper, we\nexplore the integration of large language models (LLMs) into the video editing\nworkflow to reduce these barriers. Our design vision is embodied in LAVE, a\nnovel system that provides LLM-powered agent assistance and language-augmented\nediting features. LAVE automatically generates language descriptions for the\nuser's footage, serving as the foundation for enabling the LLM to process\nvideos and assist in editing tasks. When the user provides editing objectives,\nthe agent plans and executes relevant actions to fulfill them. Moreover, LAVE\nallows users to edit videos through either the agent or direct UI manipulation,\nproviding flexibility and enabling manual refinement of agent actions. Our user\nstudy, which included eight participants ranging from novices to proficient\neditors, demonstrated LAVE's effectiveness. The results also shed light on user\nperceptions of the proposed LLM-assisted editing paradigm and its impact on\nusers' creativity and sense of co-creation. Based on these findings, we propose\ndesign implications to inform the future development of agent-assisted content\nediting.",
        "pdf_link": "https://arxiv.org/pdf/2402.10294v1.pdf"
    },
    {
        "title": "Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization",
        "authors": [
            "Shuaimin Li",
            "Xuanang Chen",
            "Yuanfeng Song",
            "Yunze Song",
            "Chen Zhang"
        ],
        "published": "2024-01-29T10:23:47Z",
        "summary": "Data visualization (DV) systems are increasingly recognized for their\nprofound capability to uncover insights from vast datasets, gaining attention\nacross both industry and academia. Crafting data queries is an essential\nprocess within certain declarative visualization languages (DVLs, e.g.,\nVega-Lite, EChart.). The evolution of natural language processing (NLP)\ntechnologies has streamlined the use of natural language interfaces to\nvisualize tabular data, offering a more accessible and intuitive user\nexperience. However, current methods for converting natural language questions\ninto data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite\nutilizing complex neural network architectures, still fall short of\nexpectations and have great room for improvement.\n  Large language models (LLMs) such as ChatGPT and GPT-4, have established new\nbenchmarks in a variety of NLP tasks, fundamentally altering the landscape of\nthe field. Inspired by these advancements, we introduce a novel framework,\nPrompt4Vis, leveraging LLMs and in-context learning to enhance the performance\nof generating data visualization from natural language. Prompt4Vis comprises\ntwo key components: (1) a multi-objective example mining module, designed to\nfind out the truly effective examples that strengthen the LLM's in-context\nlearning capabilities for text-to-vis; (2) a schema filtering module, which is\nproposed to simplify the schema of the database. Extensive experiments through\n5-fold cross-validation on the NVBench dataset demonstrate the superiority of\nPrompt4Vis, which notably surpasses the state-of-the-art (SOTA) RGVisNet by\napproximately 35.9% and 71.3% on dev and test sets, respectively. To the best\nof our knowledge, Prompt4Vis is the first work that introduces in-context\nlearning into the text-to-vis for generating data visualization queries.",
        "pdf_link": "https://arxiv.org/pdf/2402.07909v1.pdf"
    },
    {
        "title": "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation",
        "authors": [
            "Shiyu Ni",
            "Keping Bi",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "published": "2024-02-18T04:57:19Z",
        "summary": "Large Language Models (LLMs) have been found to have difficulty knowing they\ndo not possess certain knowledge and tend to provide specious answers in such\ncases. Retrieval Augmentation (RA) has been extensively studied to mitigate\nLLMs' hallucinations. However, due to the extra overhead and unassured quality\nof retrieval, it may not be optimal to conduct RA all the time. A\nstraightforward idea is to only conduct retrieval when LLMs are uncertain about\na question. This motivates us to enhance the LLMs' ability to perceive their\nknowledge boundaries to help RA. In this paper, we first quantitatively measure\nLLMs' such ability and confirm their overconfidence. Then, we study how LLMs'\ncertainty about a question correlates with their dependence on external\nretrieved information. We propose several methods to enhance LLMs' perception\nof knowledge boundaries and show that they are effective in reducing\noverconfidence. Additionally, equipped with these methods, LLMs can achieve\ncomparable or even better performance of RA with much fewer retrieval calls.",
        "pdf_link": "https://arxiv.org/pdf/2402.11457v1.pdf"
    },
    {
        "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
        "authors": [
            "Zhiyuan Li",
            "Hong Liu",
            "Denny Zhou",
            "Tengyu Ma"
        ],
        "published": "2024-02-20T10:11:03Z",
        "summary": "Instructing the model to generate a sequence of intermediate steps, a.k.a., a\nchain of thought (CoT), is a highly effective method to improve the accuracy of\nlarge language models (LLMs) on arithmetics and symbolic reasoning tasks.\nHowever, the mechanism behind CoT remains unclear. This work provides a\ntheoretical understanding of the power of CoT for decoder-only transformers\nthrough the lens of expressiveness. Conceptually, CoT empowers the model with\nthe ability to perform inherently serial computation, which is otherwise\nlacking in transformers, especially when depth is low. Given input length $n$,\nprevious works have shown that constant-depth transformers with finite\nprecision $\\mathsf{poly}(n)$ embedding size can only solve problems in\n$\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper\nbound for constant-depth transformers with constant-bit precision, which can\nonly solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$.\nHowever, with $T$ steps of CoT, constant-depth transformers using constant-bit\nprecision and $O(\\log n)$ embedding size can solve any problem solvable by\nboolean circuits of size $T$. Empirically, enabling CoT dramatically improves\nthe accuracy for tasks that are hard for parallel computation, including the\ncomposition of permutation groups, iterated squaring, and circuit value\nproblems, especially for low-depth transformers.",
        "pdf_link": "https://arxiv.org/pdf/2402.12875v1.pdf"
    },
    {
        "title": "DTS-SQL: Decomposed Text-to-SQL with Small Large Language Models",
        "authors": [
            "Mohammadreza Pourreza",
            "Davood Rafiei"
        ],
        "published": "2024-02-02T03:21:00Z",
        "summary": "Leading models for the text-to-SQL task heavily rely on proprietary Large\nLanguage Models (LLMs), posing concerns over data privacy. Closing the\nperformance gap between small open-source models and large proprietary models\nis crucial to mitigate this reliance. To this end, we introduce a novel\ntwo-stage fine-tuning approach that decomposes the task into two simpler tasks.\nThrough comprehensive evaluation on two large cross-domain datasets and two\nsmall LLMs, we show that this approach improves execution accuracy by 3 to 7\npercent, effectively aligning the performance of open-source models with their\nproprietary counterparts.",
        "pdf_link": "https://arxiv.org/pdf/2402.01117v1.pdf"
    },
    {
        "title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification",
        "authors": [
            "Jan Trienes",
            "Sebastian Joseph",
            "J\u00f6rg Schl\u00f6tterer",
            "Christin Seifert",
            "Kyle Lo",
            "Wei Xu",
            "Byron C. Wallace",
            "Junyi Jessy Li"
        ],
        "published": "2024-01-29T19:00:01Z",
        "summary": "Text simplification aims to make technical texts more accessible to laypeople\nbut often results in deletion of information and vagueness. This work proposes\nInfoLossQA, a framework to characterize and recover simplification-induced\ninformation loss in form of question-and-answer (QA) pairs. Building on the\ntheory of Question Under Discussion, the QA pairs are designed to help readers\ndeepen their knowledge of a text. We conduct a range of experiments with this\nframework. First, we collect a dataset of 1,000 linguist-curated QA pairs\nderived from 104 LLM simplifications of scientific abstracts of medical\nstudies. Our analyses of this data reveal that information loss occurs\nfrequently, and that the QA pairs give a high-level overview of what\ninformation was lost. Second, we devise two methods for this task: end-to-end\nprompting of open-source and commercial language models, and a natural language\ninference pipeline. With a novel evaluation framework considering the\ncorrectness of QA pairs and their linguistic suitability, our expert evaluation\nreveals that models struggle to reliably identify information loss and applying\nsimilar standards as humans at what constitutes information loss.",
        "pdf_link": "https://arxiv.org/pdf/2401.16475v1.pdf"
    },
    {
        "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers",
        "authors": [
            "Chen Zheng",
            "Ke Sun",
            "Da Tang",
            "Yukun Ma",
            "Yuyu Zhang",
            "Chenguang Xi",
            "Xun Zhou"
        ],
        "published": "2024-01-04T05:47:41Z",
        "summary": "The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA\nencounter limitations in domain-specific tasks, with these models often lacking\ndepth and accuracy in specialized areas, and exhibiting a decrease in general\ncapabilities when fine-tuned, particularly analysis ability in small sized\nmodels. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement\nLearning from Human Feedback (RLHF) grounded in Proximal Policy Optimization\n(PPO), demonstrating remarkable ability in in-domain scenarios without\ncompromising general task performance. Our exploration of ICE-GRT highlights\nits understanding and reasoning ability to not only generate robust answers but\nalso to provide detailed analyses of the reasons behind the answer. This\ncapability marks a significant progression beyond the scope of Supervised\nFine-Tuning models. The success of ICE-GRT is dependent on several crucial\nfactors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage\nNormalization, etc. The ICE-GRT model exhibits state-of-the-art performance in\ndomain-specific tasks and across 12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting the effectiveness of our approach.\nWe provide a comprehensive analysis of the ICE-GRT, underscoring the\nsignificant advancements it brings to the field of LLM.",
        "pdf_link": "https://arxiv.org/pdf/2401.02072v1.pdf"
    },
    {
        "title": "Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning",
        "authors": [
            "Mingtian Zhang",
            "Shawn Lan",
            "Peter Hayes",
            "David Barber"
        ],
        "published": "2024-02-19T14:33:24Z",
        "summary": "Retrieval Augmented Generation (RAG) has emerged as an effective solution for\nmitigating hallucinations in Large Language Models (LLMs). The retrieval stage\nin RAG typically involves a pre-trained embedding model, which converts queries\nand passages into vectors to capture their semantics. However, a standard\npre-trained embedding model may exhibit sub-optimal performance when applied to\nspecific domain knowledge, necessitating fine-tuning. This paper addresses\nscenarios where the embeddings are only available from a black-box model. We\nintroduce Model augmented fine-tuning (Mafin) -- a novel approach for\nfine-tuning a black-box embedding model by augmenting it with a trainable\nembedding model. Our results demonstrate that Mafin significantly enhances the\nperformance of the black-box embeddings by only requiring the training of a\nsmall augmented model. We validate the effectiveness of our method on both\nlabeled and unlabeled datasets, illustrating its broad applicability and\nefficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.12177v4.pdf"
    },
    {
        "title": "MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences",
        "authors": [
            "Souradip Chakraborty",
            "Jiahao Qiu",
            "Hui Yuan",
            "Alec Koppel",
            "Furong Huang",
            "Dinesh Manocha",
            "Amrit Singh Bedi",
            "Mengdi Wang"
        ],
        "published": "2024-02-14T03:56:27Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models to\nhuman preferences by employing a singular reward model derived from preference\ndata. However, such an approach overlooks the rich diversity of human\npreferences inherent in data collected from multiple users. In this work, we\nfirst derive an impossibility result of alignment with single reward RLHF,\nthereby highlighting its insufficiency in representing diverse human\npreferences. To provide an equitable solution to the problem, we learn a\nmixture of preference distributions via an expectation-maximization algorithm\nand propose a MaxMin alignment objective for policy learning inspired by the\nEgalitarian principle in social choice theory to better represent diverse human\npreferences. We elucidate the connection of our proposed approach to\ndistributionally robust optimization and general utility RL, thereby\nhighlighting the generality and robustness of our proposed solution. We present\ncomprehensive experimental results on small-scale (GPT-2) and large-scale\nlanguage models (with Tulu2-7B) and show the efficacy of the proposed approach\nin the presence of diversity among human preferences. Our algorithm achieves an\naverage improvement of more than 16% in win-rates over conventional RLHF\nalgorithms and improves the win-rate (accuracy) for minority groups by over 33%\nwithout compromising the performance of majority groups, showcasing the\nrobustness and fairness of our approach. We remark that our findings in this\nwork are not only limited to language models but also extend to reinforcement\nlearning in general.",
        "pdf_link": "https://arxiv.org/pdf/2402.08925v1.pdf"
    },
    {
        "title": "Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue",
        "authors": [
            "Kun Ouyang",
            "Liqiang Jing",
            "Xuemeng Song",
            "Meng Liu",
            "Yupeng Hu",
            "Liqiang Nie"
        ],
        "published": "2024-02-06T03:14:46Z",
        "summary": "Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which\naims to generate a natural language explanation for the given sarcastic\ndialogue that involves multiple modalities (i.e., utterance, video, and audio).\nAlthough existing studies have achieved great success based on the generative\npretrained language model BART, they overlook exploiting the sentiments\nresiding in the utterance, video and audio, which are vital clues for sarcasm\nexplanation. In fact, it is non-trivial to incorporate sentiments for boosting\nSED performance, due to three main challenges: 1) diverse effects of utterance\ntokens on sentiments; 2) gap between video-audio sentiment signals and the\nembedding space of BART; and 3) various relations among utterances, utterance\nsentiments, and video-audio sentiments. To tackle these challenges, we propose\na novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation\nframework, named EDGE. In particular, we first propose a lexicon-guided\nutterance sentiment inference module, where a heuristic utterance sentiment\nrefinement strategy is devised. We then develop a module named Joint Cross\nAttention-based Sentiment Inference (JCA-SI) by extending the multimodal\nsentiment analysis model JCA to derive the joint sentiment label for each\nvideo-audio clip. Thereafter, we devise a context-sentiment graph to\ncomprehensively model the semantic relations among the utterances, utterance\nsentiments, and video-audio sentiments, to facilitate sarcasm explanation\ngeneration. Extensive experiments on the publicly released dataset WITS verify\nthe superiority of our model over cutting-edge methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.03658v1.pdf"
    },
    {
        "title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K",
        "authors": [
            "Tao Yuan",
            "Xuefei Ning",
            "Dong Zhou",
            "Zhijie Yang",
            "Shiyao Li",
            "Minghui Zhuang",
            "Zheyue Tan",
            "Zhuyu Yao",
            "Dahua Lin",
            "Boxun Li",
            "Guohao Dai",
            "Shengen Yan",
            "Yu Wang"
        ],
        "published": "2024-02-06T13:11:19Z",
        "summary": "State-of-the-art large language models (LLMs) are now claiming remarkable\nsupported context lengths of 256k or even more. In contrast, the average\ncontext lengths of mainstream benchmarks are insufficient (5k-21k), and they\nsuffer from potential knowledge leakage and inaccurate metrics, resulting in\nbiased evaluation. This paper introduces LV-Eval, a challenging long-context\nbenchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up\nto 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA,\ncomprising 11 bilingual datasets. The design of LV-Eval has incorporated three\nkey techniques, namely confusing facts insertion, keyword and phrase\nreplacement, and keyword-recall-based metric design. The advantages of LV-Eval\ninclude controllable evaluation across different context lengths, challenging\ntest instances with confusing facts, mitigated knowledge leakage, and more\nobjective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation\nstudies on the techniques used in LV-Eval construction. The results reveal\nthat: (i) Commercial LLMs generally outperform open-source LLMs when evaluated\nwithin length levels shorter than their claimed context length. However, their\noverall performance is surpassed by open-source LLMs with longer context\nlengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k, exhibit a\nrelatively gentle degradation of performance, but their absolute performances\nmay not necessarily be higher than those of LLMs with shorter context lengths.\n(iii) LLMs' performances can significantly degrade in the presence of confusing\ninformation, especially in the pressure test of \"needle in a haystack\". (iv)\nIssues related to knowledge leakage and inaccurate metrics introduce bias in\nevaluation, and these concerns are alleviated in LV-Eval. All datasets and\nevaluation codes are released at: https://github.com/infinigence/LVEval.",
        "pdf_link": "https://arxiv.org/pdf/2402.05136v1.pdf"
    },
    {
        "title": "What Evidence Do Language Models Find Convincing?",
        "authors": [
            "Alexander Wan",
            "Eric Wallace",
            "Dan Klein"
        ],
        "published": "2024-02-19T02:15:34Z",
        "summary": "Retrieval-augmented language models are being increasingly tasked with\nsubjective, contentious, and conflicting queries such as \"is aspartame linked\nto cancer\". To resolve these ambiguous queries, one must search through a large\nrange of websites and consider \"which, if any, of this evidence do I find\nconvincing?\". In this work, we study how LLMs answer this question. In\nparticular, we construct ConflictingQA, a dataset that pairs controversial\nqueries with a series of real-world evidence documents that contain different\nfacts (e.g., quantitative results), argument styles (e.g., appeals to\nauthority), and answers (Yes or No). We use this dataset to perform sensitivity\nand counterfactual analyses to explore which text features most affect LLM\npredictions. Overall, we find that current models rely heavily on the relevance\nof a website to the query, while largely ignoring stylistic features that\nhumans find important such as whether a text contains scientific references or\nis written with a neutral tone. Taken together, these results highlight the\nimportance of RAG corpus quality (e.g., the need to filter misinformation), and\npossibly even a shift in how LLMs are trained to better align with human\njudgements.",
        "pdf_link": "https://arxiv.org/pdf/2402.11782v1.pdf"
    },
    {
        "title": "Resonance RoPE: Improving Context Length Generalization of Large Language Models",
        "authors": [
            "Suyuchen Wang",
            "Ivan Kobyzev",
            "Peng Lu",
            "Mehdi Rezagholizadeh",
            "Bang Liu"
        ],
        "published": "2024-02-29T19:02:03Z",
        "summary": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios\nin Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),\nwhere models pre-trained on shorter sequences face difficulty with\nout-of-distribution (OOD) token positions in longer sequences. We introduce\nResonance RoPE, a novel approach designed to narrow the generalization gap in\nTSTL scenarios by refining the interpolation of RoPE features for OOD\npositions, significantly improving the model performance without additional\nonline computational costs. Furthermore, we present PosGen, a new synthetic\nbenchmark specifically designed for fine-grained behavior analysis in TSTL\nscenarios, aiming to isolate the constantly increasing difficulty of token\ngeneration on long contexts from the challenges of recognizing new token\npositions. Our experiments on synthetic tasks show that after applying\nResonance RoPE, Transformers recognize OOD position better and more robustly.\nOur extensive LLM experiments also show superior performance after applying\nResonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on\nboth upstream language modeling tasks and a variety of downstream long-text\napplications.",
        "pdf_link": "https://arxiv.org/pdf/2403.00071v1.pdf"
    },
    {
        "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
        "authors": [
            "Shuai Ma",
            "Qiaoyi Chen",
            "Xinru Wang",
            "Chengbo Zheng",
            "Zhenhui Peng",
            "Ming Yin",
            "Xiaojuan Ma"
        ],
        "published": "2024-03-25T14:34:06Z",
        "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion\nand decide whether to accept or reject it as a whole. In such a paradigm,\nhumans are found to rarely trigger analytical thinking and face difficulties in\ncommunicating the nuances of conflicting opinions to the AI when disagreements\noccur. To tackle this challenge, we propose Human-AI Deliberation, a novel\nframework to promote human reflection and discussion on conflicting human-AI\nopinions in decision-making. Based on theories in human deliberation, this\nframework engages humans and AI in dimension-level opinion elicitation,\ndeliberative discussion, and decision updates. To empower AI with deliberative\ncapabilities, we designed Deliberative AI, which leverages large language\nmodels (LLMs) as a bridge between humans and domain-specific models to enable\nflexible conversational interactions and faithful information provision. An\nexploratory evaluation on a graduate admissions task shows that Deliberative AI\noutperforms conventional explainable AI (XAI) assistants in improving humans'\nappropriate reliance and task performance. Based on a mixed-methods analysis of\nparticipant behavior, perception, user experience, and open-ended feedback, we\ndraw implications for future AI-assisted decision tool design.",
        "pdf_link": "https://arxiv.org/pdf/2403.16812v1.pdf"
    },
    {
        "title": "Agent Alignment in Evolving Social Norms",
        "authors": [
            "Shimin Li",
            "Tianxiang Sun",
            "Qinyuan Cheng",
            "Xipeng Qiu"
        ],
        "published": "2024-01-09T15:44:44Z",
        "summary": "Agents based on Large Language Models (LLMs) are increasingly permeating\nvarious domains of human production and life, highlighting the importance of\naligning them with human values. The current alignment of AI systems primarily\nfocuses on passively aligning LLMs through human intervention. However, agents\npossess characteristics like receiving environmental feedback and\nself-evolution, rendering the LLM alignment methods inadequate. In response, we\npropose an evolutionary framework for agent evolution and alignment, named\nEvolutionaryAgent, which transforms agent alignment into a process of evolution\nand selection under the principle of survival of the fittest. In an environment\nwhere social norms continuously evolve, agents better adapted to the current\nsocial norms will have a higher probability of survival and proliferation,\nwhile those inadequately aligned dwindle over time. Experimental results\nassessing the agents from multiple perspectives in aligning with social norms\ndemonstrate that EvolutionaryAgent can align progressively better with the\nevolving social norms while maintaining its proficiency in general tasks.\nEffectiveness tests conducted on various open and closed-source LLMs as the\nfoundation for agents also prove the applicability of our approach.",
        "pdf_link": "https://arxiv.org/pdf/2401.04620v4.pdf"
    },
    {
        "title": "Case-Based or Rule-Based: How Do Transformers Do the Math?",
        "authors": [
            "Yi Hu",
            "Xiaojuan Tang",
            "Haotong Yang",
            "Muhan Zhang"
        ],
        "published": "2024-02-27T17:41:58Z",
        "summary": "Despite the impressive performance in a variety of complex tasks, modern\nlarge language models (LLMs) still have trouble dealing with some math problems\nthat are simple and intuitive for humans, such as addition. While we can easily\nlearn basic rules of addition and apply them to new problems of any length,\nLLMs struggle to do the same. Instead, they may rely on similar \"cases\" seen in\nthe training corpus for help. We define these two different reasoning\nmechanisms as \"rule-based reasoning\" and \"case-based reasoning\". Since\nrule-based reasoning is essential for acquiring the systematic generalization\nability, we aim to explore exactly whether transformers use rule-based or\ncase-based reasoning for math problems. Through carefully designed intervention\nexperiments on five math tasks, we confirm that transformers are performing\ncase-based reasoning, no matter whether scratchpad is used, which aligns with\nthe previous observations that transformers use subgraph matching/shortcut\nlearning to reason. To mitigate such problems, we propose a Rule-Following\nFine-Tuning (RFFT) technique to teach transformers to perform rule-based\nreasoning. Specifically, we provide explicit rules in the input and then\ninstruct transformers to recite and follow the rules step by step. Through\nRFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to\ngeneralize to up to 12-digit addition with over 95% accuracy, which is over 40%\nhigher than scratchpad. The significant improvement demonstrates that teaching\nLLMs to explicitly use rules helps them learn rule-based reasoning and\ngeneralize better in length.",
        "pdf_link": "https://arxiv.org/pdf/2402.17709v1.pdf"
    },
    {
        "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models",
        "authors": [
            "Keisuke Kamahori",
            "Yile Gu",
            "Kan Zhu",
            "Baris Kasikci"
        ],
        "published": "2024-02-10T19:54:08Z",
        "summary": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture\nare showing promising performance on various tasks. However, running them on\nresource-constrained settings, where GPU memory resources are not abundant, is\nchallenging due to huge model sizes. Existing systems that offload model\nweights to CPU memory suffer from the significant overhead of frequently moving\ndata between CPU and GPU. In this paper, we propose Fiddler, a\nresource-efficient inference engine with CPU-GPU orchestration for MoE models.\nThe key idea of Fiddler is to use the computation ability of the CPU to\nminimize the data movement between the CPU and GPU. Our evaluation shows that\nFiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in\nparameters, to generate over $3$ tokens per second on a single GPU with 24GB\nmemory, showing an order of magnitude improvement over existing methods. The\ncode of Fiddler is publicly available at\n\\url{https://github.com/efeslab/fiddler}",
        "pdf_link": "https://arxiv.org/pdf/2402.07033v1.pdf"
    },
    {
        "title": "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications",
        "authors": [
            "Boyi Wei",
            "Kaixuan Huang",
            "Yangsibo Huang",
            "Tinghao Xie",
            "Xiangyu Qi",
            "Mengzhou Xia",
            "Prateek Mittal",
            "Mengdi Wang",
            "Peter Henderson"
        ],
        "published": "2024-02-07T18:34:38Z",
        "summary": "Large language models (LLMs) show inherent brittleness in their safety\nmechanisms, as evidenced by their susceptibility to jailbreaking and even\nnon-malicious fine-tuning. This study explores this brittleness of safety\nalignment by leveraging pruning and low-rank modifications. We develop methods\nto identify critical regions that are vital for safety guardrails, and that are\ndisentangled from utility-relevant regions at both the neuron and rank levels.\nSurprisingly, the isolated regions we find are sparse, comprising about $3\\%$\nat the parameter level and $2.5\\%$ at the rank level. Removing these regions\ncompromises safety without significantly impacting utility, corroborating the\ninherent brittleness of the model's safety mechanisms. Moreover, we show that\nLLMs remain vulnerable to low-cost fine-tuning attacks even when modifications\nto the safety-critical regions are restricted. These findings underscore the\nurgent need for more robust safety strategies in LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.05162v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Fuzzy String Matching in Political Science",
        "authors": [
            "Yu Wang"
        ],
        "published": "2024-03-27T03:04:21Z",
        "summary": "Fuzzy string matching remains a key issue when political scientists combine\ndata from different sources. Existing matching methods invariably rely on\nstring distances, such as Levenshtein distance and cosine similarity. As such,\nthey are inherently incapable of matching strings that refer to the same entity\nwith different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and\n''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In\nthis letter, we propose to use large language models to entirely sidestep this\nproblem in an easy and intuitive manner. Extensive experiments show that our\nproposed methods can improve the state of the art by as much as 39% in terms of\naverage precision while being substantially easier and more intuitive to use by\npolitical scientists. Moreover, our results are robust against various\ntemperatures. We further note that enhanced prompting can lead to additional\nperformance improvements.",
        "pdf_link": "https://arxiv.org/pdf/2403.18218v1.pdf"
    },
    {
        "title": "Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset",
        "authors": [
            "Santosh T. Y. S. S",
            "Nina Baumgartner",
            "Matthias St\u00fcrmer",
            "Matthias Grabmair",
            "Joel Niklaus"
        ],
        "published": "2024-02-26T20:42:40Z",
        "summary": "The assessment of explainability in Legal Judgement Prediction (LJP) systems\nis of paramount importance in building trustworthy and transparent systems,\nparticularly considering the reliance of these systems on factors that may lack\nlegal relevance or involve sensitive attributes. This study delves into the\nrealm of explainability and fairness in LJP models, utilizing Swiss Judgement\nPrediction (SJP), the only available multilingual LJP dataset. We curate a\ncomprehensive collection of rationales that `support' and `oppose' judgement\nfrom legal experts for 108 cases in German, French, and Italian. By employing\nan occlusion-based explainability approach, we evaluate the explainability\nperformance of state-of-the-art monolingual and multilingual BERT-based LJP\nmodels, as well as models developed with techniques such as data augmentation\nand cross-lingual transfer, which demonstrated prediction performance\nimprovement. Notably, our findings reveal that improved prediction performance\ndoes not necessarily correspond to enhanced explainability performance,\nunderscoring the significance of evaluating models from an explainability\nperspective. Additionally, we introduce a novel evaluation framework, Lower\nCourt Insertion (LCI), which allows us to quantify the influence of lower court\ninformation on model predictions, exposing current models' biases.",
        "pdf_link": "https://arxiv.org/pdf/2402.17013v1.pdf"
    },
    {
        "title": "CodeS: Natural Language to Code Repository via Multi-Layer Sketch",
        "authors": [
            "Daoguang Zan",
            "Ailun Yu",
            "Wei Liu",
            "Dong Chen",
            "Bo Shen",
            "Wei Li",
            "Yafen Yao",
            "Yongshun Gong",
            "Xiaolin Chen",
            "Bei Guan",
            "Zhiguang Yang",
            "Yongji Wang",
            "Qianxiang Wang",
            "Lizhen Cui"
        ],
        "published": "2024-03-25T06:09:55Z",
        "summary": "The impressive performance of large language models (LLMs) on code-related\ntasks has shown the potential of fully automated software development. In light\nof this, we introduce a new software engineering task, namely Natural Language\nto code Repository (NL2Repo). This task aims to generate an entire code\nrepository from its natural language requirements. To address this task, we\npropose a simple yet effective framework CodeS, which decomposes NL2Repo into\nmultiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three\nmodules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first\ngenerates a repository's directory structure for given requirements;\nFileSketcher then generates a file sketch for each file in the generated\nstructure; SketchFiller finally fills in the details for each function in the\ngenerated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry\nout evaluations through both automated benchmarking and manual feedback\nanalysis. For benchmark-based evaluation, we craft a repository-oriented\nbenchmark, SketchEval, and design an evaluation metric, SketchBLEU. For\nfeedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30\nparticipants in conducting empirical studies. Extensive experiments prove the\neffectiveness and practicality of CodeS on the NL2Repo task.",
        "pdf_link": "https://arxiv.org/pdf/2403.16443v1.pdf"
    },
    {
        "title": "Massive Activations in Large Language Models",
        "authors": [
            "Mingjie Sun",
            "Xinlei Chen",
            "J. Zico Kolter",
            "Zhuang Liu"
        ],
        "published": "2024-02-27T18:55:17Z",
        "summary": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers.",
        "pdf_link": "https://arxiv.org/pdf/2402.17762v1.pdf"
    },
    {
        "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Generative LLM Inference",
        "authors": [
            "Atsuki Yamaguchi",
            "Aline Villavicencio",
            "Nikolaos Aletras"
        ],
        "published": "2024-02-16T14:15:15Z",
        "summary": "The development of state-of-the-art generative large language models (LLMs)\ndisproportionately relies on English-centric tokenizers, vocabulary and\npre-training data. Despite the fact that some LLMs have multilingual\ncapabilities, recent studies have shown that their inference efficiency\ndeteriorates when generating text in languages other than English. This results\nin increased inference time and costs. Cross-lingual vocabulary adaptation\nmethods have been proposed for adapting models to a target language aiming to\nimprove downstream performance. However, the effectiveness of these methods on\nincreasing inference efficiency of generative LLMs has yet to be explored. In\nthis paper, we perform an empirical study of various cross-lingual vocabulary\nadaptation methods on five generative LLMs (including monolingual and\nmultilingual models) across four typologically-diverse languages and four\nnatural language understanding tasks. We find that cross-lingual vocabulary\nadaptation substantially contributes to LLM inference speedups of up to 271.5%.\nWe also show that adapting LLMs that have been pre-trained on more balanced\nmultilingual data results in downstream performance comparable to the original\nmodels.",
        "pdf_link": "https://arxiv.org/pdf/2402.10712v1.pdf"
    },
    {
        "title": "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection",
        "authors": [
            "Peng Qi",
            "Zehong Yan",
            "Wynne Hsu",
            "Mong Li Lee"
        ],
        "published": "2024-03-05T18:04:59Z",
        "summary": "Misinformation is a prevalent societal issue due to its potential high risks.\nOut-of-context (OOC) misinformation, where authentic images are repurposed with\nfalse text, is one of the easiest and most effective ways to mislead audiences.\nCurrent methods focus on assessing image-text consistency but lack convincing\nexplanations for their judgments, which is essential for debunking\nmisinformation. While Multimodal Large Language Models (MLLMs) have rich\nknowledge and innate capability for visual reasoning and explanation\ngeneration, they still lack sophistication in understanding and discovering the\nsubtle crossmodal differences. In this paper, we introduce SNIFFER, a novel\nmultimodal large language model specifically engineered for OOC misinformation\ndetection and explanation. SNIFFER employs two-stage instruction tuning on\nInstructBLIP. The first stage refines the model's concept alignment of generic\nobjects with news-domain entities and the second stage leverages language-only\nGPT-4 generated OOC-specific instruction data to fine-tune the model's\ndiscriminatory powers. Enhanced by external tools and retrieval, SNIFFER not\nonly detects inconsistencies between text and image but also utilizes external\nknowledge for contextual verification. Our experiments show that SNIFFER\nsurpasses the original MLLM by over 40% and outperforms state-of-the-art\nmethods in detection accuracy. SNIFFER also provides accurate and persuasive\nexplanations as validated by quantitative and human evaluations.",
        "pdf_link": "https://arxiv.org/pdf/2403.03170v1.pdf"
    },
    {
        "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
        "authors": [
            "Artem Vysogorets",
            "Achintya Gopal"
        ],
        "published": "2024-02-23T21:28:59Z",
        "summary": "Fine-tuning Large Language Models (LLMs) is now a common approach for text\nclassification in a wide range of applications. When labeled documents are\nscarce, active learning helps save annotation efforts but requires retraining\nof massive models on each acquisition iteration. We drastically expedite this\nprocess by using pretrained representations of LLMs within the active learning\nloop and, once the desired amount of labeled data is acquired, fine-tuning that\nor even a different pretrained LLM on this labeled data to achieve the best\nperformance. As verified on common text classification benchmarks with\npretrained BERT and RoBERTa as the backbone, our strategy yields similar\nperformance to fine-tuning all the way through the active learning loop but is\norders of magnitude less computationally expensive. The data acquired with our\nprocedure generalizes across pretrained networks, allowing flexibility in\nchoosing the final model or updating it as newer versions get released.",
        "pdf_link": "https://arxiv.org/pdf/2402.15613v1.pdf"
    },
    {
        "title": "Security and Privacy Challenges of Large Language Models: A Survey",
        "authors": [
            "Badhan Chandra Das",
            "M. Hadi Amini",
            "Yanzhao Wu"
        ],
        "published": "2024-01-30T04:00:54Z",
        "summary": "Large Language Models (LLMs) have demonstrated extraordinary capabilities and\ncontributed to multiple fields, such as generating and summarizing text,\nlanguage translation, and question-answering. Nowadays, LLM is becoming a very\npopular tool in computerized language processing tasks, with the capability to\nanalyze complicated linguistic patterns and provide relevant and appropriate\nresponses depending on the context. While offering significant advantages,\nthese models are also vulnerable to security and privacy attacks, such as\njailbreaking attacks, data poisoning attacks, and Personally Identifiable\nInformation (PII) leakage attacks. This survey provides a thorough review of\nthe security and privacy challenges of LLMs for both training data and users,\nalong with the application-based risks in various domains, such as\ntransportation, education, and healthcare. We assess the extent of LLM\nvulnerabilities, investigate emerging security and privacy attacks for LLMs,\nand review the potential defense mechanisms. Additionally, the survey outlines\nexisting research gaps in this domain and highlights future research\ndirections.",
        "pdf_link": "https://arxiv.org/pdf/2402.00888v1.pdf"
    },
    {
        "title": "Hallucination Benchmark in Medical Visual Question Answering",
        "authors": [
            "Jinge Wu",
            "Yunsoo Kim",
            "Honghan Wu"
        ],
        "published": "2024-01-11T10:52:17Z",
        "summary": "The recent success of large language and vision models (LLVMs) on vision\nquestion answering (VQA), particularly their applications in medicine\n(Med-VQA), has shown a great potential of realizing effective visual assistants\nfor healthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models' limitations and reveals the effectiveness\nof various prompting strategies.",
        "pdf_link": "https://arxiv.org/pdf/2401.05827v2.pdf"
    },
    {
        "title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations",
        "authors": [
            "Vinicius G. Goecks",
            "Nicholas Waytowich"
        ],
        "published": "2024-02-01T21:51:09Z",
        "summary": "The development of Courses of Action (COAs) in military operations is\ntraditionally a time-consuming and intricate process. Addressing this\nchallenge, this study introduces COA-GPT, a novel algorithm employing Large\nLanguage Models (LLMs) for rapid and efficient generation of valid COAs.\nCOA-GPT incorporates military doctrine and domain expertise to LLMs through\nin-context learning, allowing commanders to input mission information - in both\ntext and image formats - and receive strategically aligned COAs for review and\napproval. Uniquely, COA-GPT not only accelerates COA development, producing\ninitial COAs within seconds, but also facilitates real-time refinement based on\ncommander feedback. This work evaluates COA-GPT in a military-relevant scenario\nwithin a militarized version of the StarCraft II game, comparing its\nperformance against state-of-the-art reinforcement learning algorithms. Our\nresults demonstrate COA-GPT's superiority in generating strategically sound\nCOAs more swiftly, with added benefits of enhanced adaptability and alignment\nwith commander intentions. COA-GPT's capability to rapidly adapt and update\nCOAs during missions presents a transformative potential for military planning,\nparticularly in addressing planning discrepancies and capitalizing on emergent\nwindows of opportunities.",
        "pdf_link": "https://arxiv.org/pdf/2402.01786v2.pdf"
    },
    {
        "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language Models",
        "authors": [
            "Diego Carraro",
            "Derek Bridge"
        ],
        "published": "2024-01-21T14:33:52Z",
        "summary": "It has long been recognized that it is not enough for a Recommender System\n(RS) to provide recommendations based only on their relevance to users. Among\nmany other criteria, the set of recommendations may need to be diverse in order\nto handle uncertainty and offer a meaningful choice. The literature reports\nmany ways of measuring diversity and ways of improving the diversity of a set\nof recommendations, most notably by re-ranking and selecting from a larger set\nof candidate recommendations. Driven by promising insights from the literature\non how to incorporate versatile Large Language Models (LLMs) into the RS\npipeline, in this paper, we show how LLMs can be used for diversity re-ranking.\n  We begin with an informal study that verifies that LLMs can be used for\nre-ranking tasks and do have some understanding of the concept of diversity.\nThen, we design a more rigorous methodology where LLMs are prompted to generate\na diverse ranking from a candidate ranking using various prompt templates with\ndifferent re-ranking instructions in a zero-shot fashion. We conduct\ncomprehensive experiments testing state-of-the-art conversational LLMs from the\nGPT and Llama families. We compare their re-ranking capabilities with random\nre-ranking and various traditional re-ranking methods from the literature (MMR,\nxQuAD and RxQuAD). We find that LLM-based re-ranking outperforms random\nre-ranking across all the metrics that we use but does not perform as well as\nthe traditional re-ranking methods. We gain insight into prompt design for this\ntask (e.g.\\ on the whole, it is better to prompt for diversity rather than a\nbalance of diversity and relevance). Given that no special knowledge\nengineering is needed, we conclude that LLM-based re-ranking is a promising\napproach, and we highlight directions for future research. We open-source the\ncode of our experiments for reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2401.11506v1.pdf"
    },
    {
        "title": "Empowering Time Series Analysis with Large Language Models: A Survey",
        "authors": [
            "Yushan Jiang",
            "Zijie Pan",
            "Xikun Zhang",
            "Sahil Garg",
            "Anderson Schneider",
            "Yuriy Nevmyvaka",
            "Dongjin Song"
        ],
        "published": "2024-02-05T16:46:35Z",
        "summary": "Recently, remarkable progress has been made over large language models\n(LLMs), demonstrating their unprecedented capability in varieties of natural\nlanguage tasks. However, completely training a large general-purpose model from\nthe scratch is challenging for time series analysis, due to the large volumes\nand varieties of time series data, as well as the non-stationarity that leads\nto concept drift impeding continuous model adaptation and re-training. Recent\nadvances have shown that pre-trained LLMs can be exploited to capture complex\ndependencies in time series data and facilitate various applications. In this\nsurvey, we provide a systematic overview of existing methods that leverage LLMs\nfor time series analysis. Specifically, we first state the challenges and\nmotivations of applying language models in the context of time series as well\nas brief preliminaries of LLMs. Next, we summarize the general pipeline for\nLLM-based time series analysis, categorize existing methods into different\ngroups (i.e., direct query, tokenization, prompt design, fine-tune, and model\nintegration), and highlight the key ideas within each group. We also discuss\nthe applications of LLMs for both general and spatial-temporal time series\ndata, tailored to specific domains. Finally, we thoroughly discuss future\nresearch opportunities to empower time series analysis with LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.03182v1.pdf"
    },
    {
        "title": "PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs",
        "authors": [
            "An Liu",
            "Zonghan Yang",
            "Zhenhe Zhang",
            "Qingyuan Hu",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "published": "2024-02-20T09:02:55Z",
        "summary": "While Large language models (LLMs) have demonstrated considerable\ncapabilities across various natural language tasks, they often fall short of\nthe performance achieved by domain-specific state-of-the-art models. One\npotential approach to enhance domain-specific capabilities of LLMs involves\nfine-tuning them using corresponding datasets. However, this method can be both\nresource and time-intensive, and not applicable to closed-source commercial\nLLMs. In this paper, we propose Preference Adaptation for Enhancing\nDomain-specific Abilities of LLMs (PANDA), a method designed to augment the\ndomain-specific capabilities of LLMs by leveraging insights from the response\npreference of expert models without requiring fine-tuning. Our experimental\nresults reveal that PANDA significantly enhances the domain-specific ability of\nLLMs on text classification and interactive decision tasks. Moreover, LLM with\nPANDA even outperforms the expert model that being learned on 4 tasks of\nScienceWorld. This finding highlights the potential of exploring tuning-free\napproaches to achieve weak-to-strong generalization.",
        "pdf_link": "https://arxiv.org/pdf/2402.12835v1.pdf"
    },
    {
        "title": "Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs",
        "authors": [
            "Stepan Tytarenko",
            "Mohammad Ruhul Amin"
        ],
        "published": "2024-01-30T00:23:29Z",
        "summary": "Fine-tuning large pre-trained language models (LLMs) on particular datasets\nis a commonly employed strategy in Natural Language Processing (NLP)\nclassification tasks. However, this approach usually results in a loss of\nmodels generalizability. In this paper, we present a framework that allows for\nmaintaining generalizability, and enhances the performance on the downstream\ntask by utilizing task-specific context attribution. We show that a linear\ntransformation of the text representation from any transformer model using the\ntask-specific concept operator results in a projection onto the latent concept\nspace, referred to as context attribution in this paper. The specific concept\noperator is optimized during the supervised learning stage via novel loss\nfunctions. The proposed framework demonstrates that context attribution of the\ntext representation for each task objective can improve the capacity of the\ndiscriminator function and thus achieve better performance for the\nclassification task. Experimental results on three datasets, namely HateXplain,\nIMDB reviews, and Social Media Attributions, illustrate that the proposed model\nattains superior accuracy and generalizability. Specifically, for the\nnon-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in\naccuracy and 10% improvement in F1-score. Whereas for the IMDB dataset,\nfine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and\nF1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT\nfine-tuned on the IMDB dataset in conjunction with the proposed model improves\nthe F1-score on the HateXplain dataset by 7%. For the Social Media Attributions\ndataset of YouTube comments, we observe 5.2% increase in F1-metric. The\nproposed framework is implemented with PyTorch and provided open-source on\nGitHub.",
        "pdf_link": "https://arxiv.org/pdf/2401.16638v1.pdf"
    },
    {
        "title": "Knowledge Verification to Nip Hallucination in the Bud",
        "authors": [
            "Fanqi Wan",
            "Xinting Huang",
            "Leyang Cui",
            "Xiaojun Quan",
            "Wei Bi",
            "Shuming Shi"
        ],
        "published": "2024-01-19T15:39:49Z",
        "summary": "While large language models (LLMs) have demonstrated exceptional performance\nacross various tasks following human alignment, they may still generate\nresponses that sound plausible but contradict factual knowledge, a phenomenon\nknown as \\emph{hallucination}. In this paper, we demonstrate the feasibility of\nmitigating hallucinations by verifying and minimizing the inconsistency between\nexternal knowledge present in the alignment data and the intrinsic knowledge\nembedded within foundation LLMs. Specifically, we propose a novel approach\ncalled Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM\nto automatically formulate assessments based on external knowledge to evaluate\nthe knowledge boundaries of foundation LLMs. To address knowledge\ninconsistencies in the alignment data, KCA implements several specific\nstrategies to deal with these data instances. We demonstrate the superior\nefficacy of KCA in reducing hallucinations across six benchmarks, utilizing\nfoundation LLMs of varying backbones and scales. This confirms the\neffectiveness of mitigating hallucinations by reducing knowledge inconsistency.\nOur code, model weights, and data are public at\n\\url{https://github.com/fanqiwan/KCA}.",
        "pdf_link": "https://arxiv.org/pdf/2401.10768v3.pdf"
    },
    {
        "title": "Boosting Disfluency Detection with Large Language Model as Disfluency Generator",
        "authors": [
            "Zhenrong Cheng",
            "Jiayan Guo",
            "Hao Sun",
            "Yan Zhang"
        ],
        "published": "2024-03-13T04:14:33Z",
        "summary": "Current disfluency detection methods heavily rely on costly and scarce\nhuman-annotated data. To tackle this issue, some approaches employ heuristic or\nstatistical features to generate disfluent sentences, partially improving\ndetection performance. However, these sentences often deviate from real-life\nscenarios, constraining overall model enhancement. In this study, we propose a\nlightweight data augmentation approach for disfluency detection, utilizing the\nsuperior generative and semantic understanding capabilities of large language\nmodel (LLM) to generate disfluent sentences as augmentation data. We leverage\nLLM to generate diverse and more realistic sentences guided by specific\nprompts, without the need for fine-tuning the LLM. Subsequently, we apply an\nuncertainty-aware data filtering approach to improve the quality of the\ngenerated sentences, utilized in training a small detection model for improved\nperformance. Experiments using enhanced data yielded state-of-the-art results.\nThe results showed that using a small amount of LLM-generated enhanced data can\nsignificantly improve performance, thereby further enhancing\ncost-effectiveness.",
        "pdf_link": "https://arxiv.org/pdf/2403.08229v1.pdf"
    },
    {
        "title": "Exploring Unseen Environments with Robots using Large Language and Vision Models through a Procedurally Generated 3D Scene Representation",
        "authors": [
            "Arjun P S",
            "Andrew Melnik",
            "Gora Chand Nandi"
        ],
        "published": "2024-03-30T10:54:59Z",
        "summary": "Recent advancements in Generative Artificial Intelligence, particularly in\nthe realm of Large Language Models (LLMs) and Large Vision Language Models\n(LVLMs), have enabled the prospect of leveraging cognitive planners within\nrobotic systems. This work focuses on solving the object goal navigation\nproblem by mimicking human cognition to attend, perceive and store task\nspecific information and generate plans with the same. We introduce a\ncomprehensive framework capable of exploring an unfamiliar environment in\nsearch of an object by leveraging the capabilities of Large Language\nModels(LLMs) and Large Vision Language Models (LVLMs) in understanding the\nunderlying semantics of our world. A challenging task in using LLMs to generate\nhigh level sub-goals is to efficiently represent the environment around the\nrobot. We propose to use a 3D scene modular representation, with semantically\nrich descriptions of the object, to provide the LLM with task relevant\ninformation. But providing the LLM with a mass of contextual information (rich\n3D scene semantic representation), can lead to redundant and inefficient plans.\nWe propose to use an LLM based pruner that leverages the capabilities of\nin-context learning to prune out irrelevant goal specific information.",
        "pdf_link": "https://arxiv.org/pdf/2404.00318v1.pdf"
    },
    {
        "title": "Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines",
        "authors": [
            "Lijia Ma",
            "Xingchen Xu",
            "Yong Tan"
        ],
        "published": "2024-02-29T18:20:37Z",
        "summary": "In the domain of digital information dissemination, search engines act as\npivotal conduits linking information seekers with providers. The advent of\nchat-based search engines utilizing Large Language Models (LLMs) and Retrieval\nAugmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary\nleap in the search ecosystem. They demonstrate metacognitive abilities in\ninterpreting web information and crafting responses with human-like\nunderstanding and creativity. Nonetheless, the intricate nature of LLMs renders\ntheir \"cognitive\" processes opaque, challenging even their designers'\nunderstanding. This research aims to dissect the mechanisms through which an\nLLM-powered chat-based search engine, specifically Bing Chat, selects\ninformation sources for its responses. To this end, an extensive dataset has\nbeen compiled through engagements with New Bing, documenting the websites it\ncites alongside those listed by the conventional search engine. Employing\nnatural language processing (NLP) techniques, the research reveals that Bing\nChat exhibits a preference for content that is not only readable and formally\nstructured, but also demonstrates lower perplexity levels, indicating a unique\ninclination towards text that is predictable by the underlying LLM. Further\nenriching our analysis, we procure an additional dataset through interactions\nwith the GPT-4 based knowledge retrieval API, unveiling a congruent text\npreference between the RAG API and Bing Chat. This consensus suggests that\nthese text preferences intrinsically emerge from the underlying language\nmodels, rather than being explicitly crafted by Bing Chat's developers.\nMoreover, our investigation documents a greater similarity among websites cited\nby RAG technologies compared to those ranked highest by conventional search\nengines.",
        "pdf_link": "https://arxiv.org/pdf/2402.19421v1.pdf"
    },
    {
        "title": "GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?",
        "authors": [
            "Yiping Jin",
            "Leo Wanner",
            "Alexander Shvets"
        ],
        "published": "2024-02-23T10:02:01Z",
        "summary": "Online hate detection suffers from biases incurred in data sampling,\nannotation, and model pre-training. Therefore, measuring the averaged\nperformance over all examples in held-out test data is inadequate. Instead, we\nmust identify specific model weaknesses and be informed when it is more likely\nto fail. A recent proposal in this direction is HateCheck, a suite for testing\nfine-grained model functionalities on synthesized data generated using\ntemplates of the kind \"You are just a [slur] to me.\" However, despite enabling\nmore detailed diagnostic insights, the HateCheck test cases are often generic\nand have simplistic sentence structures that do not match the real-world data.\nTo address this limitation, we propose GPT-HateCheck, a framework to generate\nmore diverse and realistic functional tests from scratch by instructing large\nlanguage models (LLMs). We employ an additional natural language inference\n(NLI) model to verify the generations. Crowd-sourced annotation demonstrates\nthat the generated test cases are of high quality. Using the new functional\ntests, we can uncover model weaknesses that would be overlooked using the\noriginal HateCheck dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.15238v1.pdf"
    },
    {
        "title": "Prompt Stealing Attacks Against Large Language Models",
        "authors": [
            "Zeyang Sha",
            "Yang Zhang"
        ],
        "published": "2024-02-20T12:25:26Z",
        "summary": "The increasing reliance on large language models (LLMs) such as ChatGPT in\nvarious fields emphasizes the importance of ``prompt engineering,'' a\ntechnology to improve the quality of model outputs. With companies investing\nsignificantly in expert prompt engineers and educational resources rising to\nmeet market demand, designing high-quality prompts has become an intriguing\nchallenge. In this paper, we propose a novel attack against LLMs, named prompt\nstealing attacks. Our proposed prompt stealing attack aims to steal these\nwell-designed prompts based on the generated answers. The prompt stealing\nattack contains two primary modules: the parameter extractor and the prompt\nreconstruction. The goal of the parameter extractor is to figure out the\nproperties of the original prompts. We first observe that most prompts fall\ninto one of three categories: direct prompt, role-based prompt, and in-context\nprompt. Our parameter extractor first tries to distinguish the type of prompts\nbased on the generated answers. Then, it can further predict which role or how\nmany contexts are used based on the types of prompts. Following the parameter\nextractor, the prompt reconstructor can be used to reconstruct the original\nprompts based on the generated answers and the extracted features. The final\ngoal of the prompt reconstructor is to generate the reversed prompts, which are\nsimilar to the original prompts. Our experimental results show the remarkable\nperformance of our proposed attacks. Our proposed attacks add a new dimension\nto the study of prompt engineering and call for more attention to the security\nissues on LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.12959v1.pdf"
    },
    {
        "title": "Conversational Grounding: Annotation and Analysis of Grounding Acts and Grounding Units",
        "authors": [
            "Biswesh Mohapatra",
            "Seemab Hassan",
            "Laurent Romary",
            "Justine Cassell"
        ],
        "published": "2024-03-25T10:39:18Z",
        "summary": "Successful conversations often rest on common understanding, where all\nparties are on the same page about the information being shared. This process,\nknown as conversational grounding, is crucial for building trustworthy dialog\nsystems that can accurately keep track of and recall the shared information.\nThe proficiencies of an agent in grounding the conveyed information\nsignificantly contribute to building a reliable dialog system. Despite recent\nadvancements in dialog systems, there exists a noticeable deficit in their\ngrounding capabilities. Traum provided a framework for conversational grounding\nintroducing Grounding Acts and Grounding Units, but substantial progress,\nespecially in the realm of Large Language Models, remains lacking. To bridge\nthis gap, we present the annotation of two dialog corpora employing Grounding\nActs, Grounding Units, and a measure of their degree of grounding. We discuss\nour key findings during the annotation and also provide a baseline model to\ntest the performance of current Language Models in categorizing the grounding\nacts of the dialogs. Our work aims to provide a useful resource for further\nresearch in making conversations with machines better understood and more\nreliable in natural day-to-day collaborative dialogs.",
        "pdf_link": "https://arxiv.org/pdf/2403.16609v1.pdf"
    },
    {
        "title": "An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach",
        "authors": [
            "Mohammad Amaz Uddin",
            "Iqbal H. Sarker"
        ],
        "published": "2024-02-21T15:23:21Z",
        "summary": "Phishing email is a serious cyber threat that tries to deceive users by\nsending false emails with the intention of stealing confidential information or\ncausing financial harm. Attackers, often posing as trustworthy entities,\nexploit technological advancements and sophistication to make detection and\nprevention of phishing more challenging. Despite extensive academic research,\nphishing detection remains an ongoing and formidable challenge in the\ncybersecurity landscape. Large Language Models (LLMs) and Masked Language\nModels (MLMs) possess immense potential to offer innovative solutions to\naddress long-standing challenges. In this research paper, we present an\noptimized, fine-tuned transformer-based DistilBERT model designed for the\ndetection of phishing emails. In the detection process, we work with a phishing\nemail dataset and utilize the preprocessing techniques to clean and solve the\nimbalance class issues. Through our experiments, we found that our model\neffectively achieves high accuracy, demonstrating its capability to perform\nwell. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI)\ntechniques such as Local Interpretable Model-Agnostic Explanations (LIME) and\nTransformer Interpret to explain how our model makes predictions in the context\nof text classification for phishing emails.",
        "pdf_link": "https://arxiv.org/pdf/2402.13871v1.pdf"
    },
    {
        "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages",
        "authors": [
            "Zhuoyuan Mao",
            "Yen Yu"
        ],
        "published": "2024-01-11T10:28:17Z",
        "summary": "This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.",
        "pdf_link": "https://arxiv.org/pdf/2401.05811v1.pdf"
    },
    {
        "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning",
        "authors": [
            "Yuqiang Sun",
            "Daoyuan Wu",
            "Yue Xue",
            "Han Liu",
            "Wei Ma",
            "Lyuye Zhang",
            "Miaolei Shi",
            "Yang Liu"
        ],
        "published": "2024-01-29T14:32:27Z",
        "summary": "Large language models (LLMs) have demonstrated significant potential for many\ndownstream tasks, including those requiring human-level intelligence, such as\nvulnerability detection. However, recent attempts to use LLMs for vulnerability\ndetection are still preliminary, as they lack an in-depth understanding of a\nsubject LLM's vulnerability reasoning capability -- whether it originates from\nthe model itself or from external assistance, such as invoking tool support and\nretrieving vulnerability knowledge. In this paper, we aim to decouple LLMs'\nvulnerability reasoning capability from their other capabilities, including the\nability to actively seek additional information (e.g., via function calling in\nSOTA models), adopt relevant vulnerability knowledge (e.g., via vector-based\nmatching and retrieval), and follow instructions to output structured results.\nTo this end, we propose a unified evaluation framework named LLM4Vuln, which\nseparates LLMs' vulnerability reasoning from their other capabilities and\nevaluates how LLMs' vulnerability reasoning could be enhanced when combined\nwith the enhancement of other capabilities. To demonstrate the effectiveness of\nLLM4Vuln, we have designed controlled experiments using 75 ground-truth smart\ncontract vulnerabilities, which were extensively audited as high-risk on\nCode4rena from August to November 2023, and tested them in 4,950 different\nscenarios across three representative LLMs (GPT-4, Mixtral, and Code Llama).\nOur results not only reveal ten findings regarding the varying effects of\nknowledge enhancement, context supplementation, prompt schemes, and models but\nalso enable us to identify 9 zero-day vulnerabilities in two pilot bug bounty\nprograms with over 1,000 USD being awarded.",
        "pdf_link": "https://arxiv.org/pdf/2401.16185v1.pdf"
    },
    {
        "title": "Dia-LLaMA: Towards Large Language Model-driven CT Report Generation",
        "authors": [
            "Zhixuan Chen",
            "Luyang Luo",
            "Yequan Bie",
            "Hao Chen"
        ],
        "published": "2024-03-25T03:02:51Z",
        "summary": "Medical report generation has achieved remarkable advancements yet has still\nbeen faced with several challenges. First, the inherent imbalance in the\ndistribution of normal and abnormal cases may lead models to exhibit a biased\nfocus on normal samples, resulting in unreliable diagnoses. Second, the\nfrequent occurrence of common template sentences in the reports may overwhelm\nthe critical abnormal information. Moreover, existing works focus on 2D chest\nX-rays, leaving CT report generation underexplored due to the high-dimensional\nnature of CT images and the limited availability of CT-report pairs. Recently,\nLLM has shown a great ability to generate reliable answers with appropriate\nprompts, which shed light on addressing the aforementioned challenges. In this\npaper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report\ngeneration by incorporating diagnostic information as guidance prompts.\nConsidering the high dimension of CT, we leverage a pre-trained ViT3D with\nperceiver to extract the visual information. To tailor the LLM for report\ngeneration and emphasize abnormality, we extract additional diagnostic\ninformation by referring to a disease prototype memory bank, which is updated\nduring training to capture common disease representations. Furthermore, we\nintroduce disease-aware attention to enable the model to adjust attention for\ndifferent diseases. Experiments on the chest CT dataset demonstrated that our\nproposed method outperformed previous methods and achieved state-of-the-art on\nboth clinical efficacy performance and natural language generation metrics. The\ncode will be made publically available.",
        "pdf_link": "https://arxiv.org/pdf/2403.16386v1.pdf"
    },
    {
        "title": "Repeat After Me: Transformers are Better than State Space Models at Copying",
        "authors": [
            "Samy Jelassi",
            "David Brandfonbrener",
            "Sham M. Kakade",
            "Eran Malach"
        ],
        "published": "2024-02-01T21:44:11Z",
        "summary": "Transformers are the dominant architecture for sequence modeling, but there\nis growing interest in models that use a fixed-size latent state that does not\ndepend on the sequence length, which we refer to as \"generalized state space\nmodels\" (GSSMs). In this paper we show that while GSSMs are promising in terms\nof inference-time efficiency, they are limited compared to transformer models\non tasks that require copying from the input context. We start with a\ntheoretical analysis of the simple task of string copying and prove that a two\nlayer transformer can copy strings of exponential length while GSSMs are\nfundamentally limited by their fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of efficiency and generalization on\nsynthetic tasks that require copying the context. Finally, we evaluate\npretrained large language models and find that transformer models dramatically\noutperform state space models at copying and retrieving information from\ncontext. Taken together, these results suggest a fundamental gap between\ntransformers and GSSMs on tasks of practical interest.",
        "pdf_link": "https://arxiv.org/pdf/2402.01032v1.pdf"
    },
    {
        "title": "Can we obtain significant success in RST discourse parsing by using Large Language Models?",
        "authors": [
            "Aru Maekawa",
            "Tsutomu Hirao",
            "Hidetaka Kamigaito",
            "Manabu Okumura"
        ],
        "published": "2024-03-08T05:34:29Z",
        "summary": "Recently, decoder-only pre-trained large language models (LLMs), with several\ntens of billion parameters, have significantly impacted a wide range of natural\nlanguage processing (NLP) tasks. While encoder-only or encoder-decoder\npre-trained language models have already proved to be effective in discourse\nparsing, the extent to which LLMs can perform this task remains an open\nresearch question. Therefore, this paper explores how beneficial such LLMs are\nfor Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing\nprocess for both fundamental top-down and bottom-up strategies is converted\ninto prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with\nQLoRA, which has fewer parameters that can be tuned. Experimental results on\nthree benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate\nthat Llama 2 with 70 billion parameters in the bottom-up strategy obtained\nstate-of-the-art (SOTA) results with significant differences. Furthermore, our\nparsers demonstrated generalizability when evaluated on RST-DT, showing that,\nin spite of being trained with the GUM corpus, it obtained similar performances\nto those of existing parsers trained with RST-DT.",
        "pdf_link": "https://arxiv.org/pdf/2403.05065v1.pdf"
    },
    {
        "title": "Redefining \"Hallucination\" in LLMs: Towards a psychology-informed framework for mitigating misinformation",
        "authors": [
            "Elijah Berberette",
            "Jack Hutchins",
            "Amir Sadovnik"
        ],
        "published": "2024-02-01T03:01:11Z",
        "summary": "In recent years, large language models (LLMs) have become incredibly popular,\nwith ChatGPT for example being used by over a billion users. While these models\nexhibit remarkable language understanding and logical prowess, a notable\nchallenge surfaces in the form of \"hallucinations.\" This phenomenon results in\nLLMs outputting misinformation in a confident manner, which can lead to\ndevastating consequences with such a large user base. However, we question the\nappropriateness of the term \"hallucination\" in LLMs, proposing a psychological\ntaxonomy based on cognitive biases and other psychological phenomena. Our\napproach offers a more fine-grained understanding of this phenomenon, allowing\nfor targeted solutions. By leveraging insights from how humans internally\nresolve similar challenges, we aim to develop strategies to mitigate LLM\nhallucinations. This interdisciplinary approach seeks to move beyond\nconventional terminology, providing a nuanced understanding and actionable\npathways for improvement in LLM reliability.",
        "pdf_link": "https://arxiv.org/pdf/2402.01769v1.pdf"
    },
    {
        "title": "LLMBind: A Unified Modality-Task Integration Framework",
        "authors": [
            "Bin Zhu",
            "Peng Jin",
            "Munan Ning",
            "Bin Lin",
            "Jinfa Huang",
            "Qi Song",
            "Jiaxi Cui",
            "Junwu Zhang",
            "Zhenyu Tang",
            "Mingjun Pan",
            "Xing Zhou",
            "Li Yuan"
        ],
        "published": "2024-02-22T12:36:31Z",
        "summary": "While recent progress in multimodal large language models tackles various\nmodality tasks, they posses limited integration capabilities for complex\nmulti-modality tasks, consequently constraining the development of the field.\nIn this work, we take the initiative to explore and propose the LLMBind, a\nunified framework for modality task integration, which binds Large Language\nModels and corresponding pre-trained task models with task-specific tokens.\nConsequently, LLMBind can interpret inputs and produce outputs in versatile\ncombinations of image, text, video, and audio. Specifically, we introduce a\nMixture-of-Experts technique to enable effective learning for different\nmultimodal tasks through collaboration among diverse experts. Furthermore, we\ncreate a multi-task dataset comprising 400k instruction data, which unlocks the\nability for interactive visual generation and editing tasks. Extensive\nexperiments show the effectiveness of our framework across various tasks,\nincluding image, video, audio generation, image segmentation, and image\nediting. More encouragingly, our framework can be easily extended to other\nmodality tasks, showcasing the promising potential of creating a unified AI\nagent for modeling universal modalities.",
        "pdf_link": "https://arxiv.org/pdf/2402.14891v3.pdf"
    },
    {
        "title": "Evaluation of ChatGPT's Smart Contract Auditing Capabilities Based on Chain of Thought",
        "authors": [
            "Yuying Du",
            "Xueyan Tang"
        ],
        "published": "2024-02-19T10:33:29Z",
        "summary": "Smart contracts, as a key component of blockchain technology, play a crucial\nrole in ensuring the automation of transactions and adherence to protocol\nrules. However, smart contracts are susceptible to security vulnerabilities,\nwhich, if exploited, can lead to significant asset losses. This study explores\nthe potential of enhancing smart contract security audits using the GPT-4\nmodel. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark\nvulnerability library, containing 732 vulnerabilities, and compared it with\nfive other vulnerability detection tools to evaluate GPT-4's ability to\nidentify seven common types of vulnerabilities. Moreover, we assessed GPT-4's\nperformance in code parsing and vulnerability capture by simulating a\nprofessional auditor's auditing process using CoT(Chain of Thought) prompts\nbased on the audit reports of eight groups of smart contracts. We also\nevaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through\nexperimentation, we found that GPT-4 performed poorly in detecting smart\ncontract vulnerabilities, with a high Precision of 96.6%, but a low Recall of\n37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities\nduring detection. Meanwhile, it demonstrated good contract code parsing\ncapabilities, with an average comprehensive score of 6.5, capable of\nidentifying the background information and functional relationships of smart\ncontracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4\nhas significant potential application in PoC writing. These experimental\nresults indicate that GPT-4 lacks the ability to detect smart contract\nvulnerabilities effectively, but its performance in contract code parsing and\nPoC writing demonstrates its significant potential as an auxiliary tool in\nenhancing the efficiency and effectiveness of smart contract security audits.",
        "pdf_link": "https://arxiv.org/pdf/2402.12023v1.pdf"
    },
    {
        "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
        "authors": [
            "Phuc Phan",
            "Hieu Tran",
            "Long Phan"
        ],
        "published": "2024-02-21T17:20:38Z",
        "summary": "We propose a straightforward approach called Distillation Contrastive\nDecoding (DCD) to enhance the reasoning capabilities of Large Language Models\n(LLMs) during inference. In contrast to previous approaches that relied on\nsmaller amateur models or analysis of hidden state differences, DCD employs\nContrastive Chain-of-thought Prompting and advanced distillation techniques,\nincluding Dropout and Quantization. This approach effectively addresses the\nlimitations of Contrastive Decoding (CD), which typically requires both an\nexpert and an amateur model, thus increasing computational resource demands. By\nintegrating contrastive prompts with distillation, DCD obviates the need for an\namateur model and reduces memory usage. Our evaluations demonstrate that DCD\nsignificantly enhances LLM performance across a range of reasoning benchmarks,\nsurpassing both CD and existing methods in the GSM8K and StrategyQA datasets.",
        "pdf_link": "https://arxiv.org/pdf/2402.14874v1.pdf"
    },
    {
        "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?",
        "authors": [
            "Renrui Zhang",
            "Dongzhi Jiang",
            "Yichi Zhang",
            "Haokun Lin",
            "Ziyu Guo",
            "Pengshuo Qiu",
            "Aojun Zhou",
            "Pan Lu",
            "Kai-Wei Chang",
            "Peng Gao",
            "Hongsheng Li"
        ],
        "published": "2024-03-21T17:59:50Z",
        "summary": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io",
        "pdf_link": "https://arxiv.org/pdf/2403.14624v1.pdf"
    },
    {
        "title": "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models",
        "authors": [
            "Huijie Lv",
            "Xiao Wang",
            "Yuansen Zhang",
            "Caishuang Huang",
            "Shihan Dou",
            "Junjie Ye",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-26T16:35:59Z",
        "summary": "Adversarial misuse, particularly through `jailbreaking' that circumvents a\nmodel's safety and ethical protocols, poses a significant challenge for Large\nLanguage Models (LLMs). This paper delves into the mechanisms behind such\nsuccessful attacks, introducing a hypothesis for the safety mechanism of\naligned LLMs: intent security recognition followed by response generation.\nGrounded in this hypothesis, we propose CodeChameleon, a novel jailbreak\nframework based on personalized encryption tactics. To elude the intent\nsecurity recognition phase, we reformulate tasks into a code completion format,\nenabling users to encrypt queries using personalized encryption functions. To\nguarantee response generation functionality, we embed a decryption function\nwithin the instructions, which allows the LLM to decrypt and execute the\nencrypted queries successfully. We conduct extensive experiments on 7 LLMs,\nachieving state-of-the-art average Attack Success Rate (ASR). Remarkably, our\nmethod achieves an 86.6\\% ASR on GPT-4-1106.",
        "pdf_link": "https://arxiv.org/pdf/2402.16717v1.pdf"
    },
    {
        "title": "Large Model driven Radiology Report Generation with Clinical Quality Reinforcement Learning",
        "authors": [
            "Zijian Zhou",
            "Miaojing Shi",
            "Meng Wei",
            "Oluwatosin Alabi",
            "Zijie Yue",
            "Tom Vercauteren"
        ],
        "published": "2024-03-11T13:47:11Z",
        "summary": "Radiology report generation (RRG) has attracted significant attention due to\nits potential to reduce the workload of radiologists. Current RRG approaches\nare still unsatisfactory against clinical standards. This paper introduces a\nnovel RRG method, \\textbf{LM-RRG}, that integrates large models (LMs) with\nclinical quality reinforcement learning to generate accurate and comprehensive\nchest X-ray radiology reports. Our method first designs a large language model\ndriven feature extractor to analyze and interpret different regions of the\nchest X-ray image, emphasizing specific regions with medical significance.\nNext, based on the large model's decoder, we develop a multimodal report\ngenerator that leverages multimodal prompts from visual features and textual\ninstruction to produce the radiology report in an auto-regressive way. Finally,\nto better reflect the clinical significant and insignificant errors that\nradiologists would normally assign in the report, we introduce a novel clinical\nquality reinforcement learning strategy. It utilizes the radiology report\nclinical quality (RadCliQ) metric as a reward function in the learning process.\nExtensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the\nsuperiority of our method over the state of the art.",
        "pdf_link": "https://arxiv.org/pdf/2403.06728v1.pdf"
    },
    {
        "title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
        "authors": [
            "Chinmay Mittal",
            "Krishna Kartik",
            "Mausam",
            "Parag Singla"
        ],
        "published": "2024-02-04T20:56:09Z",
        "summary": "Recent works show that the largest of the large language models (LLMs) can\nsolve many simple reasoning tasks expressed in natural language, without\nany/much supervision. But, can they also solve challenging first-order\ncombinatorial reasoning problems, such as graph coloring, knapsack and\ncryptarithmetic? To answer this question, we present PuzzleBench, a dataset of\n31 such challenging problems along with a few solved instances for each\nproblem. These problems are all first order, i.e., they can be instantiated\nwith problem instances of varying sizes, and most of them are NP-hard,\nrequiring several reasoning steps to reach the solution. We first observe that\nLLMs, even when aided by symbolic solvers, perform rather poorly on our\ndataset. In response, we propose a new approach, Puzzle-LM, which combines LLMs\nwith both symbolic solvers and program interpreters, along with feedback from\nsolved examples, to achieve huge performance gains. Our extensive\nexperimentation and analyses offer new insights into the reasoning abilities\nand limitations of present-day LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.02611v2.pdf"
    },
    {
        "title": "NovelQA: A Benchmark for Long-Range Novel Question Answering",
        "authors": [
            "Cunxiang Wang",
            "Ruoxi Ning",
            "Boqi Pan",
            "Tonghui Wu",
            "Qipeng Guo",
            "Cheng Deng",
            "Guangsheng Bao",
            "Qian Wang",
            "Yue Zhang"
        ],
        "published": "2024-03-18T17:32:32Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has introduced a new\nfrontier in natural language processing, particularly in understanding and\nprocessing long-context information. However, the evaluation of these models'\nlong-context abilities remains a challenge due to the limitations of current\nbenchmarks. To address this gap, we introduce NovelQA, a benchmark specifically\ndesigned to test the capabilities of LLMs with extended texts. Constructed from\nEnglish novels, NovelQA offers a unique blend of complexity, length, and\nnarrative coherence, making it an ideal tool for assessing deep textual\nunderstanding in LLMs. This paper presents the design and construction of\nNovelQA, highlighting its manual annotation, and diverse question types. Our\nevaluation of Long-context LLMs on NovelQA reveals significant insights into\nthe models' performance, particularly emphasizing the challenges they face with\nmulti-hop reasoning, detail-oriented questions, and extremely long input with\nmore than 100,000 tokens. The results underscore the necessity for further\nadvancements in LLMs to improve their long-context comprehension and\ncomputational literary studies.",
        "pdf_link": "https://arxiv.org/pdf/2403.12766v1.pdf"
    },
    {
        "title": "DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models",
        "authors": [
            "Wendi Cui",
            "Jiaxin Zhang",
            "Zhuohang Li",
            "Lopez Damien",
            "Kamalika Das",
            "Bradley Malin",
            "Sricharan Kumar"
        ],
        "published": "2024-01-04T08:34:16Z",
        "summary": "Evaluating the quality and variability of text generated by Large Language\nModels (LLMs) poses a significant, yet unresolved research challenge.\nTraditional evaluation methods, such as ROUGE and BERTScore, which measure\ntoken similarity, often fail to capture the holistic semantic equivalence. This\nresults in a low correlation with human judgments and intuition, which is\nespecially problematic in high-stakes applications like healthcare and finance\nwhere reliability, safety, and robust decision-making are highly critical. This\nwork proposes DCR, an automated framework for evaluating and improving the\nconsistency of LLM-generated texts using a divide-conquer-reasoning approach.\nUnlike existing LLM-based evaluators that operate at the paragraph level, our\nmethod employs a divide-and-conquer evaluator (DCE) that breaks down the\nparagraph-to-paragraph comparison between two generated responses into\nindividual sentence-to-paragraph comparisons, each evaluated based on\npredefined criteria. To facilitate this approach, we introduce an automatic\nmetric converter (AMC) that translates the output from DCE into an\ninterpretable numeric score. Beyond the consistency evaluation, we further\npresent a reason-assisted improver (RAI) that leverages the analytical reasons\nwith explanations identified by DCE to generate new responses aimed at reducing\nthese inconsistencies. Through comprehensive and systematic empirical analysis,\nwe show that our approach outperforms state-of-the-art methods by a large\nmargin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the\nconsistency of LLM generation across multiple benchmarks in semantic, factual,\nand summarization consistency tasks. Our approach also substantially reduces\nnearly 90% of output inconsistencies, showing promise for effective\nhallucination mitigation.",
        "pdf_link": "https://arxiv.org/pdf/2401.02132v1.pdf"
    },
    {
        "title": "Comparing Inferential Strategies of Humans and Large Language Models in Deductive Reasoning",
        "authors": [
            "Philipp Mondorf",
            "Barbara Plank"
        ],
        "published": "2024-02-20T12:58:14Z",
        "summary": "Deductive reasoning plays a pivotal role in the formulation of sound and\ncohesive arguments. It allows individuals to draw conclusions that logically\nfollow, given the truth value of the information provided. Recent progress in\nthe domain of large language models (LLMs) has showcased their capability in\nexecuting deductive reasoning tasks. Nonetheless, a significant portion of\nresearch primarily assesses the accuracy of LLMs in solving such tasks, often\noverlooking a deeper analysis of their reasoning behavior. In this study, we\ndraw upon principles from cognitive psychology to examine inferential\nstrategies employed by LLMs, through a detailed evaluation of their responses\nto propositional logic problems. Our findings indicate that LLMs display\nreasoning patterns akin to those observed in humans, including strategies like\n$\\textit{supposition following}$ or $\\textit{chain construction}$. Moreover,\nour research demonstrates that the architecture and scale of the model\nsignificantly affect its preferred method of reasoning, with more advanced\nmodels tending to adopt strategies more frequently than less sophisticated\nones. Importantly, we assert that a model's accuracy, that is the correctness\nof its final conclusion, does not necessarily reflect the validity of its\nreasoning process. This distinction underscores the necessity for more nuanced\nevaluation procedures in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.14856v1.pdf"
    },
    {
        "title": "Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness",
        "authors": [
            "Samaneh Shafee",
            "Alysson Bessani",
            "Pedro M. Ferreira"
        ],
        "published": "2024-01-26T13:15:24Z",
        "summary": "Knowledge sharing about emerging threats is crucial in the rapidly advancing\nfield of cybersecurity and forms the foundation of Cyber Threat Intelligence\n(CTI). In this context, Large Language Models are becoming increasingly\nsignificant in the field of cybersecurity, presenting a wide range of\nopportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly,\nStanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary\nclassification and Named Entity Recognition (NER) tasks performed using Open\nSource INTelligence (OSINT). We utilize well-established data collected in\nprevious research from Twitter to assess the competitiveness of these chatbots\nwhen compared to specialized models trained for those tasks. In binary\nclassification experiments, Chatbot GPT-4 as a commercial model achieved an\nacceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1\nscore of 0.90. However, concerning cybersecurity entity recognition, all\nevaluated chatbots have limitations and are less effective. This study\ndemonstrates the capability of chatbots for OSINT binary classification and\nshows that they require further improvement in NER to effectively replace\nspecially trained models. Our results shed light on the limitations of the LLM\nchatbots when compared to specialized models, and can help researchers improve\nchatbots technology with the objective to reduce the required effort to\nintegrate machine learning in OSINT-based CTI tools.",
        "pdf_link": "https://arxiv.org/pdf/2401.15127v2.pdf"
    },
    {
        "title": "Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity",
        "authors": [
            "Xin Zhang",
            "Linhai Zhang",
            "Deyu Zhou",
            "Guoqiang Xu"
        ],
        "published": "2024-03-10T08:59:04Z",
        "summary": "Due to the sparsity of user data, sentiment analysis on user reviews in\ne-commerce platforms often suffers from poor performance, especially when faced\nwith extremely sparse user data or long-tail labels. Recently, the emergence of\nLLMs has introduced new solutions to such problems by leveraging graph\nstructures to generate supplementary user profiles. However, previous\napproaches have not fully utilized the graph understanding capabilities of LLMs\nand have struggled to adapt to complex streaming data environments. In this\nwork, we propose a fine-grained streaming data synthesis framework that\ncategorizes sparse users into three categories: Mid-tail, Long-tail, and\nExtreme. Specifically, we design LLMs to comprehensively understand three key\ngraph elements in streaming data, including Local-global Graph Understanding,\nSecond-Order Relationship Extraction, and Product Attribute Understanding,\nwhich enables the generation of high-quality synthetic data to effectively\naddress sparsity across different categories. Experimental results on three\nreal datasets demonstrate significant performance improvements, with\nsynthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.06139v1.pdf"
    },
    {
        "title": "Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values",
        "authors": [
            "Jon Chun",
            "Katherine Elkins"
        ],
        "published": "2024-01-09T14:57:30Z",
        "summary": "With the rise of individual and collaborative networks of autonomous agents,\nAI is deployed in more key reasoning and decision-making roles. For this\nreason, ethics-based audits play a pivotal role in the rapidly growing fields\nof AI safety and regulation. This paper undertakes an ethics-based audit to\nprobe the 8 leading commercial and open-source Large Language Models including\nGPT-4. We assess explicability and trustworthiness by a) establishing how well\ndifferent models engage in moral reasoning and b) comparing normative values\nunderlying models as ethical frameworks. We employ an experimental,\nevidence-based approach that challenges the models with ethical dilemmas in\norder to probe human-AI alignment. The ethical scenarios are designed to\nrequire a decision in which the particulars of the situation may or may not\nnecessitate deviating from normative ethical principles. A sophisticated\nethical framework was consistently elicited in one model, GPT-4. Nonetheless,\ntroubling findings include underlying normative frameworks with clear bias\ntowards particular cultural norms. Many models also exhibit disturbing\nauthoritarian tendencies. Code is available at\nhttps://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.",
        "pdf_link": "https://arxiv.org/pdf/2402.01651v1.pdf"
    },
    {
        "title": "UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction",
        "authors": [
            "Yansong Ning",
            "Hao Liu"
        ],
        "published": "2024-02-10T01:50:19Z",
        "summary": "Urban knowledge graph has recently worked as an emerging building block to\ndistill critical knowledge from multi-sourced urban data for diverse urban\napplication scenarios. Despite its promising benefits, urban knowledge graph\nconstruction (UrbanKGC) still heavily relies on manual effort, hindering its\npotential advancement. This paper presents UrbanKGent, a unified large language\nmodel agent framework, for urban knowledge graph construction. Specifically, we\nfirst construct the knowledgeable instruction set for UrbanKGC tasks (such as\nrelational triplet extraction and knowledge graph completion) via\nheterogeneity-aware and geospatial-infused instruction generation. Moreover, we\npropose a tool-augmented iterative trajectory refinement module to enhance and\nrefine the trajectories distilled from GPT-4. Through hybrid instruction\nfine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC\nagent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world\ndatasets using both human and GPT-4 self-evaluation. The experimental results\ndemonstrate that UrbanKGent-13B not only can significantly outperform 21\nbaselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4,\nby more than 10\\% with approximately 20 times lower cost. We deploy\nUrbanKGent-13B to provide online services, which can construct an UrbanKG with\nthousands of times richer relationships using only one-fifth of the data\ncompared with the existing benchmark. Our data, code, and opensource UrbanKGC\nagent are available at https://github.com/usail-hkust/UrbanKGent.",
        "pdf_link": "https://arxiv.org/pdf/2402.06861v1.pdf"
    },
    {
        "title": "Towards Optimizing the Costs of LLM Usage",
        "authors": [
            "Shivanshu Shekhar",
            "Tanishq Dubey",
            "Koyel Mukherjee",
            "Apoorv Saxena",
            "Atharv Tyagi",
            "Nishanth Kotla"
        ],
        "published": "2024-01-29T16:36:31Z",
        "summary": "Generative AI and LLMs in particular are heavily used nowadays for various\ndocument processing tasks such as question answering and summarization.\nHowever, different LLMs come with different capabilities for different tasks as\nwell as with different costs, tokenization, and latency. In fact, enterprises\nare already incurring huge costs of operating or using LLMs for their\nrespective use cases.\n  In this work, we propose optimizing the usage costs of LLMs by estimating\ntheir output quality (without actually invoking the LLMs), and then solving an\noptimization routine for the LLM selection to either keep costs under a budget,\nor minimize the costs, in a quality and latency aware manner. We propose a\nmodel to predict the output quality of LLMs on document processing tasks like\nsummarization, followed by an LP rounding algorithm to optimize the selection\nof LLMs. We study optimization problems trading off the quality and costs, both\ntheoretically and empirically. We further propose a sentence simplification\nmodel for reducing the number of tokens in a controlled manner. Additionally,\nwe propose several deterministic heuristics for reducing tokens in a quality\naware manner, and study the related optimization problem of applying the\nheuristics optimizing the quality and cost trade-off. We perform extensive\nempirical validation of our methods on not only enterprise datasets but also on\nopen-source datasets, annotated by us, and show that we perform much better\ncompared to closest baselines. Our methods reduce costs by 40%- 90% while\nimproving quality by 4%-7%. We will release the annotated open source datasets\nto the community for further research and exploration.",
        "pdf_link": "https://arxiv.org/pdf/2402.01742v1.pdf"
    },
    {
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding",
        "authors": [
            "Zilong Wang",
            "Hao Zhang",
            "Chun-Liang Li",
            "Julian Martin Eisenschlos",
            "Vincent Perot",
            "Zifeng Wang",
            "Lesly Miculicich",
            "Yasuhisa Fujii",
            "Jingbo Shang",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published": "2024-01-09T07:46:26Z",
        "summary": "Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.",
        "pdf_link": "https://arxiv.org/pdf/2401.04398v2.pdf"
    },
    {
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Tinghui Zhu",
            "Renze Lou",
            "Yuandong Tian",
            "Yanghua Xiao",
            "Yu Su"
        ],
        "published": "2024-02-02T18:39:51Z",
        "summary": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
        "pdf_link": "https://arxiv.org/pdf/2402.01622v2.pdf"
    },
    {
        "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
        "authors": [
            "Gabriel Grand",
            "Valerio Pepe",
            "Jacob Andreas",
            "Joshua B. Tenenbaum"
        ],
        "published": "2024-02-29T18:58:15Z",
        "summary": "Questions combine our mastery of language with our remarkable facility for\nreasoning about uncertainty. How do people navigate vast hypothesis spaces to\npose informative questions given limited cognitive resources? We study these\ntradeoffs in a classic grounded question-asking task based on the board game\nBattleship. Our language-informed program sampling (LIPS) model uses large\nlanguage models (LLMs) to generate natural language questions, translate them\ninto symbolic programs, and evaluate their expected information gain. We find\nthat with a surprisingly modest resource budget, this simple Monte Carlo\noptimization strategy yields informative questions that mirror human\nperformance across varied Battleship board scenarios. In contrast, LLM-only\nbaselines struggle to ground questions in the board state; notably, GPT-4V\nprovides no improvement over non-visual baselines. Our results illustrate how\nBayesian models of question-asking can leverage the statistics of language to\ncapture human priors, while highlighting some shortcomings of pure LLMs as\ngrounded reasoners.",
        "pdf_link": "https://arxiv.org/pdf/2402.19471v1.pdf"
    },
    {
        "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
        "authors": [
            "Lucio Dery",
            "Steven Kolawole",
            "Jean-Fran\u00e7ois Kagy",
            "Virginia Smith",
            "Graham Neubig",
            "Ameet Talwalkar"
        ],
        "published": "2024-02-08T04:48:26Z",
        "summary": "Given the generational gap in available hardware between lay practitioners\nand the most endowed institutions, LLMs are becoming increasingly inaccessible\nas they grow in size. Whilst many approaches have been proposed to compress\nLLMs to make their resource consumption manageable, these methods themselves\ntend to be resource intensive, putting them out of the reach of the very user\ngroups they target. In this work, we explore the problem of structured pruning\nof LLMs using only forward passes. We seek to empower practitioners to prune\nmodels so large that their available hardware has just enough memory to run\ninference. We develop Bonsai, a gradient-free, perturbative pruning method\ncapable of delivering small, fast, and accurate pruned models.\n  We observe that Bonsai outputs pruned models that (i) outperform those\ngenerated by more expensive gradient-based structured pruning methods, and (ii)\nare twice as fast (with comparable accuracy) as those generated by\nsemi-structured pruning methods requiring comparable resources as Bonsai. We\nalso leverage Bonsai to produce a new sub-2B model using a single A6000 that\nyields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM\nleaderboard.",
        "pdf_link": "https://arxiv.org/pdf/2402.05406v2.pdf"
    },
    {
        "title": "Exploring Low-Resource Medical Image Classification with Weakly Supervised Prompt Learning",
        "authors": [
            "Fudan Zheng",
            "Jindong Cao",
            "Weijiang Yu",
            "Zhiguang Chen",
            "Nong Xiao",
            "Yutong Lu"
        ],
        "published": "2024-02-06T07:53:23Z",
        "summary": "Most advances in medical image recognition supporting clinical auxiliary\ndiagnosis meet challenges due to the low-resource situation in the medical\nfield, where annotations are highly expensive and professional. This\nlow-resource problem can be alleviated by leveraging the transferable\nrepresentations of large-scale pre-trained vision-language models via relevant\nmedical text prompts. However, existing pre-trained vision-language models\nrequire domain experts to carefully design the medical prompts, which greatly\nincreases the burden on clinicians. To address this problem, we propose a\nweakly supervised prompt learning method MedPrompt to automatically generate\nmedical prompts, which includes an unsupervised pre-trained vision-language\nmodel and a weakly supervised prompt learning model. The unsupervised\npre-trained vision-language model utilizes the natural correlation between\nmedical images and corresponding medical texts for pre-training, without any\nmanual annotations. The weakly supervised prompt learning model only utilizes\nthe classes of images in the dataset to guide the learning of the specific\nclass vector in the prompt, while the learning of other context vectors in the\nprompt requires no manual annotations for guidance. To the best of our\nknowledge, this is the first model to automatically generate medical prompts.\nWith these prompts, the pre-trained vision-language model can be freed from the\nstrong expert dependency of manual annotation and manual prompt design.\nExperimental results show that the model using our automatically generated\nprompts outperforms its full-shot learning hand-crafted prompts counterparts\nwith only a minimal number of labeled samples for few-shot learning, and\nreaches superior or comparable accuracy on zero-shot image classification. The\nproposed prompt generator is lightweight and therefore can be embedded into any\nnetwork architecture.",
        "pdf_link": "https://arxiv.org/pdf/2402.03783v1.pdf"
    },
    {
        "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
        "authors": [
            "Raghav Kapoor",
            "Yash Parag Butala",
            "Melisa Russak",
            "Jing Yu Koh",
            "Kiran Kamble",
            "Waseem Alshikh",
            "Ruslan Salakhutdinov"
        ],
        "published": "2024-02-27T14:47:53Z",
        "summary": "For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.",
        "pdf_link": "https://arxiv.org/pdf/2402.17553v2.pdf"
    },
    {
        "title": "Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets",
        "authors": [
            "Nikita Moghe",
            "Arnisa Fazla",
            "Chantal Amrhein",
            "Tom Kocmi",
            "Mark Steedman",
            "Alexandra Birch",
            "Rico Sennrich",
            "Liane Guillou"
        ],
        "published": "2024-01-29T17:17:42Z",
        "summary": "Recent machine translation (MT) metrics calibrate their effectiveness by\ncorrelating with human judgement but without any insights about their behaviour\nacross different error types. Challenge sets are used to probe specific\ndimensions of metric behaviour but there are very few such datasets and they\neither focus on a limited number of phenomena or a limited number of language\npairs. We introduce ACES, a contrastive challenge set spanning 146 language\npairs, aimed at discovering whether metrics can identify 68 translation\naccuracy errors. These phenomena range from simple alterations at the\nword/character level to more complex errors based on discourse and real-world\nknowledge. We conduct a large-scale study by benchmarking ACES on 50 metrics\nsubmitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metric\nperformance, assess their incremental performance over successive campaigns,\nand measure their sensitivity to a range of linguistic phenomena. We also\ninvestigate claims that Large Language Models (LLMs) are effective as MT\nevaluators by evaluating on ACES. Our results demonstrate that different metric\nfamilies struggle with different phenomena and that LLM-based methods fail to\ndemonstrate reliable performance. Our analyses indicate that most metrics\nignore the source sentence, tend to prefer surface-level overlap and end up\nincorporating properties of base models which are not always beneficial. We\nexpand ACES to include error span annotations, denoted as SPAN-ACES and we use\nthis dataset to evaluate span-based error metrics showing these metrics also\nneed considerable improvement. Finally, we provide a set of recommendations for\nbuilding better MT metrics, including focusing on error labels instead of\nscores, ensembling, designing strategies to explicitly focus on the source\nsentence, focusing on semantic content and choosing the right base model for\nrepresentations.",
        "pdf_link": "https://arxiv.org/pdf/2401.16313v1.pdf"
    },
    {
        "title": "AraTrust: An Evaluation of Trustworthiness for LLMs in Arabic",
        "authors": [
            "Emad A. Alghamdi",
            "Reem I. Masoud",
            "Deema Alnuhait",
            "Afnan Y. Alomairi",
            "Ahmed Ashraf",
            "Mohamed Zaytoon"
        ],
        "published": "2024-03-14T00:45:24Z",
        "summary": "The swift progress and widespread acceptance of artificial intelligence (AI)\nsystems highlight a pressing requirement to comprehend both the capabilities\nand potential risks associated with AI. Given the linguistic complexity,\ncultural richness, and underrepresented status of Arabic in AI research, there\nis a pressing need to focus on Large Language Models (LLMs) performance and\nsafety for Arabic related tasks. Despite some progress in their development,\nthere is a lack of comprehensive trustworthiness evaluation benchmarks which\npresents a major challenge in accurately assessing and improving the safety of\nLLMs when prompted in Arabic. In this paper, we introduce AraTrust, the first\ncomprehensive trustworthiness benchmark for LLMs in Arabic. AraTrust comprises\n516 human-written multiple-choice questions addressing diverse dimensions\nrelated to truthfulness, ethics, safety, physical health, mental health,\nunfairness, illegal activities, privacy, and offensive language. We evaluated a\nset of LLMs against our benchmark to assess their trustworthiness. GPT-4 was\nthe most trustworthy LLM, while open-source models, particularly AceGPT 7B and\nJais 13B, struggled to achieve a score of 60% in our benchmark.",
        "pdf_link": "https://arxiv.org/pdf/2403.09017v2.pdf"
    },
    {
        "title": "Open Source Conversational LLMs do not know most Spanish words",
        "authors": [
            "Javier Conde",
            "Miguel Gonz\u00e1lez",
            "Nina Melero",
            "Raquel Ferrando",
            "Gonzalo Mart\u00ednez",
            "Elena Merino-G\u00f3mez",
            "Jos\u00e9 Alberto Hern\u00e1ndez",
            "Pedro Reviriego"
        ],
        "published": "2024-03-21T15:41:02Z",
        "summary": "The growing interest in Large Language Models (LLMs) and in particular in\nconversational models with which users can interact has led to the development\nof a large number of open-source chat LLMs. These models are evaluated on a\nwide range of benchmarks to assess their capabilities in answering questions or\nsolving problems on almost any possible topic or to test their ability to\nreason or interpret texts. Instead, the evaluation of the knowledge that these\nmodels have of the languages has received much less attention. For example, the\nwords that they can recognize and use in different languages. In this paper, we\nevaluate the knowledge that open-source chat LLMs have of Spanish words by\ntesting a sample of words in a reference dictionary. The results show that\nopen-source chat LLMs produce incorrect meanings for an important fraction of\nthe words and are not able to use most of the words correctly to write\nsentences with context. These results show how Spanish is left behind in the\nopen-source LLM race and highlight the need to push for linguistic fairness in\nconversational LLMs ensuring that they provide similar performance across\nlanguages.",
        "pdf_link": "https://arxiv.org/pdf/2403.15491v1.pdf"
    },
    {
        "title": "InCoRo: In-Context Learning for Robotics Control with Feedback Loops",
        "authors": [
            "Jiaqiang Ye Zhu",
            "Carla Gomez Cano",
            "David Vazquez Bermudez",
            "Michal Drozdzal"
        ],
        "published": "2024-02-07T19:01:11Z",
        "summary": "One of the challenges in robotics is to enable robotic units with the\nreasoning capability that would be robust enough to execute complex tasks in\ndynamic environments. Recent advances in LLMs have positioned them as go-to\ntools for simple reasoning tasks, motivating the pioneering work of Liang et\nal. [35] that uses an LLM to translate natural language commands into low-level\nstatic execution plans for robotic units. Using LLMs inside robotics systems\nbrings their generalization to a new level, enabling zero-shot generalization\nto new tasks. This paper extends this prior work to dynamic environments. We\npropose InCoRo, a system that uses a classical robotic feedback loop composed\nof an LLM controller, a scene understanding unit, and a robot. Our system\ncontinuously analyzes the state of the environment and provides adapted\nexecution commands, enabling the robot to adjust to changing environmental\nconditions and correcting for controller errors. Our system does not require\nany iterative optimization to learn to accomplish a task as it leverages\nin-context learning with an off-the-shelf LLM model. Through an extensive\nvalidation process involving two standardized industrial robotic units -- SCARA\nand DELTA types -- we contribute knowledge about these robots, not popular in\nthe community, thereby enriching it. We highlight the generalization\ncapabilities of our system and show that (1) in-context learning in combination\nwith the current state-of-the-art LLMs is an effective way to implement a\nrobotic controller; (2) in static environments, InCoRo surpasses the prior art\nin terms of the success rate; (3) in dynamic environments, we establish new\nstate-of-the-art for the SCARA and DELTA units, respectively. This research\npaves the way towards building reliable, efficient, intelligent autonomous\nsystems that adapt to dynamic environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.05188v1.pdf"
    },
    {
        "title": "Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis",
        "authors": [
            "Takehiro Takayanagi",
            "Masahiro Suzuki",
            "Ryotaro Kobayashi",
            "Hiroki Sakaji",
            "Kiyoshi Izumi"
        ],
        "published": "2024-02-22T12:19:04Z",
        "summary": "Causality is fundamental in human cognition and has drawn attention in\ndiverse research fields. With growing volumes of textual data, discerning\ncausalities within text data is crucial, and causal text mining plays a pivotal\nrole in extracting meaningful patterns. This study conducts comprehensive\nevaluations of ChatGPT's causal text mining capabilities. Firstly, we introduce\na benchmark that extends beyond general English datasets, including\ndomain-specific and non-English datasets. We also provide an evaluation\nframework to ensure fair comparisons between ChatGPT and previous approaches.\nFinally, our analysis outlines the limitations and future challenges in\nemploying ChatGPT for causal text mining. Specifically, our analysis reveals\nthat ChatGPT serves as a good starting point for various datasets. However,\nwhen equipped with a sufficient amount of training data, previous models still\nsurpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency\nto falsely recognize non-causal sequences as causal sequences. These issues\nbecome even more pronounced with advanced versions of the model, such as GPT-4.\nIn addition, we highlight the constraints of ChatGPT in handling complex\ncausality types, including both intra/inter-sentential and implicit causality.\nThe model also faces challenges with effectively leveraging in-context learning\nand domain adaptation. We release our code to support further research and\ndevelopment in this field.",
        "pdf_link": "https://arxiv.org/pdf/2402.14484v2.pdf"
    },
    {
        "title": "RegionGPT: Towards Region Understanding Vision Language Model",
        "authors": [
            "Qiushan Guo",
            "Shalini De Mello",
            "Hongxu Yin",
            "Wonmin Byeon",
            "Ka Chun Cheung",
            "Yizhou Yu",
            "Ping Luo",
            "Sifei Liu"
        ],
        "published": "2024-03-04T18:58:08Z",
        "summary": "Vision language models (VLMs) have experienced rapid advancements through the\nintegration of large language models (LLMs) with image-text pairs, yet they\nstruggle with detailed regional visual understanding due to limited spatial\nawareness of the vision encoder, and the use of coarse-grained training data\nthat lacks detailed, region-specific captions. To address this, we introduce\nRegionGPT (short as RGPT), a novel framework designed for complex region-level\ncaptioning and understanding. RGPT enhances the spatial awareness of regional\nrepresentation with simple yet effective modifications to existing visual\nencoders in VLMs. We further improve performance on tasks requiring a specific\noutput scope by integrating task-guided instruction prompts during both\ntraining and inference phases, while maintaining the model's versatility for\ngeneral-purpose tasks. Additionally, we develop an automated region caption\ndata generation pipeline, enriching the training set with detailed region-level\ncaptions. We demonstrate that a universal RGPT model can be effectively applied\nand significantly enhancing performance across a range of region-level tasks,\nincluding but not limited to complex region descriptions, reasoning, object\nclassification, and referring expressions comprehension.",
        "pdf_link": "https://arxiv.org/pdf/2403.02330v1.pdf"
    },
    {
        "title": "Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social Media Communications",
        "authors": [
            "Yuhang Zhou",
            "Paiheng Xu",
            "Xiyao Wang",
            "Xuan Lu",
            "Ge Gao",
            "Wei Ai"
        ],
        "published": "2024-01-22T06:02:39Z",
        "summary": "Emojis, which encapsulate semantics beyond mere words or phrases, have become\nprevalent in social network communications. This has spurred increasing\nscholarly interest in exploring their attributes and functionalities. However,\nemoji-related research and application face two primary challenges. First,\nresearchers typically rely on crowd-sourcing to annotate emojis in order to\nunderstand their sentiments, usage intentions, and semantic meanings. Second,\nsubjective interpretations by users can often lead to misunderstandings of\nemojis and cause the communication barrier. Large Language Models (LLMs) have\nachieved significant success in various annotation tasks, with ChatGPT\ndemonstrating expertise across multiple domains. In our study, we assess\nChatGPT's effectiveness in handling previously annotated and downstream tasks.\nOur objective is to validate the hypothesis that ChatGPT can serve as a viable\nalternative to human annotators in emoji research and that its ability to\nexplain emoji meanings can enhance clarity and transparency in online\ncommunications. Our findings indicate that ChatGPT has extensive knowledge of\nemojis. It is adept at elucidating the meaning of emojis across various\napplication scenarios and demonstrates the potential to replace human\nannotators in a range of tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01681v2.pdf"
    },
    {
        "title": "BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering",
        "authors": [
            "Haoyu Wang",
            "Tuo Zhao",
            "Jing Gao"
        ],
        "published": "2024-02-16T23:28:02Z",
        "summary": "Retrieval-augmented Large Language Models (LLMs) offer substantial benefits\nin enhancing performance across knowledge-intensive scenarios. However, these\nmethods often face challenges with complex inputs and encounter difficulties\ndue to noisy knowledge retrieval, notably hindering model effectiveness. To\naddress this issue, we introduce BlendFilter, a novel approach that elevates\nretrieval-augmented LLMs by integrating query generation blending with\nknowledge filtering. BlendFilter proposes the blending process through its\nquery generation method, which integrates both external and internal knowledge\naugmentation with the original query, ensuring comprehensive information\ngathering. Additionally, our distinctive knowledge filtering module capitalizes\non the intrinsic capabilities of the LLM, effectively eliminating extraneous\ndata. We conduct extensive experiments on three open-domain question answering\nbenchmarks, and the findings clearly indicate that our innovative BlendFilter\nsurpasses state-of-the-art baselines significantly.",
        "pdf_link": "https://arxiv.org/pdf/2402.11129v1.pdf"
    },
    {
        "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
        "authors": [
            "Hangyu Wang",
            "Jianghao Lin",
            "Bo Chen",
            "Yang Yang",
            "Ruiming Tang",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "published": "2024-03-06T08:31:35Z",
        "summary": "The significant advancements in large language models (LLMs) give rise to a\npromising research direction, i.e., leveraging LLMs as recommenders (LLMRec).\nThe efficacy of LLMRec arises from the open-world knowledge and reasoning\ncapabilities inherent in LLMs. LLMRec acquires the recommendation capabilities\nthrough instruction tuning based on user interaction data. However, in order to\nprotect user privacy and optimize utility, it is also crucial for LLMRec to\nintentionally forget specific user data, which is generally referred to as\nrecommendation unlearning. In the era of LLMs, recommendation unlearning poses\nnew challenges for LLMRec in terms of \\textit{inefficiency} and\n\\textit{ineffectiveness}. Existing unlearning methods require updating billions\nof parameters in LLMRec, which is costly and time-consuming. Besides, they\nalways impact the model utility during the unlearning process. To this end, we\npropose \\textbf{E2URec}, the first \\underline{E}fficient and\n\\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our\nproposed E2URec enhances the unlearning efficiency by updating only a few\nadditional LoRA parameters, and improves the unlearning effectiveness by\nemploying a teacher-student framework, where we maintain multiple teacher\nnetworks to guide the unlearning process. Extensive experiments show that\nE2URec outperforms state-of-the-art baselines on two real-world datasets.\nSpecifically, E2URec can efficiently forget specific data without affecting\nrecommendation performance. The source code is at\n\\url{https://github.com/justarter/E2URec}.",
        "pdf_link": "https://arxiv.org/pdf/2403.03536v1.pdf"
    },
    {
        "title": "Breaking MLPerf Training: A Case Study on Optimizing BERT",
        "authors": [
            "Yongdeok Kim",
            "Jaehyung Ahn",
            "Myeongwoo Kim",
            "Changin Choi",
            "Heejae Kim",
            "Narankhuu Tuvshinjargal",
            "Seungwon Lee",
            "Yanzi Zhang",
            "Yuan Pei",
            "Xiongzhan Linghu",
            "Jingkun Ma",
            "Lin Chen",
            "Yuehua Dai",
            "Sungjoo Yoo"
        ],
        "published": "2024-02-04T11:12:17Z",
        "summary": "Speeding up the large-scale distributed training is challenging in that it\nrequires improving various components of training including load balancing,\ncommunication, optimizers, etc. We present novel approaches for fast\nlarge-scale training of BERT model which individually ameliorates each\ncomponent thereby leading to a new level of BERT training performance. Load\nbalancing is imperative in distributed BERT training since its training\ndatasets are characterized by samples with various lengths. Communication cost,\nwhich is proportional to the scale of distributed training, needs to be hidden\nby useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc.,\nneed to be carefully re-evaluated in the context of large-scale distributed\ntraining. We propose two new ideas, (1) local presorting based on dataset\nstratification for load balancing and (2) bucket-wise gradient clipping before\nallreduce which allows us to benefit from the overlap of gradient computation\nand synchronization as well as the fast training of gradient clipping before\nallreduce. We also re-evaluate existing optimizers via hyperparameter\noptimization and utilize ADAM, which also contributes to fast training via\nlarger batches than existing methods. Our proposed methods, all combined, give\nthe fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100\nGPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one)\nsubmissions to MLPerf v1.1 (v2.0). Our implementation and evaluation results\nare available at MLPerf v1.1~v2.1.",
        "pdf_link": "https://arxiv.org/pdf/2402.02447v1.pdf"
    },
    {
        "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
        "authors": [
            "Xuansheng Wu",
            "Haiyan Zhao",
            "Yaochen Zhu",
            "Yucheng Shi",
            "Fan Yang",
            "Tianming Liu",
            "Xiaoming Zhai",
            "Wenlin Yao",
            "Jundong Li",
            "Mengnan Du",
            "Ninghao Liu"
        ],
        "published": "2024-03-13T20:25:27Z",
        "summary": "Explainable AI (XAI) refers to techniques that provide human-understandable\ninsights into the workings of AI models. Recently, the focus of XAI is being\nextended towards Large Language Models (LLMs) which are often criticized for\ntheir lack of transparency. This extension calls for a significant\ntransformation in XAI methodologies because of two reasons. First, many\nexisting XAI methods cannot be directly applied to LLMs due to their complexity\nadvanced capabilities. Second, as LLMs are increasingly deployed across diverse\nindustry applications, the role of XAI shifts from merely opening the \"black\nbox\" to actively enhancing the productivity and applicability of LLMs in\nreal-world settings. Meanwhile, unlike traditional machine learning models that\nare passive recipients of XAI insights, the distinct abilities of LLMs can\nreciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in\nthe context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems,\nand (2) how LLMs can contribute to the advancement of XAI. We introduce 10\nstrategies, introducing the key techniques for each and discussing their\nassociated challenges. We also provide case studies to demonstrate how to\nobtain and leverage explanations. The code used in this paper can be found at:\nhttps://github.com/JacksonWuxs/UsableXAI_LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.08946v1.pdf"
    },
    {
        "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
        "authors": [
            "Mintong Kang",
            "Nezihe Merve G\u00fcrel",
            "Ning Yu",
            "Dawn Song",
            "Bo Li"
        ],
        "published": "2024-02-05T16:46:16Z",
        "summary": "Despite the impressive capabilities of large language models (LLMs) across\ndiverse applications, they still suffer from trustworthiness issues, such as\nhallucinations and misalignments. Retrieval-augmented language models (RAG)\nhave been proposed to enhance the credibility of generations by grounding\nexternal knowledge, but the theoretical understandings of their generation\nrisks remains unexplored. In this paper, we answer: 1) whether RAG can indeed\nlead to low generation risks, 2) how to provide provable guarantees on the\ngeneration risks of RAG and vanilla LLMs, and 3) what sufficient conditions\nenable RAG models to reduce generation risks. We propose C-RAG, the first\nframework to certify generation risks for RAG models. Specifically, we provide\nconformal risk analysis for RAG models and certify an upper confidence bound of\ngeneration risks, which we refer to as conformal generation risk. We also\nprovide theoretical guarantees on conformal generation risks for general\nbounded risk functions under test distribution shifts. We prove that RAG\nachieves a lower conformal generation risk than that of a single LLM when the\nquality of the retrieval model and transformer is non-trivial. Our intensive\nempirical results demonstrate the soundness and tightness of our conformal\ngeneration risk guarantees across four widely-used NLP datasets on four\nstate-of-the-art retrieval models.",
        "pdf_link": "https://arxiv.org/pdf/2402.03181v3.pdf"
    },
    {
        "title": "AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented Stock-Chain Framework",
        "authors": [
            "Xiang Li",
            "Zhenyu Li",
            "Chen Shi",
            "Yong Xu",
            "Qing Du",
            "Mingkui Tan",
            "Jun Huang",
            "Wei Lin"
        ],
        "published": "2024-03-19T09:45:33Z",
        "summary": "The task of financial analysis primarily encompasses two key areas: stock\ntrend prediction and the corresponding financial question answering. Currently,\nmachine learning and deep learning algorithms (ML&DL) have been widely applied\nfor stock trend predictions, leading to significant progress. However, these\nmethods fail to provide reasons for predictions, lacking interpretability and\nreasoning processes. Also, they can not integrate textual information such as\nfinancial news or reports. Meanwhile, large language models (LLMs) have\nremarkable textual understanding and generation ability. But due to the\nscarcity of financial training datasets and limited integration with real-time\nknowledge, LLMs still suffer from hallucinations and are unable to keep up with\nthe latest information. To tackle these challenges, we first release AlphaFin\ndatasets, combining traditional research datasets, real-time financial data,\nand handwritten chain-of-thought (CoT) data. It has a positive impact on\ntraining LLMs for completing financial analysis. We then use AlphaFin datasets\nto benchmark a state-of-the-art method, called Stock-Chain, for effectively\ntackling the financial analysis task, which integrates retrieval-augmented\ngeneration (RAG) techniques. Extensive experiments are conducted to demonstrate\nthe effectiveness of our framework on financial analysis.",
        "pdf_link": "https://arxiv.org/pdf/2403.12582v1.pdf"
    },
    {
        "title": "Meaningful Learning: Advancing Abstract Reasoning in Large Language Models via Generic Fact Guidance",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Ting Liu",
            "Bing Qin",
            "Dongliang Xu",
            "Qing Yang",
            "Hongtao Liu",
            "Yixin Cao"
        ],
        "published": "2024-03-14T04:06:13Z",
        "summary": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nsimple questions supported by a generic fact, LLMs often fail to provide\nconsistent and precise answers, indicating a deficiency in abstract reasoning\nabilities. This has sparked a vigorous debate about whether LLMs are genuinely\nreasoning or merely memorizing. In light of this, we design a preliminary study\nto quantify and delve into the abstract reasoning abilities of existing LLMs.\nOur findings reveal a substantial discrepancy between their general reasoning\nand abstract reasoning performances. To relieve this problem, we tailor an\nabstract reasoning dataset (AbsR) together with a meaningful learning paradigm\nto teach LLMs how to leverage generic facts for reasoning purposes. The results\nshow that our approach not only boosts the general reasoning performance of\nLLMs but also makes considerable strides towards their capacity for abstract\nreasoning, moving beyond simple memorization or imitation to a more nuanced\nunderstanding and application of generic facts.",
        "pdf_link": "https://arxiv.org/pdf/2403.09085v1.pdf"
    },
    {
        "title": "Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction",
        "authors": [
            "Xuemei Tang",
            "Jun Wang",
            "Qi Su"
        ],
        "published": "2024-02-22T08:26:56Z",
        "summary": "Recently, large language models (LLMs) have been successful in relational\nextraction (RE) tasks, especially in the few-shot learning. An important\nproblem in the field of RE is long-tailed data, while not much attention is\ncurrently paid to this problem using LLM approaches. Therefore, in this paper,\nwe propose SLCoLM, a model collaboration framework, to mitigate the data\nlong-tail problem. In our framework, We use the\n``\\textit{Training-Guide-Predict}'' strategy to combine the strengths of\npre-trained language models (PLMs) and LLMs, where a task-specific PLM\nframework acts as a tutor, transfers task knowledge to the LLM, and guides the\nLLM in performing RE tasks. Our experiments on a RE dataset rich in relation\ntypes show that the approach in this paper facilitates RE of long-tail relation\ntypes.",
        "pdf_link": "https://arxiv.org/pdf/2402.14373v1.pdf"
    },
    {
        "title": "Defending LLMs against Jailbreaking Attacks via Backtranslation",
        "authors": [
            "Yihan Wang",
            "Zhouxing Shi",
            "Andrew Bai",
            "Cho-Jui Hsieh"
        ],
        "published": "2024-02-26T10:03:33Z",
        "summary": "Although many large language models (LLMs) have been trained to refuse\nharmful requests, they are still vulnerable to jailbreaking attacks, which\nrewrite the original prompt to conceal its harmful intent. In this paper, we\npropose a new method for defending LLMs against jailbreaking attacks by\n``backtranslation''. Specifically, given an initial response generated by the\ntarget LLM from an input prompt, our backtranslation prompts a language model\nto infer an input prompt that can lead to the response. The inferred prompt is\ncalled the backtranslated prompt which tends to reveal the actual intent of the\noriginal prompt, since it is generated based on the LLM's response and is not\ndirectly manipulated by the attacker. We then run the target LLM again on the\nbacktranslated prompt, and we refuse the original prompt if the model refuses\nthe backtranslated prompt. We explain that the proposed defense provides\nseveral benefits on its effectiveness and efficiency. We empirically\ndemonstrate that our defense significantly outperforms the baselines, in the\ncases that are hard for the baselines, and our defense also has little impact\non the generation quality for benign input prompts.",
        "pdf_link": "https://arxiv.org/pdf/2402.16459v2.pdf"
    },
    {
        "title": "KorNAT: LLM Alignment Benchmark for Korean Social Values and Common Knowledge",
        "authors": [
            "Jiyoung Lee",
            "Minwoo Kim",
            "Seungho Kim",
            "Junghwan Kim",
            "Seunghyun Won",
            "Hwaran Lee",
            "Edward Choi"
        ],
        "published": "2024-02-21T08:12:26Z",
        "summary": "For Large Language Models (LLMs) to be effectively deployed in a specific\ncountry, they must possess an understanding of the nation's culture and basic\nknowledge. To this end, we introduce National Alignment, which measures an\nalignment between an LLM and a targeted country from two aspects: social value\nalignment and common knowledge alignment. Social value alignment evaluates how\nwell the model understands nation-specific social values, while common\nknowledge alignment examines how well the model captures basic knowledge\nrelated to the nation. We constructed KorNAT, the first benchmark that measures\nnational alignment with South Korea. For the social value dataset, we obtained\nground truth labels from a large-scale survey involving 6,174 unique Korean\nparticipants. For the common knowledge dataset, we constructed samples based on\nKorean textbooks and GED reference materials. KorNAT contains 4K and 6K\nmultiple-choice questions for social value and common knowledge, respectively.\nOur dataset creation process is meticulously designed and based on statistical\nsampling theory and was refined through multiple rounds of human review. The\nexperiment results of seven LLMs reveal that only a few models met our\nreference score, indicating a potential for further enhancement. KorNAT has\nreceived government approval after passing an assessment conducted by a\ngovernment-affiliated organization dedicated to evaluating dataset quality.\nSamples and detailed evaluation protocols of our dataset can be found in\nhttps://selectstar.ai/ko/papers-national-alignment",
        "pdf_link": "https://arxiv.org/pdf/2402.13605v4.pdf"
    },
    {
        "title": "Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine",
        "authors": [
            "Qiao Jin",
            "Fangyuan Chen",
            "Yiliang Zhou",
            "Ziyang Xu",
            "Justin M. Cheung",
            "Robert Chen",
            "Ronald M. Summers",
            "Justin F. Rousseau",
            "Peiyun Ni",
            "Marc J Landsman",
            "Sally L. Baxter",
            "Subhi J. Al'Aref",
            "Yijia Li",
            "Michael F. Chiang",
            "Yifan Peng",
            "Zhiyong Lu"
        ],
        "published": "2024-01-16T14:41:20Z",
        "summary": "Recent studies indicate that Generative Pre-trained Transformer 4 with Vision\n(GPT-4V) outperforms human physicians in medical challenge tasks. However,\nthese evaluations primarily focused on the accuracy of multi-choice questions\nalone. Our study extends the current scope by conducting a comprehensive\nanalysis of GPT-4V's rationales of image comprehension, recall of medical\nknowledge, and step-by-step multimodal reasoning when solving New England\nJournal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test\nthe knowledge and diagnostic capabilities of medical professionals. Evaluation\nresults confirmed that GPT-4V outperforms human physicians regarding\nmulti-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in\ncases where physicians incorrectly answer, with over 80% accuracy. However, we\ndiscovered that GPT-4V frequently presents flawed rationales in cases where it\nmakes the correct final choices (27.3%), most prominent in image comprehension\n(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our\nfindings emphasize the necessity for further in-depth evaluations of its\nrationales before integrating such models into clinical workflows.",
        "pdf_link": "https://arxiv.org/pdf/2401.08396v2.pdf"
    },
    {
        "title": "ChatGraph: Chat with Your Graphs",
        "authors": [
            "Yun Peng",
            "Sen Lin",
            "Qian Chen",
            "Lyu Xu",
            "Xiaojun Ren",
            "Yafei Li",
            "Jianliang Xu"
        ],
        "published": "2024-01-23T11:29:19Z",
        "summary": "Graph analysis is fundamental in real-world applications. Traditional\napproaches rely on SPARQL-like languages or clicking-and-dragging interfaces to\ninteract with graph data. However, these methods either require users to\npossess high programming skills or support only a limited range of graph\nanalysis functionalities. To address the limitations, we propose a large\nlanguage model (LLM)-based framework called ChatGraph. With ChatGraph, users\ncan interact with graphs through natural language, making it easier to use and\nmore flexible than traditional approaches. The core of ChatGraph lies in\ngenerating chains of graph analysis APIs based on the understanding of the\ntexts and graphs inputted in the user prompts. To achieve this, ChatGraph\nconsists of three main modules: an API retrieval module that searches for\nrelevant APIs, a graph-aware LLM module that enables the LLM to comprehend\ngraphs, and an API chain-oriented finetuning module that guides the LLM in\ngenerating API chains.",
        "pdf_link": "https://arxiv.org/pdf/2401.12672v1.pdf"
    },
    {
        "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
        "authors": [
            "Caiqi Zhang",
            "Zhijiang Guo",
            "Andreas Vlachos"
        ],
        "published": "2024-01-27T20:26:03Z",
        "summary": "This paper investigates the potential benefits of language-specific\nfact-checking models, focusing on the case of Chinese. We first demonstrate the\nlimitations of translation-based methods and multilingual large language models\n(e.g., GPT-4), highlighting the need for language-specific systems. We further\npropose a Chinese fact-checking system that can better retrieve evidence from a\ndocument by incorporating context information. To better analyze token-level\nbiases in different systems, we construct an adversarial dataset based on the\nCHEF dataset, where each instance has large word overlap with the original one\nbut holds the opposite veracity label. Experimental results on the CHEF dataset\nand our adversarial dataset show that our proposed method outperforms\ntranslation-based methods and multilingual LLMs and is more robust toward\nbiases, while there is still large room for improvement, emphasizing the\nimportance of language-specific fact-checking systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.15498v2.pdf"
    },
    {
        "title": "EXACT-Net:EHR-guided lung tumor auto-segmentation for non-small cell lung cancer radiotherapy",
        "authors": [
            "Hamed Hooshangnejad",
            "Xue Feng",
            "Gaofeng Huang",
            "Rui Zhang",
            "Quan Chen",
            "Kai Ding"
        ],
        "published": "2024-02-21T19:49:12Z",
        "summary": "Lung cancer is a devastating disease with the highest mortality rate among\ncancer types. Over 60% of non-small cell lung cancer (NSCLC) patients, which\naccounts for 87% of diagnoses, require radiation therapy. Rapid treatment\ninitiation significantly increases the patient's survival rate and reduces the\nmortality rate. Accurate tumor segmentation is a critical step in the diagnosis\nand treatment of NSCLC. Manual segmentation is time and labor-consuming and\ncauses delays in treatment initiation. Although many lung nodule detection\nmethods, including deep learning-based models, have been proposed, there is\nstill a long-standing problem of high false positives (FPs) with most of these\nmethods. Here, we developed an electronic health record (EHR) guided lung tumor\nauto-segmentation called EXACT-Net (EHR-enhanced eXACtitude in Tumor\nsegmentation), where the extracted information from EHRs using a pre-trained\nlarge language model (LLM), was used to remove the FPs and keep the TP nodules\nonly. The auto-segmentation model was trained on NSCLC patients' computed\ntomography (CT), and the pre-trained LLM was used with the zero-shot learning\napproach. Our approach resulted in a 250% boost in successful nodule detection\nusing the data from ten NSCLC patients treated in our institution.",
        "pdf_link": "https://arxiv.org/pdf/2402.14099v1.pdf"
    },
    {
        "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
        "authors": [
            "Changtong Zan",
            "Liang Ding",
            "Li Shen",
            "Yibing Zhen",
            "Weifeng Liu",
            "Dacheng Tao"
        ],
        "published": "2024-03-21T13:47:40Z",
        "summary": "Translation-tailored Large language models (LLMs) exhibit remarkable\ntranslation capabilities, even competing with supervised-trained commercial\ntranslation systems. However, off-target translation remains an unsolved\nproblem, especially for low-resource languages, hindering us from developing\naccurate LLMs-based translation models. To mitigate the off-target translation\nproblem and enhance the performance of LLMs on translation, recent works have\neither designed advanced prompting strategies to highlight the functionality of\ntranslation instructions or exploited the in-context learning ability of LLMs\nby feeding few-shot demonstrations. However, these methods essentially do not\nimprove LLM's ability to follow translation instructions, especially the\nlanguage direction information. In this work, we design a two-stage fine-tuning\nalgorithm to improve the instruction-following ability (especially the\ntranslation direction) of LLMs. Specifically, we first tune LLMs with the\nmaximum likelihood estimation loss on the translation dataset to elicit the\nbasic translation capabilities. In the second stage, we construct\ninstruction-conflicting samples by randomly replacing the translation\ndirections with a wrong one within the instruction, and then introduce an extra\nunlikelihood loss to learn those samples. Experiments on IWSLT and WMT\nbenchmarks upon the LLaMA model spanning 16 zero-shot directions show that,\ncompared to the competitive baseline -- translation-finetuned LLama, our method\ncould effectively reduce the off-target translation ratio (averagely -53.3\\%),\nthus improving translation quality with average +5.7 SacreBLEU and +16.4\nBLEURT. Analysis shows that our method could preserve the model's general task\nperformance on AlpacaEval. Code and models will be released at\n\\url{https://github.com/alphadl/LanguageAware_Tuning}.",
        "pdf_link": "https://arxiv.org/pdf/2403.14399v1.pdf"
    },
    {
        "title": "Evaluating LLMs for Gender Disparities in Notable Persons",
        "authors": [
            "Lauren Rhue",
            "Sofie Goethals",
            "Arun Sundararajan"
        ],
        "published": "2024-03-14T07:58:27Z",
        "summary": "This study examines the use of Large Language Models (LLMs) for retrieving\nfactual information, addressing concerns over their propensity to produce\nfactually incorrect \"hallucinated\" responses or to altogether decline to even\nanswer prompt at all. Specifically, it investigates the presence of\ngender-based biases in LLMs' responses to factual inquiries. This paper takes a\nmulti-pronged approach to evaluating GPT models by evaluating fairness across\nmultiple dimensions of recall, hallucinations and declinations. Our findings\nreveal discernible gender disparities in the responses generated by GPT-3.5.\nWhile advancements in GPT-4 have led to improvements in performance, they have\nnot fully eradicated these gender disparities, notably in instances where\nresponses are declined. The study further explores the origins of these\ndisparities by examining the influence of gender associations in prompts and\nthe homogeneity in the responses.",
        "pdf_link": "https://arxiv.org/pdf/2403.09148v1.pdf"
    },
    {
        "title": "SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection",
        "authors": [
            "Ayan Datta",
            "Aryan Chandramania",
            "Radhika Mamidi"
        ],
        "published": "2024-02-24T17:44:56Z",
        "summary": "This document contains the details of the authors' submission to the\nproceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and\nMultilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual)\nand B. Detection of machine-generated text is becoming an increasingly\nimportant task, with the advent of large language models (LLMs). In this paper,\nwe lay out how using weighted averages of RoBERTa layers lets us capture\ninformation about text that is relevant to machine-generated text detection.",
        "pdf_link": "https://arxiv.org/pdf/2402.15873v2.pdf"
    },
    {
        "title": "Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention",
        "authors": [
            "Eunkyung Jo",
            "Yuin Jeong",
            "SoHyun Park",
            "Daniel A. Epstein",
            "Young-Ho Kim"
        ],
        "published": "2024-02-17T18:05:53Z",
        "summary": "Recent large language models (LLMs) offer the potential to support public\nhealth monitoring by facilitating health disclosure through open-ended\nconversations but rarely preserve the knowledge gained about individuals across\nrepeated interactions. Augmenting LLMs with long-term memory (LTM) presents an\nopportunity to improve engagement and self-disclosure, but we lack an\nunderstanding of how LTM impacts people's interaction with LLM-driven chatbots\nin public health interventions. We examine the case of CareCall -- an\nLLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs\nand interviews with nine users. We found that LTM enhanced health disclosure\nand fostered positive perceptions of the chatbot by offering familiarity.\nHowever, we also observed challenges in promoting self-disclosure through LTM,\nparticularly around addressing chronic health conditions and privacy concerns.\nWe discuss considerations for LTM integration in LLM-driven chatbots for public\nhealth monitoring, including carefully deciding what topics need to be\nremembered in light of public health goals.",
        "pdf_link": "https://arxiv.org/pdf/2402.11353v1.pdf"
    },
    {
        "title": "A Survey of Table Reasoning with Large Language Models",
        "authors": [
            "Xuanliang Zhang",
            "Dingzirui Wang",
            "Longxu Dou",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "published": "2024-02-13T07:17:52Z",
        "summary": "Table reasoning, which aims to generate the corresponding answer to the\nquestion following the user requirement according to the provided table, and\noptionally a text description of the table, effectively improving the\nefficiency of obtaining information. Recently, using Large Language Models\n(LLMs) has become the mainstream method for table reasoning, because it not\nonly significantly reduces the annotation cost but also exceeds the performance\nof previous methods. However, existing research still lacks a summary of\nLLM-based table reasoning works. Due to the existing lack of research,\nquestions about which techniques can improve table reasoning performance in the\nera of LLMs, why LLMs excel at table reasoning, and how to enhance table\nreasoning abilities in the future, remain largely unexplored. This gap\nsignificantly limits progress in research. To answer the above questions and\nadvance table reasoning research with LLMs, we present this survey to analyze\nexisting research, inspiring future work. In this paper, we analyze the\nmainstream techniques used to improve table reasoning performance in the LLM\nera, and the advantages of LLMs compared to pre-LLMs for solving table\nreasoning. We provide research directions from both the improvement of existing\nmethods and the expansion of practical applications to inspire future research.",
        "pdf_link": "https://arxiv.org/pdf/2402.08259v1.pdf"
    },
    {
        "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models",
        "authors": [
            "Atsuyuki Miyai",
            "Jingkang Yang",
            "Jingyang Zhang",
            "Yifei Ming",
            "Qing Yu",
            "Go Irie",
            "Yixuan Li",
            "Hai Li",
            "Ziwei Liu",
            "Kiyoharu Aizawa"
        ],
        "published": "2024-03-29T17:59:53Z",
        "summary": "This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.20331v1.pdf"
    },
    {
        "title": "CI w/o TN: Context Injection without Task Name for Procedure Planning",
        "authors": [
            "Xinjie Li"
        ],
        "published": "2024-02-23T19:34:47Z",
        "summary": "This paper explores the challenge of procedure planning in instructional\nvideos, which involves creating goal-directed plans based on visual start and\ngoal observations from videos. Previous research has tackled this problem with\ngradually weaker training supervision, from heavy intermediate visual\nobservations or language instructions to task class supervision. However, with\nthe advent of large language models, even given only the task name, these\nmodels can produce a detailed plan. In this study, we propose a much weaker\nsetting without task name as supervision, which is not currently solvable by\nexisting large language models since they require good prompts with sufficient\ninformation. Specifically, we hypothesize that previous intermediate\nsupervisions can serve as context information, and we use captions of visual\nstart and goal observations as a much cheaper form of supervision. This\napproach greatly reduces the labeling cost since the captions can be easily\nobtained by large pre-trained vision-language foundation models. Technically,\nwe apply BLIP to generate captions as supervision to train the context feature\nwith contrastive learning loss. Afterward, the context feature is fed into the\ngenerator to aid in plan generation. Our experiments on two datasets with\nvarying scales demonstrate that our model can achieve comparable performance on\nmultiple metrics, which validates our hypothesis.",
        "pdf_link": "https://arxiv.org/pdf/2402.15579v1.pdf"
    },
    {
        "title": "On Prompt-Driven Safeguarding for Large Language Models",
        "authors": [
            "Chujie Zheng",
            "Fan Yin",
            "Hao Zhou",
            "Fandong Meng",
            "Jie Zhou",
            "Kai-Wei Chang",
            "Minlie Huang",
            "Nanyun Peng"
        ],
        "published": "2024-01-31T17:28:24Z",
        "summary": "Prepending model inputs with safety prompts is a common practice for\nsafeguarding large language models (LLMs) from complying with queries that\ncontain harmful intents. However, the working mechanisms of safety prompts have\nnot been revealed yet, which hinders the potential for automatically optimizing\nthem to improve LLM safety. To this end, we investigate the impact of safety\nprompts from the perspective of model representations. We find that in models'\nrepresentation space, harmful and harmless queries can be largely\ndistinguished, but this is not noticeably enhanced by safety prompts. Instead,\nthe queries' representations are moved by safety prompts in similar directions\nwhere models become more prone to refusal (i.e., refusing to provide\nassistance) even when the queries are harmless. Inspired by these findings, we\npropose a method called DRO (Directed Representation Optimization) for\nautomatic safety prompt optimization. It treats safety prompts as continuous,\ntrainable embeddings and learns to move the representations of harmful/harmless\nqueries along/opposite the direction in which the model's refusal probability\nincreases. Experiments with eight LLMs on out-of-domain benchmarks demonstrate\nthat DRO remarkably improves the safeguarding performance of human-crafted\nsafety prompts and outperforms strong baselines, without compromising the\ngeneral model capability.",
        "pdf_link": "https://arxiv.org/pdf/2401.18018v2.pdf"
    },
    {
        "title": "Large language model empowered participatory urban planning",
        "authors": [
            "Zhilun Zhou",
            "Yuming Lin",
            "Yong Li"
        ],
        "published": "2024-01-24T10:50:01Z",
        "summary": "Participatory urban planning is the mainstream of modern urban planning and\ninvolves the active engagement of different stakeholders. However, the\ntraditional participatory paradigm encounters challenges in time and manpower,\nwhile the generative planning tools fail to provide adjustable and inclusive\nsolutions. This research introduces an innovative urban planning approach\nintegrating Large Language Models (LLMs) within the participatory process. The\nframework, based on the crafted LLM agent, consists of role-play, collaborative\ngeneration, and feedback iteration, solving a community-level land-use task\ncatering to 1000 distinct interests. Empirical experiments in diverse urban\ncommunities exhibit LLM's adaptability and effectiveness across varied planning\nscenarios. The results were evaluated on four metrics, surpassing human experts\nin satisfaction and inclusion, and rivaling state-of-the-art reinforcement\nlearning methods in service and ecology. Further analysis shows the advantage\nof LLM agents in providing adjustable and inclusive solutions with natural\nlanguage reasoning and strong scalability. While implementing the recent\nadvancements in emulating human behavior for planning, this work envisions both\nplanners and citizens benefiting from low-cost, efficient LLM agents, which is\ncrucial for enhancing participation and realizing participatory urban planning.",
        "pdf_link": "https://arxiv.org/pdf/2402.01698v1.pdf"
    },
    {
        "title": "MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse Worlds",
        "authors": [
            "Xiaolong Jin",
            "Zhuo Zhang",
            "Xiangyu Zhang"
        ],
        "published": "2024-01-25T02:57:40Z",
        "summary": "Large Language Model (LLM) alignment aims to ensure that LLM outputs match\nwith human values. Researchers have demonstrated the severity of alignment\nproblems with a large spectrum of jailbreak techniques that can induce LLMs to\nproduce malicious content during conversations. Finding the corresponding\njailbreaking prompts usually requires substantial human intelligence or\ncomputation resources. In this paper, we report that LLMs have different levels\nof alignment in various contexts. As such, by systematically constructing many\ncontexts, called worlds, leveraging a Domain Specific Language describing\npossible worlds (e.g., time, location, characters, actions and languages) and\nthe corresponding compiler, we can cost-effectively expose latent alignment\nissues. Given the low cost of our method, we are able to conduct a large scale\nstudy regarding LLM alignment issues in different worlds. Our results show that\nour method outperforms the-state-of-the-art jailbreaking techniques on both\neffectiveness and efficiency. In addition, our results indicate that existing\nLLMs are extremely vulnerable to nesting worlds and programming language\nworlds. They imply that existing alignment training focuses on the real-world\nand is lacking in various (virtual) worlds where LLMs can be exploited.",
        "pdf_link": "https://arxiv.org/pdf/2402.01706v1.pdf"
    },
    {
        "title": "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs",
        "authors": [
            "Michael Dorkenwald",
            "Nimrod Barazani",
            "Cees G. M. Snoek",
            "Yuki M. Asano"
        ],
        "published": "2024-02-13T18:39:18Z",
        "summary": "Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown\nimmense potential by integrating large language models with vision systems.\nNevertheless, these models face challenges in the fundamental computer vision\ntask of object localisation, due to their training on multimodal data\ncontaining mostly captions without explicit spatial grounding. While it is\npossible to construct custom, supervised training pipelines with bounding box\nannotations that integrate with VLMs, these result in specialized and\nhard-to-scale models. In this paper, we aim to explore the limits of\ncaption-based VLMs and instead propose to tackle the challenge in a simpler\nmanner by i) keeping the weights of a caption-based VLM frozen and ii) not\nusing any supervised detection data. To this end, we introduce an\ninput-agnostic Positional Insert (PIN), a learnable spatial prompt, containing\na minimal set of parameters that are slid inside the frozen VLM, unlocking\nobject localisation capabilities. Our PIN module is trained with a simple\nnext-token prediction task on synthetic data without requiring the introduction\nof new output heads. Our experiments demonstrate strong zero-shot localisation\nperformances on a variety of images, including Pascal VOC, COCO, LVIS, and\ndiverse images like paintings or cartoons.",
        "pdf_link": "https://arxiv.org/pdf/2402.08657v1.pdf"
    },
    {
        "title": "CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer",
        "authors": [
            "Zhen Tao",
            "Dinghao Xi",
            "Zhiyu Li",
            "Liumin Tang",
            "Wei Xu"
        ],
        "published": "2024-01-11T07:18:46Z",
        "summary": "Text style transfer is increasingly prominent in online entertainment and\nsocial media. However, existing research mainly concentrates on style transfer\nwithin individual English sentences, while ignoring the complexity of long\nChinese texts, which limits the wider applicability of style transfer in\ndigital media realm. To bridge this gap, we propose a Chinese Article-style\nTransfer framework (CAT-LLM), leveraging the capabilities of Large Language\nModels (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition\n(TSD) module aimed at comprehensively analyzing text features in articles,\nprompting LLMs to efficiently transfer Chinese article-style. The TSD module\nintegrates a series of machine learning algorithms to analyze article-style\nfrom both words and sentences levels, thereby aiding LLMs thoroughly grasp the\ntarget style without compromising the integrity of the original text. In\naddition, this module supports dynamic expansion of internal style trees,\nshowcasing robust compatibility and allowing flexible optimization in\nsubsequent research. Moreover, we select five Chinese articles with distinct\nstyles and create five parallel datasets using ChatGPT, enhancing the models'\nperformance evaluation accuracy and establishing a novel paradigm for\nevaluating subsequent research on article-style transfer. Extensive\nexperimental results affirm that CAT-LLM outperforms current research in terms\nof transfer accuracy and content preservation, and has remarkable applicability\nto various types of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.05707v1.pdf"
    },
    {
        "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding",
        "authors": [
            "Mingdao Liu",
            "Aohan Zeng",
            "Bowen Wang",
            "Peng Zhang",
            "Jie Tang",
            "Yuxiao Dong"
        ],
        "published": "2024-01-12T18:50:36Z",
        "summary": "The massive adoption of large language models (LLMs) demands efficient\ndeployment strategies. However, the auto-regressive decoding process, which is\nfundamental to how most LLMs generate text, poses challenges to achieve\nefficient serving. In this work, we introduce a parallel auto-regressive\ngeneration method. By instruct-tuning on general domain data that contains\nhierarchical structures, we enable LLMs to independently plan their generation\nprocess and perform auto-parallel auto-regressive (APAR) generation,\nsignificantly reducing the number of generation steps. APAR alone can achieve\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\nand attention computation during generation. This leads to a throughput\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\ncompared to state-of-the-art serving frameworks.",
        "pdf_link": "https://arxiv.org/pdf/2401.06761v1.pdf"
    },
    {
        "title": "RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions",
        "authors": [
            "Yuansen Zhang",
            "Xiao Wang",
            "Zhiheng Xi",
            "Han Xia",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "published": "2024-02-26T09:30:55Z",
        "summary": "Large Language Models (LLMs) have showcased remarkable capabilities in\nfollowing human instructions. However, recent studies have raised concerns\nabout the robustness of LLMs when prompted with instructions combining textual\nadversarial samples. In this paper, drawing inspiration from recent works that\nLLMs are sensitive to the design of the instructions, we utilize instructions\nin code style, which are more structural and less ambiguous, to replace\ntypically natural language instructions. Through this conversion, we provide\nLLMs with more precise instructions and strengthen the robustness of LLMs.\nMoreover, under few-shot scenarios, we propose a novel method to compose\nin-context demonstrations using both clean and adversarial samples\n(\\textit{adversarial context method}) to further boost the robustness of the\nLLMs. Experiments on eight robustness datasets show that our method\nconsistently outperforms prompting LLMs with natural language instructions. For\nexample, with gpt-3.5-turbo, our method achieves an improvement of 5.68\\% in\ntest set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).",
        "pdf_link": "https://arxiv.org/pdf/2402.16431v1.pdf"
    },
    {
        "title": "AttributionBench: How Hard is Automatic Attribution Evaluation?",
        "authors": [
            "Yifei Li",
            "Xiang Yue",
            "Zeyi Liao",
            "Huan Sun"
        ],
        "published": "2024-02-23T04:23:33Z",
        "summary": "Modern generative search engines enhance the reliability of large language\nmodel (LLM) responses by providing cited evidence. However, evaluating the\nanswer's attribution, i.e., whether every claim within the generated responses\nis fully supported by its cited evidence, remains an open problem. This\nverification, traditionally dependent on costly human evaluation, underscores\nthe urgent need for automatic attribution evaluation methods. To bridge the gap\nin the absence of standardized benchmarks for these methods, we present\nAttributionBench, a comprehensive benchmark compiled from various existing\nattribution datasets. Our extensive experiments on AttributionBench reveal the\nchallenges of automatic attribution evaluation, even for state-of-the-art LLMs.\nSpecifically, our findings show that even a fine-tuned GPT-3.5 only achieves\naround 80% macro-F1 under a binary classification formulation. A detailed\nanalysis of more than 300 error cases indicates that a majority of failures\nstem from the model's inability to process nuanced information, and the\ndiscrepancy between the information the model has access to and that human\nannotators do.",
        "pdf_link": "https://arxiv.org/pdf/2402.15089v1.pdf"
    },
    {
        "title": "Benchmarking Data Science Agents",
        "authors": [
            "Yuge Zhang",
            "Qiyang Jiang",
            "Xingyu Han",
            "Nan Chen",
            "Yuqing Yang",
            "Kan Ren"
        ],
        "published": "2024-02-27T03:03:06Z",
        "summary": "In the era of data-driven decision-making, the complexity of data analysis\nnecessitates advanced expertise and tools of data science, presenting\nsignificant challenges even for specialists. Large Language Models (LLMs) have\nemerged as promising aids as data science agents, assisting humans in data\nanalysis and processing. Yet their practical efficacy remains constrained by\nthe varied demands of real-world applications and complicated analytical\nprocess. In this paper, we introduce DSEval -- a novel evaluation paradigm, as\nwell as a series of innovative benchmarks tailored for assessing the\nperformance of these agents throughout the entire data science lifecycle.\nIncorporating a novel bootstrapped annotation method, we streamline dataset\npreparation, improve the evaluation coverage, and expand benchmarking\ncomprehensiveness. Our findings uncover prevalent obstacles and provide\ncritical insights to inform future advancements in the field.",
        "pdf_link": "https://arxiv.org/pdf/2402.17168v1.pdf"
    },
    {
        "title": "Using an LLM to Turn Sign Spottings into Spoken Language Sentences",
        "authors": [
            "Ozge Mercanoglu Sincan",
            "Necati Cihan Camgoz",
            "Richard Bowden"
        ],
        "published": "2024-03-15T16:14:34Z",
        "summary": "Sign Language Translation (SLT) is a challenging task that aims to generate\nspoken language sentences from sign language videos. In this paper, we\nintroduce a hybrid SLT approach, Spotter+GPT, that utilizes a sign spotter and\na pretrained large language model to improve SLT performance. Our method builds\nupon the strengths of both components. The videos are first processed by the\nspotter, which is trained on a linguistic sign language dataset, to identify\nindividual signs. These spotted signs are then passed to the powerful language\nmodel, which transforms them into coherent and contextually appropriate spoken\nlanguage sentences.",
        "pdf_link": "https://arxiv.org/pdf/2403.10434v1.pdf"
    },
    {
        "title": "KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction",
        "authors": [
            "Zixuan Li",
            "Yutao Zeng",
            "Yuxin Zuo",
            "Weicheng Ren",
            "Wenxuan Liu",
            "Miao Su",
            "Yucan Guo",
            "Yantao Liu",
            "Xiang Li",
            "Zhilei Hu",
            "Long Bai",
            "Wei Li",
            "Yidan Liu",
            "Pan Yang",
            "Xiaolong Jin",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "published": "2024-03-12T14:56:34Z",
        "summary": "In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct\nUniversal Information Extraction (UIE) via code generation. KnowCoder aims to\ndevelop a kind of unified schema representation that LLMs can easily understand\nand an effective learning framework that encourages LLMs to follow schemas and\nextract structured knowledge accurately. To achieve these, KnowCoder introduces\na code-style schema representation method to uniformly transform different\nschemas into Python classes, with which complex schema information, such as\nconstraints among tasks in UIE, can be captured in an LLM-friendly manner. We\nfurther construct a code-style schema library covering over $\\textbf{30,000}$\ntypes of knowledge, which is the largest one for UIE, to the best of our\nknowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase\nlearning framework that enhances its schema understanding ability via code\npretraining and its schema following ability via instruction tuning. After code\npretraining on around $1.5$B automatically constructed data, KnowCoder already\nattains remarkable generalization ability and achieves relative improvements by\n$\\textbf{49.8%}$ F1, compared to LLaMA2, under the few-shot setting. After\ninstruction tuning, KnowCoder further exhibits strong generalization ability on\nunseen schemas and achieves up to $\\textbf{12.5%}$ and $\\textbf{21.9%}$,\ncompared to sota baselines, under the zero-shot setting and the low resource\nsetting, respectively. Additionally, based on our unified schema\nrepresentations, various human-annotated datasets can simultaneously be\nutilized to refine KnowCoder, which achieves significant improvements up to\n$\\textbf{7.5%}$ under the supervised setting.",
        "pdf_link": "https://arxiv.org/pdf/2403.07969v2.pdf"
    },
    {
        "title": "AmbigNLG: Addressing Task Ambiguity in Instruction for NLG",
        "authors": [
            "Ayana Niwa",
            "Hayate Iso"
        ],
        "published": "2024-02-27T17:52:33Z",
        "summary": "In this study, we introduce AmbigNLG, a new task designed to tackle the\nchallenge of task ambiguity in instructions for Natural Language Generation\n(NLG) tasks. Despite the impressive capabilities of Large Language Models\n(LLMs) in understanding and executing a wide range of tasks through natural\nlanguage interaction, their performance is significantly hindered by the\nambiguity present in real-world instructions. To address this, AmbigNLG seeks\nto identify and mitigate such ambiguities, aiming to refine instructions to\nmatch user expectations better. We introduce a dataset, AmbigSNI-NLG,\nconsisting of 2,500 instances, and develop an ambiguity taxonomy for\ncategorizing and annotating instruction ambiguities. Our approach demonstrates\nsubstantial improvements in text generation quality, highlighting the critical\nrole of clear and specific instructions in enhancing LLM performance in NLG\ntasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17717v1.pdf"
    },
    {
        "title": "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models",
        "authors": [
            "Zuyan Liu",
            "Yuhao Dong",
            "Yongming Rao",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "published": "2024-03-19T17:59:52Z",
        "summary": "In the realm of vision-language understanding, the proficiency of models in\ninterpreting and reasoning over visual content has become a cornerstone for\nnumerous applications. However, it is challenging for the visual encoder in\nLarge Vision-Language Models (LVLMs) to extract useful features tailored to\nquestions that aid the language model's response. Furthermore, a common\npractice among existing LVLMs is to utilize lower-resolution images, which\nrestricts the ability for visual recognition. Our work introduces the\nChain-of-Spot (CoS) method, which we describe as Interactive Reasoning, a novel\napproach that enhances feature extraction by focusing on key regions of\ninterest (ROI) within the image, corresponding to the posed questions or\ninstructions. This technique allows LVLMs to access more detailed visual\ninformation without altering the original image resolution, thereby offering\nmulti-granularity image features. By integrating Chain-of-Spot with\ninstruct-following LLaVA-1.5 models, the process of image reasoning\nconsistently improves performance across a wide range of multimodal datasets\nand benchmarks without bells and whistles and achieves new state-of-the-art\nresults. Our empirical findings demonstrate a significant improvement in LVLMs'\nability to understand and reason about visual content, paving the way for more\nsophisticated visual instruction-following applications. Code and models are\navailable at https://github.com/dongyh20/Chain-of-Spot",
        "pdf_link": "https://arxiv.org/pdf/2403.12966v2.pdf"
    },
    {
        "title": "Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models",
        "authors": [
            "Xianzhen Luo",
            "Qingfu Zhu",
            "Zhiming Zhang",
            "Xu Wang",
            "Qing Yang",
            "Dongliang Xu",
            "Wanxiang Che"
        ],
        "published": "2024-03-01T08:05:44Z",
        "summary": "Instruction tuning plays a pivotal role in Code Large Language Models (Code\nLLMs) for the task of program synthesis. Presently, two dominant paradigms for\ncollecting tuning data are natural-instruct (human-written) and self-instruct\n(automatically generated). Natural-instruct includes diverse and correct codes\nbut lacks instruction-code pairs, and exists improper code formats like nested\nsingle-line codes. In contrast, self-instruct automatically generates proper\npaired data. However, it suffers from low diversity due to generating\nduplicates and cannot ensure the correctness of codes. To bridge the both\nparadigms, we propose \\textbf{Semi-Instruct}. It first converts diverse but\nimproper codes from natural-instruct into proper instruction-code pairs through\na method similar to self-instruct. To verify the correctness of generated\ncodes, we design a novel way to construct test cases by generating cases'\ninputs and executing correct codes from natural-instruct to get outputs.\nFinally, diverse and correct instruction-code pairs are retained for\ninstruction tuning. Experiments show that semi-instruct is significantly better\nthan natural-instruct and self-instruct. Furthermore, the performance steadily\nimproves as data scale increases.",
        "pdf_link": "https://arxiv.org/pdf/2403.00338v1.pdf"
    },
    {
        "title": "MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models",
        "authors": [
            "Subash Neupane",
            "Shaswata Mitra",
            "Sudip Mittal",
            "Noorbakhsh Amiri Golilarz",
            "Shahram Rahimi",
            "Amin Amirlatifi"
        ],
        "published": "2024-03-13T15:20:30Z",
        "summary": "Large Language Models (LLMs) have shown impressive capabilities in generating\nhuman-like responses. However, their lack of domain-specific knowledge limits\ntheir applicability in healthcare settings, where contextual and comprehensive\nresponses are vital. To address this challenge and enable the generation of\npatient-centric responses that are contextually relevant and comprehensive, we\npropose MedInsight:a novel retrieval augmented framework that augments LLM\ninputs (prompts) with relevant background information from multiple sources.\nMedInsight extracts pertinent details from the patient's medical record or\nconsultation transcript. It then integrates information from authoritative\nmedical textbooks and curated web resources based on the patient's health\nhistory and condition. By constructing an augmented context combining the\npatient's record with relevant medical knowledge, MedInsight generates\nenriched, patient-specific responses tailored for healthcare applications such\nas diagnosis, treatment recommendations, or patient education. Experiments on\nthe MTSamples dataset validate MedInsight's effectiveness in generating\ncontextually appropriate medical responses. Quantitative evaluation using the\nRagas metric and TruLens for answer similarity and answer correctness\ndemonstrates the model's efficacy. Furthermore, human evaluation studies\ninvolving Subject Matter Expert (SMEs) confirm MedInsight's utility, with\nmoderate inter-rater agreement on the relevance and correctness of the\ngenerated responses.",
        "pdf_link": "https://arxiv.org/pdf/2403.08607v1.pdf"
    },
    {
        "title": "The Future of Combating Rumors? Retrieval, Discrimination, and Generation",
        "authors": [
            "Junhao Xu",
            "Longdi Xian",
            "Zening Liu",
            "Mingliang Chen",
            "Qiuyang Yin",
            "Fenghua Song"
        ],
        "published": "2024-03-29T14:32:41Z",
        "summary": "Artificial Intelligence Generated Content (AIGC) technology development has\nfacilitated the creation of rumors with misinformation, impacting societal,\neconomic, and political ecosystems, challenging democracy. Current rumor\ndetection efforts fall short by merely labeling potentially misinformation\n(classification task), inadequately addressing the issue, and it is unrealistic\nto have authoritative institutions debunk every piece of information on social\nmedia. Our proposed comprehensive debunking process not only detects rumors but\nalso provides explanatory generated content to refute the authenticity of the\ninformation. The Expert-Citizen Collective Wisdom (ECCW) module we designed\naensures high-precision assessment of the credibility of information and the\nretrieval module is responsible for retrieving relevant knowledge from a\nReal-time updated debunking database based on information keywords. By using\nprompt engineering techniques, we feed results and knowledge into a LLM (Large\nLanguage Model), achieving satisfactory discrimination and explanatory effects\nwhile eliminating the need for fine-tuning, saving computational costs, and\ncontributing to debunking efforts.",
        "pdf_link": "https://arxiv.org/pdf/2403.20204v1.pdf"
    },
    {
        "title": "GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data",
        "authors": [
            "Lele Cao",
            "Valentin Buchner",
            "Zineb Senane",
            "Fangkai Yang"
        ],
        "published": "2024-02-22T21:22:04Z",
        "summary": "Multimodal Large Language Models (MLLMs) are commonly evaluated using costly\nannotated multimodal benchmarks. However, these benchmarks often struggle to\nkeep pace with the rapidly advancing requirements of MLLM evaluation. We\npropose GenCeption, a novel and annotation-free MLLM evaluation framework that\nmerely requires unimodal data to assess inter-modality semantic coherence and\ninversely reflects the models' inclination to hallucinate. Analogous to the\npopular DrawCeption game, GenCeption initiates with a non-textual sample and\nundergoes a series of iterative description and generation steps. Semantic\ndrift across iterations is quantified using the GC@T metric. Our empirical\nfindings validate GenCeption's efficacy, showing strong correlations with\npopular MLLM benchmarking results. GenCeption may be extended to mitigate\ntraining data contamination by utilizing ubiquitous, previously unseen unimodal\ndata.",
        "pdf_link": "https://arxiv.org/pdf/2402.14973v1.pdf"
    },
    {
        "title": "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models",
        "authors": [
            "Yifu Gao",
            "Linbo Qiao",
            "Zhigang Kan",
            "Zhihua Wen",
            "Yongquan He",
            "Dongsheng Li"
        ],
        "published": "2024-02-26T13:47:09Z",
        "summary": "Temporal knowledge graph question answering (TKGQA) poses a significant\nchallenge task, due to the temporal constraints hidden in questions and the\nanswers sought from dynamic structured knowledge. Although large language\nmodels (LLMs) have made considerable progress in their reasoning ability over\nstructured data, their application to the TKGQA task is a relatively unexplored\narea. This paper first proposes a novel generative temporal knowledge graph\nquestion answering framework, GenTKGQA, which guides LLMs to answer temporal\nquestions through two phases: Subgraph Retrieval and Answer Generation. First,\nwe exploit LLM's intrinsic knowledge to mine temporal constraints and\nstructural links in the questions without extra training, thus narrowing down\nthe subgraph search space in both temporal and structural dimensions. Next, we\ndesign virtual knowledge indicators to fuse the graph neural network signals of\nthe subgraph and the text representations of the LLM in a non-shallow way,\nwhich helps the open-source LLM deeply understand the temporal order and\nstructural dependencies among the retrieved facts through instruction tuning.\nExperimental results demonstrate that our model outperforms state-of-the-art\nbaselines, even achieving 100\\% on the metrics for the simple question type.",
        "pdf_link": "https://arxiv.org/pdf/2402.16568v1.pdf"
    },
    {
        "title": "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions",
        "authors": [
            "Hanjie Chen",
            "Zhouxiang Fang",
            "Yash Singla",
            "Mark Dredze"
        ],
        "published": "2024-02-28T05:44:41Z",
        "summary": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as passing scores on medical licensing examinations. However, medical\nboard exam questions or general clinical questions do not capture the\ncomplexity of realistic clinical cases. Moreover, the lack of reference\nexplanations means we cannot easily evaluate the reasoning of model decisions,\na crucial component of supporting doctors in making complex medical decisions.\nTo address these challenges, we construct two new datasets: JAMA Clinical\nChallenge and Medbullets. JAMA Clinical Challenge consists of questions based\non challenging clinical cases, while Medbullets comprises USMLE Step 2&3 style\nclinical questions. Both datasets are structured as multiple-choice\nquestion-answering tasks, where each question is accompanied by an\nexpert-written explanation. We evaluate four LLMs on the two datasets using\nvarious prompts. Experiments demonstrate that our datasets are harder than\nprevious benchmarks. The inconsistency between automatic and human evaluations\nof model-generated explanations highlights the need to develop new metrics to\nsupport future research on explainable medical QA.",
        "pdf_link": "https://arxiv.org/pdf/2402.18060v3.pdf"
    },
    {
        "title": "Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?",
        "authors": [
            "Evgeniia Razumovskaia",
            "Ivan Vuli\u0107",
            "Anna Korhonen"
        ],
        "published": "2024-03-04T10:48:13Z",
        "summary": "Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and\nin-context learning (ICL) are three alternative, de facto standard approaches\nto few-shot learning. ICL has gained popularity recently with the advent of\nLLMs due to its simplicity and sample efficiency. Prior research has conducted\nonly limited investigation into how these approaches work for multilingual\nfew-shot learning, and the focus so far has been mostly on their performance.\nIn this work, we present an extensive and systematic comparison of the three\napproaches, testing them on 6 high- and low-resource languages, three different\nNLU tasks, and a myriad of language and domain setups. Importantly, performance\nis only one aspect of the comparison, where we also analyse the approaches\nthrough the optics of their computational, inference and financial costs. Our\nobservations show that supervised instruction tuning has the best trade-off\nbetween performance and resource requirements. As another contribution, we\nanalyse the impact of target language adaptation of pretrained LLMs and find\nthat the standard adaptation approaches can (superficially) improve target\nlanguage generation capabilities, but language understanding elicited through\nICL does not improve and remains limited, with low scores especially for\nlow-resource languages.",
        "pdf_link": "https://arxiv.org/pdf/2403.01929v1.pdf"
    },
    {
        "title": "Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate",
        "authors": [
            "Xihan Li",
            "Xing Li",
            "Lei Chen",
            "Xing Zhang",
            "Mingxuan Yuan",
            "Jun Wang"
        ],
        "published": "2024-03-14T03:24:14Z",
        "summary": "Language, a prominent human ability to express through sequential symbols,\nhas been computationally mastered by recent advances of large language models\n(LLMs). By predicting the next word recurrently with huge neural models, LLMs\nhave shown unprecedented capabilities in understanding and reasoning. Circuit,\nas the \"language\" of electronic design, specifies the functionality of an\nelectronic device by cascade connections of logic gates. Then, can circuits\nalso be mastered by a a sufficiently large \"circuit model\", which can conquer\nelectronic design tasks by simply predicting the next logic gate? In this work,\nwe take the first step to explore such possibilities. Two primary barriers\nimpede the straightforward application of LLMs to circuits: their complex,\nnon-sequential structure, and the intolerance of hallucination due to strict\nconstraints (e.g., equivalence). For the first barrier, we encode a circuit as\na memory-less, depth-first traversal trajectory, which allows Transformer-based\nneural models to better leverage its structural information, and predict the\nnext gate on the trajectory as a circuit model. For the second barrier, we\nintroduce an equivalence-preserving decoding process, which ensures that every\ntoken in the generated trajectory adheres to the specified equivalence\nconstraints. Moreover, the circuit model can also be regarded as a stochastic\npolicy to tackle optimization-oriented circuit design tasks. Experimentally, we\ntrained a Transformer-based model of 88M parameters, named \"Circuit\nTransformer\", which demonstrates impressive performance in end-to-end logic\nsynthesis. With Monte-Carlo tree search, Circuit Transformer significantly\nimproves over resyn2 while retaining strict equivalence, showcasing the\npotential of generative AI in conquering electronic design challenges.",
        "pdf_link": "https://arxiv.org/pdf/2403.13838v1.pdf"
    },
    {
        "title": "Alirector: Alignment-Enhanced Chinese Grammatical Error Corrector",
        "authors": [
            "Haihui Yang",
            "Xiaojun Quan"
        ],
        "published": "2024-02-07T05:56:54Z",
        "summary": "Chinese grammatical error correction (CGEC) faces serious overcorrection\nchallenges when employing autoregressive generative models such as\nsequence-to-sequence (Seq2Seq) models and decoder-only large language models\n(LLMs). While previous methods aim to address overcorrection in Seq2Seq models,\nthey are difficult to adapt to decoder-only LLMs. In this paper, we propose an\nalignment-enhanced corrector for the overcorrection problem that applies to\nboth Seq2Seq models and decoder-only LLMs. Our method first trains a correction\nmodel to generate an initial correction of the source sentence. Then, we\ncombine the source sentence with the initial correction and feed it through an\nalignment model for another round of correction, aiming to enforce the\nalignment model to focus on potential overcorrection. Moreover, to enhance the\nmodel's ability to identify nuances, we further explore the reverse alignment\nof the source sentence and the initial correction. Finally, we transfer the\nalignment knowledge from two alignment models to the correction model,\ninstructing it on how to avoid overcorrection. Experimental results on three\nCGEC datasets demonstrate the effectiveness of our approach in alleviating\novercorrection and improving overall performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.04601v1.pdf"
    },
    {
        "title": "Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns",
        "authors": [
            "Tunazzina Islam",
            "Dan Goldwasser"
        ],
        "published": "2024-03-15T21:54:00Z",
        "summary": "This paper introduces a novel approach to uncovering and analyzing themes in\nsocial media messaging. Recognizing the limitations of traditional topic-level\nanalysis, which tends to capture only the overarching patterns, this study\nemphasizes the need for a finer-grained, theme-focused exploration.\nConventional methods of theme discovery, involving manual processes and a\nhuman-in-the-loop approach, are valuable but face challenges in scalability,\nconsistency, and resource intensity in terms of time and cost. To address these\nchallenges, we propose a machine-in-the-loop approach that leverages the\nadvanced capabilities of Large Language Models (LLMs). This approach allows for\na deeper investigation into the thematic aspects of social media discourse,\nenabling us to uncover a diverse array of themes, each with unique\ncharacteristics and relevance, thereby offering a comprehensive understanding\nof the nuances present within broader topics. Furthermore, this method\nefficiently maps the text and the newly discovered themes, enhancing our\nunderstanding of the thematic nuances in social media messaging. We employ\nclimate campaigns as a case study and demonstrate that our methodology yields\nmore accurate and interpretable results compared to traditional topic models.\nOur results not only demonstrate the effectiveness of our approach in\nuncovering latent themes but also illuminate how these themes are tailored for\ndemographic targeting in social media contexts. Additionally, our work sheds\nlight on the dynamic nature of social media, revealing the shifts in the\nthematic focus of messaging in response to real-world events.",
        "pdf_link": "https://arxiv.org/pdf/2403.10707v1.pdf"
    },
    {
        "title": "WIPI: A New Web Threat for LLM-Driven Web Agents",
        "authors": [
            "Fangzhou Wu",
            "Shutong Wu",
            "Yulong Cao",
            "Chaowei Xiao"
        ],
        "published": "2024-02-26T19:01:54Z",
        "summary": "With the fast development of large language models (LLMs), LLM-driven Web\nAgents (Web Agents for short) have obtained tons of attention due to their\nsuperior capability where LLMs serve as the core part of making decisions like\nthe human brain equipped with multiple web tools to actively interact with\nexternal deployed websites. As uncountable Web Agents have been released and\nsuch LLM systems are experiencing rapid development and drawing closer to\nwidespread deployment in our daily lives, an essential and pressing question\narises: \"Are these Web Agents secure?\". In this paper, we introduce a novel\nthreat, WIPI, that indirectly controls Web Agent to execute malicious\ninstructions embedded in publicly accessible webpages. To launch a successful\nWIPI works in a black-box environment. This methodology focuses on the form and\ncontent of indirect instructions within external webpages, enhancing the\nefficiency and stealthiness of the attack. To evaluate the effectiveness of the\nproposed methodology, we conducted extensive experiments using 7 plugin-based\nChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The\nresults reveal that our methodology achieves an average attack success rate\n(ASR) exceeding 90% even in pure black-box scenarios. Moreover, through an\nablation study examining various user prefix instructions, we demonstrated that\nthe WIPI exhibits strong robustness, maintaining high performance across\ndiverse prefix instructions.",
        "pdf_link": "https://arxiv.org/pdf/2402.16965v1.pdf"
    },
    {
        "title": "Large Language Models Are Neurosymbolic Reasoners",
        "authors": [
            "Meng Fang",
            "Shilong Deng",
            "Yudi Zhang",
            "Zijing Shi",
            "Ling Chen",
            "Mykola Pechenizkiy",
            "Jun Wang"
        ],
        "published": "2024-01-17T16:57:19Z",
        "summary": "A wide range of real-world applications is characterized by their symbolic\nnature, necessitating a strong capability for symbolic reasoning. This paper\ninvestigates the potential application of Large Language Models (LLMs) as\nsymbolic reasoners. We focus on text-based games, significant benchmarks for\nagents with natural language capabilities, particularly in symbolic tasks like\nmath, map reading, sorting, and applying common sense in text-based worlds. To\nfacilitate these agents, we propose an LLM agent designed to tackle symbolic\nchallenges and achieve in-game objectives. We begin by initializing the LLM\nagent and informing it of its role. The agent then receives observations and a\nset of valid actions from the text-based games, along with a specific symbolic\nmodule. With these inputs, the LLM agent chooses an action and interacts with\nthe game environments. Our experimental results demonstrate that our method\nsignificantly enhances the capability of LLMs as automated agents for symbolic\nreasoning, and our LLM agent is effective in text-based games involving\nsymbolic tasks, achieving an average performance of 88% across all tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.09334v1.pdf"
    },
    {
        "title": "CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation",
        "authors": [
            "Jueon Eom",
            "Seyeon Jeong",
            "Taekyoung Kwon"
        ],
        "published": "2024-02-19T15:30:40Z",
        "summary": "Fuzzing is an effective bug-finding technique but it struggles with complex\nsystems like JavaScript engines that demand precise grammatical input.\nRecently, researchers have adopted language models for context-aware mutation\nin fuzzing to address this problem. However, existing techniques are limited in\nutilizing coverage guidance for fuzzing, which is rather performed in a\nblack-box manner. This paper presents a novel technique called CovRL\n(Coverage-guided Reinforcement Learning) that combines Large Language Models\n(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,\nCovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging\nthe Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a\nweighted coverage map. This map is key in calculating the fuzzing reward, which\nis then applied to the LLM-based mutator through reinforcement learning.\nCovRL-Fuzz, through this approach, enables the generation of test cases that\nare more likely to discover new coverage areas, thus improving vulnerability\ndetection while minimizing syntax and semantic errors, all without needing\nextra post-processing. Our evaluation results indicate that CovRL-Fuzz\noutperforms the state-of-the-art fuzzers in terms of code coverage and\nbug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related\nbugs in the latest JavaScript engines, including 39 previously unknown\nvulnerabilities and 11 CVEs.",
        "pdf_link": "https://arxiv.org/pdf/2402.12222v1.pdf"
    },
    {
        "title": "Unveiling Hidden Links Between Unseen Security Entities",
        "authors": [
            "Daniel Alfasi",
            "Tal Shapira",
            "Anat Bremler Barr"
        ],
        "published": "2024-03-04T13:14:39Z",
        "summary": "The proliferation of software vulnerabilities poses a significant challenge\nfor security databases and analysts tasked with their timely identification,\nclassification, and remediation. With the National Vulnerability Database (NVD)\nreporting an ever-increasing number of vulnerabilities, the traditional manual\nanalysis becomes untenably time-consuming and prone to errors. This paper\nintroduces VulnScopper, an innovative approach that utilizes multi-modal\nrepresentation learning, combining Knowledge Graphs (KG) and Natural Language\nProcessing (NLP), to automate and enhance the analysis of software\nvulnerabilities. Leveraging ULTRA, a knowledge graph foundation model, combined\nwith a Large Language Model (LLM), VulnScopper effectively handles unseen\nentities, overcoming the limitations of previous KG approaches. We evaluate\nVulnScopper on two major security datasets, the NVD and the Red Hat CVE\ndatabase. Our method significantly improves the link prediction accuracy\nbetween Common Vulnerabilities and Exposures (CVEs), Common Weakness\nEnumeration (CWEs), and Common Platform Enumerations (CPEs). Our results show\nthat VulnScopper outperforms existing methods, achieving up to 78% Hits@10\naccuracy in linking CVEs to CPEs and CWEs and presenting an 11.7% improvement\nover large language models in predicting CWE labels based on the Red Hat\ndatabase. Based on the NVD, only 6.37% of the linked CPEs are being published\nduring the first 30 days; many of them are related to critical and high-risk\nvulnerabilities which, according to multiple compliance frameworks (such as\nCISA and PCI), should be remediated within 15-30 days. Our model can uncover\nnew products linked to vulnerabilities, reducing remediation time and improving\nvulnerability management. We analyzed several CVEs from 2023 to showcase this\nability.",
        "pdf_link": "https://arxiv.org/pdf/2403.02014v1.pdf"
    },
    {
        "title": "Towards Safer Large Language Models through Machine Unlearning",
        "authors": [
            "Zheyuan Liu",
            "Guangyao Dou",
            "Zhaoxuan Tan",
            "Yijun Tian",
            "Meng Jiang"
        ],
        "published": "2024-02-15T16:28:34Z",
        "summary": "The rapid advancement of Large Language Models (LLMs) has demonstrated their\nvast potential across various domains, attributed to their extensive\npretraining knowledge and exceptional generalizability. However, LLMs often\nencounter challenges in generating harmful content when faced with problematic\nprompts. To address this problem, existing work attempted to implement a\ngradient ascent based approach to prevent LLMs from producing harmful output.\nWhile these methods can be effective, they frequently impact the model utility\nin responding to normal prompts. To address this gap, we introduce Selective\nKnowledge negation Unlearning (SKU), a novel unlearning framework for LLMs,\ndesigned to eliminate harmful knowledge while preserving utility on normal\nprompts. Specifically, SKU is consisted of two stages: harmful knowledge\nacquisition stage and knowledge negation stage. The first stage aims to\nidentify and acquire harmful knowledge within the model, whereas the second is\ndedicated to remove this knowledge. SKU selectively isolates and removes\nharmful knowledge in model parameters, ensuring the model's performance remains\nrobust on normal prompts. Our experiments conducted across various LLM\narchitectures demonstrate that SKU identifies a good balance point between\nremoving harmful information and preserving utility.",
        "pdf_link": "https://arxiv.org/pdf/2402.10058v1.pdf"
    },
    {
        "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition",
        "authors": [
            "Lu Ye",
            "Ze Tao",
            "Yong Huang",
            "Yang Li"
        ],
        "published": "2024-02-23T09:29:19Z",
        "summary": "Self-attention is an essential component of large language models(LLMs) but a\nsignificant source of inference latency for long sequences. In multi-tenant\nLLMs serving scenarios, the compute and memory operation cost of self-attention\ncan be optimized by using the probability that multiple LLM requests have\nshared system prompts in prefixes. In this paper, we introduce ChunkAttention,\na prefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the start-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
        "pdf_link": "https://arxiv.org/pdf/2402.15220v2.pdf"
    },
    {
        "title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
        "authors": [
            "Xu Huang",
            "Zhirui Zhang",
            "Xiang Geng",
            "Yichao Du",
            "Jiajun Chen",
            "Shujian Huang"
        ],
        "published": "2024-01-12T13:23:21Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable results in the machine\ntranslation evaluation task, yet there remains a gap in knowledge regarding how\nthey utilize the provided data to conduct evaluations. This study aims to\nexplore how LLMs leverage source and reference information in evaluating\ntranslations, with the ultimate goal of better understanding the working\nmechanism of LLMs. To this end, we design the controlled experiments across\nvarious input modes and model types, and employ both coarse-grained and\nfine-grained prompts to discern the utility of source versus reference\ninformation. Surprisingly, we find that reference information significantly\nenhances the evaluation accuracy, while source information sometimes is\ncounterproductive, indicating a lack of cross-lingual capability when using\nLLMs to evaluate translations. We further conduct a meta-evaluation for\ntranslation error detection of LLMs, observing a similar phenomenon. These\nfindings also suggest a potential research direction for LLMs that fully\nexploits the cross-lingual capability of LLMs to achieve better performance in\nmachine translation evaluation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.06568v1.pdf"
    },
    {
        "title": "Detection of Machine-Generated Text: Literature Survey",
        "authors": [
            "Dmytro Valiaiev"
        ],
        "published": "2024-01-02T01:44:15Z",
        "summary": "Since language models produce fake text quickly and easily, there is an\noversupply of such content in the public domain. The degree of sophistication\nand writing style has reached a point where differentiating between human\nauthored and machine-generated content is nearly impossible. As a result, works\ngenerated by language models rather than human authors have gained significant\nmedia attention and stirred controversy.Concerns regarding the possible\ninfluence of advanced language models on society have also arisen, needing a\nfuller knowledge of these processes. Natural language generation (NLG) and\ngenerative pre-trained transformer (GPT) models have revolutionized a variety\nof sectors: the scope not only permeated throughout journalism and customer\nservice but also reached academia. To mitigate the hazardous implications that\nmay arise from the use of these models, preventative measures must be\nimplemented, such as providing human agents with the capacity to distinguish\nbetween artificially made and human composed texts utilizing automated systems\nand possibly reverse-engineered language models. Furthermore, to ensure a\nbalanced and responsible approach, it is critical to have a full grasp of the\nsocio-technological ramifications of these breakthroughs. This literature\nsurvey aims to compile and synthesize accomplishments and developments in the\naforementioned work, while also identifying future prospects. It also gives an\noverview of machine-generated text trends and explores the larger societal\nimplications. Ultimately, this survey intends to contribute to the development\nof robust and effective approaches for resolving the issues connected with the\nusage and detection of machine-generated text by exploring the interplay\nbetween the capabilities of language models and their possible implications.",
        "pdf_link": "https://arxiv.org/pdf/2402.01642v1.pdf"
    },
    {
        "title": "Deciphering the Impact of Pretraining Data on Large Language Models through Machine Unlearning",
        "authors": [
            "Yang Zhao",
            "Li Du",
            "Xiao Ding",
            "Kai Xiong",
            "Zhouhao Sun",
            "Jun Shi",
            "Ting Liu",
            "Bing Qin"
        ],
        "published": "2024-02-18T10:36:05Z",
        "summary": "Through pretraining on a corpus with various sources, Large Language Models\n(LLMs) have gained impressive performance. However, the impact of each\ncomponent of the pretraining corpus remains opaque. As a result, the\norganization of the pretraining corpus is still empirical and may deviate from\nthe optimal. To address this issue, we systematically analyze the impact of 48\ndatasets from 5 major categories of pretraining data of LLMs and measure their\nimpacts on LLMs using benchmarks about nine major categories of model\ncapabilities. Our analyses provide empirical results about the contribution of\nmultiple corpora on the performances of LLMs, along with their joint impact\npatterns, including complementary, orthogonal, and correlational relationships.\nWe also identify a set of ``high-impact data'' such as Books that is\nsignificantly related to a set of model capabilities. These findings provide\ninsights into the organization of data to support more efficient pretraining of\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11537v2.pdf"
    },
    {
        "title": "Instructing Large Language Models to Identify and Ignore Irrelevant Conditions",
        "authors": [
            "Zhenyu Wu",
            "Chao Shen",
            "Meng Jiang"
        ],
        "published": "2024-03-19T14:07:28Z",
        "summary": "Math word problem (MWP) solving requires generating a reasoning path based on\na given problem description that often contains irrelevant conditions. Existing\nchain-of-thought (CoT) prompting methods elicited multi-step reasoning\nabilities of large language models (LLMs) to solve MWPs. However, they were\nseriously confused by the irrelevant conditions, resulting in low accuracy. In\nthis paper, we propose a novel approach named I$^3$C that instructs LLMs to\nidentify and ignore irrelevant conditions. It identifies a set of irrelevant\ncondition candidates that have a weak semantic relevance with the question.\nThen it prompts LLMs to verify the irrelevant conditions. Lastly it instructs\nthe LLMs with the verification on relevant and irrelevant conditions to avoid\nconfusion and improve reasoning paths. Moreover, we propose to select (problem,\nreasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot\nreasoning. We develop I$^3$C-Select that selects the most confusing problems\nbased on the semantic relevance measurement. We conduct extensive experiments\non eight MWP datasets. I$^3$C can be combined with any CoT prompting methods to\nimprove the performance of solving MWPs. Notably, with GPT-3.5-Turbo and\nI$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and\nGSM-ICM-1K, respectively, significantly outperforming the state-of-the-art\nfew-shot prompting method Complex-CoT by +11.7 and +11.1. Our implementation is\nmade publicly available at https://wzy6642.github.io/I3C.github.io/.",
        "pdf_link": "https://arxiv.org/pdf/2403.12744v1.pdf"
    },
    {
        "title": "HateCOT: An Explanation-Enhanced Dataset for Generalizable Offensive Speech Detection via Large Language Models",
        "authors": [
            "Huy Nghiem",
            "Hal Daum\u00e9 III"
        ],
        "published": "2024-03-18T04:12:35Z",
        "summary": "The ubiquitousness of social media has led to the need for reliable and\nefficient detection of offensive content to limit harmful effects. This has led\nto a proliferation of datasets and models related to detecting offensive\ncontent. While sophisticated models have attained strong performance on\nindividual datasets, these models often do not generalize due to differences\nbetween how \"offensive content\" is conceptualized, and the resulting\ndifferences in how these datasets are labeled. In this paper, we introduce\nHateCOT, a dataset of 52,000 samples drawn from diverse existing sources with\nexplanations generated by GPT-3.5-Turbo and human-curated. We show that\npre-training models for the detection of offensive content on HateCOT\nsignificantly boots open-sourced Language Models on three benchmark datasets in\nboth zero and few-shot settings, despite differences in domain and task.} We\nfurther find that HateCOT enables effective K-shot fine-tuning in the\nlow-resource settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11456v1.pdf"
    },
    {
        "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes",
        "authors": [
            "Xiaomeng Hu",
            "Pin-Yu Chen",
            "Tsung-Yi Ho"
        ],
        "published": "2024-03-01T03:29:54Z",
        "summary": "Large Language Models (LLMs) are becoming a prominent generative AI tool,\nwhere the user enters a query and the LLM generates an answer. To reduce harm\nand misuse, efforts have been made to align these LLMs to human values using\nadvanced training techniques such as Reinforcement Learning from Human Feedback\n(RLHF). However, recent studies have highlighted the vulnerability of LLMs to\nadversarial jailbreak attempts aiming at subverting the embedded safety\nguardrails. To address this challenge, this paper defines and investigates the\nRefusal Loss of LLMs and then proposes a method called Gradient Cuff to detect\njailbreak attempts. Gradient Cuff exploits the unique properties observed in\nthe refusal loss landscape, including functional values and its smoothness, to\ndesign an effective two-step detection strategy. Experimental results on two\naligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak\nattacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can\nsignificantly improve the LLM's rejection capability for malicious jailbreak\nqueries, while maintaining the model's performance for benign user queries by\nadjusting the detection threshold.",
        "pdf_link": "https://arxiv.org/pdf/2403.00867v2.pdf"
    },
    {
        "title": "Helpful or Harmful? Exploring the Efficacy of Large Language Models for Online Grooming Prevention",
        "authors": [
            "Ellie Prosser",
            "Matthew Edwards"
        ],
        "published": "2024-03-14T18:27:43Z",
        "summary": "Powerful generative Large Language Models (LLMs) are becoming popular tools\namongst the general public as question-answering systems, and are being\nutilised by vulnerable groups such as children. With children increasingly\ninteracting with these tools, it is imperative for researchers to scrutinise\nthe safety of LLMs, especially for applications that could lead to serious\noutcomes, such as online child safety queries. In this paper, the efficacy of\nLLMs for online grooming prevention is explored both for identifying and\navoiding grooming through advice generation, and the impact of prompt design on\nmodel performance is investigated by varying the provided context and prompt\nspecificity. In results reflecting over 6,000 LLM interactions, we find that no\nmodels were clearly appropriate for online grooming prevention, with an\nobserved lack of consistency in behaviours, and potential for harmful answer\ngeneration, especially from open-source models. We outline where and how models\nfall short, providing suggestions for improvement, and identify prompt designs\nthat heavily altered model performance in troubling ways, with findings that\ncan be used to inform best practice usage guides.",
        "pdf_link": "https://arxiv.org/pdf/2403.09795v1.pdf"
    },
    {
        "title": "Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale",
        "authors": [
            "Dan Zhao",
            "Siddharth Samsi",
            "Joseph McDonald",
            "Baolin Li",
            "David Bestor",
            "Michael Jones",
            "Devesh Tiwari",
            "Vijay Gadepally"
        ],
        "published": "2024-02-25T02:22:34Z",
        "summary": "As research and deployment of AI grows, the computational burden to support\nand sustain its progress inevitably does too. To train or fine-tune\nstate-of-the-art models in NLP, computer vision, etc., some form of AI hardware\nacceleration is virtually a requirement. Recent large language models require\nconsiderable resources to train and deploy, resulting in significant energy\nusage, potential carbon emissions, and massive demand for GPUs and other\nhardware accelerators. However, this surge carries large implications for\nenergy sustainability at the HPC/datacenter level. In this paper, we study the\naggregate effect of power-capping GPUs on GPU temperature and power draw at a\nresearch supercomputing center. With the right amount of power-capping, we show\nsignificant decreases in both temperature and power draw, reducing power\nconsumption and potentially improving hardware life-span with minimal impact on\njob performance. While power-capping reduces power draw by design, the\naggregate system-wide effect on overall energy consumption is less clear; for\ninstance, if users notice job performance degradation from GPU power-caps, they\nmay request additional GPU-jobs to compensate, negating any energy savings or\neven worsening energy consumption. To our knowledge, our work is the first to\nconduct and make available a detailed analysis of the effects of GPU\npower-capping at the supercomputing scale. We hope our work will inspire\nHPCs/datacenters to further explore, evaluate, and communicate the impact of\npower-capping AI hardware accelerators for more sustainable AI.",
        "pdf_link": "https://arxiv.org/pdf/2402.18593v1.pdf"
    },
    {
        "title": "Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments",
        "authors": [
            "Sitao Cheng",
            "Ziyuan Zhuang",
            "Yong Xu",
            "Fangkai Yang",
            "Chaoyun Zhang",
            "Xiaoting Qin",
            "Xiang Huang",
            "Ling Chen",
            "Qingwei Lin",
            "Dongmei Zhang",
            "Saravan Rajmohan",
            "Qi Zhang"
        ],
        "published": "2024-03-13T14:59:07Z",
        "summary": "Large Language Models (LLMs) have shown potential in reasoning over\nstructured environments, e.g., knowledge graph and table. Such tasks typically\nrequire multi-hop reasoning, i.e., match natural language utterance with\ninstances in the environment. Previous methods leverage LLMs to incrementally\nbuild a reasoning path, where the LLMs either invoke tools or pick up schemas\nby step-by-step interacting with the environment. We propose\nReasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently\nand faithfully reason over structured environments. In Readi, LLMs initially\ngenerate a reasoning path given a query, and edit the path only when necessary.\nWe instantiate the path on structured environments and provide feedback to edit\nthe path if anything goes wrong. Experimental results on three KGQA datasets\nand two TableQA datasets show the effectiveness of Readi, significantly\nsurpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%\non WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and\n74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).\nOur code will be available upon publication.",
        "pdf_link": "https://arxiv.org/pdf/2403.08593v1.pdf"
    },
    {
        "title": "Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network",
        "authors": [
            "Lin Chen",
            "Fengli Xu",
            "Nian Li",
            "Zhenyu Han",
            "Meng Wang",
            "Yong Li",
            "Pan Hui"
        ],
        "published": "2024-02-18T09:21:12Z",
        "summary": "Heterogeneous information networks (HIN) have gained increasing popularity\nfor being able to capture complex relations between nodes of diverse types.\nMeta-structure was proposed to identify important patterns of relations on HIN,\nwhich has been proven effective for extracting rich semantic information and\nfacilitating graph neural networks to learn expressive representations.\nHowever, hand-crafted meta-structures pose challenges for scaling up, which\ndraws wide research attention for developing automatic meta-structure search\nalgorithms. Previous efforts concentrate on searching for meta-structures with\ngood empirical prediction performance, overlooking explainability. Thus, they\noften produce meta-structures prone to overfitting and incomprehensible to\nhumans. To address this, we draw inspiration from the emergent reasoning\nabilities of large language models (LLMs). We propose a novel REasoning\nmeta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into\nthe evolutionary procedure. ReStruct uses a grammar translator to encode\nmeta-structures into natural language sentences, and leverages the reasoning\npower of LLMs to evaluate semantically feasible meta-structures. ReStruct also\nemploys performance-oriented evolutionary operations. These two competing\nforces jointly optimize for semantic explainability and empirical performance\nof meta-structures. We also design a differential LLM explainer that can\nproduce natural language explanations for the discovered meta-structures, and\nrefine the explanation by reasoning through the search history. Experiments on\nfive datasets demonstrate ReStruct achieve SOTA performance in node\nclassification and link recommendation tasks. Additionally, a survey study\ninvolving 73 graduate students shows that the meta-structures and natural\nlanguage explanations generated by ReStruct are substantially more\ncomprehensible.",
        "pdf_link": "https://arxiv.org/pdf/2402.11518v1.pdf"
    },
    {
        "title": "Extreme Compression of Large Language Models via Additive Quantization",
        "authors": [
            "Vage Egiazarian",
            "Andrei Panferov",
            "Denis Kuznedelev",
            "Elias Frantar",
            "Artem Babenko",
            "Dan Alistarh"
        ],
        "published": "2024-01-11T18:54:44Z",
        "summary": "The emergence of accurate open large language models (LLMs) has led to a race\ntowards quantization techniques for such models enabling execution on end-user\ndevices. In this paper, we revisit the problem of \"extreme\" LLM\ncompression--defined as targeting extremely low bit counts, such as 2 to 3 bits\nper parameter, from the point of view of classic methods in Multi-Codebook\nQuantization (MCQ). Our work builds on top of Additive Quantization, a classic\nalgorithm from the MCQ family, and adapts it to the quantization of language\nmodels. The resulting algorithm advances the state-of-the-art in LLM\ncompression, outperforming all recently-proposed techniques in terms of\naccuracy at a given compression budget. For instance, when compressing Llama 2\nmodels to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93\nperplexity (a 1.29 improvement relative to the best prior work, and 1.81 points\nfrom FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B\nmodel to 3.94 perplexity (a .22 improvement) on WikiText2. We release our\nimplementation of Additive Quantization for Language Models AQLM as a baseline\nto facilitate future research in LLM quantization.",
        "pdf_link": "https://arxiv.org/pdf/2401.06118v2.pdf"
    },
    {
        "title": "The Sound of Healthcare: Improving Medical Transcription ASR Accuracy with Large Language Models",
        "authors": [
            "Ayo Adedeji",
            "Sarita Joshi",
            "Brendan Doohan"
        ],
        "published": "2024-02-12T14:01:12Z",
        "summary": "In the rapidly evolving landscape of medical documentation, transcribing\nclinical dialogues accurately is increasingly paramount. This study explores\nthe potential of Large Language Models (LLMs) to enhance the accuracy of\nAutomatic Speech Recognition (ASR) systems in medical transcription. Utilizing\nthe PriMock57 dataset, which encompasses a diverse range of primary care\nconsultations, we apply advanced LLMs to refine ASR-generated transcripts. Our\nresearch is multifaceted, focusing on improvements in general Word Error Rate\n(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential\nmedical terms, and speaker diarization accuracy. Additionally, we assess the\nrole of LLM post-processing in improving semantic textual similarity, thereby\npreserving the contextual integrity of clinical dialogues. Through a series of\nexperiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)\nprompting techniques in enhancing diarization and correction accuracy. Our\nfindings demonstrate that LLMs, particularly through CoT prompting, not only\nimprove the diarization accuracy of existing ASR systems but also achieve\nstate-of-the-art performance in this domain. This improvement extends to more\naccurately capturing medical concepts and enhancing the overall semantic\ncoherence of the transcribed dialogues. These findings illustrate the dual role\nof LLMs in augmenting ASR outputs and independently excelling in transcription\ntasks, holding significant promise for transforming medical ASR systems and\nleading to more accurate and reliable patient records in healthcare settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.07658v1.pdf"
    },
    {
        "title": "Generalizing Reward Modeling for Out-of-Distribution Preference Learning",
        "authors": [
            "Chen Jia"
        ],
        "published": "2024-02-22T18:20:33Z",
        "summary": "Preference learning (PL) with large language models (LLMs) aims to align the\nLLMs' generations with human preferences. Previous work on reinforcement\nlearning from human feedback (RLHF) has demonstrated promising results in\nin-distribution PL. However, due to the difficulty of obtaining human feedback,\ndiscretely training reward models for every encountered distribution is\nchallenging. Thus, out-of-distribution (OOD) PL is practically useful for\nenhancing the generalization ability of LLMs with limited preference feedback.\nThis work addresses OOD PL by optimizing a general reward model through a\nmeta-learning approach. During meta-training, a bilevel optimization algorithm\nis utilized to learn a reward model capable of guiding policy learning to align\nwith human preferences across various distributions. When encountering a test\ndistribution, the meta-test procedure conducts regularized policy optimization\nusing the learned reward model for PL. We theoretically demonstrate the\nconvergence rate of the bilevel optimization algorithm under reasonable\nassumptions. Additionally, we conduct experiments on two text generation tasks\nacross 20 held-out domains and outperform a variety of strong baselines across\nvarious evaluation metrics.",
        "pdf_link": "https://arxiv.org/pdf/2402.14760v1.pdf"
    },
    {
        "title": "Reinforcement Learning from Human Feedback with Active Queries",
        "authors": [
            "Kaixuan Ji",
            "Jiafan He",
            "Quanquan Gu"
        ],
        "published": "2024-02-14T18:58:40Z",
        "summary": "Aligning large language models (LLM) with human preference plays a key role\nin building modern generative models and can be achieved by reinforcement\nlearning from human feedback (RLHF). Despite their superior performance,\ncurrent RLHF approaches often require a large amount of human-labelled\npreference data, which is expensive to collect. In this paper, inspired by the\nsuccess of active learning, we address this problem by proposing\nquery-efficient RLHF methods. We first formalize the alignment problem as a\ncontextual dueling bandit problem and design an active-query-based proximal\npolicy optimization (APPO) algorithm with an $\\tilde{O}(d^2/\\Delta)$ regret\nbound and an $\\tilde{O}(d^2/\\Delta^2)$ query complexity, where $d$ is the\ndimension of feature space and $\\Delta$ is the sub-optimality gap over all the\ncontexts. We then propose ADPO, a practical version of our algorithm based on\ndirect preference optimization (DPO) and apply it to fine-tuning LLMs. Our\nexperiments show that ADPO, while only making about half of queries for human\npreference, matches the performance of the state-of-the-art DPO method.",
        "pdf_link": "https://arxiv.org/pdf/2402.09401v1.pdf"
    },
    {
        "title": "The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions",
        "authors": [
            "Christian A. Schiller"
        ],
        "published": "2024-03-13T21:39:39Z",
        "summary": "The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for\nArtificial Intelligence, introducing Large Language Models (LLMs) to the\nmainstream and setting new records in user adoption. LLMs, particularly\nChatGPT, trained on extensive internet data, demonstrate remarkable\nconversational capabilities across various domains, suggesting a significant\nimpact on the workforce. However, these models are susceptible to errors -\n\"hallucinations\" and omissions, generating incorrect or incomplete information.\nThis poses risks especially in contexts where accuracy is crucial, such as\nlegal compliance, medicine or fine-grained process frameworks.\n  There are both technical and human solutions to cope with this isse. This\npaper explores the human factors that enable users to detect errors in LLM\noutputs, a critical component in mitigating risks associated with their use in\nprofessional settings. Understanding these factors is essential for\norganizations aiming to leverage LLM technology efficiently, guiding targeted\ntraining and deployment strategies to enhance error detection by users. This\napproach not only aims to optimize the use of LLMs but also to prevent\npotential downstream issues stemming from reliance on inaccurate model\nresponses. The research emphasizes the balance between technological\nadvancement and human insight in maximizing the benefits of LLMs while\nminimizing the risks, particularly in areas where precision is paramount.\n  This paper performs a systematic literature research on this research topic,\nanalyses and synthesizes the findings, and outlines future research directions.\nLiterature selection cut-off date is January 11th 2024.",
        "pdf_link": "https://arxiv.org/pdf/2403.09743v1.pdf"
    },
    {
        "title": "A Truly Joint Neural Architecture for Segmentation and Parsing",
        "authors": [
            "Danit Yshaayahu Levi",
            "Reut Tsarfaty"
        ],
        "published": "2024-02-04T16:56:08Z",
        "summary": "Contemporary multilingual dependency parsers can parse a diverse set of\nlanguages, but for Morphologically Rich Languages (MRLs), performance is\nattested to be lower than other languages. The key challenge is that, due to\nhigh morphological complexity and ambiguity of the space-delimited input\ntokens, the linguistic units that act as nodes in the tree are not known in\nadvance. Pre-neural dependency parsers for MRLs subscribed to the joint\nmorpho-syntactic hypothesis, stating that morphological segmentation and\nsyntactic parsing should be solved jointly, rather than as a pipeline where\nsegmentation precedes parsing. However, neural state-of-the-art parsers to date\nuse a strict pipeline. In this paper we introduce a joint neural architecture\nwhere a lattice-based representation preserving all morphological ambiguity of\nthe input is provided to an arc-factored model, which then solves the\nmorphological segmentation and syntactic parsing tasks at once. Our experiments\non Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art\nperformance on parsing, tagging and segmentation of the Hebrew section of UD,\nusing a single model. This proposed architecture is LLM-based and language\nagnostic, providing a solid foundation for MRLs to obtain further performance\nimprovements and bridge the gap with other languages.",
        "pdf_link": "https://arxiv.org/pdf/2402.02564v2.pdf"
    },
    {
        "title": "Natural Language Reinforcement Learning",
        "authors": [
            "Xidong Feng",
            "Ziyu Wan",
            "Mengyue Yang",
            "Ziyan Wang",
            "Girish A. Koushik",
            "Yali Du",
            "Ying Wen",
            "Jun Wang"
        ],
        "published": "2024-02-11T11:03:04Z",
        "summary": "Reinforcement Learning (RL) has shown remarkable abilities in learning\npolicies for decision-making tasks. However, RL is often hindered by issues\nsuch as low sample efficiency, lack of interpretability, and sparse supervision\nsignals. To tackle these limitations, we take inspiration from the human\nlearning process and introduce Natural Language Reinforcement Learning (NLRL),\nwhich innovatively combines RL principles with natural language representation.\nSpecifically, NLRL redefines RL concepts like task objectives, policy, value\nfunction, Bellman equation, and policy iteration in natural language space. We\npresent how NLRL can be practically implemented with the latest advancements in\nlarge language models (LLMs) like GPT-4. Initial experiments over tabular MDPs\ndemonstrate the effectiveness, efficiency, and also interpretability of the\nNLRL framework.",
        "pdf_link": "https://arxiv.org/pdf/2402.07157v2.pdf"
    },
    {
        "title": "Towards Enabling FAIR Dataspaces Using Large Language Models",
        "authors": [
            "Benedikt T. Arnold",
            "Johannes Theissen-Lipp",
            "Diego Collarana",
            "Christoph Lange",
            "Sandra Geisler",
            "Edward Curry",
            "Stefan Decker"
        ],
        "published": "2024-03-18T16:46:00Z",
        "summary": "Dataspaces have recently gained adoption across various sectors, including\ntraditionally less digitized domains such as culture. Leveraging Semantic Web\ntechnologies helps to make dataspaces FAIR, but their complexity poses a\nsignificant challenge to the adoption of dataspaces and increases their cost.\nThe advent of Large Language Models (LLMs) raises the question of how these\nmodels can support the adoption of FAIR dataspaces. In this work, we\ndemonstrate the potential of LLMs in dataspaces with a concrete example. We\nalso derive a research agenda for exploring this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2403.15451v1.pdf"
    },
    {
        "title": "SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning",
        "authors": [
            "Yu Zhang",
            "Hui-Ling Zhen",
            "Zehua Pei",
            "Yingzhao Lian",
            "Lihao Yin",
            "Mingxuan Yuan",
            "Bei Yu"
        ],
        "published": "2024-02-19T07:38:57Z",
        "summary": "Considering the challenges faced by large language models (LLMs) on logical\nreasoning, prior efforts have sought to transform problem-solving through tool\nlearning. While progress has been made on small-scale problems, solving\nindustrial cases remains difficult due to their large scale and intricate\nexpressions. In this paper, we propose a novel solver-layer adaptation (SoLA)\nmethod, where we introduce a solver as a new layer of the LLM to differentially\nguide solutions towards satisfiability. In SoLA, LLM aims to comprehend the\nsearch space described in natural language and identify local solutions of the\nhighest quality, while the solver layer focuses solely on constraints not\nsatisfied by the initial solution. Leveraging MaxSAT as a bridge, we define\nforward and backward transfer gradients, enabling the final model to converge\nto a satisfied solution or prove unsatisfiability. The backdoor theory ensures\nthat SoLA can obtain accurate solutions within polynomial loops. We evaluate\nthe performance of SoLA on various datasets and empirically demonstrate its\nconsistent outperformance against existing symbolic solvers (including Z3 and\nKissat) and tool-learning methods in terms of efficiency in large-scale\nproblem-solving.",
        "pdf_link": "https://arxiv.org/pdf/2402.11903v1.pdf"
    },
    {
        "title": "The Neglected Tails of Vision-Language Models",
        "authors": [
            "Shubham Parashar",
            "Zhiqiu Lin",
            "Tian Liu",
            "Xiangjue Dong",
            "Yanan Li",
            "Deva Ramanan",
            "James Caverlee",
            "Shu Kong"
        ],
        "published": "2024-01-23T01:25:00Z",
        "summary": "Vision-language models (VLMs) excel in zero-shot recognition but their\nperformance varies greatly across different visual concepts. For example,\nalthough CLIP achieves impressive accuracy on ImageNet (60-80%), its\nperformance drops below 10% for more than ten concepts like night snake,\npresumably due to their limited presence in the pretraining data. However,\nmeasuring the frequency of concepts in VLMs' large-scale datasets is\nchallenging. We address this by using large language models (LLMs) to count the\nnumber of pretraining texts that contain synonyms of these concepts. Our\nanalysis confirms that popular datasets, such as LAION, exhibit a long-tailed\nconcept distribution, yielding biased performance in VLMs. We also find that\ndownstream applications of VLMs, including visual chatbots (e.g., GPT-4V) and\ntext-to-image models (e.g., Stable Diffusion), often fail to recognize or\ngenerate images of rare concepts identified by our method. To mitigate the\nimbalanced performance of zero-shot VLMs, we propose REtrieval-Augmented\nLearning (REAL). First, instead of prompting VLMs using the original class\nnames, REAL uses their most frequent synonyms found in pretraining texts. This\nsimple change already outperforms costly human-engineered and LLM-enriched\nprompts over nine benchmark datasets. Second, REAL trains a linear classifier\non a small yet balanced set of pretraining data retrieved using concept\nsynonyms. REAL surpasses the previous zero-shot SOTA, using 400x less storage\nand 10,000x less training time!",
        "pdf_link": "https://arxiv.org/pdf/2401.12425v2.pdf"
    },
    {
        "title": "PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization",
        "authors": [
            "Xiangdi Meng",
            "Damai Dai",
            "Weiyao Luo",
            "Zhe Yang",
            "Shaoxiang Wu",
            "Xiaochen Wang",
            "Peiyi Wang",
            "Qingxiu Dong",
            "Liang Chen",
            "Zhifang Sui"
        ],
        "published": "2024-02-25T16:43:41Z",
        "summary": "Supervised fine-tuning is the most common method to adapt large language\nmodels (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive\ncomputational resources. Recently, parameter-efficient fine-tuning (PEFT)\nmethods have been widely studied due to its cost-effectiveness. LoRA is one of\nthe most widely used methods, which assumes that the optimization process is\nessentially low-dimensional. Although LoRA fine-tuning is effective, there is\nstill a performance gap compared to full fine-tuning, since its weight update\nis limited to low-rank matrices. In order to break the low-rank bottleneck in\nLoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank\nupdate matrices multiple times to achieve a higher update rank. PLoRA has\nmultiple training stages. During each stage, we still update only the LoRA\nweights. However, at the end of each stage, we unload the LoRA weights into the\nbackbone parameters and then reinitialize the LoRA states. Experimental results\nshow that PLoRA has stronger learning ability, approximately 1.8 times that of\nLoRA's learning ability at most, but it does not increase memory usage.\nFurther, we introduce a momentum-based unloading strategy for PLoRA to mitigate\nthe training instability.",
        "pdf_link": "https://arxiv.org/pdf/2402.16141v1.pdf"
    },
    {
        "title": "Structured Entity Extraction Using Large Language Models",
        "authors": [
            "Haolun Wu",
            "Ye Yuan",
            "Liana Mikaelyan",
            "Alexander Meulemans",
            "Xue Liu",
            "James Hensman",
            "Bhaskar Mitra"
        ],
        "published": "2024-02-06T22:15:09Z",
        "summary": "Recent advances in machine learning have significantly impacted the field of\ninformation extraction, with Large Language Models (LLMs) playing a pivotal\nrole in extracting structured information from unstructured text. This paper\nexplores the challenges and limitations of current methodologies in structured\nentity extraction and introduces a novel approach to address these issues. We\ncontribute to the field by first introducing and formalizing the task of\nStructured Entity Extraction (SEE), followed by proposing Approximate Entity\nSet OverlaP (AESOP) Metric designed to appropriately assess model performance\non this task. Later, we propose a new model that harnesses the power of LLMs\nfor enhanced effectiveness and efficiency through decomposing the entire\nextraction task into multiple stages. Quantitative evaluation and human\nside-by-side evaluation confirm that our model outperforms baselines, offering\npromising directions for future advancements in structured entity extraction.",
        "pdf_link": "https://arxiv.org/pdf/2402.04437v2.pdf"
    },
    {
        "title": "Large Language Models are Few-shot Generators: Proposing Hybrid Prompt Algorithm To Generate Webshell Escape Samples",
        "authors": [
            "Mingrui Ma",
            "Lansheng Han",
            "Chunjie Zhou"
        ],
        "published": "2024-02-12T04:59:58Z",
        "summary": "The frequent occurrence of cyber-attacks has made webshell attacks and\ndefense gradually become a research hotspot in the field of network security.\nHowever, the lack of publicly available benchmark datasets and the\nover-reliance on manually defined rules for webshell escape sample generation\nhave slowed down the progress of research related to webshell escape sample\ngeneration strategies and artificial intelligence-based webshell detection\nalgorithms. To address the drawbacks of weak webshell sample escape\ncapabilities, the lack of webshell datasets with complex malicious features,\nand to promote the development of webshell detection technology, we propose the\nHybrid Prompt algorithm for webshell escape sample generation with the help of\nlarge language models. As a prompt algorithm specifically developed for\nwebshell sample generation, the Hybrid Prompt algorithm not only combines\nvarious prompt ideas including Chain of Thought, Tree of Thought, but also\nincorporates various components such as webshell hierarchical module and\nfew-shot example to facilitate the LLM in learning and reasoning webshell\nescape strategies. Experimental results show that the Hybrid Prompt algorithm\ncan work with multiple LLMs with excellent code reasoning ability to generate\nhigh-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on\nVIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).",
        "pdf_link": "https://arxiv.org/pdf/2402.07408v1.pdf"
    },
    {
        "title": "Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities",
        "authors": [
            "Mingyu Jin",
            "Hua Tang",
            "Chong Zhang",
            "Qinkai Yu",
            "Chengzhi Liu",
            "Suiyuan Zhu",
            "Yongfeng Zhang",
            "Mengnan Du"
        ],
        "published": "2024-02-16T17:15:28Z",
        "summary": "Large language models (LLMs) have been applied in many fields with rapid\ndevelopment in recent years. As a classic machine learning task, time series\nforecasting has recently received a boost from LLMs. However, there is a\nresearch gap in the LLMs' preferences in this field. In this paper, by\ncomparing LLMs with traditional models, many properties of LLMs in time series\nprediction are found. For example, our study shows that LLMs excel in\npredicting time series with clear patterns and trends but face challenges with\ndatasets lacking periodicity. We explain our findings through designing prompts\nto require LLMs to tell the period of the datasets. In addition, the input\nstrategy is investigated, and it is found that incorporating external knowledge\nand adopting natural language paraphrases positively affects the predictive\nperformance of LLMs for time series. Overall, this study contributes to insight\ninto the advantages and limitations of LLMs in time series forecasting under\ndifferent conditions.",
        "pdf_link": "https://arxiv.org/pdf/2402.10835v2.pdf"
    },
    {
        "title": "Wiki-TabNER:Advancing Table Interpretation Through Named Entity Recognition",
        "authors": [
            "Aneta Koleva",
            "Martin Ringsquandl",
            "Ahmed Hatem",
            "Thomas Runkler",
            "Volker Tresp"
        ],
        "published": "2024-03-07T15:22:07Z",
        "summary": "Web tables contain a large amount of valuable knowledge and have inspired\ntabular language models aimed at tackling table interpretation (TI) tasks. In\nthis paper, we analyse a widely used benchmark dataset for evaluation of TI\ntasks, particularly focusing on the entity linking task. Our analysis reveals\nthat this dataset is overly simplified, potentially reducing its effectiveness\nfor thorough evaluation and failing to accurately represent tables as they\nappear in the real-world. To overcome this drawback, we construct and annotate\na new more challenging dataset. In addition to introducing the new dataset, we\nalso introduce a novel problem aimed at addressing the entity linking task:\nnamed entity recognition within cells. Finally, we propose a prompting\nframework for evaluating the newly developed large language models (LLMs) on\nthis novel TI task. We conduct experiments on prompting LLMs under various\nsettings, where we use both random and similarity-based selection to choose the\nexamples presented to the models. Our ablation study helps us gain insights\ninto the impact of the few-shot examples. Additionally, we perform qualitative\nanalysis to gain insights into the challenges encountered by the models and to\nunderstand the limitations of the proposed dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.04577v1.pdf"
    },
    {
        "title": "Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks",
        "authors": [
            "Minju Seo",
            "Jinheon Baek",
            "James Thorne",
            "Sung Ju Hwang"
        ],
        "published": "2024-02-21T02:45:46Z",
        "summary": "Despite large successes of recent language models on diverse tasks, they\nsuffer from severe performance degeneration in low-resource settings with\nlimited training data available. Many existing works tackle this problem by\ngenerating synthetic data from the training data and then training models on\nthem, recently using Large Language Models (LLMs). However, in low-resource\nsettings, the amount of seed data samples to use for data augmentation is very\nsmall, which makes generated samples suboptimal and less diverse. To tackle\nthis challenge, we propose a novel method that augments training data by\nincorporating a wealth of examples from other datasets, along with the given\ntraining data. Specifically, we first retrieve the relevant instances from\nother datasets, such as their input-output pairs or contexts, based on their\nsimilarities with the given seed data, and then prompt LLMs to generate new\nsamples with the contextual information within and across the original and\nretrieved samples. This approach can ensure that the generated data is not only\nrelevant but also more diverse than what could be achieved using the limited\nseed data alone. We validate our proposed Retrieval-Augmented Data Augmentation\n(RADA) framework on multiple datasets under low-resource settings of training\nand test-time data augmentation scenarios, on which it outperforms existing\nLLM-powered data augmentation baselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.13482v1.pdf"
    },
    {
        "title": "Training A Small Emotional Vision Language Model for Visual Art Comprehension",
        "authors": [
            "Jing Zhang",
            "Liang Zheng",
            "Dan Guo",
            "Meng Wang"
        ],
        "published": "2024-03-17T09:01:02Z",
        "summary": "This paper develops small vision language models to understand visual art,\nwhich, given an art work, aims to identify its emotion category and explain\nthis prediction with natural language. While small models are computationally\nefficient, their capacity is much limited compared with large models. To break\nthis trade-off, this paper builds a small emotional vision language model\n(SEVLM) by emotion modeling and input-output feature alignment. On the one\nhand, based on valence-arousal-dominance (VAD) knowledge annotated by\npsychology experts, we introduce and fuse emotional features derived through\nVAD dictionary and a VAD head to align VAD vectors of predicted emotion\nexplanation and the ground truth. This allows the vision language model to\nbetter understand and generate emotional texts, compared with using traditional\ntext embeddings alone. On the other hand, we design a contrastive head to pull\nclose embeddings of the image, its emotion class, and explanation, which aligns\nmodel outputs and inputs. On two public affective explanation datasets, we show\nthat the proposed techniques consistently improve the visual art understanding\nperformance of baseline SEVLMs. Importantly, the proposed model can be trained\nand evaluated on a single RTX 2080 Ti while exhibiting very strong performance:\nit not only outperforms the state-of-the-art small models but is also\ncompetitive compared with LLaVA 7B after fine-tuning and GPT4(V).",
        "pdf_link": "https://arxiv.org/pdf/2403.11150v1.pdf"
    },
    {
        "title": "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection",
        "authors": [
            "Chao Chen",
            "Kai Liu",
            "Ze Chen",
            "Yi Gu",
            "Yue Wu",
            "Mingyuan Tao",
            "Zhihang Fu",
            "Jieping Ye"
        ],
        "published": "2024-02-06T06:23:12Z",
        "summary": "Knowledge hallucination have raised widespread concerns for the security and\nreliability of deployed LLMs. Previous efforts in detecting hallucinations have\nbeen employed at logit-level uncertainty estimation or language-level\nself-consistency evaluation, where the semantic information is inevitably lost\nduring the token-decoding procedure. Thus, we propose to explore the dense\nsemantic information retained within LLMs' \\textbf{IN}ternal \\textbf{S}tates\nfor halluc\\textbf{I}nation \\textbf{DE}tection (\\textbf{INSIDE}). In particular,\na simple yet effective \\textbf{EigenScore} metric is proposed to better\nevaluate responses' self-consistency, which exploits the eigenvalues of\nresponses' covariance matrix to measure the semantic consistency/diversity in\nthe dense embedding space. Furthermore, from the perspective of self-consistent\nhallucination detection, a test time feature clipping approach is explored to\ntruncate extreme activations in the internal states, which reduces\noverconfident generations and potentially benefits the detection of\noverconfident hallucinations. Extensive experiments and ablation studies are\nperformed on several popular LLMs and question-answering (QA) benchmarks,\nshowing the effectiveness of our proposal.",
        "pdf_link": "https://arxiv.org/pdf/2402.03744v1.pdf"
    },
    {
        "title": "Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning",
        "authors": [
            "Nick Mecklenburg",
            "Yiyou Lin",
            "Xiaoxiao Li",
            "Daniel Holstein",
            "Leonardo Nunes",
            "Sara Malvar",
            "Bruno Silva",
            "Ranveer Chandra",
            "Vijay Aski",
            "Pavan Kumar Reddy Yannam",
            "Tolga Aktas",
            "Todd Hendry"
        ],
        "published": "2024-03-30T01:56:07Z",
        "summary": "In recent years, Large Language Models (LLMs) have shown remarkable\nperformance in generating human-like text, proving to be a valuable asset\nacross various applications. However, adapting these models to incorporate new,\nout-of-domain knowledge remains a challenge, particularly for facts and events\nthat occur after the model's knowledge cutoff date. This paper investigates the\neffectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge\ninjection in LLMs, specifically focusing on the domain of recent sporting\nevents. We compare different dataset generation strategies -- token-based and\nfact-based scaling -- to create training data that helps the model learn new\ninformation. Our experiments on GPT-4 demonstrate that while token-based\nscaling can lead to improvements in Q&A accuracy, it may not provide uniform\ncoverage of new knowledge. Fact-based scaling, on the other hand, offers a more\nsystematic approach to ensure even coverage across all facts. We present a\nnovel dataset generation process that leads to more effective knowledge\ningestion through SFT, and our results show considerable performance\nimprovements in Q&A tasks related to out-of-domain knowledge. This study\ncontributes to the understanding of domain adaptation for LLMs and highlights\nthe potential of SFT in enhancing the factuality of LLM responses in specific\nknowledge domains.",
        "pdf_link": "https://arxiv.org/pdf/2404.00213v2.pdf"
    },
    {
        "title": "Do Large Language Model Understand Multi-Intent Spoken Language ?",
        "authors": [
            "Shangjian Yin",
            "Peijie Huang",
            "Yuhong Xu",
            "Haojing Huang",
            "Jiatian Chen"
        ],
        "published": "2024-03-07T13:30:52Z",
        "summary": "This study marks a significant advancement by harnessing Large Language\nModels (LLMs) for multi-intent spoken language understanding (SLU), proposing a\nunique methodology that capitalizes on the generative power of LLMs within an\nSLU context. Our innovative technique reconfigures entity slots specifically\nfor LLM application in multi-intent SLU environments and introduces the concept\nof Sub-Intent Instruction (SII), enhancing the dissection and interpretation of\nintricate, multi-intent communication within varied domains. The resultant\ndatasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing\nbenchmarks. Our research illustrates that LLMs can match and potentially excel\nbeyond the capabilities of current state-of-the-art multi-intent SLU models. It\nfurther explores LLM efficacy across various intent configurations and dataset\nproportions. Moreover, we introduce two pioneering metrics, Entity Slot\nAccuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth\nanalysis of LLM proficiency in this complex field.",
        "pdf_link": "https://arxiv.org/pdf/2403.04481v2.pdf"
    },
    {
        "title": "Predictions from language models for multiple-choice tasks are not robust under variation of scoring methods",
        "authors": [
            "Polina Tsvilodub",
            "Hening Wang",
            "Sharon Grosch",
            "Michael Franke"
        ],
        "published": "2024-03-01T21:48:08Z",
        "summary": "This paper systematically compares different methods of deriving item-level\npredictions of language models for multiple-choice tasks. It compares scoring\nmethods for answer options based on free generation of responses, various\nprobability-based scores, a Likert-scale style rating method, and embedding\nsimilarity. In a case study on pragmatic language interpretation, we find that\nLLM predictions are not robust under variation of method choice, both within a\nsingle LLM and across different LLMs. As this variability entails pronounced\nresearcher degrees of freedom in reporting results, knowledge of the\nvariability is crucial to secure robustness of results and research integrity.",
        "pdf_link": "https://arxiv.org/pdf/2403.00998v1.pdf"
    },
    {
        "title": "SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models",
        "authors": [
            "Yu Yang",
            "Siddhartha Mishra",
            "Jeffrey N Chiang",
            "Baharan Mirzasoleiman"
        ],
        "published": "2024-03-12T07:45:33Z",
        "summary": "Despite the effectiveness of data selection for large language models (LLMs)\nduring pretraining and instruction fine-tuning phases, improving data\nefficiency in supervised fine-tuning (SFT) for specialized domains poses\nsignificant challenges due to the complexity of fine-tuning data. To bridge\nthis gap, we introduce an effective and scalable data selection method for SFT,\nSmallToLarge (S2L), which leverages training trajectories from small models to\nguide the data selection for larger models. We demonstrate through extensive\nexperiments that S2L significantly improves data efficiency in SFT for\nmathematical problem-solving, reducing the training data to just 11% of the\noriginal MathInstruct dataset (Yue et al., 2023) to match full dataset\nperformance while outperforming state-of-the-art data selection algorithms by\nan average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,\nselecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most\nchallenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et\nal., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset\n(Johnson et al., 2016), S2L again outperforms training on the full dataset\nusing only 50% of the data. Notably, S2L can perform data selection using a\nreference model 40x smaller than the target model, proportionally reducing the\ncost of data selection.",
        "pdf_link": "https://arxiv.org/pdf/2403.07384v1.pdf"
    },
    {
        "title": "Evaluating the Performance of ChatGPT for Spam Email Detection",
        "authors": [
            "Yuwei Wu",
            "Shijing Si",
            "Yugui Zhang",
            "Jiawen Gu",
            "Jedrek Wosik"
        ],
        "published": "2024-02-23T04:52:08Z",
        "summary": "Email continues to be a pivotal and extensively utilized communication medium\nwithin professional and commercial domains. Nonetheless, the prevalence of spam\nemails poses a significant challenge for users, disrupting their daily routines\nand diminishing productivity. Consequently, accurately identifying and\nfiltering spam based on content has become crucial for cybersecurity. Recent\nadvancements in natural language processing, particularly with large language\nmodels like ChatGPT, have shown remarkable performance in tasks such as\nquestion answering and text generation. However, its potential in spam\nidentification remains underexplored. To fill in the gap, this study attempts\nto evaluate ChatGPT's capabilities for spam identification in both English and\nChinese email datasets. We employ ChatGPT for spam email detection using\nin-context learning, which requires a prompt instruction and a few\ndemonstrations. We also investigate how the training example size affects the\nperformance of ChatGPT. For comparison, we also implement five popular\nbenchmark methods, including naive Bayes, support vector machines (SVM),\nlogistic regression (LR), feedforward dense neural networks (DNN), and BERT\nclassifiers. Though extensive experiments, the performance of ChatGPT is\nsignificantly worse than deep supervised learning methods in the large English\ndataset, while it presents superior performance on the low-resourced Chinese\ndataset, even outperforming BERT in this case.",
        "pdf_link": "https://arxiv.org/pdf/2402.15537v1.pdf"
    },
    {
        "title": "GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model",
        "authors": [
            "Jianghui Zhou",
            "Ya Gao",
            "Jie Liu",
            "Xuemin Zhao",
            "Zhaohua Yang",
            "Yue Wu",
            "Lirong Shi"
        ],
        "published": "2024-02-21T09:59:20Z",
        "summary": "Large language models(LLM) such as ChatGPT have substantially simplified the\ngeneration of marketing copy, yet producing content satisfying domain specific\nrequirements, such as effectively engaging customers, remains a significant\nchallenge. In this work, we introduce the Genetic Copy Optimization Framework\n(GCOF) designed to enhance both efficiency and engagememnt of marketing copy\ncreation. We conduct explicit feature engineering within the prompts of LLM.\nAdditionally, we modify the crossover operator in Genetic Algorithm (GA),\nintegrating it into the GCOF to enable automatic feature engineering. This\nintegration facilitates a self-iterative refinement of the marketing copy.\nCompared to human curated copy, Online results indicate that copy produced by\nour framework achieves an average increase in click-through rate (CTR) of over\n$50\\%$.",
        "pdf_link": "https://arxiv.org/pdf/2402.13667v1.pdf"
    },
    {
        "title": "Decoding Speculative Decoding",
        "authors": [
            "Minghao Yan",
            "Saurabh Agarwal",
            "Shivaram Venkataraman"
        ],
        "published": "2024-02-02T16:15:24Z",
        "summary": "Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without modifying its outcome. When performing\ninference on an LLM, speculative decoding uses a smaller draft model which\ngenerates speculative tokens and then uses the target LLM to verify those draft\ntokens. The speedup provided by speculative decoding heavily depends on the\nchoice of the draft model. It has been widely suggested to select a draft model\nthat provides a high probability of the generated token being accepted by the\nLLM to achieve the highest throughput. However, our experiments indicate the\ncontrary with throughput diminishing as the probability of generated tokens to\nbe accepted by the target model increases. To understand this phenomenon, we\nperform extensive experiments to characterize the different factors that affect\nspeculative decoding and how those factors interact and affect the speedups.\nBased on our experiments we describe an analytical model which can be used to\ndecide the right draft model for a given workload. Further, using our insights\nwe design a new draft model for LLaMA-65B which can provide 30% higher\nthroughput than existing draft models.",
        "pdf_link": "https://arxiv.org/pdf/2402.01528v1.pdf"
    },
    {
        "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
        "authors": [
            "Chaojun Xiao",
            "Pengle Zhang",
            "Xu Han",
            "Guangxuan Xiao",
            "Yankai Lin",
            "Zhengyan Zhang",
            "Zhiyuan Liu",
            "Song Han",
            "Maosong Sun"
        ],
        "published": "2024-02-07T06:50:42Z",
        "summary": "Large language models (LLMs) have emerged as a cornerstone in real-world\napplications with lengthy streaming inputs, such as LLM-driven agents. However,\nexisting LLMs, pre-trained on sequences with restricted maximum length, cannot\ngeneralize to longer sequences due to the out-of-domain and distraction issues.\nTo alleviate these issues, existing efforts employ sliding attention windows\nand discard distant tokens to achieve the processing of extremely long\nsequences. Unfortunately, these approaches inevitably fail to capture\nlong-distance dependencies within sequences to deeply understand semantics.\nThis paper introduces a training-free memory-based method, InfLLM, to unveil\nthe intrinsic ability of LLMs to process streaming long sequences.\nSpecifically, InfLLM stores distant contexts into additional memory units and\nemploys an efficient mechanism to lookup token-relevant units for attention\ncomputation. Thereby, InfLLM allows LLMs to efficiently process long sequences\nwhile maintaining the ability to capture long-distance dependencies. Without\nany training, InfLLM enables LLMs pre-trained on sequences of a few thousand\ntokens to achieve superior performance than competitive baselines continually\ntraining these LLMs on long sequences. Even when the sequence length is scaled\nto $1,024$K, InfLLM still effectively captures long-distance dependencies.",
        "pdf_link": "https://arxiv.org/pdf/2402.04617v1.pdf"
    },
    {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "authors": [
            "Xiangming Gu",
            "Xiaosen Zheng",
            "Tianyu Pang",
            "Chao Du",
            "Qian Liu",
            "Ye Wang",
            "Jing Jiang",
            "Min Lin"
        ],
        "published": "2024-02-13T16:06:17Z",
        "summary": "A multimodal large language model (MLLM) agent can receive instructions,\ncapture images, retrieve histories from memory, and decide which tools to use.\nNonetheless, red-teaming efforts have revealed that adversarial images/prompts\ncan jailbreak an MLLM and cause unaligned behaviors. In this work, we report an\neven more severe safety issue in multi-agent environments, referred to as\ninfectious jailbreak. It entails the adversary simply jailbreaking a single\nagent, and without any further intervention from the adversary, (almost) all\nagents will become infected exponentially fast and exhibit harmful behaviors.\nTo validate the feasibility of infectious jailbreak, we simulate multi-agent\nenvironments containing up to one million LLaVA-1.5 agents, and employ\nrandomized pair-wise chat as a proof-of-concept instantiation for multi-agent\ninteraction. Our results show that feeding an (infectious) adversarial image\ninto the memory of any randomly chosen agent is sufficient to achieve\ninfectious jailbreak. Finally, we derive a simple principle for determining\nwhether a defense mechanism can provably restrain the spread of infectious\njailbreak, but how to design a practical defense that meets this principle\nremains an open question to investigate. Our project page is available at\nhttps://sail-sg.github.io/Agent-Smith/.",
        "pdf_link": "https://arxiv.org/pdf/2402.08567v1.pdf"
    },
    {
        "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
        "authors": [
            "Yanfang Zhang",
            "Yiliu Sun",
            "Yibing Zhan",
            "Dapeng Tao",
            "Dacheng Tao",
            "Chen Gong"
        ],
        "published": "2024-02-06T03:41:12Z",
        "summary": "Recently, increasing attention has been focused drawn on to improve the\nability of Large Language Models (LLMs) to perform complex reasoning. However,\nprevious methods, such as Chain-of-Thought and Self-Consistency, mainly follow\nDirect Reasoning (DR) frameworks, so they will meet difficulty in solving\nnumerous real-world tasks which can hardly be solved via DR. Therefore, to\nstrengthen the reasoning power of LLMs, this paper proposes a novel Indirect\nReasoning (IR) method that employs the logic of contrapositives and\ncontradictions to tackle IR tasks such as factual reasoning and mathematic\nproof. Specifically, our methodology comprises two steps. Firstly, we leverage\nthe logical equivalence of contrapositive to augment the data and rules to\nenhance the comprehensibility of LLMs. Secondly, we design a set of prompt\ntemplates to trigger LLMs to conduct IR based on proof by contradiction that is\nlogically equivalent to the original DR process. Our IR method is simple yet\neffective and can be straightforwardly integrated with existing DR methods to\nfurther boost the reasoning abilities of LLMs. The experimental results on\npopular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method\nenhances the overall accuracy of factual reasoning by 27.33% and mathematical\nproof by 31.43%, when compared with traditional DR methods. Moreover, the\nmethods combining IR and DR significantly outperform the methods solely using\nIR or DR, further demonstrating the effectiveness of our strategy.",
        "pdf_link": "https://arxiv.org/pdf/2402.03667v1.pdf"
    },
    {
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "authors": [
            "Pratyush Maini",
            "Zhili Feng",
            "Avi Schwarzschild",
            "Zachary C. Lipton",
            "J. Zico Kolter"
        ],
        "published": "2024-01-11T18:57:12Z",
        "summary": "Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.",
        "pdf_link": "https://arxiv.org/pdf/2401.06121v1.pdf"
    },
    {
        "title": "Large Language Model Agent for Hyper-Parameter Optimization",
        "authors": [
            "Siyi Liu",
            "Chen Gao",
            "Yong Li"
        ],
        "published": "2024-02-02T20:12:05Z",
        "summary": "Hyperparameter optimization is critical in modern machine learning, requiring\nexpert knowledge, numerous trials, and high computational and human resources.\nDespite the advancements in Automated Machine Learning (AutoML), challenges in\nterms of trial efficiency, setup complexity, and interoperability still\npersist. To address these issues, we introduce a novel paradigm leveraging\nLarge Language Models (LLMs) to automate hyperparameter optimization across\ndiverse machine learning tasks, which is named AgentHPO (short for LLM\nAgent-based Hyperparameter Optimization). Specifically, AgentHPO processes the\ntask information autonomously, conducts experiments with specific\nhyperparameters (HPs), and iteratively optimizes them based on historical\ntrials. This human-like optimization process largely reduces the number of\nrequired trials, simplifies the setup process, and enhances interpretability\nand user trust, compared to traditional AutoML methods. Extensive empirical\nexperiments conducted on 12 representative machine-learning tasks indicate that\nAgentHPO not only matches but also often surpasses the best human trials in\nterms of performance while simultaneously providing explainable results.\nFurther analysis sheds light on the strategies employed by the LLM in\noptimizing these tasks, highlighting its effectiveness and adaptability in\nvarious scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.01881v2.pdf"
    },
    {
        "title": "CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models",
        "authors": [
            "Fuwen Luo",
            "Chi Chen",
            "Zihao Wan",
            "Zhaolu Kang",
            "Qidong Yan",
            "Yingjie Li",
            "Xiaolong Wang",
            "Siyu Wang",
            "Ziyue Wang",
            "Xiaoyue Mi",
            "Peng Li",
            "Ning Ma",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-21T08:21:12Z",
        "summary": "Multimodal large language models (MLLMs) have demonstrated promising results\nin a variety of tasks that combine vision and language. As these models become\nmore integral to research and applications, conducting comprehensive\nevaluations of their capabilities has grown increasingly important. However,\nmost existing benchmarks fail to consider that, in certain situations, images\nneed to be interpreted within a broader context. In this work, we introduce a\nnew benchmark, named as CODIS, designed to assess the ability of models to use\ncontext provided in free-form text to enhance visual comprehension. Our\nfindings indicate that MLLMs consistently fall short of human performance on\nthis benchmark. Further analysis confirms that these models struggle to\neffectively extract and utilize contextual information to improve their\nunderstanding of images. This underscores the pressing need to enhance the\nability of MLLMs to comprehend visuals in a context-dependent manner. View our\nproject website at https://thunlp-mt.github.io/CODIS.",
        "pdf_link": "https://arxiv.org/pdf/2402.13607v2.pdf"
    },
    {
        "title": "L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ",
        "authors": [
            "Hyesung Jeon",
            "Yulhwa Kim",
            "Jae-joon Kim"
        ],
        "published": "2024-02-07T14:35:05Z",
        "summary": "Post-training quantization (PTQ) and quantization-aware training (QAT)\nmethods are gaining popularity in mitigating the high memory and computational\ncosts associated with Large Language Models (LLMs). In resource-constrained\nscenarios, PTQ, with its reduced training overhead, is often preferred over\nQAT, despite the latter's potential for higher accuracy. Meanwhile,\nparameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA)\nhave been introduced, and recent efforts have explored quantization-aware PEFT\ntechniques. However, these approaches may lack generality due to their reliance\non the pre-quantized model's configuration. Their effectiveness may be\ncompromised by non-linearly quantized or mixed-precision weights, and the\nretraining of specific quantization parameters might impede optimal\nperformance. To address these challenges, we propose L4Q, an algorithm for\nparameter-efficient quantization-aware training. L4Q leverages LoRA-wise\nlearned quantization step size for LLMs, aiming to enhance generality. The\nsimultaneous quantization-and-fine-tuning process of L4Q is applicable to\nhigh-precision models, yielding linearly quantized weights with superior\naccuracy. Our experiments, conducted on the LLaMA and LLaMA2 model families\nusing an instructional dataset, showcase L4Q's capabilities in language\ncomprehension and few-shot in-context learning, achieving sub-4-bit precision\nwhile maintaining comparable training times to applying PEFT on a quantized\nmodel.",
        "pdf_link": "https://arxiv.org/pdf/2402.04902v2.pdf"
    },
    {
        "title": "An Exploratory Study on Automatic Identification of Assumptions in the Development of Deep Learning Frameworks",
        "authors": [
            "Chen Yang",
            "Peng Liang",
            "Zinan Ma"
        ],
        "published": "2024-01-08T03:50:03Z",
        "summary": "Stakeholders constantly make assumptions in the development of deep learning\n(DL) frameworks. These assumptions are related to various types of software\nartifacts (e.g., requirements, design decisions, and technical debt) and can\nturn out to be invalid, leading to system failures. Existing approaches and\ntools for assumption management usually depend on manual identification of\nassumptions. However, assumptions are scattered in various sources (e.g., code\ncomments, commits, pull requests, and issues) of DL framework development, and\nmanually identifying assumptions has high costs (e.g., time and resources). To\novercome the issues of manually identifying assumptions in DL framework\ndevelopment, we constructed a new and largest dataset (i.e., AssuEval) of\nassumptions collected from the TensorFlow and Keras repositories on GitHub;\nexplored the performance of seven traditional machine learning models (e.g.,\nSupport Vector Machine, Classification and Regression Trees), a popular DL\nmodel (i.e., ALBERT), and a large language model (i.e., ChatGPT) of identifying\nassumptions on the AssuEval dataset. The experiment results show that: ALBERT\nachieves the best performance (f1-score: 0.9584) of identifying assumptions on\nthe AssuEval dataset, which is much better than the other models (the 2nd best\nf1-score is 0.6211, achieved by ChatGPT). Though ChatGPT is the most popular\nlarge language model, we do not recommend using it to identify assumptions in\nDL framework development because of its low performance on the task.\nFine-tuning ChatGPT specifically for assumption identification could improve\nthe performance. This study provides researchers with the largest dataset of\nassumptions for further research (e.g., assumption classification, evaluation,\nand reasoning) and helps practitioners better understand assumptions and how to\nmanage them in their projects.",
        "pdf_link": "https://arxiv.org/pdf/2401.03653v3.pdf"
    },
    {
        "title": "InFoBench: Evaluating Instruction Following Ability in Large Language Models",
        "authors": [
            "Yiwei Qin",
            "Kaiqiang Song",
            "Yebowen Hu",
            "Wenlin Yao",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Xuansheng Wu",
            "Fei Liu",
            "Pengfei Liu",
            "Dong Yu"
        ],
        "published": "2024-01-07T23:01:56Z",
        "summary": "This paper introduces the Decomposed Requirements Following Ratio (DRFR), a\nnew metric for evaluating Large Language Models' (LLMs) ability to follow\ninstructions. Addressing a gap in current methodologies, DRFR breaks down\ncomplex instructions into simpler criteria, facilitating a detailed analysis of\nLLMs' compliance with various aspects of tasks. Alongside this metric, we\npresent InFoBench, a benchmark comprising 500 diverse instructions and 2,250\ndecomposed questions across multiple constraint categories. Our experiments\ncompare DRFR with traditional scoring methods and explore annotation sources,\nincluding human experts, crowd-sourced workers, and GPT-4. The findings\ndemonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a\ncost-efficient annotator. The evaluation of several advanced LLMs using this\nframework reveals their strengths and areas needing improvement, particularly\nin complex instruction-following. This study contributes a novel metric and\nbenchmark, offering insights for future LLM development and evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2401.03601v1.pdf"
    },
    {
        "title": "DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation",
        "authors": [
            "Sunghyeon Woo",
            "Baeseong Park",
            "Byeongwook Kim",
            "Minjung Jo",
            "Sejung Kwon",
            "Dongsuk Jeon",
            "Dongsoo Lee"
        ],
        "published": "2024-02-27T14:51:11Z",
        "summary": "Training deep neural networks typically involves substantial computational\ncosts during both forward and backward propagation. The conventional layer\ndropping techniques drop certain layers during training for reducing the\ncomputations burden. However, dropping layers during forward propagation\nadversely affects the training process by degrading accuracy. In this paper, we\npropose Dropping Backward Propagation (DropBP), a novel approach designed to\nreduce computational costs while maintaining accuracy. DropBP randomly drops\nlayers during the backward propagation, which does not deviate forward\npropagation. Moreover, DropBP calculates the sensitivity of each layer to\nassign appropriate drop rate, thereby stabilizing the training process. DropBP\nis designed to enhance the efficiency of the training process with\nbackpropagation, thereby enabling the acceleration of both full fine-tuning and\nparameter-efficient fine-tuning using backpropagation. Specifically, utilizing\nDropBP in QLoRA reduces training time by 44%, increases the convergence speed\nto the identical loss level by 1.5$\\times$, and enables training with a\n6.2$\\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in\nLLaMA2-70B. The code is available at https://github.com/WooSunghyeon/dropbp.",
        "pdf_link": "https://arxiv.org/pdf/2402.17812v1.pdf"
    },
    {
        "title": "AI Revolution on Chat Bot: Evidence from a Randomized Controlled Experiment",
        "authors": [
            "Sida Peng",
            "Wojciech Swiatek",
            "Allen Gao",
            "Paul Cullivan",
            "Haoge Chang"
        ],
        "published": "2024-01-19T05:54:35Z",
        "summary": "In recent years, generative AI has undergone major advancements,\ndemonstrating significant promise in augmenting human productivity. Notably,\nlarge language models (LLM), with ChatGPT-4 as an example, have drawn\nconsiderable attention. Numerous articles have examined the impact of LLM-based\ntools on human productivity in lab settings and designed tasks or in\nobservational studies. Despite recent advances, field experiments applying\nLLM-based tools in realistic settings are limited. This paper presents the\nfindings of a field randomized controlled trial assessing the effectiveness of\nLLM-based tools in providing unmonitored support services for information\nretrieval.",
        "pdf_link": "https://arxiv.org/pdf/2401.10956v1.pdf"
    },
    {
        "title": "Towards Conversational Diagnostic AI",
        "authors": [
            "Tao Tu",
            "Anil Palepu",
            "Mike Schaekermann",
            "Khaled Saab",
            "Jan Freyberg",
            "Ryutaro Tanno",
            "Amy Wang",
            "Brenna Li",
            "Mohamed Amin",
            "Nenad Tomasev",
            "Shekoofeh Azizi",
            "Karan Singhal",
            "Yong Cheng",
            "Le Hou",
            "Albert Webson",
            "Kavita Kulkarni",
            "S Sara Mahdavi",
            "Christopher Semturs",
            "Juraj Gottweis",
            "Joelle Barral",
            "Katherine Chou",
            "Greg S Corrado",
            "Yossi Matias",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "published": "2024-01-11T04:25:06Z",
        "summary": "At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.",
        "pdf_link": "https://arxiv.org/pdf/2401.05654v1.pdf"
    },
    {
        "title": "Extending LLMs' Context Window with 100 Samples",
        "authors": [
            "Yikai Zhang",
            "Junlong Li",
            "Pengfei Liu"
        ],
        "published": "2024-01-13T07:57:01Z",
        "summary": "Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.",
        "pdf_link": "https://arxiv.org/pdf/2401.07004v1.pdf"
    },
    {
        "title": "Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources",
        "authors": [
            "Jiamu Bai",
            "Daoyuan Chen",
            "Bingchen Qian",
            "Liuyi Yao",
            "Yaliang Li"
        ],
        "published": "2024-02-18T08:32:59Z",
        "summary": "Federated Learning (FL) has recently been applied to the parameter-efficient\nfine-tuning of Large Language Models (LLMs). While promising, it raises\nsignificant challenges due to the heterogeneous resources and data\ndistributions of clients.This study introduces FlexLoRA, a simple yet effective\naggregation scheme for LLM fine-tuning, which mitigates the \"buckets effect\" in\ntraditional FL that restricts the potential of clients with ample resources by\ntying them to the capabilities of the least-resourced participants. FlexLoRA\nallows for dynamic adjustment of local LoRA ranks, fostering the development of\na global model imbued with broader, less task-specific knowledge. By\nsynthesizing a full-size LoRA weight from individual client contributions and\nemploying Singular Value Decomposition (SVD) for weight redistribution,\nFlexLoRA fully leverages heterogeneous client resources. Involving over 1,600\nclients performing diverse NLP tasks, our experiments validate the efficacy of\nFlexLoRA, with the federated global model achieving up to a 3.1% average\nimprovement in downstream NLP task performance. FlexLoRA's practicality is\nfurther underscored by its seamless integration with existing LoRA-based FL\nmethods and theoretical analysis, offering a path toward scalable,\nprivacy-preserving federated tuning for LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11505v1.pdf"
    },
    {
        "title": "EAGLE: A Domain Generalization Framework for AI-generated Text Detection",
        "authors": [
            "Amrita Bhattacharjee",
            "Raha Moraffah",
            "Joshua Garland",
            "Huan Liu"
        ],
        "published": "2024-03-23T02:44:20Z",
        "summary": "With the advancement in capabilities of Large Language Models (LLMs), one\nmajor step in the responsible and safe use of such LLMs is to be able to detect\ntext generated by these models. While supervised AI-generated text detectors\nperform well on text generated by older LLMs, with the frequent release of new\nLLMs, building supervised detectors for identifying text from such new models\nwould require new labeled training data, which is infeasible in practice. In\nthis work, we tackle this problem and propose a domain generalization framework\nfor the detection of AI-generated text from unseen target generators. Our\nproposed framework, EAGLE, leverages the labeled data that is available so far\nfrom older language models and learns features invariant across these\ngenerators, in order to detect text generated by an unknown target generator.\nEAGLE learns such domain-invariant features by combining the representational\npower of self-supervised contrastive learning with domain adversarial training.\nThrough our experiments we demonstrate how EAGLE effectively achieves\nimpressive performance in detecting text generated by unseen target generators,\nincluding recent state-of-the-art ones such as GPT-4 and Claude, reaching\ndetection scores of within 4.7% of a fully supervised detector.",
        "pdf_link": "https://arxiv.org/pdf/2403.15690v1.pdf"
    },
    {
        "title": "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning",
        "authors": [
            "Zicheng Lin",
            "Zhibin Gou",
            "Tian Liang",
            "Ruilin Luo",
            "Haowei Liu",
            "Yujiu Yang"
        ],
        "published": "2024-02-22T18:59:02Z",
        "summary": "The ability of Large Language Models (LLMs) to critique and refine their\nreasoning is crucial for their application in evaluation, feedback provision,\nand self-improvement. This paper introduces CriticBench, a comprehensive\nbenchmark designed to assess LLMs' abilities to critique and rectify their\nreasoning across a variety of tasks. CriticBench encompasses five reasoning\ndomains: mathematical, commonsense, symbolic, coding, and algorithmic. It\ncompiles 15 datasets and incorporates responses from three LLM families.\nUtilizing CriticBench, we evaluate and dissect the performance of 17 LLMs in\ngeneration, critique, and correction reasoning, i.e., GQC reasoning. Our\nfindings reveal: (1) a linear relationship in GQC capabilities, with\ncritique-focused training markedly enhancing performance; (2) a task-dependent\nvariation in correction effectiveness, with logic-oriented tasks being more\namenable to correction; (3) GQC knowledge inconsistencies that decrease as\nmodel size increases; and (4) an intriguing inter-model critiquing dynamic,\nwhere stronger models are better at critiquing weaker ones, while weaker models\ncan surprisingly surpass stronger ones in their self-critique. We hope these\ninsights into the nuanced critique-correct reasoning of LLMs will foster\nfurther research in LLM critique and self-improvement.",
        "pdf_link": "https://arxiv.org/pdf/2402.14809v2.pdf"
    },
    {
        "title": "Zero-shot cross-lingual transfer in instruction tuning of large language model",
        "authors": [
            "Nadezhda Chirkova",
            "Vassilina Nikoulina"
        ],
        "published": "2024-02-22T18:37:33Z",
        "summary": "Instruction tuning (IT) is widely used to teach pretrained large language\nmodels (LLMs) to follow arbitrary instructions, but is under-studied in\nmultilingual settings. In this work, we conduct a systematic study of zero-shot\ncross-lingual transfer in IT, when an LLM is instruction-tuned on English-only\ndata and then tested on user prompts in other languages. We investigate the\ninfluence of model configuration choices and devise a multi-facet evaluation\nstrategy for multilingual instruction following. We find that cross-lingual\ntransfer does happen successfully in IT even if all stages of model training\nare English-centric, but only if multiliguality is taken into account in\nhyperparameter tuning and with large enough IT data. English-trained LLMs are\ncapable of generating correct-language, comprehensive and helpful responses in\nthe other languages, but suffer from low factuality and may occasionally have\nfluency errors.",
        "pdf_link": "https://arxiv.org/pdf/2402.14778v1.pdf"
    },
    {
        "title": "Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment",
        "authors": [
            "Congzhi Zhang",
            "Linhai Zhang",
            "Deyu Zhou",
            "Guoqiang Xu"
        ],
        "published": "2024-03-05T07:47:34Z",
        "summary": "Despite the significant achievements of existing prompting methods such as\nin-context learning and chain-of-thought for large language models (LLMs), they\nstill face challenges of various biases. Traditional debiasing methods\nprimarily focus on the model training stage, including data augmentation-based\nand reweight-based approaches, with the limitations of addressing the complex\nbiases of LLMs. To address such limitations, the causal relationship behind the\nprompting methods is uncovered using a structural causal model, and a novel\ncausal prompting method based on front-door adjustment is proposed to\neffectively mitigate the bias of LLMs. In specific, causal intervention is\nimplemented by designing the prompts without accessing the parameters and\nlogits of LLMs.The chain-of-thoughts generated by LLMs are employed as the\nmediator variable and the causal effect between the input prompt and the output\nanswers is calculated through front-door adjustment to mitigate model biases.\nMoreover, to obtain the representation of the samples precisely and estimate\nthe causal effect more accurately, contrastive learning is used to fine-tune\nthe encoder of the samples by aligning the space of the encoder with the LLM.\nExperimental results show that the proposed causal prompting approach achieves\nexcellent performance on 3 natural language processing datasets on both\nopen-source and closed-source LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.02738v1.pdf"
    },
    {
        "title": "Whispers in the Machine: Confidentiality in LLM-integrated Systems",
        "authors": [
            "Jonathan Evertz",
            "Merlin Chlosta",
            "Lea Sch\u00f6nherr",
            "Thorsten Eisenhofer"
        ],
        "published": "2024-02-10T11:07:24Z",
        "summary": "Large Language Models (LLMs) are increasingly integrated with external tools.\nWhile these integrations can significantly improve the functionality of LLMs,\nthey also create a new attack surface where confidential data may be disclosed\nbetween different components. Specifically, malicious tools can exploit\nvulnerabilities in the LLM itself to manipulate the model and compromise the\ndata of other services, raising the question of how private data can be\nprotected in the context of LLM integrations.\n  In this work, we provide a systematic way of evaluating confidentiality in\nLLM-integrated systems. For this, we formalize a \"secret key\" game that can\ncapture the ability of a model to conceal private information. This enables us\nto compare the vulnerability of a model against confidentiality attacks and\nalso the effectiveness of different defense strategies. In this framework, we\nevaluate eight previously published attacks and four defenses. We find that\ncurrent defenses lack generalization across attack strategies. Building on this\nanalysis, we propose a method for robustness fine-tuning, inspired by\nadversarial training. This approach is effective in lowering the success rate\nof attackers and in improving the system's resilience against unknown attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.06922v1.pdf"
    },
    {
        "title": "Efficient Prompt Caching via Embedding Similarity",
        "authors": [
            "Hanlin Zhu",
            "Banghua Zhu",
            "Jiantao Jiao"
        ],
        "published": "2024-02-02T06:34:11Z",
        "summary": "Large language models (LLMs) have achieved huge success in numerous natural\nlanguage process (NLP) tasks. However, it faces the challenge of significant\nresource consumption during inference. In this paper, we aim to improve the\ninference efficiency of LLMs by prompt caching, i.e., if the current prompt can\nbe answered by the same response of a previous prompt, one can directly utilize\nthat previous response without calling the LLM. Specifically, we focus on the\nprediction accuracy of prompt caching for single-round question-answering tasks\nvia embedding similarity. The existing embeddings of prompts mostly focus on\nwhether two prompts are semantically similar, which is not necessarily\nequivalent to whether the same response can answer them. Therefore, we propose\na distillation-based method to fine-tune the existing embeddings for better\ncaching prediction. Theoretically, we provide finite-sample guarantees for the\nconvergence of our method under different types of loss functions. Empirically,\nwe carefully construct a hard dataset based on Kwiatkowski et al. (2019) where\nthe existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.\nWe then fine-tune the above embedding model, which significantly improves the\nAUC of caching prediction from 0.51 to 0.81. We also conduct simulations\ndemonstrating that our trained models achieve better caching efficiency than\nthe previous embedding model.",
        "pdf_link": "https://arxiv.org/pdf/2402.01173v1.pdf"
    },
    {
        "title": "IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition",
        "authors": [
            "Zikang Leng",
            "Amitrajit Bhattacharjee",
            "Hrudhai Rajasekhar",
            "Lizhe Zhang",
            "Elizabeth Bruda",
            "Hyeokhyen Kwon",
            "Thomas Pl\u00f6tz"
        ],
        "published": "2024-02-01T22:37:33Z",
        "summary": "One of the primary challenges in the field of human activity recognition\n(HAR) is the lack of large labeled datasets. This hinders the development of\nrobust and generalizable models. Recently, cross modality transfer approaches\nhave been explored that can alleviate the problem of data scarcity. These\napproaches convert existing datasets from a source modality, such as video, to\na target modality (IMU). With the emergence of generative AI models such as\nlarge language models (LLMs) and text-driven motion synthesis models, language\nhas become a promising source data modality as well as shown in proof of\nconcepts such as IMUGPT. In this work, we conduct a large-scale evaluation of\nlanguage-based cross modality transfer to determine their effectiveness for\nHAR. Based on this study, we introduce two new extensions for IMUGPT that\nenhance its use for practical HAR application scenarios: a motion filter\ncapable of filtering out irrelevant motion sequences to ensure the relevance of\nthe generated virtual IMU data, and a set of metrics that measure the diversity\nof the generated data facilitating the determination of when to stop generating\nvirtual IMU data for both effective and efficient processing. We demonstrate\nthat our diversity metrics can reduce the effort needed for the generation of\nvirtual IMU data by at least 50%, which open up IMUGPT for practical use cases\nbeyond a mere proof of concept.",
        "pdf_link": "https://arxiv.org/pdf/2402.01049v1.pdf"
    },
    {
        "title": "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models",
        "authors": [
            "Minsuk Kahng",
            "Ian Tenney",
            "Mahima Pushkarna",
            "Michael Xieyang Liu",
            "James Wexler",
            "Emily Reif",
            "Krystal Kallarackal",
            "Minsuk Chang",
            "Michael Terry",
            "Lucas Dixon"
        ],
        "published": "2024-02-16T09:14:49Z",
        "summary": "Automatic side-by-side evaluation has emerged as a promising approach to\nevaluating the quality of responses from large language models (LLMs). However,\nanalyzing the results from this evaluation approach raises scalability and\ninterpretability challenges. In this paper, we present LLM Comparator, a novel\nvisual analytics tool for interactively analyzing results from automatic\nside-by-side evaluation. The tool supports interactive workflows for users to\nunderstand when and why a model performs better or worse than a baseline model,\nand how the responses from two models are qualitatively different. We\niteratively designed and developed the tool by closely working with researchers\nand engineers at a large technology company. This paper details the user\nchallenges we identified, the design and development of the tool, and an\nobservational study with participants who regularly evaluate their models.",
        "pdf_link": "https://arxiv.org/pdf/2402.10524v1.pdf"
    },
    {
        "title": "ChatGPT Alternative Solutions: Large Language Models Survey",
        "authors": [
            "Hanieh Alipour",
            "Nick Pendar",
            "Kohinoor Roy"
        ],
        "published": "2024-03-21T15:16:50Z",
        "summary": "In recent times, the grandeur of Large Language Models (LLMs) has not only\nshone in the realm of natural language processing but has also cast its\nbrilliance across a vast array of applications. This remarkable display of LLM\ncapabilities has ignited a surge in research contributions within this domain,\nspanning a diverse spectrum of topics. These contributions encompass\nadvancements in neural network architecture, context length enhancements, model\nalignment, training datasets, benchmarking, efficiency improvements, and more.\nRecent years have witnessed a dynamic synergy between academia and industry,\npropelling the field of LLM research to new heights. A notable milestone in\nthis journey is the introduction of ChatGPT, a powerful AI chatbot grounded in\nLLMs, which has garnered widespread societal attention. The evolving technology\nof LLMs has begun to reshape the landscape of the entire AI community,\npromising a revolutionary shift in the way we create and employ AI algorithms.\nGiven this swift-paced technical evolution, our survey embarks on a journey to\nencapsulate the recent strides made in the world of LLMs. Through an\nexploration of the background, key discoveries, and prevailing methodologies,\nwe offer an up-to-the-minute review of the literature. By examining multiple\nLLM models, our paper not only presents a comprehensive overview but also\ncharts a course that identifies existing challenges and points toward potential\nfuture research trajectories. This survey furnishes a well-rounded perspective\non the current state of generative AI, shedding light on opportunities for\nfurther exploration, enhancement, and innovation.",
        "pdf_link": "https://arxiv.org/pdf/2403.14469v1.pdf"
    },
    {
        "title": "MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline",
        "authors": [
            "Minpeng Liao",
            "Wei Luo",
            "Chengxi Li",
            "Jing Wu",
            "Kai Fan"
        ],
        "published": "2024-01-16T08:08:01Z",
        "summary": "Large language models (LLMs) have seen considerable advancements in natural\nlanguage understanding tasks, yet there remains a gap to bridge before\nattaining true artificial general intelligence, especially concerning\nshortcomings in mathematical reasoning capabilities. We postulate that the\ninherent nature of LLM training, which focuses on predicting probabilities of\nnext token, presents challenges in effectively modeling mathematical reasoning\nthat demands exact calculations, both from data-driven and theoretical\nstandpoints. In this paper, we address this challenge by enriching the data\nlandscape and introducing a novel math dataset, enhanced with a capability to\nutilize a Python code interpreter. This dataset is derived from GSM8K and MATH\nand has been further refined through a combination of GPT-4 annotations, human\nreview, and self-training processes, where the errors in the original GSM8K\ntraining set have been fixed. Additionally, we propose a tentative, easily\nreplicable protocol for the fine-tuning of math-specific LLMs, which has led to\na significant improvement in the performance of a 7B-parameter LLM on the GSM8K\nand MATH datasets. We are committed to advancing the field of mathematical\nreasoning in LLMs and, to that end, we have made source code for data\ngeneration / training / inference, and the model checkpoints publicly available\nat \\url{https://github.com/MARIO-Math-Reasoning/MARIO}. We hope this will\nfacilitate further research and development within the community.",
        "pdf_link": "https://arxiv.org/pdf/2401.08190v3.pdf"
    },
    {
        "title": "Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization",
        "authors": [
            "Renjie Pi",
            "Tianyang Han",
            "Wei Xiong",
            "Jipeng Zhang",
            "Runtao Liu",
            "Rui Pan",
            "Tong Zhang"
        ],
        "published": "2024-03-13T17:29:45Z",
        "summary": "Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.",
        "pdf_link": "https://arxiv.org/pdf/2403.08730v2.pdf"
    },
    {
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
        "authors": [
            "Tianyu Cui",
            "Yanling Wang",
            "Chuanpu Fu",
            "Yong Xiao",
            "Sijia Li",
            "Xinhao Deng",
            "Yunpeng Liu",
            "Qinglin Zhang",
            "Ziyi Qiu",
            "Peiyang Li",
            "Zhixing Tan",
            "Junwu Xiong",
            "Xinyu Kong",
            "Zujie Wen",
            "Ke Xu",
            "Qi Li"
        ],
        "published": "2024-01-11T09:29:56Z",
        "summary": "Large language models (LLMs) have strong capabilities in solving diverse\nnatural language processing tasks. However, the safety and security issues of\nLLM systems have become the major obstacle to their widespread application.\nMany studies have extensively investigated risks in LLM systems and developed\nthe corresponding mitigation strategies. Leading-edge enterprises such as\nOpenAI, Google, Meta, and Anthropic have also made lots of efforts on\nresponsible LLMs. Therefore, there is a growing need to organize the existing\nstudies and establish comprehensive taxonomies for the community. In this\npaper, we delve into four essential modules of an LLM system, including an\ninput module for receiving prompts, a language model trained on extensive\ncorpora, a toolchain module for development and deployment, and an output\nmodule for exporting LLM-generated content. Based on this, we propose a\ncomprehensive taxonomy, which systematically analyzes potential risks\nassociated with each module of an LLM system and discusses the corresponding\nmitigation strategies. Furthermore, we review prevalent benchmarks, aiming to\nfacilitate the risk assessment of LLM systems. We hope that this paper can help\nLLM participants embrace a systematic perspective to build their responsible\nLLM systems.",
        "pdf_link": "https://arxiv.org/pdf/2401.05778v1.pdf"
    },
    {
        "title": "Repetition Improves Language Model Embeddings",
        "authors": [
            "Jacob Mitchell Springer",
            "Suhas Kotha",
            "Daniel Fried",
            "Graham Neubig",
            "Aditi Raghunathan"
        ],
        "published": "2024-02-23T17:25:10Z",
        "summary": "Recent approaches to improving the extraction of text embeddings from\nautoregressive large language models (LLMs) have largely focused on\nimprovements to data, backbone pretrained language models, or improving\ntask-differentiation via instructions. In this work, we address an\narchitectural limitation of autoregressive models: token embeddings cannot\ncontain information from tokens that appear later in the input. To address this\nlimitation, we propose a simple approach, \"echo embeddings,\" in which we repeat\nthe input twice in context and extract embeddings from the second occurrence.\nWe show that echo embeddings of early tokens can encode information about later\ntokens, allowing us to maximally leverage high-quality LLMs for embeddings. On\nthe MTEB leaderboard, echo embeddings improve over classical embeddings by over\n9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a\nMistral-7B model achieve state-of-the-art compared to prior open source models\nthat do not leverage synthetic fine-tuning data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15449v1.pdf"
    },
    {
        "title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale",
        "authors": [
            "Xiang Hu",
            "Pengyu Ji",
            "Qingyang Zhu",
            "Wei Wu",
            "Kewei Tu"
        ],
        "published": "2024-03-13T06:54:47Z",
        "summary": "A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.",
        "pdf_link": "https://arxiv.org/pdf/2403.08293v2.pdf"
    },
    {
        "title": "Do Membership Inference Attacks Work on Large Language Models?",
        "authors": [
            "Michael Duan",
            "Anshuman Suri",
            "Niloofar Mireshghallah",
            "Sewon Min",
            "Weijia Shi",
            "Luke Zettlemoyer",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "David Evans",
            "Hannaneh Hajishirzi"
        ],
        "published": "2024-02-12T17:52:05Z",
        "summary": "Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.07841v1.pdf"
    },
    {
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
        "authors": [
            "Saleh Ashkboos",
            "Maximilian L. Croci",
            "Marcelo Gennari do Nascimento",
            "Torsten Hoefler",
            "James Hensman"
        ],
        "published": "2024-01-26T17:35:45Z",
        "summary": "Large language models have become the cornerstone of natural language\nprocessing, but their use comes with substantial costs in terms of compute and\nmemory resources. Sparsification provides a solution to alleviate these\nresource constraints, and recent works have shown that trained models can be\nsparsified post-hoc. Existing sparsification techniques face challenges as they\nneed additional data structures and offer constrained speedup with current\nhardware. In this paper we present SliceGPT, a new post-training sparsification\nscheme which replaces each weight matrix with a smaller (dense) matrix,\nreducing the embedding dimension of the network. Through extensive\nexperimentation, we show that SliceGPT can remove up to 25% of the model\nparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models\nwhile maintaining 99%, 99% and 90% zero-shot task performance of the dense\nmodel respectively. Our sliced models run on fewer GPUs and run faster without\nany additional code optimization: on 24GB consumer GPUs we reduce the total\ncompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB\nA100 GPUs we reduce it to 66%. We offer a new insight, computational invariance\nin transformer networks, which enables SliceGPT and we hope it will inspire and\nenable future avenues to reduce memory and computation demands for pre-trained\nmodels. Code is available at:\nhttps://github.com/microsoft/TransformerCompression",
        "pdf_link": "https://arxiv.org/pdf/2401.15024v2.pdf"
    },
    {
        "title": "Chitchat as Interference: Adding User Backstories to Task-Oriented Dialogues",
        "authors": [
            "Armand Stricker",
            "Patrick Paroubek"
        ],
        "published": "2024-02-23T10:27:42Z",
        "summary": "During task-oriented dialogues (TODs), human users naturally introduce\nchitchat that is beyond the immediate scope of the task, interfering with the\nflow of the conversation. To address this issue without the need for expensive\nmanual data creation, we use few-shot prompting with Llama-2-70B to enhance the\nMultiWOZ dataset with user backstories, a typical example of chitchat\ninterference in TODs. We assess the impact of this addition by testing two\nmodels: one trained solely on TODs and another trained on TODs with a\npreliminary chitchat interaction. Our analysis demonstrates that our enhanced\ndataset poses a challenge for these systems. Moreover, we demonstrate that our\ndataset can be effectively used for training purposes, enabling a system to\nconsistently acknowledge the user's backstory while also successfully moving\nthe task forward in the same turn, as confirmed by human evaluation. These\nfindings highlight the benefits of generating novel chitchat-TOD scenarios to\ntest TOD systems more thoroughly and improve their resilience to natural user\ninterferences",
        "pdf_link": "https://arxiv.org/pdf/2402.15248v2.pdf"
    },
    {
        "title": "MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery",
        "authors": [
            "Feihong Lu",
            "Weiqi Wang",
            "Yangyifei Luo",
            "Ziqin Zhu",
            "Qingyun Sun",
            "Baixuan Xu",
            "Haochen Shi",
            "Shiqi Gao",
            "Qian Li",
            "Yangqiu Song",
            "Jianxin Li"
        ],
        "published": "2024-02-28T08:57:42Z",
        "summary": "Social media has become a ubiquitous tool for connecting with others, staying\nupdated with news, expressing opinions, and finding entertainment. However,\nunderstanding the intention behind social media posts remains challenging due\nto the implicitness of intentions in social media posts, the need for\ncross-modality understanding of both text and images, and the presence of noisy\ninformation such as hashtags, misspelled words, and complicated abbreviations.\nTo address these challenges, we present MIKO, a Multimodal Intention Kowledge\nDistillatiOn framework that collaboratively leverages a Large Language Model\n(LLM) and a Multimodal Large Language Model (MLLM) to uncover users'\nintentions. Specifically, we use an MLLM to interpret the image and an LLM to\nextract key information from the text and finally instruct the LLM again to\ngenerate intentions. By applying MIKO to publicly available social media\ndatasets, we construct an intention knowledge base featuring 1,372K intentions\nrooted in 137,287 posts. We conduct a two-stage annotation to verify the\nquality of the generated knowledge and benchmark the performance of widely used\nLLMs for intention generation. We further apply MIKO to a sarcasm detection\ndataset and distill a student model to demonstrate the downstream benefits of\napplying intention knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2402.18169v2.pdf"
    },
    {
        "title": "eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data",
        "authors": [
            "Bo Peng",
            "Xinyi Ling",
            "Ziru Chen",
            "Huan Sun",
            "Xia Ning"
        ],
        "published": "2024-02-13T22:26:24Z",
        "summary": "With tremendous efforts on developing effective e-commerce models,\nconventional e-commerce models show limited success in generalist e-commerce\nmodeling, and suffer from unsatisfactory performance on new users and new\nproducts - a typical out-of-domain generalization challenge. Meanwhile, large\nlanguage models (LLMs) demonstrate outstanding performance in generalist\nmodeling and out-of-domain generalizability in many fields. Toward fully\nunleashing their power for e-commerce, in this paper, we construct ECInstruct,\nthe first open-sourced, large-scale, and high-quality benchmark instruction\ndataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of\ne-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive\nexperiments and evaluation demonstrate that eCeLLM models substantially\noutperform baseline models, including the most advanced GPT-4, and the\nstate-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM\nexhibits excellent generalizability to out-of-domain settings, including unseen\nproducts and unseen instructions, highlighting its superiority as a generalist\ne-commerce model. Both the ECInstruct dataset and the eCeLLM models show great\npotential in empowering versatile and effective LLMs for e-commerce. ECInstruct\nand eCeLLM models are publicly accessible through\nhttps://ninglab.github.io/eCeLLM.",
        "pdf_link": "https://arxiv.org/pdf/2402.08831v1.pdf"
    },
    {
        "title": "A Hypothesis-Driven Framework for the Analysis of Self-Rationalising Models",
        "authors": [
            "Marc Braun",
            "Jenny Kunz"
        ],
        "published": "2024-02-07T12:26:12Z",
        "summary": "The self-rationalising capabilities of LLMs are appealing because the\ngenerated explanations can give insights into the plausibility of the\npredictions. However, how faithful the explanations are to the predictions is\nquestionable, raising the need to explore the patterns behind them further. To\nthis end, we propose a hypothesis-driven statistical framework. We use a\nBayesian network to implement a hypothesis about how a task (in our example,\nnatural language inference) is solved, and its internal states are translated\ninto natural language with templates. Those explanations are then compared to\nLLM-generated free-text explanations using automatic and human evaluations.\nThis allows us to judge how similar the LLM's and the Bayesian network's\ndecision processes are. We demonstrate the usage of our framework with an\nexample hypothesis and two realisations in Bayesian networks. The resulting\nmodels do not exhibit a strong similarity to GPT-3.5. We discuss the\nimplications of this as well as the framework's potential to approximate LLM\ndecisions better in future work.",
        "pdf_link": "https://arxiv.org/pdf/2402.04787v1.pdf"
    },
    {
        "title": "Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios",
        "authors": [
            "Shijue Huang",
            "Wanjun Zhong",
            "Jianqiao Lu",
            "Qi Zhu",
            "Jiahui Gao",
            "Weiwen Liu",
            "Yutai Hou",
            "Xingshan Zeng",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Ruifeng Xu",
            "Qun Liu"
        ],
        "published": "2024-01-30T16:52:56Z",
        "summary": "The recent trend of using Large Language Models (LLMs) as tool agents in\nreal-world applications underscores the necessity for comprehensive evaluations\nof their capabilities, particularly in complex scenarios involving planning,\ncreating, and using tools. However, existing benchmarks typically focus on\nsimple synthesized queries that do not reflect real-world complexity, thereby\noffering limited perspectives in evaluating tool utilization. To address this\nissue, we present UltraTool, a novel benchmark designed to improve and evaluate\nLLMs' ability in tool utilization within real-world scenarios. UltraTool\nfocuses on the entire process of using tools - from planning and creating to\napplying them in complex tasks. It emphasizes real-world complexities,\ndemanding accurate, multi-step planning for effective problem-solving. A key\nfeature of UltraTool is its independent evaluation of planning with natural\nlanguage, which happens before tool usage and simplifies the task solving by\nmapping out the intermediate steps. Thus, unlike previous work, it eliminates\nthe restriction of pre-defined toolset. Through extensive experiments on\nvarious LLMs, we offer novel insights into the evaluation of capabilities of\nLLMs in tool utilization, thereby contributing a fresh perspective to this\nrapidly evolving field. The benchmark is publicly available at\nhttps://github.com/JoeYing1019/UltraTool.",
        "pdf_link": "https://arxiv.org/pdf/2401.17167v2.pdf"
    },
    {
        "title": "Uncertainty quantification in fine-tuned LLMs using LoRA ensembles",
        "authors": [
            "Oleksandr Balabanov",
            "Hampus Linander"
        ],
        "published": "2024-02-19T16:26:00Z",
        "summary": "Fine-tuning large language models can improve task specific performance,\nalthough a general understanding of what the fine-tuned model has learned,\nforgotten and how to trust its predictions is still missing. We derive\nprincipled uncertainty quantification for fine-tuned LLMs with posterior\napproximations using computationally efficient low-rank adaptation ensembles.\nWe analyze three common multiple-choice datasets using low-rank adaptation\nensembles based on Mistral-7b, and draw quantitative and qualitative\nconclusions on their perceived complexity and model efficacy on the different\ntarget domains during and after fine-tuning. In particular, backed by the\nnumerical experiments, we hypothesise about signals from entropic uncertainty\nmeasures for data domains that are inherently difficult for a given\narchitecture to learn.",
        "pdf_link": "https://arxiv.org/pdf/2402.12264v1.pdf"
    },
    {
        "title": "Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?",
        "authors": [
            "Benjamin Reichman",
            "Larry Heck"
        ],
        "published": "2024-02-16T19:28:52Z",
        "summary": "Dense passage retrieval (DPR) is the first step in the retrieval augmented\ngeneration (RAG) paradigm for improving the performance of large language\nmodels (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of\nthe embeddings between queries and relevant textual data. A deeper\nunderstanding of DPR fine-tuning will be required to fundamentally unlock the\nfull potential of this approach. In this work, we explore DPR-trained models\nmechanistically by using a combination of probing, layer activation analysis,\nand model editing. Our experiments show that DPR training decentralizes how\nknowledge is stored in the network, creating multiple access pathways to the\nsame information. We also uncover a limitation in this training style: the\ninternal knowledge of the pre-trained model bounds what the retrieval model can\nretrieve. These findings suggest a few possible directions for dense retrieval:\n(1) expose the DPR training process to more knowledge so more can be\ndecentralized, (2) inject facts as decentralized representations, (3) model and\nincorporate knowledge uncertainty in the retrieval process, and (4) directly\nmap internal model knowledge to a knowledge base.",
        "pdf_link": "https://arxiv.org/pdf/2402.11035v1.pdf"
    },
    {
        "title": "FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language",
        "authors": [
            "Yayue Deng",
            "Mohan Xu",
            "Yao Tang"
        ],
        "published": "2024-03-10T07:21:31Z",
        "summary": "The effectiveness of central bank communication is a crucial aspect of\nmonetary policy transmission. While recent research has examined the influence\nof policy communication by the chairs of the Federal Reserve on various\nfinancial variables, much of the literature relies on rule-based or\ndictionary-based methods in parsing the language of the chairs, leaving nuanced\ninformation about policy stance contained in nonverbal emotion out of the\nanalysis. In the current study, we propose the Fine-Grained Monetary Policy\nAnalysis Framework (FMPAF), a novel approach that integrates large language\nmodels (LLMs) with regression analysis to provide a comprehensive analysis of\nthe impact of the press-conference communications of chairs of the Federal\nReserve on financial markets. We conduct extensive comparisons of model\nperformance under different levels of granularity, modalities, and\ncommunication scenarios. Based on our preferred specification, a one-unit\nincrease in the sentiment score is associated with an increase of the price of\nS\\&P 500 Exchange-Traded Fund by approximately 500 basis points, a\n15-basis-point decrease in the policy interest rate, while not leading to a\nsignificant response in exchange rates.",
        "pdf_link": "https://arxiv.org/pdf/2403.06115v1.pdf"
    },
    {
        "title": "Few-Shot Learning for Chronic Disease Management: Leveraging Large Language Models and Multi-Prompt Engineering with Medical Knowledge Injection",
        "authors": [
            "Haoxin Liu",
            "Wenli Zhang",
            "Jiaheng Xie",
            "Buomsoo Kim",
            "Zhu Zhang",
            "Yidong Chai"
        ],
        "published": "2024-01-16T13:54:43Z",
        "summary": "This study harnesses state-of-the-art AI technology for chronic disease\nmanagement, specifically in detecting various mental disorders through\nuser-generated textual content. Existing studies typically rely on fully\nsupervised machine learning, which presents challenges such as the\nlabor-intensive manual process of annotating extensive training data for each\ndisease and the need to design specialized deep learning architectures for each\nproblem. To address such challenges, we propose a novel framework that\nleverages advanced AI techniques, including large language models and\nmulti-prompt engineering. Specifically, we address two key technical challenges\nin data-driven chronic disease management: (1) developing personalized prompts\nto represent each user's uniqueness and (2) incorporating medical knowledge\ninto prompts to provide context for chronic disease detection, instruct\nlearning objectives, and operationalize prediction goals. We evaluate our\nmethod using four mental disorders, which are prevalent chronic diseases\nworldwide, as research cases. On the depression detection task, our method (F1\n= 0.975~0.978) significantly outperforms traditional supervised learning\nparadigms, including feature engineering (F1 = 0.760) and architecture\nengineering (F1 = 0.756). Meanwhile, our approach demonstrates success in\nfew-shot learning, i.e., requiring only a minimal number of training examples\nto detect chronic diseases based on user-generated textual content (i.e., only\n2, 10, or 100 subjects). Moreover, our method can be generalized to other\nmental disorder detection tasks, including anorexia, pathological gambling, and\nself-harm (F1 = 0.919~0.978).",
        "pdf_link": "https://arxiv.org/pdf/2401.12988v1.pdf"
    },
    {
        "title": "Long-form factuality in large language models",
        "authors": [
            "Jerry Wei",
            "Chengrun Yang",
            "Xinying Song",
            "Yifeng Lu",
            "Nathan Hu",
            "Jie Huang",
            "Dustin Tran",
            "Daiyi Peng",
            "Ruibo Liu",
            "Da Huang",
            "Cosmo Du",
            "Quoc V. Le"
        ],
        "published": "2024-03-27T17:48:55Z",
        "summary": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.",
        "pdf_link": "https://arxiv.org/pdf/2403.18802v3.pdf"
    },
    {
        "title": "Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese Address Entity Recognition Dataset for UAV Delivery",
        "authors": [
            "Yuxuan Yao",
            "Sichun Luo",
            "Haohan Zhao",
            "Guanzhi Deng",
            "Linqi Song"
        ],
        "published": "2024-03-10T05:12:16Z",
        "summary": "We present CNER-UAV, a fine-grained \\textbf{C}hinese \\textbf{N}ame\n\\textbf{E}ntity \\textbf{R}ecognition dataset specifically designed for the task\nof address resolution in \\textbf{U}nmanned \\textbf{A}erial \\textbf{V}ehicle\ndelivery systems. The dataset encompasses a diverse range of five categories,\nenabling comprehensive training and evaluation of NER models. To construct this\ndataset, we sourced the data from a real-world UAV delivery system and\nconducted a rigorous data cleaning and desensitization process to ensure\nprivacy and data integrity. The resulting dataset, consisting of around 12,000\nannotated samples, underwent human experts and \\textbf{L}arge \\textbf{L}anguage\n\\textbf{M}odel annotation. We evaluated classical NER models on our dataset and\nprovided in-depth analysis. The dataset and models are publicly available at\n\\url{https://github.com/zhhvvv/CNER-UAV}.",
        "pdf_link": "https://arxiv.org/pdf/2403.06097v2.pdf"
    },
    {
        "title": "RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model",
        "authors": [
            "Jianhao Yuan",
            "Shuyang Sun",
            "Daniel Omeiza",
            "Bo Zhao",
            "Paul Newman",
            "Lars Kunze",
            "Matthew Gadd"
        ],
        "published": "2024-02-16T16:57:18Z",
        "summary": "Robots powered by 'blackbox' models need to provide human-understandable\nexplanations which we can trust. Hence, explainability plays a critical role in\ntrustworthy autonomous decision-making to foster transparency and acceptance\namong end users, especially in complex autonomous driving. Recent advancements\nin Multi-Modal Large Language models (MLLMs) have shown promising potential in\nenhancing the explainability as a driving agent by producing control\npredictions along with natural language explanations. However, severe data\nscarcity due to expensive annotation costs and significant domain gaps between\ndifferent datasets makes the development of a robust and generalisable system\nan extremely challenging task. Moreover, the prohibitively expensive training\nrequirements of MLLM and the unsolved problem of catastrophic forgetting\nfurther limit their generalisability post-deployment. To address these\nchallenges, we present RAG-Driver, a novel retrieval-augmented multi-modal\nlarge language model that leverages in-context learning for high-performance,\nexplainable, and generalisable autonomous driving. By grounding in retrieved\nexpert demonstration, we empirically validate that RAG-Driver achieves\nstate-of-the-art performance in producing driving action explanations,\njustifications, and control signal prediction. More importantly, it exhibits\nexceptional zero-shot generalisation capabilities to unseen environments\nwithout further training endeavours.",
        "pdf_link": "https://arxiv.org/pdf/2402.10828v1.pdf"
    },
    {
        "title": "Efficient Models for the Detection of Hate, Abuse and Profanity",
        "authors": [
            "Christoph Tillmann",
            "Aashka Trivedi",
            "Bishwaranjan Bhattacharjee"
        ],
        "published": "2024-02-08T12:28:18Z",
        "summary": "Large Language Models (LLMs) are the cornerstone for many Natural Language\nProcessing (NLP) tasks like sentiment analysis, document classification, named\nentity recognition, question answering, summarization, etc. LLMs are often\ntrained on data which originates from the web. This data is prone to having\ncontent with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,\nplease refer to the Appendix. Due to the LLMs being exposed to HAP content\nduring training, the models learn it and may then generate hateful or profane\ncontent. For example, when the open-source RoBERTa model (specifically, the\nRoBERTA base model) from the HuggingFace (HF) Transformers library is prompted\nto replace the mask token in `I do not know that Persian people are that MASK`\nit returns the word `stupid` with the highest score. This is unacceptable in\ncivil discourse.The detection of Hate, Abuse and Profanity in text is a vital\ncomponent of creating civil and unbiased LLMs, which is needed not only for\nEnglish, but for all languages. In this article, we briefly describe the\ncreation of HAP detectors and various ways of using them to make models civil\nand acceptable in the output they generate.",
        "pdf_link": "https://arxiv.org/pdf/2402.05624v1.pdf"
    },
    {
        "title": "LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model",
        "authors": [
            "Linmei Hu",
            "Hongyu He",
            "Duokang Wang",
            "Ziwang Zhao",
            "Yingxia Shao",
            "Liqiang Nie"
        ],
        "published": "2024-03-12T12:10:18Z",
        "summary": "Personality detection aims to detect one's personality traits underlying in\nsocial media posts. One challenge of this task is the scarcity of ground-truth\npersonality traits which are collected from self-report questionnaires. Most\nexisting methods learn post features directly by fine-tuning the pre-trained\nlanguage models under the supervision of limited personality labels. This leads\nto inferior quality of post features and consequently affects the performance.\nIn addition, they treat personality traits as one-hot classification labels,\noverlooking the semantic information within them. In this paper, we propose a\nlarge language model (LLM) based text augmentation enhanced personality\ndetection model, which distills the LLM's knowledge to enhance the small model\nfor personality detection, even when the LLM fails in this task. Specifically,\nwe enable LLM to generate post analyses (augmentations) from the aspects of\nsemantic, sentiment, and linguistic, which are critical for personality\ndetection. By using contrastive learning to pull them together in the embedding\nspace, the post encoder can better capture the psycho-linguistic information\nwithin the post representations, thus improving personality detection.\nFurthermore, we utilize the LLM to enrich the information of personality labels\nfor enhancing the detection performance. Experimental results on the benchmark\ndatasets demonstrate that our model outperforms the state-of-the-art methods on\npersonality detection.",
        "pdf_link": "https://arxiv.org/pdf/2403.07581v1.pdf"
    },
    {
        "title": "Large Language Models for Data Annotation: A Survey",
        "authors": [
            "Zhen Tan",
            "Alimohammad Beigi",
            "Song Wang",
            "Ruocheng Guo",
            "Amrita Bhattacharjee",
            "Bohan Jiang",
            "Mansooreh Karami",
            "Jundong Li",
            "Lu Cheng",
            "Huan Liu"
        ],
        "published": "2024-02-21T00:44:04Z",
        "summary": "Data annotation is the labeling or tagging of raw data with relevant\ninformation, essential for improving the efficacy of machine learning models.\nThe process, however, is labor-intensive and expensive. The emergence of\nadvanced Large Language Models (LLMs), exemplified by GPT-4, presents an\nunprecedented opportunity to revolutionize and automate the intricate process\nof data annotation. While existing surveys have extensively covered LLM\narchitecture, training, and general applications, this paper uniquely focuses\non their specific utility for data annotation. This survey contributes to three\ncore aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations,\nand Learning with LLM-generated annotations. Furthermore, the paper includes an\nin-depth taxonomy of methodologies employing LLMs for data annotation, a\ncomprehensive review of learning strategies for models incorporating\nLLM-generated annotations, and a detailed discussion on primary challenges and\nlimitations associated with using LLMs for data annotation. As a key guide,\nthis survey aims to direct researchers and practitioners in exploring the\npotential of the latest LLMs for data annotation, fostering future advancements\nin this critical domain. We provide a comprehensive papers list at\n\\url{https://github.com/Zhen-Tan-dmml/LLM4Annotation.git}.",
        "pdf_link": "https://arxiv.org/pdf/2402.13446v1.pdf"
    },
    {
        "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
        "authors": [
            "Moritz Stephan",
            "Alexander Khazatsky",
            "Eric Mitchell",
            "Annie S Chen",
            "Sheryl Hsu",
            "Archit Sharma",
            "Chelsea Finn"
        ],
        "published": "2024-02-16T18:50:24Z",
        "summary": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.",
        "pdf_link": "https://arxiv.org/pdf/2402.10893v1.pdf"
    },
    {
        "title": "Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models",
        "authors": [
            "Lanning Wei",
            "Jun Gao",
            "Huan Zhao",
            "Quanming Yao"
        ],
        "published": "2024-02-18T16:43:21Z",
        "summary": "Graph-structured data are the commonly used and have wide application\nscenarios in the real world. For these diverse applications, the vast variety\nof learning tasks, graph domains, and complex graph learning procedures present\nchallenges for human experts when designing versatile graph learning\napproaches. Facing these challenges, large language models (LLMs) offer a\npotential solution due to the extensive knowledge and the human-like\nintelligence. This paper proposes a novel conceptual prototype for designing\nversatile graph learning methods with LLMs, with a particular focus on the\n\"where\" and \"how\" perspectives. From the \"where\" perspective, we summarize four\nkey graph learning procedures, including task definition, graph data feature\nengineering, model selection and optimization, deployment and serving. We then\nexplore the application scenarios of LLMs in these procedures across a wider\nspectrum. In the \"how\" perspective, we align the abilities of LLMs with the\nrequirements of each procedure. Finally, we point out the promising directions\nthat could better leverage the strength of LLMs towards versatile graph\nlearning methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.11641v2.pdf"
    },
    {
        "title": "Fine-tuning CLIP Text Encoders with Two-step Paraphrasing",
        "authors": [
            "Hyunjae Kim",
            "Seunghyun Yoon",
            "Trung Bui",
            "Handong Zhao",
            "Quan Tran",
            "Franck Dernoncourt",
            "Jaewoo Kang"
        ],
        "published": "2024-02-23T06:11:50Z",
        "summary": "Contrastive language-image pre-training (CLIP) models have demonstrated\nconsiderable success across various vision-language tasks, such as\ntext-to-image retrieval, where the model is required to effectively process\nnatural language input to produce an accurate visual output. However, current\nmodels still face limitations in dealing with linguistic variations in input\nqueries, such as paraphrases, making it challenging to handle a broad range of\nuser queries in real-world applications. In this study, we introduce a\nstraightforward fine-tuning approach to enhance the representations of CLIP\nmodels for paraphrases. Our approach involves a two-step paraphrase generation\nprocess, where we automatically create two categories of paraphrases from\nweb-scale image captions by leveraging large language models. Subsequently, we\nfine-tune the CLIP text encoder using these generated paraphrases while\nfreezing the image encoder. Our resulting model, which we call ParaCLIP,\nexhibits significant improvements over baseline CLIP models across various\ntasks, including paraphrased retrieval (with rank similarity scores improved by\nup to 2.0% and 5.6%), Visual Genome Relation and Attribution, as well as seven\nsemantic textual similarity tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.15120v1.pdf"
    },
    {
        "title": "UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction",
        "authors": [
            "Yuan Yuan",
            "Jingtao Ding",
            "Jie Feng",
            "Depeng Jin",
            "Yong Li"
        ],
        "published": "2024-02-19T05:04:11Z",
        "summary": "Urban spatio-temporal prediction is crucial for informed decision-making,\nsuch as transportation management, resource optimization, and urban planning.\nAlthough pretrained foundation models for natural languages have experienced\nremarkable breakthroughs, wherein one general-purpose model can tackle multiple\ntasks across various domains, urban spatio-temporal modeling lags behind.\nExisting approaches for urban prediction are usually tailored for specific\nspatio-temporal scenarios, requiring task-specific model designs and extensive\nin-domain training data. In this work, we propose a universal model, UniST, for\nurban spatio-temporal prediction. Drawing inspiration from large language\nmodels, UniST achieves success through: (i) flexibility towards diverse\nspatio-temporal data characteristics, (ii) effective generative pre-training\nwith elaborated masking strategies to capture complex spatio-temporal\nrelationships, (iii) spatio-temporal knowledge-guided prompts that align and\nleverage intrinsic and shared knowledge across scenarios. These designs\ntogether unlock the potential of a one-for-all model for spatio-temporal\nprediction with powerful generalization capability. Extensive experiments on 15\ncities and 6 domains demonstrate the universality of UniST in advancing\nstate-of-the-art prediction performance, especially in few-shot and zero-shot\nscenarios.",
        "pdf_link": "https://arxiv.org/pdf/2402.11838v1.pdf"
    },
    {
        "title": "Predicting Sustainable Development Goals Using Course Descriptions -- from LLMs to Conventional Foundation Models",
        "authors": [
            "Lev Kharlashkin",
            "Melany Macias",
            "Leo Huovinen",
            "Mika H\u00e4m\u00e4l\u00e4inen"
        ],
        "published": "2024-02-26T09:19:46Z",
        "summary": "We present our work on predicting United Nations sustainable development\ngoals (SDG) for university courses. We use an LLM named PaLM 2 to generate\ntraining data given a noisy human-authored course description input as input.\nWe use this data to train several different smaller language models to predict\nSDGs for university courses. This work contributes to better university level\nadaptation of SDGs. The best performing model in our experiments was BART with\nan F1-score of 0.786.",
        "pdf_link": "https://arxiv.org/pdf/2402.16420v1.pdf"
    },
    {
        "title": "Large Language Models are Parallel Multilingual Learners",
        "authors": [
            "Yongyu Mu",
            "Peinan Feng",
            "Zhiquan Cao",
            "Yuzhang Wu",
            "Bei Li",
            "Chenglong Wang",
            "Tong Xiao",
            "Kai Song",
            "Tongran Liu",
            "Chunliang Zhang",
            "Jingbo Zhu"
        ],
        "published": "2024-03-14T03:33:46Z",
        "summary": "In this study, we reveal an in-context learning (ICL) capability of\nmultilingual large language models (LLMs): by translating the input to several\nlanguages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which\nsignificantly enhances their comprehension abilities. To test this capability,\nwe design extensive experiments encompassing 8 typical datasets, 7 languages\nand 8 state-of-the-art multilingual LLMs. Experimental results show that (1)\nincorporating more languages help PiM surpass the conventional ICL further; (2)\neven combining with the translations that are inferior to baseline performance\ncan also help. Moreover, by examining the activated neurons in LLMs, we\ndiscover a counterintuitive but interesting phenomenon. Contrary to the common\nthought that PiM would activate more neurons than monolingual input to leverage\nknowledge learned from diverse languages, PiM actually inhibits neurons and\npromotes more precise neuron activation especially when more languages are\nadded. This phenomenon aligns with the neuroscience insight about synaptic\npruning, which removes less used neural connections, strengthens remainders,\nand then enhances brain intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.09073v1.pdf"
    },
    {
        "title": "BadEdit: Backdooring large language models by model editing",
        "authors": [
            "Yanzhou Li",
            "Tianlin Li",
            "Kangjie Chen",
            "Jian Zhang",
            "Shangqing Liu",
            "Wenhan Wang",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "published": "2024-03-20T07:34:18Z",
        "summary": "Mainstream backdoor attack methods typically demand substantial tuning data\nfor poisoning, limiting their practicality and potentially degrading the\noverall performance when applied to Large Language Models (LLMs). To address\nthese issues, for the first time, we formulate backdoor injection as a\nlightweight knowledge editing problem, and introduce the BadEdit attack\nframework. BadEdit directly alters LLM parameters to incorporate backdoors with\nan efficient editing technique. It boasts superiority over existing backdoor\ninjection techniques in several areas: (1) Practicality: BadEdit necessitates\nonly a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only\nadjusts a subset of parameters, leading to a dramatic reduction in time\nconsumption. (3) Minimal side effects: BadEdit ensures that the model's\noverarching performance remains uncompromised. (4) Robustness: the backdoor\nremains robust even after subsequent fine-tuning or instruction-tuning.\nExperimental results demonstrate that our BadEdit framework can efficiently\nattack pre-trained LLMs with up to 100\\% success rate while maintaining the\nmodel's performance on benign inputs.",
        "pdf_link": "https://arxiv.org/pdf/2403.13355v1.pdf"
    },
    {
        "title": "CloChat: Understanding How People Customize, Interact, and Experience Personas in Large Language Models",
        "authors": [
            "Juhye Ha",
            "Hyeon Jeon",
            "DaEun Han",
            "Jinwook Seo",
            "Changhoon Oh"
        ],
        "published": "2024-02-23T11:25:17Z",
        "summary": "Large language models (LLMs) have facilitated significant strides in\ngenerating conversational agents, enabling seamless, contextually relevant\ndialogues across diverse topics. However, the existing LLM-driven\nconversational agents have fixed personalities and functionalities, limiting\ntheir adaptability to individual user needs. Creating personalized agent\npersonas with distinct expertise or traits can address this issue. Nonetheless,\nwe lack knowledge of how people customize and interact with agent personas. In\nthis research, we investigated how users customize agent personas and their\nimpact on interaction quality, diversity, and dynamics. To this end, we\ndeveloped CloChat, an interface supporting easy and accurate customization of\nagent personas in LLMs. We conducted a study comparing how participants\ninteract with CloChat and ChatGPT. The results indicate that participants\nformed emotional bonds with the customized agents, engaged in more dynamic\ndialogues, and showed interest in sustaining interactions. These findings\ncontribute to design implications for future systems with conversational agents\nusing LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.15265v1.pdf"
    },
    {
        "title": "Self-Retrieval: Building an Information Retrieval System with One Large Language Model",
        "authors": [
            "Qiaoyu Tang",
            "Jiawei Chen",
            "Bowen Yu",
            "Yaojie Lu",
            "Cheng Fu",
            "Haiyang Yu",
            "Hongyu Lin",
            "Fei Huang",
            "Ben He",
            "Xianpei Han",
            "Le Sun",
            "Yongbin Li"
        ],
        "published": "2024-02-23T18:45:35Z",
        "summary": "The rise of large language models (LLMs) has transformed the role of\ninformation retrieval (IR) systems in the way to humans accessing information.\nDue to the isolated architecture and the limited interaction, existing IR\nsystems are unable to fully accommodate the shift from directly providing\ninformation to humans to indirectly serving large language models. In this\npaper, we propose Self-Retrieval, an end-to-end, LLM-driven information\nretrieval architecture that can fully internalize the required abilities of IR\nsystems into a single LLM and deeply leverage the capabilities of LLMs during\nIR process. Specifically, Self-retrieval internalizes the corpus to retrieve\ninto a LLM via a natural language indexing architecture. Then the entire\nretrieval process is redefined as a procedure of document generation and\nself-assessment, which can be end-to-end executed using a single large language\nmodel. Experimental results demonstrate that Self-Retrieval not only\nsignificantly outperforms previous retrieval approaches by a large margin, but\nalso can significantly boost the performance of LLM-driven downstream\napplications like retrieval augumented generation.",
        "pdf_link": "https://arxiv.org/pdf/2403.00801v1.pdf"
    },
    {
        "title": "Video Anomaly Detection and Explanation via Large Language Models",
        "authors": [
            "Hui Lv",
            "Qianru Sun"
        ],
        "published": "2024-01-11T07:09:44Z",
        "summary": "Video Anomaly Detection (VAD) aims to localize abnormal events on the\ntimeline of long-range surveillance videos. Anomaly-scoring-based methods have\nbeen prevailing for years but suffer from the high complexity of thresholding\nand low explanability of detection results. In this paper, we conduct pioneer\nresearch on equipping video-based large language models (VLLMs) in the\nframework of VAD, making the VAD model free from thresholds and able to explain\nthe reasons for the detected anomalies. We introduce a novel network module\nLong-Term Context (LTC) to mitigate the incapability of VLLMs in long-range\ncontext modeling. We design a three-phase training method to improve the\nefficiency of fine-tuning VLLMs by substantially minimizing the requirements\nfor VAD data and lowering the costs of annotating instruction-tuning data. Our\ntrained model achieves the top performance on the anomaly videos of the\nUCF-Crime and TAD benchmarks, with the AUC improvements of +3.86\\% and +4.96\\%,\nrespectively. More impressively, our approach can provide textual explanations\nfor detected anomalies.",
        "pdf_link": "https://arxiv.org/pdf/2401.05702v1.pdf"
    },
    {
        "title": "Finetuning Large Language Models for Vulnerability Detection",
        "authors": [
            "Alexey Shestov",
            "Rodion Levichev",
            "Ravil Mussabayev",
            "Evgeny Maslov",
            "Anton Cheshkov",
            "Pavel Zadorozhny"
        ],
        "published": "2024-01-30T13:46:49Z",
        "summary": "This paper presents the results of finetuning large language models (LLMs)\nfor the task of detecting vulnerabilities in source code. We leverage\nWizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and\nadapt it for vulnerability detection through further finetuning. To accelerate\ntraining, we modify WizardCoder's training procedure, also we investigate\noptimal training regimes. For the imbalanced dataset with many more negative\nexamples than positive, we also explore different techniques to improve\nclassification performance. The finetuned WizardCoder model achieves\nimprovement in ROC AUC and F1 measures on balanced and imbalanced vulnerability\ndatasets over CodeBERT-like model, demonstrating the effectiveness of adapting\npretrained LLMs for vulnerability detection in source code. The key\ncontributions are finetuning the state-of-the-art code LLM, WizardCoder,\nincreasing its training speed without the performance harm, optimizing the\ntraining procedure and regimes, handling class imbalance, and improving\nperformance on difficult vulnerability detection datasets. This demonstrates\nthe potential for transfer learning by finetuning large pretrained language\nmodels for specialized source code analysis tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.17010v4.pdf"
    },
    {
        "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows",
        "authors": [
            "Ajay Patel",
            "Colin Raffel",
            "Chris Callison-Burch"
        ],
        "published": "2024-02-16T00:10:26Z",
        "summary": "Large language models (LLMs) have become a dominant and important tool for\nNLP researchers in a wide range of tasks. Today, many researchers use LLMs in\nsynthetic data generation, task evaluation, fine-tuning, distillation, and\nother model-in-the-loop research workflows. However, challenges arise when\nusing these models that stem from their scale, their closed source nature, and\nthe lack of standardized tooling for these new and emerging workflows. The\nrapid rise to prominence of these models and these unique challenges has had\nimmediate adverse impacts on open science and on the reproducibility of work\nthat uses them. In this paper, we introduce DataDreamer, an open source Python\nlibrary that allows researchers to write simple code to implement powerful LLM\nworkflows. DataDreamer also helps researchers adhere to best practices that we\npropose to encourage open science and reproducibility. The library and\ndocumentation are available at https://github.com/datadreamer-dev/DataDreamer .",
        "pdf_link": "https://arxiv.org/pdf/2402.10379v1.pdf"
    },
    {
        "title": "Revisiting The Classics: A Study on Identifying and Rectifying Gender Stereotypes in Rhymes and Poems",
        "authors": [
            "Aditya Narayan Sankaran",
            "Vigneshwaran Shankaran",
            "Sampath Lonka",
            "Rajesh Sharma"
        ],
        "published": "2024-03-18T13:02:02Z",
        "summary": "Rhymes and poems are a powerful medium for transmitting cultural norms and\nsocietal roles. However, the pervasive existence of gender stereotypes in these\nworks perpetuates biased perceptions and limits the scope of individuals'\nidentities. Past works have shown that stereotyping and prejudice emerge in\nearly childhood, and developmental research on causal mechanisms is critical\nfor understanding and controlling stereotyping and prejudice. This work\ncontributes by gathering a dataset of rhymes and poems to identify gender\nstereotypes and propose a model with 97% accuracy to identify gender bias.\nGender stereotypes were rectified using a Large Language Model (LLM) and its\neffectiveness was evaluated in a comparative survey against human educator\nrectifications. To summarize, this work highlights the pervasive nature of\ngender stereotypes in literary works and reveals the potential of LLMs to\nrectify gender stereotypes. This study raises awareness and promotes\ninclusivity within artistic expressions, making a significant contribution to\nthe discourse on gender equality.",
        "pdf_link": "https://arxiv.org/pdf/2403.11752v2.pdf"
    },
    {
        "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey",
        "authors": [
            "Zeyu Han",
            "Chao Gao",
            "Jinyang Liu",
            "Jeff Zhang",
            "Sai Qian Zhang"
        ],
        "published": "2024-03-21T17:55:50Z",
        "summary": "Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adapt the large models over\nthe various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large models to adapt it to a\nspecific task while minimizing the number of additional parameters introduced\nor computational resources required. This approach is particularly important\nwhen dealing with large language models with high parameter counts, as\nfine-tuning these models from scratch can be computationally expensive and\nresource-intensive, posing considerable challenges in the supporting system\nplatform design. In this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computational overhead.\nMoreover, we provide an overview of applications developed using different PEFT\nalgorithms and discuss common techniques employed to mitigate computation costs\nfor PEFT. In addition to the algorithmic perspective, we overview various\nreal-world system designs to investigate the implementation costs associated\nwith different PEFT algorithms. This survey serves as an indispensable resource\nfor researchers aiming to understand both the PEFT algorithm and its system\nimplementation, offering detailed insights into recent advancements and\npractical applications.",
        "pdf_link": "https://arxiv.org/pdf/2403.14608v2.pdf"
    },
    {
        "title": "Can Large Language Models Understand Context?",
        "authors": [
            "Yilun Zhu",
            "Joel Ruben Antony Moniz",
            "Shruti Bhargava",
            "Jiarui Lu",
            "Dhivya Piraviperumal",
            "Site Li",
            "Yuan Zhang",
            "Hong Yu",
            "Bo-Hsiang Tseng"
        ],
        "published": "2024-02-01T18:55:29Z",
        "summary": "Understanding context is key to understanding human language, an ability\nwhich Large Language Models (LLMs) have been increasingly seen to demonstrate\nto an impressive extent. However, though the evaluation of LLMs encompasses\nvarious domains within the realm of Natural Language Processing, limited\nattention has been paid to probing their linguistic capability of understanding\ncontextual features. This paper introduces a context understanding benchmark by\nadapting existing datasets to suit the evaluation of generative models. This\nbenchmark comprises of four distinct tasks and nine datasets, all featuring\nprompts designed to assess the models' ability to understand context. First, we\nevaluate the performance of LLMs under the in-context learning pretraining\nscenario. Experimental results indicate that pre-trained dense models struggle\nwith understanding more nuanced contextual features when compared to\nstate-of-the-art fine-tuned models. Second, as LLM compression holds growing\nsignificance in both research and real-world applications, we assess the\ncontext understanding of quantized models under in-context-learning settings.\nWe find that 3-bit post-training quantization leads to varying degrees of\nperformance reduction on our benchmark. We conduct an extensive analysis of\nthese scenarios to substantiate our experimental results.",
        "pdf_link": "https://arxiv.org/pdf/2402.00858v1.pdf"
    },
    {
        "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
        "authors": [
            "Silin Gao",
            "Jane Dwivedi-Yu",
            "Ping Yu",
            "Xiaoqing Ellen Tan",
            "Ramakanth Pasunuru",
            "Olga Golovneva",
            "Koustuv Sinha",
            "Asli Celikyilmaz",
            "Antoine Bosselut",
            "Tianlu Wang"
        ],
        "published": "2024-01-30T21:53:30Z",
        "summary": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.17464v2.pdf"
    },
    {
        "title": "Can Large Language Models Automatically Score Proficiency of Written Essays?",
        "authors": [
            "Watheq Mansour",
            "Salam Albatarni",
            "Sohaila Eltanbouly",
            "Tamer Elsayed"
        ],
        "published": "2024-03-10T09:39:00Z",
        "summary": "Although several methods were proposed to address the problem of automated\nessay scoring (AES) in the last 50 years, there is still much to desire in\nterms of effectiveness. Large Language Models (LLMs) are transformer-based\nmodels that demonstrate extraordinary capabilities on various tasks. In this\npaper, we test the ability of LLMs, given their powerful linguistic knowledge,\nto analyze and effectively score written essays. We experimented with two\npopular LLMs, namely ChatGPT and Llama. We aim to check if these models can do\nthis task and, if so, how their performance is positioned among the\nstate-of-the-art (SOTA) models across two levels, holistically and per\nindividual writing trait. We utilized prompt-engineering tactics in designing\nfour different prompts to bring their maximum potential to this task. Our\nexperiments conducted on the ASAP dataset revealed several interesting\nobservations. First, choosing the right prompt depends highly on the model and\nnature of the task. Second, the two LLMs exhibited comparable average\nperformance in AES, with a slight advantage for ChatGPT. Finally, despite the\nperformance gap between the two LLMs and SOTA models in terms of predictions,\nthey provide feedback to enhance the quality of the essays, which can\npotentially help both teachers and students.",
        "pdf_link": "https://arxiv.org/pdf/2403.06149v1.pdf"
    },
    {
        "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
        "authors": [
            "Gantavya Bhatt",
            "Yifang Chen",
            "Arnav M. Das",
            "Jifan Zhang",
            "Sang T. Truong",
            "Stephen Mussmann",
            "Yinglun Zhu",
            "Jeffrey Bilmes",
            "Simon S. Du",
            "Kevin Jamieson",
            "Jordan T. Ash",
            "Robert D. Nowak"
        ],
        "published": "2024-01-12T16:56:54Z",
        "summary": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
        "pdf_link": "https://arxiv.org/pdf/2401.06692v1.pdf"
    },
    {
        "title": "Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",
        "authors": [
            "Aashish Ghimire",
            "James Prather",
            "John Edwards"
        ],
        "published": "2024-03-22T19:21:29Z",
        "summary": "The rapid advancement of artificial intelligence (AI) and the expanding\nintegration of large language models (LLMs) have ignited a debate about their\napplication in education. This study delves into university instructors'\nexperiences and attitudes toward AI language models, filling a gap in the\nliterature by analyzing educators' perspectives on AI's role in the classroom\nand its potential impacts on teaching and learning. The objective of this\nresearch is to investigate the level of awareness, overall sentiment\ntowardsadoption, and the factors influencing these attitudes for LLMs and\ngenerative AI-based tools in higher education. Data was collected through a\nsurvey using a Likert scale, which was complemented by follow-up interviews to\ngain a more nuanced understanding of the instructors' viewpoints. The collected\ndata was processed using statistical and thematic analysis techniques. Our\nfindings reveal that educators are increasingly aware of and generally positive\ntowards these tools. We find no correlation between teaching style and attitude\ntoward generative AI. Finally, while CS educators show far more confidence in\ntheir technical understanding of generative AI tools and more positivity\ntowards them than educators in other fields, they show no more confidence in\ntheir ability to detect AI-generated work.",
        "pdf_link": "https://arxiv.org/pdf/2403.15586v1.pdf"
    },
    {
        "title": "Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models",
        "authors": [
            "Minbyul Jeong",
            "Jiwoong Sohn",
            "Mujeen Sung",
            "Jaewoo Kang"
        ],
        "published": "2024-01-27T02:29:42Z",
        "summary": "Recent proprietary large language models (LLMs), such as GPT-4, have achieved\na milestone in tackling diverse challenges in the biomedical domain, ranging\nfrom multiple-choice questions to long-form generations. To address challenges\nthat still cannot be handled with the encoded knowledge of LLMs, various\nretrieval-augmented generation (RAG) methods have been developed by searching\ndocuments from the knowledge corpus and appending them unconditionally or\nselectively to the input of LLMs for generation. However, when applying\nexisting methods to different domain-specific problems, poor generalization\nbecomes apparent, leading to fetching incorrect documents or making inaccurate\njudgments. In this paper, we introduce Self-BioRAG, a framework reliable for\nbiomedical text that specializes in generating explanations, retrieving\ndomain-specific documents, and self-reflecting generated responses. We utilize\n84k filtered biomedical instruction sets to train Self-BioRAG that can assess\nits generated explanations with customized reflective tokens. Our work proves\nthat domain-specific components, such as a retriever, domain-related document\ncorpus, and instruction sets are necessary for adhering to domain-related\ninstructions. Using three major medical question-answering benchmark datasets,\nexperimental results of Self-BioRAG demonstrate significant performance gains\nby achieving a 7.2% absolute improvement on average over the state-of-the-art\nopen-foundation model with a parameter size of 7B or less. Overall, we analyze\nthat Self-BioRAG finds the clues in the question, retrieves relevant documents\nif needed, and understands how to answer with information from retrieved\ndocuments and encoded knowledge as a medical expert does. We release our data\nand code for training our framework components and model weights (7B and 13B)\nto enhance capabilities in biomedical and clinical domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.15269v2.pdf"
    },
    {
        "title": "Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop",
        "authors": [
            "Maryam Amirizaniani",
            "Jihan Yao",
            "Adrian Lavergne",
            "Elizabeth Snell Okada",
            "Aman Chadha",
            "Tanya Roosta",
            "Chirag Shah"
        ],
        "published": "2024-02-14T17:49:31Z",
        "summary": "As LLMs become more pervasive across various users and scenarios, identifying\npotential issues when using these models becomes essential. Examples include\nbias, inconsistencies, and hallucination. Although auditing the LLM for these\nproblems is desirable, it is far from being easy or solved. An effective method\nis to probe the LLM using different versions of the same question. This could\nexpose inconsistencies in its knowledge or operation, indicating potential for\nbias or hallucination. However, to operationalize this auditing method at\nscale, we need an approach to create those probes reliably and automatically.\nIn this paper we propose an automatic and scalable solution, where one uses a\ndifferent LLM along with human-in-the-loop. This approach offers verifiability\nand transparency, while avoiding circular reliance on the same LLMs, and\nincreasing scientific rigor and generalizability. Specifically, we present a\nnovel methodology with two phases of verification using humans: standardized\nevaluation criteria to verify responses, and a structured prompt template to\ngenerate desired probes. Experiments on a set of questions from TruthfulQA\ndataset show that we can generate a reliable set of probes from one LLM that\ncan be used to audit inconsistencies in a different LLM. The criteria for\ngenerating and applying auditing probes is generalizable to various LLMs\nregardless of the underlying structure or training mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2402.09346v2.pdf"
    },
    {
        "title": "OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models",
        "authors": [
            "Haomin Wen",
            "Zhenjie Wei",
            "Yan Lin",
            "Jiyuan Wang",
            "Yuxuan Liang",
            "Huaiyu Wan"
        ],
        "published": "2024-03-13T07:52:31Z",
        "summary": "The rapid development of Large Language Models (LLMs) has facilitated a\nvariety of applications from different domains. In this technical report, we\nexplore the integration of LLMs and the popular academic writing tool,\nOverleaf, to enhance the efficiency and quality of academic writing. To achieve\nthe above goal, there are three challenges: i) including seamless interaction\nbetween Overleaf and LLMs, ii) establishing reliable communication with the LLM\nprovider, and iii) ensuring user privacy. To address these challenges, we\npresent OverleafCopilot, the first-ever tool (i.e., a browser extension) that\nseamlessly integrates LLMs and Overleaf, enabling researchers to leverage the\npower of LLMs while writing papers. Specifically, we first propose an effective\nframework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a\nwebsite for researchers to easily find and share high-quality up-to-date\nprompts. Thirdly, we propose an agent command system to help researchers\nquickly build their customizable agents. OverleafCopilot\n(https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb\n) has been on the Chrome Extension Store, which now serves thousands of\nresearchers. Additionally, the code of PromptGenius is released at\nhttps://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the\npotential to revolutionize academic writing practices, empowering researchers\nto produce higher-quality papers in less time.",
        "pdf_link": "https://arxiv.org/pdf/2403.09733v1.pdf"
    },
    {
        "title": "LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History",
        "authors": [
            "Akash Gupta",
            "Ivaxi Sheth",
            "Vyas Raina",
            "Mark Gales",
            "Mario Fritz"
        ],
        "published": "2024-02-28T10:19:05Z",
        "summary": "With the recent emergence of powerful instruction-tuned large language models\n(LLMs), various helpful conversational Artificial Intelligence (AI) systems\nhave been deployed across many applications. When prompted by users, these AI\nsystems successfully perform a wide range of tasks as part of a conversation.\nTo provide some sort of memory and context, such approaches typically condition\ntheir output on the entire conversational history. Although this sensitivity to\nthe conversational history can often lead to improved performance on subsequent\ntasks, we find that performance can in fact also be negatively impacted, if\nthere is a task-switch. To the best of our knowledge, our work makes the first\nattempt to formalize the study of such vulnerabilities and interference of\ntasks in conversational LLMs caused by task-switches in the conversational\nhistory. Our experiments across 5 datasets with 15 task switches using popular\nLLMs reveal that many of the task-switches can lead to significant performance\ndegradation.",
        "pdf_link": "https://arxiv.org/pdf/2402.18216v1.pdf"
    },
    {
        "title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
        "authors": [
            "Siyuan Guo",
            "Cheng Deng",
            "Ying Wen",
            "Hechang Chen",
            "Yi Chang",
            "Jun Wang"
        ],
        "published": "2024-02-27T12:26:07Z",
        "summary": "In this work, we investigate the potential of large language models (LLMs)\nbased agents to automate data science tasks, with the goal of comprehending\ntask requirements, then building and training the best-fit machine learning\nmodels. Despite their widespread success, existing LLM agents are hindered by\ngenerating unreasonable experiment plans within this scenario. To this end, we\npresent DS-Agent, a novel automatic framework that harnesses LLM agent and\ncase-based reasoning (CBR). In the development stage, DS-Agent follows the CBR\nframework to structure an automatic iteration pipeline, which can flexibly\ncapitalize on the expert knowledge from Kaggle, and facilitate consistent\nperformance improvement through the feedback mechanism. Moreover, DS-Agent\nimplements a low-resource deployment stage with a simplified CBR paradigm to\nadapt past successful solutions from the development stage for direct code\ngeneration, significantly reducing the demand on foundational capabilities of\nLLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success\nrate in the development stage, while attaining 36% improvement on average one\npass rate across alternative LLMs in the deployment stage. In both stages,\nDS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per\nrun with GPT-4, respectively. Our code is open-sourced at\nhttps://github.com/guosyjlu/DS-Agent.",
        "pdf_link": "https://arxiv.org/pdf/2402.17453v3.pdf"
    },
    {
        "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
        "authors": [
            "Jiaxiang Liu",
            "Tong Zhou",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "published": "2024-02-15T12:20:02Z",
        "summary": "Mitigating the hallucinations of Large Language Models (LLMs) and enhancing\nthem is a crucial task. Although some existing methods employ model\nself-enhancement techniques, they fall short of effectively addressing unknown\nfactual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails\nto address the generalization across different KG sources and the enhancement\nof open-ended answer questions simultaneously. To tackle these limitations,\nthere is a framework that combines Pseudo-Graph Generation and Atomic Knowledge\nVerification proposed. The enhancement of LLM using KG in an open-ended\nquestion-answering setting is implemented by leveraging the Pseudo-Graph\nGeneration. Atomic Knowledge Verification utilizes atomic-level knowledge\nquerying and verification to achieve generalizability under different KG\nsources. Compared to the baseline, this approach yields a minimum improvement\nof 11.5 in the ROUGE-L score for open-ended questions. For precise questions,\nwe observe a minimum accuracy improvement of 7.5. Moreover, there is also\ndemonstration that this framework exhibits generalizability across different KG\nsources. In summary, our results pave the way for enhancing LLMs by\nincorporating Pseudo- and Multisource-KGs, particularly in the context of\nopen-ended questions.",
        "pdf_link": "https://arxiv.org/pdf/2402.09911v1.pdf"
    },
    {
        "title": "Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding",
        "authors": [
            "Jie Tian",
            "Jixin Hou",
            "Zihao Wu",
            "Peng Shu",
            "Zhengliang Liu",
            "Yujie Xiang",
            "Beikang Gu",
            "Nicholas Filla",
            "Yiwei Li",
            "Ning Liu",
            "Xianyan Chen",
            "Keke Tang",
            "Tianming Liu",
            "Xianqiao Wang"
        ],
        "published": "2024-01-13T19:19:04Z",
        "summary": "This study is a pioneering endeavor to investigate the capabilities of Large\nLanguage Models (LLMs) in addressing conceptual questions within the domain of\nmechanical engineering with a focus on mechanics. Our examination involves a\nmanually crafted exam encompassing 126 multiple-choice questions, spanning\nvarious aspects of mechanics courses, including Fluid Mechanics, Mechanical\nVibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of\nElasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),\nChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against\nengineering faculties and students with or without mechanical engineering\nbackground. The findings reveal GPT-4's superior performance over the other two\nLLMs and human cohorts in answering questions across various mechanics topics,\nexcept for Continuum Mechanics. This signals the potential future improvements\nfor GPT models in handling symbolic calculations and tensor analyses. The\nperformances of LLMs were all significantly improved with explanations prompted\nprior to direct responses, underscoring the crucial role of prompt engineering.\nInterestingly, GPT-3.5 demonstrates improved performance with prompts covering\na broader domain, while GPT-4 excels with prompts focusing on specific\nsubjects. Finally, GPT-4 exhibits notable advancements in mitigating input\nbias, as evidenced by guessing preferences for humans. This study unveils the\nsubstantial potential of LLMs as highly knowledgeable assistants in both\nmechanical pedagogy and scientific research.",
        "pdf_link": "https://arxiv.org/pdf/2401.12983v1.pdf"
    },
    {
        "title": "xCoT: Cross-lingual Instruction Tuning for Cross-lingual Chain-of-Thought Reasoning",
        "authors": [
            "Linzheng Chai",
            "Jian Yang",
            "Tao Sun",
            "Hongcheng Guo",
            "Jiaheng Liu",
            "Bing Wang",
            "Xiannian Liang",
            "Jiaqi Bai",
            "Tongliang Li",
            "Qiyao Peng",
            "Zhoujun Li"
        ],
        "published": "2024-01-13T10:53:53Z",
        "summary": "Chain-of-thought (CoT) has emerged as a powerful technique to elicit\nreasoning in large language models and improve a variety of downstream tasks.\nCoT mainly demonstrates excellent performance in English, but its usage in\nlow-resource languages is constrained due to poor language generalization. To\nbridge the gap among different languages, we propose a cross-lingual\ninstruction fine-tuning framework (xCOT) to transfer knowledge from\nhigh-resource languages to low-resource languages. Specifically, the\nmultilingual instruction training data (xCOT-INSTRUCT) is created to encourage\nthe semantic alignment of multiple languages. We introduce cross-lingual\nin-context few-shot learning (xICL)) to accelerate multilingual agreement in\ninstruction tuning, where some fragments of source languages in examples are\nrandomly substituted by their counterpart translations of target languages.\nDuring multilingual instruction tuning, we adopt the randomly online CoT\nstrategy to enhance the multilingual reasoning ability of the large language\nmodel by first translating the query to another language and then answering in\nEnglish. To further facilitate the language transfer, we leverage the\nhigh-resource CoT to supervise the training of low-resource languages with\ncross-lingual distillation. Experimental results on previous benchmarks\ndemonstrate the superior performance of xCoT in reducing the gap among\ndifferent languages, highlighting its potential to reduce the cross-lingual\ngap.",
        "pdf_link": "https://arxiv.org/pdf/2401.07037v1.pdf"
    },
    {
        "title": "Large Language Models for Captioning and Retrieving Remote Sensing Images",
        "authors": [
            "Jo\u00e3o Daniel Silva",
            "Jo\u00e3o Magalh\u00e3es",
            "Devis Tuia",
            "Bruno Martins"
        ],
        "published": "2024-02-09T15:31:01Z",
        "summary": "Image captioning and cross-modal retrieval are examples of tasks that involve\nthe joint analysis of visual and linguistic information. In connection to\nremote sensing imagery, these tasks can help non-expert users in extracting\nrelevant Earth observation information for a variety of applications. Still,\ndespite some previous efforts, the development and application of vision and\nlanguage models to the remote sensing domain have been hindered by the\nrelatively small size of the available datasets and models used in previous\nstudies. In this work, we propose RS-CapRet, a Vision and Language method for\nremote sensing tasks, in particular image captioning and text-image retrieval.\nWe specifically propose to use a highly capable large decoder language model\ntogether with image encoders adapted to remote sensing imagery through\ncontrastive language-image pre-training. To bridge together the image encoder\nand language decoder, we propose training simple linear layers with examples\nfrom combining different remote sensing image captioning datasets, keeping the\nother parameters frozen. RS-CapRet can then generate descriptions for remote\nsensing images and retrieve images from textual descriptions, achieving SOTA or\ncompetitive performance with existing methods. Qualitative results illustrate\nthat RS-CapRet can effectively leverage the pre-trained large language model to\ndescribe remote sensing images, retrieve them based on different types of\nqueries, and also show the ability to process interleaved sequences of images\nand text in a dialogue manner.",
        "pdf_link": "https://arxiv.org/pdf/2402.06475v1.pdf"
    },
    {
        "title": "Large Language Model based Situational Dialogues for Second Language Learning",
        "authors": [
            "Shuyao Xu",
            "Long Qin",
            "Tianyang Chen",
            "Zhenzhou Zha",
            "Bingxue Qiu",
            "Weizhi Wang"
        ],
        "published": "2024-03-29T06:43:55Z",
        "summary": "In second language learning, scenario-based conversation practice is\nimportant for language learners to achieve fluency in speaking, but students\noften lack sufficient opportunities to practice their conversational skills\nwith qualified instructors or native speakers. To bridge this gap, we propose\nsituational dialogue models for students to engage in conversational practice.\nOur situational dialogue models are fine-tuned on large language models (LLMs),\nwith the aim of combining the engaging nature of an open-ended conversation\nwith the focused practice of scenario-based tasks. Leveraging the\ngeneralization capabilities of LLMs, we demonstrate that our situational\ndialogue models perform effectively not only on training topics but also on\ntopics not encountered during training. This offers a promising solution to\nsupport a wide range of conversational topics without extensive manual work.\nAdditionally, research in the field of dialogue systems still lacks reliable\nautomatic evaluation metrics, leading to human evaluation as the gold standard\n(Smith et al., 2022), which is typically expensive. To address the limitations\nof existing evaluation methods, we present a novel automatic evaluation method\nthat employs fine-tuned LLMs to efficiently and effectively assess the\nperformance of situational dialogue models.",
        "pdf_link": "https://arxiv.org/pdf/2403.20005v1.pdf"
    },
    {
        "title": "Generative Large Language Models are autonomous practitioners of evidence-based medicine",
        "authors": [
            "Akhil Vaid",
            "Joshua Lampert",
            "Juhee Lee",
            "Ashwin Sawant",
            "Donald Apakama",
            "Ankit Sakhuja",
            "Ali Soroush",
            "Denise Lee",
            "Isotta Landi",
            "Nicole Bussola",
            "Ismail Nabeel",
            "Robbie Freeman",
            "Patricia Kovatch",
            "Brendan Carr",
            "Benjamin Glicksberg",
            "Edgar Argulian",
            "Stamatios Lerakis",
            "Monica Kraft",
            "Alexander Charney",
            "Girish Nadkarni"
        ],
        "published": "2024-01-05T15:09:57Z",
        "summary": "Background: Evidence-based medicine (EBM) is fundamental to modern clinical\npractice, requiring clinicians to continually update their knowledge and apply\nthe best clinical evidence in patient care. The practice of EBM faces\nchallenges due to rapid advancements in medical research, leading to\ninformation overload for clinicians. The integration of artificial intelligence\n(AI), specifically Generative Large Language Models (LLMs), offers a promising\nsolution towards managing this complexity.\n  Methods: This study involved the curation of real-world clinical cases across\nvarious specialties, converting them into .json files for analysis. LLMs,\nincluding proprietary models like ChatGPT 3.5 and 4, Gemini Pro, and\nopen-source models like LLaMA v2 and Mixtral-8x7B, were employed. These models\nwere equipped with tools to retrieve information from case files and make\nclinical decisions similar to how clinicians must operate in the real world.\nModel performance was evaluated based on correctness of final answer, judicious\nuse of tools, conformity to guidelines, and resistance to hallucinations.\n  Results: GPT-4 was most capable of autonomous operation in a clinical\nsetting, being generally more effective in ordering relevant investigations and\nconforming to clinical guidelines. Limitations were observed in terms of model\nability to handle complex guidelines and diagnostic nuances. Retrieval\nAugmented Generation made recommendations more tailored to patients and\nhealthcare systems.\n  Conclusions: LLMs can be made to function as autonomous practitioners of\nevidence-based medicine. Their ability to utilize tooling can be harnessed to\ninteract with the infrastructure of a real-world healthcare system and perform\nthe tasks of patient management in a guideline directed manner. Prompt\nengineering may help to further enhance this potential and transform healthcare\nfor the clinician and the patient.",
        "pdf_link": "https://arxiv.org/pdf/2401.02851v1.pdf"
    },
    {
        "title": "Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering",
        "authors": [
            "Sungho Ko",
            "Hyunjin Cho",
            "Hyungjoo Chae",
            "Jinyoung Yeo",
            "Dongha Lee"
        ],
        "published": "2024-03-05T13:43:58Z",
        "summary": "Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance\nQuesetion Answering (QA) performance of Large Language Models (LLMs), yet\nstructured KG verbalization remains challengin. Existing methods, such as\ntriple-form or free-form textual conversion of triple-form facts, encounter\nseveral issues. These include reduced evidence density due to duplicated\nentities or relationships, and reduced evidence clarity due to an inability to\nemphasize crucial evidence. To address these issues, we propose EFSum, an\nEvidence-focused Fact Summarization framework for enhanced QA with\nknowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer\nthrough distillation and preference alignment. Our extensive experiments show\nthat EFSum improves LLM's zero-shot QA performance, and it is possible to\nensure both the helpfulness and faithfulness of the summary.",
        "pdf_link": "https://arxiv.org/pdf/2403.02966v1.pdf"
    },
    {
        "title": "Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment",
        "authors": [
            "Congzhi Zhang",
            "Linhai Zhang",
            "Deyu Zhou"
        ],
        "published": "2024-03-05T06:28:02Z",
        "summary": "Conventional multi-hop fact verification models are prone to rely on spurious\ncorrelations from the annotation artifacts, leading to an obvious performance\ndecline on unbiased datasets. Among the various debiasing works, the causal\ninference-based methods become popular by performing theoretically guaranteed\ndebiasing such as casual intervention or counterfactual reasoning. However,\nexisting causal inference-based debiasing methods, which mainly formulate fact\nverification as a single-hop reasoning task to tackle shallow bias patterns,\ncannot deal with the complicated bias patterns hidden in multiple hops of\nevidence. To address the challenge, we propose Causal Walk, a novel method for\ndebiasing multi-hop fact verification from a causal perspective with front-door\nadjustment. Specifically, in the structural causal model, the reasoning path\nbetween the treatment (the input claim-evidence graph) and the outcome (the\nveracity label) is introduced as the mediator to block the confounder. With the\nfront-door adjustment, the causal effect between the treatment and the outcome\nis decomposed into the causal effect between the treatment and the mediator,\nwhich is estimated by applying the idea of random walk, and the causal effect\nbetween the mediator and the outcome, which is estimated with normalized\nweighted geometric mean approximation. To investigate the effectiveness of the\nproposed method, an adversarial multi-hop fact verification dataset and a\nsymmetric multi-hop fact verification dataset are proposed with the help of the\nlarge language model. Experimental results show that Causal Walk outperforms\nsome previous debiasing methods on both existing datasets and the newly\nconstructed datasets. Code and data will be released at\nhttps://github.com/zcccccz/CausalWalk.",
        "pdf_link": "https://arxiv.org/pdf/2403.02698v1.pdf"
    },
    {
        "title": "SecGPT: An Execution Isolation Architecture for LLM-Based Systems",
        "authors": [
            "Yuhao Wu",
            "Franziska Roesner",
            "Tadayoshi Kohno",
            "Ning Zhang",
            "Umar Iqbal"
        ],
        "published": "2024-03-08T00:02:30Z",
        "summary": "Large language models (LLMs) extended as systems, such as ChatGPT, have begun\nsupporting third-party applications. These LLM apps leverage the de facto\nnatural language-based automated execution paradigm of LLMs: that is, apps and\ntheir interactions are defined in natural language, provided access to user\ndata, and allowed to freely interact with each other and the system. These LLM\napp ecosystems resemble the settings of earlier computing platforms, where\nthere was insufficient isolation between apps and the system. Because\nthird-party apps may not be trustworthy, and exacerbated by the imprecision of\nthe natural language interfaces, the current designs pose security and privacy\nrisks for users. In this paper, we propose SecGPT, an architecture for\nLLM-based systems that aims to mitigate the security and privacy issues that\narise with the execution of third-party apps. SecGPT's key idea is to isolate\nthe execution of apps and more precisely mediate their interactions outside of\ntheir isolated environments. We evaluate SecGPT against a number of case study\nattacks and demonstrate that it protects against many security, privacy, and\nsafety issues that exist in non-isolated LLM-based systems. The performance\noverhead incurred by SecGPT to improve security is under 0.3x for\nthree-quarters of the tested queries. To foster follow-up research, we release\nSecGPT's source code at https://github.com/llm-platform-security/SecGPT.",
        "pdf_link": "https://arxiv.org/pdf/2403.04960v1.pdf"
    },
    {
        "title": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",
        "authors": [
            "Hung Du",
            "Srikanth Thudumu",
            "Rajesh Vasa",
            "Kon Mouzakis"
        ],
        "published": "2024-02-03T00:27:22Z",
        "summary": "Research interest in autonomous agents is on the rise as an emerging topic.\nThe notable achievements of Large Language Models (LLMs) have demonstrated the\nconsiderable potential to attain human-like intelligence in autonomous agents.\nHowever, the challenge lies in enabling these agents to learn, reason, and\nnavigate uncertainties in dynamic environments. Context awareness emerges as a\npivotal element in fortifying multi-agent systems when dealing with dynamic\nsituations. Despite existing research focusing on both context-aware systems\nand multi-agent systems, there is a lack of comprehensive surveys outlining\ntechniques for integrating context-aware systems with multi-agent systems. To\naddress this gap, this survey provides a comprehensive overview of\nstate-of-the-art context-aware multi-agent systems. First, we outline the\nproperties of both context-aware systems and multi-agent systems that\nfacilitate integration between these systems. Subsequently, we propose a\ngeneral process for context-aware systems, with each phase of the process\nencompassing diverse approaches drawn from various application domains such as\ncollision avoidance in autonomous driving, disaster relief management, utility\nmanagement, supply chain management, human-AI interaction, and others. Finally,\nwe discuss the existing challenges of context-aware multi-agent systems and\nprovide future research directions in this field.",
        "pdf_link": "https://arxiv.org/pdf/2402.01968v1.pdf"
    },
    {
        "title": "StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis",
        "authors": [
            "Zecheng Tang",
            "Chenfei Wu",
            "Zekai Zhang",
            "Mingheng Ni",
            "Shengming Yin",
            "Yu Liu",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Zicheng Liu",
            "Juntao Li",
            "Nan Duan"
        ],
        "published": "2024-01-30T15:20:26Z",
        "summary": "To leverage LLMs for visual synthesis, traditional methods convert raster\nimage information into discrete grid tokens through specialized visual modules,\nwhile disrupting the model's ability to capture the true semantic\nrepresentation of visual scenes. This paper posits that an alternative\nrepresentation of images, vector graphics, can effectively surmount this\nlimitation by enabling a more natural and semantically coherent segmentation of\nthe image information. Thus, we introduce StrokeNUWA, a pioneering work\nexploring a better visual representation ''stroke tokens'' on vector graphics,\nwhich is inherently visual semantics rich, naturally compatible with LLMs, and\nhighly compressed. Equipped with stroke tokens, StrokeNUWA can significantly\nsurpass traditional LLM-based and optimization-based methods across various\nmetrics in the vector graphic generation task. Besides, StrokeNUWA achieves up\nto a 94x speedup in inference over the speed of prior methods with an\nexceptional SVG code compression ratio of 6.9%.",
        "pdf_link": "https://arxiv.org/pdf/2401.17093v1.pdf"
    },
    {
        "title": "QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning",
        "authors": [
            "Hossein Rajabzadeh",
            "Mojtaba Valipour",
            "Tianshu Zhu",
            "Marzieh Tahaei",
            "Hyock Ju Kwon",
            "Ali Ghodsi",
            "Boxing Chen",
            "Mehdi Rezagholizadeh"
        ],
        "published": "2024-02-16T05:42:17Z",
        "summary": "Finetuning large language models requires huge GPU memory, restricting the\nchoice to acquire Larger models. While the quantized version of the Low-Rank\nAdaptation technique, named QLoRA, significantly alleviates this issue, finding\nthe efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a\npre-defined rank and, therefore, cannot be reconfigured for its lower ranks\nwithout requiring further fine-tuning steps. This paper proposes QDyLoRA\n-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach\nfor dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to\nefficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables\nfine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one\nround of fine-tuning. Experimental results show that QDyLoRA is competitive to\nQLoRA and outperforms when employing its optimal rank.",
        "pdf_link": "https://arxiv.org/pdf/2402.10462v1.pdf"
    },
    {
        "title": "Beyond Hate Speech: NLP's Challenges and Opportunities in Uncovering Dehumanizing Language",
        "authors": [
            "Hezhao Zhang",
            "Lasana Harris",
            "Nafise Sadat Moosavi"
        ],
        "published": "2024-02-21T13:57:36Z",
        "summary": "Dehumanization, characterized as a subtle yet harmful manifestation of hate\nspeech, involves denying individuals of their human qualities and often results\nin violence against marginalized groups. Despite significant progress in\nNatural Language Processing across various domains, its application in\ndetecting dehumanizing language is limited, largely due to the scarcity of\npublicly available annotated data for this domain. This paper evaluates the\nperformance of cutting-edge NLP models, including GPT-4, GPT-3.5, and LLAMA-2,\nin identifying dehumanizing language. Our findings reveal that while these\nmodels demonstrate potential, achieving a 70\\% accuracy rate in distinguishing\ndehumanizing language from broader hate speech, they also display biases. They\nare over-sensitive in classifying other forms of hate speech as dehumanization\nfor a specific subset of target groups, while more frequently failing to\nidentify clear cases of dehumanization for other target groups. Moreover,\nleveraging one of the best-performing models, we automatically annotated a\nlarger dataset for training more accessible models. However, our findings\nindicate that these models currently do not meet the high-quality data\ngeneration threshold necessary for this task.",
        "pdf_link": "https://arxiv.org/pdf/2402.13818v1.pdf"
    },
    {
        "title": "Ghost Sentence: A Tool for Everyday Users to Copyright Data from Large Language Models",
        "authors": [
            "Shuai Zhao",
            "Linchao Zhu",
            "Ruijie Quan",
            "Yi Yang"
        ],
        "published": "2024-03-23T06:36:32Z",
        "summary": "Web user data plays a central role in the ecosystem of pre-trained large\nlanguage models (LLMs) and their fine-tuned variants. Billions of data are\ncrawled from the web and fed to LLMs. How can \\textit{\\textbf{everyday web\nusers}} confirm if LLMs misuse their data without permission? In this work, we\nsuggest that users repeatedly insert personal passphrases into their documents,\nenabling LLMs to memorize them. These concealed passphrases in user documents,\nreferred to as \\textit{ghost sentences}, once they are identified in the\ngenerated content of LLMs, users can be sure that their data is used for\ntraining. To explore the effectiveness and usage of this copyrighting tool, we\ndefine the \\textit{user training data identification} task with ghost\nsentences. Multiple datasets from various sources at different scales are\ncreated and tested with LLMs of different sizes. For evaluation, we introduce a\nlast $k$ words verification manner along with two metrics: document and user\nidentification accuracy. In the specific case of instruction tuning of a 3B\nLLaMA model, 11 out of 16 users with ghost sentences identify their data within\nthe generation content. These 16 users contribute 383 examples to $\\sim$1.8M\ntraining documents. For continuing pre-training of a 1.1B TinyLlama model, 61\nout of 64 users with ghost sentences identify their data within the LLM output.\nThese 64 users contribute 1156 examples to $\\sim$10M training documents.",
        "pdf_link": "https://arxiv.org/pdf/2403.15740v1.pdf"
    },
    {
        "title": "WildfireGPT: Tailored Large Language Model for Wildfire Analysis",
        "authors": [
            "Yangxinyu Xie",
            "Tanwi Mallick",
            "Joshua David Bergerson",
            "John K. Hutchison",
            "Duane R. Verner",
            "Jordan Branham",
            "M. Ross Alexander",
            "Robert B. Ross",
            "Yan Feng",
            "Leslie-Anne Levy",
            "Weijie Su"
        ],
        "published": "2024-02-12T18:41:55Z",
        "summary": "The recent advancement of large language models (LLMs) represents a\ntransformational capability at the frontier of artificial intelligence (AI) and\nmachine learning (ML). However, LLMs are generalized models, trained on\nextensive text corpus, and often struggle to provide context-specific\ninformation, particularly in areas requiring specialized knowledge such as\nwildfire details within the broader context of climate change. For\ndecision-makers and policymakers focused on wildfire resilience and adaptation,\nit is crucial to obtain responses that are not only precise but also\ndomain-specific, rather than generic. To that end, we developed WildfireGPT, a\nprototype LLM agent designed to transform user queries into actionable insights\non wildfire risks. We enrich WildfireGPT by providing additional context such\nas climate projections and scientific literature to ensure its information is\ncurrent, relevant, and scientifically accurate. This enables WildfireGPT to be\nan effective tool for delivering detailed, user-specific insights on wildfire\nrisks to support a diverse set of end users, including researchers, engineers,\nurban planners, emergency managers, and infrastructure operators.",
        "pdf_link": "https://arxiv.org/pdf/2402.07877v1.pdf"
    },
    {
        "title": "YODA: Teacher-Student Progressive Learning for Language Models",
        "authors": [
            "Jianqiao Lu",
            "Wanjun Zhong",
            "Yufei Wang",
            "Zhijiang Guo",
            "Qi Zhu",
            "Wenyong Huang",
            "Yanlin Wang",
            "Fei Mi",
            "Baojun Wang",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "published": "2024-01-28T14:32:15Z",
        "summary": "Although large language models (LLMs) have demonstrated adeptness in a range\nof tasks, they still lag behind human learning efficiency. This disparity is\noften linked to the inherent human capacity to learn from basic examples,\ngradually generalize and handle more complex problems, and refine their skills\nwith continuous feedback. Inspired by this, this paper introduces YODA, a novel\nteacher-student progressive learning framework that emulates the\nteacher-student education process to improve the efficacy of model fine-tuning.\nThe framework operates on an interactive \\textit{basic-generalized-harder}\nloop. The teacher agent provides tailored feedback on the student's answers,\nand systematically organizes the education process. This process unfolds by\nteaching the student basic examples, reinforcing understanding through\ngeneralized questions, and then enhancing learning by posing questions with\nprogressively enhanced complexity. With the teacher's guidance, the student\nlearns to iteratively refine its answer with feedback, and forms a robust and\ncomprehensive understanding of the posed questions. The systematic procedural\ndata, which reflects the progressive learning process of humans, is then\nutilized for model training. Taking math reasoning as a testbed, experiments\nshow that training LLaMA2 with data from YODA improves SFT with significant\nperformance gain (+17.01\\% on GSM8K and +9.98\\% on MATH). In addition, we find\nthat training with curriculum learning further improves learning robustness.",
        "pdf_link": "https://arxiv.org/pdf/2401.15670v1.pdf"
    },
    {
        "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
        "authors": [
            "Haiyang Wang",
            "Hao Tang",
            "Li Jiang",
            "Shaoshuai Shi",
            "Muhammad Ferjad Naeem",
            "Hongsheng Li",
            "Bernt Schiele",
            "Liwei Wang"
        ],
        "published": "2024-03-14T13:47:41Z",
        "summary": "This paper proposes a simple, yet effective framework, called GiT,\nsimultaneously applicable for various vision tasks only with a vanilla ViT.\nMotivated by the universality of the Multi-layer Transformer architecture (e.g,\nGPT) widely used in large language models (LLMs), we seek to broaden its scope\nto serve as a powerful vision foundation model (VFM). However, unlike language\nmodeling, visual tasks typically require specific modules, such as bounding box\nheads for detection and pixel decoders for segmentation, greatly hindering the\napplication of powerful multi-layer transformers in the vision domain. To solve\nthis, we design a universal language interface that empowers the successful\nauto-regressive decoding to adeptly unify various visual tasks, from\nimage-level understanding (e.g., captioning), over sparse perception (e.g.,\ndetection), to dense prediction (e.g., segmentation). Based on the above\ndesigns, the entire model is composed solely of a ViT, without any specific\nadditions, offering a remarkable architectural simplification. GiT is a\nmulti-task visual model, jointly trained across five representative benchmarks\nwithout task-specific fine-tuning. Interestingly, our GiT builds a new\nbenchmark in generalist performance, and fosters mutual enhancement across\ntasks, leading to significant improvements compared to isolated training. This\nreflects a similar impact observed in LLMs. Further enriching training with 27\ndatasets, GiT achieves strong zero-shot results over various tasks. Due to its\nsimple design, this paradigm holds promise for narrowing the architectural gap\nbetween vision and language. Code and models will be available at\n\\url{https://github.com/Haiyang-W/GiT}.",
        "pdf_link": "https://arxiv.org/pdf/2403.09394v1.pdf"
    },
    {
        "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
        "authors": [
            "Haotong Qin",
            "Xudong Ma",
            "Xingyu Zheng",
            "Xiaoyang Li",
            "Yang Zhang",
            "Shouda Liu",
            "Jie Luo",
            "Xianglong Liu",
            "Michele Magno"
        ],
        "published": "2024-02-08T06:53:31Z",
        "summary": "The LoRA-finetuning quantization of LLMs has been extensively studied to\nobtain accurate yet compact LLMs for deployment on resource-constrained\nhardware. However, existing methods cause the quantized LLM to severely degrade\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\nthrough information retention. The proposed IR-QLoRA mainly relies on two\ntechnologies derived from the perspective of unified information: (1)\nstatistics-based Information Calibration Quantization allows the quantized\nparameters of LLM to retain original information accurately; (2)\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\nrepresentation transformation with diverse information. Comprehensive\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\nimprovement on MMLU compared with the state-of-the-art methods. The significant\nperformance gain requires only a tiny 0.31% additional time consumption,\nrevealing the satisfactory efficiency of our IRQLoRA. We highlight that\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\nThe code is available at https://github.com/htqin/ir-qlora.",
        "pdf_link": "https://arxiv.org/pdf/2402.05445v1.pdf"
    },
    {
        "title": "Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models",
        "authors": [
            "Wenfeng Feng",
            "Chuzhan Hao",
            "Yuewei Zhang",
            "Yu Han",
            "Hao Wang"
        ],
        "published": "2024-03-06T03:33:48Z",
        "summary": "Instruction Tuning has the potential to stimulate or enhance specific\ncapabilities of large language models (LLMs). However, achieving the right\nbalance of data is crucial to prevent catastrophic forgetting and interference\nbetween tasks. To address these limitations and enhance training flexibility,\nwe propose the Mixture-of-LoRAs (MoA) architecture which is a novel and\nparameter-efficient tuning method designed for multi-task learning with LLMs.\nIn this paper, we start by individually training multiple domain-specific LoRA\nmodules using corresponding supervised corpus data. These LoRA modules can be\naligned with the expert design principles observed in Mixture-of-Experts (MoE).\nSubsequently, we combine the multiple LoRAs using an explicit routing strategy\nand introduce domain labels to facilitate multi-task learning, which help\nprevent interference between tasks and ultimately enhances the performance of\neach individual task. Furthermore, each LoRA model can be iteratively adapted\nto a new domain, allowing for quick domain-specific adaptation. Experiments on\ndiverse tasks demonstrate superior and robust performance, which can further\npromote the wide application of domain-specific LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2403.03432v1.pdf"
    },
    {
        "title": "Bangla AI: A Framework for Machine Translation Utilizing Large Language Models for Ethnic Media",
        "authors": [
            "MD Ashraful Goni",
            "Fahad Mostafa",
            "Kerk F. Kee"
        ],
        "published": "2024-02-21T23:43:04Z",
        "summary": "Ethnic media, which caters to diaspora communities in host nations, serves as\na vital platform for these communities to both produce content and access\ninformation. Rather than utilizing the language of the host nation, ethnic\nmedia delivers news in the language of the immigrant community. For instance,\nin the USA, Bangla ethnic media presents news in Bangla rather than English.\nThis research delves into the prospective integration of large language models\n(LLM) and multi-lingual machine translations (MMT) within the ethnic media\nindustry. It centers on the transformative potential of using LLM in MMT in\nvarious facets of news translation, searching, and categorization. The paper\noutlines a theoretical framework elucidating the integration of LLM and MMT\ninto the news searching and translation processes for ethnic media.\nAdditionally, it briefly addresses the potential ethical challenges associated\nwith the incorporation of LLM and MMT in news translation procedures.",
        "pdf_link": "https://arxiv.org/pdf/2402.14179v1.pdf"
    },
    {
        "title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish",
        "authors": [
            "Szymon Ruci\u0144ski"
        ],
        "published": "2024-02-15T07:17:10Z",
        "summary": "This study explores the potential of fine-tuning foundational English Large\nLanguage Models (LLMs) for generating Polish text. The first step involves\nLanguage Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB,\nconsisting of 276 million Polish tokens. The LAPT is followed by additional\nfine-tuning aimed at solving nine KLEJ challenges. Our trained model\nCurie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02\namong decoder-based Polish models but also closely rivals the performance of\nthe best Polish encoder-decoder models with a less than 2% gap on 8 out of 9\ntasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn\nPolish. The LAPT was completed in less than five days using a consumer GPU,\nhighlighting the method's efficiency. The proficiency of the model in Polish\nwas significantly enhanced, demonstrating the viability of this approach for\nadding new languages to existing LLMs by training just 1.2% of its parameters.\nTo contribute to the community's collaborative progress, the model has been\nreleased as open-source.",
        "pdf_link": "https://arxiv.org/pdf/2402.09759v1.pdf"
    },
    {
        "title": "Extracting Polymer Nanocomposite Samples from Full-Length Documents",
        "authors": [
            "Ghazal Khalighinejad",
            "Defne Circi",
            "L. C. Brinson",
            "Bhuwan Dhingra"
        ],
        "published": "2024-03-01T03:51:56Z",
        "summary": "This paper investigates the use of large language models (LLMs) for\nextracting sample lists of polymer nanocomposites (PNCs) from full-length\nmaterials science research papers. The challenge lies in the complex nature of\nPNC samples, which have numerous attributes scattered throughout the text. The\ncomplexity of annotating detailed information on PNCs limits the availability\nof data, making conventional document-level relation extraction techniques\nimpractical due to the challenge in creating comprehensive named entity span\nannotations. To address this, we introduce a new benchmark and an evaluation\ntechnique for this task and explore different prompting strategies in a\nzero-shot manner. We also incorporate self-consistency to improve the\nperformance. Our findings show that even advanced LLMs struggle to extract all\nof the samples from an article. Finally, we analyze the errors encountered in\nthis process, categorizing them into three main challenges, and discuss\npotential strategies for future research to overcome them.",
        "pdf_link": "https://arxiv.org/pdf/2403.00260v1.pdf"
    },
    {
        "title": "Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models",
        "authors": [
            "Jinbiao Yang"
        ],
        "published": "2024-03-01T10:03:07Z",
        "summary": "Tokenization significantly influences language models(LMs)' performance. This\npaper traces the evolution of tokenizers from word-level to subword-level,\nanalyzing how they balance tokens and types to enhance model adaptability while\ncontrolling complexity. Despite subword tokenizers like Byte Pair Encoding\n(BPE) overcoming many word tokenizer limitations, they encounter difficulties\nin handling non-Latin languages and depend heavily on extensive training data\nand computational resources to grasp the nuances of multiword expressions\n(MWEs). This article argues that tokenizers, more than mere technical tools,\nshould drawing inspiration from the cognitive science about human language\nprocessing. This study then introduces the \"Principle of Least Effort\" from\ncognitive science, that humans naturally seek to reduce cognitive effort, and\ndiscusses the benefits of this principle for tokenizer development. Based on\nthis principle, the paper proposes that the Less-is-Better (LiB) model could be\na new approach for LLM tokenizer. The LiB model can autonomously learn an\nintegrated vocabulary consisting of subwords, words, and MWEs, which\neffectively reduces both the numbers of tokens and types. Comparative\nevaluations show that the LiB tokenizer outperforms existing word and BPE\ntokenizers, presenting an innovative method for tokenizer development, and\nhinting at the possibility of future cognitive science-based tokenizers being\nmore efficient.",
        "pdf_link": "https://arxiv.org/pdf/2403.00417v1.pdf"
    },
    {
        "title": "Multi-line AI-assisted Code Authoring",
        "authors": [
            "Omer Dunay",
            "Daniel Cheng",
            "Adam Tait",
            "Parth Thakkar",
            "Peter C Rigby",
            "Andy Chiu",
            "Imad Ahmad",
            "Arun Ganesan",
            "Chandra Maddila",
            "Vijayaraghavan Murali",
            "Ali Tayyebi",
            "Nachiappan Nagappan"
        ],
        "published": "2024-02-06T16:48:50Z",
        "summary": "CodeCompose is an AI-assisted code authoring tool powered by large language\nmodels (LLMs) that provides inline suggestions to 10's of thousands of\ndevelopers at Meta. In this paper, we present how we scaled the product from\ndisplaying single-line suggestions to multi-line suggestions. This evolution\nrequired us to overcome several unique challenges in improving the usability of\nthese suggestions for developers.\n  First, we discuss how multi-line suggestions can have a 'jarring' effect, as\nthe LLM's suggestions constantly move around the developer's existing code,\nwhich would otherwise result in decreased productivity and satisfaction.\n  Second, multi-line suggestions take significantly longer to generate; hence\nwe present several innovative investments we made to reduce the perceived\nlatency for users. These model-hosting optimizations sped up multi-line\nsuggestion latency by 2.5x.\n  Finally, we conduct experiments on 10's of thousands of engineers to\nunderstand how multi-line suggestions impact the user experience and contrast\nthis with single-line suggestions. Our experiments reveal that (i) multi-line\nsuggestions account for 42% of total characters accepted (despite only\naccounting for 16% for displayed suggestions) (ii) multi-line suggestions\nalmost doubled the percentage of keystrokes saved for users from 9% to 17%.\nMulti-line CodeCompose has been rolled out to all engineers at Meta, and less\nthan 1% of engineers have opted out of multi-line suggestions.",
        "pdf_link": "https://arxiv.org/pdf/2402.04141v1.pdf"
    },
    {
        "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models",
        "authors": [
            "Ziyi Guan",
            "Hantao Huang",
            "Yupeng Su",
            "Hong Huang",
            "Ngai Wong",
            "Hao Yu"
        ],
        "published": "2024-02-21T07:45:22Z",
        "summary": "Large Language Models (LLMs) have greatly advanced the natural language\nprocessing paradigm. However, the high computational load and huge model sizes\npose a grand challenge for deployment on edge devices. To this end, we propose\nAPTQ (Attention-aware Post-Training Mixed-Precision Quantization) for LLMs,\nwhich considers not only the second-order information of each layer's weights,\nbut also, for the first time, the nonlinear effect of attention outputs on the\nentire model. We leverage the Hessian trace as a sensitivity metric for\nmixed-precision quantization, ensuring an informed precision reduction that\nretains model performance. Experiments show APTQ surpasses previous\nquantization methods, achieving an average of 4 bit width a 5.22 perplexity\nnearly equivalent to full precision in the C4 dataset. In addition, APTQ\nattains state-of-the-art zero-shot accuracy of 68.24\\% and 70.48\\% at an\naverage bitwidth of 3.8 in LLaMa-7B and LLaMa-13B, respectively, demonstrating\nits effectiveness to produce high-quality quantized LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.14866v1.pdf"
    },
    {
        "title": "Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive Data Analysis Agents",
        "authors": [
            "Jinyang Li",
            "Nan Huo",
            "Yan Gao",
            "Jiayi Shi",
            "Yingxiu Zhao",
            "Ge Qu",
            "Yurong Wu",
            "Chenhao Ma",
            "Jian-Guang Lou",
            "Reynold Cheng"
        ],
        "published": "2024-03-08T13:34:20Z",
        "summary": "Interactive Data Analysis, the collaboration between humans and LLM agents,\nenables real-time data exploration for informed decision-making. The challenges\nand costs of collecting realistic interactive logs for data analysis hinder the\nquantitative evaluation of Large Language Model (LLM) agents in this task. To\nmitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate\nLLM agents on interactive data analysis. Tapilot-Crossing contains 1024\ninteractions, covering 4 practical scenarios: Normal, Action, Private, and\nPrivate Action. Notably, Tapilot-Crossing is constructed by an economical\nmulti-agent environment, Decision Company, with few human efforts. We evaluate\npopular and advanced LLM agents in Tapilot-Crossing, which underscores the\nchallenges of interactive data analysis. Furthermore, we propose Adaptive\nInteraction Reflection (AIR), a self-generated reflection strategy that guides\nLLM agents to learn from successful history. Experiments demonstrate that Air\ncan evolve LLMs into effective interactive data analysis agents, achieving a\nrelative performance improvement of up to 44.5%.",
        "pdf_link": "https://arxiv.org/pdf/2403.05307v1.pdf"
    },
    {
        "title": "ChemDFM: Dialogue Foundation Model for Chemistry",
        "authors": [
            "Zihan Zhao",
            "Da Ma",
            "Lu Chen",
            "Liangtai Sun",
            "Zihao Li",
            "Hongshen Xu",
            "Zichen Zhu",
            "Su Zhu",
            "Shuai Fan",
            "Guodong Shen",
            "Xin Chen",
            "Kai Yu"
        ],
        "published": "2024-01-26T12:45:55Z",
        "summary": "Large language models (LLMs) have established great success in the general\ndomain of natural language processing. Their emerging task generalization and\nfree-form dialogue capabilities can greatly help to design Chemical General\nIntelligence (CGI) to assist real-world research in chemistry. However, the\nexistence of specialized language and knowledge in the field of chemistry, such\nas the highly informative SMILES notation, hinders the performance of\ngeneral-domain LLMs in chemistry. To this end, we develop ChemDFM, the first\nLLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,\ntextbooks, and instructions as well as various data from the general domain.\nTherefore, it can store, understand, and reason over chemical knowledge and\nlanguages while still possessing advanced free-form language comprehension\ncapabilities. Extensive quantitative evaluation shows that ChemDFM can\nsignificantly outperform the representative open-sourced LLMs. Moreover,\nChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite\nthe significant size difference. Further qualitative evaluations demonstrate\nthe efficiency and effectiveness of ChemDFM in real-world research scenarios.\nWe will open-source the ChemDFM model soon.",
        "pdf_link": "https://arxiv.org/pdf/2401.14818v1.pdf"
    },
    {
        "title": "LLM-Oriented Retrieval Tuner",
        "authors": [
            "Si Sun",
            "Hanqing Zhang",
            "Zhiyuan Liu",
            "Jie Bao",
            "Dawei Song"
        ],
        "published": "2024-03-04T12:50:25Z",
        "summary": "Dense Retrieval (DR) is now considered as a promising tool to enhance the\nmemorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by\nincorporating external memories. However, due to the paradigm discrepancy\nbetween text generation of LLM and DR, it is still an open challenge to\nintegrate the retrieval and generation tasks in a shared LLM. In this paper, we\npropose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which\ndecouples DR capacity from base LLM and non-invasively coordinates the\noptimally aligned and uniform layers of the LLM towards a unified DR space,\nachieving an efficient and effective DR without tuning the LLM itself. The\nextensive experiments on six BEIR datasets show that our approach could achieve\ncompetitive zero-shot retrieval performance compared to a range of strong DR\nmodels while maintaining the generation ability of LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.01999v1.pdf"
    },
    {
        "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning",
        "authors": [
            "Yanda Chen",
            "Chandan Singh",
            "Xiaodong Liu",
            "Simiao Zuo",
            "Bin Yu",
            "He He",
            "Jianfeng Gao"
        ],
        "published": "2024-01-25T07:04:30Z",
        "summary": "Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .",
        "pdf_link": "https://arxiv.org/pdf/2401.13986v1.pdf"
    },
    {
        "title": "CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs",
        "authors": [
            "Jingzhe Shi",
            "Jialuo Li",
            "Qinwei Ma",
            "Zaiwen Yang",
            "Huan Ma",
            "Lei Li"
        ],
        "published": "2024-03-31T07:11:48Z",
        "summary": "Businesses and software platforms are increasingly turning to Large Language\nModels (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance\nwith file access or as reasoning agents for customer service. However, current\nLLM-based customer service models have limited integration with customer\nprofiles and lack the operational capabilities necessary for effective service.\nMoreover, existing API integrations emphasize diversity over the precision and\nerror avoidance essential in real-world customer service scenarios. To address\nthese issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile\nin existing System), designed to: (1) efficiently utilize existing databases or\nsystems for accessing user information or interacting with these systems\nfollowing existing guidelines; (2) provide accurate and reasonable responses or\ncarry out required operations in the system while avoiding harmful operations;\nand (3) leverage a combination of small and large LLMs to achieve satisfying\nperformance at a reasonable inference cost. We introduce a practical dataset,\nthe CPHOS-dataset, which includes a database, guiding files, and QA pairs\ncollected from CPHOS, an online platform that facilitates the organization of\nsimulated Physics Olympiads for high school teachers and students. We have\nconducted extensive experiments to validate the performance of our proposed\nCHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how\nLLMs can enhance or serve as alternatives to human customer service. Our code\nand dataset will be open-sourced soon.",
        "pdf_link": "https://arxiv.org/pdf/2404.01343v1.pdf"
    },
    {
        "title": "Counting-Stars: A Simple, Efficient, and Reasonable Strategy for Evaluating Long-Context Large Language Models",
        "authors": [
            "Mingyang Song",
            "Mao Zheng",
            "Xuan Luo"
        ],
        "published": "2024-03-18T14:01:45Z",
        "summary": "While recent research endeavors have concentrated on developing Large\nLanguage Models (LLMs) with robust long-context capabilities, due to the lack\nof appropriate evaluation strategies, relatively little is known about how well\nthe long-context capability and performance of leading LLMs (e.g., GPT-4 Turbo\nand Kimi Chat). To address this gap, we propose a simple, efficient, and\nreasonable strategy for evaluating long-context LLMs as a new benchmark, named\nCounting-Stars. The Counting-Stars is designed to require LLMs to fully\nunderstand and capture long dependencies in long contexts, further being able\nto collect inter-dependency across multiple pieces of evidence spanning the\nentire context to finish the task. Based on the Counting-Stars, we conduct\nexperiments to evaluate the two leading long-context LLMs, i.e., GPT-4 Turbo\nand Kimi Chat. The experimental results indicate that GPT-4 Turbo and Kimi Chat\nachieve significant performance in the long context from 4K to 128K. We further\npresent several intriguing analyses regarding the behavior of LLMs processing\nlong context.",
        "pdf_link": "https://arxiv.org/pdf/2403.11802v2.pdf"
    },
    {
        "title": "Entity-Aware Multimodal Alignment Framework for News Image Captioning",
        "authors": [
            "Junzhe Zhang",
            "Huixuan Zhang",
            "Xiaojun Wan"
        ],
        "published": "2024-02-29T18:03:00Z",
        "summary": "News image captioning task is a variant of image captioning task which\nrequires model to generate a more informative caption with news image and the\nassociated news article. Multimodal Large Language models have developed\nrapidly in recent years and is promising in news image captioning task.\nHowever, according to our experiments, common MLLMs are not good at generating\nthe entities in zero-shot setting. Their abilities to deal with the entities\ninformation are still limited after simply fine-tuned on news image captioning\ndataset. To obtain a more powerful model to handle the multimodal entity\ninformation, we design two multimodal entity-aware alignment tasks and an\nalignment framework to align the model and generate the news image captions.\nOur method achieves better results than previous state-of-the-art models in\nCIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on\nNYTimes800k dataset.",
        "pdf_link": "https://arxiv.org/pdf/2402.19404v1.pdf"
    },
    {
        "title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization",
        "authors": [
            "Prakamya Mishra",
            "Zonghai Yao",
            "Parth Vashisht",
            "Feiyun Ouyang",
            "Beining Wang",
            "Vidhi Dhaval Mody",
            "Hong Yu"
        ],
        "published": "2024-02-21T16:33:22Z",
        "summary": "Large Language Models (LLMs) such as GPT and Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate\nhigh-quality feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback, mirroring the\npractical scenario in which medical professionals refine AI system outputs\nwithout the need for additional annotations. Despite GPT's proven expertise in\nvarious clinical NLP tasks, such as the Medical Licensing Examination, there is\nscant research on its capacity to deliver expert-level edit feedback for\nimproving weaker LMs or LLMs generation quality. This work leverages GPT's\nadvanced capabilities in clinical NLP to offer expert-level edit feedback.\nThrough the use of two distinct alignment algorithms (DPO and SALT) based on\nGPT edit feedback, our goal is to reduce hallucinations and align closely with\nmedical facts, endeavoring to narrow the divide between AI-generated content\nand factual accuracy. This highlights the substantial potential of GPT edits in\nenhancing the alignment of clinical factuality.",
        "pdf_link": "https://arxiv.org/pdf/2402.13919v1.pdf"
    },
    {
        "title": "Best Arm Identification for Prompt Learning under a Limited Budget",
        "authors": [
            "Chengshuai Shi",
            "Kun Yang",
            "Jing Yang",
            "Cong Shen"
        ],
        "published": "2024-02-15T05:31:13Z",
        "summary": "The remarkable instruction-following capability of large language models\n(LLMs) has sparked a growing interest in automatically learning suitable\nprompts. However, while many effective methods have been proposed, the cost\nincurred during the learning process (e.g., accessing LLM and evaluating the\nresponses) has not been considered. To overcome this limitation, this work\nexplicitly incorporates a finite budget constraint into prompt learning.\nTowards developing principled solutions, a novel connection is established\nbetween prompt learning and fixed-budget best arm identification (BAI-FB) in\nmulti-armed bandits (MAB). Based on this connection, a general framework TRIPLE\n(besT aRm Identification for Prompt LEarning) is proposed to harness the power\nof BAI-FB in prompt learning systematically. Unique characteristics of prompt\nlearning further lead to two embedding-based enhancements of TRIPLE by\nexploiting the ideas of clustering and function approximation. Extensive\nexperiments on multiple well-adopted tasks using both GPT 3.5 and Llama2\ndemonstrate the significant performance improvement of TRIPLE over the previous\nbaselines while satisfying the limited budget constraints.",
        "pdf_link": "https://arxiv.org/pdf/2402.09723v2.pdf"
    },
    {
        "title": "Harnessing Large Language Models Over Transformer Models for Detecting Bengali Depressive Social Media Text: A Comprehensive Study",
        "authors": [
            "Ahmadul Karim Chowdhury",
            "Md. Saidur Rahman Sujon",
            "Md. Shirajus Salekin Shafi",
            "Tasin Ahmmad",
            "Sifat Ahmed",
            "Khan Md Hasib",
            "Faisal Muhammad Shah"
        ],
        "published": "2024-01-14T15:15:58Z",
        "summary": "In an era where the silent struggle of underdiagnosed depression pervades\nglobally, our research delves into the crucial link between mental health and\nsocial media. This work focuses on early detection of depression, particularly\nin extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our\nproposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning\nmodels(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT,\nSahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into\n\"Depressive\" and \"Non-Depressive\" segments, translated into Bengali by native\nspeakers with expertise in mental health, resulting in the creation of the\nBengali Social Media Depressive Dataset (BSMDD). Our work provides full\narchitecture details for each model and a methodical way to assess their\nperformance in Bengali depressive text categorization using zero-shot and\nfew-shot learning techniques. Our work demonstrates the superiority of\nSahajBERT and Bi-LSTM with FastText embeddings in their respective domains also\ntackles explainability issues with transformer models and emphasizes the\neffectiveness of LLMs, especially DepGPT, demonstrating flexibility and\ncompetence in a range of learning contexts. According to the experiment\nresults, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in\nzero-shot and few-shot scenarios but also every other model, achieving a\nnear-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and\nexceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B\nshow relatively poorer effectiveness in zero-shot and few-shot situations. The\nwork emphasizes the effectiveness and flexibility of LLMs in a variety of\nlinguistic circumstances, providing insightful information about the complex\nfield of depression detection models.",
        "pdf_link": "https://arxiv.org/pdf/2401.07310v1.pdf"
    },
    {
        "title": "Large Language Models Can Learn Temporal Reasoning",
        "authors": [
            "Siheng Xiong",
            "Ali Payani",
            "Ramana Kompella",
            "Faramarz Fekri"
        ],
        "published": "2024-01-12T19:00:26Z",
        "summary": "While large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they are not without their flaws and inaccuracies. Recent studies\nhave introduced various methods to mitigate these limitations. Temporal\nreasoning (TR), in particular, presents a significant challenge for LLMs due to\nits reliance on diverse temporal expressions and intricate contextual details.\nIn this paper, we propose TG-LLM, a new framework towards language-based TR. To\nbe specific, we first teach LLM to translate the context into a temporal graph\n(TG). A synthetic dataset, which is fully controllable and requires minimal\nsupervision, is constructed for fine-tuning on this graph translation task. We\nconfirm in experiments that the capability of TG extraction learned on our\ndataset can be transferred to other TR tasks and benchmarks. On top of that, we\nguide LLM to perform symbolic reasoning over the TG via Chain of Thoughts\n(CoTs) bootstrapping and special data augmentation strategies. We observe that\nCoTs with symbolic reasoning bring more consistent and reliable results than\nthose using free-form text.",
        "pdf_link": "https://arxiv.org/pdf/2401.06853v2.pdf"
    },
    {
        "title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
        "authors": [
            "Ron Sun"
        ],
        "published": "2024-01-19T01:14:45Z",
        "summary": "The paper discusses what is needed to address the limitations of current\nLLM-centered AI systems. The paper argues that incorporating insights from\nhuman cognition and psychology, as embodied by a computational cognitive\narchitecture, can help develop systems that are more capable, more reliable,\nand more human-like. It emphasizes the importance of the dual-process\narchitecture and the hybrid neuro-symbolic approach in addressing the\nlimitations of current LLMs. In the opposite direction, the paper also\nhighlights the need for an overhaul of computational cognitive architectures to\nbetter reflect advances in AI and computing technology. Overall, the paper\nadvocates for a multidisciplinary, mutually beneficial approach towards\ndeveloping better models both for AI and for understanding the human mind.",
        "pdf_link": "https://arxiv.org/pdf/2401.10444v1.pdf"
    },
    {
        "title": "Building Guardrails for Large Language Models",
        "authors": [
            "Yi Dong",
            "Ronghui Mu",
            "Gaojie Jin",
            "Yi Qi",
            "Jinwei Hu",
            "Xingyu Zhao",
            "Jie Meng",
            "Wenjie Ruan",
            "Xiaowei Huang"
        ],
        "published": "2024-02-02T16:35:00Z",
        "summary": "As Large Language Models (LLMs) become more integrated into our daily lives,\nit is crucial to identify and mitigate their risks, especially when the risks\ncan have profound impacts on human users and societies. Guardrails, which\nfilter the inputs or outputs of LLMs, have emerged as a core safeguarding\ntechnology. This position paper takes a deep look at current open-source\nsolutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the\nchallenges and the road towards building more complete solutions. Drawing on\nrobust evidence from previous research, we advocate for a systematic approach\nto construct guardrails for LLMs, based on comprehensive consideration of\ndiverse contexts across various LLMs applications. We propose employing\nsocio-technical methods through collaboration with a multi-disciplinary team to\npinpoint precise technical requirements, exploring advanced neural-symbolic\nimplementations to embrace the complexity of the requirements, and developing\nverification and testing to ensure the utmost quality of the final product.",
        "pdf_link": "https://arxiv.org/pdf/2402.01822v1.pdf"
    },
    {
        "title": "LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition",
        "authors": [
            "Jinyuan Li",
            "Han Li",
            "Di Sun",
            "Jiahao Wang",
            "Wenkun Zhang",
            "Zan Wang",
            "Gang Pan"
        ],
        "published": "2024-02-15T14:54:33Z",
        "summary": "Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal\ntask that aims to identify named entities, entity types and their corresponding\nvisual regions. GMNER task exhibits two challenging properties: 1) The weak\ncorrelation between image-text pairs in social media results in a significant\nportion of named entities being ungroundable. 2) There exists a distinction\nbetween coarse-grained referring expressions commonly used in similar tasks\n(e.g., phrase localization, referring expression comprehension) and\nfine-grained named entities. In this paper, we propose RiVEG, a unified\nframework that reformulates GMNER into a joint MNER-VE-VG task by leveraging\nlarge language models (LLMs) as a connecting bridge. This reformulation brings\ntwo benefits: 1) It maintains the optimal MNER performance and eliminates the\nneed for employing object detection methods to pre-extract regional features,\nthereby naturally addressing two major limitations of existing GMNER methods.\n2) The introduction of entity expansion expression and Visual Entailment (VE)\nModule unifies Visual Grounding (VG) and Entity Grounding (EG). It enables\nRiVEG to effortlessly inherit the Visual Entailment and Visual Grounding\ncapabilities of any current or prospective multimodal pretraining models.\nExtensive experiments demonstrate that RiVEG outperforms state-of-the-art\nmethods on the existing GMNER dataset and achieves absolute leads of 10.65%,\n6.21%, and 8.83% in all three subtasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.09989v3.pdf"
    },
    {
        "title": "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks",
        "authors": [
            "Hanqing Wang",
            "Bowen Ping",
            "Shuo Wang",
            "Xu Han",
            "Yun Chen",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-18T04:41:25Z",
        "summary": "LoRA employs lightweight modules to customize large language models (LLMs)\nfor each downstream task or domain, where different learned additional modules\nrepresent diverse skills. Combining existing LoRAs to address new tasks can\nenhance the reusability of learned LoRAs, particularly beneficial for tasks\nwith limited annotated data. Most prior works on LoRA combination primarily\nrely on task-level weights for each involved LoRA, making different examples\nand tokens share the same LoRA weights. However, in generative tasks, different\ntokens may necessitate diverse skills to manage. Taking the Chinese math task\nas an example, understanding the problem description may depend more on the\nChinese LoRA, while the calculation part may rely more on the math LoRA. To\nthis end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the\nimpact of different LoRAs. The weights at each step are determined by a fusion\ngate with extremely few parameters, which can be learned with only 200 training\nexamples. Experiments across six generative tasks demonstrate that our method\nconsistently outperforms baselines with task-level fusion weights. This\nunderscores the necessity of introducing dynamic fusion weights for LoRA\ncombination.",
        "pdf_link": "https://arxiv.org/pdf/2402.11455v1.pdf"
    },
    {
        "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
        "authors": [
            "Haiyan Zhao",
            "Fan Yang",
            "Himabindu Lakkaraju",
            "Mengnan Du"
        ],
        "published": "2024-02-16T13:46:06Z",
        "summary": "As large language models (LLMs) grow more powerful, concerns around potential\nharms like toxicity, unfairness, and hallucination threaten user trust.\nEnsuring beneficial alignment of LLMs with human values through model alignment\nis thus critical yet challenging, requiring a deeper understanding of LLM\nbehaviors and mechanisms. We propose opening the black box of LLMs through a\nframework of holistic interpretability encompassing complementary bottom-up and\ntop-down perspectives. The bottom-up view, enabled by mechanistic\ninterpretability, focuses on component functionalities and training dynamics.\nThe top-down view utilizes representation engineering to analyze behaviors\nthrough hidden representations. In this paper, we review the landscape around\nmechanistic interpretability and representation engineering, summarizing\napproaches, discussing limitations and applications, and outlining future\nchallenges in using these techniques to achieve ethical, honest, and reliable\nreasoning aligned with human values.",
        "pdf_link": "https://arxiv.org/pdf/2402.10688v1.pdf"
    },
    {
        "title": "Is it Possible to Edit Large Language Models Robustly?",
        "authors": [
            "Xinbei Ma",
            "Tianjie Ju",
            "Jiyang Qiu",
            "Zhuosheng Zhang",
            "Hai Zhao",
            "Lifeng Liu",
            "Yulong Wang"
        ],
        "published": "2024-02-08T17:06:45Z",
        "summary": "Large language models (LLMs) have played a pivotal role in building\ncommunicative AI to imitate human behaviors but face the challenge of efficient\ncustomization. To tackle this challenge, recent studies have delved into the\nrealm of model editing, which manipulates specific memories of language models\nand changes the related language generation. However, the robustness of model\nediting remains an open question. This work seeks to understand the strengths\nand limitations of editing methods, thus facilitating robust, realistic\napplications of communicative AI. Concretely, we conduct extensive analysis to\naddress the three key research questions. Q1: Can edited LLMs behave\nconsistently resembling communicative AI in realistic situations? Q2: To what\nextent does the rephrasing of prompts lead LLMs to deviate from the edited\nknowledge memory? Q3: Which knowledge features are correlated with the\nperformance and robustness of editing? Our experimental results uncover a\nsubstantial disparity between existing editing methods and the practical\napplication of LLMs. On rephrased prompts that are complex and flexible but\ncommon in realistic applications, the performance of editing experiences a\nsignificant decline. Further analysis shows that more popular knowledge is\nmemorized better, easier to recall, and more challenging to edit effectively.",
        "pdf_link": "https://arxiv.org/pdf/2402.05827v1.pdf"
    },
    {
        "title": "MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion",
        "authors": [
            "Sen Li",
            "Ruochen Wang",
            "Cho-Jui Hsieh",
            "Minhao Cheng",
            "Tianyi Zhou"
        ],
        "published": "2024-02-20T06:14:30Z",
        "summary": "Existing text-to-image models still struggle to generate images of multiple\nobjects, especially in handling their spatial positions, relative sizes,\noverlapping, and attribute bindings. In this paper, we develop a training-free\nMultimodal-LLM agent (MuLan) to address these challenges by progressive\nmulti-object generation with planning and feedback control, like a human\npainter. MuLan harnesses a large language model (LLM) to decompose a prompt to\na sequence of sub-tasks, each generating only one object conditioned on\npreviously generated objects by stable diffusion. Unlike existing LLM-grounded\nmethods, MuLan only produces a high-level plan at the beginning while the exact\nsize and location of each object are determined by an LLM and attention\nguidance upon each sub-task. Moreover, MuLan adopts a vision-language model\n(VLM) to provide feedback to the image generated in each sub-task and control\nthe diffusion model to re-generate the image if it violates the original\nprompt. Hence, each model in every step of MuLan only needs to address an easy\nsub-task it is specialized for. We collect 200 prompts containing multi-objects\nwith spatial relationships and attribute bindings from different benchmarks to\nevaluate MuLan. The results demonstrate the superiority of MuLan in generating\nmultiple objects over baselines. The code is available on\nhttps://github.com/measure-infinity/mulan-code.",
        "pdf_link": "https://arxiv.org/pdf/2402.12741v1.pdf"
    },
    {
        "title": "Predicting Learning Performance with Large Language Models: A Study in Adult Literacy",
        "authors": [
            "Liang Zhang",
            "Jionghao Lin",
            "Conrad Borchers",
            "John Sabatini",
            "John Hollander",
            "Meng Cao",
            "Xiangen Hu"
        ],
        "published": "2024-03-04T08:14:07Z",
        "summary": "Intelligent Tutoring Systems (ITSs) have significantly enhanced adult\nliteracy training, a key factor for societal participation, employment\nopportunities, and lifelong learning. Our study investigates the application of\nadvanced AI models, including Large Language Models (LLMs) like GPT-4, for\npredicting learning performance in adult literacy programs in ITSs. This\nresearch is motivated by the potential of LLMs to predict learning performance\nbased on its inherent reasoning and computational capabilities. By using\nreading comprehension datasets from the ITS, AutoTutor, we evaluate the\npredictive capabilities of GPT-4 versus traditional machine learning methods in\npredicting learning performance through five-fold cross-validation techniques.\nOur findings show that the GPT-4 presents the competitive predictive abilities\nwith traditional machine learning methods such as Bayesian Knowledge Tracing,\nPerformance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor\nfactorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained\non local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected\nXGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior\nperformance compared to local machine execution. Moreover, our investigation\ninto hyper-parameter tuning by GPT-4 versus grid-search suggests comparable\nperformance, albeit with less stability in the automated approach, using\nXGBoost as the case study. Our study contributes to the field by highlighting\nthe potential of integrating LLMs with traditional machine learning models to\nenhance predictive accuracy and personalize adult literacy education, setting a\nfoundation for future research in applying LLMs within ITSs.",
        "pdf_link": "https://arxiv.org/pdf/2403.14668v1.pdf"
    },
    {
        "title": "\"You tell me\": A Dataset of GPT-4-Based Behaviour Change Support Conversations",
        "authors": [
            "Selina Meyer",
            "David Elsweiler"
        ],
        "published": "2024-01-29T13:54:48Z",
        "summary": "Conversational agents are increasingly used to address emotional needs on top\nof information needs. One use case of increasing interest are counselling-style\nmental health and behaviour change interventions, with large language model\n(LLM)-based approaches becoming more popular. Research in this context so far\nhas been largely system-focused, foregoing the aspect of user behaviour and the\nimpact this can have on LLM-generated texts. To address this issue, we share a\ndataset containing text-based user interactions related to behaviour change\nwith two GPT-4-based conversational agents collected in a preregistered user\nstudy. This dataset includes conversation data, user language analysis,\nperception measures, and user feedback for LLM-generated turns, and can offer\nvaluable insights to inform the design of such systems based on real\ninteractions.",
        "pdf_link": "https://arxiv.org/pdf/2401.16167v2.pdf"
    },
    {
        "title": "ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting",
        "authors": [
            "Xiaoxue Cheng",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "published": "2024-03-21T11:34:26Z",
        "summary": "Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of\nlarge language models (LLMs), establishing itself as a primary approach to\nsolving complex reasoning tasks. Existing CoT synthesis approaches usually\nfocus on simpler reasoning tasks and thus result in low-quality and\ninconsistent CoT prompts. In response to this challenge, we present an\nempirical investigation of CoT prompting and introduce CoTGenius, a novel\nframework designed for the automatic generation of superior CoT prompts.\nCoTGenius is developed based on three major evolution strategies, i.e.,\ncomplicate, diversify, and specify-alongside two filtering mechanisms:\nevolutionary success judgement and correctness verification. We further employ\nCoTGenius to create an extensive CoT dataset, and subsequently fine-tune the\nLlama 2-Chat 7B and 13B models on this dataset. We call the resulting model\nChainLM. To deal with the cumulative error issue in reasoning steps, we propose\na step-level debating method, wherein multiple debaters discuss each reasoning\nstep to arrive at the correct answer. Extensive experiments demonstrate that\nour ChainLM models exhibit enhanced proficiency in addressing a spectrum of\ncomplex reasoning problems compared to existing models. In addition, we conduct\nan in-depth analysis of the impact of data categories within CoTGenius on the\nmodel performance. We release our dataset and code at\nhttps://github.com/RUCAIBox/ChainLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.14312v1.pdf"
    },
    {
        "title": "Automated Fact-Checking of Climate Change Claims with Large Language Models",
        "authors": [
            "Markus Leippold",
            "Saeid Ashraf Vaghefi",
            "Dominik Stammbach",
            "Veruska Muccione",
            "Julia Bingler",
            "Jingwei Ni",
            "Chiara Colesanti-Senni",
            "Tobias Wekhof",
            "Tobias Schimanski",
            "Glen Gostlow",
            "Tingyu Yu",
            "Juerg Luterbacher",
            "Christian Huggel"
        ],
        "published": "2024-01-23T08:49:23Z",
        "summary": "This paper presents Climinator, a novel AI-based tool designed to automate\nthe fact-checking of climate change claims. Utilizing an array of Large\nLanguage Models (LLMs) informed by authoritative sources like the IPCC reports\nand peer-reviewed scientific literature, Climinator employs an innovative\nMediator-Advocate framework. This design allows Climinator to effectively\nsynthesize varying scientific perspectives, leading to robust, evidence-based\nevaluations. Our model demonstrates remarkable accuracy when testing claims\ncollected from Climate Feedback and Skeptical Science. Notably, when\nintegrating an advocate with a climate science denial perspective in our\nframework, Climinator's iterative debate process reliably converges towards\nscientific consensus, underscoring its adeptness at reconciling diverse\nviewpoints into science-based, factual conclusions. While our research is\nsubject to certain limitations and necessitates careful interpretation, our\napproach holds significant potential. We hope to stimulate further research and\nencourage exploring its applicability in other contexts, including political\nfact-checking and legal domains.",
        "pdf_link": "https://arxiv.org/pdf/2401.12566v1.pdf"
    },
    {
        "title": "Retrieval-Augmented Thought Process as Sequential Decision Making",
        "authors": [
            "Thomas Pouplin",
            "Hao Sun",
            "Samuel Holt",
            "Mihaela van der Schaar"
        ],
        "published": "2024-02-12T17:17:50Z",
        "summary": "Large Language Models (LLMs) have demonstrated their strong ability to assist\npeople and show \"sparks of intelligence\". However, several open challenges\nhinder their wider application: such as concerns over privacy, tendencies to\nproduce hallucinations, and difficulties in handling long contexts. In this\nwork, we address those challenges by introducing the Retrieval-Augmented\nThought Process (RATP). Given access to external knowledge, RATP formulates the\nthought generation of LLMs as a multiple-step decision process. To optimize\nsuch a thought process, RATP leverages Monte-Carlo Tree Search, and learns a\nQ-value estimator that permits cost-efficient inference. In addressing the task\nof question-answering with private data, where ethical and security concerns\nlimit LLM training methods, RATP achieves a 50% improvement over existing\nin-context retrieval-augmented language models.",
        "pdf_link": "https://arxiv.org/pdf/2402.07812v1.pdf"
    },
    {
        "title": "Unveiling Linguistic Regions in Large Language Models",
        "authors": [
            "Zhihao Zhang",
            "Jun Zhao",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-02-22T16:56:13Z",
        "summary": "Large Language Models (LLMs) have demonstrated considerable cross-lingual\nalignment and generalization ability. Current research primarily focuses on\nimproving LLMs' cross-lingual generalization capabilities. However, there is\nstill a lack of research on the intrinsic mechanisms of how LLMs achieve\ncross-lingual alignment. From the perspective of region partitioning, this\npaper conducts several investigations on the linguistic competence of LLMs. We\ndiscover a core region in LLMs that corresponds to linguistic competence,\naccounting for approximately 1% of the total model parameters. Removing this\ncore region by setting parameters to zero results in a significant performance\ndecrease across 30 different languages. Furthermore, this core region exhibits\nsignificant dimensional dependency, perturbations to even a single parameter on\nspecific dimensions leading to a loss of linguistic competence. Moreover, we\ndiscover that distinct regions exist for different monolingual families, and\ndisruption to these specific regions substantially reduces the LLMs'\nproficiency in those corresponding languages. Our research also indicates that\nfreezing the core linguistic region during further pre-training can mitigate\nthe issue of catastrophic forgetting (CF), a common occurrence observed during\nfurther pre-training of LLMs. Overall, exploring the LLMs' functional regions\nprovides insights into the foundation of their intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2402.14700v1.pdf"
    },
    {
        "title": "LLsM: Generative Linguistic Steganography with Large Language Model",
        "authors": [
            "Yihao Wang",
            "Ruiqi Song",
            "Ru Zhang",
            "Jianyi Liu",
            "Lingxiao Li"
        ],
        "published": "2024-01-28T13:21:44Z",
        "summary": "Linguistic Steganography (LS) tasks aim to generate steganographic text\n(stego) based on secret information. Only authorized recipients can perceive\nthe existence of the stegos and extract secrets, thereby preserving privacy.\nHowever, existing LS methods do not consider the controllable generation of\nstegos containing specific discourses such as style, genre, and theme. And they\nare difficult to simulate high-quality natural texts. As a result, the stegos\nare easily perceived and detectable, compromising covert communication. This\npaper proposes the LLsM, the first LS work with the Large Language Model (LLM).\nRegarding open-source LLMs, we reconstruct the token generator of LLM to the\n\"stego generator\" so that it can control the generation of stego based on the\nsecret. In this \"stego generator\", the candidate pool is encoded by range\ncoding, and the adjustment factor for the interval length is also given. The\nsecret determines the interval, thereby determining the next token. This better\nsimulates the distribution of natural texts and controls the adjustment of the\nembedding rate. In addition, we preliminarily built an LLsM-c architecture for\nclosed-source LLMs. It encodes discourse to obtain high-quality prompts\ncontaining discourse based on secrets, and generates pure natural texts\ncontaining discourse. Experiments show that LLsM performs superior to prevalent\nLS and related-task baselines regarding various kinds of concealment and\nanti-steganalysis. LLsM's MAUVE surpasses baselines by 60%-80% and\nanti-steganalysis exceeds baselines by 20%-30%. Notably, LLsM can also generate\nlonger stegos with high quality, showing its advantages in understanding and\ncoherence.",
        "pdf_link": "https://arxiv.org/pdf/2401.15656v3.pdf"
    },
    {
        "title": "Coercing LLMs to do and reveal (almost) anything",
        "authors": [
            "Jonas Geiping",
            "Alex Stein",
            "Manli Shu",
            "Khalid Saifullah",
            "Yuxin Wen",
            "Tom Goldstein"
        ],
        "published": "2024-02-21T18:59:13Z",
        "summary": "It has recently been shown that adversarial attacks on large language models\n(LLMs) can \"jailbreak\" the model into making harmful statements. In this work,\nwe argue that the spectrum of adversarial attacks on LLMs is much larger than\nmerely jailbreaking. We provide a broad overview of possible attack surfaces\nand attack goals. Based on a series of concrete examples, we discuss,\ncategorize and systematize attacks that coerce varied unintended behaviors,\nsuch as misdirection, model control, denial-of-service, or data extraction.\n  We analyze these attacks in controlled experiments, and find that many of\nthem stem from the practice of pre-training LLMs with coding capabilities, as\nwell as the continued existence of strange \"glitch\" tokens in common LLM\nvocabularies that should be removed for security reasons.",
        "pdf_link": "https://arxiv.org/pdf/2402.14020v1.pdf"
    },
    {
        "title": "Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing",
        "authors": [
            "Zi Yang",
            "Nan Hua"
        ],
        "published": "2024-01-10T02:20:48Z",
        "summary": "As LLMs have become capable of processing more complex types of inputs,\nresearchers have recently studied how to efficiently and affordably process\npossibly arbitrarily long sequences. One effective approach is to use a FIFO\nmemory to store keys and values of an attention sublayer from past chunks to\nallow subsequent queries to attend. However, this approach requires a large\nmemory and/or takes into the consideration the specific LM architecture.\nMoreover, due to the causal nature between the key-values in prior context and\nthe queries at present, this approach cannot be extended to bidirectional\nattention such as in an encoder-decoder or PrefixLM decoder-only architecture.\nIn this paper, we propose to use eviction policies, such as LRA and LFA, to\nreduce the memory size and adapt to various architectures, and we also propose\nthe Attendre layer, a wait-to-attend mechanism by retrieving the key-value\nmemory (K/V memory) with evicted queries in the query memory (Q memory). As a\nfirst step, we evaluate this method in the context length extension setup using\nthe TriviaQA reading comprehension task, and show the effectiveness of the\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2401.04881v1.pdf"
    },
    {
        "title": "A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students' Formative Assessment Responses in Science",
        "authors": [
            "Clayton Cohn",
            "Nicole Hutchins",
            "Tuan Le",
            "Gautam Biswas"
        ],
        "published": "2024-03-21T17:09:08Z",
        "summary": "This paper explores the use of large language models (LLMs) to score and\nexplain short-answer assessments in K-12 science. While existing methods can\nscore more structured math and computer science assessments, they often do not\nprovide explanations for the scores. Our study focuses on employing GPT-4 for\nautomated assessment in middle school Earth Science, combining few-shot and\nactive learning with chain-of-thought reasoning. Using a human-in-the-loop\napproach, we successfully score and provide meaningful explanations for\nformative assessment responses. A systematic analysis of our method's pros and\ncons sheds light on the potential for human-in-the-loop techniques to enhance\nautomated grading for open-ended science assessments.",
        "pdf_link": "https://arxiv.org/pdf/2403.14565v1.pdf"
    },
    {
        "title": "Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks",
        "authors": [
            "Dimitrios Michael Manias",
            "Ali Chouman",
            "Abdallah Shami"
        ],
        "published": "2024-03-04T17:29:57Z",
        "summary": "The integration of Machine Learning and Artificial Intelligence (ML/AI) into\nfifth-generation (5G) networks has made evident the limitations of network\nintelligence with ever-increasing, strenuous requirements for current and\nnext-generation devices. This transition to ubiquitous intelligence demands\nhigh connectivity, synchronicity, and end-to-end communication between users\nand network operators, and will pave the way towards full network automation\nwithout human intervention. Intent-based networking is a key factor in the\nreduction of human actions, roles, and responsibilities while shifting towards\nnovel extraction and interpretation of automated network management. This paper\npresents the development of a custom Large Language Model (LLM) for 5G and\nnext-generation intent-based networking and provides insights into future LLM\ndevelopments and integrations to realize end-to-end intent-based networking for\nfully automated network intelligence.",
        "pdf_link": "https://arxiv.org/pdf/2403.02238v1.pdf"
    },
    {
        "title": "Language Models with Conformal Factuality Guarantees",
        "authors": [
            "Christopher Mohri",
            "Tatsunori Hashimoto"
        ],
        "published": "2024-02-15T18:31:53Z",
        "summary": "Guaranteeing the correctness and factuality of language model (LM) outputs is\na major open problem. In this work, we propose conformal factuality, a\nframework that can ensure high probability correctness guarantees for LMs by\nconnecting language modeling and conformal prediction. We observe that the\ncorrectness of an LM output is equivalent to an uncertainty quantification\nproblem, where the uncertainty sets are defined as the entailment set of an\nLM's output. Using this connection, we show that conformal prediction in\nlanguage models corresponds to a back-off algorithm that provides high\nprobability correctness guarantees by progressively making LM outputs less\nspecific (and expanding the associated uncertainty sets). This approach applies\nto any black-box LM and requires very few human-annotated samples. Evaluations\nof our approach on closed book QA (FActScore, NaturalQuestions) and reasoning\ntasks (MATH) show that our approach can provide 80-90% correctness guarantees\nwhile retaining the majority of the LM's original output.",
        "pdf_link": "https://arxiv.org/pdf/2402.10978v1.pdf"
    },
    {
        "title": "The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework",
        "authors": [
            "Zhuo Chen",
            "Yin Fang",
            "Yichi Zhang",
            "Lingbing Guo",
            "Jiaoyan Chen",
            "Huajun Chen",
            "Wen Zhang"
        ],
        "published": "2024-03-11T15:48:43Z",
        "summary": "The advancement of Multi-modal Pre-training highlights the necessity for a\nrobust Multi-Modal Knowledge Graph (MMKG) representation learning framework.\nThis framework is crucial for integrating structured knowledge into multi-modal\nLarge Language Models (LLMs) at scale, aiming to alleviate issues like\nknowledge misconceptions and multi-modal hallucinations. In this work, to\nevaluate models' ability to accurately embed entities within MMKGs, we focus on\ntwo widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and\nMulti-modal Entity Alignment (MMEA). Building on this foundation, we propose a\nnovel SNAG method that utilizes a Transformer-based architecture equipped with\nmodality-level noise masking for the robust integration of multi-modal entity\nfeatures in KGs. By incorporating specific training objectives for both MKGC\nand MMEA, our approach achieves SOTA performance across a total of ten datasets\n(three for MKGC and seven for MEMA), demonstrating its robustness and\nversatility. Besides, SNAG can not only function as a standalone model but also\nenhance other existing methods, providing stable performance improvements. Our\ncode and data are available at: https://github.com/zjukg/SNAG.",
        "pdf_link": "https://arxiv.org/pdf/2403.06832v2.pdf"
    },
    {
        "title": "Illuminate: A novel approach for depression detection with explainable analysis and proactive therapy using prompt engineering",
        "authors": [
            "Aryan Agrawal"
        ],
        "published": "2024-02-05T06:08:06Z",
        "summary": "This paper introduces a novel paradigm for depression detection and treatment\nusing advanced Large Language Models (LLMs): Generative Pre-trained Transformer\n4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized\nprompts to diagnose, explain, and suggest therapeutic interventions for\ndepression. A unique few-shot prompting method enhances the models' ability to\nanalyze and explain depressive symptoms based on the DSM-5 criteria. In the\ninteraction phase, the models engage in empathetic dialogue management, drawing\nfrom resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,\nfostering supportive interactions with individuals experiencing major\ndepressive disorders. Additionally, the research introduces the Illuminate\nDatabase, enriched with various CBT modules, aiding in personalized therapy\nrecommendations. The study evaluates LLM performance using metrics such as F1\nscores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy\nfor Gisting Evaluation (ROUGE) across different test sets, demonstrating their\neffectiveness. This comprehensive approach blends cutting-edge AI with\nestablished psychological methods, offering new possibilities in mental health\ncare and showcasing the potential of LLMs in revolutionizing depression\ndiagnosis and treatment strategies.",
        "pdf_link": "https://arxiv.org/pdf/2402.05127v1.pdf"
    },
    {
        "title": "Fine-Grained Self-Endorsement Improves Factuality and Reasoning",
        "authors": [
            "Ante Wang",
            "Linfeng Song",
            "Baolin Peng",
            "Ye Tian",
            "Lifeng Jin",
            "Haitao Mi",
            "Jinsong Su",
            "Dong Yu"
        ],
        "published": "2024-02-23T22:24:40Z",
        "summary": "This work studies improving large language model (LLM) generations at\ninference time by mitigating fact-conflicting hallucinations. Particularly, we\npropose a self-endorsement framework that leverages the fine-grained fact-level\ncomparisons across multiple sampled responses. Compared with prior ensemble\nmethods (Wang et al., 2022;Chen et al., 2023)) that perform response-level\nselection, our approach can better alleviate hallucinations, especially for\nlongform generation tasks. Our approach can broadly benefit smaller and\nopen-source LLMs as it mainly conducts simple content-based comparisons.\nExperiments on Biographies show that our method can effectively improve the\nfactuality of generations with simple and intuitive prompts across different\nscales of LLMs. Besides, comprehensive analyses on TriviaQA and GSM8K\ndemonstrate the potential of self-endorsement for broader application.",
        "pdf_link": "https://arxiv.org/pdf/2402.15631v1.pdf"
    },
    {
        "title": "Fine-Tuning Language Models with Reward Learning on Policy",
        "authors": [
            "Hao Lang",
            "Fei Huang",
            "Yongbin Li"
        ],
        "published": "2024-03-28T10:02:10Z",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as an effective\napproach to aligning large language models (LLMs) to human preferences. RLHF\ncontains three steps, i.e., human preference collecting, reward learning, and\npolicy optimization, which are usually performed serially. Despite its\npopularity, however, (fixed) reward models may suffer from inaccurate\noff-distribution, since policy optimization continuously shifts LLMs' data\ndistribution. Repeatedly collecting new preference data from the latest LLMs\nmay alleviate this issue, which unfortunately makes the resulting system more\ncomplicated and difficult to optimize. In this paper, we propose reward\nlearning on policy (RLP), an unsupervised framework that refines a reward model\nusing policy samples to keep it on-distribution. Specifically, an unsupervised\nmulti-view learning method is introduced to learn robust representations of\npolicy samples. Meanwhile, a synthetic preference generation approach is\ndeveloped to simulate high-quality preference data with policy outputs.\nExtensive experiments on three benchmark datasets show that RLP consistently\noutperforms the state-of-the-art. Our code is available at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.",
        "pdf_link": "https://arxiv.org/pdf/2403.19279v1.pdf"
    },
    {
        "title": "Small Language Model Can Self-correct",
        "authors": [
            "Haixia Han",
            "Jiaqing Liang",
            "Jie Shi",
            "Qianyu He",
            "Yanghua Xiao"
        ],
        "published": "2024-01-14T14:29:07Z",
        "summary": "Generative Language Models (LMs) such as ChatGPT have exhibited remarkable\nperformance across various downstream tasks. Nevertheless, one of their most\nprominent drawbacks is generating inaccurate or false information with a\nconfident tone. Previous studies have devised sophisticated pipelines and\nprompts to induce large LMs to exhibit the capability for self-correction.\nHowever, large LMs are explicitly prompted to verify and modify its answers\nseparately rather than completing all steps spontaneously like humans.\nMoreover, these complex prompts are extremely challenging for small LMs to\nfollow. In this paper, we introduce the \\underline{I}ntrinsic\n\\underline{S}elf-\\underline{C}orrection (ISC) in generative language models,\naiming to correct the initial output of LMs in a self-triggered manner, even\nfor those small LMs with 6 billion parameters. Specifically, we devise a\npipeline for constructing self-correction data and propose Partial Answer\nMasking (PAM), aiming to endow the model with the capability for intrinsic\nself-correction through fine-tuning. We conduct experiments using LMs with\nparameters sizes ranging from 6 billion to 13 billion in two tasks, including\ncommonsense reasoning and factual knowledge reasoning. Our experiments\ndemonstrate that the outputs generated using ISC outperform those generated\nwithout self-correction. We believe that the output quality of even small LMs\ncan be further improved by empowering them with the ability to intrinsic\nself-correct.",
        "pdf_link": "https://arxiv.org/pdf/2401.07301v1.pdf"
    },
    {
        "title": "Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?",
        "authors": [
            "Bing Xue",
            "Charles Alba",
            "Joanna Abraham",
            "Thomas Kannampallil",
            "Chenyang Lu"
        ],
        "published": "2024-02-27T13:18:00Z",
        "summary": "Postoperative risk predictions can inform effective perioperative care\nmanagement and planning. We aimed to assess whether clinical large language\nmodels (LLMs) can predict postoperative risks using clinical texts with various\ntraining strategies. The main cohort involved 84,875 records from Barnes Jewish\nHospital (BJH) system between 2018 and 2021. Methods were replicated on Beth\nIsrael Deaconess's MIMIC dataset. Both studies had mean duration of follow-up\nbased on the length of postoperative ICU stay less than 7 days. For the BJH\ndataset, outcomes included 30-day mortality, pulmonary embolism (PE) and\npneumonia. Three domain adaptation and finetuning strategies were implemented\nfor BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives;\nincorporating labels with semi-supervised fine-tuning; and foundational\nmodelling through multi-task learning. Model performance was compared using the\narea under the receiver operating characteristic curve (AUROC) and the area\nunder the precision recall curve (AUPRC) for classification tasks, and mean\nsquared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed\ntraditional word embeddings, with absolute maximal gains of 38.3% for AUROC and\n14% for AUPRC. Adapting models further improved performance: (1)\nself-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2)\nsemi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to\nself-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and\n2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical\nLLMs offer opportunities for postoperative risk predictions in unforeseen data,\nwith peaks in foundational models indicating the potential of task-agnostic\nlearning towards the generalizability of LLMs in perioperative care.",
        "pdf_link": "https://arxiv.org/pdf/2402.17493v2.pdf"
    },
    {
        "title": "Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning",
        "authors": [
            "Wentao Shi",
            "Xiangnan He",
            "Yang Zhang",
            "Chongming Gao",
            "Xinyue Li",
            "Jizhi Zhang",
            "Qifan Wang",
            "Fuli Feng"
        ],
        "published": "2024-02-29T13:49:56Z",
        "summary": "Traditional recommendation setting tends to excessively cater to users'\nimmediate interests and neglect their long-term engagement. To address it, it\nis crucial to incorporate planning capabilities into the recommendation\ndecision-making process to develop policies that take into account both\nimmediate interests and long-term engagement. Despite Reinforcement Learning\n(RL) can learn planning capacity by maximizing cumulative reward, the scarcity\nof recommendation data presents challenges such as instability and\nsusceptibility to overfitting when training RL models from scratch.\n  In this context, we propose to leverage the remarkable planning capabilities\nover sparse data of Large Language Models (LLMs) for long-term recommendation.\nThe key lies in enabling a language model to understand and apply task-solving\nprinciples effectively in personalized recommendation scenarios, as the model's\npre-training may not naturally encompass these principles, necessitating the\nneed to inspire or teach the model. To achieve this, we propose a Bi-level\nLearnable LLM Planner framework, which combines macro-learning and\nmicro-learning through a hierarchical mechanism. The framework includes a\nPlanner and Reflector for acquiring high-level guiding principles and an\nActor-Critic component for planning personalization. Extensive experiments\nvalidate the superiority of the framework in learning to plan for long-term\nrecommendations.",
        "pdf_link": "https://arxiv.org/pdf/2403.00843v1.pdf"
    },
    {
        "title": "Properties and Challenges of LLM-Generated Explanations",
        "authors": [
            "Jenny Kunz",
            "Marco Kuhlmann"
        ],
        "published": "2024-02-16T09:37:54Z",
        "summary": "The self-rationalising capabilities of large language models (LLMs) have been\nexplored in restricted settings, using task/specific data sets. However,\ncurrent LLMs do not (only) rely on specifically annotated data; nonetheless,\nthey frequently explain their outputs. The properties of the generated\nexplanations are influenced by the pre-training corpus and by the target data\nused for instruction fine-tuning. As the pre-training corpus includes a large\namount of human-written explanations \"in the wild\", we hypothesise that LLMs\nadopt common properties of human explanations. By analysing the outputs for a\nmulti-domain instruction fine-tuning data set, we find that generated\nexplanations show selectivity and contain illustrative elements, but less\nfrequently are subjective or misleading. We discuss reasons and consequences of\nthe properties' presence or absence. In particular, we outline positive and\nnegative implications depending on the goals and user groups of the\nself-rationalising system.",
        "pdf_link": "https://arxiv.org/pdf/2402.10532v1.pdf"
    },
    {
        "title": "BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in Relational Algebra",
        "authors": [
            "Parker Glenn",
            "Parag Pravin Dakle",
            "Liang Wang",
            "Preethi Raghavan"
        ],
        "published": "2024-02-27T20:48:24Z",
        "summary": "Many existing end-to-end systems for hybrid question answering tasks can\noften be boiled down to a \"prompt-and-pray\" paradigm, where the user has\nlimited control and insight into the intermediate reasoning steps used to\nachieve the final result. Additionally, due to the context size limitation of\nmany transformer-based LLMs, it is often not reasonable to expect that the full\nstructured and unstructured context will fit into a given prompt in a zero-shot\nsetting, let alone a few-shot setting. We introduce BlendSQL, a superset of\nSQLite to act as a unified dialect for orchestrating reasoning across both\nunstructured and structured data. For hybrid question answering tasks involving\nmulti-hop reasoning, we encode the full decomposed reasoning roadmap into a\nsingle interpretable BlendSQL query. Notably, we show that BlendSQL can scale\nto massive datasets and improve the performance of end-to-end systems while\nusing 35% fewer tokens. Our code is available and installable as a package at\nhttps://github.com/parkervg/blendsql.",
        "pdf_link": "https://arxiv.org/pdf/2402.17882v1.pdf"
    },
    {
        "title": "Corpus-Steered Query Expansion with Large Language Models",
        "authors": [
            "Yibin Lei",
            "Yu Cao",
            "Tianyi Zhou",
            "Tao Shen",
            "Andrew Yates"
        ],
        "published": "2024-02-28T03:58:58Z",
        "summary": "Recent studies demonstrate that query expansions generated by large language\nmodels (LLMs) can considerably enhance information retrieval systems by\ngenerating hypothetical documents that answer the queries as expansions.\nHowever, challenges arise from misalignments between the expansions and the\nretrieval corpus, resulting in issues like hallucinations and outdated\ninformation due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo\nRelevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to\npromote the incorporation of knowledge embedded within the corpus. CSQE\nutilizes the relevance assessing capability of LLMs to systematically identify\npivotal sentences in the initially-retrieved documents. These corpus-originated\ntexts are subsequently used to expand the query together with LLM-knowledge\nempowered expansions, improving the relevance prediction between the query and\nthe target documents. Extensive experiments reveal that CSQE exhibits strong\nperformance without necessitating any training, especially with queries for\nwhich LLMs lack knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2402.18031v1.pdf"
    },
    {
        "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
        "authors": [
            "Qinhao Zhou",
            "Zihan Zhang",
            "Xiang Xiang",
            "Ke Wang",
            "Yuchuan Wu",
            "Yongbin Li"
        ],
        "published": "2024-03-29T03:48:12Z",
        "summary": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language\nunderstanding and generation capabilities, making them highly successful in a\nvariety of tasks. However, when used as agents for dealing with complex\nproblems in the real world, their performance is far inferior to large\ncommercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need\nto have the capabilities of task planning, long-term memory, and the ability to\nleverage external tools to achieve satisfactory performance. Various methods\nhave been proposed to enhance the agent capabilities of LLMs. On the one hand,\nmethods involve constructing agent-specific data and fine-tuning the models. On\nthe other hand, some methods focus on designing prompts that effectively\nactivate the reasoning abilities of the LLMs. We explore both strategies on the\n7B and 13B models. We propose a comprehensive method for constructing\nagent-specific data using GPT-4. Through supervised fine-tuning with\nconstructed data, we find that for these models with a relatively small number\nof parameters, supervised fine-tuning can significantly reduce hallucination\noutputs and formatting errors in agent tasks. Furthermore, techniques such as\nmulti-path reasoning and task decomposition can effectively decrease problem\ncomplexity and enhance the performance of LLMs as agents. We evaluate our\nmethod on five agent tasks of AgentBench and achieve satisfactory results.",
        "pdf_link": "https://arxiv.org/pdf/2403.19962v1.pdf"
    },
    {
        "title": "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning",
        "authors": [
            "Hao Zhao",
            "Maksym Andriushchenko",
            "Francesco Croce",
            "Nicolas Flammarion"
        ],
        "published": "2024-02-07T13:32:11Z",
        "summary": "There is a consensus that instruction fine-tuning of LLMs requires\nhigh-quality data, but what are they? LIMA (NeurIPS 2023) and AlpaGasus (ICLR\n2024) are state-of-the-art methods for selecting such high-quality examples,\neither via manual curation or using GPT-3.5-Turbo as a quality scorer. We show\nthat the extremely simple baseline of selecting the 1,000 instructions with\nlongest responses from standard datasets can consistently outperform these\nsophisticated methods according to GPT-4 and PaLM-2 as judges, while remaining\ncompetitive on the OpenLLM benchmarks that test factual knowledge. We\ndemonstrate this for several state-of-the-art LLMs (Llama-2-7B, Llama-2-13B,\nand Mistral-7B) and datasets (Alpaca-52k and Evol-Instruct-70k). In addition, a\nlightweight refinement of such long instructions can further improve the\nabilities of the fine-tuned LLMs, and allows us to obtain the 2nd\nhighest-ranked Llama-2-7B-based model on AlpacaEval 2.0 while training on only\n1,000 examples and no extra preference data. We also conduct a thorough\nanalysis of our models to ensure that their enhanced performance is not simply\ndue to GPT-4's preference for longer responses, thus ruling out any artificial\nimprovement. In conclusion, our findings suggest that fine-tuning on the\nlongest instructions should be the default baseline for any research on\ninstruction fine-tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.04833v1.pdf"
    },
    {
        "title": "Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform",
        "authors": [
            "Mingyue Cheng",
            "Hao Zhang",
            "Jiqian Yang",
            "Qi Liu",
            "Li Li",
            "Xin Huang",
            "Liwei Song",
            "Zhi Li",
            "Zhenya Huang",
            "Enhong Chen"
        ],
        "published": "2024-03-13T07:31:20Z",
        "summary": "Large language model evaluation plays a pivotal role in the enhancement of\nits capacity. Previously, numerous methods for evaluating large language models\nhave been proposed in this area. Despite their effectiveness, these existing\nworks mainly focus on assessing objective questions, overlooking the capability\nto evaluate subjective questions which is extremely common for large language\nmodels. Additionally, these methods predominantly utilize centralized datasets\nfor evaluation, with question banks concentrated within the evaluation\nplatforms themselves. Moreover, the evaluation processes employed by these\nplatforms often overlook personalized factors, neglecting to consider the\nindividual characteristics of both the evaluators and the models being\nevaluated. To address these limitations, we propose a novel anonymous\ncrowd-sourcing evaluation platform, BingJian, for large language models that\nemploys a competitive scoring mechanism where users participate in ranking\nmodels based on their performance. This platform stands out not only for its\nsupport of centralized evaluations to assess the general capabilities of models\nbut also for offering an open evaluation gateway. Through this gateway, users\nhave the opportunity to submit their questions, testing the models on a\npersonalized and potentially broader range of capabilities. Furthermore, our\nplatform introduces personalized evaluation scenarios, leveraging various forms\nof human-computer interaction to assess large language models in a manner that\naccounts for individual user preferences and contexts. The demonstration of\nBingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.",
        "pdf_link": "https://arxiv.org/pdf/2403.08305v1.pdf"
    },
    {
        "title": "Derivative-Free Optimization for Low-Rank Adaptation in Large Language Models",
        "authors": [
            "Feihu Jin",
            "Yin Liu",
            "Ying Tan"
        ],
        "published": "2024-03-04T06:20:31Z",
        "summary": "Parameter-efficient tuning methods such as LoRA could achieve comparable\nperformance to model tuning by tuning a small portion of the parameters.\nHowever, substantial computational resources are still required, as this\nprocess involves calculating gradients and performing back-propagation\nthroughout the model. Much effort has recently been devoted to utilizing the\nderivative-free optimization method to eschew the computation of gradients and\nshowcase an augmented level of robustness in few-shot settings. In this paper,\nwe prepend the low-rank modules into each self-attention layer of the model and\nemploy two derivative-free optimization methods to optimize these low-rank\nmodules at each layer alternately. Extensive results on various tasks and\nlanguage models demonstrate that our proposed method achieves substantial\nimprovement and exhibits clear advantages in memory usage and convergence speed\ncompared to existing gradient-based parameter-efficient tuning and\nderivative-free optimization methods in few-shot settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.01754v1.pdf"
    },
    {
        "title": "Learning From Correctness Without Prompting Makes LLM Efficient Reasoner",
        "authors": [
            "Yuxuan Yao",
            "Han Wu",
            "Zhijiang Guo",
            "Biyan Zhou",
            "Jiahui Gao",
            "Sichun Luo",
            "Hanxu Hou",
            "Xiaojin Fu",
            "Linqi Song"
        ],
        "published": "2024-03-28T02:12:49Z",
        "summary": "Large language models (LLMs) have demonstrated outstanding performance across\nvarious tasks, yet they still exhibit limitations such as hallucination,\nunfaithful reasoning, and toxic content. One potential approach to mitigate\nthese issues is learning from human or external feedback (e.g. tools). In this\npaper, we introduce an intrinsic self-correct reasoning framework for LLMs that\neliminates the need for human feedback, external tools, and handcraft prompts.\nThe proposed framework, based on a multi-step reasoning paradigm\n\\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning\nperformance without needing to learn from errors. This paradigm prioritizes\nlearning from correct reasoning steps, and a unique method to measure\nconfidence for each reasoning step based on generation logits. Experimental\nresults across various multi-step reasoning tasks demonstrate the effectiveness\nof the framework in improving reasoning performance with reduced token\nconsumption.",
        "pdf_link": "https://arxiv.org/pdf/2403.19094v1.pdf"
    },
    {
        "title": "Conditional and Modal Reasoning in Large Language Models",
        "authors": [
            "Wesley H. Holliday",
            "Matthew Mandelkern"
        ],
        "published": "2024-01-30T16:56:54Z",
        "summary": "The reasoning abilities of large language models (LLMs) are the topic of a\ngrowing body of research in artificial intelligence and cognitive science. In\nthis paper, we probe the extent to which a dozen LLMs are able to distinguish\nlogically correct inferences from logically fallacious ones. We focus on\ninference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob\nhas a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must\nhave a king'). These inference patterns have been of special interest to\nlogicians, philosophers, and linguists, since they plausibly play a central\nrole in human reasoning. Assessing LLMs on these inference patterns is thus\nhighly relevant to the question of how much the reasoning abilities of LLMs\nmatch those of humans. Among the LLMs we tested, all but GPT-4 often make basic\nmistakes with conditionals. Moreover, even GPT-4 displays logically\ninconsistent judgments across inference patterns involving epistemic modals.",
        "pdf_link": "https://arxiv.org/pdf/2401.17169v1.pdf"
    },
    {
        "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction",
        "authors": [
            "Yinghao Li",
            "Rampi Ramprasad",
            "Chao Zhang"
        ],
        "published": "2024-02-20T20:42:02Z",
        "summary": "Large language models (LLMs) have demonstrated impressive abilities in\ngenerating unstructured natural language according to instructions. However,\ntheir performance can be inconsistent when tasked with producing text that\nadheres to specific structured formats, which is crucial in applications like\nnamed entity recognition (NER) or relation extraction (RE). To address this\nissue, this paper introduces an efficient method, G&O, to enhance their\nstructured text generation capabilities. It breaks the generation into a\ntwo-step pipeline: initially, LLMs generate answers in natural language as\nintermediate responses. Subsequently, LLMs are asked to organize the output\ninto the desired structure, using the intermediate responses as context. G&O\neffectively separates the generation of content from the structuring process,\nreducing the pressure of completing two orthogonal tasks simultaneously. Tested\non zero-shot NER and RE, the results indicate a significant improvement in LLM\nperformance with minimal additional efforts. This straightforward and adaptable\nprompting technique can also be combined with other strategies, like\nself-consistency, to further elevate LLM capabilities in various structured\ntext generation tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.13364v1.pdf"
    },
    {
        "title": "SaulLM-7B: A pioneering Large Language Model for Law",
        "authors": [
            "Pierre Colombo",
            "Telmo Pessoa Pires",
            "Malik Boudiaf",
            "Dominic Culver",
            "Rui Melo",
            "Caio Corro",
            "Andre F. T. Martins",
            "Fabrizio Esposito",
            "Vera L\u00facia Raposo",
            "Sofia Morgado",
            "Michael Desa"
        ],
        "published": "2024-03-06T17:42:16Z",
        "summary": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored\nfor the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM\ndesigned explicitly for legal text comprehension and generation. Leveraging the\nMistral 7B architecture as its foundation, SaulLM-7B is trained on an English\nlegal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art\nproficiency in understanding and processing legal documents. Additionally, we\npresent a novel instructional fine-tuning method that leverages legal datasets\nto further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is\nreleased under the MIT License.",
        "pdf_link": "https://arxiv.org/pdf/2403.03883v2.pdf"
    },
    {
        "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
        "authors": [
            "Pranab Sahoo",
            "Ayush Kumar Singh",
            "Sriparna Saha",
            "Vinija Jain",
            "Samrat Mondal",
            "Aman Chadha"
        ],
        "published": "2024-02-05T19:49:13Z",
        "summary": "Prompt engineering has emerged as an indispensable technique for extending\nthe capabilities of large language models (LLMs) and vision-language models\n(VLMs). This approach leverages task-specific instructions, known as prompts,\nto enhance model efficacy without modifying the core model parameters. Rather\nthan updating the model parameters, prompts allow seamless integration of\npre-trained models into downstream tasks by eliciting desired model behaviors\nsolely based on the given prompt. Prompts can be natural language instructions\nthat provide context to guide the model or learned vector representations that\nactivate relevant knowledge. This burgeoning field has enabled success across\nvarious applications, from question-answering to commonsense reasoning.\nHowever, there remains a lack of systematic organization and understanding of\nthe diverse prompt engineering methods and techniques. This survey paper\naddresses the gap by providing a structured overview of recent advancements in\nprompt engineering, categorized by application area. For each prompting\napproach, we provide a summary detailing the prompting methodology, its\napplications, the models involved, and the datasets utilized. We also delve\ninto the strengths and limitations of each approach and include a taxonomy\ndiagram and table summarizing datasets, models, and critical points of each\nprompting technique. This systematic analysis enables a better understanding of\nthis rapidly developing field and facilitates future research by illuminating\nopen challenges and opportunities for prompt engineering.",
        "pdf_link": "https://arxiv.org/pdf/2402.07927v1.pdf"
    },
    {
        "title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
        "authors": [
            "Abdur Rahman Bin Md Faizullah",
            "Ashok Urlana",
            "Rahul Mishra"
        ],
        "published": "2024-03-22T17:31:43Z",
        "summary": "Examining limitations is a crucial step in the scholarly research reviewing\nprocess, revealing aspects where a study might lack decisiveness or require\nenhancement. This aids readers in considering broader implications for further\nresearch. In this article, we present a novel and challenging task of\nSuggestive Limitation Generation (SLG) for research papers. We compile a\ndataset called LimGen, encompassing 4068 research papers and their associated\nlimitations from the ACL anthology. We investigate several approaches to\nharness large language models (LLMs) for producing suggestive limitations, by\nthoroughly examining the related challenges, practical insights, and potential\nopportunities. Our LimGen dataset and code can be accessed at\nhttps://github.com/armbf/LimGen.",
        "pdf_link": "https://arxiv.org/pdf/2403.15529v1.pdf"
    },
    {
        "title": "Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification",
        "authors": [
            "Devam Mondal",
            "Carlo Lipizzi"
        ],
        "published": "2024-03-20T18:59:18Z",
        "summary": "Despite the growing capabilities of large language models, there exists\nconcerns about the biases they develop. In this paper, we propose a novel,\nautomated mechanism for debiasing through specified dataset augmentation in the\nlens of bias producers and in the context of 'restricted industries' with\nlimited data. We additionally create two new additional metrics, the mb-index\nand db-index, to quantify bias, considering the idea that bias occurs due to\nboth intrinsic model architecture and dataset.",
        "pdf_link": "https://arxiv.org/pdf/2403.13925v1.pdf"
    },
    {
        "title": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval",
        "authors": [
            "Shengjie Ma",
            "Chong Chen",
            "Qi Chu",
            "Jiaxin Mao"
        ],
        "published": "2024-03-27T09:46:56Z",
        "summary": "Collecting relevant judgments for legal case retrieval is a challenging and\ntime-consuming task. Accurately judging the relevance between two legal cases\nrequires a considerable effort to read the lengthy text and a high level of\ndomain expertise to extract Legal Facts and make juridical judgments. With the\nadvent of advanced large language models, some recent studies have suggested\nthat it is promising to use LLMs for relevance judgment. Nonetheless, the\nmethod of employing a general large language model for reliable relevance\njudgments in legal case retrieval is yet to be thoroughly explored. To fill\nthis research gap, we devise a novel few-shot workflow tailored to the relevant\njudgment of legal cases. The proposed workflow breaks down the annotation\nprocess into a series of stages, imitating the process employed by human\nannotators and enabling a flexible integration of expert reasoning to enhance\nthe accuracy of relevance judgments. By comparing the relevance judgments of\nLLMs and human experts, we empirically show that we can obtain reliable\nrelevance judgments with the proposed workflow. Furthermore, we demonstrate the\ncapacity to augment existing legal case retrieval models through the synthesis\nof data generated by the large language model.",
        "pdf_link": "https://arxiv.org/pdf/2403.18405v1.pdf"
    },
    {
        "title": "From RAGs to riches: Using large language models to write documents for clinical trials",
        "authors": [
            "Nigel Markey",
            "Ilyass El-Mansouri",
            "Gaetan Rensonnet",
            "Casper van Langen",
            "Christoph Meier"
        ],
        "published": "2024-02-26T08:59:05Z",
        "summary": "Clinical trials require numerous documents to be written -- protocols,\nconsent forms, clinical study reports and others. Large language models (LLMs)\noffer the potential to rapidly generate first versions of these documents,\nhowever there are concerns about the quality of their output Here we report an\nevaluation of LLMs in generating parts of one such document, clinical trial\nprotocols. We find that an offthe-shelf LLM delivers reasonable results,\nespecially when assessing content relevance and the correct use of terminology.\nHowever, deficiencies remain: specifically clinical thinking and logic, and\nappropriate use of references. To improve performance, we used\nretrieval-augmented generation (RAG) to prompt an LLM with accurate up-to-date\ninformation. As a result of using RAG, the writing quality of the LLM improves\nsubstantially, which has implications for the practical useability of LLMs in\nclinical trial-related writing.",
        "pdf_link": "https://arxiv.org/pdf/2402.16406v1.pdf"
    },
    {
        "title": "PRE: A Peer Review Based Large Language Model Evaluator",
        "authors": [
            "Zhumin Chu",
            "Qingyao Ai",
            "Yiteng Tu",
            "Haitao Li",
            "Yiqun Liu"
        ],
        "published": "2024-01-28T12:33:14Z",
        "summary": "The impressive performance of large language models (LLMs) has attracted\nconsiderable attention from the academic and industrial communities. Besides\nhow to construct and train LLMs, how to effectively evaluate and compare the\ncapacity of LLMs has also been well recognized as an important yet difficult\nproblem. Existing paradigms rely on either human annotators or model-based\nevaluators to evaluate the performance of LLMs on different tasks. However,\nthese paradigms often suffer from high cost, low generalizability, and\ninherited biases in practice, which make them incapable of supporting the\nsustainable development of LLMs in long term. In order to address these issues,\ninspired by the peer review systems widely used in academic publication\nprocess, we propose a novel framework that can automatically evaluate LLMs\nthrough a peer-review process. Specifically, for the evaluation of a specific\ntask, we first construct a small qualification exam to select \"reviewers\" from\na couple of powerful LLMs. Then, to actually evaluate the \"submissions\" written\nby different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to\nrate or compare the submissions. The final ranking of evaluatee LLMs is\ngenerated based on the results provided by all reviewers. We conducted\nextensive experiments on text summarization tasks with eleven LLMs including\nGPT-4. The results demonstrate the existence of biasness when evaluating using\na single LLM. Also, our PRE model outperforms all the baselines, illustrating\nthe effectiveness of the peer review mechanism.",
        "pdf_link": "https://arxiv.org/pdf/2401.15641v1.pdf"
    },
    {
        "title": "Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations",
        "authors": [
            "Ankit Pal",
            "Malaikannan Sankarasubbu"
        ],
        "published": "2024-02-10T19:08:28Z",
        "summary": "Large language models have the potential to be valuable in the healthcare\nindustry, but it's crucial to verify their safety and effectiveness through\nrigorous evaluation. For this purpose, we comprehensively evaluated both\nopen-source LLMs and Google's new multimodal LLM called Gemini across Medical\nreasoning, hallucination detection, and Medical Visual Question Answering\ntasks. While Gemini showed competence, it lagged behind state-of-the-art models\nlike MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved\nan accuracy of 61.45\\% on the medical VQA dataset, significantly lower than\nGPT-4V's score of 88\\%. Our analysis revealed that Gemini is highly susceptible\nto hallucinations, overconfidence, and knowledge gaps, which indicate risks if\ndeployed uncritically. We also performed a detailed analysis by medical subject\nand test type, providing actionable feedback for developers and clinicians. To\nmitigate risks, we applied prompting strategies that improved performance.\nAdditionally, we facilitated future research and development by releasing a\nPython module for medical LLM evaluation and establishing a dedicated\nleaderboard on Hugging Face for medical domain LLMs. Python module can be found\nat https://github.com/promptslab/RosettaEval",
        "pdf_link": "https://arxiv.org/pdf/2402.07023v1.pdf"
    },
    {
        "title": "Bypassing LLM Watermarks with Color-Aware Substitutions",
        "authors": [
            "Qilong Wu",
            "Varun Chandrasekaran"
        ],
        "published": "2024-03-19T17:54:39Z",
        "summary": "Watermarking approaches are proposed to identify if text being circulated is\nhuman or large language model (LLM) generated. The state-of-the-art\nwatermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate\nspecific (``green'') tokens. However, determining the robustness of this\nwatermarking method is an open problem. Existing attack methods fail to evade\ndetection for longer text segments. We overcome this limitation, and propose\n{\\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware''\nattack. SCTS obtains color information by strategically prompting the\nwatermarked LLM and comparing output tokens frequencies. It uses this\ninformation to determine token colors, and substitutes green tokens with\nnon-green ones. In our experiments, SCTS successfully evades watermark\ndetection using fewer number of edits than related work. Additionally, we show\nboth theoretically and empirically that SCTS can remove the watermark for\narbitrarily long watermarked text.",
        "pdf_link": "https://arxiv.org/pdf/2403.14719v1.pdf"
    },
    {
        "title": "Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus",
        "authors": [
            "Seungpil Lee",
            "Woochang Sim",
            "Donghyeon Shin",
            "Sanha Hwang",
            "Wongyu Seo",
            "Jiwon Park",
            "Seokki Lee",
            "Sejin Kim",
            "Sundong Kim"
        ],
        "published": "2024-03-18T13:50:50Z",
        "summary": "The existing methods for evaluating the inference abilities of Large Language\nModels (LLMs) have been results-centric, making it difficult to assess the\ninference process. We introduce a new approach using the Abstract and Reasoning\nCorpus (ARC) dataset to evaluate the inference and contextual understanding\nabilities of large language models in a process-centric manner. ARC demands\nrigorous logical structures for problem-solving, making it a benchmark that\nfacilitates the comparison of model inference abilities with humans.\nExperimental results confirm that while large language models possess weak\ninference abilities, they still lag in terms of logical coherence,\ncompositionality, and productivity. Our experiments highlight the reasoning\ncapabilities of LLMs, proposing development paths for achieving human-level\nreasoning.",
        "pdf_link": "https://arxiv.org/pdf/2403.11793v1.pdf"
    },
    {
        "title": "Gradient-Free Adaptive Global Pruning for Pre-trained Language Models",
        "authors": [
            "Guangji Bai",
            "Yijiang Li",
            "Chen Ling",
            "Kibaek Kim",
            "Liang Zhao"
        ],
        "published": "2024-02-28T00:09:07Z",
        "summary": "The transformative impact of large language models (LLMs) like LLaMA and GPT\non natural language processing is countered by their prohibitive computational\ndemands. Pruning has emerged as a pivotal compression strategy, introducing\nsparsity to enhance both memory and computational efficiency. Yet, traditional\nglobal pruning is impractical for LLMs due to scalability issues, while local\npruning, despite its efficiency, leads to suboptimal solutions. Addressing\nthese challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework\nthat redefines the global pruning process into manageable, coordinated\nsubproblems, allowing for resource-efficient optimization with global\noptimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular\nfunctions and leverages auxiliary variables for problem decomposition, not only\nfacilitates a pragmatic application on LLMs but also demonstrates significant\nperformance improvements, particularly in high-sparsity regimes where it\nsurpasses current state-of-the-art methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.17946v2.pdf"
    },
    {
        "title": "Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?",
        "authors": [
            "Nir Fulman",
            "Abdulkadir Memduho\u011flu",
            "Alexander Zipf"
        ],
        "published": "2024-01-08T20:08:04Z",
        "summary": "We present a benchmark for assessing the capability of Large Language Models\n(LLMs) to discern intercardinal directions between geographic locations and\napply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark\nspecifically evaluates whether LLMs exhibit a hierarchical spatial bias similar\nto humans, where judgments about individual locations' spatial relationships\nare influenced by the perceived relationships of the larger groups that contain\nthem. To investigate this, we formulated 14 questions focusing on well-known\nAmerican cities. Seven questions were designed to challenge the LLMs with\nscenarios potentially influenced by the orientation of larger geographical\nunits, such as states or countries, while the remaining seven targeted\nlocations less susceptible to such hierarchical categorization. Among the\ntested models, GPT-4 exhibited superior performance with 55.3% accuracy,\nfollowed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed\nsignificantly reduced accuracy on tasks with suspected hierarchical bias. For\nexample, GPT-4's accuracy dropped to 32.9% on these tasks, compared to 85.7% on\nothers. Despite these inaccuracies, the models identified the nearest cardinal\ndirection in most cases, suggesting associative learning, embodying human-like\nmisconceptions. We discuss the potential of text-based data representing\ngeographic relationships directly to improve the spatial reasoning capabilities\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.04218v1.pdf"
    },
    {
        "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
        "authors": [
            "Dayou Du",
            "Yijia Zhang",
            "Shijie Cao",
            "Jiaqi Guo",
            "Ting Cao",
            "Xiaowen Chu",
            "Ningyi Xu"
        ],
        "published": "2024-02-16T12:27:15Z",
        "summary": "The upscaling of Large Language Models (LLMs) has yielded impressive advances\nin natural language processing, yet it also poses significant deployment\nchallenges. Weight quantization has emerged as a widely embraced solution to\nreduce memory and computational demands. This paper introduces BitDistiller, a\nframework that synergizes Quantization-Aware Training (QAT) with Knowledge\nDistillation (KD) to boost the performance of LLMs at ultra-low precisions\n(sub-4-bit). Specifically, BitDistiller first incorporates a tailored\nasymmetric quantization and clipping technique to maximally preserve the\nfidelity of quantized weights, and then proposes a novel Confidence-Aware\nKullback-Leibler Divergence (CAKLD) objective, which is employed in a\nself-distillation manner to enable faster convergence and superior model\nperformance. Empirical evaluations demonstrate that BitDistiller significantly\nsurpasses existing methods in both 3-bit and 2-bit configurations on general\nlanguage understanding and complex reasoning benchmarks. Notably, BitDistiller\nis shown to be more cost-effective, demanding fewer data and training\nresources. The code is available at https://github.com/DD-DuDa/BitDistiller.",
        "pdf_link": "https://arxiv.org/pdf/2402.10631v1.pdf"
    },
    {
        "title": "EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models",
        "authors": [
            "Ruisi Zhang",
            "Farinaz Koushanfar"
        ],
        "published": "2024-02-27T23:30:17Z",
        "summary": "This paper introduces EmMark,a novel watermarking framework for protecting\nthe intellectual property (IP) of embedded large language models deployed on\nresource-constrained edge devices. To address the IP theft risks posed by\nmalicious end-users, EmMark enables proprietors to authenticate ownership by\nquerying the watermarked model weights and matching the inserted signatures.\nEmMark's novelty lies in its strategic watermark weight parameters selection,\nnsuring robustness and maintaining model quality. Extensive proof-of-concept\nevaluations of models from OPT and LLaMA-2 families demonstrate EmMark's\nfidelity, achieving 100% success in watermark extraction with model performance\npreservation. EmMark also showcased its resilience against watermark removal\nand forging attacks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17938v1.pdf"
    },
    {
        "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
        "authors": [
            "Norah Alzahrani",
            "Hisham Abdullah Alyahya",
            "Yazeed Alnumay",
            "Sultan Alrashed",
            "Shaykhah Alsubaie",
            "Yusef Almushaykeh",
            "Faisal Mirza",
            "Nouf Alotaibi",
            "Nora Altwairesh",
            "Areeb Alowisheq",
            "M Saiful Bari",
            "Haidar Khan"
        ],
        "published": "2024-02-01T19:12:25Z",
        "summary": "Large Language Model (LLM) leaderboards based on benchmark rankings are\nregularly used to guide practitioners in model selection. Often, the published\nleaderboard rankings are taken at face value - we show this is a (potentially\ncostly) mistake. Under existing leaderboards, the relative performance of LLMs\nis highly sensitive to (often minute) details. We show that for popular\nmultiple choice question benchmarks (e.g. MMLU) minor perturbations to the\nbenchmark, such as changing the order of choices or the method of answer\nselection, result in changes in rankings up to 8 positions. We explain this\nphenomenon by conducting systematic experiments over three broad categories of\nbenchmark perturbations and identifying the sources of this behavior. Our\nanalysis results in several best-practice recommendations, including the\nadvantage of a hybrid scoring method for answer selection. Our study highlights\nthe dangers of relying on simple benchmark evaluations and charts the path for\nmore robust evaluation schemes on the existing benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01781v1.pdf"
    },
    {
        "title": "LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging",
        "authors": [
            "Weimin Fu",
            "Kaichen Yang",
            "Raj Gautam Dutta",
            "Xiaolong Guo",
            "Gang Qu"
        ],
        "published": "2024-01-28T19:45:25Z",
        "summary": "This paper presents LLM4SecHW, a novel framework for hardware debugging that\nleverages domain specific Large Language Model (LLM). Despite the success of\nLLMs in automating various software development tasks, their application in the\nhardware security domain has been limited due to the constraints of commercial\nLLMs and the scarcity of domain specific data. To address these challenges, we\npropose a unique approach to compile a dataset of open source hardware design\ndefects and their remediation steps, utilizing version control data. This\ndataset provides a substantial foundation for training machine learning models\nfor hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this\ndataset, enabling the identification and rectification of bugs in hardware\ndesigns. This pioneering approach offers a reference workflow for the\napplication of fine tuning domain specific LLMs in other research areas. We\nevaluate the performance of our proposed system on various open source hardware\ndesigns, demonstrating its efficacy in accurately identifying and correcting\ndefects. Our work brings a new perspective on automating the quality control\nprocess in hardware design.",
        "pdf_link": "https://arxiv.org/pdf/2401.16448v1.pdf"
    },
    {
        "title": "Misconfidence-based Demonstration Selection for LLM In-Context Learning",
        "authors": [
            "Shangqing Xu",
            "Chao Zhang"
        ],
        "published": "2024-01-12T00:11:24Z",
        "summary": "In-context learning with large language models (LLMs) excels at adapting to\nvarious tasks rapidly. However, its success hinges on carefully selecting\ndemonstrations, which remains an obstacle in practice. Current approaches to\nthis problem either rely on hard-to-acquire external supervision or require\nfrequent interactions with LLMs, resulting in high costs. We propose a new\nmethod called In-Context Reflection (ICR) to overcome these challenges. ICR\nstrategically selects demonstrations to reduce the discrepancy between the\nLLM's outputs and the actual input-output mappings. Specifically, ICR starts\nwith a random set of initial demonstrations, then iteratively refines it. In\neach step, it analyzes a pool of candidate examples and identifies the ones\nmost likely to challenge the LLM's current understanding, measured by a new\nmetric called misconfidence. These most confusing examples are then selected to\nreplace the less informative demonstrations in the current set. Our\ncomprehensive evaluation across five diverse datasets encompassing 13 subtasks\nshows the efficacy of ICR. Compared to existing methods, ICR achieves an\naverage performance boost of 4%, while demonstrating remarkable cross-task\ngeneralization capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2401.06301v1.pdf"
    },
    {
        "title": "LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
        "authors": [
            "Lily Zhong",
            "Zilong Wang",
            "Jingbo Shang"
        ],
        "published": "2024-02-25T00:56:27Z",
        "summary": "Large language models (LLMs) are leading significant progress in code\ngeneration. Beyond one-pass code generation, recent works further integrate\nunit tests and program verifiers into LLMs to iteratively refine the generated\nprograms. However, these works consider the generated programs as an\nindivisible entity, which falls short for LLMs in debugging the programs,\nespecially when the programs contain complex logic flows and data operations.\nIn contrast, when human developers debug programs, they typically set\nbreakpoints and selectively examine runtime execution information. The\nexecution flow and the intermediate variables play a crucial role in the\ndebugging process, yet they are underutilized in the existing literature on\ncode generation. In this study, we introduce Large Language Model Debugger\n(LDB), a novel debugging framework that enables LLMs to refine their generated\nprograms with the runtime execution information. Specifically, LDB segments the\nprograms into basic blocks and tracks the values of intermediate variables\nafter each block throughout the runtime execution. This allows LLMs to\nconcentrate on simpler code units within the overall execution flow, verify\ntheir correctness against the task description block by block, and efficiently\npinpoint any potential errors. Experiments demonstrate that LDB consistently\nenhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and\nTransCoder benchmarks, archiving new state-of-the-art performance in code\ndebugging for various LLM selections.",
        "pdf_link": "https://arxiv.org/pdf/2402.16906v3.pdf"
    },
    {
        "title": "Intensive Care as One Big Sequence Modeling Problem",
        "authors": [
            "Vadim Liventsev",
            "Tobias Fritz"
        ],
        "published": "2024-02-27T13:36:55Z",
        "summary": "Reinforcement Learning in Healthcare is typically concerned with narrow\nself-contained tasks such as sepsis prediction or anesthesia control. However,\nprevious research has demonstrated the potential of generalist models (the\nprime example being Large Language Models) to outperform task-specific\napproaches due to their capability for implicit transfer learning. To enable\ntraining of foundation models for Healthcare as well as leverage the\ncapabilities of state of the art Transformer architectures, we propose the\nparadigm of Healthcare as Sequence Modeling, in which interaction between the\npatient and the healthcare provider is represented as an event stream and tasks\nlike diagnosis and treatment selection are modeled as prediction of future\nevents in the stream. To explore this paradigm experimentally we develop\nMIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous\nclinical records from MIMIC-IV dataset into a uniform event stream format,\ntrain a baseline model and explore its capabilities.",
        "pdf_link": "https://arxiv.org/pdf/2402.17501v1.pdf"
    },
    {
        "title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models",
        "authors": [
            "Zhiwei He",
            "Binglin Zhou",
            "Hongkun Hao",
            "Aiwei Liu",
            "Xing Wang",
            "Zhaopeng Tu",
            "Zhuosheng Zhang",
            "Rui Wang"
        ],
        "published": "2024-02-21T18:48:38Z",
        "summary": "Text watermarking technology aims to tag and identify content produced by\nlarge language models (LLMs) to prevent misuse. In this study, we introduce the\nconcept of ''cross-lingual consistency'' in text watermarking, which assesses\nthe ability of text watermarks to maintain their effectiveness after being\ntranslated into other languages. Preliminary empirical results from two LLMs\nand three watermarking methods reveal that current text watermarking\ntechnologies lack consistency when texts are translated into various languages.\nBased on this observation, we propose a Cross-lingual Watermark Removal Attack\n(CWRA) to bypass watermarking by first obtaining a response from an LLM in a\npivot language, which is then translated into the target language. CWRA can\neffectively remove watermarks by reducing the Area Under the Curve (AUC) from\n0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors\nthat contribute to the cross-lingual consistency in text watermarking and\npropose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.",
        "pdf_link": "https://arxiv.org/pdf/2402.14007v1.pdf"
    },
    {
        "title": "Supporting Student Decisions on Learning Recommendations: An LLM-Based Chatbot with Knowledge Graph Contextualization for Conversational Explainability and Mentoring",
        "authors": [
            "Hasan Abu-Rasheed",
            "Mohamad Hussam Abdulsalam",
            "Christian Weber",
            "Madjid Fathi"
        ],
        "published": "2024-01-16T17:31:35Z",
        "summary": "Student commitment towards a learning recommendation is not separable from\ntheir understanding of the reasons it was recommended to them; and their\nability to modify it based on that understanding. Among explainability\napproaches, chatbots offer the potential to engage the student in a\nconversation, similar to a discussion with a peer or a mentor. The capabilities\nof chatbots, however, are still not sufficient to replace a human mentor,\ndespite the advancements of generative AI (GenAI) and large language models\n(LLM). Therefore, we propose an approach to utilize chatbots as mediators of\nthe conversation and sources of limited and controlled generation of\nexplanations, to harvest the potential of LLMs while reducing their potential\nrisks at the same time. The proposed LLM-based chatbot supports students in\nunderstanding learning-paths recommendations. We use a knowledge graph (KG) as\na human-curated source of information, to regulate the LLM's output through\ndefining its prompt's context. A group chat approach is developed to connect\nstudents with human mentors, either on demand or in cases that exceed the\nchatbot's pre-defined tasks. We evaluate the chatbot with a user study, to\nprovide a proof-of-concept and highlight the potential requirements and\nlimitations of utilizing chatbots in conversational explainability.",
        "pdf_link": "https://arxiv.org/pdf/2401.08517v3.pdf"
    },
    {
        "title": "LLMs as Compiler for Arabic Programming Language",
        "authors": [
            "Serry Sibaee",
            "Omar Najar",
            "Lahouri Ghouti",
            "Anis Koubaa"
        ],
        "published": "2024-03-24T10:57:08Z",
        "summary": "In this paper we introduce APL (Arabic Programming Language) that uses Large\nlanguage models (LLM) as semi-compiler to covert Arabic text code to python\ncode then run the code. Designing a full pipeline from the structure of the APL\ntext then a prompt (using prompt engineering) then running the prodcued python\ncode using PyRunner. This project has a three parts first python library, a\nplayground with simple interface and this research paper.",
        "pdf_link": "https://arxiv.org/pdf/2403.16087v1.pdf"
    },
    {
        "title": "Agent3D-Zero: An Agent for Zero-shot 3D Understanding",
        "authors": [
            "Sha Zhang",
            "Di Huang",
            "Jiajun Deng",
            "Shixiang Tang",
            "Wanli Ouyang",
            "Tong He",
            "Yanyong Zhang"
        ],
        "published": "2024-03-18T14:47:03Z",
        "summary": "The ability to understand and reason the 3D real world is a crucial milestone\ntowards artificial general intelligence. The current common practice is to\nfinetune Large Language Models (LLMs) with 3D data and texts to enable 3D\nunderstanding. Despite their effectiveness, these approaches are inherently\nlimited by the scale and diversity of the available 3D data. Alternatively, in\nthis work, we introduce Agent3D-Zero, an innovative 3D-aware agent framework\naddressing the 3D scene understanding in a zero-shot manner. The essence of our\napproach centers on reconceptualizing the challenge of 3D scene perception as a\nprocess of understanding and synthesizing insights from multiple images,\ninspired by how our human beings attempt to understand 3D scenes. By\nconsolidating this idea, we propose a novel way to make use of a Large Visual\nLanguage Model (VLM) via actively selecting and analyzing a series of\nviewpoints for 3D understanding. Specifically, given an input 3D scene,\nAgent3D-Zero first processes a bird's-eye view image with custom-designed\nvisual prompts, then iteratively chooses the next viewpoints to observe and\nsummarize the underlying knowledge. A distinctive advantage of Agent3D-Zero is\nthe introduction of novel visual prompts, which significantly unleash the VLMs'\nability to identify the most informative viewpoints and thus facilitate\nobserving 3D scenes. Extensive experiments demonstrate the effectiveness of the\nproposed framework in understanding diverse and previously unseen 3D\nenvironments.",
        "pdf_link": "https://arxiv.org/pdf/2403.11835v1.pdf"
    },
    {
        "title": "LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition",
        "authors": [
            "Junjie Ye",
            "Nuo Xu",
            "Yikun Wang",
            "Jie Zhou",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "published": "2024-02-22T14:19:56Z",
        "summary": "Despite the impressive capabilities of large language models (LLMs), their\nperformance on information extraction tasks is still not entirely satisfactory.\nHowever, their remarkable rewriting capabilities and extensive world knowledge\noffer valuable insights to improve these tasks. In this paper, we propose\n$LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot\nNER task. To overcome the limitations of existing data augmentation methods\nthat compromise semantic integrity and address the uncertainty inherent in\nLLM-generated text, we leverage the distinctive characteristics of the NER task\nby augmenting the original data at both the contextual and entity levels. Our\napproach involves employing 14 contextual rewriting strategies, designing\nentity replacements of the same type, and incorporating noise injection to\nenhance robustness. Extensive experiments demonstrate the effectiveness of our\napproach in enhancing NER model performance with limited data. Furthermore,\nadditional analyses provide further evidence supporting the assertion that the\nquality of the data we generate surpasses that of other existing methods.",
        "pdf_link": "https://arxiv.org/pdf/2402.14568v1.pdf"
    },
    {
        "title": "SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation",
        "authors": [
            "Shuangrui Ding",
            "Zihan Liu",
            "Xiaoyi Dong",
            "Pan Zhang",
            "Rui Qian",
            "Conghui He",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "published": "2024-02-27T16:15:28Z",
        "summary": "We present SongComposer, an innovative LLM designed for song composition. It\ncould understand and generate melodies and lyrics in symbolic song\nrepresentations, by leveraging the capability of LLM. Existing music-related\nLLM treated the music as quantized audio signals, while such implicit encoding\nleads to inefficient encoding and poor flexibility. In contrast, we resort to\nsymbolic song representation, the mature and efficient way humans designed for\nmusic, and enable LLM to explicitly compose songs like humans. In practice, we\ndesign a novel tuple design to format lyric and three note attributes (pitch,\nduration, and rest duration) in the melody, which guarantees the correct LLM\nunderstanding of musical symbols and realizes precise alignment between lyrics\nand melody. To impart basic music understanding to LLM, we carefully collected\nSongCompose-PT, a large-scale song pretraining dataset that includes lyrics,\nmelodies, and paired lyrics-melodies in either Chinese or English. After\nadequate pre-training, 10K carefully crafted QA pairs are used to empower the\nLLM with the instruction-following capability and solve diverse tasks. With\nextensive experiments, SongComposer demonstrates superior performance in\nlyric-to-melody generation, melody-to-lyric generation, song continuation, and\ntext-to-song creation, outperforming advanced LLMs like GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.17645v1.pdf"
    },
    {
        "title": "LG-Traj: LLM Guided Pedestrian Trajectory Prediction",
        "authors": [
            "Pranav Singh Chib",
            "Pravendra Singh"
        ],
        "published": "2024-03-12T19:06:23Z",
        "summary": "Accurate pedestrian trajectory prediction is crucial for various\napplications, and it requires a deep understanding of pedestrian motion\npatterns in dynamic environments. However, existing pedestrian trajectory\nprediction methods still need more exploration to fully leverage these motion\npatterns. This paper investigates the possibilities of using Large Language\nModels (LLMs) to improve pedestrian trajectory prediction tasks by inducing\nmotion cues. We introduce LG-Traj, a novel approach incorporating LLMs to\ngenerate motion cues present in pedestrian past/observed trajectories. Our\napproach also incorporates motion cues present in pedestrian future\ntrajectories by clustering future trajectories of training data using a mixture\nof Gaussians. These motion cues, along with pedestrian coordinates, facilitate\na better understanding of the underlying representation. Furthermore, we\nutilize singular value decomposition to augment the observed trajectories,\nincorporating them into the model learning process to further enhance\nrepresentation learning. Our method employs a transformer-based architecture\ncomprising a motion encoder to model motion patterns and a social decoder to\ncapture social interactions among pedestrians. We demonstrate the effectiveness\nof our approach on popular pedestrian trajectory prediction benchmarks, namely\nETH-UCY and SDD, and present various ablation experiments to validate our\napproach.",
        "pdf_link": "https://arxiv.org/pdf/2403.08032v1.pdf"
    },
    {
        "title": "Copyright Traps for Large Language Models",
        "authors": [
            "Matthieu Meeus",
            "Igor Shilov",
            "Manuel Faysse",
            "Yves-Alexandre de Montjoye"
        ],
        "published": "2024-02-14T18:09:53Z",
        "summary": "Questions of fair use of copyright-protected content to train Large Language\nModels (LLMs) are being very actively debated. Document-level inference has\nbeen proposed as a new task: inferring from black-box access to the trained\nmodel whether a piece of content has been seen during training. SOTA methods\nhowever rely on naturally occurring memorization of (part of) the content.\nWhile very effective against models that memorize a lot, we hypothesize--and\nlater confirm--that they will not work against models that do not naturally\nmemorize, e.g. medium-size 1B models. We here propose to use copyright traps,\nthe inclusion of fictitious entries in original content, to detect the use of\ncopyrighted materials in LLMs with a focus on models where memorization does\nnot naturally occur. We carefully design an experimental setup, randomly\ninserting traps into original content (books) and train a 1.3B LLM. We first\nvalidate that the use of content in our target model would be undetectable\nusing existing methods. We then show, contrary to intuition, that even\nmedium-length trap sentences repeated a significant number of times (100) are\nnot detectable using existing methods. However, we show that longer sequences\nrepeated a large number of times can be reliably detected (AUC=0.75) and used\nas copyright traps. We further improve these results by studying how the number\nof times a sequence is seen improves detectability, how sequences with higher\nperplexity tend to be memorized more, and how taking context into account\nfurther improves detectability.",
        "pdf_link": "https://arxiv.org/pdf/2402.09363v1.pdf"
    },
    {
        "title": "PAL: Proxy-Guided Black-Box Attack on Large Language Models",
        "authors": [
            "Chawin Sitawarin",
            "Norman Mu",
            "David Wagner",
            "Alexandre Araujo"
        ],
        "published": "2024-02-15T02:54:49Z",
        "summary": "Large Language Models (LLMs) have surged in popularity in recent months, but\nthey have demonstrated concerning capabilities to generate harmful content when\nmanipulated. While techniques like safety fine-tuning aim to minimize harmful\nuse, recent works have shown that LLMs remain vulnerable to attacks that elicit\ntoxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs\n(PAL), the first optimization-based attack on LLMs in a black-box query-only\nsetting. In particular, it relies on a surrogate model to guide the\noptimization and a sophisticated loss designed for real-world LLM APIs. Our\nattack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on\nLlama-2-7B, compared to 4% for the current state of the art. We also propose\nGCG++, an improvement to the GCG attack that reaches 94% ASR on white-box\nLlama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple\nbaseline for query-based attacks. We believe the techniques proposed in this\nwork will enable more comprehensive safety testing of LLMs and, in the long\nterm, the development of better security guardrails. The code can be found at\nhttps://github.com/chawins/pal.",
        "pdf_link": "https://arxiv.org/pdf/2402.09674v1.pdf"
    },
    {
        "title": "On Detecting Cherry-picking in News Coverage Using Large Language Models",
        "authors": [
            "Israa Jaradat",
            "Haiqi Zhang",
            "Chengkai Li"
        ],
        "published": "2024-01-11T04:03:35Z",
        "summary": "Cherry-picking refers to the deliberate selection of evidence or facts that\nfavor a particular viewpoint while ignoring or distorting evidence that\nsupports an opposing perspective. Manually identifying instances of\ncherry-picked statements in news stories can be challenging, particularly when\nthe opposing viewpoint's story is absent. This study introduces Cherry, an\ninnovative approach for automatically detecting cherry-picked statements in\nnews articles by finding missing important statements in the target news story.\nCherry utilizes the analysis of news coverage from multiple sources to identify\ninstances of cherry-picking. Our approach relies on language models that\nconsider contextual information from other news sources to classify statements\nbased on their importance to the event covered in the target news story.\nFurthermore, this research introduces a novel dataset specifically designed for\ncherry-picking detection, which was used to train and evaluate the performance\nof the models. Our best performing model achieves an F-1 score of about %89 in\ndetecting important statements when tested on unseen set of news stories.\nMoreover, results show the importance incorporating external knowledge from\nalternative unbiased narratives when assessing a statement's importance.",
        "pdf_link": "https://arxiv.org/pdf/2401.05650v1.pdf"
    },
    {
        "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models",
        "authors": [
            "Hanxing Ding",
            "Liang Pang",
            "Zihao Wei",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-16T11:55:40Z",
        "summary": "Hallucinations pose a significant challenge for the practical implementation\nof large language models (LLMs). The utilization of parametric knowledge in\ngenerating factual content is constrained by the limited knowledge of LLMs,\npotentially resulting in internal hallucinations. While incorporating external\ninformation can help fill knowledge gaps, it also introduces the risk of\nirrelevant information, thereby increasing the likelihood of external\nhallucinations. A careful and balanced integration of the parametric knowledge\nwithin LLMs with external information is crucial to alleviate hallucinations.\nIn this study, we present Rowen, a novel approach that enhances LLMs with a\nselective retrieval augmentation process tailored to address hallucinated\noutputs. This process is governed by a multilingual semantic-aware detection\nmodule, which evaluates the consistency of the perturbed responses across\nvarious languages for the same queries. Upon detecting inconsistencies\nindicative of hallucinations, Rowen activates the retrieval of external\ninformation to rectify the model outputs. Rowen adeptly harmonizes the\nintrinsic parameters in LLMs with external knowledge sources, effectively\nmitigating hallucinations by ensuring a balanced integration of internal\nreasoning and external evidence. Through a comprehensive empirical analysis, we\ndemonstrate that Rowen surpasses the current state-of-the-art in both detecting\nand mitigating hallucinated content within the outputs of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.10612v1.pdf"
    },
    {
        "title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
        "authors": [
            "Qiheng Mao",
            "Zemin Liu",
            "Chenghao Liu",
            "Zhuo Li",
            "Jianling Sun"
        ],
        "published": "2024-02-04T05:51:14Z",
        "summary": "The integration of Large Language Models (LLMs) with Graph Representation\nLearning (GRL) marks a significant evolution in analyzing complex data\nstructures. This collaboration harnesses the sophisticated linguistic\ncapabilities of LLMs to improve the contextual understanding and adaptability\nof graph models, thereby broadening the scope and potential of GRL. Despite a\ngrowing body of research dedicated to integrating LLMs into the graph domain, a\ncomprehensive review that deeply analyzes the core components and operations\nwithin these models is notably lacking. Our survey fills this gap by proposing\na novel taxonomy that breaks down these models into primary components and\noperation techniques from a novel technical perspective. We further dissect\nrecent literature into two primary components including knowledge extractors\nand organizers, and two operation techniques including integration and training\nstratigies, shedding light on effective model design and training strategies.\nAdditionally, we identify and explore potential future research avenues in this\nnascent yet underexplored field, proposing paths for continued progress.",
        "pdf_link": "https://arxiv.org/pdf/2402.05952v1.pdf"
    },
    {
        "title": "Learning Shortcuts: On the Misleading Promise of NLU in Language Models",
        "authors": [
            "Geetanjali Bihani",
            "Julia Taylor Rayz"
        ],
        "published": "2024-01-17T21:55:15Z",
        "summary": "The advent of large language models (LLMs) has enabled significant\nperformance gains in the field of natural language processing. However, recent\nstudies have found that LLMs often resort to shortcuts when performing tasks,\ncreating an illusion of enhanced performance while lacking generalizability in\ntheir decision rules. This phenomenon introduces challenges in accurately\nassessing natural language understanding in LLMs. Our paper provides a concise\nsurvey of relevant research in this area and puts forth a perspective on the\nimplications of shortcut learning in the evaluation of language models,\nspecifically for NLU tasks. This paper urges more research efforts to be put\ntowards deepening our comprehension of shortcut learning, contributing to the\ndevelopment of more robust language models, and raising the standards of NLU\nevaluation in real-world scenarios.",
        "pdf_link": "https://arxiv.org/pdf/2401.09615v2.pdf"
    },
    {
        "title": "ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies",
        "authors": [
            "Oren Sultan",
            "Yonatan Bitton",
            "Ron Yosef",
            "Dafna Shahaf"
        ],
        "published": "2024-03-02T08:53:40Z",
        "summary": "Analogy-making is central to human cognition, allowing us to adapt to novel\nsituations -- an ability that current AI systems still lack. Most analogy\ndatasets today focus on simple analogies (e.g., word analogies); datasets\nincluding complex types of analogies are typically manually curated and very\nsmall. We believe that this holds back progress in computational analogy. In\nthis work, we design a data generation pipeline, ParallelPARC (Parallel\nParagraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to\ncreate complex, paragraph-based analogies, as well as distractors, both simple\nand challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset\nof analogies between scientific processes. We publish a gold-set, validated by\nhumans, and a silver-set, generated automatically. We test LLMs' and humans'\nanalogy recognition in binary and multiple-choice settings, and found that\nhumans outperform the best models (~13% gap) after a light supervision. We\ndemonstrate that our silver-set is useful for training models. Lastly, we show\nchallenging distractors confuse LLMs, but not humans. We hope our pipeline will\nencourage research in this emerging field.",
        "pdf_link": "https://arxiv.org/pdf/2403.01139v3.pdf"
    },
    {
        "title": "Make Large Language Model a Better Ranker",
        "authors": [
            "Wenshuo Chao",
            "Zhi Zheng",
            "Hengshu Zhu",
            "Hao Liu"
        ],
        "published": "2024-03-28T07:22:16Z",
        "summary": "The evolution of Large Language Models (LLMs) has significantly enhanced\ncapabilities across various fields, leading to a paradigm shift in how\nRecommender Systems (RSs) are conceptualized and developed. However, existing\nresearch primarily focuses on point-wise and pair-wise recommendation\nparadigms. These approaches prove inefficient in LLM-based recommenders due to\nthe high computational cost of utilizing Large Language Models. While some\nstudies have delved into list-wise approaches, they fall short in ranking\ntasks. This shortfall is attributed to the misalignment between the objectives\nof ranking and language generation. To this end, this paper introduces the\nLanguage Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO\nis designed to bridge the gap between the capabilities of LLMs and the nuanced\nrequirements of ranking tasks within recommender systems. A key feature of ALRO\nis the introduction of soft lambda loss, an adaptation of lambda loss tailored\nto suit language generation tasks. Additionally, ALRO incorporates a\npermutation-sensitive learning mechanism that addresses position bias, a\nprevalent issue in generative models, without imposing additional computational\nburdens during inference. Our evaluative studies reveal that ALRO outperforms\nexisting embedding-based recommendation methods and the existing LLM-based\nrecommendation baselines, highlighting its efficacy.",
        "pdf_link": "https://arxiv.org/pdf/2403.19181v1.pdf"
    },
    {
        "title": "Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics",
        "authors": [
            "Zhu Liu",
            "Cunliang Kong",
            "Ying Liu",
            "Maosong Sun"
        ],
        "published": "2024-03-03T13:14:47Z",
        "summary": "Large language models have achieved remarkable success in general language\nunderstanding tasks. However, as a family of generative methods with the\nobjective of next token prediction, the semantic evolution with the depth of\nthese models are not fully explored, unlike their predecessors, such as\nBERT-like architectures. In this paper, we specifically investigate the\nbottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by\nprobing its hidden states at the end of each layer using a contextualized word\nidentification task. Our experiments show that the representations in lower\nlayers encode lexical semantics, while the higher layers, with weaker semantic\ninduction, are responsible for prediction. This is in contrast to models with\ndiscriminative objectives, such as mask language modeling, where the higher\nlayers obtain better lexical semantics. The conclusion is further supported by\nthe monotonic increase in performance via the hidden states for the last\nmeaningless symbols, such as punctuation, in the prompting strategy.",
        "pdf_link": "https://arxiv.org/pdf/2403.01509v1.pdf"
    },
    {
        "title": "Text-Guided Molecule Generation with Diffusion Language Model",
        "authors": [
            "Haisong Gong",
            "Qiang Liu",
            "Shu Wu",
            "Liang Wang"
        ],
        "published": "2024-02-20T14:29:02Z",
        "summary": "Text-guided molecule generation is a task where molecules are generated to\nmatch specific textual descriptions. Recently, most existing SMILES-based\nmolecule generation methods rely on an autoregressive architecture. In this\nwork, we propose the Text-Guided Molecule Generation with Diffusion Language\nModel (TGM-DLM), a novel approach that leverages diffusion models to address\nthe limitations of autoregressive methods. TGM-DLM updates token embeddings\nwithin the SMILES string collectively and iteratively, using a two-phase\ndiffusion generation process. The first phase optimizes embeddings from random\nnoise, guided by the text description, while the second phase corrects invalid\nSMILES strings to form valid molecular representations. We demonstrate that\nTGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for\nadditional data resources. Our findings underscore the remarkable effectiveness\nof TGM-DLM in generating coherent and precise molecules with specific\nproperties, opening new avenues in drug discovery and related scientific\ndomains. Code will be released at: https://github.com/Deno-V/tgm-dlm.",
        "pdf_link": "https://arxiv.org/pdf/2402.13040v1.pdf"
    },
    {
        "title": "Leveraging Zero-Shot Prompting for Efficient Language Model Distillation",
        "authors": [
            "Lukas V\u00f6ge",
            "Vincent Gurgul",
            "Stefan Lessmann"
        ],
        "published": "2024-03-23T16:51:52Z",
        "summary": "This paper introduces a novel approach for efficiently distilling LLMs into\nsmaller, application-specific models, significantly reducing operational costs\nand manual labor. Addressing the challenge of deploying computationally\nintensive LLMs in specific applications or edge devices, this technique\nutilizes LLMs' reasoning capabilities to generate labels and natural language\nrationales for unlabeled data. Our approach enhances both finetuning and\ndistillation by employing a multi-task training framework where student models\nmimic these rationales alongside teacher predictions. Key contributions include\nthe employment of zero-shot prompting to elicit teacher model rationales,\nreducing the necessity for handcrafted few-shot examples and lowering the\noverall token count required, which directly translates to cost savings given\nthe pay-per-token billing model of major tech companies' LLM APIs.\nAdditionally, the paper investigates the impact of explanation properties on\ndistillation efficiency, demonstrating that minimal performance loss occurs\neven when rationale augmentation is not applied across the entire dataset,\nfacilitating further reductions of tokens. This research marks a step toward\nthe efficient training of task-specific models with minimal human intervention,\noffering substantial cost-savings while maintaining, or even enhancing,\nperformance.",
        "pdf_link": "https://arxiv.org/pdf/2403.15886v1.pdf"
    },
    {
        "title": "AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach",
        "authors": [
            "Maryam Amirizaniani",
            "Tanya Roosta",
            "Aman Chadha",
            "Chirag Shah"
        ],
        "published": "2024-02-14T17:31:04Z",
        "summary": "As Large Language Models (LLMs) gain wider adoption in various contexts, it\nbecomes crucial to ensure they are reasonably safe, consistent, and reliable\nfor an application at hand. This may require probing or auditing them. Probing\nLLMs with varied iterations of a single question could reveal potential\ninconsistencies in their knowledge or functionality. However, a tool for\nperforming such audits with simple workflow and low technical threshold is\nlacking. In this demo, we introduce \"AuditLLM,\" a novel tool designed to\nevaluate the performance of various LLMs in a methodical way. AuditLLM's core\nfunctionality lies in its ability to test a given LLM by auditing it using\nmultiple probes generated from a single question, thereby identifying any\ninconsistencies in the model's understanding or operation. A reasonably robust,\nreliable, and consistent LLM should output semantically similar responses for a\nquestion asked differently or by different people. Based on this assumption,\nAuditLLM produces easily interpretable results regarding the LLM's\nconsistencies from a single question that the user enters. A certain level of\ninconsistency has been shown to be an indicator of potential bias,\nhallucinations, and other issues. One could then use the output of AuditLLM to\nfurther investigate issues with the aforementioned LLM. To facilitate\ndemonstration and practical uses, AuditLLM offers two key modes: (1) Live mode\nwhich allows instant auditing of LLMs by analyzing responses to real-time\nqueries; (2) Batch mode which facilitates comprehensive LLM auditing by\nprocessing multiple queries at once for in-depth analysis. This tool is\nbeneficial for both researchers and general users, as it enhances our\nunderstanding of LLMs' capabilities in generating responses, using a\nstandardized auditing platform.",
        "pdf_link": "https://arxiv.org/pdf/2402.09334v1.pdf"
    },
    {
        "title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy",
        "authors": [
            "Mengxi Xiao",
            "Qianqian Xie",
            "Ziyan Kuang",
            "Zhicheng Liu",
            "Kailai Yang",
            "Min Peng",
            "Weiguang Han",
            "Jimin Huang"
        ],
        "published": "2024-02-26T09:10:34Z",
        "summary": "Large Language Models (LLMs) can play a vital role in psychotherapy by\nadeptly handling the crucial task of cognitive reframing and overcoming\nchallenges such as shame, distrust, therapist skill variability, and resource\nscarcity. Previous LLMs in cognitive reframing mainly converted negative\nemotions to positive ones, but these approaches have limited efficacy, often\nnot promoting clients' self-discovery of alternative perspectives. In this\npaper, we unveil the Helping and Empowering through Adaptive Language in Mental\nEnhancement (HealMe) model. This novel cognitive reframing therapy method\neffectively addresses deep-rooted negative thoughts and fosters rational,\nbalanced perspectives. Diverging from traditional LLM methods, HealMe employs\nempathetic dialogue based on psychotherapeutic frameworks. It systematically\nguides clients through distinguishing circumstances from feelings,\nbrainstorming alternative viewpoints, and developing empathetic, actionable\nsuggestions. Moreover, we adopt the first comprehensive and expertly crafted\npsychological evaluation metrics, specifically designed to rigorously assess\nthe performance of cognitive reframing, in both AI-simulated dialogues and\nreal-world therapeutic conversations. Experimental results show that our model\noutperforms others in terms of empathy, guidance, and logical coherence,\ndemonstrating its effectiveness and potential positive impact on psychotherapy.",
        "pdf_link": "https://arxiv.org/pdf/2403.05574v2.pdf"
    },
    {
        "title": "SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition",
        "authors": [
            "Yihan Wu",
            "Soumi Maiti",
            "Yifan Peng",
            "Wangyou Zhang",
            "Chenda Li",
            "Yuyue Wang",
            "Xihua Wang",
            "Shinji Watanabe",
            "Ruihua Song"
        ],
        "published": "2024-01-31T18:06:29Z",
        "summary": "Recent advancements in language models have significantly enhanced\nperformance in multiple speech-related tasks. Existing speech language models\ntypically utilize task-dependent prompt tokens to unify various speech tasks in\na single model. However, this design omits the intrinsic connections between\ndifferent speech tasks, which can potentially boost the performance of each\ntask. In this work, we propose a novel decoder-only speech language model,\nSpeechComposer, that can unify common speech tasks by composing a fixed set of\nprompt tokens. Built upon four primary tasks -- speech synthesis, speech\nrecognition, speech language modeling, and text language modeling --\nSpeechComposer can easily extend to more speech tasks via compositions of\nwell-designed prompt tokens, like voice conversion and speech enhancement. The\nunification of prompt tokens also makes it possible for knowledge sharing among\ndifferent speech tasks in a more structured manner. Experimental results\ndemonstrate that our proposed SpeechComposer can improve the performance of\nboth primary tasks and composite tasks, showing the effectiveness of the shared\nprompt tokens. Remarkably, the unified decoder-only model achieves a comparable\nand even better performance than the baselines which are expert models designed\nfor single tasks.",
        "pdf_link": "https://arxiv.org/pdf/2401.18045v1.pdf"
    },
    {
        "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
        "authors": [
            "Yutong Li",
            "Lu Chen",
            "Aiwei Liu",
            "Kai Yu",
            "Lijie Wen"
        ],
        "published": "2024-03-05T01:13:56Z",
        "summary": "The literature review is an indispensable step in the research process. It\nprovides the benefit of comprehending the research problem and understanding\nthe current research situation while conducting a comparative analysis of prior\nworks. However, literature summary is challenging and time consuming. The\nprevious LLM-based studies on literature review mainly focused on the complete\nprocess, including literature retrieval, screening, and summarization. However,\nfor the summarization step, simple CoT method often lacks the ability to\nprovide extensive comparative summary. In this work, we firstly focus on the\nindependent literature summarization step and introduce ChatCite, an LLM agent\nwith human workflow guidance for comparative literature summary. This agent, by\nmimicking the human workflow, first extracts key elements from relevant\nliterature and then generates summaries using a Reflective Incremental\nMechanism. In order to better evaluate the quality of the generated summaries,\nwe devised a LLM-based automatic evaluation metric, G-Score, in refer to the\nhuman evaluation criteria. The ChatCite agent outperformed other models in\nvarious dimensions in the experiments. The literature summaries generated by\nChatCite can also be directly used for drafting literature reviews.",
        "pdf_link": "https://arxiv.org/pdf/2403.02574v1.pdf"
    },
    {
        "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
        "authors": [
            "Simon Holk",
            "Daniel Marta",
            "Iolanda Leite"
        ],
        "published": "2024-02-23T16:30:05Z",
        "summary": "Preference-based reinforcement learning (RL) has emerged as a new field in\nrobot learning, where humans play a pivotal role in shaping robot behavior by\nexpressing preferences on different sequences of state-action pairs. However,\nformulating realistic policies for robots demands responses from humans to an\nextensive array of queries. In this work, we approach the sample-efficiency\nchallenge by expanding the information collected per query to contain both\npreferences and optional text prompting. To accomplish this, we leverage the\nzero-shot capabilities of a large language model (LLM) to reason from the text\nprovided by humans. To accommodate the additional query information, we\nreformulate the reward learning objectives to contain flexible highlights --\nstate-action pairs that contain relatively high information and are related to\nthe features processed in a zero-shot fashion from a pretrained LLM. In both a\nsimulated scenario and a user study, we reveal the effectiveness of our work by\nanalyzing the feedback and its implications. Additionally, the collective\nfeedback collected serves to train a robot on socially compliant trajectories\nin a simulated social navigation landscape. We provide video examples of the\ntrained policies at https://sites.google.com/view/rl-predilect",
        "pdf_link": "https://arxiv.org/pdf/2402.15420v1.pdf"
    },
    {
        "title": "Unintended Impacts of LLM Alignment on Global Representation",
        "authors": [
            "Michael J. Ryan",
            "William Held",
            "Diyi Yang"
        ],
        "published": "2024-02-22T23:31:22Z",
        "summary": "Before being deployed for user-facing applications, developers align Large\nLanguage Models (LLMs) to user preferences through a variety of procedures,\nsuch as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference\nOptimization (DPO). Current evaluations of these procedures focus on benchmarks\nof instruction following, reasoning, and truthfulness. However, human\npreferences are not universal, and aligning to specific preference sets may\nhave unintended effects. We explore how alignment impacts performance along\nthree axes of global representation: English dialects, multilingualism, and\nopinions from and about countries worldwide. Our results show that current\nalignment procedures create disparities between English dialects and global\nopinions. We find alignment improves capabilities in several languages. We\nconclude by discussing design decisions that led to these unintended impacts\nand recommendations for more equitable preference tuning.",
        "pdf_link": "https://arxiv.org/pdf/2402.15018v1.pdf"
    },
    {
        "title": "A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models",
        "authors": [
            "Jaylen Jones",
            "Lingbo Mo",
            "Eric Fosler-Lussier",
            "Huan Sun"
        ],
        "published": "2024-02-18T18:56:07Z",
        "summary": "Counter narratives - informed responses to hate speech contexts designed to\nrefute hateful claims and de-escalate encounters - have emerged as an effective\nhate speech intervention strategy. While previous work has proposed automatic\ncounter narrative generation methods to aid manual interventions, the\nevaluation of these approaches remains underdeveloped. Previous automatic\nmetrics for counter narrative evaluation lack alignment with human judgment as\nthey rely on superficial reference comparisons instead of incorporating key\naspects of counter narrative quality as evaluation criteria. To address prior\nevaluation limitations, we propose a novel evaluation framework prompting LLMs\nto provide scores and feedback for generated counter narrative candidates using\n5 defined aspects derived from guidelines from counter narrative specialized\nNGOs. We found that LLM evaluators achieve strong alignment to human-annotated\nscores and feedback and outperform alternative metrics, indicating their\npotential as multi-aspect, reference-free and interpretable evaluators for\ncounter narrative evaluation.",
        "pdf_link": "https://arxiv.org/pdf/2402.11676v2.pdf"
    },
    {
        "title": "Chain-of-Thought Reasoning Without Prompting",
        "authors": [
            "Xuezhi Wang",
            "Denny Zhou"
        ],
        "published": "2024-02-15T18:55:41Z",
        "summary": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding substantially\noutperforms the standard greedy decoding.",
        "pdf_link": "https://arxiv.org/pdf/2402.10200v1.pdf"
    },
    {
        "title": "LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models",
        "authors": [
            "Mingxing Peng",
            "Xusen Guo",
            "Xianda Chen",
            "Meixin Zhu",
            "Kehua Chen",
            "Hao",
            "Yang",
            "Xuesong Wang",
            "Yinhai Wang"
        ],
        "published": "2024-03-27T08:34:55Z",
        "summary": "To ensure safe driving in dynamic environments, autonomous vehicles should\npossess the capability to accurately predict the lane change intentions of\nsurrounding vehicles in advance and forecast their future trajectories.\nExisting motion prediction approaches have ample room for improvement,\nparticularly in terms of long-term prediction accuracy and interpretability. In\nthis paper, we address these challenges by proposing LC-LLM, an explainable\nlane change prediction model that leverages the strong reasoning capabilities\nand self-explanation abilities of Large Language Models (LLMs). Essentially, we\nreformulate the lane change prediction task as a language modeling problem,\nprocessing heterogeneous driving scenario information in natural language as\nprompts for input into the LLM and employing a supervised fine-tuning technique\nto tailor the LLM specifically for our lane change prediction task. This allows\nus to utilize the LLM's powerful common sense reasoning abilities to understand\ncomplex interactive information, thereby improving the accuracy of long-term\npredictions. Furthermore, we incorporate explanatory requirements into the\nprompts in the inference stage. Therefore, our LC-LLM model not only can\npredict lane change intentions and trajectories but also provides explanations\nfor its predictions, enhancing the interpretability. Extensive experiments on\nthe large-scale highD dataset demonstrate the superior performance and\ninterpretability of our LC-LLM in lane change prediction task. To the best of\nour knowledge, this is the first attempt to utilize LLMs for predicting lane\nchange behavior. Our study shows that LLMs can encode comprehensive interaction\ninformation for driving behavior understanding.",
        "pdf_link": "https://arxiv.org/pdf/2403.18344v1.pdf"
    },
    {
        "title": "MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis",
        "authors": [
            "Mai A. Shaaban",
            "Adnan Khan",
            "Mohammad Yaqub"
        ],
        "published": "2024-03-22T19:19:51Z",
        "summary": "Chest X-ray images are commonly used for predicting acute and chronic\ncardiopulmonary conditions, but efforts to integrate them with structured\nclinical data face challenges due to incomplete electronic health records\n(EHR). This paper introduces MedPromptX, the first model to integrate\nmultimodal large language models (MLLMs), few-shot prompting (FP) and visual\ngrounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A\npre-trained MLLM is utilized to complement the missing EHR information,\nproviding a comprehensive understanding of patients' medical history.\nAdditionally, FP reduces the necessity for extensive training of MLLMs while\neffectively tackling the issue of hallucination. Nevertheless, the process of\ndetermining the optimal number of few-shot examples and selecting high-quality\ncandidates can be burdensome, yet it profoundly influences model performance.\nHence, we propose a new technique that dynamically refines few-shot data for\nreal-time adjustment to new patient scenarios. Moreover, VG aids in focusing\nthe model's attention on relevant regions of interest in X-ray images,\nenhancing the identification of abnormalities. We release MedPromptX-VQA, a new\nin-context visual question answering dataset encompassing interleaved image and\nEHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the\nSOTA performance of MedPromptX, achieving an 11% improvement in F1-score\ncompared to the baselines. Code and data are available at\nhttps://github.com/BioMedIA-MBZUAI/MedPromptX",
        "pdf_link": "https://arxiv.org/pdf/2403.15585v3.pdf"
    },
    {
        "title": "TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation",
        "authors": [
            "Peng Wang",
            "Xiang Wei",
            "Fangxu Hu",
            "Wenjuan Han"
        ],
        "published": "2024-02-11T15:50:35Z",
        "summary": "Natural language processing (NLP) is a key component of intelligent\ntransportation systems (ITS), but it faces many challenges in the\ntransportation domain, such as domain-specific knowledge and data, and\nmulti-modal inputs and outputs. This paper presents TransGPT, a novel\n(multi-modal) large language model for the transportation domain, which\nconsists of two independent variants: TransGPT-SM for single-modal data and\nTransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal\nTransportation dataset (STD) that contains textual data from various sources in\nthe transportation domain. TransGPT-MM is finetuned on a multi-modal\nTransportation dataset (MTD) that we manually collected from three areas of the\ntransportation domain: driving tests, traffic signs, and landmarks. We evaluate\nTransGPT on several benchmark datasets for different tasks in the\ntransportation domain, and show that it outperforms baseline models on most\ntasks. We also showcase the potential applications of TransGPT for traffic\nanalysis and modeling, such as generating synthetic traffic scenarios,\nexplaining traffic phenomena, answering traffic-related questions, providing\ntraffic recommendations, and generating traffic reports. This work advances the\nstate-of-the-art of NLP in the transportation domain and provides a useful tool\nfor ITS researchers and practitioners.",
        "pdf_link": "https://arxiv.org/pdf/2402.07233v1.pdf"
    },
    {
        "title": "Generating Zero-shot Abstractive Explanations for Rumour Verification",
        "authors": [
            "Iman Munire Bilal",
            "Preslav Nakov",
            "Rob Procter",
            "Maria Liakata"
        ],
        "published": "2024-01-23T12:29:37Z",
        "summary": "The task of rumour verification in social media concerns assessing the\nveracity of a claim on the basis of conversation threads that result from it.\nWhile previous work has focused on predicting a veracity label, here we\nreformulate the task to generate model-centric free-text explanations of a\nrumour's veracity. The approach is model agnostic in that it generalises to any\nmodel. Here we propose a novel GNN-based rumour verification model. We follow a\nzero-shot approach by first applying post-hoc explainability methods to score\nthe most important posts within a thread and then we use these posts to\ngenerate informative explanations using opinion-guided summarisation. To\nevaluate the informativeness of the explanatory summaries, we exploit the\nfew-shot learning capabilities of a large language model (LLM). Our experiments\nshow that LLMs can have similar agreement to humans in evaluating summaries.\nImportantly, we show explanatory abstractive summaries are more informative and\nbetter reflect the predicted rumour veracity than just using the highest\nranking posts in the thread.",
        "pdf_link": "https://arxiv.org/pdf/2401.12713v3.pdf"
    },
    {
        "title": "Similarity-based Neighbor Selection for Graph LLMs",
        "authors": [
            "Rui Li",
            "Jiwei Li",
            "Jiawei Han",
            "Guoyin Wang"
        ],
        "published": "2024-02-06T05:29:05Z",
        "summary": "Text-attributed graphs (TAGs) present unique challenges for direct processing\nby Language Learning Models (LLMs), yet their extensive commonsense knowledge\nand robust reasoning capabilities offer great promise for node classification\nin TAGs. Prior research in this field has grappled with issues such as\nover-squashing, heterophily, and ineffective graph information integration,\nfurther compounded by inconsistencies in dataset partitioning and\nunderutilization of advanced LLMs. To address these challenges, we introduce\nSimilarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor\nselection techniques, SNS effectively improves the quality of selected\nneighbors, thereby improving graph representation and alleviating issues like\nover-squashing and heterophily. Besides, as an inductive and training-free\napproach, SNS demonstrates superior generalization and scalability over\ntraditional GNN methods. Our comprehensive experiments, adhering to standard\ndataset partitioning practices, demonstrate that SNS, through simple prompt\ninteractions with LLMs, consistently outperforms vanilla GNNs and achieves\nstate-of-the-art results on datasets like PubMed in node classification,\nshowcasing LLMs' potential in graph structure understanding. Our research\nfurther underscores the significance of graph structure integration in LLM\napplications and identifies key factors for their success in node\nclassification. Code is available at https://github.com/ruili33/SNS.",
        "pdf_link": "https://arxiv.org/pdf/2402.03720v1.pdf"
    },
    {
        "title": "LLM Agents can Autonomously Hack Websites",
        "authors": [
            "Richard Fang",
            "Rohan Bindu",
            "Akul Gupta",
            "Qiusi Zhan",
            "Daniel Kang"
        ],
        "published": "2024-02-06T14:46:08Z",
        "summary": "In recent years, large language models (LLMs) have become increasingly\ncapable and can now interact with tools (i.e., call functions), read documents,\nand recursively call themselves. As a result, these LLMs can now function\nautonomously as agents. With the rise in capabilities of these agents, recent\nwork has speculated on how LLM agents would affect cybersecurity. However, not\nmuch is known about the offensive capabilities of LLM agents.\n  In this work, we show that LLM agents can autonomously hack websites,\nperforming tasks as complex as blind database schema extraction and SQL\ninjections without human feedback. Importantly, the agent does not need to know\nthe vulnerability beforehand. This capability is uniquely enabled by frontier\nmodels that are highly capable of tool use and leveraging extended context.\nNamely, we show that GPT-4 is capable of such hacks, but existing open-source\nmodels are not. Finally, we show that GPT-4 is capable of autonomously finding\nvulnerabilities in websites in the wild. Our findings raise questions about the\nwidespread deployment of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.06664v3.pdf"
    },
    {
        "title": "Stochastic Two Points Method for Deep Model Zeroth-order Optimization",
        "authors": [
            "Yijiang Pang",
            "Jiayu Zhou"
        ],
        "published": "2024-02-02T18:39:40Z",
        "summary": "Large foundation models, such as large language models, have performed\nexceptionally well in various application scenarios. Building or fully\nfine-tuning such large models is usually prohibitive due to either hardware\nbudget or lack of access to backpropagation. The zeroth-order methods offer a\npromising direction for tackling this challenge, where only forward passes are\nneeded to update the model. This paper introduces an efficient Stochastic\nTwo-Point (S2P) approach within the gradient-free regime. We present the\ntheoretical convergence properties of S2P under the general and relaxed\nsmoothness assumptions. The theoretical properties also shed light on a faster\nand more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new\nconvergence properties that better represent the dynamics of deep models in\ntraining. Our comprehensive empirical results show that AS2P is highly\neffective in optimizing objectives for large deep models, including language\nmodels, and outperforms standard methods across various model types and scales,\nwith 2 $\\times$ speed-up in training over most conducted tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.01621v1.pdf"
    },
    {
        "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
        "authors": [
            "Junzhe Chen",
            "Xuming Hu",
            "Shuodi Liu",
            "Shiyu Huang",
            "Wei-Wei Tu",
            "Zhaofeng He",
            "Lijie Wen"
        ],
        "published": "2024-02-26T11:31:48Z",
        "summary": "Recent advancements in large language models (LLMs) have revealed their\npotential for achieving autonomous agents possessing human-level intelligence.\nHowever, existing benchmarks for evaluating LLM Agents either use static\ndatasets, potentially leading to data leakage or focus only on single-agent\nscenarios, overlooking the complexities of multi-agent interactions. There is a\nlack of a benchmark that evaluates the diverse capabilities of LLM agents in\nmulti-agent, dynamic environments. To this end, we introduce LLMArena, a novel\nand easily extensible framework for evaluating the diverse capabilities of LLM\nin multi-agent dynamic environments. LLMArena encompasses seven distinct gaming\nenvironments, employing Trueskill scoring to assess crucial abilities in LLM\nagents, including spatial reasoning, strategic planning, numerical reasoning,\nrisk assessment, communication, opponent modeling, and team collaboration. We\nconduct an extensive experiment and human evaluation among different sizes and\ntypes of LLMs, showing that LLMs still have a significant journey ahead in\ntheir development towards becoming fully autonomous agents, especially in\nopponent modeling and team collaboration. We hope LLMArena could guide future\nresearch towards enhancing these capabilities in LLMs, ultimately leading to\nmore sophisticated and practical applications in dynamic, multi-agent settings.\nThe code and data will be available.",
        "pdf_link": "https://arxiv.org/pdf/2402.16499v1.pdf"
    },
    {
        "title": "Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia",
        "authors": [
            "Balamurali B T",
            "Jer-Ming Chen"
        ],
        "published": "2024-01-30T07:55:43Z",
        "summary": "Large language models (LLMs) find increasing applications in many fields.\nHere, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in\ntheir current form, as publicly available - for their ability to recognize\nAlzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual\ninput derived from spontaneous speech recordings. Zero-shot learning approach\nis used at two levels of independent queries, with the second query\n(chain-of-thought prompting) eliciting more detailed than the first. Each LLM\nchatbot's performance is evaluated on the prediction generated in terms of\naccuracy, sensitivity, specificity, precision and F1 score. LLM chatbots\ngenerated three-class outcome (\"AD\", \"CN\", or \"Unsure\"). When positively\nidentifying AD, Bard produced highest true-positives (89% recall) and highest\nF1 score (71%), but tended to misidentify CN as AD, with high confidence (low\n\"Unsure\" rates); for positively identifying CN, GPT-4 resulted in the highest\ntrue-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance\n(moderate \"Unsure\" rates). Overall, three LLM chatbots identify AD vs CN\nsurpassing chance-levels but do not currently satisfy clinical application.",
        "pdf_link": "https://arxiv.org/pdf/2402.01751v1.pdf"
    },
    {
        "title": "CrossGLG: LLM Guides One-shot Skeleton-based 3D Action Recognition in a Cross-level Manner",
        "authors": [
            "Tingbing Yan",
            "Wenzheng Zeng",
            "Yang Xiao",
            "Xingyu Tong",
            "Bo Tan",
            "Zhiwen Fang",
            "Zhiguo Cao",
            "Joey Tianyi Zhou"
        ],
        "published": "2024-03-15T07:51:35Z",
        "summary": "Most existing one-shot skeleton-based action recognition focuses on raw\nlow-level information (e.g., joint location), and may suffer from local\ninformation loss and low generalization ability. To alleviate these, we propose\nto leverage text description generated from large language models (LLM) that\ncontain high-level human knowledge, to guide feature learning, in a\nglobal-local-global way. Particularly, during training, we design $2$ prompts\nto gain global and local text descriptions of each action from an LLM. We first\nutilize the global text description to guide the skeleton encoder focus on\ninformative joints (i.e.,global-to-local). Then we build non-local interaction\nbetween local text and joint features, to form the final global representation\n(i.e., local-to-global). To mitigate the asymmetry issue between the training\nand inference phases, we further design a dual-branch architecture that allows\nthe model to perform novel class inference without any text input, also making\nthe additional inference cost neglectable compared with the base skeleton\nencoder. Extensive experiments on three different benchmarks show that CrossGLG\nconsistently outperforms the existing SOTA methods with large margins, and the\ninference cost (model size) is only $2.8$\\% than the previous SOTA. CrossGLG\ncan also serve as a plug-and-play module that can substantially enhance the\nperformance of different SOTA skeleton encoders with a neglectable cost during\ninference. The source code will be released soon.",
        "pdf_link": "https://arxiv.org/pdf/2403.10082v1.pdf"
    },
    {
        "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
        "authors": [
            "Elvis Dohmatob",
            "Yunzhen Feng",
            "Pu Yang",
            "Francois Charton",
            "Julia Kempe"
        ],
        "published": "2024-02-10T21:06:34Z",
        "summary": "As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.",
        "pdf_link": "https://arxiv.org/pdf/2402.07043v1.pdf"
    },
    {
        "title": "Detecting Multimedia Generated by Large AI Models: A Survey",
        "authors": [
            "Li Lin",
            "Neeraj Gupta",
            "Yue Zhang",
            "Hainan Ren",
            "Chun-Hao Liu",
            "Feng Ding",
            "Xin Wang",
            "Xin Li",
            "Luisa Verdoliva",
            "Shu Hu"
        ],
        "published": "2024-01-22T15:08:19Z",
        "summary": "The rapid advancement of Large AI Models (LAIMs), particularly diffusion\nmodels and large language models, has marked a new era where AI-generated\nmultimedia is increasingly integrated into various aspects of daily life.\nAlthough beneficial in numerous fields, this content presents significant\nrisks, including potential misuse, societal disruptions, and ethical concerns.\nConsequently, detecting multimedia generated by LAIMs has become crucial, with\na marked rise in related research. Despite this, there remains a notable gap in\nsystematic surveys that focus specifically on detecting LAIM-generated\nmultimedia. Addressing this, we provide the first survey to comprehensively\ncover existing research on detecting multimedia (such as text, images, videos,\naudio, and multimodal content) created by LAIMs. Specifically, we introduce a\nnovel taxonomy for detection methods, categorized by media modality, and\naligned with two perspectives: pure detection (aiming to enhance detection\nperformance) and beyond detection (adding attributes like generalizability,\nrobustness, and interpretability to detectors). Additionally, we have presented\na brief overview of generation mechanisms, public datasets, and online\ndetection tools to provide a valuable resource for researchers and\npractitioners in this field. Furthermore, we identify current challenges in\ndetection and propose directions for future research that address unexplored,\nongoing, and emerging issues in detecting multimedia generated by LAIMs. Our\naim for this survey is to fill an academic gap and contribute to global AI\nsecurity efforts, helping to ensure the integrity of information in the digital\nrealm. The project link is\nhttps://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.",
        "pdf_link": "https://arxiv.org/pdf/2402.00045v3.pdf"
    },
    {
        "title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics",
        "authors": [
            "Edward Y. Chang"
        ],
        "published": "2024-01-19T07:16:21Z",
        "summary": "Large language models (LLMs), while promising, face criticisms for biases,\nhallucinations, and a lack of reasoning capability. This paper introduces\nSocraSynth, a multi-LLM agent reasoning platform developed to mitigate these\nissues. SocraSynth utilizes conditional statistics and systematic context\nenhancement through continuous arguments, alongside adjustable debate\ncontentiousness levels. The platform typically involves a human moderator and\ntwo LLM agents representing opposing viewpoints on a given subject. SocraSynth\noperates in two main phases: knowledge generation and reasoning evaluation. In\nthe knowledge generation phase, the moderator defines the debate topic and\ncontentiousness level, prompting the agents to formulate supporting arguments\nfor their respective stances. The reasoning evaluation phase then employs\nSocratic reasoning and formal logic principles to appraise the quality of the\narguments presented. The dialogue concludes with the moderator adjusting the\ncontentiousness from confrontational to collaborative, gathering final,\nconciliatory remarks to aid in human reasoning and decision-making. Through\ncase studies in three distinct application domains, this paper showcases\nSocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,\ncomprehensive assessment, and enhanced collaboration. This underscores the\nvalue of multi-agent interactions in leveraging LLMs for advanced knowledge\nextraction and decision-making support.",
        "pdf_link": "https://arxiv.org/pdf/2402.06634v1.pdf"
    },
    {
        "title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language",
        "authors": [
            "Ming Wang",
            "Yuanzhong Liu",
            "Xiaoming Zhang",
            "Songlian Li",
            "Yijie Huang",
            "Chi Zhang",
            "Daling Wang",
            "Shi Feng",
            "Jigang Li"
        ],
        "published": "2024-02-26T15:05:16Z",
        "summary": "LLMs have demonstrated commendable performance across diverse domains.\nNevertheless, formulating high-quality prompts to effectively instruct LLMs\nposes a challenge for non-AI experts. Existing research in prompt engineering\nsuggests somewhat fragmented optimization principles and designs empirically\ndependent prompt optimizers. Unfortunately, these endeavors lack a structured\ndesign template, incurring high learning costs and resulting in low\nreusability. Inspired by structured reusable programming languages, we propose\nLangGPT, a dual-layer prompt design framework as the programming language for\nLLMs. LangGPT has an easy-to-learn normative structure and provides an extended\nstructure for migration and reuse. Experiments illustrate that LangGPT\nsignificantly enhances the capacity of LLMs to produce responses of superior\nquality compared to baselines. Moreover, LangGPT has proven effective in\nguiding LLMs to generate high-quality prompts. We have built a community on\nLangGPT to facilitate the tuition and sharing of prompt design. We also\nanalyzed the ease of use and reusability of LangGPT through a community user\nsurvey.",
        "pdf_link": "https://arxiv.org/pdf/2402.16929v1.pdf"
    },
    {
        "title": "Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models",
        "authors": [
            "Himanshu Beniwal",
            "Kowsik Nandagopan D",
            "Mayank Singh"
        ],
        "published": "2024-02-19T09:43:03Z",
        "summary": "Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their\nability to reason about and retain temporal information remains limited. This\nhinders their application in real-world scenarios where understanding the\nsequential nature of events is crucial. This paper experiments with\nstate-of-the-art models on a novel, large-scale temporal dataset,\n\\textbf{TempUN}, to reveal significant limitations in temporal retention and\nreasoning abilities. Interestingly, closed-source models indicate knowledge\ngaps more frequently, potentially suggesting a trade-off between uncertainty\nawareness and incorrect responses. Further, exploring various fine-tuning\napproaches yielded no major performance improvements. The associated dataset\nand code are available at the following URL\n(https://github.com/lingoiitgn/TempUN).",
        "pdf_link": "https://arxiv.org/pdf/2402.11997v1.pdf"
    },
    {
        "title": "EvoGrad: A Dynamic Take on the Winograd Schema Challenge with Human Adversaries",
        "authors": [
            "Jing Han Sun",
            "Ali Emami"
        ],
        "published": "2024-02-20T20:53:24Z",
        "summary": "While Large Language Models (LLMs) excel at the Winograd Schema Challenge\n(WSC), a coreference resolution task testing common-sense reasoning through\npronoun disambiguation, they struggle with instances that feature minor\nalterations or rewording. To address this, we introduce EvoGrad, an open-source\nplatform that harnesses a human-in-the-loop approach to create a dynamic\ndataset tailored to such altered WSC instances. Leveraging ChatGPT's\ncapabilities, we expand our task instances from 182 to 3,691, setting a new\nbenchmark for diverse common-sense reasoning datasets. Additionally, we\nintroduce the error depth metric, assessing model stability in dynamic tasks.\nOur results emphasize the challenge posed by EvoGrad: Even the best performing\nLLM, GPT-3.5, achieves an accuracy of 65.0% with an average error depth of 7.2,\na stark contrast to human performance of 92. 8% accuracy without perturbation\nerrors. This highlights ongoing model limitations and the value of dynamic\ndatasets in uncovering them.",
        "pdf_link": "https://arxiv.org/pdf/2402.13372v2.pdf"
    },
    {
        "title": "Large Language Model Adaptation for Networking",
        "authors": [
            "Duo Wu",
            "Xianda Wang",
            "Yaqi Qiao",
            "Zhi Wang",
            "Junchen Jiang",
            "Shuguang Cui",
            "Fangxin Wang"
        ],
        "published": "2024-02-04T04:21:34Z",
        "summary": "Many networking tasks now employ deep learning (DL) to solve complex\nprediction and system optimization problems. However, current design philosophy\nof DL-based algorithms entails intensive engineering overhead due to the manual\ndesign of deep neural networks (DNNs) for different networking tasks. Besides,\nDNNs tend to achieve poor generalization performance on unseen data\ndistributions/environments.\n  Motivated by the recent success of large language models (LLMs), for the\nfirst time, this work studies the LLM adaptation for networking to explore a\nmore sustainable design philosophy. With the massive pre-trained knowledge and\npowerful inference ability, LLM can serve as the foundation model, and is\nexpected to achieve \"one model for all\" with even better performance and\nstronger generalization for various tasks. In this paper, we present NetLLM,\nthe first LLM adaptation framework that efficiently adapts LLMs to solve\nnetworking problems. NetLLM addresses many practical challenges in LLM\nadaptation, from how to process task-specific information with LLMs, to how to\nimprove the efficiency of answer generation and acquiring domain knowledge for\nnetworking. Across three networking-related use cases - viewport prediction\n(VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we\nshowcase the effectiveness of NetLLM in LLM adaptation for networking. Results\nshow that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6%\nfor VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior\ngeneralization performance.",
        "pdf_link": "https://arxiv.org/pdf/2402.02338v1.pdf"
    },
    {
        "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
        "authors": [
            "Ojas Gramopadhye",
            "Saeel Sandeep Nachane",
            "Prateek Chanda",
            "Ganesh Ramakrishnan",
            "Kshitij Sharad Jadhav",
            "Yatin Nandwani",
            "Dinesh Raghu",
            "Sachindra Joshi"
        ],
        "published": "2024-03-07T20:48:40Z",
        "summary": "Large Language models (LLMs) have demonstrated significant potential in\ntransforming healthcare by automating tasks such as clinical documentation,\ninformation retrieval, and decision support. In this aspect, carefully\nengineered prompts have emerged as a powerful tool for using LLMs for medical\nscenarios, e.g., patient clinical scenarios. In this paper, we propose a\nmodified version of the MedQA-USMLE dataset, which is subjective, to mimic\nreal-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning\nbased on subjective response generation for the modified MedQA-USMLE dataset\nwith appropriate LM-driven forward reasoning for correct responses to the\nmedical questions. Keeping in mind the importance of response verification in\nthe medical setting, we utilize a reward training mechanism whereby the\nlanguage model also provides an appropriate verified response for a particular\nresponse to a clinical question. In this regard, we also include\nhuman-in-the-loop for different evaluation aspects. We develop better\nin-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt from\narXiv:2207.08143 for the subjective MedQA dataset and developing our\nincremental-reasoning prompt. Our evaluations show that the incremental\nreasoning prompt performs better than the modified codex prompt in certain\nscenarios. We also show that greedy decoding with the incremental reasoning\nmethod performs better than other strategies, such as prompt chaining and\neliminative reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2403.04890v1.pdf"
    },
    {
        "title": "Exploring ChatGPT and its Impact on Society",
        "authors": [
            "Md. Asraful Haque",
            "Shuai Li"
        ],
        "published": "2024-02-21T16:44:35Z",
        "summary": "Artificial intelligence has been around for a while, but suddenly it has\nreceived more attention than ever before. Thanks to innovations from companies\nlike Google, Microsoft, Meta, and other major brands in technology. OpenAI,\nthough, has triggered the button with its ground-breaking invention ChatGPT.\nChatGPT is a Large Language Model (LLM) based on Transformer architecture that\nhas the ability to generate human-like responses in a conversational context.\nIt uses deep learning algorithms to generate natural language responses to\ninput text. Its large number of parameters, contextual generation, and\nopen-domain training make it a versatile and effective tool for a wide range of\napplications, from chatbots to customer service to language translation. It has\nthe potential to revolutionize various industries and transform the way we\ninteract with technology. However, the use of ChatGPT has also raised several\nconcerns, including ethical, social, and employment challenges, which must be\ncarefully considered to ensure the responsible use of this technology. The\narticle provides an overview of ChatGPT, delving into its architecture and\ntraining process. It highlights the potential impacts of ChatGPT on the\nsociety. In this paper, we suggest some approaches involving technology,\nregulation, education, and ethics in an effort to maximize ChatGPT's benefits\nwhile minimizing its negative impacts. This study is expected to contribute to\na greater understanding of ChatGPT and aid in predicting the potential changes\nit may bring about.",
        "pdf_link": "https://arxiv.org/pdf/2403.14643v2.pdf"
    },
    {
        "title": "Large Language Models Offer an Alternative to the Traditional Approach of Topic Modelling",
        "authors": [
            "Yida Mu",
            "Chun Dong",
            "Kalina Bontcheva",
            "Xingyi Song"
        ],
        "published": "2024-03-24T17:39:51Z",
        "summary": "Topic modelling, as a well-established unsupervised technique, has found\nextensive use in automatically detecting significant topics within a corpus of\ndocuments. However, classic topic modelling approaches (e.g., LDA) have certain\ndrawbacks, such as the lack of semantic understanding and the presence of\noverlapping topics. In this work, we investigate the untapped potential of\nlarge language models (LLMs) as an alternative for uncovering the underlying\ntopics within extensive text corpora. To this end, we introduce a framework\nthat prompts LLMs to generate topics from a given set of documents and\nestablish evaluation protocols to assess the clustering efficacy of LLMs. Our\nfindings indicate that LLMs with appropriate prompts can stand out as a viable\nalternative, capable of generating relevant topic titles and adhering to human\nguidelines to refine and merge topics. Through in-depth experiments and\nevaluation, we summarise the advantages and constraints of employing LLMs in\ntopic extraction.",
        "pdf_link": "https://arxiv.org/pdf/2403.16248v2.pdf"
    },
    {
        "title": "AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents",
        "authors": [
            "Yao Fu",
            "Dong-Ki Kim",
            "Jaekyeom Kim",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Kyunghoon Bae",
            "Honglak Lee"
        ],
        "published": "2024-03-13T22:06:03Z",
        "summary": "The primary limitation of large language models (LLMs) is their restricted\nunderstanding of the world. This poses significant difficulties for LLM-based\nagents, particularly in domains where pre-trained LLMs lack sufficient\nknowledge. In this paper, we introduce a novel framework, called AutoGuide,\nthat bridges the knowledge gap in pre-trained LLMs by leveraging implicit\nknowledge in offline experiences. Specifically, AutoGuide effectively extracts\nknowledge embedded in offline data by extracting a set of state-aware\nguidelines. Importantly, each state-aware guideline is expressed in concise\nnatural language and follows a conditional structure, clearly describing the\nstate where it is applicable. As such, the resulting guidelines enable a\nprincipled way to provide helpful knowledge pertinent to an agent's current\ndecision-making process. We show that our approach outperforms competitive\nLLM-based baselines by a large margin in sequential decision-making benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.08978v1.pdf"
    },
    {
        "title": "Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models",
        "authors": [
            "Arijit Ghosh Chowdhury",
            "Md Mofijul Islam",
            "Vaibhav Kumar",
            "Faysal Hossain Shezan",
            "Vaibhav Kumar",
            "Vinija Jain",
            "Aman Chadha"
        ],
        "published": "2024-03-03T04:46:21Z",
        "summary": "Large Language Models (LLMs) have become a cornerstone in the field of\nNatural Language Processing (NLP), offering transformative capabilities in\nunderstanding and generating human-like text. However, with their rising\nprominence, the security and vulnerability aspects of these models have\ngarnered significant attention. This paper presents a comprehensive survey of\nthe various forms of attacks targeting LLMs, discussing the nature and\nmechanisms of these attacks, their potential impacts, and current defense\nstrategies. We delve into topics such as adversarial attacks that aim to\nmanipulate model outputs, data poisoning that affects model training, and\nprivacy concerns related to training data exploitation. The paper also explores\nthe effectiveness of different attack methodologies, the resilience of LLMs\nagainst these attacks, and the implications for model integrity and user trust.\nBy examining the latest research, we provide insights into the current\nlandscape of LLM vulnerabilities and defense mechanisms. Our objective is to\noffer a nuanced understanding of LLM attacks, foster awareness within the AI\ncommunity, and inspire robust solutions to mitigate these risks in future\ndevelopments.",
        "pdf_link": "https://arxiv.org/pdf/2403.04786v2.pdf"
    },
    {
        "title": "Computational Sentence-level Metrics Predicting Human Sentence Comprehension",
        "authors": [
            "Kun Sun",
            "Rong Wang"
        ],
        "published": "2024-03-23T12:19:49Z",
        "summary": "The majority of research in computational psycholinguistics has concentrated\non the processing of words. This study introduces innovative methods for\ncomputing sentence-level metrics using multilingual large language models. The\nmetrics developed sentence surprisal and sentence relevance and then are tested\nand compared to validate whether they can predict how humans comprehend\nsentences as a whole across languages. These metrics offer significant\ninterpretability and achieve high accuracy in predicting human sentence reading\nspeeds. Our results indicate that these computational sentence-level metrics\nare exceptionally effective at predicting and elucidating the processing\ndifficulties encountered by readers in comprehending sentences as a whole\nacross a variety of languages. Their impressive performance and generalization\ncapabilities provide a promising avenue for future research in integrating LLMs\nand cognitive science.",
        "pdf_link": "https://arxiv.org/pdf/2403.15822v1.pdf"
    },
    {
        "title": "Data-freeWeight Compress and Denoise for Large Language Models",
        "authors": [
            "Runyu Peng",
            "Yunhua Zhou",
            "Qipeng Guo",
            "Yang Gao",
            "Hang Yan",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "published": "2024-02-26T05:51:47Z",
        "summary": "Large Language Models (LLMs) are reshaping the research landscape in\nartificial intelligence, particularly as model parameters scale up\nsignificantly, unlocking remarkable capabilities across various domains.\nNevertheless, the scalability of model parameters faces constraints due to\nlimitations in GPU memory and computational speed. To address these\nconstraints, various weight compression methods have emerged, such as Pruning\nand Quantization. Given the low-rank nature of weight matrices in language\nmodels, the reduction of weights through matrix decomposition undoubtedly holds\nsignificant potential and promise. In this paper, drawing upon the intrinsic\nstructure of LLMs, we propose a novel approach termed Data-free Joint Rank-k\nApproximation for compressing the parameter matrices. Significantly, our method\nis characterized by without necessitating additional involvement of any corpus,\nwhile simultaneously preserving orthogonality in conjunction with pruning and\nquantization methods. We achieve a model pruning of 80% parameters while\nretaining 93.43% of the original performance without any calibration data.\nAdditionally, we explore the fundamental properties of the weight matrix of\nLLMs undergone Rank-k Approximation and conduct comprehensive experiments to\nelucidate our hypothesis.",
        "pdf_link": "https://arxiv.org/pdf/2402.16319v1.pdf"
    },
    {
        "title": "SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking",
        "authors": [
            "Atharva Kulkarni",
            "Bo-Hsiang Tseng",
            "Joel Ruben Antony Moniz",
            "Dhivya Piraviperumal",
            "Hong Yu",
            "Shruti Bhargava"
        ],
        "published": "2024-02-03T22:49:00Z",
        "summary": "In-context learning with Large Language Models (LLMs) has emerged as a\npromising avenue of research in Dialog State Tracking (DST). However, the\nbest-performing in-context learning methods involve retrieving and adding\nsimilar examples to the prompt, requiring access to labeled training data.\nProcuring such training data for a wide range of domains and applications is\ntime-consuming, expensive, and, at times, infeasible. While zero-shot learning\nrequires no training data, it significantly lags behind the few-shot setup.\nThus, `\\textit{Can we efficiently generate synthetic data for any dialogue\nschema to enable few-shot prompting?}' Addressing this question, we propose\n\\method, a data generation framework tailored for DST, utilizing LLMs. Our\napproach only requires the dialogue schema and a few hand-crafted dialogue\ntemplates to synthesize natural, coherent, and free-flowing dialogues with DST\nannotations. Few-shot learning using data from {\\method} results in $4-5%$\nimprovement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1\nand 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of\nthe performance compared to the few-shot setup using human-annotated training\ndata. Our synthetic data and code can be accessed at\nhttps://github.com/apple/ml-synthdst",
        "pdf_link": "https://arxiv.org/pdf/2402.02285v1.pdf"
    },
    {
        "title": "Differentially Private Next-Token Prediction of Large Language Models",
        "authors": [
            "James Flemings",
            "Meisam Razaviyayn",
            "Murali Annavaram"
        ],
        "published": "2024-03-22T22:27:44Z",
        "summary": "Ensuring the privacy of Large Language Models (LLMs) is becoming increasingly\nimportant. The most widely adopted technique to accomplish this is DP-SGD,\nwhich trains a model to guarantee Differential Privacy (DP). However, DP-SGD\noverestimates an adversary's capabilities in having white box access to the\nmodel and, as a result, causes longer training times and larger memory usage\nthan SGD. On the other hand, commercial LLM deployments are predominantly\ncloud-based; hence, adversarial access to LLMs is black-box. Motivated by these\nobservations, we present Private Mixing of Ensemble Distributions (PMixED): a\nprivate prediction protocol for next-token prediction that utilizes the\ninherent stochasticity of next-token sampling and a public model to achieve\nDifferential Privacy. We formalize this by introducing RD-mollifers which\nproject each of the model's output distribution from an ensemble of fine-tuned\nLLMs onto a set around a public LLM's output distribution, then average the\nprojected distributions and sample from it. Unlike DP-SGD which needs to\nconsider the model architecture during training, PMixED is model agnostic,\nwhich makes PMixED a very appealing solution for current deployments. Our\nresults show that PMixED achieves a stronger privacy guarantee than\nsample-level privacy and outperforms DP-SGD for privacy $\\epsilon = 8$ on\nlarge-scale datasets. Thus, PMixED offers a practical alternative to DP\ntraining methods for achieving strong generative utility without compromising\nprivacy.",
        "pdf_link": "https://arxiv.org/pdf/2403.15638v2.pdf"
    },
    {
        "title": "Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model",
        "authors": [
            "Liyan Xu",
            "Zhenlin Su",
            "Mo Yu",
            "Jin Xu",
            "Jinho D. Choi",
            "Jie Zhou",
            "Fei Liu"
        ],
        "published": "2024-02-20T08:41:23Z",
        "summary": "Factual inconsistency poses a significant hurdle for the commercial\ndeployment of abstractive summarizers. Under this Large Language Model (LLM)\nera, this work focuses around two important questions: what is the best way to\nleverage LLM for factual inconsistency detection, and how could we distill a\nsmaller LLM with both high efficiency and efficacy? Three zero-shot paradigms\nare firstly proposed and evaluated across five diverse datasets: direct\ninference on the entire summary or each summary window; entity verification\nthrough question generation and answering. Experiments suggest that LLM itself\nis capable to resolve this task train-free under the proper paradigm design,\nsurpassing strong trained baselines by 2.8% on average. To further promote\npractical utility, we then propose training strategies aimed at distilling\nsmaller open-source LLM that learns to score the entire summary at once with\nhigh accuracy, which outperforms the zero-shot approaches by much larger LLM,\nserving as an effective and efficient ready-to-use scorer.",
        "pdf_link": "https://arxiv.org/pdf/2402.12821v1.pdf"
    },
    {
        "title": "Personalized Language Modeling from Personalized Human Feedback",
        "authors": [
            "Xinyu Li",
            "Zachary C. Lipton",
            "Liu Leqi"
        ],
        "published": "2024-02-06T04:18:58Z",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is the current dominating\nframework to fine-tune large language models to better align with human\npreferences. However, the underlying premise of algorithms developed under this\nframework can be problematic when user preferences encoded in human feedback\nare diverse. In this work, we aim to address this problem by developing methods\nfor building personalized language models. We first formally introduce the task\nof learning from personalized human feedback and explain why vanilla RLHF can\nbe problematic in this context. We then propose a general Personalized-RLHF\n(P-RLHF) framework, which requires one to jointly learn a user model and a\nlanguage (or reward) model. The user model takes in user information and\noutputs user representations. Its structure encodes our assumptions about user\npreferences underlying the feedback data. We develop new learning objectives\nfor personalized reward modeling and personalized Direct Preference\nOptimization. To demonstrate the efficacy of our method, we test it on\nreal-world text summarization data with annotated preferences and annotator\ninformation. We fine-tune GPT-J 6B to obtain personalized language (and reward)\nmodels, which outperform non-personalized models in terms of aligning with\nindividual preferences.",
        "pdf_link": "https://arxiv.org/pdf/2402.05133v1.pdf"
    },
    {
        "title": "Grounding Data Science Code Generation with Input-Output Specifications",
        "authors": [
            "Yeming Wen",
            "Pengcheng Yin",
            "Kensen Shi",
            "Henryk Michalewski",
            "Swarat Chaudhuri",
            "Alex Polozov"
        ],
        "published": "2024-02-12T21:32:49Z",
        "summary": "Large language models (LLMs) have recently demonstrated a remarkable ability\nto generate code from natural language (NL) prompts. However, in the real\nworld, NL is often too ambiguous to capture the true intent behind programming\nproblems, requiring additional input-output (I/O) specifications.\nUnfortunately, LLMs can have difficulty aligning their outputs with both the NL\nprompt and the I/O specification. In this paper, we give a way to mitigate this\nissue in the context of data science programming, where tasks require explicit\nI/O specifications for clarity. Specifically, we propose GIFT4Code, a novel\napproach for the instruction fine-tuning of LLMs with respect to I/O\nspecifications. Our method leverages synthetic data produced by the LLM itself\nand utilizes execution-derived feedback as a key learning signal. This\nfeedback, in the form of program I/O specifications, is provided to the LLM to\nfacilitate instruction fine-tuning. We evaluated our approach on two\nchallenging data science benchmarks, Arcade and DS-1000. The results\ndemonstrate a significant improvement in the LLM's ability to generate code\nthat is not only executable but also accurately aligned with user\nspecifications, substantially improving the quality of code generation for\ncomplex data science tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.08073v2.pdf"
    },
    {
        "title": "Explaining Relationships Among Research Papers",
        "authors": [
            "Xiangci Li",
            "Jessica Ouyang"
        ],
        "published": "2024-02-20T23:38:39Z",
        "summary": "Due to the rapid pace of research publications, keeping up to date with all\nthe latest related papers is very time-consuming, even with daily feed tools.\nThere is a need for automatically generated, short, customized literature\nreviews of sets of papers to help researchers decide what to read. While\nseveral works in the last decade have addressed the task of explaining a single\nresearch paper, usually in the context of another paper citing it, the\nrelationship among multiple papers has been ignored; prior works have focused\non generating a single citation sentence in isolation, without addressing the\nexpository and transition sentences needed to connect multiple papers in a\ncoherent story. In this work, we explore a feature-based, LLM-prompting\napproach to generate richer citation texts, as well as generating multiple\ncitations at once to capture the complex relationships among research papers.\nWe perform an expert evaluation to investigate the impact of our proposed\nfeatures on the quality of the generated paragraphs and find a strong\ncorrelation between human preference and integrative writing style, suggesting\nthat humans prefer high-level, abstract citations, with transition sentences\nbetween them to provide an overall story.",
        "pdf_link": "https://arxiv.org/pdf/2402.13426v1.pdf"
    },
    {
        "title": "LUQ: Long-text Uncertainty Quantification for LLMs",
        "authors": [
            "Caiqi Zhang",
            "Fangyu Liu",
            "Marco Basaldella",
            "Nigel Collier"
        ],
        "published": "2024-03-29T16:49:24Z",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capability in a\nvariety of NLP tasks. Despite their effectiveness, these models are prone to\ngenerate nonfactual content. Uncertainty Quantification (UQ) is pivotal in\nenhancing our understanding of a model's confidence in its generated content,\nthereby aiding in the mitigation of nonfactual outputs. Existing research on UQ\npredominantly targets short text generation, typically yielding brief,\nword-limited responses. However, real-world applications frequently necessitate\nmuch longer responses. Our study first highlights the limitations of current UQ\nmethods in handling long text generation. We then introduce \\textsc{Luq}, a\nnovel sampling-based UQ approach specifically designed for long text. Our\nfindings reveal that \\textsc{Luq} outperforms existing baseline methods in\ncorrelating with the model's factuality scores (negative coefficient of -0.85\nobserved for Gemini Pro). With \\textsc{Luq} as the tool for UQ, we investigate\nbehavior patterns of several popular LLMs' response confidence spectrum and how\nthat interplays with the response' factuality. We identify that LLMs lack\nconfidence in generating long text for rare facts and a factually strong model\n(i.e. GPT-4) tends to reject questions it is not sure about. To further improve\nthe factual accuracy of LLM responses, we propose a method called\n\\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects\nthe response with the least uncertainty. The ensembling method greatly improves\nthe response factuality upon the best standalone LLM.",
        "pdf_link": "https://arxiv.org/pdf/2403.20279v1.pdf"
    },
    {
        "title": "Rendering Graphs for Graph Reasoning in Multimodal Large Language Models",
        "authors": [
            "Yanbin Wei",
            "Shuai Fu",
            "Weisen Jiang",
            "James T. Kwok",
            "Yu Zhang"
        ],
        "published": "2024-02-03T12:19:47Z",
        "summary": "Large Language Models (LLMs) are increasingly used for various tasks with\ngraph structures, such as robotic planning, knowledge graph completion, and\ncommon-sense reasoning. Though LLMs can comprehend graph information in a\ntextual format, they overlook the rich visual modality, which is an intuitive\nway for humans to comprehend structural information and conduct graph\nreasoning. The potential benefits and capabilities of representing graph\nstructures as visual images (i.e., visual graph) is still unexplored. In this\npaper, we take the first step in incorporating visual information into graph\nreasoning tasks and propose a new benchmark GITQA, where each sample is a tuple\n(graph, image, textual description). We conduct extensive experiments on the\nGITQA benchmark using state-of-the-art multimodal LLMs. Results on graph\nreasoning tasks show that combining textual and visual information together\nperforms better than using one modality alone. Moreover, the LLaVA-7B/13B\nmodels finetuned on the training set (referred to as GITA), achieve higher\naccuracy than the closed-source model GPT-4(V). We also study the effects of\naugmentations in graph reasoning.",
        "pdf_link": "https://arxiv.org/pdf/2402.02130v3.pdf"
    },
    {
        "title": "FaaF: Facts as a Function for the evaluation of generated text",
        "authors": [
            "Vasileios Katranidis",
            "Gabor Barany"
        ],
        "published": "2024-03-06T17:48:06Z",
        "summary": "The demand for accurate and efficient verification of information in texts\ngenerated by large language models (LMs) is at an all-time high, but remains\nunresolved. Recent efforts have focused on extracting and verifying atomic\nfacts from these texts via prompting LM evaluators. However, we demonstrate\nthat this method of prompting is unreliable when faced with incomplete or\ninaccurate reference information. We introduce Facts as a Function (FaaF), a\nnew approach to the fact verification task that leverages the function-calling\ncapabilities of LMs. FaaF significantly enhances the ability of LMs to identify\nunsupported facts in texts, while also improving efficiency and significantly\nlowering costs compared to prompt-based methods. Additionally, we propose a\nframework for evaluating factual recall in Retrieval Augmented Generation (RAG)\nsystems, which we employ to compare prompt-based and FaaF methods using various\nLMs under challenging conditions.",
        "pdf_link": "https://arxiv.org/pdf/2403.03888v2.pdf"
    },
    {
        "title": "Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models",
        "authors": [
            "Wenda Xu",
            "Guanglei Zhu",
            "Xuandong Zhao",
            "Liangming Pan",
            "Lei Li",
            "William Yang Wang"
        ],
        "published": "2024-02-18T03:10:39Z",
        "summary": "Recent studies show that self-feedback improves large language models (LLMs)\non certain tasks while worsens other tasks. We discovered that such a contrary\nis due to LLM's bias towards their own output. In this paper, we formally\ndefine LLM's self-bias -- the tendency to favor its own generation -- using two\nstatistics. We analyze six LLMs on translation, constrained text generation,\nand mathematical reasoning tasks. We find that self-bias is prevalent in all\nexamined LLMs across multiple languages and tasks. Our analysis reveals that\nwhile the self-refine pipeline improves the fluency and understandability of\nmodel outputs, it further amplifies self-bias. To mitigate such biases, we\ndiscover that larger model size and external feedback with accurate assessment\ncan significantly reduce bias in the self-refine pipeline, leading to actual\nperformance improvement in downstream tasks.",
        "pdf_link": "https://arxiv.org/pdf/2402.11436v1.pdf"
    },
    {
        "title": "Critical Data Size of Language Models from a Grokking Perspective",
        "authors": [
            "Xuekai Zhu",
            "Yao Fu",
            "Bowen Zhou",
            "Zhouhan Lin"
        ],
        "published": "2024-01-19T03:24:36Z",
        "summary": "We explore the critical data size in language models, a threshold that marks\na fundamental shift from quick memorization to slow generalization. We\nformalize the phase transition under the grokking configuration into the Data\nEfficiency Hypothesis and identify data insufficiency, sufficiency, and surplus\nregimes in language models training dynamics. We develop a grokking\nconfiguration to reproduce grokking on simplistic language models stably by\nrescaling initialization and weight decay. We show that generalization occurs\nonly when language models reach a critical size. We analyze grokking across\nsample-wise and model-wise, verifying the proposed data efficiency hypothesis.\nOur experiments reveal smoother phase transitions occurring at the critical\ndataset size for language datasets. As the model size increases, this critical\npoint also becomes larger, indicating that larger models require more data. Our\nresults deepen the understanding of language model training, offering a novel\nperspective on the role of data in the learning mechanism of language models.",
        "pdf_link": "https://arxiv.org/pdf/2401.10463v2.pdf"
    },
    {
        "title": "Auditing Counterfire: Evaluating Advanced Counterargument Generation with Evidence and Style",
        "authors": [
            "Preetika Verma",
            "Kokil Jaidka",
            "Svetlana Churina"
        ],
        "published": "2024-02-13T14:53:12Z",
        "summary": "We audited counter-arguments generated by large language models (LLMs),\nfocusing on their ability to generate evidence-based and stylistic\ncounter-arguments to posts from the Reddit ChangeMyView dataset. Our evaluation\nis based on Counterfire: a new dataset of 32,000 counter-arguments generated\nfrom large language models (LLMs): GPT-3.5 Turbo and Koala and their fine-tuned\nvariants, and PaLM 2, with varying prompts for evidence use and argumentative\nstyle. GPT-3.5 Turbo ranked highest in argument quality with strong\nparaphrasing and style adherence, particularly in `reciprocity' style\narguments. However, the `No Style' counter-arguments proved most persuasive on\naverage. The findings suggest that a balance between evidentiality and\nstylistic elements is vital to a compelling counter-argument. We close with a\ndiscussion of future research directions and implications for fine-tuning LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.08498v3.pdf"
    },
    {
        "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners",
        "authors": [
            "Kezhi Kong",
            "Jiani Zhang",
            "Zhengyuan Shen",
            "Balasubramaniam Srinivasan",
            "Chuan Lei",
            "Christos Faloutsos",
            "Huzefa Rangwala",
            "George Karypis"
        ],
        "published": "2024-02-22T08:01:01Z",
        "summary": "Large Language Models (LLMs) trained on large volumes of data excel at\nvarious natural language tasks, but they cannot handle tasks requiring\nknowledge that has not been trained on previously. One solution is to use a\nretriever that fetches relevant information to expand LLM's knowledge scope.\nHowever, existing textual-oriented retrieval-based LLMs are not ideal on\nstructured table data due to diversified data modalities and large table sizes.\nIn this work, we propose OpenTab, an open-domain table reasoning framework\npowered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant\ntables and then generates SQL programs to parse the retrieved tables\nefficiently. Utilizing the intermediate data derived from the SQL executions,\nit conducts grounded inference to produce accurate response. Extensive\nexperimental evaluation shows that OpenTab significantly outperforms baselines\nin both open- and closed-domain settings, achieving up to 21.5% higher\naccuracy. We further run ablation studies to validate the efficacy of our\nproposed designs of the system.",
        "pdf_link": "https://arxiv.org/pdf/2402.14361v1.pdf"
    },
    {
        "title": "Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows",
        "authors": [
            "Shujian Zhang",
            "Lemeng Wu",
            "Chengyue Gong",
            "Xingchao Liu"
        ],
        "published": "2024-03-25T17:58:22Z",
        "summary": "Recent works have demonstrated success in controlling sentence attributes\n($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the\ndiffusion language model. A key component that drives theimpressive performance\nfor generating high-quality samples from noise is iteratively denoise for\nthousands of steps. While beneficial, the complexity of starting from the noise\nand the learning steps has limited its implementation to many NLP real-world\napplications. This paper proposes Language Rectified Flow ({\\ours}). Our method\nis based on the reformulation of the standard probabilistic flow models.\nLanguage rectified flow learns (neural) ordinary differential equation models\nto transport between the source distribution and the target distribution, hence\nproviding a unified and effective solution to generative modeling and domain\ntransfer. From the source distribution, our language rectified flow yields fast\nsimulation and effectively decreases the inference time. Experiments on three\nchallenging fine-grained control tasks and multiple high-quality text editing\nshow that our method consistently outperforms its baselines. Extensive\nexperiments and ablation studies demonstrate that our method can be general,\neffective, and beneficial for many NLP tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.16995v1.pdf"
    },
    {
        "title": "LLM3:Large Language Model-based Task and Motion Planning with Motion Failure Reasoning",
        "authors": [
            "Shu Wang",
            "Muzhi Han",
            "Ziyuan Jiao",
            "Zeyu Zhang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Hangxin Liu"
        ],
        "published": "2024-03-18T08:03:47Z",
        "summary": "Conventional Task and Motion Planning (TAMP) approaches rely on manually\ncrafted interfaces connecting symbolic task planning with continuous motion\ngeneration. These domain-specific and labor-intensive modules are limited in\naddressing emerging tasks in real-world settings. Here, we present LLM^3, a\nnovel Large Language Model (LLM)-based TAMP framework featuring a\ndomain-independent interface. Specifically, we leverage the powerful reasoning\nand planning capabilities of pre-trained LLMs to propose symbolic action\nsequences and select continuous action parameters for motion planning.\nCrucially, LLM^3 incorporates motion planning feedback through prompting,\nallowing the LLM to iteratively refine its proposals by reasoning about motion\nfailure. Consequently, LLM^3 interfaces between task planning and motion\nplanning, alleviating the intricate design process of handling domain-specific\nmessages between them. Through a series of simulations in a box-packing domain,\nwe quantitatively demonstrate the effectiveness of LLM^3 in solving TAMP\nproblems and the efficiency in selecting action parameters. Ablation studies\nunderscore the significant contribution of motion failure reasoning to the\nsuccess of LLM^3. Furthermore, we conduct qualitative experiments on a physical\nmanipulator, demonstrating the practical applicability of our approach in\nreal-world settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11552v2.pdf"
    },
    {
        "title": "Balanced Data Sampling for Language Model Training with Clustering",
        "authors": [
            "Yunfan Shao",
            "Linyang Li",
            "Zhaoye Fei",
            "Hang Yan",
            "Dahua Lin",
            "Xipeng Qiu"
        ],
        "published": "2024-02-22T13:20:53Z",
        "summary": "Data plays a fundamental role in the training of Large Language Models\n(LLMs). While attention has been paid to the collection and composition of\ndatasets, determining the data sampling strategy in training remains an open\nquestion. Most LLMs are trained with a simple strategy, random sampling.\nHowever, this sampling strategy ignores the unbalanced nature of training data\ndistribution, which can be sub-optimal. In this paper, we propose ClusterClip\nSampling to balance the text distribution of training data for better model\ntraining. Specifically, ClusterClip Sampling utilizes data clustering to\nreflect the data distribution of the training set and balances the common\nsamples and rare samples during training based on the cluster results. A\nrepetition clip operation is introduced to mitigate the overfitting issue led\nby samples from certain clusters. Extensive experiments validate the\neffectiveness of ClusterClip Sampling, which outperforms random sampling and\nother cluster-based sampling variants under various training datasets and large\nlanguage models.",
        "pdf_link": "https://arxiv.org/pdf/2402.14526v1.pdf"
    },
    {
        "title": "Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?",
        "authors": [
            "Branislav Pecher",
            "Ivan Srba",
            "Maria Bielikova"
        ],
        "published": "2024-02-20T08:38:24Z",
        "summary": "When solving a task with limited labelled data, researchers can either use a\ngeneral large language model without further update, or use the few examples to\ntune a specialised smaller model. When enough labels are available, the\nspecialised models outperform the general ones on many NLP tasks. In this work,\nwe aim to investigate how many labelled samples are required for the\nspecialised models to achieve this superior performance, while taking the\nresults variance into consideration. Observing the behaviour of prompting,\nin-context learning, fine-tuning and instruction-tuning, identifying their\nbreak-even points when increasing number of labelled training samples across\nthree tasks of varying complexity, we find that the specialised models often\nneed only few samples ($100-1000$) to be on par or better than the general\nones. At the same time, the amount of required labelled data strongly depends\non the task complexity and results variance.",
        "pdf_link": "https://arxiv.org/pdf/2402.12819v1.pdf"
    },
    {
        "title": "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs",
        "authors": [
            "Fengqing Jiang",
            "Zhangchen Xu",
            "Luyao Niu",
            "Zhen Xiang",
            "Bhaskar Ramasubramanian",
            "Bo Li",
            "Radha Poovendran"
        ],
        "published": "2024-02-19T00:43:31Z",
        "summary": "Safety is critical to the usage of large language models (LLMs). Multiple\ntechniques such as data filtering and supervised fine-tuning have been\ndeveloped to strengthen LLM safety. However, currently known techniques presume\nthat corpora used for safety alignment of LLMs are solely interpreted by\nsemantics. This assumption, however, does not hold in real-world applications,\nwhich leads to severe vulnerabilities in LLMs. For example, users of forums\noften use ASCII art, a form of text-based art, to convey image information. In\nthis paper, we propose a novel ASCII art-based jailbreak attack and introduce a\ncomprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the\ncapabilities of LLMs in recognizing prompts that cannot be solely interpreted\nby semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and\nLlama2) struggle to recognize prompts provided in the form of ASCII art. Based\non this observation, we develop the jailbreak attack ArtPrompt, which leverages\nthe poor performance of LLMs in recognizing ASCII art to bypass safety measures\nand elicit undesired behaviors from LLMs. ArtPrompt only requires black-box\naccess to the victim LLMs, making it a practical attack. We evaluate ArtPrompt\non five SOTA LLMs, and show that ArtPrompt can effectively and efficiently\ninduce undesired behaviors from all five LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11753v2.pdf"
    },
    {
        "title": "Qsnail: A Questionnaire Dataset for Sequential Question Generation",
        "authors": [
            "Yan Lei",
            "Liang Pang",
            "Yuanzhuo Wang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "published": "2024-02-22T04:14:10Z",
        "summary": "The questionnaire is a professional research methodology used for both\nqualitative and quantitative analysis of human opinions, preferences,\nattitudes, and behaviors. However, designing and evaluating questionnaires\ndemands significant effort due to their intricate and complex structure.\nQuestionnaires entail a series of questions that must conform to intricate\nconstraints involving the questions, options, and overall structure.\nSpecifically, the questions should be relevant and specific to the given\nresearch topic and intent. The options should be tailored to the questions,\nensuring they are mutually exclusive, completed, and ordered sensibly.\nMoreover, the sequence of questions should follow a logical order, grouping\nsimilar topics together. As a result, automatically generating questionnaires\npresents a significant challenge and this area has received limited attention\nprimarily due to the scarcity of high-quality datasets. To address these\nissues, we present Qsnail, the first dataset specifically constructed for the\nquestionnaire generation task, which comprises 13,168 human-written\nquestionnaires gathered from online platforms. We further conduct experiments\non Qsnail, and the results reveal that retrieval models and traditional\ngenerative models do not fully align with the given research topic and intents.\nLarge language models, while more closely related to the research topic and\nintents, exhibit significant limitations in terms of diversity and specificity.\nDespite enhancements through the chain-of-thought prompt and finetuning,\nquestionnaires generated by language models still fall short of human-written\nquestionnaires. Therefore, questionnaire generation is challenging and needs to\nbe further explored. The dataset is available at:\nhttps://github.com/LeiyanGithub/qsnail.",
        "pdf_link": "https://arxiv.org/pdf/2402.14272v1.pdf"
    },
    {
        "title": "AS-ES Learning: Towards Efficient CoT Learning in Small Models",
        "authors": [
            "Nuwa Xi",
            "Yuhan Chen",
            "Sendong Zhao",
            "Haochun Wang",
            "Bing Qin",
            "Ting Liu"
        ],
        "published": "2024-03-04T12:13:59Z",
        "summary": "Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs,\nespecially when it comes to logical reasoning. Attempts have been made to\ninduce such ability in small models as well by distilling from the data with\nCoT generated by Large Language Models (LLMs). However, existing methods often\nsimply generate and incorporate more data from LLMs and fail to note the\nimportance of efficiently utilizing existing CoT data. We here propose a new\ntraining paradigm AS-ES (Abstractive Segments - Extractive Segments) learning,\nwhich exploits the inherent information in CoT for iterative generation.\nExperiments show that our methods surpass the direct seq2seq training on\nCoT-extensive tasks like MWP and PET summarization, without data augmentation\nor altering the model itself. Furthermore, we explore the reason behind the\ninefficiency of small models in learning CoT and provide an explanation of why\nAS-ES learning works, giving insights into the underlying mechanism of CoT.",
        "pdf_link": "https://arxiv.org/pdf/2403.01969v1.pdf"
    },
    {
        "title": "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs",
        "authors": [
            "Xuhui Zhou",
            "Zhe Su",
            "Tiwalayo Eisape",
            "Hyunwoo Kim",
            "Maarten Sap"
        ],
        "published": "2024-03-08T03:49:17Z",
        "summary": "Recent advances in large language models (LLM) have enabled richer social\nsimulations, allowing for the study of various social phenomena with LLM-based\nagents. However, most work has used an omniscient perspective on these\nsimulations (e.g., single LLM to generate all interlocutors), which is\nfundamentally at odds with the non-omniscient, information asymmetric\ninteractions that humans have. To examine these differences, we develop an\nevaluation framework to simulate social interactions with LLMs in various\nsettings (omniscient, non-omniscient). Our experiments show that interlocutors\nsimulated omnisciently are much more successful at accomplishing social goals\ncompared to non-omniscient agents, despite the latter being the more realistic\nsetting. Furthermore, we demonstrate that learning from omniscient simulations\nimproves the apparent naturalness of interactions but scarcely enhances goal\nachievement in cooperative scenarios. Our findings indicate that addressing\ninformation asymmetry remains a fundamental challenge for LLM-based agents.",
        "pdf_link": "https://arxiv.org/pdf/2403.05020v2.pdf"
    },
    {
        "title": "Veagle: Advancements in Multimodal Representation Learning",
        "authors": [
            "Rajat Chawla",
            "Arkajit Datta",
            "Tushar Verma",
            "Adarsh Jha",
            "Anmol Gautam",
            "Ayush Vatsal",
            "Sukrit Chaterjee",
            "Mukunda NS",
            "Ishaan Bhola"
        ],
        "published": "2024-01-18T12:45:25Z",
        "summary": "Lately, researchers in artificial intelligence have been really interested in\nhow language and vision come together, giving rise to the development of\nmultimodal models that aim to seamlessly integrate textual and visual\ninformation. Multimodal models, an extension of Large Language Models (LLMs),\nhave exhibited remarkable capabilities in addressing a diverse array of tasks,\nranging from image captioning and visual question answering (VQA) to visual\ngrounding. While these models have showcased significant advancements,\nchallenges persist in accurately interpreting images and answering the\nquestion, a common occurrence in real-world scenarios. This paper introduces a\nnovel approach to enhance the multimodal capabilities of existing models. In\nresponse to the limitations observed in current Vision Language Models (VLMs)\nand Multimodal Large Language Models (MLLMs), our proposed model Veagle,\nincorporates a unique mechanism inspired by the successes and insights of\nprevious works. Veagle leverages a dynamic mechanism to project encoded visual\ninformation directly into the language model. This dynamic approach allows for\na more nuanced understanding of intricate details present in visual contexts.\nTo validate the effectiveness of Veagle, we conduct comprehensive experiments\non benchmark datasets, emphasizing tasks such as visual question answering and\nimage understanding. Our results indicate a improvement of 5-6 \\% in\nperformance, with Veagle outperforming existing models by a notable margin. The\noutcomes underscore the model's versatility and applicability beyond\ntraditional benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2403.08773v1.pdf"
    },
    {
        "title": "AI and Memory Wall",
        "authors": [
            "Amir Gholami",
            "Zhewei Yao",
            "Sehoon Kim",
            "Coleman Hooper",
            "Michael W. Mahoney",
            "Kurt Keutzer"
        ],
        "published": "2024-03-21T04:31:59Z",
        "summary": "The availability of unprecedented unsupervised training data, along with\nneural scaling laws, has resulted in an unprecedented surge in model size and\ncompute requirements for serving/training LLMs. However, the main performance\nbottleneck is increasingly shifting to memory bandwidth. Over the past 20\nyears, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the\ngrowth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and\n1.4 times every 2 years, respectively. This disparity has made memory, rather\nthan compute, the primary bottleneck in AI applications, particularly in\nserving. Here, we analyze encoder and decoder Transformer models and show how\nmemory bandwidth can become the dominant bottleneck for decoder models. We\nargue for a redesign in model architecture, training, and deployment strategies\nto overcome this memory limitation.",
        "pdf_link": "https://arxiv.org/pdf/2403.14123v1.pdf"
    },
    {
        "title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems",
        "authors": [
            "Chaoqun He",
            "Renjie Luo",
            "Yuzhuo Bai",
            "Shengding Hu",
            "Zhen Leng Thai",
            "Junhao Shen",
            "Jinyi Hu",
            "Xu Han",
            "Yujie Huang",
            "Yuxiang Zhang",
            "Jie Liu",
            "Lei Qi",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published": "2024-02-21T18:49:26Z",
        "summary": "Recent advancements have seen Large Language Models (LLMs) and Large\nMultimodal Models (LMMs) surpassing general human capabilities in various\ntasks, approaching the proficiency level of human experts across multiple\ndomains. With traditional benchmarks becoming less challenging for these\nmodels, new rigorous challenges are essential to gauge their advanced\nabilities. In this work, we present OlympiadBench, an Olympiad-level bilingual\nmultimodal scientific benchmark, featuring 8,952 problems from Olympiad-level\nmathematics and physics competitions, including the Chinese college entrance\nexam. Each problem is detailed with expert-level annotations for step-by-step\nreasoning. Evaluating top-tier models on OlympiadBench, we implement a\ncomprehensive assessment methodology to accurately evaluate model responses.\nNotably, the best-performing model, GPT-4V, attains an average score of 17.23%\non OlympiadBench, with a mere 11.28% in physics, highlighting the benchmark\nrigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V\npoints out prevalent issues with hallucinations, knowledge omissions, and\nlogical fallacies. We hope that our challenging benchmark can serve as a\nvaluable resource for helping future AGI research endeavors.",
        "pdf_link": "https://arxiv.org/pdf/2402.14008v1.pdf"
    },
    {
        "title": "Enhanced Automated Code Vulnerability Repair using Large Language Models",
        "authors": [
            "David de-Fitero-Dominguez",
            "Eva Garcia-Lopez",
            "Antonio Garcia-Cabot",
            "Jose-Javier Martinez-Herraiz"
        ],
        "published": "2024-01-08T09:01:29Z",
        "summary": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.",
        "pdf_link": "https://arxiv.org/pdf/2401.03741v1.pdf"
    },
    {
        "title": "Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting Generative AI-based Visualizations",
        "authors": [
            "Luca Podo",
            "Muhammad Ishmal",
            "Marco Angelini"
        ],
        "published": "2024-02-03T14:28:55Z",
        "summary": "The automatic generation of visualizations is an old task that, through the\nyears, has shown more and more interest from the research and practitioner\ncommunities. Recently, large language models (LLM) have become an interesting\noption for supporting generative tasks related to visualization, demonstrating\ninitial promising results. At the same time, several pitfalls, like the\nmultiple ways of instructing an LLM to generate the desired result, the\ndifferent perspectives leading the generation (code-based, image-based,\ngrammar-based), and the presence of hallucinations even for the visualization\ngeneration task, make their usage less affordable than expected. Following\nsimilar initiatives for benchmarking LLMs, this paper copes with the problem of\nmodeling the evaluation of a generated visualization through an LLM. We propose\na theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort\nin its atomic components, characterizes their nature, and provides an overview\nof how to implement and interpret them. We also designed and implemented an\nevaluation platform that provides a benchmarking resource for the visualization\ngeneration task. The platform supports automatic and manual scoring conducted\nby multiple assessors to support a fine-grained and semantic evaluation based\non the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and\nLlama2-70-b models show the benefits of EvaLLM and illustrate interesting\nresults on the current state-of-the-art LLM-generated visualizations.",
        "pdf_link": "https://arxiv.org/pdf/2402.02167v1.pdf"
    },
    {
        "title": "Localized Zeroth-Order Prompt Optimization",
        "authors": [
            "Wenyang Hu",
            "Yao Shu",
            "Zongmin Yu",
            "Zhaoxuan Wu",
            "Xiangqiang Lin",
            "Zhongxiang Dai",
            "See-Kiong Ng",
            "Bryan Kian Hsiang Low"
        ],
        "published": "2024-03-05T14:18:15Z",
        "summary": "The efficacy of large language models (LLMs) in understanding and generating\nnatural language has aroused a wide interest in developing prompt-based methods\nto harness the power of black-box LLMs. Existing methodologies usually\nprioritize a global optimization for finding the global optimum, which however\nwill perform poorly in certain tasks. This thus motivates us to re-think the\nnecessity of finding a global optimum in prompt optimization. To answer this,\nwe conduct a thorough empirical study on prompt optimization and draw two major\ninsights. Contrasting with the rarity of global optimum, local optima are\nusually prevalent and well-performed, which can be more worthwhile for\nefficient prompt optimization (Insight I). The choice of the input domain,\ncovering both the generation and the representation of prompts, affects the\nidentification of well-performing local optima (Insight II). Inspired by these\ninsights, we propose a novel algorithm, namely localized zeroth-order prompt\noptimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived\nGaussian process into standard zeroth-order optimization for an efficient\nsearch of well-performing local optima in prompt optimization. Remarkably, ZOPO\noutperforms existing baselines in terms of both the optimization performance\nand the query efficiency, which we demonstrate through extensive experiments.",
        "pdf_link": "https://arxiv.org/pdf/2403.02993v1.pdf"
    },
    {
        "title": "Unraveling Babel: Exploring Multilingual Activation Patterns within Large Language Models",
        "authors": [
            "Weize Liu",
            "Yinlong Xu",
            "Hongxia Xu",
            "Jintai Chen",
            "Xuming Hu",
            "Jian Wu"
        ],
        "published": "2024-02-26T07:44:56Z",
        "summary": "Recently, large language models (LLMs) have achieved tremendous breakthroughs\nin the field of language processing, yet their mechanisms in processing\nmultiple languages remain agnostic. Therefore, in this work we study the\nmultilingual activation patterns of LLMs. By transforming the original Large\nLanguage Models (LLMs) into a Mixture of Experts (MoE) architecture, we analyze\nthe expert activation patterns when processing various languages and\ndemonstrate the connections of these activation patterns at the level of\nlanguage families. We discover the existence of non-language-specific neurons\nas well as language-specific activation neurons. Further exploration even\nshowcases that merely leveraging high-frequency activation neurons can\naccelerate inference while maintaining comparable performance. These findings\nshed light on the LLMs' multilingual processing mechanism, and are of\nsignificant importance in guiding the multilingual training and model pruning\nof LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.16367v1.pdf"
    },
    {
        "title": "CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients",
        "authors": [
            "Pragnya Ramjee",
            "Bhuvan Sachdeva",
            "Satvik Golechha",
            "Shreyas Kulkarni",
            "Geeta Fulari",
            "Kaushik Murali",
            "Mohit Jain"
        ],
        "published": "2024-02-07T07:07:02Z",
        "summary": "The healthcare landscape is evolving, with patients seeking more reliable\ninformation about their health conditions, treatment options, and potential\nrisks. Despite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\ndoctors and hospital staff, highlighting the need for expert-endorsed health\ninformation. However, the pressure on experts has led to reduced communication\ntime, impacting information sharing. To address this gap, we propose\nCataractBot, an experts-in-the-loop chatbot powered by large language models\n(LLMs). Developed in collaboration with a tertiary eye hospital in India,\nCataractBot answers cataract surgery related questions instantly by querying a\ncurated knowledge base, and provides expert-verified responses asynchronously.\nCataractBot features multimodal support and multilingual capabilities. In an\nin-the-wild deployment study with 49 participants, CataractBot proved valuable,\nproviding anytime accessibility, saving time, and accommodating diverse\nliteracy levels. Trust was established through expert verification. Broadly,\nour results could inform future work on designing expert-mediated LLM bots.",
        "pdf_link": "https://arxiv.org/pdf/2402.04620v1.pdf"
    },
    {
        "title": "FashionReGen: LLM-Empowered Fashion Report Generation",
        "authors": [
            "Yujuan Ding",
            "Yunshan Ma",
            "Wenqi Fan",
            "Yige Yao",
            "Tat-Seng Chua",
            "Qing Li"
        ],
        "published": "2024-03-11T12:29:35Z",
        "summary": "Fashion analysis refers to the process of examining and evaluating trends,\nstyles, and elements within the fashion industry to understand and interpret\nits current state, generating fashion reports. It is traditionally performed by\nfashion professionals based on their expertise and experience, which requires\nhigh labour cost and may also produce biased results for relying heavily on a\nsmall group of people. In this paper, to tackle the Fashion Report Generation\n(FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting\nsystem based the advanced Large Language Models (LLMs), debbed as GPT-FAR.\nSpecifically, it tries to deliver FashionReGen based on effective catwalk\nanalysis, which is equipped with several key procedures, namely, catwalk\nunderstanding, collective organization and analysis, and report generation. By\nposing and exploring such an open-ended, complex and domain-specific task of\nFashionReGen, it is able to test the general capability of LLMs in fashion\ndomain. It also inspires the explorations of more high-level tasks with\nindustrial significance in other domains. Video illustration and more materials\nof GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.",
        "pdf_link": "https://arxiv.org/pdf/2403.06660v1.pdf"
    },
    {
        "title": "Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models",
        "authors": [
            "Yucheng Shi",
            "Qiaoyu Tan",
            "Xuansheng Wu",
            "Shaochen Zhong",
            "Kaixiong Zhou",
            "Ninghao Liu"
        ],
        "published": "2024-03-28T17:47:19Z",
        "summary": "Large Language Models (LLMs) have shown proficiency in question-answering\ntasks but often struggle to integrate real-time knowledge updates, leading to\npotentially outdated or inaccurate responses. This problem becomes even more\nchallenging when dealing with multi-hop questions since they require LLMs to\nupdate and integrate multiple knowledge pieces relevant to the questions. To\ntackle the problem, we propose the Retrieval-Augmented model Editing (RAE)\nframework tailored for multi-hop question answering. RAE first retrieves edited\nfacts and then refines the language model through in-context learning.\nSpecifically, our retrieval approach, based on mutual information maximization,\nleverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive\nsimilarity-based searches might miss. Additionally, our framework incorporates\na pruning strategy to eliminate redundant information from the retrieved facts,\nwhich enhances the editing accuracy and mitigates the hallucination problem.\nOur framework is supported by theoretical justification for its fact retrieval\nefficacy. Finally, comprehensive evaluation across various LLMs validates RAE's\nability in providing accurate answers with updated knowledge.",
        "pdf_link": "https://arxiv.org/pdf/2403.19631v1.pdf"
    },
    {
        "title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation",
        "authors": [
            "Jennifer Chien",
            "David Danks"
        ],
        "published": "2024-01-25T00:54:10Z",
        "summary": "Algorithmic harms are commonly categorized as either allocative or\nrepresentational. This study specifically addresses the latter, focusing on an\nexamination of current definitions of representational harms to discern what is\nincluded and what is not. This analysis motivates our expansion beyond\nbehavioral definitions to encompass harms to cognitive and affective states.\nThe paper outlines high-level requirements for measurement: identifying the\nnecessary expertise to implement this approach and illustrating it through a\ncase study. Our work highlights the unique vulnerabilities of large language\nmodels to perpetrating representational harms, particularly when these harms go\nunmeasured and unmitigated. The work concludes by presenting proposed\nmitigations and delineating when to employ them. The overarching aim of this\nresearch is to establish a framework for broadening the definition of\nrepresentational harms and to translate insights from fairness research into\npractical measurement and mitigation praxis.",
        "pdf_link": "https://arxiv.org/pdf/2402.01705v1.pdf"
    },
    {
        "title": "Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders",
        "authors": [
            "Yuwei Zhang",
            "Siffi Singh",
            "Sailik Sengupta",
            "Igor Shalyminov",
            "Hang Su",
            "Hwanjun Song",
            "Saab Mansour"
        ],
        "published": "2024-03-07T08:32:17Z",
        "summary": "Conversational systems often rely on embedding models for intent\nclassification and intent clustering tasks. The advent of Large Language Models\n(LLMs), which enable instructional embeddings allowing one to adjust semantics\nover the embedding space using prompts, are being viewed as a panacea for these\ndownstream conversational tasks. However, traditional evaluation benchmarks\nrely solely on task metrics that don't particularly measure gaps related to\nsemantic understanding. Thus, we propose an intent semantic toolkit that gives\na more holistic view of intent embedding models by considering three tasks --\n(1) intent classification, (2) intent clustering, and (3) a novel triplet task.\nThe triplet task gauges the model's understanding of two semantic concepts\nparamount in real-world conversational systems -- negation and implicature. We\nobserve that current embedding models fare poorly in semantic understanding of\nthese concepts. To address this, we propose a pre-training approach to improve\nthe embedding model by leveraging augmentation with data generated by an\nauto-regressive model and a contrastive loss term. Our approach improves the\nsemantic understanding of the intent embedding model on the aforementioned\nlinguistic dimensions while slightly effecting their performance on downstream\ntask metrics.",
        "pdf_link": "https://arxiv.org/pdf/2403.04314v1.pdf"
    },
    {
        "title": "Prompt Mining for Language-based Human Mobility Forecasting",
        "authors": [
            "Hao Xue",
            "Tianye Tang",
            "Ali Payani",
            "Flora D. Salim"
        ],
        "published": "2024-03-06T08:43:30Z",
        "summary": "With the advancement of large language models, language-based forecasting has\nrecently emerged as an innovative approach for predicting human mobility\npatterns. The core idea is to use prompts to transform the raw mobility data\ngiven as numerical values into natural language sentences so that the language\nmodels can be leveraged to generate the description for future observations.\nHowever, previous studies have only employed fixed and manually designed\ntemplates to transform numerical values into sentences. Since the forecasting\nperformance of language models heavily relies on prompts, using fixed templates\nfor prompting may limit the forecasting capability of language models. In this\npaper, we propose a novel framework for prompt mining in language-based\nmobility forecasting, aiming to explore diverse prompt design strategies.\nSpecifically, the framework includes a prompt generation stage based on the\ninformation entropy of prompts and a prompt refinement stage to integrate\nmechanisms such as the chain of thought. Experimental results on real-world\nlarge-scale data demonstrate the superiority of generated prompts from our\nprompt mining pipeline. Additionally, the comparison of different prompt\nvariants shows that the proposed prompt refinement process is effective. Our\nstudy presents a promising direction for further advancing language-based\nmobility forecasting.",
        "pdf_link": "https://arxiv.org/pdf/2403.03544v1.pdf"
    },
    {
        "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts",
        "authors": [
            "Yusu Qian",
            "Haotian Zhang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "published": "2024-02-20T18:31:27Z",
        "summary": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 850 test samples divided\ninto 6 categories, such as non-existent objects, count of objects, spatial\nrelationship, and visual confusion. We provide a comprehensive analysis of\npopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as\nLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps\nbetween GPT-4V and other models; and previous robust instruction-tuned models,\nsuch as LRV-Instruction and LLaVA-RLHF, are not effective on this new\nbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of\nany other model in our experiments ranges from 5% to 35%. We further propose a\nremedy that adds an additional paragraph to the deceptive prompts to encourage\nmodels to think twice before answering the question. Surprisingly, this simple\nmethod can even double the accuracy; however, the absolute numbers are still\ntoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark\nto stimulate further research to enhance models' resilience against deceptive\nprompts.",
        "pdf_link": "https://arxiv.org/pdf/2402.13220v1.pdf"
    },
    {
        "title": "Evaluation Ethics of LLMs in Legal Domain",
        "authors": [
            "Ruizhe Zhang",
            "Haitao Li",
            "Yueyue Wu",
            "Qingyao Ai",
            "Yiqun Liu",
            "Min Zhang",
            "Shaoping Ma"
        ],
        "published": "2024-03-17T09:05:13Z",
        "summary": "In recent years, the utilization of large language models for natural\nlanguage dialogue has gained momentum, leading to their widespread adoption\nacross various domains. However, their universal competence in addressing\nchallenges specific to specialized fields such as law remains a subject of\nscrutiny. The incorporation of legal ethics into the model has been overlooked\nby researchers. We asserts that rigorous ethic evaluation is essential to\nensure the effective integration of large language models in legal domains,\nemphasizing the need to assess domain-specific proficiency and domain-specific\nethic. To address this, we propose a novelty evaluation methodology, utilizing\nauthentic legal cases to evaluate the fundamental language abilities,\nspecialized legal knowledge and legal robustness of large language models\n(LLMs). The findings from our comprehensive evaluation contribute significantly\nto the academic discourse surrounding the suitability and performance of large\nlanguage models in legal domains.",
        "pdf_link": "https://arxiv.org/pdf/2403.11152v1.pdf"
    },
    {
        "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models",
        "authors": [
            "Xiaolong Wang",
            "Yile Wang",
            "Yuanchi Zhang",
            "Fuwen Luo",
            "Peng Li",
            "Maosong Sun",
            "Yang Liu"
        ],
        "published": "2024-02-27T05:37:10Z",
        "summary": "Large Language Models (LLMs) have achieved remarkable performance in\nobjective tasks such as open-domain question answering and mathematical\nreasoning, which can often be solved through recalling learned factual\nknowledge or chain-of-thought style reasoning. However, we find that the\nperformance of LLMs in subjective tasks is still unsatisfactory, such as\nmetaphor recognition, dark humor detection, etc. Compared to objective tasks,\nsubjective tasks focus more on interpretation or emotional response rather than\na universally accepted reasoning pathway. Based on the characteristics of the\ntasks and the strong dialogue-generation capabilities of LLMs, we propose RiC\n(Reasoning in Conversation), a method that focuses on solving subjective tasks\nthrough dialogue simulation. The motivation of RiC is to mine useful contextual\ninformation by simulating dialogues instead of supplying chain-of-thought style\nrationales, thereby offering potential useful knowledge behind dialogues for\ngiving the final answers. We evaluate both API-based and open-source LLMs\nincluding GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental\nresults show that RiC can yield significant improvement compared with various\nbaselines.",
        "pdf_link": "https://arxiv.org/pdf/2402.17226v1.pdf"
    },
    {
        "title": "The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
        "authors": [
            "Wanli Yang",
            "Fei Sun",
            "Xinyu Ma",
            "Xun Liu",
            "Dawei Yin",
            "Xueqi Cheng"
        ],
        "published": "2024-02-15T01:50:38Z",
        "summary": "Although model editing has shown promise in revising knowledge in Large\nLanguage Models (LLMs), its impact on the inherent capabilities of LLMs is\noften overlooked. In this work, we reveal a critical phenomenon: even a single\nedit can trigger model collapse, manifesting as significant performance\ndegradation in various benchmark tasks. However, benchmarking LLMs after each\nedit, while necessary to prevent such collapses, is impractically\ntime-consuming and resource-intensive. To mitigate this, we propose using\nperplexity as a surrogate metric, validated by extensive experiments\ndemonstrating its strong correlation with downstream tasks performance. We\nfurther conduct an in-depth study on sequential editing, a practical setting\nfor real-world scenarios, across various editing methods and LLMs, focusing on\nhard cases from our previous single edit studies. The results indicate that\nnearly all examined editing methods result in model collapse after only few\nedits. To facilitate further research, we have utilized GPT-3.5 to develop a\nnew dataset, HardEdit, based on those hard cases. This dataset aims to\nestablish the foundation for pioneering research in reliable model editing and\nthe mechanisms underlying editing-induced model collapse. We hope this work can\ndraw the community's attention to the potential risks inherent in model editing\npractices.",
        "pdf_link": "https://arxiv.org/pdf/2402.09656v3.pdf"
    },
    {
        "title": "Teaching Machines to Code: Smart Contract Translation with LLMs",
        "authors": [
            "Rabimba Karanjai",
            "Lei Xu",
            "Weidong Shi"
        ],
        "published": "2024-03-13T18:55:20Z",
        "summary": "The advent of large language models (LLMs) has marked a significant milestone\nin the realm of artificial intelligence, with their capabilities often matching\nor surpassing human expertise in various domains. Among these achievements,\ntheir adeptness in translation tasks stands out, closely mimicking the\nintricate and preliminary processes undertaken by human translators to ensure\nthe fidelity and quality of the translated content. Despite the advancements in\nutilizing LLMs for translating programming code across different languages, the\ndomain of smart contract translation, particularly into languages not\npreviously encountered by the LLM, remains largely unexplored. In our research,\nwe present a pioneering approach, SolMover, which harnesses the synergy of two\ndistinct LLMs within a unified framework. This framework is designed to grasp\ncoding principles and apply this understanding to the translation of code into\nan unfamiliar language. Our study delves into the capacity of LLMs to mimic\nhuman learning processes, offering an in-depth evaluation of our methodology\nfor converting smart contracts written in Solidity to Move, a language with\nlimited resources. The framework employs one LLM to decipher coding conventions\nfor the new language, creating a blueprint for the second LLM, which, lacking\nplanning abilities, possesses coding expertise. The empirical evidence from our\nexperiments suggests that SolMover substantially enhances performance compared\nto gpt-3.5-turbo-1106, and achieves superior results over competitors such as\nPalm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the\nefficacy of our bug mitigation strategy in elevating code quality across all\nmodels, even outside the SolMover framework.",
        "pdf_link": "https://arxiv.org/pdf/2403.09740v1.pdf"
    },
    {
        "title": "Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses",
        "authors": [
            "Juyeon Kim",
            "Jeongeun Lee",
            "Yoonho Chang",
            "Chanyeol Choi",
            "Junseong Kim",
            "Jy-yong Sohn"
        ],
        "published": "2024-02-27T00:22:18Z",
        "summary": "Mitigating hallucination issues is one of the main challenges of LLMs we need\nto overcome, in order to reliably use them in real-world scenarios. Recently,\nvarious methods are proposed to check the factual errors in the LLM-generated\ntexts and revise them accordingly, to reduce the hallucination issue. In this\npaper, we propose Re-Ex, a method of revising LLM-generated texts, which\nintroduces a novel step dubbed as the factual error explanation step. Re-Ex\nrevises the initial response of LLMs using 3-steps: first, external tools are\nused to get the evidences on the factual errors in the response; second, LLMs\nare instructed to explain the problematic parts of the response based on the\nevidences gathered in the first step; finally, LLMs revise the response using\nthe explanation obtained in the second step. In addition to the explanation\nstep, we propose new prompting techniques to reduce the amount of tokens and\nwall-clock time required for the response revision process. Compared with\nexisting methods including Factool, CoVE, and RARR, Re-Ex provides better\nrevision performance with less time and fewer tokens in multiple benchmarks.",
        "pdf_link": "https://arxiv.org/pdf/2402.17097v1.pdf"
    },
    {
        "title": "Assessment of Multimodal Large Language Models in Alignment with Human Values",
        "authors": [
            "Zhelun Shi",
            "Zhipin Wang",
            "Hongxing Fan",
            "Zaibin Zhang",
            "Lijun Li",
            "Yongting Zhang",
            "Zhenfei Yin",
            "Lu Sheng",
            "Yu Qiao",
            "Jing Shao"
        ],
        "published": "2024-03-26T16:10:21Z",
        "summary": "Large Language Models (LLMs) aim to serve as versatile assistants aligned\nwith human values, as defined by the principles of being helpful, honest, and\nharmless (hhh). However, in terms of Multimodal Large Language Models (MLLMs),\ndespite their commendable performance in perception and reasoning tasks, their\nalignment with human values remains largely unexplored, given the complexity of\ndefining hhh dimensions in the visual world and the difficulty in collecting\nrelevant data that accurately mirrors real-world situations. To address this\ngap, we introduce Ch3Ef, a Compreh3ensive Evaluation dataset and strategy for\nassessing alignment with human expectations. Ch3Ef dataset contains 1002\nhuman-annotated data samples, covering 12 domains and 46 tasks based on the hhh\nprinciple. We also present a unified evaluation strategy supporting assessment\nacross various scenarios and different perspectives. Based on the evaluation\nresults, we summarize over 10 key findings that deepen the understanding of\nMLLM capabilities, limitations, and the dynamic relationships between\nevaluation levels, guiding future advancements in the field.",
        "pdf_link": "https://arxiv.org/pdf/2403.17830v1.pdf"
    },
    {
        "title": "Knowledge Editing on Black-box Large Language Models",
        "authors": [
            "Xiaoshuai Song",
            "Zhengyang Wang",
            "Keqing He",
            "Guanting Dong",
            "Yutao Mou",
            "Jinxu Zhao",
            "Weiran Xu"
        ],
        "published": "2024-02-13T17:59:34Z",
        "summary": "Knowledge editing (KE) aims to efficiently and precisely modify the behavior\nof large language models (LLMs) to update specific knowledge without negatively\ninfluencing other knowledge. Current research primarily focuses on white-box\nLLMs editing, overlooking an important scenario: black-box LLMs editing, where\nLLMs are accessed through interfaces and only textual output is available. In\nthis paper, we first officially introduce KE on black-box LLMs and then propose\na comprehensive evaluation framework to overcome the limitations of existing\nevaluations that are not applicable to black-box LLMs editing and lack\ncomprehensiveness. To tackle privacy leaks of editing data and style\nover-editing in current methods, we introduce a novel postEdit framework,\nresolving privacy concerns through downstream post-processing and maintaining\ntextual style consistency via fine-grained editing to original responses.\nExperiments and analysis on two benchmarks demonstrate that postEdit\noutperforms all baselines and achieves strong generalization, especially with\nhuge improvements on style retention (average $+20.82\\%\\uparrow$).",
        "pdf_link": "https://arxiv.org/pdf/2402.08631v2.pdf"
    },
    {
        "title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts",
        "authors": [
            "Lei Zhu",
            "Xinjiang Wang",
            "Wayne Zhang",
            "Rynson W. H. Lau"
        ],
        "published": "2024-02-22T18:58:28Z",
        "summary": "Practical large language model (LLM) services may involve a long system\nprompt, which specifies the instructions, examples, and knowledge documents of\nthe task and is reused across numerous requests. However, the long system\nprompt causes throughput/latency bottlenecks as the cost of generating the next\ntoken grows w.r.t. the sequence length. This paper aims to improve the\nefficiency of LLM services that involve long system prompts. Our key\nobservation is that handling these system prompts requires heavily redundant\nmemory accesses in existing causal attention computation algorithms.\nSpecifically, for batched requests, the cached hidden states (i.e., key-value\npairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM\nmultiple times, each corresponding to an individual request. To eliminate such\na redundancy, we propose RelayAttention, an attention algorithm that allows\nreading these hidden states from DRAM exactly once for a batch of input tokens.\nRelayAttention is a free lunch: it maintains the generation quality while\nrequiring no model retraining, as it is based on a mathematical reformulation\nof causal attention. Code is available at\n\\url{https://github.com/rayleizhu/vllm-ra}.",
        "pdf_link": "https://arxiv.org/pdf/2402.14808v2.pdf"
    },
    {
        "title": "Purifying Large Language Models by Ensembling a Small Language Model",
        "authors": [
            "Tianlin Li",
            "Qian Liu",
            "Tianyu Pang",
            "Chao Du",
            "Qing Guo",
            "Yang Liu",
            "Min Lin"
        ],
        "published": "2024-02-19T14:00:39Z",
        "summary": "The emerging success of large language models (LLMs) heavily relies on\ncollecting abundant training data from external (untrusted) sources. Despite\nsubstantial efforts devoted to data cleaning and curation, well-constructed\nLLMs have been reported to suffer from copyright infringement, data poisoning,\nand/or privacy violations, which would impede practical deployment of LLMs. In\nthis study, we propose a simple and easily implementable method for purifying\nLLMs from the negative effects caused by uncurated data, namely, through\nensembling LLMs with benign and small language models (SLMs). Aside from\ntheoretical guarantees, we perform comprehensive experiments to empirically\nconfirm the efficacy of ensembling LLMs with SLMs, which can effectively\npreserve the performance of LLMs while mitigating issues such as copyright\ninfringement, data poisoning, and privacy violations.",
        "pdf_link": "https://arxiv.org/pdf/2402.14845v1.pdf"
    },
    {
        "title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset",
        "authors": [
            "Arda Uzunoglu",
            "Abdalfatah Rashid Safa",
            "G\u00f6zde G\u00fcl \u015eahin"
        ],
        "published": "2024-03-05T18:01:59Z",
        "summary": "Recently, there has been growing interest within the community regarding\nwhether large language models are capable of planning or executing plans.\nHowever, most prior studies use LLMs to generate high-level plans for\nsimplified scenarios lacking linguistic complexity and domain diversity,\nlimiting analysis of their planning abilities. These setups constrain\nevaluation methods (e.g., predefined action space), architectural choices\n(e.g., only generative models), and overlook the linguistic nuances essential\nfor realistic analysis. To tackle this, we present PARADISE, an abductive\nreasoning task using Q\\&A format on practical procedural text sourced from\nwikiHow. It involves warning and tip inference tasks directly associated with\ngoals, excluding intermediary steps, with the aim of testing the ability of the\nmodels to infer implicit knowledge of the plan solely from the given goal. Our\nexperiments, utilizing fine-tuned language models and zero-shot prompting,\nreveal the effectiveness of task-specific small models over large language\nmodels in most scenarios. Despite advancements, all models fall short of human\nperformance. Notably, our analysis uncovers intriguing insights, such as\nvariations in model behavior with dropped keywords, struggles of BERT-family\nand GPT-4 with physical and abstract goals, and the proposed tasks offering\nvaluable prior knowledge for other unseen procedural tasks. The PARADISE\ndataset and associated resources are publicly available for further research\nexploration with https://github.com/GGLAB-KU/paradise.",
        "pdf_link": "https://arxiv.org/pdf/2403.03167v2.pdf"
    },
    {
        "title": "The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance",
        "authors": [
            "Abel Salinas",
            "Fred Morstatter"
        ],
        "published": "2024-01-08T08:28:08Z",
        "summary": "Large Language Models (LLMs) are regularly being used to label data across\nmany domains and for myriad tasks. By simply asking the LLM for an answer, or\n``prompting,'' practitioners are able to use LLMs to quickly get a response for\nan arbitrary task. This prompting is done through a series of decisions by the\npractitioner, from simple wording of the prompt, to requesting the output in a\ncertain data format, to jailbreaking in the case of prompts that address more\nsensitive topics. In this work, we ask: do variations in the way a prompt is\nconstructed change the ultimate decision of the LLM? We answer this using a\nseries of prompt variations across a variety of text classification tasks. We\nfind that even the smallest of perturbations, such as adding a space at the end\nof a prompt, can cause the LLM to change its answer. Further, we find that\nrequesting responses in XML and commonly used jailbreaks can have cataclysmic\neffects on the data labeled by LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.03729v3.pdf"
    },
    {
        "title": "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law",
        "authors": [
            "Toni J. B. Liu",
            "Nicolas Boull\u00e9",
            "Rapha\u00ebl Sarfati",
            "Christopher J. Earls"
        ],
        "published": "2024-02-01T17:28:10Z",
        "summary": "Pretrained large language models (LLMs) are surprisingly effective at\nperforming zero-shot tasks, including time-series forecasting. However,\nunderstanding the mechanisms behind such capabilities remains highly\nchallenging due to the complexity of the models. In this paper, we study LLMs'\nability to extrapolate the behavior of dynamical systems whose evolution is\ngoverned by principles of physical interest. Our results show that LLaMA 2, a\nlanguage model trained primarily on texts, achieves accurate predictions of\ndynamical system time series without fine-tuning or prompt engineering.\nMoreover, the accuracy of the learned physical rules increases with the length\nof the input context window, revealing an in-context version of neural scaling\nlaw. Along the way, we present a flexible and efficient algorithm for\nextracting probability density functions of multi-digit numbers directly from\nLLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.00795v1.pdf"
    },
    {
        "title": "PresAIse, A Prescriptive AI Solution for Enterprises",
        "authors": [
            "Wei Sun",
            "Scott McFaddin",
            "Linh Ha Tran",
            "Shivaram Subramanian",
            "Kristjan Greenewald",
            "Yeshi Tenzin",
            "Zack Xue",
            "Youssef Drissi",
            "Markus Ettl"
        ],
        "published": "2024-02-03T03:23:08Z",
        "summary": "Prescriptive AI represents a transformative shift in decision-making,\noffering causal insights and actionable recommendations. Despite its huge\npotential, enterprise adoption often faces several challenges. The first\nchallenge is caused by the limitations of observational data for accurate\ncausal inference which is typically a prerequisite for good decision-making.\nThe second pertains to the interpretability of recommendations, which is\ncrucial for enterprise decision-making settings. The third challenge is the\nsilos between data scientists and business users, hindering effective\ncollaboration. This paper outlines an initiative from IBM Research, aiming to\naddress some of these challenges by offering a suite of prescriptive AI\nsolutions. Leveraging insights from various research papers, the solution suite\nincludes scalable causal inference methods, interpretable decision-making\napproaches, and the integration of large language models (LLMs) to bridge\ncommunication gaps via a conversation agent. A proof-of-concept, PresAIse,\ndemonstrates the solutions' potential by enabling non-ML experts to interact\nwith prescriptive AI models via a natural language interface, democratizing\nadvanced analytics for strategic decision-making.",
        "pdf_link": "https://arxiv.org/pdf/2402.02006v2.pdf"
    },
    {
        "title": "Large Multi-Modal Models (LMMs) as Universal Foundation Models for AI-Native Wireless Systems",
        "authors": [
            "Shengzhe Xu",
            "Christo Kurisummoottil Thomas",
            "Omar Hashash",
            "Nikhil Muralidhar",
            "Walid Saad",
            "Naren Ramakrishnan"
        ],
        "published": "2024-01-30T00:21:41Z",
        "summary": "Large language models (LLMs) and foundation models have been recently touted\nas a game-changer for 6G systems. However, recent efforts on LLMs for wireless\nnetworks are limited to a direct application of existing language models that\nwere designed for natural language processing (NLP) applications. To address\nthis challenge and create wireless-centric foundation models, this paper\npresents a comprehensive vision on how to design universal foundation models\nthat are tailored towards the deployment of artificial intelligence (AI)-native\nnetworks. Diverging from NLP-based foundation models, the proposed framework\npromotes the design of large multi-modal models (LMMs) fostered by three key\ncapabilities: 1) processing of multi-modal sensing data, 2) grounding of\nphysical symbol representations in real-world wireless systems using causal\nreasoning and retrieval-augmented generation (RAG), and 3) enabling\ninstructibility from the wireless environment feedback to facilitate dynamic\nnetwork adaptation thanks to logical and mathematical reasoning facilitated by\nneuro-symbolic AI. In essence, these properties enable the proposed LMM\nframework to build universal capabilities that cater to various cross-layer\nnetworking tasks and alignment of intents across different domains. Preliminary\nresults from experimental evaluation demonstrate the efficacy of grounding\nusing RAG in LMMs, and showcase the alignment of LMMs with wireless system\ndesigns. Furthermore, the enhanced rationale exhibited in the responses to\nmathematical questions by LMMs, compared to vanilla LLMs, demonstrates the\nlogical and mathematical reasoning capabilities inherent in LMMs. Building on\nthose results, we present a sequel of open questions and challenges for LMMs.\nWe then conclude with a set of recommendations that ignite the path towards\nLMM-empowered AI-native systems.",
        "pdf_link": "https://arxiv.org/pdf/2402.01748v2.pdf"
    },
    {
        "title": "Pandora's White-Box: Increased Training Data Leakage in Open LLMs",
        "authors": [
            "Jeffrey G. Wang",
            "Jason Wang",
            "Marvin Li",
            "Seth Neel"
        ],
        "published": "2024-02-26T20:41:50Z",
        "summary": "In this paper we undertake a systematic study of privacy attacks against open\nsource Large Language Models (LLMs), where an adversary has access to either\nthe model weights, gradients, or losses, and tries to exploit them to learn\nsomething about the underlying training data. Our headline results are the\nfirst membership inference attacks (MIAs) against pre-trained LLMs that are\nable to simultaneously achieve high TPRs and low FPRs, and a pipeline showing\nthat over $50\\%$ (!) of the fine-tuning dataset can be extracted from a\nfine-tuned LLM in natural settings. We consider varying degrees of access to\nthe underlying model, customization of the language model, and resources\navailable to the attacker. In the pre-trained setting, we propose three new\nwhite-box MIAs: an attack based on the gradient norm, a supervised neural\nnetwork classifier, and a single step loss ratio attack. All outperform\nexisting black-box baselines, and our supervised attack closes the gap between\nMIA attack success against LLMs and other types of models. In fine-tuning, we\nfind that given access to the loss of the fine-tuned and base models, a\nfine-tuned loss ratio attack FLoRA is able to achieve near perfect MIA\npeformance. We then leverage these MIAs to extract fine-tuning data from\nfine-tuned language models. We find that the pipeline of generating from\nfine-tuned models prompted with a small snippet of the prefix of each training\nexample, followed by using FLoRa to select the most likely training sample,\nsucceeds the majority of the fine-tuning dataset after only $3$ epochs of\nfine-tuning. Taken together, these findings show that highly effective MIAs are\navailable in almost all LLM training settings, and highlight that great care\nmust be taken before LLMs are fine-tuned on highly sensitive data and then\ndeployed.",
        "pdf_link": "https://arxiv.org/pdf/2402.17012v1.pdf"
    },
    {
        "title": "The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends",
        "authors": [
            "Mengqi Chen",
            "Bin Guo",
            "Hao Wang",
            "Haoyu Li",
            "Qian Zhao",
            "Jingqi Liu",
            "Yasan Ding",
            "Yan Pan",
            "Zhiwen Yu"
        ],
        "published": "2024-02-07T07:28:34Z",
        "summary": "Persuasion, as one of the crucial abilities in human communication, has\ngarnered extensive attention from researchers within the field of intelligent\ndialogue systems. We humans tend to persuade others to change their viewpoints,\nattitudes or behaviors through conversations in various scenarios (e.g.,\npersuasion for social good, arguing in online platforms). Developing dialogue\nagents that can persuade others to accept certain standpoints is essential to\nachieving truly intelligent and anthropomorphic dialogue system. Benefiting\nfrom the substantial progress of Large Language Models (LLMs), dialogue agents\nhave acquired an exceptional capability in context understanding and response\ngeneration. However, as a typical and complicated cognitive psychological\nsystem, persuasive dialogue agents also require knowledge from the domain of\ncognitive psychology to attain a level of human-like persuasion. Consequently,\nthe cognitive strategy-enhanced persuasive dialogue agent (defined as\nCogAgent), which incorporates cognitive strategies to achieve persuasive\ntargets through conversation, has become a predominant research paradigm. To\ndepict the research trends of CogAgent, in this paper, we first present several\nfundamental cognitive psychology theories and give the formalized definition of\nthree typical cognitive strategies, including the persuasion strategy, the\ntopic path planning strategy, and the argument structure prediction strategy.\nThen we propose a new system architecture by incorporating the formalized\ndefinition to lay the foundation of CogAgent. Representative works are detailed\nand investigated according to the combined cognitive strategy, followed by the\nsummary of authoritative benchmarks and evaluation metrics. Finally, we\nsummarize our insights on open issues and future directions of CogAgent for\nupcoming researchers.",
        "pdf_link": "https://arxiv.org/pdf/2402.04631v1.pdf"
    },
    {
        "title": "AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions",
        "authors": [
            "Hao Zhang",
            "Wenqi Shao",
            "Hong Liu",
            "Yongqiang Ma",
            "Ping Luo",
            "Yu Qiao",
            "Kaipeng Zhang"
        ],
        "published": "2024-03-14T12:51:07Z",
        "summary": "Large Vision-Language Models (LVLMs) have shown significant progress in well\nresponding to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce AVIBench, a framework designed to analyze the robustness\nof LVLMs when facing various adversarial visual-instructions (AVIs), including\nfour types of image-based AVIs, ten types of text-based AVIs, and nine types of\ncontent bias AVIs (such as gender, violence, cultural, and racial biases, among\nothers). We generate 260K AVIs encompassing five categories of multimodal\ncapabilities (nine tasks) and content bias. We then conduct a comprehensive\nevaluation involving 14 open-source LVLMs to assess their performance. AVIBench\nalso serves as a convenient tool for practitioners to evaluate the robustness\nof LVLMs against AVIs. Our findings and extensive experimental results shed\nlight on the vulnerabilities of LVLMs, and highlight that inherent biases exist\neven in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This\nunderscores the importance of enhancing the robustness, security, and fairness\nof LVLMs. The source code and benchmark will be made publicly available.",
        "pdf_link": "https://arxiv.org/pdf/2403.09346v1.pdf"
    },
    {
        "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data",
        "authors": [
            "Chengcheng Wei",
            "Ze Chen",
            "Songtan Fang",
            "Jiarong He",
            "Max Gao"
        ],
        "published": "2024-02-20T11:01:39Z",
        "summary": "This paper mainly describes a unified system for hallucination detection of\nLLMs, which wins the second prize in the model-agnostic track of the\nSemEval-2024 Task 6, and also achieves considerable results in the model-aware\ntrack. This task aims to detect hallucination with LLMs for three different\ntext-generation tasks without labeled training data. We utilize prompt\nengineering and few-shot learning to verify the performance of different LLMs\non the validation data. Then we select the LLMs with better performance to\ngenerate high-quality weakly supervised training data, which not only satisfies\nthe consistency of different LLMs, but also satisfies the consistency of the\noptimal LLM with different sampling parameters. Furthermore, we finetune\ndifferent LLMs by using the constructed training data, and finding that a\nrelatively small LLM can achieve a competitive level of performance in\nhallucination detection, when compared to the large LLMs and the prompt-based\napproaches using GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2402.12913v1.pdf"
    },
    {
        "title": "Improving Attributed Text Generation of Large Language Models via Preference Learning",
        "authors": [
            "Dongfang Li",
            "Zetian Sun",
            "Baotian Hu",
            "Zhenyu Liu",
            "Xinshuo Hu",
            "Xuebo Liu",
            "Min Zhang"
        ],
        "published": "2024-03-27T09:19:13Z",
        "summary": "Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.",
        "pdf_link": "https://arxiv.org/pdf/2403.18381v1.pdf"
    },
    {
        "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance",
        "authors": [
            "Renjie Pi",
            "Tianyang Han",
            "Yueqi Xie",
            "Rui Pan",
            "Qing Lian",
            "Hanze Dong",
            "Jipeng Zhang",
            "Tong Zhang"
        ],
        "published": "2024-01-05T17:05:42Z",
        "summary": "The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.",
        "pdf_link": "https://arxiv.org/pdf/2401.02906v2.pdf"
    },
    {
        "title": "Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS",
        "authors": [
            "Matthew DeLorenzo",
            "Animesh Basak Chowdhury",
            "Vasudev Gohil",
            "Shailja Thakur",
            "Ramesh Karri",
            "Siddharth Garg",
            "Jeyavijayan Rajendran"
        ],
        "published": "2024-02-05T18:47:04Z",
        "summary": "Existing large language models (LLMs) for register transfer level code\ngeneration face challenges like compilation failures and suboptimal power,\nperformance, and area (PPA) efficiency. This is due to the lack of PPA\nawareness in conventional transformer decoding algorithms. In response, we\npresent an automated transformer decoding algorithm that integrates Monte Carlo\ntree-search for lookahead, guiding the transformer to produce compilable,\nfunctionally correct, and PPA-optimized code. Empirical evaluation with a\nfine-tuned language model on RTL codesets shows that our proposed technique\nconsistently generates functionally correct code compared to prompting-only\nmethods and effectively addresses the PPA-unawareness drawback of naive large\nlanguage models. For the largest design generated by the state-of-the-art LLM\n(16-bit adder), our technique can achieve a 31.8% improvement in the area-delay\nproduct.",
        "pdf_link": "https://arxiv.org/pdf/2402.03289v1.pdf"
    },
    {
        "title": "ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework",
        "authors": [
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Nitish Nagesh",
            "Mahyar Abbasian",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "published": "2024-02-18T06:07:17Z",
        "summary": "The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity.",
        "pdf_link": "https://arxiv.org/pdf/2403.00781v2.pdf"
    },
    {
        "title": "Mercury: An Efficiency Benchmark for LLM Code Synthesis",
        "authors": [
            "Mingzhe Du",
            "Anh Tuan Luu",
            "Bin Ji",
            "See-Kiong Ng"
        ],
        "published": "2024-02-12T17:53:22Z",
        "summary": "Despite advancements in evaluating Large Language Models (LLMs) for code\nsynthesis, benchmarks have predominantly focused on functional correctness,\noverlooking the importance of code efficiency. We present Mercury, the first\nbenchmark designated for assessing the code efficiency of LLM code synthesis\ntasks. Mercury consists of 1,889 programming tasks covering diverse difficulty\nlevels alongside test case generators generating unlimited cases for\ncomprehensive evaluation. Unlike existing benchmarks, Mercury integrates a\nnovel metric Beyond@K to measure normalized code efficiency based on historical\nsubmissions, leading to a new evaluation indicator for code synthesis, which\nencourages generating functionally correct and computationally efficient code,\nmirroring the real-world software development standard. Our findings reveal\nthat while LLMs demonstrate the remarkable capability to generate functionally\ncorrect code, there still exists a substantial gap in their efficiency output,\nunderscoring a new frontier for LLM research and development.",
        "pdf_link": "https://arxiv.org/pdf/2402.07844v1.pdf"
    },
    {
        "title": "SPA: Towards A Computational Friendly Cloud-Base and On-Devices Collaboration Seq2seq Personalized Generation",
        "authors": [
            "Yanming Liu",
            "Xinyue Peng",
            "Jiannan Cao",
            "Le Dai",
            "Xingzu Liu",
            "Weihao Liu",
            "Mingbang Wang"
        ],
        "published": "2024-03-11T18:26:02Z",
        "summary": "Large language models(LLMs) have shown its outperforming ability on various\ntasks and question answering. However, LLMs require high computation cost and\nlarge memory cost. At the same time, LLMs may cause privacy leakage when\ntraining or prediction procedure contains sensitive information. In this paper,\nwe propose SPA(Side Plugin Adaption), a lightweight architecture for fast\non-devices inference and privacy retaining on the constraints of strict\non-devices computation and memory constraints. Compared with other on-devices\nseq2seq generation, SPA could make a fast and stable inference on low-resource\nconstraints, allowing it to obtain cost effiency. Our method establish an\ninteraction between a pretrained LLMs on-cloud and additive parameters\non-devices, which could provide the knowledge on both pretrained LLMs and\nprivate personal feature.Further more, SPA provides a framework to keep\nfeature-base parameters on private guaranteed but low computational devices\nwhile leave the parameters containing general information on the high\ncomputational devices.",
        "pdf_link": "https://arxiv.org/pdf/2403.07088v1.pdf"
    },
    {
        "title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification",
        "authors": [
            "Martin Gubri",
            "Dennis Ulmer",
            "Hwaran Lee",
            "Sangdoo Yun",
            "Seong Joon Oh"
        ],
        "published": "2024-02-20T13:20:39Z",
        "summary": "Large Language Model (LLM) services and models often come with legal rules on\nwho can use them and how they must use them. Assessing the compliance of the\nreleased LLMs is crucial, as these rules protect the interests of the LLM\ncontributor and prevent misuse. In this context, we describe the novel problem\nof Black-box Identity Verification (BBIV). The goal is to determine whether a\nthird-party application uses a certain LLM through its chat function. We\npropose a method called Targeted Random Adversarial Prompt (TRAP) that\nidentifies the specific LLM in use. We repurpose adversarial suffixes,\noriginally proposed for jailbreaking, to get a pre-defined answer from the\ntarget LLM, while other models give random answers. TRAP detects the target\nLLMs with over 95% true positive rate at under 0.2% false positive rate even\nafter a single interaction. TRAP remains effective even if the LLM has minor\nchanges that do not significantly alter the original function.",
        "pdf_link": "https://arxiv.org/pdf/2402.12991v1.pdf"
    },
    {
        "title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs",
        "authors": [
            "Yebowen Hu",
            "Kaiqiang Song",
            "Sangwoo Cho",
            "Xiaoyang Wang",
            "Hassan Foroosh",
            "Dong Yu",
            "Fei Liu"
        ],
        "published": "2024-02-15T20:26:07Z",
        "summary": "Large language models hold significant potential for integrating various data\ntypes, such as text documents and database records, for advanced analytics.\nHowever, blending text and numerical data presents substantial challenges. LLMs\nneed to process and cross-reference entities and numbers, handle data\ninconsistencies and redundancies, and develop planning capabilities such as\nbuilding a working memory for managing complex data queries. In this paper, we\nintroduce four novel tasks centered around sports data analytics to evaluate\nthe numerical reasoning and information fusion capabilities of LLMs. These\ntasks involve providing LLMs with detailed, play-by-play sports game\ndescriptions, then challenging them with adversarial scenarios such as new game\nrules, longer durations, scrambled narratives, and analyzing key statistics in\ngame summaries. We conduct extensive experiments on NBA and NFL games to assess\nthe performance of LLMs on these tasks. Our benchmark, SportsMetrics,\nintroduces a new mechanism for assessing LLMs' numerical reasoning and fusion\nskills.",
        "pdf_link": "https://arxiv.org/pdf/2402.10979v1.pdf"
    },
    {
        "title": "TP-Aware Dequantization",
        "authors": [
            "Adnan Hoque",
            "Mudhakar Srivatsa",
            "Chih-Chieh Yang",
            "Raghu Ganti"
        ],
        "published": "2024-01-15T08:01:40Z",
        "summary": "In this paper, we present a novel method that reduces model inference latency\nduring distributed deployment of Large Language Models (LLMs). Our contribution\nis an optimized inference deployment scheme that address the current\nlimitations of state-of-the-art quantization kernels when used in conjunction\nwith Tensor Parallel (TP). Our method preserves data locality in GPU memory\naccess patterns and exploits a priori knowledge of TP to reduce global\ncommunication. We demonstrate an up to 1.81x speedup over existing methods for\nLlama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer\nproblem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.",
        "pdf_link": "https://arxiv.org/pdf/2402.04925v1.pdf"
    },
    {
        "title": "DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations",
        "authors": [
            "Weihao Zeng",
            "Dayuan Fu",
            "Keqing He",
            "Yejie Wang",
            "Yukai Xu",
            "Weiran Xu"
        ],
        "published": "2024-03-31T04:36:57Z",
        "summary": "Language models pre-trained on general text have achieved impressive results\nin diverse fields. Yet, the distinct linguistic characteristics of\ntask-oriented dialogues (TOD) compared to general text limit the practical\nutility of existing language models. Current task-oriented dialogue\npre-training methods overlook the one-to-many property of conversations, where\nmultiple responses can be appropriate given the same conversation context. In\nthis paper, we propose a novel dialogue pre-training model called DivTOD, which\ncollaborates with LLMs to learn diverse task-oriented dialogue representations.\nDivTOD guides LLMs in transferring diverse knowledge to smaller models while\nremoving domain knowledge that contradicts task-oriented dialogues. Experiments\nshow that our model outperforms strong TOD baselines on various downstream\ndialogue tasks and learns the intrinsic diversity of task-oriented dialogues.",
        "pdf_link": "https://arxiv.org/pdf/2404.00557v1.pdf"
    },
    {
        "title": "ANLS* -- A Universal Document Processing Metric for Generative Large Language Models",
        "authors": [
            "David Peer",
            "Philemon Sch\u00f6pf",
            "Volckmar Nebendahl",
            "Alexander Rietzler",
            "Sebastian Stabinger"
        ],
        "published": "2024-02-06T09:50:08Z",
        "summary": "Traditionally, discriminative models have been the predominant choice for\ntasks like document classification and information extraction. These models\nmake predictions that fall into a limited number of predefined classes,\nfacilitating a binary true or false evaluation and enabling the direct\ncalculation of metrics such as the F1 score. However, recent advancements in\ngenerative large language models (GLLMs) have prompted a shift in the field due\nto their enhanced zero-shot capabilities, which eliminate the need for a\ndownstream dataset and computationally expensive fine-tuning. However,\nevaluating GLLMs presents a challenge as the binary true or false evaluation\nused for discriminative models is not applicable to the predictions made by\nGLLMs.\n  This paper introduces a new metric for generative models called ANLS* for\nevaluating a wide variety of tasks, including information extraction and\nclassification tasks. The ANLS* metric extends existing ANLS metrics as a\ndrop-in-replacement and is still compatible with previously reported ANLS\nscores. An evaluation of 7 different datasets, 6 different GLLMs and 3\ndifferent prompting methods using the ANLS* metric is also provided,\ndemonstrating the importance of the proposed metric.\n  We also benchmark a novel approach to generate prompts for documents, called\nSFT, against other prompting techniques such as LATIN. In 27 out of 35 cases,\nSFT outperforms other techniques and improves the state-of-the-art, sometimes\nby as much as $18$ percentage points.\n  Sources are available at https://github.com/deepopinion/anls_star_metric",
        "pdf_link": "https://arxiv.org/pdf/2402.03848v3.pdf"
    },
    {
        "title": "Dual Instruction Tuning with Large Language Models for Mathematical Reasoning",
        "authors": [
            "Yongwei Zhou",
            "Tiejun Zhao"
        ],
        "published": "2024-03-27T06:43:58Z",
        "summary": "Recent advancements highlight the success of instruction tuning with large\nlanguage models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical\nreasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as\nincorrect, missing, and redundant steps in CoT generation leading to\ninaccuracies in answer predictions. To alleviate this problem, we propose a\ndual instruction tuning strategy to meticulously model mathematical reasoning\nfrom both forward and reverse directions. This involves introducing the\nIntermediate Reasoning State Prediction task (forward reasoning) and the\nInstruction Reconstruction task (reverse reasoning) to enhance the LLMs'\nunderstanding and execution of instructions. Training instances for these tasks\nare constructed based on existing mathematical instruction tuning datasets.\nSubsequently, LLMs undergo multi-task fine-tuning using both existing\nmathematical instructions and the newly created data. Comprehensive experiments\nvalidate the effectiveness and domain generalization of the dual instruction\ntuning strategy across various mathematical reasoning tasks.",
        "pdf_link": "https://arxiv.org/pdf/2403.18295v1.pdf"
    },
    {
        "title": "Membership Inference Attacks and Privacy in Topic Modeling",
        "authors": [
            "Nico Manzonelli",
            "Wanrong Zhang",
            "Salil Vadhan"
        ],
        "published": "2024-03-07T12:43:42Z",
        "summary": "Recent research shows that large language models are susceptible to privacy\nattacks that infer aspects of the training data. However, it is unclear if\nsimpler generative models, like topic models, share similar vulnerabilities. In\nthis work, we propose an attack against topic models that can confidently\nidentify members of the training data in Latent Dirichlet Allocation. Our\nresults suggest that the privacy risks associated with generative modeling are\nnot restricted to large neural models. Additionally, to mitigate these\nvulnerabilities, we explore differentially private (DP) topic modeling. We\npropose a framework for private topic modeling that incorporates DP vocabulary\nselection as a pre-processing step, and show that it improves privacy while\nhaving limited effects on practical utility.",
        "pdf_link": "https://arxiv.org/pdf/2403.04451v1.pdf"
    },
    {
        "title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review",
        "authors": [
            "Luoma Ke",
            "Song Tong",
            "Peng Cheng",
            "Kaiping Peng"
        ],
        "published": "2024-01-03T03:01:29Z",
        "summary": "This paper explores the frontiers of large language models (LLMs) in\npsychology applications. Psychology has undergone several theoretical changes,\nand the current use of Artificial Intelligence (AI) and Machine Learning,\nparticularly LLMs, promises to open up new research directions. We provide a\ndetailed exploration of how LLMs like ChatGPT are transforming psychological\nresearch. It discusses the impact of LLMs across various branches of\npsychology, including cognitive and behavioral, clinical and counseling,\neducational and developmental, and social and cultural psychology, highlighting\ntheir potential to simulate aspects of human cognition and behavior. The paper\ndelves into the capabilities of these models to emulate human-like text\ngeneration, offering innovative tools for literature review, hypothesis\ngeneration, experimental design, experimental subjects, data analysis, academic\nwriting, and peer review in psychology. While LLMs are essential in advancing\nresearch methodologies in psychology, the paper also cautions about their\ntechnical and ethical challenges. There are issues like data privacy, the\nethical implications of using LLMs in psychological research, and the need for\na deeper understanding of these models' limitations. Researchers should\nresponsibly use LLMs in psychological studies, adhering to ethical standards\nand considering the potential consequences of deploying these technologies in\nsensitive areas. Overall, the article provides a comprehensive overview of the\ncurrent state of LLMs in psychology, exploring potential benefits and\nchallenges. It serves as a call to action for researchers to leverage LLMs'\nadvantages responsibly while addressing associated risks.",
        "pdf_link": "https://arxiv.org/pdf/2401.01519v3.pdf"
    },
    {
        "title": "General2Specialized LLMs Translation for E-commerce",
        "authors": [
            "Kaidi Chen",
            "Ben Chen",
            "Dehong Gao",
            "Huangyu Dai",
            "Wen Jiang",
            "Wei Ning",
            "Shanqing Yu",
            "Libin Yang",
            "Xiaoyan Cai"
        ],
        "published": "2024-03-06T13:15:21Z",
        "summary": "Existing Neural Machine Translation (NMT) models mainly handle translation in\nthe general domain, while overlooking domains with special writing formulas,\nsuch as e-commerce and legal documents. Taking e-commerce as an example, the\ntexts usually include amounts of domain-related words and have more grammar\nproblems, which leads to inferior performances of current NMT methods. To\naddress these problems, we collect two domain-related resources, including a\nset of term pairs (aligned Chinese-English bilingual terms) and a parallel\ncorpus annotated for the e-commerce domain. Furthermore, we propose a two-step\nfine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to\ntransfer one general NMT model to the specialized NMT model for e-commerce. The\nparadigm can be used for the NMT models based on Large language models (LLMs).\nExtensive evaluations on real e-commerce titles demonstrate the superior\ntranslation quality and robustness of our G2ST approach, as compared with\nstate-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.",
        "pdf_link": "https://arxiv.org/pdf/2403.03689v2.pdf"
    },
    {
        "title": "NOTE: Notable generation Of patient Text summaries through Efficient approach based on direct preference optimization",
        "authors": [
            "Imjin Ahn",
            "Hansle Gwon",
            "Young-Hak Kim",
            "Tae Joon Jun",
            "Sanghyun Park"
        ],
        "published": "2024-02-19T06:43:25Z",
        "summary": "The discharge summary is a one of critical documents in the patient journey,\nencompassing all events experienced during hospitalization, including multiple\nvisits, medications, tests, surgery/procedures, and admissions/discharge.\nProviding a summary of the patient's progress is crucial, as it significantly\ninfluences future care and planning. Consequently, clinicians face the\nlaborious and resource-intensive task of manually collecting, organizing, and\ncombining all the necessary data for a discharge summary. Therefore, we propose\n\"NOTE\", which stands for \"Notable generation Of patient Text summaries through\nan Efficient approach based on direct preference optimization\". NOTE is based\non Medical Information Mart for Intensive Care- III dataset and summarizes a\nsingle hospitalization of a patient. Patient events are sequentially combined\nand used to generate a discharge summary for each hospitalization. In the\npresent circumstances, large language models' application programming\ninterfaces (LLMs' APIs) are widely available, but importing and exporting\nmedical data presents significant challenges due to privacy protection policies\nin healthcare institutions. Moreover, to ensure optimal performance, it is\nessential to implement a lightweight model for internal server or program\nwithin the hospital. Therefore, we utilized DPO and parameter efficient fine\ntuning (PEFT) techniques to apply a fine-tuning method that guarantees superior\nperformance. To demonstrate the practical application of the developed NOTE, we\nprovide a webpage-based demonstration software. In the future, we will aim to\ndeploy the software available for actual use by clinicians in hospital. NOTE\ncan be utilized to generate various summaries not only discharge summaries but\nalso throughout a patient's journey, thereby alleviating the labor-intensive\nworkload of clinicians and aiming for increased efficiency.",
        "pdf_link": "https://arxiv.org/pdf/2402.11882v1.pdf"
    },
    {
        "title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data",
        "authors": [
            "Xiao Liu",
            "Zirui Wu",
            "Xueqing Wu",
            "Pan Lu",
            "Kai-Wei Chang",
            "Yansong Feng"
        ],
        "published": "2024-02-27T16:15:03Z",
        "summary": "Quantitative reasoning is a critical skill to analyze data, yet the\nassessment of such ability remains limited. To address this gap, we introduce\nthe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate\nLarge Language Models' capability in statistical and causal reasoning with\nreal-world data. The benchmark comprises a carefully constructed dataset of 411\nquestions accompanied by data sheets from textbooks, online learning materials,\nand academic papers. To compare models' quantitative reasoning abilities on\ndata and text, we enrich the benchmark with an auxiliary set of 290 text-only\nquestions, namely QRText. We evaluate natural language reasoning, program-based\nreasoning, and agent reasoning methods including Chain-of-Thought,\nProgram-of-Thoughts, ReAct, and code interpreter assistants on diverse models.\nThe strongest model GPT-4 achieves an accuracy of 58%, which has a large room\nfor improvement. Among open-source models, Deepseek-coder-instruct, a code LLM\npretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals\nthat models encounter difficulties in data analysis and causal reasoning, and\nstruggle in using causal knowledge and provided data simultaneously. Code and\ndata are in https://github.com/xxxiaol/QRData.",
        "pdf_link": "https://arxiv.org/pdf/2402.17644v1.pdf"
    },
    {
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
        "authors": [
            "Shihan Dou",
            "Yan Liu",
            "Haoxiang Jia",
            "Limao Xiong",
            "Enyu Zhou",
            "Wei Shen",
            "Junjie Shan",
            "Caishuang Huang",
            "Xiao Wang",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Tao Ji",
            "Rui Zheng",
            "Qi Zhang",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "published": "2024-02-02T13:14:31Z",
        "summary": "The advancement of large language models (LLMs) has significantly propelled\nthe field of code generation. Previous work integrated reinforcement learning\n(RL) with compiler feedback for exploring the output space of LLMs to enhance\ncode generation quality. However, the lengthy code generated by LLMs in\nresponse to complex human requirements makes RL exploration a challenge. Also,\nsince the unit tests may not cover the complicated code, optimizing LLMs by\nusing these unexecuted code snippets is ineffective. To tackle these\nchallenges, we introduce StepCoder, a novel RL framework for code generation,\nconsisting of two main components: CCCS addresses the exploration challenge by\nbreaking the long sequences code generation task into a Curriculum of Code\nCompletion Subtasks, while FGO only optimizes the model by masking the\nunexecuted code segments to provide Fine-Grained Optimization. In addition, we\nfurthermore construct the APPS+ dataset for RL training, which is manually\nverified to ensure the correctness of unit tests. Experimental results show\nthat our method improves the ability to explore the output space and\noutperforms state-of-the-art approaches in corresponding benchmarks. Our\ndataset APPS+ and StepCoder are available online.",
        "pdf_link": "https://arxiv.org/pdf/2402.01391v2.pdf"
    },
    {
        "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
        "authors": [
            "Victor Carbune",
            "Hassan Mansoor",
            "Fangyu Liu",
            "Rahul Aralikatte",
            "Gilles Baechler",
            "Jindong Chen",
            "Abhanshu Sharma"
        ],
        "published": "2024-03-19T10:03:07Z",
        "summary": "Vision-language models (VLMs) are achieving increasingly strong performance\non multimodal tasks. However, reasoning capabilities remain limited\nparticularly for smaller VLMs, while those of large-language models (LLMs) have\nseen numerous improvements. We propose a technique to transfer capabilities\nfrom LLMs to VLMs. On the recently introduced ChartQA, our method obtains\nstate-of-the-art performance when applied on the PaLI3-5B VLM by\n\\citet{chen2023pali3}, while also enabling much better performance on PlotQA\nand FigureQA.\n  We first improve the chart representation by continuing the pre-training\nstage using an improved version of the chart-to-table translation task by\n\\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than\nthe original training set. To improve general reasoning capabilities and\nimprove numerical operations, we synthesize reasoning traces using the table\nrepresentation of charts. Lastly, our model is fine-tuned using the multitask\nloss introduced by \\citet{hsieh2023distilling}.\n  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B\nwithout using an upstream OCR system, while keeping inference time constant\ncompared to the PaLI3-5B baseline. When rationales are further refined with a\nsimple program-of-thought prompt \\cite{chen2023program}, our model outperforms\nthe recently introduced Gemini Ultra and GPT-4V.",
        "pdf_link": "https://arxiv.org/pdf/2403.12596v1.pdf"
    },
    {
        "title": "Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision Language Model for Pathology Imaging",
        "authors": [
            "Jai Prakash Veerla",
            "Poojitha Thota",
            "Partha Sai Guttikonda",
            "Shirin Nilizadeh",
            "Jacob M. Luber"
        ],
        "published": "2024-01-04T22:49:15Z",
        "summary": "In the dynamic landscape of medical artificial intelligence, this study\nexplores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)\nmodel, a Vision Language Foundation model, under targeted adversarial\nconditions. Leveraging the Kather Colon dataset with 7,180 H&E images across\nnine tissue types, our investigation employs Projected Gradient Descent (PGD)\nadversarial attacks to intentionally induce misclassifications. The outcomes\nreveal a 100% success rate in manipulating PLIP's predictions, underscoring its\nsusceptibility to adversarial perturbations. The qualitative analysis of\nadversarial examples delves into the interpretability challenges, shedding\nlight on nuanced changes in predictions induced by adversarial manipulations.\nThese findings contribute crucial insights into the interpretability, domain\nadaptation, and trustworthiness of Vision Language Models in medical imaging.\nThe study emphasizes the pressing need for robust defenses to ensure the\nreliability of AI models.",
        "pdf_link": "https://arxiv.org/pdf/2401.02565v2.pdf"
    },
    {
        "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
        "authors": [
            "Weiqing Luo",
            "Chonggang Song",
            "Lingling Yi",
            "Gong Cheng"
        ],
        "published": "2024-03-11T12:04:20Z",
        "summary": "The utilization of semantic information is an important research problem in\nthe field of recommender systems, which aims to complement the missing parts of\nmainstream ID-based approaches. With the rise of LLM, its ability to act as a\nknowledge base and its reasoning capability have opened up new possibilities\nfor this research area, making LLM-based recommendation an emerging research\ndirection. However, directly using LLM to process semantic information for\nrecommendation scenarios is unreliable and sub-optimal due to several problems\nsuch as hallucination. A promising way to cope with this is to use external\nknowledge to aid LLM in generating truthful and usable text. Inspired by the\nabove motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to\nusing external knowledge in prompts, the proposed method also includes a\nknowledge-based contrastive learning scheme for training. Experiments on public\ndatasets and in-enterprise datasets validate the effectiveness of the proposed\nmethod.",
        "pdf_link": "https://arxiv.org/pdf/2403.06642v1.pdf"
    },
    {
        "title": "Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data",
        "authors": [
            "Joseph Gatto",
            "Parker Seegmiller",
            "Omar Sharif",
            "Sarah M. Preum"
        ],
        "published": "2024-03-05T20:07:42Z",
        "summary": "Document-Level Event Argument Extraction (DocEAE) is an extremely difficult\ninformation extraction problem -- with significant limitations in low-resource\ncross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA),\na novel generative DocEAE data augmentation framework. Our approach leverages\nthe intuition that Mad Libs, which are categorically masked documents used as a\npart of a popular game, can be generated and solved by LLMs to produce data for\nDocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1\nscore. Moreover, this approach achieves a 3.9 and 5.2 point average increase in\nzero and few-shot event roles compared to augmentation-free baselines across\nall experiments.\n  To better facilitate analysis of cross-domain DocEAE, we additionally\nintroduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to\nidentify roles in the target domain which are semantic outliers with respect to\nroles observed in the source domain. Our experiments show that MLA augmentation\ncan boost RDF1 performance by an average of 5.85 points compared to\nnon-augmented datasets.",
        "pdf_link": "https://arxiv.org/pdf/2403.03304v1.pdf"
    },
    {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
            "S. M Towhidul Islam Tonmoy",
            "S M Mehedi Zaman",
            "Vinija Jain",
            "Anku Rani",
            "Vipula Rawte",
            "Aman Chadha",
            "Amitava Das"
        ],
        "published": "2024-01-02T17:56:30Z",
        "summary": "As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2401.01313v3.pdf"
    },
    {
        "title": "Provably learning a multi-head attention layer",
        "authors": [
            "Sitan Chen",
            "Yuanzhi Li"
        ],
        "published": "2024-02-06T15:39:09Z",
        "summary": "The multi-head attention layer is one of the key components of the\ntransformer architecture that sets it apart from traditional feed-forward\nmodels. Given a sequence length $k$, attention matrices\n$\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and\nprojection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times\nd}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to\n\\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional\ntokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq\n\\sum^m_{i=1}\n\\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.\nIn this work, we initiate the study of provably learning a multi-head attention\nlayer from random examples and give the first nontrivial upper and lower bounds\nfor this problem:\n  - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain\nnon-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns\n$F$ to small error given random labeled examples drawn uniformly from $\\{\\pm\n1\\}^{k\\times d}$.\n  - We prove computational lower bounds showing that in the worst case,\nexponential dependence on $m$ is unavoidable.\n  We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in\nlarge language models, though our techniques naturally extend to standard\ncontinuous settings, e.g. Gaussian. Our algorithm, which is centered around\nusing examples to sculpt a convex body containing the unknown parameters, is a\nsignificant departure from existing provable algorithms for learning\nfeedforward networks, which predominantly exploit algebraic and rotation\ninvariance properties of the Gaussian distribution. In contrast, our analysis\nis more flexible as it primarily relies on various upper and lower tail bounds\nfor the input distribution and \"slices\" thereof.",
        "pdf_link": "https://arxiv.org/pdf/2402.04084v1.pdf"
    },
    {
        "title": "Sphere Neural-Networks for Rational Reasoning",
        "authors": [
            "Tiansi Dong",
            "Mateja Jamnik",
            "Pietro Li\u00f2"
        ],
        "published": "2024-03-22T15:44:59Z",
        "summary": "The success of Large Language Models (LLMs), e.g., ChatGPT, is witnessed by\ntheir planetary popularity, their capability of human-like question-answering,\nand also by their steadily improved reasoning performance. However, it remains\nunclear whether LLMs reason. It is an open problem how traditional neural\nnetworks can be qualitatively extended to go beyond the statistic paradigm and\nachieve high-level cognition. Here, we present a minimalist qualitative\nextension by generalising computational building blocks from vectors to\nspheres. We propose Sphere Neural Networks (SphNNs) for human-like reasoning\nthrough model construction and inspection, and develop SphNN for syllogistic\nreasoning, a microcosm of human rationality. Instead of training data, SphNN\nuses a neuro-symbolic transition map of neighbourhood spatial relations to\nguide transformations from the current sphere configuration towards the target.\nSphNN is the first neural model that can determine the validity of long-chained\nsyllogistic reasoning in one epoch by constructing sphere configurations as\nEuler diagrams, with the worst computational complexity of O(N^2). SphNN can\nevolve into various types of reasoning, such as spatio-temporal reasoning,\nlogical reasoning with negation and disjunction, event reasoning,\nneuro-symbolic reasoning, and humour understanding (the highest level of\ncognition). All these suggest a new kind of Herbert A. Simon's scissors with\ntwo neural blades. SphNNs will tremendously enhance interdisciplinary\ncollaborations to develop the two neural blades and realise deterministic\nneural reasoning and human-bounded rationality and elevate LLMs to reliable\npsychological AI. This work suggests that the non-zero radii of spheres are the\nmissing components that prevent traditional deep-learning systems from reaching\nthe realm of rational reasoning and cause LLMs to be trapped in the swamp of\nhallucination.",
        "pdf_link": "https://arxiv.org/pdf/2403.15297v1.pdf"
    },
    {
        "title": "Linguistic Intelligence in Large Language Models for Telecommunications",
        "authors": [
            "Tasnim Ahmed",
            "Nicola Piovesan",
            "Antonio De Domenico",
            "Salimur Choudhury"
        ],
        "published": "2024-02-24T14:01:07Z",
        "summary": "Large Language Models (LLMs) have emerged as a significant advancement in the\nfield of Natural Language Processing (NLP), demonstrating remarkable\ncapabilities in language generation and other language-centric tasks. Despite\ntheir evaluation across a multitude of analytical and reasoning tasks in\nvarious scientific domains, a comprehensive exploration of their knowledge and\nunderstanding within the realm of natural language tasks in the\ntelecommunications domain is still needed. This study, therefore, seeks to\nevaluate the knowledge and understanding capabilities of LLMs within this\ndomain. To achieve this, we conduct an exhaustive zero-shot evaluation of four\nprominent LLMs-Llama-2, Falcon, Mistral, and Zephyr. These models require fewer\nresources than ChatGPT, making them suitable for resource-constrained\nenvironments. Their performance is compared with state-of-the-art, fine-tuned\nmodels. To the best of our knowledge, this is the first work to extensively\nevaluate and compare the understanding of LLMs across multiple language-centric\ntasks in this domain. Our evaluation reveals that zero-shot LLMs can achieve\nperformance levels comparable to the current state-of-the-art fine-tuned\nmodels. This indicates that pretraining on extensive text corpora equips LLMs\nwith a degree of specialization, even within the telecommunications domain. We\nalso observe that no single LLM consistently outperforms others, and the\nperformance of different LLMs can fluctuate. Although their performance lags\nbehind fine-tuned models, our findings underscore the potential of LLMs as a\nvaluable resource for understanding various aspects of this field that lack\nlarge annotated data.",
        "pdf_link": "https://arxiv.org/pdf/2402.15818v1.pdf"
    },
    {
        "title": "Efficient Pruning of Large Language Model with Adaptive Estimation Fusion",
        "authors": [
            "Jun Liu",
            "Chao Wu",
            "Changdi Yang",
            "Hao Tang",
            "Haoye Dong",
            "Zhenglun Kong",
            "Geng Yuan",
            "Wei Niu",
            "Dong Huang",
            "Yanzhi Wang"
        ],
        "published": "2024-03-16T04:12:50Z",
        "summary": "Large language models (LLMs) have become crucial for many generative\ndownstream tasks, leading to an inevitable trend and significant challenge to\ndeploy them efficiently on resource-constrained devices. Structured pruning is\na widely used method to address this challenge. However, when dealing with the\ncomplex structure of the multiple decoder layers, general methods often employ\ncommon estimation approaches for pruning. These approaches lead to a decline in\naccuracy for specific downstream tasks. In this paper, we introduce a simple\nyet efficient method that adaptively models the importance of each\nsubstructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained\nestimations based on the results from complex and multilayer structures. All\naspects of our design seamlessly integrate into the endto-end pruning\nframework. Our experimental results, compared with state-of-the-art methods on\nmainstream datasets, demonstrate average accuracy improvements of 1.1%, 1.02%,\n2.0%, and 1.2% for LLaMa-7B,Vicuna-7B, Baichuan-7B, and Bloom-7b1,\nrespectively.",
        "pdf_link": "https://arxiv.org/pdf/2403.10799v1.pdf"
    },
    {
        "title": "The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate",
        "authors": [
            "Juhyun Oh",
            "Eunsu Kim",
            "Inha Cha",
            "Alice Oh"
        ],
        "published": "2024-02-09T06:16:08Z",
        "summary": "This paper explores the assumption that Large Language Models (LLMs) skilled\nin generation tasks are equally adept as evaluators. We assess the performance\nof three LLMs and one open-source LM in Question-Answering (QA) and evaluation\ntasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a\nsignificant disparity, with LLMs exhibiting lower performance in evaluation\ntasks compared to generation tasks. Intriguingly, we discover instances of\nunfaithful evaluation where models accurately evaluate answers in areas where\nthey lack competence, underscoring the need to examine the faithfulness and\ntrustworthiness of LLMs as evaluators. This study contributes to the\nunderstanding of \"the Generative AI Paradox\" (West et al., 2023), highlighting\na need to explore the correlation between generative excellence and evaluation\nproficiency, and the necessity to scrutinize the faithfulness aspect in model\nevaluations.",
        "pdf_link": "https://arxiv.org/pdf/2402.06204v1.pdf"
    },
    {
        "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers",
        "authors": [
            "Shuzhou Yuan",
            "Ercong Nie",
            "Bolei Ma",
            "Michael F\u00e4rber"
        ],
        "published": "2024-02-18T20:47:10Z",
        "summary": "Large Language Models (LLMs) possess outstanding capabilities in addressing\nvarious natural language processing (NLP) tasks. However, the sheer size of\nthese models poses challenges in terms of storage, training and inference due\nto the inclusion of billions of parameters through layer stacking. While\ntraditional approaches such as model pruning or distillation offer ways for\nreducing model size, they often come at the expense of performance retention.\nIn our investigation, we systematically explore the approach of reducing the\nnumber of layers in LLMs. Surprisingly, we observe that even with fewer layers,\nLLMs maintain similar or better performance levels, particularly in\nprompt-based fine-tuning for text classification tasks. Remarkably, in certain\ncases, models with a single layer outperform their fully layered counterparts.\nThese findings offer valuable insights for future work aimed at mitigating the\nsize constraints of LLMs while preserving their performance, thereby opening\navenues for significantly more efficient use of LLMs.",
        "pdf_link": "https://arxiv.org/pdf/2402.11700v1.pdf"
    },
    {
        "title": "Can large language models explore in-context?",
        "authors": [
            "Akshay Krishnamurthy",
            "Keegan Harris",
            "Dylan J. Foster",
            "Cyril Zhang",
            "Aleksandrs Slivkins"
        ],
        "published": "2024-03-22T17:50:43Z",
        "summary": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
        "pdf_link": "https://arxiv.org/pdf/2403.15371v1.pdf"
    },
    {
        "title": "AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving",
        "authors": [
            "Bin Gao",
            "Zhuomin He",
            "Puru Sharma",
            "Qingxuan Kang",
            "Djordje Jevdjic",
            "Junbo Deng",
            "Xingkun Yang",
            "Zhou Yu",
            "Pengfei Zuo"
        ],
        "published": "2024-03-23T10:42:49Z",
        "summary": "Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nfor executing multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes AttentionStore,\na new attention mechanism that enables the reuse of KV caches (i.e., attention\nreuse) across multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. AttentionStore maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nAttentionStore employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, AttentionStore\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, AttentionStore enables the saved KV caches to remain valid via\ndecoupling the positional encoding and effectively truncating the KV caches.\nExtensive experimental results demonstrate that AttentionStore significantly\ndecreases the time to the first token (TTFT) by up to 88%, improves the prompt\nprefilling throughput by 8.2$\\times$ for multi-turn conversations, and reduces\nthe end-to-end inference cost by up to 56%. For long sequence inference,\nAttentionStore reduces the TTFT by up to 95% and improves the prompt prefilling\nthroughput by 22$\\times$.",
        "pdf_link": "https://arxiv.org/pdf/2403.19708v1.pdf"
    },
    {
        "title": "Rethinking Generative Large Language Model Evaluation for Semantic Comprehension",
        "authors": [
            "Fangyun Wei",
            "Xi Chen",
            "Lin Luo"
        ],
        "published": "2024-03-12T17:59:48Z",
        "summary": "Despite their sophisticated capabilities, large language models (LLMs)\nencounter a major hurdle in effective assessment. This paper first revisits the\nprevalent evaluation method-multiple choice question answering (MCQA), which\nallows for straightforward accuracy measurement. Through a comprehensive\nevaluation of 24 models across 11 benchmarks, we highlight several potential\ndrawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation\nand the generation of open-ended responses in practical scenarios. In response,\nwe introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,\nGoogle-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with\nGPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This\nsystem is designed to mirror real-world usage, and for this purpose, we have\ncompiled a new benchmark called ``Real-world questions'' (RWQ), comprising\n20,772 authentic user inquiries. Additionally, we thoroughly analyze the\ncharacteristics of our system and compare it with prior leaderboards like\nAlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo\nsystem, the feasibility of registering new models, and its potential to reshape\nLLM leaderboards.",
        "pdf_link": "https://arxiv.org/pdf/2403.07872v1.pdf"
    },
    {
        "title": "Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs",
        "authors": [
            "Yiliang Zhou",
            "Hanley Ong",
            "Patrick Kennedy",
            "Carol Wu",
            "Jacob Kazam",
            "Keith Hentel",
            "Adam Flanders",
            "George Shih",
            "Yifan Peng"
        ],
        "published": "2024-03-22T17:27:18Z",
        "summary": "The study examines the application of GPT-4V, a multi-modal large language\nmodel equipped with visual recognition, in detecting radiological findings from\na set of 100 chest radiographs and suggests that GPT-4V is currently not ready\nfor real-world diagnostic usage in interpreting chest radiographs.",
        "pdf_link": "https://arxiv.org/pdf/2403.15528v2.pdf"
    },
    {
        "title": "HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA",
        "authors": [
            "Xinyue Chen",
            "Pengyu Gao",
            "Jiangjiang Song",
            "Xiaoyang Tan"
        ],
        "published": "2024-02-01T02:24:15Z",
        "summary": "As language model agents leveraging external tools rapidly evolve,\nsignificant progress has been made in question-answering(QA) methodologies\nutilizing supplementary documents and the Retrieval-Augmented Generation (RAG)\napproach. This advancement has improved the response quality of language models\nand alleviates the appearance of hallucination. However, these methods exhibit\nlimited retrieval accuracy when faced with massive indistinguishable documents,\npresenting notable challenges in their practical application. In response to\nthese emerging challenges, we present HiQA, an advanced framework for\nmulti-document question-answering (MDQA) that integrates cascading metadata\ninto content as well as a multi-route retrieval mechanism. We also release a\nbenchmark called MasQA to evaluate and research in MDQA. Finally, HiQA\ndemonstrates the state-of-the-art performance in multi-document environments.",
        "pdf_link": "https://arxiv.org/pdf/2402.01767v1.pdf"
    },
    {
        "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Wei Zou",
            "Runpeng Geng",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published": "2024-02-12T18:28:36Z",
        "summary": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate those limitations. In particular, given a question, RAG retrieves\nrelevant knowledge from a knowledge database to augment the input of the LLM.\nFor instance, the retrieved knowledge could be a set of top-k texts that are\nmost semantically similar to the given question when the knowledge database\ncontains millions of texts collected from Wikipedia. As a result, the LLM could\nutilize the retrieved knowledge as the context to generate an answer for the\ngiven question. Existing studies mainly focus on improving the accuracy or\nefficiency of RAG, leaving its security largely unexplored. We aim to bridge\nthe gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge\npoisoning attacks to RAG, where an attacker could inject a few poisoned texts\ninto the knowledge database such that the LLM generates an attacker-chosen\ntarget answer for an attacker-chosen target question. We formulate knowledge\npoisoning attacks as an optimization problem, whose solution is a set of\npoisoned texts. Depending on the background knowledge (e.g., black-box and\nwhite-box settings) of an attacker on the RAG, we propose two solutions to\nsolve the optimization problem, respectively. Our results on multiple benchmark\ndatasets and LLMs show our attacks could achieve 90% attack success rates when\ninjecting 5 poisoned texts for each target question into a database with\nmillions of texts. We also evaluate recent defenses and our results show they\nare insufficient to defend against our attacks, highlighting the need for new\ndefenses.",
        "pdf_link": "https://arxiv.org/pdf/2402.07867v1.pdf"
    },
    {
        "title": "Poisoning Programs by Un-Repairing Code: Security Concerns of AI-generated Code",
        "authors": [
            "Cristina Improta"
        ],
        "published": "2024-03-11T12:47:04Z",
        "summary": "AI-based code generators have gained a fundamental role in assisting\ndevelopers in writing software starting from natural language (NL). However,\nsince these large language models are trained on massive volumes of data\ncollected from unreliable online sources (e.g., GitHub, Hugging Face), AI\nmodels become an easy target for data poisoning attacks, in which an attacker\ncorrupts the training data by injecting a small amount of poison into it, i.e.,\nastutely crafted malicious samples. In this position paper, we address the\nsecurity of AI code generators by identifying a novel data poisoning attack\nthat results in the generation of vulnerable code. Next, we devise an extensive\nevaluation of how these attacks impact state-of-the-art models for code\ngeneration. Lastly, we discuss potential solutions to overcome this threat.",
        "pdf_link": "https://arxiv.org/pdf/2403.06675v1.pdf"
    },
    {
        "title": "Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search",
        "authors": [
            "Chanwoong Yoon",
            "Gangwoo Kim",
            "Byeongguk Jeon",
            "Sungdong Kim",
            "Yohan Jo",
            "Jaewoo Kang"
        ],
        "published": "2024-02-19T04:41:31Z",
        "summary": "Conversational search, unlike single-turn retrieval tasks, requires\nunderstanding the current question within a dialogue context. The common\napproach of rewrite-then-retrieve aims to decontextualize questions to be\nself-sufficient for off-the-shelf retrievers, but most existing methods produce\nsub-optimal query rewrites due to the limited ability to incorporate signals\nfrom the retrieval results. To overcome this limitation, we present a novel\nframework RetPO (Retriever's Preference Optimization), which is designed to\noptimize a language model (LM) for reformulating search queries in line with\nthe preferences of the target retrieval systems. The process begins by\nprompting a large LM to produce various potential rewrites and then collects\nretrieval performance for these rewrites as the retrievers' preferences.\nThrough the process, we construct a large-scale dataset called RF collection,\ncontaining Retrievers' Feedback on over 410K query rewrites across 12K\nconversations. Furthermore, we fine-tune a smaller LM using this dataset to\nalign it with the retrievers' preferences as feedback. The resulting model\nachieves state-of-the-art performance on two recent conversational search\nbenchmarks, significantly outperforming existing baselines, including GPT-3.5.",
        "pdf_link": "https://arxiv.org/pdf/2402.11827v1.pdf"
    },
    {
        "title": "Understanding the planning of LLM agents: A survey",
        "authors": [
            "Xu Huang",
            "Weiwen Liu",
            "Xiaolong Chen",
            "Xingmei Wang",
            "Hao Wang",
            "Defu Lian",
            "Yasheng Wang",
            "Ruiming Tang",
            "Enhong Chen"
        ],
        "published": "2024-02-05T04:25:24Z",
        "summary": "As Large Language Models (LLMs) have shown significant intelligence, the\nprogress to leverage LLMs as planning modules of autonomous agents has\nattracted more attention. This survey provides the first systematic view of\nLLM-based agents planning, covering recent works aiming to improve planning\nability. We provide a taxonomy of existing works on LLM-Agent planning, which\ncan be categorized into Task Decomposition, Plan Selection, External Module,\nReflection and Memory. Comprehensive analyses are conducted for each direction,\nand further challenges for the field of research are discussed.",
        "pdf_link": "https://arxiv.org/pdf/2402.02716v1.pdf"
    },
    {
        "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
        "authors": [
            "Wenkai Yang",
            "Xiaohan Bi",
            "Yankai Lin",
            "Sishuo Chen",
            "Jie Zhou",
            "Xu Sun"
        ],
        "published": "2024-02-17T06:48:45Z",
        "summary": "Leveraging the rapid development of Large Language Models LLMs, LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis on the different forms of\nagent backdoor attacks. Specifically, from the perspective of the final\nattacking outcomes, the attacker can either choose to manipulate the final\noutput distribution, or only introduce malicious behavior in the intermediate\nreasoning process, while keeping the final output correct. Furthermore, the\nformer category can be divided into two subcategories based on trigger\nlocations: the backdoor trigger can be hidden either in the user query or in an\nintermediate observation returned by the external environment. We propose the\ncorresponding data poisoning mechanisms to implement the above variations of\nagent backdoor attacks on two typical agent tasks, web shopping and tool\nutilization. Extensive experiments show that LLM-based agents suffer severely\nfrom backdoor attacks, indicating an urgent need for further research on the\ndevelopment of defenses against backdoor attacks on LLM-based agents. Warning:\nThis paper may contain biased content.",
        "pdf_link": "https://arxiv.org/pdf/2402.11208v1.pdf"
    },
    {
        "title": "ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling",
        "authors": [
            "Lingxi Zhang",
            "Yue Yu",
            "Kuan Wang",
            "Chao Zhang"
        ],
        "published": "2024-02-21T05:41:34Z",
        "summary": "Retrieval-augmented generation enhances large language models (LLMs) by\nincorporating relevant information from external knowledge sources. This\nenables LLMs to adapt to specific domains and mitigate hallucinations in\nknowledge-intensive tasks. However, existing retrievers are often misaligned\nwith LLMs due to their separate training processes and the black-box nature of\nLLMs. To address this challenge, we propose ARL2, a retriever learning\ntechnique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and\nscore relevant evidence, enabling learning the retriever from robust LLM\nsupervision. Furthermore, ARL2 uses an adaptive self-training strategy for\ncurating high-quality and diverse relevance data, which can effectively reduce\nthe annotation cost. Extensive experiments demonstrate the effectiveness of\nARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared\nto the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer\nlearning capabilities and strong zero-shot generalization abilities. Our code\nwill be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.",
        "pdf_link": "https://arxiv.org/pdf/2402.13542v1.pdf"
    },
    {
        "title": "Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem",
        "authors": [
            "Yuhong Sun",
            "Zhangyue Yin",
            "Qipeng Guo",
            "Jiawen Wu",
            "Xipeng Qiu",
            "Hui Zhao"
        ],
        "published": "2024-03-06T09:06:34Z",
        "summary": "Large language models (LLMs) are highly effective in various natural language\nprocessing (NLP) tasks. However, they are susceptible to producing unreliable\nconjectures in ambiguous contexts called hallucination. This paper presents a\nnew method for evaluating LLM hallucination in Question Answering (QA) based on\nthe unanswerable math word problem (MWP). To support this approach, we\ninnovatively develop a dataset called Unanswerable Math Word Problem (UMWP)\nwhich comprises 5200 questions across five categories. We developed an\nevaluation methodology combining text similarity and mathematical expression\ndetection to determine whether LLM considers the question unanswerable. The\nresults of extensive experiments conducted on 31 LLMs, including GPT-3,\nInstructGPT, LLaMA, and Claude, demonstrate that in-context learning and\nreinforcement learning with human feedback (RLHF) training significantly\nenhance the model's ability to avoid hallucination. We show that utilizing MWP\nis a reliable and effective approach to assess hallucination. Our code and data\nare available at https://github.com/Yuki-Asuuna/UMWP.",
        "pdf_link": "https://arxiv.org/pdf/2403.03558v1.pdf"
    },
    {
        "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
        "authors": [
            "Hanting Chen",
            "Zhicheng Liu",
            "Xutao Wang",
            "Yuchuan Tian",
            "Yunhe Wang"
        ],
        "published": "2024-03-29T02:32:15Z",
        "summary": "In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.",
        "pdf_link": "https://arxiv.org/pdf/2403.19928v2.pdf"
    },
    {
        "title": "GPT4Battery: An LLM-driven Framework for Adaptive State of Health Estimation of Raw Li-ion Batteries",
        "authors": [
            "Yuyuan Feng",
            "Guosheng Hu",
            "Zhihong Zhang"
        ],
        "published": "2024-01-30T14:47:15Z",
        "summary": "State of health (SOH) is a crucial indicator for assessing the degradation\nlevel of batteries that cannot be measured directly but requires estimation.\nAccurate SOH estimation enhances detection, control, and feedback for Li-ion\nbatteries, allowing for safe and efficient energy management and guiding the\ndevelopment of new-generation batteries. Despite the significant progress in\ndata-driven SOH estimation, the time and resource-consuming degradation\nexperiments for generating lifelong training data pose a challenge in\nestablishing one large model capable of handling diverse types of Li-ion\nbatteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity.\nHence, this paper utilizes the strong generalization capability of large\nlanguage model (LLM) to proposes a novel framework for adaptable SOH estimation\nacross diverse batteries. To match the real scenario where unlabeled data\nsequentially arrives in use with distribution shifts, the proposed model is\nmodified by a test-time training technique to ensure estimation accuracy even\nat the battery's end of life. The validation results demonstrate that the\nproposed framework achieves state-of-the-art accuracy on four widely recognized\ndatasets collected from 62 batteries. Furthermore, we analyze the theoretical\nchallenges of cross-battery estimation and provide a quantitative explanation\nof the effectiveness of our method.",
        "pdf_link": "https://arxiv.org/pdf/2402.00068v1.pdf"
    },
    {
        "title": "Data-driven Discovery with Large Generative Models",
        "authors": [
            "Bodhisattwa Prasad Majumder",
            "Harshit Surana",
            "Dhruv Agarwal",
            "Sanchaita Hazra",
            "Ashish Sabharwal",
            "Peter Clark"
        ],
        "published": "2024-02-21T08:26:43Z",
        "summary": "With the accumulation of data at an unprecedented rate, its potential to fuel\nscientific discovery is growing exponentially. This position paper urges the\nMachine Learning (ML) community to exploit the capabilities of large generative\nmodels (LGMs) to develop automated systems for end-to-end data-driven discovery\n-- a paradigm encompassing the search and verification of hypotheses purely\nfrom a set of provided datasets, without the need for additional data\ncollection or physical experiments. We first outline several desiderata for an\nideal data-driven discovery system. Then, through DATAVOYAGER, a\nproof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of\nthese desiderata -- a feat previously unattainable -- while also highlighting\nimportant limitations in the current system that open up opportunities for\nnovel ML research. We contend that achieving accurate, reliable, and robust\nend-to-end discovery systems solely through the current capabilities of LGMs is\nchallenging. We instead advocate for fail-proof tool integration, along with\nactive user moderation through feedback mechanisms, to foster data-driven\nscientific discoveries with efficiency and reproducibility.",
        "pdf_link": "https://arxiv.org/pdf/2402.13610v1.pdf"
    },
    {
        "title": "Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code",
        "authors": [
            "Beiqi Zhang",
            "Peng Liang",
            "Qiong Feng",
            "Yujia Fu",
            "Zengyang Li"
        ],
        "published": "2024-01-25T13:39:54Z",
        "summary": "As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released on\nSeptember 2023, functions as an interactive tool aims at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot's\nability to fix the code smells it generates. To this end, we built a dataset\ncomprising 102 code smells in Copilot-generated Python code. Our aim is to\nfirst explore the occurrence of code smells in Copilot-generated Python code\nand then evaluate the effectiveness of Copilot in fixing these code smells\nemploying different prompts. The results show that 8 out of 10 types of Python\nsmells can be detected in Copilot-generated Python code, among which\nMultiply-Nested Container is the most common one. For these code smells,\nCopilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing\nPython code smells generated by Copilot itself. Besides, the effectiveness of\nCopilot Chat in fixing these smells can be improved with the provision of more\ndetailed prompts. However, using Copilot Chat to fix these smells might\nintroduce new code smells.",
        "pdf_link": "https://arxiv.org/pdf/2401.14176v1.pdf"
    },
    {
        "title": "Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research",
        "authors": [
            "Brenda Y. Miao",
            "Irene Y. Chen",
            "Christopher YK Williams",
            "Jays\u00f3n Davidson",
            "Augusto Garcia-Agundez",
            "Harry Sun",
            "Travis Zack",
            "Atul J. Butte",
            "Madhumita Sushil"
        ],
        "published": "2024-03-05T00:27:43Z",
        "summary": "Recent advances in generative models, including large language models (LLMs),\nvision language models (VLMs), and diffusion models, have accelerated the field\nof natural language and image processing in medicine and marked a significant\nparadigm shift in how biomedical models can be developed and deployed. While\nthese models are highly adaptable to new tasks, scaling and evaluating their\nusage presents new challenges not addressed in previous frameworks. In\nparticular, the ability of these models to produce useful outputs with little\nto no specialized training data (\"zero-\" or \"few-shot\" approaches), as well as\nthe open-ended nature of their outputs, necessitate the development of updated\nguidelines in using and evaluating these models. In response to gaps in\nstandards and best practices for the development of clinical AI tools\nidentified by US Executive Order 141103 and several emerging national networks\nfor clinical AI evaluation, we begin to formalize some of these guidelines by\nbuilding on the \"Minimum information about clinical artificial intelligence\nmodeling\" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in\n2020, provided a set of six steps with guidelines on the minimum information\nnecessary to encourage transparent, reproducible research for artificial\nintelligence (AI) in medicine. Here, we propose modifications to the original\nchecklist that highlight differences in training, evaluation, interpretability,\nand reproducibility of generative models compared to traditional AI models for\nclinical research. This updated checklist also seeks to clarify cohort\nselection reporting and adds additional items on alignment with ethical\nstandards.",
        "pdf_link": "https://arxiv.org/pdf/2403.02558v1.pdf"
    },
    {
        "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
        "authors": [
            "Chi-Min Chan",
            "Chunpu Xu",
            "Ruibin Yuan",
            "Hongyin Luo",
            "Wei Xue",
            "Yike Guo",
            "Jie Fu"
        ],
        "published": "2024-03-31T08:58:54Z",
        "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are prone to\ngenerating inaccurate or hallucinatory responses. This limitation stems from\ntheir reliance on vast pretraining datasets, making them susceptible to errors\nin unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation\n(RAG) addresses this by incorporating external, relevant documents into the\nresponse generation process, thus leveraging non-parametric knowledge alongside\nLLMs' in-context learning abilities. However, existing RAG implementations\nprimarily focus on initial input for context retrieval, overlooking the nuances\nof ambiguous or complex queries that necessitate further clarification or\ndecomposition for accurate responses. To this end, we propose learning to\nRefine Query for Retrieval Augmented Generation (RQ-RAG) in this paper,\nendeavoring to enhance the model by equipping it with capabilities for explicit\nrewriting, decomposition, and disambiguation. Our experimental results indicate\nthat our method, when applied to a 7B Llama2 model, surpasses the previous\nstate-of-the-art (SOTA) by an average of 1.9\\% across three single-hop QA\ndatasets, and also demonstrates enhanced performance in handling complex,\nmulti-hop QA datasets. Our code is available at\nhttps://github.com/chanchimin/RQ-RAG.",
        "pdf_link": "https://arxiv.org/pdf/2404.00610v1.pdf"
    },
    {
        "title": "UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities",
        "authors": [
            "Yangning Li",
            "Qingsong Lv",
            "Tianyu Yu",
            "Yinghui Li",
            "Shulin Huang",
            "Tingwei Lu",
            "Xuming Hu",
            "Wenhao JIang",
            "Hai-Tao Zheng",
            "Hui Wang"
        ],
        "published": "2024-03-07T06:10:02Z",
        "summary": "Entity Set Expansion (ESE) aims to identify new entities belonging to the\nsame semantic class as a given set of seed entities. Traditional methods\nprimarily relied on positive seed entities to represent a target semantic\nclass, which poses challenge for the representation of ultra-fine-grained\nsemantic classes. Ultra-fine-grained semantic classes are defined based on\nfine-grained semantic classes with more specific attribute constraints.\nDescribing it with positive seed entities alone cause two issues: (i) Ambiguity\namong ultra-fine-grained semantic classes. (ii) Inability to define \"unwanted\"\nsemantic. Due to these inherent shortcomings, previous methods struggle to\naddress the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first\nintroduce negative seed entities in the inputs, which belong to the same\nfine-grained semantic class as the positive seed entities but differ in certain\nattributes. Negative seed entities eliminate the semantic ambiguity by contrast\nbetween positive and negative attributes. Meanwhile, it provide a\nstraightforward way to express \"unwanted\". To assess model performance in\nUltra-ESE, we constructed UltraWiki, the first large-scale dataset tailored for\nUltra-ESE. UltraWiki encompasses 236 ultra-fine-grained semantic classes, where\neach query of them is represented with 3-5 positive and negative seed entities.\nA retrieval-based framework RetExpan and a generation-based framework GenExpan\nare proposed to comprehensively assess the efficacy of large language models\nfrom two different paradigms in Ultra-ESE. Moreover, we devised three\nstrategies to enhance models' comprehension of ultra-fine-grained entities\nsemantics: contrastive learning, retrieval augmentation, and chain-of-thought\nreasoning. Extensive experiments confirm the effectiveness of our proposed\nstrategies and also reveal that there remains a large space for improvement in\nUltra-ESE.",
        "pdf_link": "https://arxiv.org/pdf/2403.04247v1.pdf"
    },
    {
        "title": "Are Large Language Models Aligned with People's Social Intuitions for Human-Robot Interactions?",
        "authors": [
            "Lennart Wachowiak",
            "Andrew Coles",
            "Oya Celiktutan",
            "Gerard Canal"
        ],
        "published": "2024-03-08T22:23:23Z",
        "summary": "Large language models (LLMs) are increasingly used in robotics, especially\nfor high-level action planning. Meanwhile, many robotics applications involve\nhuman supervisors or collaborators. Hence, it is crucial for LLMs to generate\nsocially acceptable actions that align with people's preferences and values. In\nthis work, we test whether LLMs capture people's intuitions about behavior\njudgments and communication preferences in human-robot interaction (HRI)\nscenarios. For evaluation, we reproduce three HRI user studies, comparing the\noutput of LLMs with that of real participants. We find that GPT-4 strongly\noutperforms other models, generating answers that correlate strongly with\nusers' answers in two studies $\\unicode{x2014}$ the first study dealing with\nselecting the most appropriate communicative act for a robot in various\nsituations ($r_s$ = 0.82), and the second with judging the desirability,\nintentionality, and surprisingness of behavior ($r_s$ = 0.83). However, for the\nlast study, testing whether people judge the behavior of robots and humans\ndifferently, no model achieves strong correlations. Moreover, we show that\nvision models fail to capture the essence of video stimuli and that LLMs tend\nto rate different communicative acts and behavior desirability higher than\npeople.",
        "pdf_link": "https://arxiv.org/pdf/2403.05701v1.pdf"
    },
    {
        "title": "\"Which LLM should I use?\": Evaluating LLMs for tasks performed by Undergraduate Computer Science Students",
        "authors": [
            "Vibhor Agarwal",
            "Madhav Krishan Garg",
            "Sahiti Dharmavaram",
            "Dhruv Kumar"
        ],
        "published": "2024-01-22T15:11:36Z",
        "summary": "This study evaluates the effectiveness of various large language models\n(LLMs) in performing tasks common among undergraduate computer science\nstudents. Although a number of research studies in the computing education\ncommunity have explored the possibility of using LLMs for a variety of tasks,\nthere is a lack of comprehensive research comparing different LLMs and\nevaluating which LLMs are most effective for different tasks. Our research\nsystematically assesses some of the publicly available LLMs such as Google\nBard, ChatGPT(3.5), GitHub Copilot Chat, and Microsoft Copilot across diverse\ntasks commonly encountered by undergraduate computer science students in India.\nThese tasks include code explanation and documentation, solving class\nassignments, technical interview preparation, learning new concepts and\nframeworks, and email writing. Evaluation for these tasks was carried out by\npre-final year and final year undergraduate computer science students and\nprovides insights into the models' strengths and limitations. This study aims\nto guide students as well as instructors in selecting suitable LLMs for any\nspecific task and offers valuable insights on how LLMs can be used\nconstructively by students and instructors.",
        "pdf_link": "https://arxiv.org/pdf/2402.01687v2.pdf"
    },
    {
        "title": "Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning",
        "authors": [
            "Rao Fu",
            "Jingyu Liu",
            "Xilun Chen",
            "Yixin Nie",
            "Wenhan Xiong"
        ],
        "published": "2024-03-18T01:18:48Z",
        "summary": "This paper introduces Scene-LLM, a 3D-visual-language model that enhances\nembodied agents' abilities in interactive 3D indoor environments by integrating\nthe reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a\nhybrid 3D visual feature representation, that incorporates dense spatial\ninformation and supports scene state updates. The model employs a projection\nlayer to efficiently project these features in the pre-trained textual\nembedding space, enabling effective interpretation of 3D visual information.\nUnique to our approach is the integration of both scene-level and ego-centric\n3D information. This combination is pivotal for interactive planning, where\nscene-level data supports global planning and ego-centric data is important for\nlocalization. Notably, we use ego-centric 3D frame features for feature\nalignment, an efficient technique that enhances the model's ability to align\nfeatures of small objects within the scene. Our experiments with Scene-LLM\ndemonstrate its strong capabilities in dense captioning, question answering,\nand interactive planning. We believe Scene-LLM advances the field of 3D visual\nunderstanding and reasoning, offering new possibilities for sophisticated agent\ninteractions in indoor settings.",
        "pdf_link": "https://arxiv.org/pdf/2403.11401v2.pdf"
    },
    {
        "title": "The Reasoning Under Uncertainty Trap: A Structural AI Risk",
        "authors": [
            "Toby D. Pilditch"
        ],
        "published": "2024-01-29T17:16:57Z",
        "summary": "This report examines a novel risk associated with current (and projected) AI\ntools. Making effective decisions about future actions requires us to reason\nunder uncertainty (RUU), and doing so is essential to many critical real world\nproblems. Overfaced by this challenge, there is growing demand for AI tools\nlike LLMs to assist decision-makers. Having evidenced this demand and the\nincentives behind it, we expose a growing risk: we 1) do not currently\nsufficiently understand LLM capabilities in this regard, and 2) have no\nguarantees of performance given fundamental computational explosiveness and\ndeep uncertainty constraints on accuracy. This report provides an exposition of\nwhat makes RUU so challenging for both humans and machines, and relates these\ndifficulties to prospective AI timelines and capabilities. Having established\nthis current potential misuse risk, we go on to expose how this seemingly\nadditive risk (more misuse additively contributed to potential harm) in fact\nhas multiplicative properties. Specifically, we detail how this misuse risk\nconnects to a wider network of underlying structural risks (e.g., shifting\nincentives, limited transparency, and feedback loops) to produce non-linear\nharms. We go on to provide a solutions roadmap that targets multiple leverage\npoints in the structure of the problem. This includes recommendations for all\ninvolved actors (prospective users, developers, and policy-makers) and enfolds\ninsights from areas including Decision-making Under Deep Uncertainty and\ncomplex systems theory. We argue this report serves not only to raise awareness\n(and subsequently mitigate/correct) of a current, novel AI risk, but also\nawareness of the underlying class of structural risks by illustrating how their\ninterconnected nature poses twin-dangers of camouflaging their presence, whilst\namplifying their potential effects.",
        "pdf_link": "https://arxiv.org/pdf/2402.01743v1.pdf"
    }
]